
Access Control,  
Security, and Trust
A Logical Approach

CHAPMAN & HALL/CRC
CRYPTOGRAPHY AND NETWORK SECURITY
Series Editor
Douglas R. Stinson
Published Titles 
 
Jonathan Katz and Yehuda Lindell, Introduction to Modern 
Cryptography
Antoine Joux, Algorithmic Cryptanalysis
M. Jason Hinek, Cryptanalysis of RSA and Its Variants
Burton Rosenberg, Handbook of Financial Cryptography and Security
Shiu-Kai Chin and Susan Older, Access Control, Security, and Trust: 
A Logical Approach
Forthcoming Titles 
 
Maria Isabel Vasco, Spyros Magliveras, and Rainer Steinwandt, 
Group Theoretic Cryptography

Chapman & Hall/CRC
CRYPTOGRAPHY AND NETWORK SECURITY
Shiu-Kai Chin
Syracuse University
Syracuse, New York, USA
Susan Older
Syracuse University
Syracuse, New York, USA
Access Control,  
Security, and Trust
A Logical Approach

About the cover: The cover image of a mother loon carrying her chick across the water depicts the interde-
pendent nature of this book‚Äôs main themes: access-control, security, and trust. Loons are fiercely protective 
of their offspring. In turn, the chicks often ride on their parents‚Äô backs, trusting them to provide both warmth 
and protection from predators.
Chapman & Hall/CRC
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
¬© 2011 by Taylor and Francis Group, LLC
Chapman & Hall/CRC is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Printed in the United States of America on acid-free paper
10 9 8 7 6 5 4 3 2 1
International Standard Book Number-13: 978-1-58488-863-5 (Ebook-PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts 
have been made to publish reliable data and information, but the author and publisher cannot assume 
responsibility for the validity of all materials or the consequences of their use. The authors and publishers 
have attempted to trace the copyright holders of all material reproduced in this publication and apologize to 
copyright holders if permission to publish in this form has not been obtained. If any copyright material has 
not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmit-
ted, or utilized in any form by any electronic, mechanical, or other means, now known or hereafter invented, 
including photocopying, microfilming, and recording, or in any information storage or retrieval system, 
without written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.
com (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC), 222 Rosewood 
Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and 
registration for a variety of users. For organizations that have been granted a photocopy license by the CCC, 
a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used 
only for identification and explanation without intent to infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com 

To Linda, Benjamin, Emily, and my mom for their love and support
To Garth, for his patience; and to Ryan, who couldn‚Äôt wait for this book to be
completed


Contents
List of Tables
xiii
List of Figures
xv
Preface
xix
1
Access Control, Security, Trust, and Logic
1
1.1
Deconstructing Access-Control Decisions . . . . . . . . . . . . . .
3
1.2
A Logical Approach to Access Control
. . . . . . . . . . . . . . .
6
I
Preliminaries
9
2
A Language for Access Control
11
2.1
Sets and Relations
. . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.1.1
Notation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.1.2
Approaches for Mathematical Proofs
. . . . . . . . . . . .
13
2.2
Syntax
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.2.1
Principal Expressions . . . . . . . . . . . . . . . . . . . . .
17
2.2.2
Access-Control Statements . . . . . . . . . . . . . . . . . .
18
2.2.3
Well-Formed Formulas . . . . . . . . . . . . . . . . . . . .
20
2.3
Semantics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
2.3.1
Kripke Structures . . . . . . . . . . . . . . . . . . . . . . .
23
2.3.2
Semantics of the Logic . . . . . . . . . . . . . . . . . . . .
28
2.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
37
3
Reasoning about Access Control
39
3.1
Logical Rules . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3.1.1
The Taut Rule . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.1.2
The Modus Ponens Rule . . . . . . . . . . . . . . . . . . .
42
3.1.3
The Says Rule
. . . . . . . . . . . . . . . . . . . . . . . .
42
3.1.4
The MP Says Rule . . . . . . . . . . . . . . . . . . . . . .
42
3.1.5
The Speaks For Rule . . . . . . . . . . . . . . . . . . . . .
43
3.1.6
The & Says and Quoting Rules
. . . . . . . . . . . . . . .
43
3.1.7
Properties of ‚áí. . . . . . . . . . . . . . . . . . . . . . . .
43
3.1.8
The Equivalence Rule
. . . . . . . . . . . . . . . . . . . .
45
vii

viii
3.1.9
The Controls DeÔ¨Ånition
. . . . . . . . . . . . . . . . . . .
46
3.2
Formal Proofs and Theorems . . . . . . . . . . . . . . . . . . . . .
47
3.3
Soundness of Logical Rules
. . . . . . . . . . . . . . . . . . . . .
50
3.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
54
4
Basic Concepts
57
4.1
Reference Monitors
. . . . . . . . . . . . . . . . . . . . . . . . .
57
4.2
Access-Control Mechanisms: Tickets and Lists . . . . . . . . . . .
60
4.2.1
Tickets
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
4.2.2
Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
4.2.3
Logical and Pragmatic Implications . . . . . . . . . . . . .
66
4.3
Authentication
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.3.1
Two-Factor Authentication . . . . . . . . . . . . . . . . . .
68
4.3.2
Using Credentials from Other Authorities . . . . . . . . . .
70
4.3.3
Groups
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
76
5
Security Policies
77
5.1
ConÔ¨Ådentiality, Integrity, and Availability . . . . . . . . . . . . . .
77
5.2
Discretionary Security Policies . . . . . . . . . . . . . . . . . . . .
79
5.3
Mandatory Security Policies
. . . . . . . . . . . . . . . . . . . . .
81
5.4
Military Security Policies
. . . . . . . . . . . . . . . . . . . . . .
85
5.4.1
Extending the Logic with Security Levels . . . . . . . . . .
85
5.4.2
Expressing Military Security Policies . . . . . . . . . . . .
87
5.4.3
Military Security Policies: An Extended Example . . . . . .
90
5.5
Commercial Policies
. . . . . . . . . . . . . . . . . . . . . . . . .
94
5.5.1
Extending the Logic with Integrity Levels . . . . . . . . . .
95
5.5.2
Protecting Integrity . . . . . . . . . . . . . . . . . . . . . .
97
5.5.3
Strict Integrity
. . . . . . . . . . . . . . . . . . . . . . . .
98
5.5.4
An Extended Example of a Strict Integrity Policy . . . . . .
100
5.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
5.7
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
105
II
Distributed Access Control
107
6
Digital Authentication
109
6.1
Public-Key Cryptography
. . . . . . . . . . . . . . . . . . . . . .
109
6.2
EfÔ¨Åciency Mechanisms . . . . . . . . . . . . . . . . . . . . . . . .
112
6.2.1
Cryptographic Hash Functions . . . . . . . . . . . . . . . .
112
6.2.2
Data-Encryption Keys
. . . . . . . . . . . . . . . . . . . .
113
6.2.3
Digital Signatures
. . . . . . . . . . . . . . . . . . . . . .
113
6.3
Reasoning about Cryptographic Communications . . . . . . . . . .
114

ix
6.4
CertiÔ¨Åcates, CertiÔ¨Åcate Authorities, and Trust . . . . . . . . . . . .
116
6.5
Symmetric-Key Cryptography
. . . . . . . . . . . . . . . . . . . .
125
6.6
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
6.7
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
131
7
Delegation
133
7.1
Simple Delegations . . . . . . . . . . . . . . . . . . . . . . . . . .
133
7.2
Delegation and Its Properties . . . . . . . . . . . . . . . . . . . . .
135
7.3
A Delegation Example: Simple Checking
. . . . . . . . . . . . . .
141
7.3.1
Formal DeÔ¨Ånitions of Checks
. . . . . . . . . . . . . . . .
142
7.3.2
Bank Policies on Checks . . . . . . . . . . . . . . . . . . .
143
7.3.3
Operating Rules for Checks
. . . . . . . . . . . . . . . . .
144
7.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
7.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
147
8
Networks: Case Studies
149
8.1
SSL and TLS: Authentication across the Web . . . . . . . . . . . .
149
8.1.1
Handshake Protocol
. . . . . . . . . . . . . . . . . . . . .
150
8.1.2
Record Protocol
. . . . . . . . . . . . . . . . . . . . . . .
155
8.2
Kerberos: Authentication for Distributed Systems . . . . . . . . . .
157
8.2.1
Initial Authentication Requests . . . . . . . . . . . . . . . .
157
8.2.2
Requests for Service-SpeciÔ¨Åc Tickets . . . . . . . . . . . .
159
8.2.3
Requests for Services . . . . . . . . . . . . . . . . . . . . .
161
8.2.4
Proxiable Tickets . . . . . . . . . . . . . . . . . . . . . . .
162
8.3
Financial Networks . . . . . . . . . . . . . . . . . . . . . . . . . .
166
8.3.1
Electronic Clearinghouses . . . . . . . . . . . . . . . . . .
166
8.3.2
Bank Authorities, Jurisdiction, and Policies . . . . . . . . .
169
8.3.3
Bank Operating Rules
. . . . . . . . . . . . . . . . . . . .
170
8.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
172
8.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
173
III
Isolation and Sharing
175
9
A Primer on Computer Hardware
177
9.1
Ones and Zeros . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
9.2
Synchronous Design
. . . . . . . . . . . . . . . . . . . . . . . . .
178
9.2.1
Synchronous Registers . . . . . . . . . . . . . . . . . . . .
178
9.2.2
Registers with Load Control . . . . . . . . . . . . . . . . .
179
9.2.3
Registers with Tri-State Outputs . . . . . . . . . . . . . . .
179
9.2.4
Combinational Logic and Functions . . . . . . . . . . . . .
182
9.2.5
Arithmetic Logic Units . . . . . . . . . . . . . . . . . . . .
184
9.3
Microcode
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
9.3.1
Data Paths and Control Paths . . . . . . . . . . . . . . . . .
190
9.3.2
Microprogramming . . . . . . . . . . . . . . . . . . . . . .
192

x
9.4
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
9.5
Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
195
10 Virtual Machines and Memory Protection
197
10.1 A Simple Processor . . . . . . . . . . . . . . . . . . . . . . . . . .
198
10.1.1 Processor Components . . . . . . . . . . . . . . . . . . . .
199
10.1.2 Machine Instructions . . . . . . . . . . . . . . . . . . . . .
201
10.2 Processors with Memory Segmentation
. . . . . . . . . . . . . . .
204
10.2.1 Segmentation Using a Relocation Register . . . . . . . . . .
204
10.2.2 Processor State and Instructions . . . . . . . . . . . . . . .
207
10.2.3 Program Status Word . . . . . . . . . . . . . . . . . . . . .
207
10.2.4 Traps . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208
10.3 Controlling Access to Memory and Segmentation Registers
. . . .
209
10.3.1 Access to Program Memory . . . . . . . . . . . . . . . . .
210
10.3.2 Implementation Details . . . . . . . . . . . . . . . . . . . .
212
10.3.3 Access to the Relocation Register . . . . . . . . . . . . . .
213
10.3.4 Setting the Mode Bit . . . . . . . . . . . . . . . . . . . . .
215
10.4 Design of the Virtual Machine Monitor
. . . . . . . . . . . . . . .
217
10.4.1 Privileged Instructions . . . . . . . . . . . . . . . . . . . .
220
10.4.2 Sensitive Instructions . . . . . . . . . . . . . . . . . . . . .
221
10.4.3 Virtualizable Processor Architectures
. . . . . . . . . . . .
223
10.5 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
224
10.6 Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
225
11 Access Control Using Descriptors and Capabilities
227
11.1 Address Descriptors and Capabilities
. . . . . . . . . . . . . . . .
227
11.2 Tagged Architectures . . . . . . . . . . . . . . . . . . . . . . . . .
231
11.3 Capability Systems . . . . . . . . . . . . . . . . . . . . . . . . . .
233
11.3.1 Catalogs
. . . . . . . . . . . . . . . . . . . . . . . . . . .
233
11.3.2 Creating New Segments
. . . . . . . . . . . . . . . . . . .
235
11.3.3 Dynamic Sharing . . . . . . . . . . . . . . . . . . . . . . .
237
11.3.4 Revocation of Capabilities . . . . . . . . . . . . . . . . . .
239
11.4 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
241
11.5 Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
242
12 Access Control Using Lists and Rings
245
12.1 Generalized Addresses . . . . . . . . . . . . . . . . . . . . . . . .
245
12.2 Segment Access Controllers
. . . . . . . . . . . . . . . . . . . . .
247
12.3 ACL-Based Access Policy for Memory Accesses
. . . . . . . . . .
249
12.4 Ring-Based Access Control
. . . . . . . . . . . . . . . . . . . . .
253
12.4.1 Access Brackets
. . . . . . . . . . . . . . . . . . . . . . .
254
12.4.2 Call Brackets . . . . . . . . . . . . . . . . . . . . . . . . .
255
12.5 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258
12.6 Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
259

xi
IV
Access Policies
261
13 ConÔ¨Ådentiality and Integrity Policies
263
13.1 ClassiÔ¨Åcations and Categories
. . . . . . . . . . . . . . . . . . . .
263
13.2 Bell‚ÄìLa Padula Model, Revisited
. . . . . . . . . . . . . . . . . .
266
13.3 ConÔ¨Ådentiality Levels: Some Practical Considerations
. . . . . . .
269
13.4 Biba‚Äôs Strict Integrity, Revisited
. . . . . . . . . . . . . . . . . . .
272
13.5 Lipner‚Äôs Integrity Model
. . . . . . . . . . . . . . . . . . . . . . .
276
13.5.1 Commercial Integrity Requirements . . . . . . . . . . . . .
277
13.5.2 Commercial Integrity via Bell‚ÄìLa Padula . . . . . . . . . .
277
13.5.3 Commercial Integrity via Bell‚ÄìLa Padula and Strict Integrity 281
13.6 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
13.7 Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
285
14 Role-Based Access Control
289
14.1 RBAC Fundamentals . . . . . . . . . . . . . . . . . . . . . . . . .
289
14.1.1 Role Inheritance
. . . . . . . . . . . . . . . . . . . . . . .
290
14.1.2 Sessions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
295
14.2 Separation of Duty
. . . . . . . . . . . . . . . . . . . . . . . . . .
297
14.2.1 Static Separation of Duty . . . . . . . . . . . . . . . . . . .
297
14.2.2 Dynamic Separation of Duty . . . . . . . . . . . . . . . . .
299
14.3 Representing RBAC Systems in the Logic . . . . . . . . . . . . . .
304
14.3.1 RBAC Extensions to the Logic . . . . . . . . . . . . . . . .
304
14.3.2 Translating RBAC into the Logic
. . . . . . . . . . . . . .
305
14.4 Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
14.5 Further Reading
. . . . . . . . . . . . . . . . . . . . . . . . . . .
312
A Summary of the Access-Control Logic
313
A.1
Syntax
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
A.2
Core Rules, Derived Rules, and Extensions
. . . . . . . . . . . . .
315
Bibliography
321
Notation Index
324
General Index
325


List of Tables
2.1
Truth table for the propositional formula (p‚àßq) ‚äÉr . . . . . . . .
23
2.2
State-transition table for Ô¨Ånite-state machine M
. . . . . . . . . .
26
2.3
Truth values of primitive propositions p, q, r, and s in each world .
26
4.1
Example of an access-control matrix . . . . . . . . . . . . . . . .
59
5.1
Access-control matrix for foo under Susan‚Äôs control . . . . . . . .
80
5.2
ClassiÔ¨Åcation level of documents . . . . . . . . . . . . . . . . . .
90
5.3
FX-1 and FX-2 project personnel, functions, and clearances . . . .
91
5.4
FX-1 access matrix
. . . . . . . . . . . . . . . . . . . . . . . . .
92
5.5
FX-2 access matrix
. . . . . . . . . . . . . . . . . . . . . . . . .
92
5.6
Access matrix for vegetarian and non-vegetarian meals
. . . . . .
101
5.7
Integrity levels for subjects and objects . . . . . . . . . . . . . . .
101
9.1
Tri-state data-bus values . . . . . . . . . . . . . . . . . . . . . . .
181
9.2
Register with high-impedance output . . . . . . . . . . . . . . . .
182
9.3
Simple combinational-logic functions . . . . . . . . . . . . . . . .
182
9.4
An arithmetic interpretation of a full adder . . . . . . . . . . . . .
183
9.5
ALU functions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
10.1
Operations for @B ‚Üê@A+@B
. . . . . . . . . . . . . . . . . .
202
10.2
Timing and control unit operations . . . . . . . . . . . . . . . . .
202
10.3
Memory-protected operations for LDA @A, ADD @A, and STO @B 213
10.4
LDARR: Reading the contents of relocation register
. . . . . . . .
215
10.5
Timing and control unit operations for LDARR . . . . . . . . . . .
215
11.1
Interpretation of capability-register values
. . . . . . . . . . . . .
229
11.2
Interpretation of tag bit
. . . . . . . . . . . . . . . . . . . . . . .
232
13.1
ConÔ¨Ådentiality levels
. . . . . . . . . . . . . . . . . . . . . . . .
267
13.2
Discretionary access-control matrix . . . . . . . . . . . . . . . . .
267
13.3
Integrity levels . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
13.4
Discretionary access-control matrix . . . . . . . . . . . . . . . . .
274
13.5
Access-control matrix for commercial integrity via Bell‚ÄìLa Padula
279
13.6
ConÔ¨Ådentiality levels for subjects . . . . . . . . . . . . . . . . . .
279
13.7
ConÔ¨Ådentiality levels for objects
. . . . . . . . . . . . . . . . . .
280
13.8
Access-control matrix for both conÔ¨Ådentiality and strict integrity
.
281
xiii

xiv
13.9
ConÔ¨Ådentiality and integrity levels for objects
. . . . . . . . . . .
283
13.10 ConÔ¨Ådentiality and integrity levels for subjects . . . . . . . . . . .
284

List of Figures
2.1
Semantics of core logic, for each M = ‚ü®W,I,J‚ü©
. . . . . . . . . .
29
2.2
Learning outcomes for Chapter 2 . . . . . . . . . . . . . . . . . .
38
3.1
Logical rules for the access-control logic . . . . . . . . . . . . . .
40
3.2
Common propositional-logic tautologies . . . . . . . . . . . . . .
41
3.3
A simple formal proof . . . . . . . . . . . . . . . . . . . . . . . .
47
3.4
A formal proof of the Controls rule . . . . . . . . . . . . . . . . .
47
3.5
Some useful derived rules . . . . . . . . . . . . . . . . . . . . . .
49
3.6
A formal proof of Conjunction
. . . . . . . . . . . . . . . . . . .
49
3.7
Learning outcomes for Chapter 3 . . . . . . . . . . . . . . . . . .
55
4.1
Abstract view of reference monitors . . . . . . . . . . . . . . . . .
58
4.2
Formal proof of the Ticket Rule . . . . . . . . . . . . . . . . . . .
62
4.3
Template for two-factor authentication . . . . . . . . . . . . . . .
70
4.4
Template for using credentials from multiple authorities . . . . . .
72
4.5
Learning outcomes for Chapter 4 . . . . . . . . . . . . . . . . . .
76
5.1
Conceptual diagram of a virtual machine and monitor . . . . . . .
82
5.2
Inference rules for relating security levels . . . . . . . . . . . . . .
87
5.3
Proof of ‚â§s Subst
. . . . . . . . . . . . . . . . . . . . . . . . . .
87
5.4
Proof justifying Jude‚Äôs access to status report . . . . . . . . . . . .
94
5.5
Inference rules for relating integrity levels
. . . . . . . . . . . . .
96
5.6
Subjects, objects, domains, and access
. . . . . . . . . . . . . . .
98
5.7
Preparation of vegetarian and non-vegetarian meals
. . . . . . . .
100
5.8
Partial access diagram for gasoline . . . . . . . . . . . . . . . . .
102
5.9
Learning outcomes for Chapter 5 . . . . . . . . . . . . . . . . . .
106
6.1
Process for using public-key encryption for privacy
. . . . . . . .
110
6.2
Process for using private-key encryption for authenticity . . . . . .
111
6.3
Process for creating a digital signature
. . . . . . . . . . . . . . .
114
6.4
Process for verifying a digital signature . . . . . . . . . . . . . . .
114
6.5
Simple analysis of digital signatures
. . . . . . . . . . . . . . . .
115
6.6
Network of certiÔ¨Åcate authorities . . . . . . . . . . . . . . . . . .
119
6.7
Learning outcomes for Chapter 6 . . . . . . . . . . . . . . . . . .
132
7.1
Logical rules for delegation . . . . . . . . . . . . . . . . . . . . .
137
7.2
Grace‚Äôs health-care proxy . . . . . . . . . . . . . . . . . . . . . .
140
xv

xvi
7.3
Simple checking . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
7.4
Learning outcomes for Chapter 7 . . . . . . . . . . . . . . . . . .
148
8.1
Handshake protocol: Establishing associations among principals
and keys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
8.2
Record protocol: Payload construction . . . . . . . . . . . . . . .
155
8.3
Record protocol: Payload integrity checking . . . . . . . . . . . .
156
8.4
Overview of Kerberos protocol . . . . . . . . . . . . . . . . . . .
157
8.5
Interbank checking using the ACH electronic clearinghouse . . . .
168
8.6
Learning outcomes for Chapter 8 . . . . . . . . . . . . . . . . . .
173
9.1
Timing diagram for synchronous registers and memory
. . . . . .
178
9.2
Timing diagram for synchronous registers with load control . . . .
180
9.3
Tri-state buffer . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
9.4
Tri-state bus example
. . . . . . . . . . . . . . . . . . . . . . . .
181
9.5
Register with tri-state output
. . . . . . . . . . . . . . . . . . . .
182
9.6
Basic combinational-logic gates . . . . . . . . . . . . . . . . . . .
183
9.7
Full-adder implementation . . . . . . . . . . . . . . . . . . . . . .
184
9.8
An n-bit adder and ripple adder . . . . . . . . . . . . . . . . . . .
185
9.9
ALU implementation
. . . . . . . . . . . . . . . . . . . . . . . .
186
9.10
Simple data- and control-path example . . . . . . . . . . . . . . .
191
9.11
Learning outcomes for Chapter 9 . . . . . . . . . . . . . . . . . .
194
10.1
Virtual machine monitor protecting memory . . . . . . . . . . . .
198
10.2
A simple processor
. . . . . . . . . . . . . . . . . . . . . . . . .
199
10.3
Timing of CPU operations . . . . . . . . . . . . . . . . . . . . . .
203
10.4
Memory segmentation . . . . . . . . . . . . . . . . . . . . . . . .
205
10.5
Processor model for virtual machines . . . . . . . . . . . . . . . .
206
10.6
Derived inference rules for an operation OP @A . . . . . . . . . .
211
10.7
Simple processor with relocation register . . . . . . . . . . . . . .
214
10.8
Timing diagram for loading RR into ACC
. . . . . . . . . . . . .
216
10.9
Top-level operation of virtual machine monitor . . . . . . . . . . .
219
10.10 Graphic representation of control program for handling trapped in-
structions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
10.11 Learning outcomes for Chapter 10
. . . . . . . . . . . . . . . . .
226
11.1
Memory protection with address descriptors and capabilities . . . .
228
11.2
Catalogs and capabilities
. . . . . . . . . . . . . . . . . . . . . .
234
11.3
Inference rules for MKSEG instruction
. . . . . . . . . . . . . . .
236
11.4
Pairwise shared communications segments . . . . . . . . . . . . .
238
11.5
Formal analysis for pairwise shared communication segment
. . .
238
11.6
Mailbox segments . . . . . . . . . . . . . . . . . . . . . . . . . .
239
11.7
Formal analysis for mailbox communication segment
. . . . . . .
239
11.8
Learning outcomes for Chapter 11
. . . . . . . . . . . . . . . . .
243

xvii
12.1
User view of memory . . . . . . . . . . . . . . . . . . . . . . . .
246
12.2
Address formation for address descriptor . . . . . . . . . . . . . .
247
12.3
Segment access controller . . . . . . . . . . . . . . . . . . . . . .
248
12.4
Data or instruction segment . . . . . . . . . . . . . . . . . . . . .
248
12.5
Generalized address formation and access control
. . . . . . . . .
249
12.6
Virtual machine and virtual machine monitor for ACL system . . .
250
12.7
Protection rings
. . . . . . . . . . . . . . . . . . . . . . . . . . .
253
12.8
Call gates
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
256
12.9
Learning outcomes for Chapter 12
. . . . . . . . . . . . . . . . .
259
13.1
Partial ordering on conÔ¨Ådentiality levels
. . . . . . . . . . . . . .
265
13.2
Carol‚Äôs request and Kate‚Äôs response . . . . . . . . . . . . . . . . .
275
13.3
Learning outcomes for Chapter 13
. . . . . . . . . . . . . . . . .
287
14.1
Sample Hasse diagram
. . . . . . . . . . . . . . . . . . . . . . .
291
14.2
Role hierarchy for a CS/CE department . . . . . . . . . . . . . . .
299
14.3
Logical rules regarding principal equality . . . . . . . . . . . . . .
305
14.4
Formal justiÔ¨Åcation to allow Dora to read student grades . . . . . .
308
14.5
Learning outcomes for Chapter 14
. . . . . . . . . . . . . . . . .
311
A.1
Summary of core rules for the access-control logic . . . . . . . . .
316
A.2
Summary of useful derived rules
. . . . . . . . . . . . . . . . . .
317
A.3
Summary of rules for delegation
. . . . . . . . . . . . . . . . . .
318
A.4
Inference rules for relating security levels . . . . . . . . . . . . . .
318
A.5
Inference rules for relating integrity levels
. . . . . . . . . . . . .
318
A.6
Logical rules regarding principal equality . . . . . . . . . . . . . .
319


Preface
Our intent in developing this textbook is to serve the needs of computer engineers
and computer scientists who are responsible for designing, implementing, and ver-
ifying secure computer and information systems. Engineers who serve in the roles
of designers and veriÔ¨Åers must be able to translate concepts and ideas into calcu-
lations and derivations as part of the design and veriÔ¨Åcation process. For exam-
ple, electrical-engineering students routinely learn how to specify, implement, and
verify control and communications systems using Laplace and Fourier transforms.
They can translate their concepts into networks of system transfer functions and use
Laplace and Fourier transforms to analyze, predict, and verify the behavior of their
designs. Computer-hardware designers rely on switching theory, discrete mathemat-
ics, and Ô¨Ånite-state machines to specify, implement, and verify their designs. In an
analogous fashion, our goal is to equip engineers with an access-control logic they
can use to specify, implement, and verify their security designs.
Our focus is on access control and reference monitors. Controlling access to pro-
tected objects is central to any security requirement. Reference monitors are the
means to protect objects of value in systems. Just as propositional logic and Ô¨Ånite-
state machines are used to deÔ¨Åne and explain computer hardware design and veriÔ¨Å-
cation principles, we use a propositional modal logic to explain access-control prin-
ciples. Our view is this: if you are the hardware designer and you are given the input
values to your design, then you should be able to justify mathematically whether or
not the value on any particular output is a 0 or a 1. Similarly, if you are the engineer
who has security requirements to meet and you are given a policy and a request, you
should be able to justify mathematically if your answer is a yes or a no.
To be explicit, the logic we present is meant to inform an implementation, not be
the implementation. The logic is not a programming language, and we are not sug-
gesting that access controllers should be implemented as theorem provers. Rather,
we believe the logic is a useful tool for analyzing security designs and for making ex-
plicit the conditions upon which access-control decisions depend. Access controllers
can be implemented as checklists that check and verify the relevant artifacts (e.g.,
certiÔ¨Åcates and credentials) against existing policies and trust assumptions. These
checklists correspond to provably sound inference rules in the logic.
Another intent of our book is to Ô¨Åll a gap left by many books on computer and
network security. Several are compendiums of computer security models and meth-
ods. They are highly mathematical and encyclopedic‚Äîand usually beyond the skills
and interests of design engineers. Most do not have illustrative examples or exer-
cises. While excellent references, the breadth and depth of mathematics are beyond
what is needed by most engineers. In contrast, many other books are introductory
xix

xx
in nature and are primarily descriptive. To make security concepts accessible to a
wide audience, these books deliberately omit mathematical treatments of security.
While informative, this approach does not equip engineers to do the derivations and
calculations with the same degree of mathematical precision and accuracy expected
of hardware and electrical engineers. The result is that security is often taught at the
lowest levels of knowledge‚Äîrecall and comprehension‚Äîas opposed to the higher
knowledge levels expected of engineers: application, analysis, synthesis, and evalu-
ation.
To Ô¨Åll this gap, we use a single access-control logic based on a simple proposi-
tional modal logic throughout this textbook. We introduce the basic logic early in
the book and use it throughout to deÔ¨Åne and derive access-control principles and
properties. Our focus is on reference monitors because they are the parts of a system
that systems engineers must worry about specifying, designing, implementing, and
verifying. Reference monitors in security play an analogous role to the role played
by Ô¨Ånite-state machines in computer hardware.
We developed much of the content of this book in our own courses at Syracuse
University and in intense summer courses on access control for hundreds of Air
Force Reserve OfÔ¨Åcer Training Corp (ROTC) cadets from over forty US universi-
ties. Our experience is that ‚Äúpractice makes perfect‚Äù and that our students beneÔ¨Åted
from having illustrations and exercises. Thus, we have included numerous examples
to illustrate principles, as well as many exercises to serve as assessments of knowl-
edge. We have annotated each exercise to indicate the level of knowledge it assesses,
according to the following legend:
‚Ä¢ The symbol  denotes exercises at the application level of knowledge. These
exercises typically ask the reader to apply particular knowledge or use partic-
ular techniques to solve a problem in a new context. Straightforward calcula-
tions fall into this category of exercises.
‚Ä¢ The symbol
denotes exercises at the analysis level of knowledge. These
exercises generally require the reader to decompose a problem into its con-
stituent parts in order to perform the necessary calculations or experiments
necessary to solve the problem.
‚Ä¢ The symbol
denotes exercises at the synthesis level of knowledge. These
exercises typically require the reader to integrate various techniques or com-
ponents to design, construct, or formulate an entirely new structure, pattern, or
proof.
‚Ä¢ The symbol
denotes exercises at the evaluation level of knowledge. These
exercises require the reader to identify and use relevant criteria to assess and
judge the suitability of a solution to a given problem.
Exercises at higher levels of knowledge are not necessarily harder than exercises
at lower levels of knowledge. We recommend that readers try at least one exercise at
each level of knowledge to get the most beneÔ¨Åt.

xxi
We have taught both undergraduate and graduate students using this book. We rec-
ommend that all students learn what is in Part I (Preliminaries). This part covers the
syntax and semantics of the access-control logic, basic access control concepts, and
an introduction to conÔ¨Ådentiality and integrity policies. Our experience with ROTC
cadets is that undergraduates who have successfully mastered a sophomore discrete-
mathematics course do master the syntax and semantics of the access-control logic.
Undergraduates can skip the section on soundness in Chapter 3.
Part II (Distributed Access Control) and Part III (Isolation and Sharing) focus on
access control in networks and access control in hardware, respectively. These parts
are relatively independent of one another. For courses with an emphasis on networks,
Part III may be omitted. For courses with an emphasis on hardware and virtual
machines, Part II may be omitted. Most everything in these parts is well within
the capabilities of undergraduate students who have mastered the preliminaries in
Part I. Part III includes a primer that provides a brief review of sophomore-level
courses on assembly language and computer architecture; our experience is that this
primer is sufÔ¨Åcient for those students with limited hardware backgrounds (e.g., many
computer-science majors).
Part IV (Access Policies) is a treatment of conÔ¨Ådentiality, integrity, and role-based
access control in the access-control logic. Although it does make use of delegation
(introduced in Part II), it is largely independent of Part III. Readers can pick and
choose the policies of interest to them without reading this part‚Äôs chapters in order.
Acknowledgments
We are grateful for the support of numerous colleagues and
students. Our Ô¨Årst attempts at the graduate level took place in CIS/CSE 774: Prin-
ciples of Distributed Access Control. Over the course of seven summers, we further
reÔ¨Åned our text at the undergraduate level with the cadets of the Advanced Course in
Engineering (ACE) Cybersecurity Bootcamp at the Air Force Research Laboratory
in Rome, New York, and on our own Syracuse University students in CIS/CSE 400.
The outstanding tenacity of these students convinced us of the feasibility of teach-
ing undergraduates a logical and rigorous approach to access control. Dr. Kamal
Jabbour, Air Force Research Laboratory Senior Scientist for Information Assurance,
was unwavering in his support of us and our methods in ACE.
Our book was typeset using LATEX2Œµ. The clip art in our book comes from the
Open Clip Art Library.
Finally, this book would not exist were it not for the encouragement and patience
of our editor, Robert Stern, Executive Editor, Taylor & Francis Group.


Chapter 1
Access Control, Security, Trust, and
Logic
This book is about access control, security, and trust. We wrote this book for people
who specify, design, build, or certify computer and information systems that must
be trustworthy and secure. Most every information system or computer has some
security requirement. Common examples include computers handling sensitive in-
formation such as Ô¨Ånancial information, health records, or military secrets. If you are
responsible for designing, building, testing, or certifying systems that have security
concerns, then you are concerned with the following:
‚Ä¢ who or what can access protected resources,
‚Ä¢ how to protect the conÔ¨Ådentiality, integrity, and availability of those resources,
‚Ä¢ who or what is trusted or believed, and
‚Ä¢ compelling reasons to conclude a system is worthy of trust.
For example, if you are responsible for the speciÔ¨Åcation, design, or operation of
computers holding bank accounts, you are very concerned about the following ques-
tions:
‚Ä¢ Who can withdraw funds from a customer‚Äôs bank account electronically?
‚Ä¢ Who is allowed to alter the balance or available funds in a customer‚Äôs account?
‚Ä¢ Who has authority to grant account access?
‚Ä¢ What evidence is there to substantiate that the computerized banking system
is secure and operating correctly?
When we talk about access control, security, and trust in this book, we mean the
following:
‚Ä¢ Access control is concerned with the policies and mechanisms that permit or
deny the use of a resource or capability.
‚Ä¢ Security is concerned with the policy and mechanisms that protect the conÔ¨Å-
dentiality, integrity, and availability of information, resources, or capabilities.
‚Ä¢ Trust focuses on who or what is believed and under what circumstances.
1

2
Access Control, Security, and Trust: A Logical Approach
If a system has appropriate policies, mechanisms, and trust assumptions for access
control and security, and if these policies and mechanisms are logically consistent
and correctly implemented, then we are more likely to willingly believe or depend
on the system. That is, we are more likely to deem the system as trustworthy and less
likely to fail. Systems fail for at least four reasons. They wear out; they are Ô¨Çawed;
they are used in unintended ways; the operating or design assumptions are wrong.
While this book does not deal with wear as a cause of failure, this book does address
the remaining three causes.
Any engineer or computer scientist who has designed, certiÔ¨Åed, or worked with
systems of any size or consequence knows that a key question is how will we know?
They know that undetected design Ô¨Çaws or inappropriate assumptions that are built
into deployed systems are potentially disastrous and life threatening. They know
that Ô¨Çaws in deployed systems are orders of magnitude more costly to remedy when
compared to corrections made in the design phase. They know that undetected Ô¨Çaws
potentially destroy systems, destroy reputations, and destroy credibility, leading to
failed missions, failed services, and failed corporations. In short, as systems become
larger and more complex, it is increasingly difÔ¨Åcult for designers and certiÔ¨Åers to get
a good night‚Äôs sleep.
Our purpose in writing this book is to help designers and certiÔ¨Åers sleep well at
night. Our experience shows us that the system designers and certiÔ¨Åers who sleep
best at night are those who combine their experience and intuition with mathematics
and logic. Experience and intuition are powerful tools that inform the selection of
a design approach. Mathematics and logic are unparalleled in providing assurances
of correct coverage of all cases and instances, some of which might not have been
imagined by designers or certiÔ¨Åers.
We follow the same approach used by civil, mechanical, and electrical engineers.
Mathematics and logic properly used clarify the underlying principles and properties
of systems. Systems with mathematical and logical descriptions are amenable to
independent and automated veriÔ¨Åcation and testing. The effects of system changes
or consequences of altering assumptions are easier to deduce with logic than without.
Our experience shows us that Ô¨Çaws and misconceptions often exist between levels
of abstraction in systems. For example, how will we know that a security policy re-
lated to information integrity is correctly implemented by hardware and software to-
gether? Requirement writers, software engineers, and hardware engineers might in-
terpret the meaning of integrity differently leading to improper assumptions, Ô¨Çawed
policies, Ô¨Çawed designs, and failed systems. Our approach to dealing with this ob-
servation is to use a logic that spans many levels of abstraction including hardware,
software, and policy.
The logic used here describes policies and mechanisms at the hardware level, the
network level, and the abstract level of certiÔ¨Åcates, jurisdiction of authority, delega-
tions, conÔ¨Ådentiality models, and integrity models. What you will Ô¨Ånd is that the
logic and calculations (proofs) in the logic are straightforward. We use logic as a
means to an end. It is a language for describing policies and mechanisms with the
added beneÔ¨Åt of having a semantics that allows us to do calculations. If require-
ments writers, software and hardware engineers, and system certiÔ¨Åers use the same

Access Control, Security, Trust, and Logic
3
language with a mathematical semantics, differences in interpretation and the effects
of assumptions are sorted out rigorously. Thus, only two chapters are devoted to the
logic itself. The remaining chapters are all about access control, security, and trust.
A key ability we intend for you to acquire is the ability to translate informal de-
scriptions of system requirements and behavior into formal logic. Often, this trans-
lation is done by breaking down an informal system description into its conceptual
parts by answering questions regarding what is being protected, what is being re-
quested, who or what are the guards, and on what basis do the guards decide to grant
or deny access requests.
We often rely on everyday situations as illustrations. The following example and
the exercises that follow illustrate the types of common access-control decisions we
encounter daily.
Example 1.1
Consider a movie theater. What is being protected, what is requested by patrons,
who are the guards, and how do the guards make their decisions?
In a movie theater, what is being protected is the ability to see (and hear) the
movies being shown. Patrons request access to see and hear the movie of their choice.
There are typically two guards. The Ô¨Årst guard is the box-ofÔ¨Åce attendant, who
handles requests for tickets: he grants tickets to patrons when there are available
seats and when the patrons have paid. The second guard governs access into the
theater itself: she checks to see if the movie patron has a valid ticket. The ticket
symbolizes the capability of the ticket bearer to see the movie named on the ticket.‚ô¶
Exercise 1.0.1
Consider a public library. What exactly is being requested, what

is being protected, and who are the guards? How do the guards make their access
decisions?
Exercise 1.0.2
Consider a bank that offers online access to customer accounts.

What exactly is being requested, what is being protected, and who or what are the
guards? How do the guards make their access decisions?
Exercise 1.0.3
Recall the last time you took a commercial Ô¨Çight from an airport.

Identify all of the guards you encountered, from the moment you stepped into the
airport until you boarded your Ô¨Çight. From the perspective of each of these guards,
what exactly were you requesting? How did the guards make their access decisions?
1.1
Deconstructing Access-Control Decisions
A common adage among security pundits is that security must be built in from
the beginning and not added on as an afterthought. However, engineers and system

4
Access Control, Security, and Trust: A Logical Approach
designers must wrestle with design details at the chip, board, software, middleware,
and network levels while Ô¨Åghting changing requirements, speciÔ¨Åcations, deadlines,
and resources. The challenge for engineers and designers is to quickly and accurately
comprehend what is needed security-wise and to be able to routinely account for
security speciÔ¨Åcations along with all the other functional and resource speciÔ¨Åcations
and constraints.
Our goal in writing this book is to provide system creators and veriÔ¨Åers with a
means to reason about the system‚Äôs security requirements in a rigorous manner.
SpeciÔ¨Åcally, we hope to provide them with analogous mathematical tools to those
that hardware engineers enjoy, as illustrated in the following example.
Example 1.2
Amanda is one of the lead hardware engineers on a project. At a critical design
review, a reviewer Frank says ‚ÄúShow me how you‚Äôve met the requirement to track
all 221 cases.‚Äù Amanda replies that there is an 8-bit register used to index each
case. Frank then asks Amanda, ‚ÄúWhat exactly happens when the input x2 is a 1 and
input y2 is a 0?‚Äù Amanda answers that x2 and y2 are inputs to a two-input or-gate,
and she writes ‚Äú1 ‚à®0 = 1‚Äù to demonstrate the calculation. Later during the review,
Frank challenges Amanda with the statement ‚ÄúI don‚Äôt think your design works in this
case!‚Äù Amanda calmly responds by putting up two formulas, one representing the
speciÔ¨Åcation and another representing her hardware implementation. Both look very
different. She uses the laws of switching theory to show that both formulas have the
same meaning.
‚ô¶
Breaking down the engineer‚Äôs answers to the reviewer‚Äôs questions and challenges,
the following points emerge:
‚Ä¢ Amanda implies that she has met the requirement to track all 221 cases of
interest by appealing to the mathematical property that an 8-bit register can
account for as many as 28 = 256 cases. This approach also allows room for
some requirements creep (i.e., the tendency for requirements to grow in size).
‚Ä¢ Amanda is able to account precisely for the consequences of her design when
asked what happens for a particular assignment of values to x2 and y2. A two-
input or-gate has a precise meaning (in this case, the value of x2 ‚à®y2).
‚Ä¢ When challenged to show that her implementation meets the speciÔ¨Åcation,
Amanda gives a formal representation of both and uses the laws of switching
theory (i.e., propositional logic) to prove that both are the same. Her proof
compels the reviewer to conclude that her design is correct.
One of our goals for this textbook is to give Amanda and other engineers the same
kind of logical support that will enable them to answer questions about access con-
trol. The central concept we rely upon is that of the reference monitor. Reference

Access Control, Security, Trust, and Logic
5
monitors are the guards that protect resources such as Ô¨Åle systems, memory, commu-
nications channels, and so on. Reference monitors play an analogous role in access
control to the role played by Ô¨Ånite-state machine controllers in hardware: they de-
termine whether to grant access requests based on access-control policies. In this
book, we will look at reference monitors and access-control decisions in a multitude
of contexts, from the security guard at an airport who checks picture identiÔ¨Åcation
cards and airline tickets, to virtual-machine monitors and electronic requests made
by users acting in particular roles.
As we will see many times over, access-control decisions are made within the
context of (1) information or evidence in the form of statements, credentials, li-
censes, tickets, or certiÔ¨Åcates, (2) trust assumptions regarding proxy relationships
or the jurisdiction of authorities, and (3) some interpretation of credentials and other
statements. The following example and subsequent exercises highlight these aspects
across a variety of common scenarios.
Example 1.3
Imagine you are a rental agent for a rental car company in the United States. You
are directed by the company‚Äôs policy to ask for a valid US driver‚Äôs license from the
renter. If the renter presents a current driver‚Äôs license with their picture on it issued
by a department of motor vehicles (DMV) ofÔ¨Åce in one of the Ô¨Åfty states, the renter
is allowed to rent a car (assuming he or she can pay for it). What is the thinking
behind this policy?
1. A license that is free from physical or electronic tampering and has been issued
by a state DMV is interpreted as evidence that the person named and pictured
on the license is certiÔ¨Åed by that state as having the required skills to drive a
car legally.
2. The recognition of the state‚Äôs authority to certify drivers is indicated by the
rental car company‚Äôs policy to accept a valid US driver‚Äôs license as evidence
of being a legal driver.
3. There is a trust assumption that licenses free from physical or electronic tam-
pering truly represent the states‚Äô DMV Ô¨Åndings. In other words, the assump-
tion is that licenses truly speak for the states‚Äô DMV.
If any one of the preceding things change, then the rental procedure will likely need
to be changed too. For example, suppose it is discovered that, due to some security
breach, tens of thousands of counterfeit licenses bearing the seal of the State of New
York were issued. What changes would you make and why?
A plausible change is to require a customer presenting a New York State driver‚Äôs
license to undergo additional scrutiny, perhaps by consulting a New York State
database of legal drivers and asking for additional picture ID. In this case, we would
want to make sure that the New York database is trustworthy (i.e., accessible only
from a secure site), and we would ask for an additional picture ID such as a US
passport.
‚ô¶

6
Access Control, Security, and Trust: A Logical Approach
Exercise 1.1.1
Suppose that you are an online merchant who accepts credit cards
over the Internet. What assumptions about evidence, authority, and trust are you
making when you ask a customer over the Internet for their name, card number,
expiration date, and the three- or four-digit security code on the back of the card?
Exercise 1.1.2
Suppose that you are the airport security guard that checks picture
IDs and boarding passes before letting passengers into the security line that checks
carry-on baggage and passengers. What assumptions about evidence, authority, and
trust are you using when you ask a passenger for their ticket and picture ID?
Exercise 1.1.3
Suppose that you own a bar next to a college campus and that the
legal drinking age is twenty-one. You can lose your liquor license if you serve alcohol
to people under twenty-one. What do you ask your bartender to do before serving
drinks? If you have someone guarding the door, what do you ask them to do? What
assumptions are you making about evidence, authority, and trust?
1.2
A Logical Approach to Access Control
The exercises at the end of the preceding section share an underlying structure, but
their descriptions quickly become verbose and unwieldy: how would you convince a
boss or critic in a project review that the system is making the correct access decision
in a given situation? A calculus based only on natural language is at best cumber-
some and at worst impossible to use. In this book, we present a formal logic that
allows us to represent and reason about these scenarios concisely and symbolically.
The use of formal logic allows us to more easily detect patterns of correct or
incorrect reasoning. The capability to recognize familiar patterns of reasoning, albeit
in many different guises, is a good thing. The realization that situations that on the
surface seem different are really the same helps tremendously by giving us insight
and clarity, reducing the apparent complexity of problems. The access-control logic
that we introduce in the next chapter will allow us to see succinctly the form or
structure behind a policy, a set of trust assumptions, and the access decisions made
by a reference monitor.
We shall also see that, in most cases, the actual proofs are quite easy, which is
a good thing: few people like complicated proofs. The value of having an access-
control logic is that it provides a means for describing situations precisely using
formulas that have precise meanings. Second, recall the rationale for preliminary de-
sign reviews: it is important to account for assumptions when determining the likely
adequacy of a proposed approach. Using an access-control logic with inference rules
requires one to account for all necessary assumptions explicitly in order to reach the
desired conclusions. Finally, having the basis for access-control decisions and ref-
erence monitors described formally in an access-control logic gives us the capacity
to quickly determine how we are potentially affected or made vulnerable if certain

Access Control, Security, Trust, and Logic
7
assumptions change‚Äîthis information can direct us into determining how to adapt
to changing requirements or situations.
Throughout this textbook, we use this access-control logic in much the same way
as hardware engineers use switching theory and propositional logic to specify, de-
sign, and verify hardware. Just as the laws of switching theory are used to calcu-
late whether a signal is 0 or 1, we use the access-control logic to determine if an
access-control request should be honored or not. To be explicit, the logic is a use-
ful tool for analyzing security designs and for making explicit the conditions upon
which access-control decisions depend. Although the logic can be used to inform an
implementation, it is not a programming language, and we are not suggesting that
access controllers be implemented as theorem provers. Instead, access controllers
can be implemented as checklists that check and verify the relevant artifacts (e.g.,
certiÔ¨Åcates and credentials) against existing policies and trust assumptions; these
checklists correspond to provably sound inference rules in the logic.
Two chapters (Chapter 2 and Chapter 3) are devoted to formally deÔ¨Åning the syn-
tax and semantics of the logic. The logic is straightforward and typical of the kinds
of modal and temporal logics used by industry for hardware veriÔ¨Åcation and model
checking. The remainder of the book focuses on access control itself, from basic
concepts (e.g., tickets, lists, authentication) to applications (e.g., virtual machines
and network protocols) to models (e.g., role-based access control).


Part I
Preliminaries


Chapter 2
A Language for Access Control
In this chapter we introduce the core logic that relates principals to the statements
they make. The term principal refers to any person, process, or agent that attempts
to exercise an action on an object (i.e., assert its rights). The statements we care
about include access requests (e.g., Alice states her desire to read a certain Ô¨Åle),
as well as statements of rights (e.g., the Department of Motor Vehicles states that
Bob is authorized to drive a truck), authority, and jurisdiction (e.g., the Department
of Motor Vehicles has the jurisdiction to authorize Bob to drive a truck), and even
the association of keys with principals (e.g., a speciÔ¨Åc 128-bit encryption key is
associated with Carol).
In subsequent chapters, we will express and reason about a variety of access-
control concepts using this logic. The purpose of this chapter and the next is to
provide a primer to the logic itself. There are three important components of any
logic: the syntax (‚Äúwhat do the formulas look like?‚Äù), the semantics (‚Äúwhat do the
formulas mean?‚Äù), and the logical rules (‚Äúhow can I manipulate formulas to carry
out my reasoning?‚Äù). We introduce the Ô¨Årst two of these components in this chapter,
leaving the inference rules for Chapter 3. However, we begin by reviewing important
concepts and notation from discrete mathematics that we will use throughout this
book, including sets, relations, and certain operations on them.
2.1
Sets and Relations
We assume a working knowledge of discrete mathematics and propositional logic,
and thus we do not provide a detailed introduction to them. Instead, we brieÔ¨Çy
introduce the notation that we will be using, and we review some simple and common
approaches for proving properties about sets and relations.
The exercises in this section can be used as a guide to determine whether you have
the requisite knowledge of discrete mathematics and propositional logic. Also, the
references at the end of this chapter contain several suggested texts to consult if you
need a detailed introduction.
11

12
Access Control, Security, and Trust: A Logical Approach
2.1.1
Notation
We use standard set notation. Set elements are contained within braces, as in
{e0,e1,...,en}. The empty set (i.e., the set that has no elements) is denoted as either
{} or /0. Set union is denoted by ‚à™, set intersection is denoted by ‚à©, and set difference
is denoted by ‚àí, as illustrated by the following simple examples:
{1,2}‚à™{1,3,4} = {1,2,3,4}
{1,2,3}‚à©{2,3,4,5} = {2,3}
{2,3,4,5}‚àí{1,4,5} = {2,3}.
The order in which elements are listed in a set is unimportant. Thus, for example,
{1,2} and {2,1} denote the same set.
We write x ‚ààS to indicate that x is an element of the set S; likewise, x Ã∏‚ààS indicates
that x is not an element of the set S.
A set S is a subset of T, written S ‚äÜT, if every element of S is also an element of
T. The power set of a set S, written P(S), is the set of all subsets of S. For example,
the power set of {red,blue} is
P({red,blue}) = {{},{red},{blue},{red,blue}}.
Note that the empty set is a subset of every set (i.e., /0 ‚äÜS for all S), and every set S
is a subset of itself (i.e., S ‚äÜS). Thus, /0 and S are always elements of P(S).
The Cartesian product of sets A and B (written A √ó B) is the set of ordered pairs
whose Ô¨Årst component is drawn from A and whose second component is drawn from
B:
A√óB = {(a,b) | a ‚ààA and b ‚ààB}.
A binary relation is simply a set R ‚äÜA √ó B of pairs whose Ô¨Årst components are
drawn from A and whose second components are drawn from B. We say that R is a
binary relation over (or on) A when R ‚äÜA√óA.
The identity relation on a set A is the relation idA ‚äÜA√óA deÔ¨Åned by:
idA = {(a,a) | a ‚ààA}.
The composition of relations R1 ‚äÜA √ó B and R2 ‚äÜB √óC is the relation R1 ‚ó¶R2 ‚äÜ
A√óC deÔ¨Åned as follows:
R1 ‚ó¶R2 = {(x,z) | there exists y such that ((x,y) ‚ààR1 and (y,z) ‚ààR2)}.
Finally, given a relation R ‚äÜA √ó B and an element a ‚ààA, we deÔ¨Åne R(a) to be the
image of R under a:
R(a) = {b ‚ààB | (a,b) ‚ààR}.
That is, R(a) is the set of elements in B related to a by the relation R. For example,
if S is the relation
S = {(1,2),(2,3),(2,4),(3,5),(3,1),(4,1),(5,2)},
then S(2) = {3,4} and S(3) = {5,1}.

A Language for Access Control
13
Exercise 2.1.1
Let T and U be relations over the set A = {1,2,3,4}, as follows:

T = {(1,1),(2,1),(3,3),(4,4),(3,4)}
U = {(2,4),(1,3),(3,3),(3,2)}.
Calculate the following sets:
a. P({x,y,z})
b. U(3)
c. U(4)
d. T ‚à™U
e. T ‚à©U
f. T ‚àíU
g. U ‚ó¶T
h. T ‚ó¶U
2.1.2
Approaches for Mathematical Proofs
Throughout this book, our examples and exercises include mathematical proofs
involving sets, relations, and relationships among them. We highlight some common
approaches for structuring these proofs.
Recall that a set S is a subset of T if every element of S is also an element of T.
Thus, to prove that some set A is a subset of B, it sufÔ¨Åces to demonstrate that any
arbitrary element of A is necessarily also an element of B. The following simple
example demonstrates this proof approach.
Example 2.1
Property: The subset relation is transitive: that is, if A ‚äÜB and B ‚äÜC, then A ‚äÜC.
Proof: Consider sets A,B,C such that A ‚äÜB and B ‚äÜC, and consider an arbitrary
element x ‚ààA; we need to show that x ‚ààC as well.
Because A ‚äÜB, we know that x ‚ààB; because B ‚äÜC, it follows that x ‚ààC as
necessary.
‚ô¶
Two sets A and B are equal (i.e., contain exactly the same elements) provided that
each is a subset of the other. This fact forms the foundation for another standard
proof approach: to prove that A = B, it sufÔ¨Åces to demonstrate that A ‚äÜB and that
B ‚äÜA. The following example, which includes a useful property for subsequent
sections, demonstrates this proof approach.

14
Access Control, Security, and Trust: A Logical Approach
Example 2.2
Property: Suppose that R and T are arbitrary relations over a set A and that Y is a
set; then the following equality holds:
{x | (R‚à™T)(x) ‚äÜY} = {x | R(x) ‚äÜY}‚à©{x | T(x) ‚äÜY}.
Proof: Consider arbitrary relations R,T and an arbitrary set Y. Our analysis involves
two steps:
1. Consider an arbitrary element a ‚àà{x | (R‚à™T)(x) ‚äÜY}: thus (R‚à™T)(a) ‚äÜ
Y. By deÔ¨Ånition,
(R‚à™T)(a) = {b | (a,b) ‚ààR‚à™T}
= {b | (a,b) ‚ààR}‚à™{b | (a,b) ‚ààT}
= R(a)‚à™T(a).
Therefore, R(a) ‚à™T(a) ‚äÜY, and hence we also have that R(a) ‚äÜY and
T(a) ‚äÜY. It follows that a ‚àà{x | R(x) ‚äÜY} and a ‚àà{x | T(x) ‚äÜY}, and
therefore a ‚àà{x | R(x) ‚äÜY}‚à©{x | T(x) ‚äÜY}. Because a was an arbitrary
element of {x | (R‚à™T)(x) ‚äÜY}, we have shown that
{x | (R‚à™T)(x) ‚äÜY} ‚äÜ{x | R(x) ‚äÜY}‚à©{x | T(x) ‚äÜY}.
2. Consider an arbitrary element a ‚àà{x | R(x) ‚äÜY} ‚à©{x | T(x) ‚äÜY}. It
follows that R(a) ‚äÜY and T(a) ‚äÜY, and hence R(a) ‚à™T(a) ‚äÜY. Be-
cause R(a) ‚à™T(a) = (R ‚à™T)(a), we also have that (R ‚à™T)(a) ‚äÜY, and
thus a ‚àà{x | (R ‚à™T)(x) ‚äÜY}. Because a was an arbitrary element of
{x | R(x) ‚äÜY}‚à©{x | T(x) ‚äÜY}, we have shown that
{x | R(x) ‚äÜY}‚à©{x | T(x) ‚äÜY} ‚äÜ{x | (R‚à™T)(x) ‚äÜY}.
Having demonstrated that each set is a subset of the other, we have shown that
{x | (R‚à™T)(x) ‚äÜY} = {x | R(x) ‚äÜY}‚à©{x | T(x) ‚äÜY}.
‚ô¶
One often encounters statements that indicate a given property (say, Property 1)
is true if and only if another property (say, Property 2) is true. Proving that an ‚Äúif
and only if‚Äù statement is true involves two steps: (1) demonstrating that, whenever
Property 1 holds, Property 2 must also hold; and (2) demonstrating that, whenever
Property 2 holds, Property 1 must also hold. The following example demonstrates
this proof approach to prove a property that will be useful for calculations in subse-
quent sections.
Example 2.3
Property: Let A, X, and Y be sets such that X and Y are both subsets of A. Then
A ‚äÜ(A‚àíX)‚à™Y if and only if X ‚äÜY.

A Language for Access Control
15
Proof: Our analysis involves two steps:
1. The ‚Äúforward‚Äù direction: suppose A ‚äÜ(A ‚àíX) ‚à™Y, and consider any
x ‚ààX. Since X ‚äÜA, x ‚ààA, and thus x Ã∏‚ààA‚àíX. Therefore, x must be an
element of Y. Since x was arbitrary, X ‚äÜY.
2. The ‚Äúreverse‚Äù direction: suppose X ‚äÜY, and consider any a ‚ààA. If
a ‚ààX, then by the deÔ¨Ånition of subset, a ‚ààY, and hence a ‚àà(A‚àíX)‚à™Y
as necessary; if, instead, a Ã∏‚ààX, then a ‚àà(A‚àíX) and therefore a ‚àà(A‚àí
X)‚à™Y. Since a was arbitrary, A ‚äÜ(A‚àíX)‚à™Y.
Having shown that each property implies the other, we have demonstrated that
A ‚äÜ(A‚àíX)‚à™Y if and only if X ‚äÜY.
‚ô¶
The exercises that follow include properties that will be useful for exercises in
subsequent sections.
Exercise 2.1.2
Prove that, for all relations R,S,T, the following property holds:
If R ‚äÜS, then R‚ó¶T ‚äÜS‚ó¶T.
Exercise 2.1.3
Prove that, for all relations R,S,T, the following property holds:
(R‚à™S)‚ó¶T = (R‚ó¶T)‚à™(S‚ó¶T).
Exercise 2.1.4
Prove that, for all sets A,B,C, the following property holds:
((A‚àíB)‚à™C)‚à©((A‚àíC)‚à™B) = (B‚à©C)‚à™((A‚àíB)‚à©(A‚àíC)).
Exercise 2.1.5
Prove that, for all relations R,S, sets X, and items u, the following
property holds:
(R‚ó¶S)(u) ‚äÜX if and only if R(u) ‚äÜ{y | S(y) ‚äÜX}.
2.2
Syntax
Our goal is to deÔ¨Åne a logic for expressing access-control policies and analyzing
their ramiÔ¨Åcations. However, to do so, we must Ô¨Årst identify the most primitive
concepts that we wish to express.
The Ô¨Årst step is to introduce the syntax of the logic: that is, we must specify
what the formulas of the logic look like. To do so, we make use of BNF (Backus-
Naur Form) speciÔ¨Åcations, which provide a way to state syntactic rules precisely

16
Access Control, Security, and Trust: A Logical Approach
and unambiguously. We explain how BNF speciÔ¨Åcations work through the following
example, which describes the syntax for a simple language of arithmetic expressions:
AExp ::= BinNumber
AExp ::= ( AExp + AExp )
AExp ::= ( AExp ‚àóAExp )
BinNumber ::= Bit
BinNumber ::= Bit BinNumber
Bit ::= 0
Bit ::= 1
The items in boldface (e.g., AExp and BinNumber) are called non-terminal symbols
and represent syntactic categories (i.e., collections of syntactic expressions). The
symbols ‚Äú(‚Äù, ‚Äú)‚Äù, ‚Äú+‚Äù, ‚Äú‚àó‚Äù,‚Äú0‚Äù, and ‚Äú1‚Äù are called terminal symbols (or terminals)
and correspond to actual symbols in the syntax. The notation ‚Äú ::= ‚Äù is meta-notation
for specifying production rules for the syntax. In general, each line of form
AExp ::= right-hand-side
provides a rule for creating new elements of AExp (and likewise for BinNumber and
Bit). A syntactic derivation is a sequence of rewriting steps: each step uses one of the
production rules to replace one occurrence of a nonterminal symbol. The syntactic
derivation is complete when there are no more nonterminal symbols to replace.
The following two examples contain syntactic derivations that respectively demon-
strate that 101 belongs to the syntactic category BinNumber and that (11‚àó(1+10))
belongs to the syntactic category AExp. Note that a given piece of syntax (such
as 101) may have multiple derivations possible, corresponding to choosing different
nonterminals to expand at a given step. The order in which nonterminal symbols are
replaced is not important; what is important is that at least one syntactic derivation
must exist for a given piece of syntax in order for it to be considered well formed.
Example 2.4
The following derivation demonstrates that 101 belongs to the syntactic category
BinNumber:
BinNumber ‚áùBit BinNumber
‚áù1 BinNumber
‚áù1 Bit BinNumber
‚áù1 Bit Bit
‚áù10 Bit
‚áù101
‚ô¶

A Language for Access Control
17
Example 2.5
The following derivation demonstrates that (11‚àó(1+10)) belongs to the syntactic
category AExp:
AExp ‚áù( AExp‚àóAExp)
‚áù( AExp‚àó( AExp+AExp))
‚áù( AExp‚àó( BinNumber+AExp))
‚áù( AExp‚àó( Bit+AExp))
‚áù( AExp‚àó( 1+AExp))
‚áù( AExp‚àó( 1+BinNumber))
‚áù( AExp‚àó( 1+Bit BinNumber))
‚áù( AExp‚àó( 1+1 Bit))
‚áù( AExp‚àó( 1+10))
‚áù( BinNumber‚àó( 1+10))
‚áù( Bit BinNumber ‚àó( 1+10))
‚áù( Bit Bit ‚àó( 1+10))
‚áù( 1 Bit ‚àó( 1+10))
‚áù( 11 ‚àó( 1+10))
‚ô¶
Multiple production rules for the same syntactic category can be combined into a
single rule by using the meta-symbol ‚Äú / ‚Äù, which separates possible alternatives. For
example, the three production rules for the AExp syntactic category could instead be
speciÔ¨Åed as follows:
AExp ::= BinNumber / ( AExp + AExp ) / ( AExp ‚àóAExp )
This production rule states that an element of the syntactic category AExp has either
the form BinNumber, the form ( AExp + AExp ), or the form ( AExp ‚àóAExp ).
2.2.1
Principal Expressions
Principals are the major actors in a system. They are the entities that make re-
quests, and the class of principals includes (but is not limited to) people, processes,
cryptographic keys, personal identiÔ¨Åcation numbers (PINs), userid‚Äìpassword pairs,
and so on.
In general, principals may be either simple or compound. A simple principal is
an entity that cannot be decomposed further: individuals, cryptographic keys, and
userid‚Äìpassword pairs are all simple principals. We deÔ¨Åne PName to be the collec-
tion of all simple principal names, which can be used to refer to any simple principal.
For example, the following are all allowable principal names: Alice, Bob, the key
KAlice, the PIN 1234, and the userid‚Äìpassword pair ‚ü®alice,bAdPsWd!‚ü©.
Compound principals are abstract entities that connote a combination of princi-
pals: for example, ‚Äúthe President in conjunction with Congress‚Äù connotes an abstract

18
Access Control, Security, and Trust: A Logical Approach
principal comprising both the President and Congress. Intuitively, such a principal
makes exactly those statements that are made by both the President and Congress.
Similarly, ‚Äúthe reporter quoting her source‚Äù connotes an abstract principal that com-
prises both the reporter and her source. Intuitively, a statement made by such a
principal represents a statement that the reporter is (rightly or wrongly) attributing to
his source.
The set Princ of all principal expressions is given by the following BNF speciÔ¨Å-
cation:
Princ ::= PName / Princ & Princ / Princ | Princ
That is, a principal expression is either a simple name, an expression of form P & Q
(where P and Q are both principal expressions), or an expression of form P | Q
(where, again, P and Q are both principal expressions).
The principal expression P & Q denotes the abstract principal ‚ÄúP in conjunction
with Q,‚Äù while P | Q denotes the abstract principal ‚ÄúP quoting Q.‚Äù Thus, the expres-
sion President & Congress denotes the abstract principal ‚Äúthe President together with
Congress,‚Äù and Reporter | Source denotes the abstract principal ‚Äúthe reporter quoting
her source.‚Äù In subsequent chapters, we will introduce a few more compound princi-
pal expressions to cover other important concepts, such as delegation. However, we
shall see that those additional expressions are merely special cases of quoting and
conjunctive principals.
Parentheses can be added to disambiguate compound principal expressions. For
example, (Sal & Ted) | Uly denotes the conjunctive principal Sal & Ted quoting the
principal Uly. In contrast, Sal & (Ted | Uly) denotes the principal Sal acting in con-
cert with the compound principal Ted | Uly. The standard convention in such expres-
sions is that & binds more tightly than | , so that (for example) Sal & Ted | Uly is
equivalent to (Sal & Ted) | Uly. However, in this book we shall always include the
parentheses necessary to disambiguate principal expressions. Both & and | are asso-
ciative operators, and hence it is unnecessary to include parentheses around principal
expressions such as Fritz & Hans & Leon or Terry | Kitty | Sandy.
2.2.2
Access-Control Statements
Ultimately, we are concerned with being able to determine with precision and
accuracy which access requests from which principals should be granted, and which
should be denied. Thus, we need to be able to express our assumptions and our
expectations, such as which authorities we trust, which principals should be granted
access to which objects, and so on. Toward this end, we introduce a language of
statements that will allow us to express these sorts of concepts.
The simplest statements are basic requests, such as ‚Äúread Ô¨Åle foo‚Äù or ‚Äúmodify Ô¨Åle
bar.‚Äù We will represent such statements in our logic by propositional variables, in
much the same way that statements such as ‚Äúit is raining‚Äù can be represented in

A Language for Access Control
19
propositional logic.1
However, a simple request such as ‚Äúread Ô¨Åle foo‚Äù by itself is generally insufÔ¨Åcient
for determining whether or not to grant the request. At a minimum, we need some
way of accounting for the source of the request. In our logic, we can associate
requests (and other statements) with their source by statements of the form
P says œï,
where P is a principal and œï is a speciÔ¨Åc statement. For example, if rff is a proposi-
tional variable representing the request ‚Äúread Ô¨Åle foo,‚Äù then we can represent Deena‚Äôs
request to read the Ô¨Åle foo by the statement
Deena says rff.
The says operator can also ascribe non-request statements to particular principals.
For example, we may wish to express that Rob believes (or has stated) that Deena is
making a request to read Ô¨Åle foo. We can express such a concept in the logic by the
statement
Rob says (Deena says rff).
Access policies specify which principals are authorized to access particular ob-
jects. Such authorizations can be expressed in our logic by statements of the form
P controls œï,
where P is a principal and œï is a speciÔ¨Åc statement. Thus, for example, we can
express Deena‚Äôs entitlement to read the Ô¨Åle foo as the statement
Deena controls rff.
Note that this statement is different from Deena‚Äôs request to read the Ô¨Åle. This state-
ment merely says that if Deena says that it‚Äôs a good idea to read Ô¨Åle foo (i.e., if
Deena makes the request), then it is indeed a good idea to read Ô¨Åle foo (i.e., the
request should be granted).
In any given situation, there are particular authorities who are deemed to have ju-
risdiction over particular statements: that is, we will unquestioningly believe certain
statements that they make. For example, Anna‚Äôs New York State driver‚Äôs license in
effect represents a statement from the New York State Department of Motor Vehi-
cles (DMV) that vouches for Anna‚Äôs ability to drive a car. Anyone who accepts the
DMV‚Äôs jurisdiction for making that statement will, upon seeing Anna‚Äôs valid driving
license, accept as truth that she is entitled to drive. Like authorizations, jurisdiction
is represented in our logic by statements of the form
P controls œï,
1Technically, propositions are statements that are interpreted to be either true or false, such as the propo-
sition ‚Äúit is raining.‚Äù If we view an access request such as ‚Äúread Ô¨Åle foo‚Äù as shorthand for ‚Äúit would be a
good thing to let me read Ô¨Åle foo,‚Äù then we indeed have a legitimate proposition.

20
Access Control, Security, and Trust: A Logical Approach
where P is a principal with jurisdiction over the statement œï.
Finally, we would like a way to be able to make statements about the relative
trustedness of different principals, which we do through the use of the operator ‚áí
(pronounced ‚Äúspeaks for‚Äù). Intuitively, the statement
P ‚áíQ
describes a proxy relationship between the two principals P and Q such that any
statement made by P can also be safely attributed to Q (i.e., acted upon as if Q had
made it).
In addition to the sorts of statements mentioned above, we make use of the stan-
dard logical operators: negation (¬¨œï), conjunction (œï1 ‚àßœï2), disjunction (œï1 ‚à®œï2),
implication (œï1 ‚äÉœï2), and equivalence (œï1 ‚â°œï2). Having provided an informal
overview of our logic, we give a formal deÔ¨Ånition of our logic‚Äôs syntax in the next
subsection.
2.2.3
Well-Formed Formulas
We give the name PropVar to the collection of all propositional variables, and
we typically use lowercase identiÔ¨Åers (such as p,q,r) to range over this set. We
can construct more interesting statements from these simple propositional variables
and from our set Princ of principal expressions. The set Form of all well-formed
expressions (also known as well-formed formulas) in our language is given by the
following BNF speciÔ¨Åcation:
Form ::= PropVar / ¬¨ Form / (Form‚à®Form) /
(Form‚àßForm) / (Form ‚äÉForm) / (Form ‚â°Form) /
(Princ ‚áíPrinc) / (Princ says Form) / (Princ controls Form)
This BNF speciÔ¨Åcation provides all of the information necessary to determine the
structure of well-formed formulas in our access-control logic. The following are
examples of well-formed formulas:
r
((¬¨q‚àßr) ‚äÉs)
(Jill says (r ‚äÉ(p‚à®q)))
Convention: Throughout this book, we will distinguish between principal
names and propositional variables through capitalization.
SpeciÔ¨Åcally,
we will use capitalized identiÔ¨Åers‚Äîsuch as Josh and Reader‚Äîfor sim-
ple principal names. We will use lowercase identiÔ¨Åers‚Äîsuch as r, write,
and rff‚Äîfor propositional variables.

A Language for Access Control
21
Example 2.6
The following syntactic derivation demonstrates that (Jill says (r ‚äÉ(p ‚à®q))) is a
well-formed formula:
Form ‚áù(Princ says Form)
‚áù(PName says Form)
‚áù(Jill says Form)
‚áù(Jill says (Form ‚äÉForm))
‚áù(Jill says (PropVar ‚äÉForm))
‚áù(Jill says (r ‚äÉForm))
‚áù(Jill says (r ‚äÉ(Form‚à®Form)))
‚áù(Jill says (r ‚äÉ(PropVar‚à®Form)))
‚áù(Jill says (r ‚äÉ(p‚à®Form)))
‚áù(Jill says (r ‚äÉ(p‚à®PropVar)))
‚áù(Jill says (r ‚äÉ(p‚à®q)))
‚ô¶
In contrast, the following examples are not well-formed formulas, for the reasons
stated:
‚Ä¢ Orly & Mitch is a principal expression, but not an access-control formula.
‚Ä¢ ¬¨Orly, because Orly is a principal expression, not an access-control formula;
the negation operator ¬¨ must precede an access-control formula.
‚Ä¢ (Orly ‚áí(p‚àßq)), because (p‚àßq) is not a principal expression: the speaks-for
operator ‚áímust appear between two principal expressions.
‚Ä¢ (Orly controls Mitch), because Mitch is a principal expression, not an access-
control formula; the controls operator requires its second argument to be an
access-control formula.
The parentheses ensure that the grammar is completely unambiguous, but their ex-
cessive proliferation can make the language cumbersome to use. Thus, we typically
omit the outermost parentheses, and we occasionally also omit additional parenthe-
ses according to standard conventions for operator precedence: ¬¨ binds most tightly,
followed in order by ‚àß, ‚à®, ‚äÉ, and ‚â°. Thus, for example, the formula p ‚äÉq ‚àßr
is an abbreviation of (p ‚äÉ(q ‚àßr)).
Likewise, the formulas ((¬¨q ‚àßr) ‚äÉs) and
(Jill says (r ‚äÉ(p‚à®q))) can be abbreviated respectively as follows:
¬¨q‚àßr ‚äÉs,
Jill says (r ‚äÉp‚à®q).
When the same binary operator appears multiple times in a formula‚Äîfor example,
p ‚äÉq ‚äÉr‚Äîthe parentheses associate from left to right: (p ‚äÉq) ‚äÉr. The operators
says and controls bind even more tightly than ‚àß, and thus have as small a scope as

22
Access Control, Security, and Trust: A Logical Approach
possible. For example, Kent says r ‚à®p ‚äÉq is equivalent to (((Kent says r) ‚à®p) ‚äÉ
q), and similarly for controls . Because ‚áíis an operator that relates only principal
expressions (as opposed to logical formulas), its use is always unambiguous.
Exercise 2.2.1
Which of the following are well-formed formulas in the access-

control logic? Support your answers by appealing to the BNF speciÔ¨Åcation.
a. ((p‚àß¬¨q) ‚äÉ(Cal controls r))
b. ((Gin ‚áír)‚àßq)
c. (Mel | Ned says (r ‚äÉt))
d. (¬¨t ‚áíSal)
e. (Ulf controls (Vic | Wes ‚áíTor))
f. (Pat controls (Quint controls (Ryne says s)))
Exercise 2.2.2
Fully parenthesize each of the following formulas:

a. p ‚äÉ¬¨q‚à®r ‚äÉs
b. ¬¨p ‚äÉr ‚â°q‚à®r ‚äÉt
c. X controls t ‚à®s ‚äÉY says q ‚äÉr
d. Cy says q‚àßDi controls p ‚äÉr
e. Ike ‚áíJan‚àßKai & Lee controls q‚àßr
2.3
Semantics
Although we provided an informal reading of the logical formulas in the previous
section, we have not yet provided sufÔ¨Åcient details to enable precise or rigorous use
of the logic. In the next chapter, we introduce logical rules that we can use to reason
about a variety of access-control situations. However, as with any logic, several
important questions arise:
What statements are true in this logic, and how do we know? What does
a given statement really mean? How do we know that the inference rules
are trustworthy? Under what conditions can we add new inference rules
and guarantee that the logic‚Äôs trustworthiness is preserved?

A Language for Access Control
23
p
q
r
p‚àßq
(p‚àßq) ‚äÉr
true
true
true
true
true
true
true
false
true
false
true
false
true
false
true
true
false
false
false
true
false
true
true
false
true
false
true
false
false
true
false
false
true
false
true
false
false
false
false
true
Table 2.1: Truth table for the propositional formula (p‚àßq) ‚äÉr
The key to answering these questions for any logic is to have rigorous, mathemat-
ical semantics that deÔ¨Åne precisely what a given statement means. These formal
semantics provide a basis by which one can independently assess the trustworthiness
of a logical system. For example, in propositional logic, the formal meaning of a
statement such as
(p‚àßq) ‚äÉr
can be calculated using a truth table, as illustrated in Table 2.1. Each line in the truth
table corresponds to a particular interpretation of the propositional variables (i.e., a
mapping of variables to speciÔ¨Åc truth values). Truth tables calculate the meaning
of larger formulas in a syntax-directed way, based on the meanings of their compo-
nents: for example, the meaning of (p‚àßq) ‚äÉr for a given interpretation is calculated
using the meanings of p ‚àßq and r, as well as a speciÔ¨Åc rule for the operator ‚äÉ. A
propositional-logic formula is a tautology‚Äîand therefore safe to use as an axiom of
the system‚Äîif it is true for every possible interpretation of the propositional vari-
ables.
These same core ideas apply to the semantics for our access-control logic. Because
we must account for the interpretation of principals in addition to propositional vari-
ables, the semantics requires a little more structure than truth tables provide. We can
Ô¨Ånd this additional structure in the form of Kripke structures.
2.3.1
Kripke Structures
Kripke structures are useful models for analyzing a variety of situations. They
are commonly used to provide semantics for modal and temporal logics, providing a
basis for automated model checking.
DeÔ¨Ånition 2.1 A Kripke structure M is a three-tuple ‚ü®W,I,J‚ü©, where:
‚Ä¢ W is a nonempty set, whose elements are called worlds.
‚Ä¢ I : PropVar ‚ÜíP(W) is an interpretation function that maps each propositional
variable to a set of worlds.

24
Access Control, Security, and Trust: A Logical Approach
‚Ä¢ J : PName ‚ÜíP(W √óW) is a function that maps each principal name to a
relation on worlds (i.e., a subset of W √óW).
Before we look at some examples of Kripke structures, a few comments about this
deÔ¨Ånition are in order. First, the concept of worlds is an abstract one. In reality,
W is simply a set: its contents (whatever they may be) are called worlds. In many
situations, the notion of worlds corresponds to the notion of system states or to the
concept of possible alternatives.
Second, the functions I and J provide meanings (or interpretations) for our propo-
sitional variables and simple principals. These meanings will form the basis for our
semantics of arbitrary formulas in our logic. Intuitively, I(p) is the set of worlds in
which we consider p to be true. J(A) is a relation that describes how the simple prin-
cipal A views the relationships between worlds: each pair (w,w‚Ä≤) ‚ààJ(A) indicates
that, when the current world is w, principal A believes it possible that the current
world is w‚Ä≤.
For illustration purposes, we introduce some examples of Kripke structures. The
Ô¨Årst example provides some intuition as to what the interpretation functions I and J
represent, illustrating how each relation J(P) might reÔ¨Çect principal P‚Äôs understand-
ing of the universe.
Example 2.7
Consider the situation of three young children (Flo, Gil, and Hal), who are being
looked after by an overprotective babysitter. This babysitter will let them go outside
to play only if the weather is both sunny and sufÔ¨Åciently warm.
To keep things simple, let us imagine that there are only three possible situations:
it is sunny and warm, it is sunny but cool, or it is not sunny. We can represent these
possible alternatives with a set of three worlds: W0 = {sw,sc,ns}.
We use the propositional variable g to represent the proposition ‚ÄúThe children
can go outside.‚Äù The baby sitter‚Äôs overprotectiveness can be represented by any
interpretation function
I0 : PropVar ‚ÜíP({sw,sc,ns})
for which I0(g) = {sw}. That is, the proposition g (‚Äúthe children can go outside‚Äù) is
true only in the world sw (i.e., when the weather is both sunny and warm).
Now, the children themselves are standing by the window, trying to determine
whether or not they‚Äôll be allowed to go outside. Gil, who is tall enough to see the
outdoor thermometer, possesses perfect knowledge of the situation, as he will be able
to determine whether it is both sunny and sufÔ¨Åciently warm. This perfect knowledge
corresponds to a possible-worlds relation
J0(Gil) = {(sw,sw),(sc,sc),(ns,ns)}.
Whatever the current situation is, Gil has the correct understanding of the situation.
(Note that J0(Gil) is the identity relation idW0 over the set W0.)

A Language for Access Control
25
In contrast, Flo is too short to see the outdoor thermometer, and thus she cannot
distinguish between the ‚Äúsunny and warm‚Äù and ‚Äúsunny and cool‚Äù alternatives. This
uncertainty corresponds to a possible-worlds relation
J0(Flo) = {(sw,sw),(sw,sc),(sc,sw),(sc,sc),(ns,ns)}.
Thus, for example, if the current situation is ‚Äúsunny and warm‚Äù (i.e., sw), Flo con-
siders both ‚Äúsunny and warm‚Äù and ‚Äúsunny and cool‚Äù as legitimate possibilities. That
is, J0(Flo)(sw) = {sw,sc}.
Finally, Hal is too young to understand that it can be simultaneously sunny and
cool: he believes that the presence of the sun automatically makes it warm outside.
His confusion corresponds to a possible-worlds relation
J0(Hal) = {(sw,sw),(sc,sw),(ns,ns)}.
Whenever the actual weather is sunny and cool, Hal believes it to be sunny and
warm: J0(Hal)(sc) = {sw}.
The tuple ‚ü®W0,I0,J0‚ü©forms a Kripke structure.
‚ô¶
The next example introduces a Kripke structure that does not necessarily reÔ¨Çect
any particular scenario or vignette. Rather, the Kripke structure is merely a three-
tuple that contains a set and two functions that match the requirements of DeÔ¨Åni-
tion 2.1.
Example 2.8
Let W1 = {w0,w1,w2} be a set of worlds, and let I1 : PropVar ‚ÜíP(W1) be the
interpretation function deÔ¨Åned as follows2:
I1(q) = {w0,w2},
I1(r) = {w1},
I1(s) = {w1,w2}.
In addition, let J1 : PName ‚ÜíP(W1 √óW1) be the function deÔ¨Åned as follows3:
J1(Alice) = {(w0,w0),(w1,w1),(w2,w2)},
J1(Bob) = {(w0,w0),(w0,w1),(w1,w2),(w2,w1)}.
The three-tuple ‚ü®W1,I1,J1‚ü©is a Kripke structure. Intuitively, proposition q is true in
worlds w0 and w2, r is true in world w1, and s is true in worlds w1 and w2. All other
propositions are false in all worlds.
‚ô¶
2In this example and those that follow, we adopt the convention of specifying only those propositional
variables that the interpretation function maps to nonempty sets of worlds. Thus, for any propositional
variable p not explicitly mentioned, we assume that I1(p) = /0.
3We adopt a similar convention for principal-mapping functions J: for any principal name A for which
J(A) is not explicitly deÔ¨Åned, we assume that J(A) = /0.

26
Access Control, Security, and Trust: A Logical Approach
Next State
Present State
x = 0
x = 1
A
A
D
B
A
C
C
C
B
D
C
A
Table 2.2: State-transition table for Ô¨Ånite-state machine M
World
p
q
r
s
A
true
true
false
true
B
false
true
false
true
C
true
false
false
true
D
false
true
false
true
Table 2.3: Truth values of primitive propositions p, q, r, and s in each world
The next example illustrates how a Kripke structure might be used to represent a
state machine.
Example 2.9
Consider the state-transition table for a Ô¨Ånite-state machine M shown in Table 2.2.
This machine has four states: A, B, C, and D. The column labeled ‚ÄúPresent State‚Äù
lists the possible present states of M. The two columns under the label ‚ÄúNext State‚Äù
list the next states of M if the input x is either 0 or 1, respectively. For example, the
second row of Table 2.2 describes M‚Äôs behavior whenever it is currently in state B: if
the input is x is 0, then the next state will be A; if x is 1, then the next state will be C.
We can construct a Kripke structure ‚ü®W2,I2,J2‚ü©to model this machine by deÔ¨Åning
W2 to be the set of M‚Äôs states:
W2 = {A,B,C,D}.
Now, suppose that there are four primitive propositions (p,q,r,s) associated with
the state machine M, with their truth values in the various states given by Table 2.3.
This table effectively speciÔ¨Åes the interpretation function I2 on these propositions,
namely:
I2(p) = {A,C},
I2(q) = {A,B,D},
I2(r) = {},
I2(s) = {A,B,C,D}.
Finally, imagine that there is an observer Obs of the machine‚Äôs execution. This
observer has faulty knowledge of M‚Äôs states: whenever M is in state C, Obs incor-
rectly believes M to be in state D. We‚Äôll assume that the observer does correctly
know when M is in states A, B, or D.

A Language for Access Control
27
This observer‚Äôs state knowledge can be captured by the following relation:
J2(Obs) = {(A,A),(B,B),(C,D),(D,D)}.
In the relation J2(Obs), the Ô¨Årst element of each pair represents the present state of
M, and the second element is the observed state of M. Thus the pair (C,D) reÔ¨Çects
that, whenever M is in state C, Obs always believes the current state is D.
The tuple ‚ü®{A,B,C,D},I2,J2‚ü©forms a Kripke structure.
‚ô¶
In the next example, we model the same state machine, but we consider the inputs
(i.e., ‚Äúx=0‚Äù or ‚Äúx=1‚Äù) as the ‚Äúobservers‚Äù of the state machine, and we use each ‚Äúnext
state‚Äù as the perceived state by the particular observer. Although the set of worlds
and the interpretation function do not change, the principal-mapping function does
change.
Example 2.10
Let W2 and I2 be as deÔ¨Åned in Example 2.9, and deÔ¨Åne J3 as follows:
J3(X0) = {(A,A),(B,A),(C,C),(D,C)},
J3(X1) = {(A,D),(B,C),(C,B),(D,A)}.
The tuple ‚ü®{A,B,C,D},I2,J3‚ü©forms a Kripke structure.
‚ô¶
Just as the interpretation function I of a Kripke structure provides the base inter-
pretation for propositional variables, the function J provides a base interpretation for
simple principal names. We extend J to work over arbitrary principal expressions,
using set union and relational composition as follows:
J(P & Q) = J(P)‚à™J(Q),
J(P | Q) = J(P)‚ó¶J(Q).
Example 2.11
Suppose that we have the following relations:
J(Andy) = {(w0,w0),(w0,w2),(w1,w1),(w2,w1)},
J(Stu) = {(w1,w2)},
J(Keri) = {(w0,w2),(w1,w2),(w2,w2)}.
Then J(Keri | (Andy & Stu)) is calculated as follows:
J(Keri | (Andy & Stu))
= J(Keri)‚ó¶J(Andy & Stu),
= J(Keri)‚ó¶(J(Andy)‚à™J(Stu)),
= J(Keri)‚ó¶{(w0,w0),(w0,w2),(w1,w1),(w2,w1),(w1,w2)}
= {(w0,w1),(w1,w1),(w2,w1)}.
‚ô¶

28
Access Control, Security, and Trust: A Logical Approach
Exercise 2.3.1
Recall the Kripke structure ‚ü®W0,I0,J0‚ü©from Example 2.7, and fur-

ther suppose that
J0(Ida) = {(sw,sc),(sc,sw),(ns,sc),(ns,ns)}.
Calculate the following relations:
a. J0(Hal & Gil)
b. J0(Gil | Hal)
c. J0(Flo & Ida)
d. J0(Hal | Ida)
e. J0(Ida | Hal)
f. J0(Hal & (Ida | Hal))
g. J0(Hal | (Ida & Hal))
2.3.2
Semantics of the Logic
The Kripke structures provide the foundation for a formal, precise, and rigorous
interpretation of formulas in our logic. For each Kripke structure M = ‚ü®W,I,J‚ü©, we
can deÔ¨Åne what it means for formulas in our logic to be satisÔ¨Åed in the structure. We
can also identify those worlds in W for which a given formula is said to be true.
To deÔ¨Åne the semantics, we introduce a family of evaluation functions. Each
Kripke structure M = ‚ü®W,I,J‚ü©gives rise to an evaluation function EM that maps
well-formed formulas in the logic to subsets of W. Intuitively, EM [[œï]] is the set of
worlds from the Kripke structure M for which the well-formed formula œï is consid-
ered true. We say that M satisÔ¨Åes œï (written M |= œï) whenever œï is true in all of
the worlds of M : that is, when EM [[œï]] = W. It follows that a Kripke structure M
does not satisfy œï (written M Ã∏|= œï) when there exists at least one w ‚ààW such that
w Ã∏‚ààEM [[œï]].
Each EM is deÔ¨Åned inductively on the structure of well-formed formulas, making
use of the interpretation functions I and J within the Kripke structure M = ‚ü®W,I,J‚ü©.
We discuss the individual cases separately, starting with the standard propositional
operators and then moving on to the access-control speciÔ¨Åc cases. The full set of
deÔ¨Ånitions is also summarized in Figure 2.1.
Standard Propositional Operators
The semantics for propositional variables and the standard logical connectives
(e.g., negation, conjunction, implication) are very similar to the truth-table interpre-
tations for standard propositional logic. The interpretation function I identiÔ¨Åes those
worlds in which the various propositional variables are true, while the semantics of
the other operators are deÔ¨Åned using standard set operations. We handle these cases
in turn.

A Language for Access Control
29
FIGURE 2.1 Semantics of core logic, for each M = ‚ü®W,I,J‚ü©
EM [[p]] = I(p)
EM [[¬¨œï]] = W ‚àíEM [[œï]]
EM [[œï1 ‚àßœï2]] = EM [[œï1]]‚à©EM [[œï2]]
EM [[œï1 ‚à®œï2]] = EM [[œï1]]‚à™EM [[œï2]]
EM [[œï1 ‚äÉœï2]] = (W ‚àíEM [[œï1]])‚à™EM [[œï2]]
EM [[œï1 ‚â°œï2]] = EM [[œï1 ‚äÉœï2]]‚à©EM [[œï2 ‚äÉœï1]]
EM [[P ‚áíQ]] =
(
W,
if J(Q) ‚äÜJ(P)
/0,
otherwise
EM [[P says œï]] = {w|J(P)(w) ‚äÜEM [[œï]]}
EM [[P controls œï]] = EM [[(P says œï) ‚äÉœï]]
Propositional Variables: The truth of a propositional variable p is determined by
the interpretation function I: a variable p is considered true in world w pre-
cisely when w ‚ààI(p). Thus, for all propositional variables p,
EM [[p]] = I(p).
For example, if M0 is the Kripke structure ‚ü®W0,I0,J0‚ü©from Example 2.7,
EM0[[g]] = I0(g) = {sw}.
Negation: A formula with form ¬¨œï is true in precisely those worlds in which œï is
not true. Because (by deÔ¨Ånition) EM [[œï]] is the set of worlds in which œï is
true, we deÔ¨Åne
EM [[¬¨œï]] = W ‚àíEM [[œï]].
Thus, returning to Example 2.7,
EM0[[¬¨g]] = W0 ‚àíEM0[[g]] = {sw,sc,ns}‚àí{sw} = {sc,ns}.
Notice that EM0[[¬¨g]] is the set of worlds in which the children are not allowed
to go outside.
Conjunction: A conjunctive formula œï1 ‚àßœï2 is considered true in those worlds for
which both œï1 and œï2 are true: that is, œï1 ‚àßœï2 is true in those worlds w for
which w ‚ààEM [[œï1]] and w ‚ààEM [[œï2]]. Thus, we can deÔ¨Åne EM [[œï1 ‚àßœï2]] in
terms of set intersection:
EM [[œï1 ‚àßœï2]] = EM [[œï1]]‚à©EM [[œï2]].

30
Access Control, Security, and Trust: A Logical Approach
Disjunction: Likewise, a disjunctive formula œï1 ‚à®œï2 is considered true in those
worlds for which at least one of œï1 and œï2 is true: that is, œï1 ‚à®œï2 is true
in those worlds w for which w ‚ààEM [[œï1]] or w ‚ààEM [[œï2]]. Thus, we deÔ¨Åne
EM [[œï1 ‚à®œï2]] in terms of set union:
EM [[œï1 ‚à®œï2]] = EM [[œï1]]‚à™EM [[œï2]].
Implication: An implication œï1 ‚äÉœï2 is true in those worlds w for which either
œï2 is true (i.e., w ‚ààEM [[œï2]]) or œï1 is not true (i.e., w Ã∏‚ààEM [[œï1]], and thus
w ‚ààEM [[¬¨œï1]]). That is, œï1 ‚äÉœï2 is true in those worlds in which, if œï1 is true,
then œï2 is also true; if œï1 is false, then œï2‚Äôs interpretation is immaterial. Thus,
we deÔ¨Åne the semantics of implications as follows:
EM [[œï1 ‚äÉœï2]] = (W ‚àíEM [[œï1]])‚à™EM [[œï2]].
Equivalence: An equivalence œï1 ‚â°œï2 is true in exactly those worlds w in which the
implications œï1 ‚äÉœï2 and œï2 ‚äÉœï1 are both true. Thus, we deÔ¨Åne the semantics
of implications as follows:
EM [[œï1 ‚â°œï2]] = EM [[œï1 ‚äÉœï2]]‚à©EM [[œï2 ‚äÉœï1]].
Example 2.12
Let M1 be the Kripke structure ‚ü®W1,I1,J1‚ü©from Example 2.8. The set EM1[[q ‚äÉ
(r ‚àßs)]] of worlds in W1 in which the formula q ‚äÉ(r ‚àßs) is true is calculated as
follows:
EM1[[q ‚äÉ(r ‚àßs)]] = (W1 ‚àíEM1[[q]])‚à™EM1[[r ‚àßs]]
= (W1 ‚àíI1(q))‚à™(EM1[[r]]‚à©EM1[[s]])
= (W1 ‚àí{w0,w2})‚à™(I1(r)‚à©I1(s))
= {w1}‚à™({w1}‚à©{w1,w2})
= {w1}‚à™{w1}
= {w1}.
‚ô¶
In the following example, we evaluate the same formula as in the previous exam-
ple, but with respect to a different Kripke structure.

A Language for Access Control
31
Example 2.13
Let M2 be the Kripke structure ‚ü®W2,I2,J2‚ü©from Example 2.9. The set EM2[[q ‚äÉ
(r‚àßs)]] of worlds W2 in which the formula q ‚äÉ(r‚àßs) is true is calculated as follows:
EM2[[q ‚äÉ(r ‚àßs)]] = (W2 ‚àíEM2[[q]])‚à™EM2[[r ‚àßs]]
= (W2 ‚àíI2(q))‚à™(EM2[[r]]‚à©EM2[[s]])
= (W2 ‚àí{A,B,D})‚à™(I2(r)‚à©I2(s))
= {C}‚à™(/0‚à©{A,B,C,D})
= {C}‚à™/0
= {C}.
‚ô¶
Access-Control Operators
The access-control operators of the logic (e.g., says , controls , and ‚áí) have more
interesting semantics.
Says: A formula P says œï is meant to denote a situation in which the principal P
makes the statement œï. Intuitively, a principal should make statements that
they believe to be true. What does it mean for a principal to believe a statement
is true in a given world? The standard answer is that a principal P believes œï
to be true in a speciÔ¨Åc world w if œï is true in all of the worlds w‚Ä≤ that P can
conceive the current world to be (i.e., all w‚Ä≤ such that (w,w‚Ä≤) is in J(P)). Of
course, this set of possible worlds is simply the set J(P)(w); œï is true in every
world in J(P)(w) if and only if J(P)(w) ‚äÜEM [[œï]]. Therefore, we deÔ¨Åne
EM [[P says œï]] = {w | J(P)(w) ‚äÜEM [[œï]]}.
Controls: Formulas of the form P controls œï express a principal P‚Äôs jurisdiction or
authority regarding the statement œï. We interpret P controls œï as syntactic sugar
for the statement (P says œï) ‚äÉœï, which captures the desired intuition: if the
authority P says that œï is true, then œï is true. Thus, we give the meaning of
P controls œï directly as the meaning of this rewriting:
EM [[P controls œï]] = EM [[(P says œï) ‚äÉœï]].
Speaks For: To understand the semantics of formulas with form P ‚áíQ, recall the
purpose of such formulas: we wish to express a proxy relationship between P
and Q that will permit us to safely attribute P‚Äôs statements to Q as well, inde-
pendent of a particular world. That is, if P ‚áíQ, then it should be reasonable
to interpret any statement from P as being a statement that Q would also make.
In terms of the semantics, we have seen that a principal P making a statement
œï in a world w means that J(P)(w) ‚äÜEM [[œï]]. Thus, if we wish to associate
all of P‚Äôs statements to Q, then we need to know that J(Q)(w) ‚äÜJ(P)(w) for

32
Access Control, Security, and Trust: A Logical Approach
all worlds w. If J(Q) ‚äÜJ(P), then this relationship naturally holds. Therefore,
we deÔ¨Åne
EM [[P ‚áíQ]] =
(
W,
if J(Q) ‚äÜJ(P)
/0,
otherwise.
The following examples illustrate these semantic deÔ¨Ånitions.
Example 2.14
Recall M0 = ‚ü®W0,I0,J0‚ü©from Example 2.7. The set of worlds in W0 in which the for-
mula Hal says g is true is given by EM0[[Hal says g]], which is calculated as follows:
EM0[[Hal says g]] = {w | J0(Hal)(w) ‚äÜEM0[[g]]}
= {w | J0(Hal)(w) ‚äÜ{sw}}
= {sw,sc}.
This result captures Hal‚Äôs mistaken belief that, whenever it is sunny (i.e., when the
current world is either sw or sc), the children will be able to go outside.
In contrast, recall that Flo is unable to distinguish the two worlds sw and sc.
SpeciÔ¨Åcally, the relation J0(Flo) has the following properties:
J0(Flo)(sw) = {sw,sc},
J0(Flo)(sc) = {sw,sc},
J0(Flo)(ns) = {ns}.
Thus, the worlds in which Flo says g is true can be calculated as follows:
EM0[[Flo says g]] = {w | J0(Flo)(w) ‚äÜEM0[[g]]}
= {w | J0(Flo)(w) ‚äÜ{sw}}
= /0.
That is, there are no worlds in which Flo is convinced that the children will be able
to go outside.
‚ô¶
Example 2.15
Recall M1 = ‚ü®W1,I1,J1‚ü©from Examples 2.8 and Example 2.12. The set of worlds in
W1 in which the formula Alice says (q ‚äÉ(r‚àßs)) is true is given by EM1[[Alice says (q ‚äÉ
(r ‚àßs))]], calculated as follows:
EM1[[Alice says (q ‚äÉ(r ‚àßs))]] = {w | J1(Alice)(w) ‚äÜEM1[[q ‚äÉ(r ‚àßs)]]}
= {w | J1(Alice)(w) ‚äÜ{w1}}
= {w1}.
This result is not surprising, because Alice had perfect knowledge of the separate
worlds: thus, she believes q ‚äÉ(r ‚àßs) to be true in precisely those worlds in which it
is true.
‚ô¶

A Language for Access Control
33
Example 2.16
Consider the same Kripke structure M1 = ‚ü®W1,I1,J1‚ü©as in the previous example, but
now we investigate Bob‚Äôs view of the situation. Recall the following details about
J1(Bob):
J1(Bob)(w0) = {w0,w1},
J1(Bob)(w1) = {w2},
J1(Bob)(w2) = {w1}.
The set of worlds in W1 in which the formula Bob says (q ‚äÉ(r ‚àßs)) is true is given
by EM1[[Bob says (q ‚äÉ(r ‚àßs))]], which is calculated as follows:
EM1[[Bob says (q ‚äÉ(r ‚àßs))]] = {w | J1(Bob)(w) ‚äÜEM1[[q ‚äÉ(r ‚àßs)]]}
= {w | J1(Bob)(w) ‚äÜ{w1}}
= {w2}.
‚ô¶
As remarked at the beginning of this section, a particular Kripke structure M =
‚ü®W,I,J‚ü©satisÔ¨Åes a formula œï, written M |= œï, if and only if œï is true of every world
in W (i.e., if EM [[œï]] = W). The following examples illustrate this concept.
Example 2.17
Recall the Kripke structure M1 = ‚ü®W1,I1,J1‚ü©from Examples 2.8 and 2.12. M1 satis-
Ô¨Åes the formula q‚à®r, but not the formula q ‚äÉ(r ‚àßs):
EM1[[q‚à®r]] = {w0,w1,w2} = W1,
EM1[[q ‚äÉ(r ‚àßs)]] = {w1} Ã∏= W1.
Thus, we write M1 |= q‚à®r and M1 Ã∏|= q ‚äÉ(r ‚àßs).
‚ô¶
Example 2.18
The same Kripke structure M1 = ‚ü®W1,I1,J1‚ü©from Examples 2.8 and 2.12 also satis-
Ô¨Åes the formula Alice controls (q ‚äÉ(r ‚àßs)), because:
EM1[[Alice controls (q ‚äÉ(r ‚àßs))]]
= EM1[[(Alice says (q ‚äÉ(r ‚àßs))) ‚äÉ(q ‚äÉ(r ‚àßs))]]
= (W1 ‚àíEM1[[Alice says (q ‚äÉ(r ‚àßs))]])‚à™EM1[[q ‚äÉ(r ‚àßs)]]
= (W1 ‚àí{w1})‚à™{w1}
= W1.
Therefore, we can write M1 |= Alice controls (q ‚äÉ(r ‚àßs)).
‚ô¶

34
Access Control, Security, and Trust: A Logical Approach
The following exercises provide opportunities to use the Kripke semantics to cal-
culate the meanings of access-control formulas, to identify structures that satisfy
given formulas, and to prove useful properties about the semantics.
Exercise 2.3.2
Suppose M is the Kripke structure ‚ü®W,I,J‚ü©, where W, I, and J are

deÔ¨Åned as follows:
W = {w0,w1,w2}
I(s) = {w1,w2}
I(t) = {w2}
J(Cy) = {(w1,w0),(w1,w1),(w2,w0)}
J(Di) = {(w0,w1),(w1,w0),(w2,w2)}.
For each of the following formulas, give the set of worlds in W for which the
formula is true (i.e., calculate EM [[œï]] for each formula œï).
a. s ‚äÉt
b. ¬¨(s ‚äÉt)
c. Cy says (s ‚äÉt)
d. Cy says ¬¨(s ‚äÉt)
e. Di says (s ‚äÉt)
f. Di says ¬¨(s ‚äÉt)
g. (Cy & Di) says (s ‚äÉt)
h. (Cy & Di) says ¬¨(s ‚äÉt)
i. (Di | Cy) says (s ‚äÉt)
j. (Di | Cy) says ¬¨(s ‚äÉt)
k. Di ‚áíCy
l. Cy says (Di ‚áíCy)
m. Di says (Di ‚áíCy)
n. Di says (Di & Cy ‚áíCy)
Exercise 2.3.3
Let M be the Kripke structure ‚ü®W,I,J‚ü©, where W, I, and J are

deÔ¨Åned as follows:
‚Ä¢ W = {t,u,v,x,y,z}

A Language for Access Control
35
‚Ä¢ I : PropVar ‚Üí2W given by:
I(p) = {x,y,z}
I(q) = {x,y,t}
I(r) = {y,t,u,z}
‚Ä¢ J : PName ‚Üí2W√óW given by:
J(A) = {(w,w) | w ‚ààW}‚à™{(x,y),(x,z),(z,t),(y,v),(v,y),(v,x)}
J(B) = {(x,w) | w ‚ààW}‚à™{(y,t),(z,t),(t,v)}.
Calculate each of the following sets.
a. EM [[(p ‚äÉq) ‚äÉr]]
b. EM [[A says (p ‚äÉr)]]
c. EM [[A says (B says q)]]
d. EM [[B says (B says q)]]
e. EM [[A controls (B says q)]]
f. EM [[A controls (B controls q)]]
Exercise 2.3.4
Let M be the Kripke structure ‚ü®W,I,J‚ü©, where
W = {n,r,c,d,rc,rd,cd,rcd}
I(read) = {r,rc,rd,rcd}
I(copy) = {c,rc,cd,rcd}
I(del) = {d,rd,cd,rcd}
J(X) = {(w,rcd) | w ‚ààW}
J(Y) = idW
J(Z) = {(w,n) | w ‚ààW}.
a. Prove that M |= Y controls (read ‚àßcopy).
b. Prove that M Ã∏|= X controls del.
Exercise 2.3.5
Let A = ‚ü®WA,IA,JA‚ü©and B = ‚ü®WB,IB,JB‚ü©be the Kripke structures

36
Access Control, Security, and Trust: A Logical Approach
deÔ¨Åned by:
WA = {a1,a2,a3}
IA(p) = {a3}
IA(q) = {a1,a3}
IA(r) = {a1,a2}
JA(Val) = {(a1,a1),(a1,a2),(a2,a1),(a3,a2)}
JA(Wyn) = {(a1,a3),(a2,a3),(a3,a2)}.
WB = {b1,b2,b3,b4}
IB(p) = {b2,b1}
IB(q) = {b3}
IB(r) = {b2,b4,b1}
JB(Val) = {(b1,b2),(b2,b3),(b2,b4),(b4,b4)}
JB(Wyn) = {(b1,b1),(b2,b1),(b3,b2),(b3,b4),(b4,b2)}.
Which of the following statements are true? Support your answers with calcula-
tions using the evaluation functions EA[[‚àí]] and EB[[‚àí]].
a. A |= p‚à®(q ‚äÉr)
b. B |= p‚à®(q ‚äÉr)
c. A |= Val ‚áíWyn
d. B |= Val ‚áíWyn
e. A |= Val | Wyn says r
f. B |= Val | Wyn says r
g. A |= Wyn controls (p‚àßr)
h. B |= Wyn controls (p‚àßr)
Exercise 2.3.6
For each of the following formulas œï, Ô¨Ånd Kripke structures Mx and
My such that Mx |= œï and My Ã∏|= œï.
a. Ned says (p ‚â°q)
b. Olaf controls q
c. Pam says (Rue controls r)
d. Sal controls (Tom ‚áíUgo)

A Language for Access Control
37
Exercise 2.3.7
Prove that, for any Kripke structure M = ‚ü®W,I,J‚ü©, principal P, and
formulas œï1 and œï2, the following relationship holds:
EM [[P says (œï1 ‚â°œï2)]]‚à©EM [[P says œï1]] ‚äÜEM [[P says œï2]].
Exercise 2.3.8
Prove that, for every Kripke structure M = ‚ü®W,I,J‚ü©, principal P,
and formulas œï1 and œï2, the following relationship holds:
M |= P says (œï1 ‚â°œï2) ‚äÉ(P says œï1 ‚äÉP says œï2).
Hint: Exercise 2.3.7 is helpful here.
Exercise 2.3.9
Prove that, for any Kripke structure M = ‚ü®W,I,J‚ü©and formulas œï1
and œï2, EM [[œï1 ‚â°œï2]] = W if and only if EM [[œï1]] = EM [[œï2]]. That is, prove the
following two statements:
a. If EM [[œï1 ‚â°œï2]] = W, then EM [[œï1]] = EM [[œï2]].
b. If EM [[œï1]] = EM [[œï2]], then EM [[œï1 ‚â°œï2]] = W.
2.4
Summary
Providing and maintaining security‚Äîin both the physical and digital worlds‚Äî
requires us to be able to determine whether or not any given decision to grant access
to resources or services is correct. Unfortunately, natural language expressions have
imprecise meanings. We need a way to unambiguously express the policies, trust
assumptions, recognized authorities, and statements made by various principals and
be able to justify the resulting access-control decisions.
In this chapter, we introduced a language that allows us to express our policies,
trust assumptions, recognized authorities, and statements in a precise and unambigu-
ous way. Expressions in this language are given precise, mathematical meanings
through the use of Kripke structures. This Kripke semantics provides the initial ba-
sis for mathematically justifying access-control decisions: given a Kripke structure
and an expression in the language, we can compute those worlds in which the ex-
pression is true. This semantics will allow us to introduce sound rules for justifying
access-control decisions, as demonstrated in the next chapter.
The learning outcomes associated with this chapter appear in Figure 2.2.
2.5
Further Reading
For a detailed introduction to discrete mathematics and propositional logic, we
direct the interested reader to some of the standard undergraduate textbooks on the

38
Access Control, Security, and Trust: A Logical Approach
FIGURE 2.2 Learning outcomes for Chapter 2
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Application
‚Ä¢ Determine whether or not a given collection of symbols constitutes a
well-formed formula of the access-control logic.
‚Ä¢ When given an informal statement in English, translate it into a well-
formed formula of the access-control logic.
‚Ä¢ When given a Kripke structure and a formula of the access-control logic,
determine the set of worlds in which the formula is true.
Analysis
‚Ä¢ When given a formula and a Kripke structure that satisÔ¨Åes it, prove that
the Kripke structure satisÔ¨Åes it.
Synthesis
‚Ä¢ When given a satisÔ¨Åable formula, construct a Kripke structure that satis-
Ô¨Åes the formula.
‚Ä¢ When given a refutable formula, construct a Kripke structure that refutes
(i.e., fails to satisfy) the formula.
‚Ä¢ When given a general property regarding Kripke structures, prove that
the property holds.
Evaluation
‚Ä¢ When given a Kripke structure and a formula, determine whether or not
the Kripke structure satisÔ¨Åes the formula.
subject (Rosen, 2003; Ross and Wright, 2002).
The access-control logic presented here is based on the logic of Abadi, Lampson,
and colleagues (Lampson et al., 1992; Abadi et al., 1993), which in turn was built
on top of a standard modal logic. Modal logic is the logical study of necessity and
possibility, and Hughes and Cresswell provide an excellent introduction to the Ô¨Åeld
(Hughes and Cresswell, 1996).

Chapter 3
Reasoning about Access Control
In the previous chapter, we introduced a language for describing access-control sce-
narios. We presented the syntax for well-formed statements in the language (i.e.,
formulas). We also speciÔ¨Åed their semantics through the use of Kripke structures.
Although Kripke structures provide precise meanings for these statements, it is not
practical to analyze access-control situations at such a low level. First of all, Kripke
structures are cumbersome to use for even simple situations. Second, in any given
situation, it is unclear which structures accurately capture the state of the universe.
How could we possibly know which Kripke structure to use for any given analysis?
In this chapter, we introduce a collection of inference rules that will provide the
basis for reasoning rigorously about access control. These rules describe a system
for manipulating well-formed formulas as a way of calculating the consequences of
various assumptions. A crucial property for these rules is that they must be sound
with respect to the Kripke-structure semantics. That is, the rules should ensure that,
in any situation where a given Kripke structure satisÔ¨Åes all of a rule‚Äôs premises, the
Kripke structure also satisÔ¨Åes the rule‚Äôs consequent. Informally, soundness means
that it is impossible to deduce a ‚Äúfalse‚Äù statement from ‚Äútrue‚Äù ones. In this chapter,
we demonstrate how to verify that the rules we introduce are sound.
3.1
Logical Rules
The logical rules are summarized in Figure 3.1. Each rule of the logic has the form
H1
¬∑¬∑¬∑
Hk
C
,
where the items written above the line (e.g., H1,...Hk) correspond to hypotheses (or
premises) and the item below the line (e.g., C) corresponds to a consequence (or
conclusion). A special case occurs when there are no hypotheses on the top of the
rule (i.e., when k = 0): an inference rule with an ‚Äúempty top‚Äù is called an axiom.
Informally, we often read such rules as ‚ÄúIf all the assertions on the top are true,
then the consequence below the line will also be true.‚Äù More accurately, however, the
logical rules describe a system for manipulating well-formed formulas of the logic.
In fact, one can think of the logical rules as deÔ¨Åning a game, in which a player writes
39

40
Access Control, Security, and Trust: A Logical Approach
FIGURE 3.1 Logical rules for the access-control logic
Taut
œï
if œï is an instance of a prop-logic tautology
Modus Ponens
œï
œï ‚äÉœï‚Ä≤
œï‚Ä≤
Says
œï
P says œï
MP Says
(P says (œï ‚äÉœï‚Ä≤)) ‚äÉ(P says œï ‚äÉP says œï‚Ä≤)
Speaks For
P ‚áíQ ‚äÉ(P says œï ‚äÉQ says œï)
& Says
(P & Q says œï) ‚â°((P says œï)‚àß(Q says œï))
Quoting
(P | Q says œï) ‚â°(P says Q says œï)
Idempotency of ‚áí
P ‚áíP
Transitivity
of ‚áí
P ‚áíQ
Q ‚áíR
P ‚áíR
Monotonicity
of ‚áí
P ‚áíP‚Ä≤
Q ‚áíQ‚Ä≤
P | Q ‚áíP‚Ä≤ | Q‚Ä≤
Equivalence
œï1 ‚â°œï2
œà[œï1/q]
œà[œï2/q]
P controls œï
def
=
(P says œï) ‚äÉœï

Reasoning about Access Control
41
FIGURE 3.2 Common propositional-logic tautologies
p‚à®¬¨p
p ‚â°(¬¨¬¨p)
p ‚äÉ(q‚à®p)
p ‚äÉ(q ‚äÉp)
(p‚àßq) ‚äÉp
¬¨(¬¨p‚àßp)
p ‚äÉ(q ‚äÉ(p‚àßq))
(p‚àßq) ‚äÉ(p ‚äÉq)
(p‚àßq) ‚äÉ(q‚àßp)
(p ‚â°q) ‚äÉ(p ‚äÉq)
((p‚à®q)‚àß¬¨p) ‚äÉq
((p ‚äÉq)‚àß(q ‚äÉr)) ‚äÉ(p ‚äÉr)
(i.e., derives) various formulas on a piece of paper. Each rule states that, if all the
premises of an inference rule have already been written down (derived), then the
conclusion can also be written down (derived). Axioms can always be written down.
We return to this notion of derivations in Section 3.2, where we introduce formal
proofs. For now, however, we discuss each of the logical rules in turn.
3.1.1
The Taut Rule
The simplest rule is the axiom Taut:
Taut
œï
if œï is an instance of a prop-logic tautology
This axiom states that any instance of a tautology from propositional logic can
be introduced at any time as a derivable statement in the access-control logic. To
understand what this rule means, Ô¨Årst recall that a propositional-logic tautology is a
formula that evaluates to true under all possible interpretations of its propositional
variables. For example, the propositional formula p ‚à®¬¨p always evaluates to true,
independent of whether the propositional variable p is assigned the value true or the
value false. In contrast, the formula p ‚äÉ¬¨p is not a tautology, because it evaluates to
false whenever p is assigned the value true. Although it does not constitute a com-
plete listing, Figure 3.2 summarizes some common propositional-logic tautologies.
A formula œï is an instance of the formula œà if there exist propositional variables
p1,..., pk (for some k ‚â•0) and modal formulas œï1,...,œïk such that œï is obtained by
replacing each pi in œà by œïi. For example, the formula
(Alice says go)‚à®((sit ‚àßread) ‚äÉ(Alice says go))
is an instance of the formula q‚à®(r ‚äÉq): it can be obtained by replacing every q by
(Alice says go) and every r by (sit ‚àßread). In contrast, the formula
(Alice says go)‚à®((sit ‚àßread) ‚äÉstay)
is not an instance of the formula q ‚à®(r ‚äÉq), because the two separate occurrences
of q were not replaced by the same formula.

42
Access Control, Security, and Trust: A Logical Approach
Thus, the Taut rule allows us to introduce (i.e., derive) any formula that can be
obtained from a propositional-logic tautology by a uniform substitution as described
above. For example, since p‚à®¬¨p is a tautology, we can derive the following formula:
(Paul controls (read ‚àßwrite))‚à®¬¨(Paul controls (read ‚àßwrite))
3.1.2
The Modus Ponens Rule
Another common rule is Modus Ponens:
Modus Ponens
œï
œï ‚äÉœï‚Ä≤
œï‚Ä≤
This rule states that, if both the implication œï1 ‚äÉœï2 and the formula œï1 have been
previously introduced, then we can also introduce the formula œï2.
For example, if we have previously derived the two formulas
(Bill says sell) ‚äÉbuy
and
Bill says sell,
then we can use the Modus Ponens rule to also derive buy.
3.1.3
The Says Rule
The Says rule is deÔ¨Åned as follows:
Says
œï
P says œï
Informally, this rule states that any principal can make any statement (or safely be as-
sumed to have made any statement) that has already been derived. Thus, for example,
if we have previously derived (read ‚àßcopy), then we can derive Cara says (read ‚àß
copy).
3.1.4
The MP Says Rule
The MP Says axiom serves as a version of modus ponens for statements made by
principals:
MP Says
(P says (œï ‚äÉœï‚Ä≤)) ‚äÉ(P says œï ‚äÉP says œï‚Ä≤)
In effect, this rule allows us to distribute the says operator over implications. For
example, this axiom allows us to derive the following formula:
(Graham says (sit ‚äÉeat)) ‚äÉ((Graham says sit) ‚äÉ(Graham says eat)).

Reasoning about Access Control
43
3.1.5
The Speaks For Rule
The Speaks For axiom is deÔ¨Åned as follows:
Speaks For
P ‚áíQ ‚äÉ(P says œï ‚äÉQ says œï)
In effect, this rule captures our intuition about the speaks-for relation. It states that,
if P speaks for Q, then any statements P makes should also be attributable to Q. For
example, this axiom allows us to derive the following statement:
Del ‚áíEd ‚äÉ((Del says buy) ‚äÉ(Ed says buy)).
Were we to subsequently to derive the formula Del ‚áíEd, the Modus Ponens rule
would let us also derive the formula
(Del says buy) ‚äÉ(Ed says buy).
3.1.6
The & Says and Quoting Rules
There are two rules that relate statements made by compound principals to those
made by simple principals. The Ô¨Årst of these is the & Says rule:
& Says
(P & Q says œï) ‚â°((P says œï)‚àß(Q says œï))
This rule reÔ¨Çects the conjunctive nature of a principal P&Q: the statements made
by the compound principal P&Q (i.e., P in conjunction with Q) are precisely those
statements that both P and Q are willing to make individually. For example, the
& Says rule allows us to derive the following formula:
(Faith & Gale says sing) ‚â°((Faith says sing)‚àß(Gale says sing)).
The second such rule is the Quoting rule:
Quoting
(P | Q says œï) ‚â°(P says Q says œï)
This rule captures the underlying intuition behind the compound principal P | Q:
the statements made by P | Q (i.e., P quoting Q) are precisely those statements that
P claims Q has made. As an example, here is an instance of the Quoting rule:
(Iona | J¬®urgen says vote for Kory) ‚â°(Iona says J¬®urgen says vote for Kory).
3.1.7
Properties of ‚áí
There are three rules that relate to properties of the ‚áírelation, namely idempo-
tency, transitivity, and monotonicity. These rules are all quite simple, but they are

44
Access Control, Security, and Trust: A Logical Approach
useful for analyzing situations that involve chains of principals speaking for one an-
other.
The Idempotency of ‚áírule states that every principal speaks for itself:
Idempotency of ‚áí
P ‚áíP
Thus, for example, with this rule we can derive the following formula:
Wallace ‚áíWallace.
Although this rule may seem both obvious and unnecessary, it can be useful in con-
junction with monotonicity to reason about quoting principals, as illustrated later in
this subsection.
The Transitivity of ‚áírule supports reasoning about chains of principals that rep-
resent one another:
Transitivity of ‚áí
P ‚áíQ
Q ‚áíR
P ‚áíR
This rule states that, if one principal speaks for a second and the second also speaks
for a third, then it is also safe to view the Ô¨Årst principal as speaking for the third
principal. For example, if we have previously derived the two formulas
Kanda ‚áíTheo,
Theo ‚áíVance,
then the Transitivity of ‚áírule allows us to also derive
Kanda ‚áíVance.
Finally, the Monotonicity of ‚áírule states that quoting principals preserve the
speaks-for relationship:
Monotonicity of ‚áí
P ‚áíP‚Ä≤
Q ‚áíQ‚Ä≤
P | Q ‚áíP‚Ä≤ | Q‚Ä≤
As an example, suppose that we have already derived the following two formulas:
Lowell ‚áíMinnie,
Norma ‚áíOrson.
The Monotonicity of ‚áírule allows us to also derive the formula
Lowell | Norma ‚áíMinnie | Orson.
To see the utility of idempotency, consider the case where we have already derived
the formula Penny ‚áíRonald, and we would like to derive that
Penny | Sylvester ‚áíRonald | Sylvester.
By Ô¨Årst using idempotency to derive Sylvester ‚áíSylvester, we can use monotonicity
to derive the desired formula.

Reasoning about Access Control
45
3.1.8
The Equivalence Rule
The Equivalence rule allows one to replace subformulas in a formula with equiv-
alent subformulas:
Equivalence
œï1 ‚â°œï2
œà[œï1/q]
œà[œï2/q]
To understand this rule, one must Ô¨Årst understand the meta-notation œà[œï/q], which
denotes the result of replacing every occurrence of the propositional variable q within
the formula œà by the formula œï. For example,
(t ‚äÉ(P says (r ‚àßt)))[Q says s/t]
is simply the formula
(Q says s) ‚äÉ(P says (r ‚àßQ says s)).
Thus, the equivalence rule states that, if formulas œï1 and œï2 are equivalent, then
every occurrence of œï1 in a formula œà[œï1/q] can be replaced by œï2, resulting in the
formula œà[œï2/q]. For example, suppose that we have already derived the following
two formulas:
s‚àßt ‚â°Tom says r,
Arnie | May controls (s‚àßt).
Because the latter formula is equivalent to (Arnie | May controls p)[(s ‚àßt)/p], the
Equivalence rule allows us to derive the formula
Arnie | May controls (Tom says r).
In fact, by choosing œà and the propositional variable q judiciously, one can also
use the Equivalence rule to replace only some of the occurrences of œï1 with œï2. As
an example, suppose that we have previously derived the following two formulas:
(t ‚äÉr) ‚â°P says w,
((R says (t ‚äÉr)) ‚àß(t ‚äÉr)) ‚äÉ(T | R controls (t ‚äÉr)).
The latter formula can be obtained by any of the following substitutions (among
others):
(((R says (t ‚äÉr)) ‚àßq) ‚äÉ(T | R controls (t ‚äÉr)))[t ‚äÉr/q]
(((R says (t ‚äÉr)) ‚àßq) ‚äÉ(T | R controls q))[t ‚äÉr/q]
(((R says q) ‚àßq) ‚äÉ(T | R controls q))[t ‚äÉr/q].
Consequently, the Equivalence rule would allow us to derive any of the following
formulas:
(((R says (t ‚äÉr)) ‚àßq) ‚äÉ(T | R controls (t ‚äÉr)))[P says w/q]
(((R says (t ‚äÉr)) ‚àßq) ‚äÉ(T | R controls q))[P says w/q]
(((R says q) ‚àßq) ‚äÉ(T | R controls q))[P says w/q].

46
Access Control, Security, and Trust: A Logical Approach
Exercise 3.1.1
Calculate the results of each of the following substitutions.

a. ((t ‚äÉr)‚à®(P says r))[Q controls t/r]
b. w ‚äÉP says t[P | Q says r/w]
c. ((s‚àß(Q says s)) ‚äÉQ says p)[P says q/s]
d. ((s‚àß(Q says t)) ‚äÉQ says p)[P says q/s]
Exercise 3.1.2
For each pair of formulas (œï,œï‚Ä≤) given below, give formulas œà,œï1,œï2

such that œà[œï1/q] is œï and œà[œï2/q] is œï‚Ä≤. The formula œà should represent as much
shared structure as possible between œï and œï‚Ä≤, allowing œï1 and œï2 to be as simple
as possible.
a. œï = (P & Q says (s ‚äÉt)) ‚äÉt
œï‚Ä≤ = (P | Q says w) ‚äÉt
b. œï = (Q controls (P controls t))‚àß(P | Q says t)
œï‚Ä≤ = (Q controls (P controls t))‚àß((P controls (Q controls t)) ‚äÉt)
c. œï = ((R says s)‚àß(Q says R says s)) ‚äÉ(Q controls (R says s))
œï‚Ä≤ = ((R says s)‚àß(Q says R says t)) ‚äÉ(Q controls (R says t))
d. œï = ((R says s)‚àß(Q says R says s)) ‚äÉ(Q controls (R says s))
œï‚Ä≤ = ((T controls w)‚àß(Q says T controls w)) ‚äÉ(Q controls (T controls w))
3.1.9
The Controls DeÔ¨Ånition
Finally, the following deÔ¨Ånition1 governs the use of controls in our logic:
P controls œï
def
=
(P says œï) ‚äÉœï
This deÔ¨Ånition states that a formula of the form P controls œï is syntactic sugar for
the longer expression (P says œï) ‚äÉœï. That is, controls doesn‚Äôt give our logic any
additional expressiveness, but it provides a useful way to make more explicit what
will turn out to be a common idiom. This deÔ¨Ånition means that, any time we see an
expression of form P controls œï, we can replace it by (P says œï) ‚äÉœï, and vice versa.
This deÔ¨Ånition matches the semantics we deÔ¨Åned for our logic in Section 2.3.2: we
deÔ¨Åned the meaning of P controls œï to be the meaning of (P says œï) ‚äÉœï.
As an example of the use of this deÔ¨Ånition, we can replace any occurrence of
the formula Lily controls read‚Äîeven within the context of a larger formula‚Äîby the
1Note that def
= is meta-notation, rather than a part of the logic‚Äôs syntax. The net effect, however, of this
deÔ¨Ånition is the same as if we introduced an axiom stating P controls œï ‚â°((P says œï) ‚äÉœï), in that either
side of the deÔ¨Ånition may safely be replaced by the other in any formula.

Reasoning about Access Control
47
FIGURE 3.3 A simple formal proof
1. Al says (r ‚äÉs)
Assumption
2. r
Assumption
3. (Al says (r ‚äÉs)) ‚äÉ(Al says r ‚äÉAl says s)
MP Says
4. Al says r ‚äÉAl says s
1,3 Modus Ponens
5. Al says r
2 Says
6. Al says s
4,5 Modus Ponens
FIGURE 3.4 A formal proof of the Controls rule
1. P controls œï
Assumption
2. P says œï
Assumption
3. (P says œï) ‚äÉœï
1 Defn controls
4. œï
2,3 Modus Ponens
formula (Lily says read) ‚äÉread, and vice versa. Thus, for example, the Controls
deÔ¨Ånition allows us to replace the formula
Manny says (Lily controls read)
by the formula
Manny says ((Lily says read) ‚äÉread).
3.2
Formal Proofs and Theorems
A formal proof is a sequence of statements of the logic, where each statement is
either an assumption or a statement that can be derived by applying one of the infer-
ence rules (or deÔ¨Ånitions) to previous statements in that sequence. It is customary
to sequentially number each of these statements, and to label them either with ‚ÄúAs-
sumption‚Äù or with the statement numbers and inference-rule name by which it was
deduced. In this book, we will always place all assumptions at the beginning of the
proof, so that it is quick and easy to determine the premises upon which a conclusion
depends.
For example, Figure 3.3 presents a simple formal proof. In this case, only the
Ô¨Årst two statements in the proof are assumptions; every other statement is either an
instance of axiom (e.g., Step 3) or a consequence of applying one of the inference
rules. As another simple example, Figure 3.4 demonstrates how the deÔ¨Ånition of
the controls operator can be used in a formal proof.
Every formal proof represents a theorem, which is really just a derived infer-
ence rule. SpeciÔ¨Åcally, if the only assumptions of the formal proof are statements

48
Access Control, Security, and Trust: A Logical Approach
H1,...Hk, and if the Ô¨Ånal statement of the proof is C, then the proof corresponds to
a derived inference rule with the following form:
H1
¬∑¬∑¬∑
Hk
C
.
Thus, the formal proofs in Figure 3.3 and Figure 3.4 correspond respectively to
the following two derived theorems2:
Al says (r ‚äÉs)
r
Al says s
P controls œï
P says œï
œï
.
These theorems can now be used as additional inference rules in any future proof,
without affecting that proof‚Äôs validity. We shall Ô¨Ånd the latter derived rule very useful
in subsequent chapters, and hence we give it a speciÔ¨Åc name (Controls). In fact,
there are several derived rules‚Äîsome from propositional logic and others related
to access control‚Äîthat we will Ô¨Ånd convenient to have in our arsenal. For easy
reference, we summarize those rules (along with Controls) in Figure 3.5, leaving
most of their proofs as exercises for the reader. However, Figure 3.6 gives a proof
for the Conjunction rule, which also demonstrates the general form for many of the
other proofs.
Exercise 3.2.1
Give a formal proof of the Hypothetical Syllogism derived rule from

Figure 3.5.
Exercise 3.2.2
Technically speaking, the Equivalence rule given in Figure 3.1 per-

mits replacements in only direction: having deduced œï1 ‚â°œï2, one can replace oc-
currences of œï1 in a formula by œï2, but not vice versa.
Give a formal proof of the following derived rule which permits replacements in
the opposite direction3:
œï1 ‚â°œï2
œà[œï2/q]
œà[œï1/q]
.
Exercise 3.2.3
Give a formal proof for the Derived Speaks For rule given in Fig-

ure 3.5.
Exercise 3.2.4
Give a formal proof for the Derived Controls rule given in Fig-

ure 3.5.
Exercise 3.2.5
Give a formal proof for the Says SimpliÔ¨Åcation derived rules given

in Figure 3.5.
Exercise 3.2.6
Give a formal proof for the following derivable inference rule:

2Technically, the proof of Figure 3.4 is a proof schema that represents a theorem schema, in that P and œï
are meta-variables that range over (respectively) all possible principal expressions and formulas. Thus,
it provides a recipe for any particular instances of P and œï. In this book, we shall blur the distinction
between proofs and proof schemas, as well as theorems and theorem schemas.
3Henceforth in this book, we shall not distinguish between these two versions of Equivalence.

Reasoning about Access Control
49
FIGURE 3.5 Some useful derived rules
Conjunction
œï1
œï2
œï1 ‚àßœï2
SimpliÔ¨Åcation (1)
œï1 ‚àßœï2
œï1
SimpliÔ¨Åcation (2)
œï1 ‚àßœï2
œï2
Disjunction (1)
œï1
œï1 ‚à®œï2
Disjunction (2)
œï2
œï1 ‚à®œï2
Modus Tollens
œï1 ‚äÉœï2
¬¨œï2
¬¨œï1
Double negation
¬¨¬¨œï
œï
Disjunctive
Syllogism
œï1 ‚à®œï2
¬¨œï1
œï2
Hypothetical
Syllogism
œï1 ‚äÉœï2
œï2 ‚äÉœï3
œï1 ‚äÉœï3
Controls
P controls œï
P says œï
œï
Derived
Speaks For
P ‚áíQ
P says œï
Q says œï
Derived
Controls
P ‚áíQ
Q controls œï
P controls œï
Says
SimpliÔ¨Åcation (1)
P says (œï1 ‚àßœï2)
P says œï1
Says
SimpliÔ¨Åcation (2)
P says (œï1 ‚àßœï2)
P says œï2
FIGURE 3.6 A formal proof of Conjunction
1. œï1
Assumption
2. œï2
Assumption
3. œï1 ‚äÉ(œï2 ‚äÉ(œï1 ‚àßœï2))
Taut
4. œï2 ‚äÉ(œï1 ‚àßœï2)
1,3 Modus Ponens
5. œï1 ‚àßœï2
2,4 Modus Ponens

50
Access Control, Security, and Trust: A Logical Approach
P says œï1
P ‚áíQ
Q says (œï2 ‚äÉœï1)
.
Exercise 3.2.7
Give a formal proof for the following derivable inference rule:

P says (Q controls œï)
P | Q says œï
P says œï
.
Exercise 3.2.8
Give a formal proof for the following derivable inference rule:

(P | Q) controls œï
R ‚áíQ
P says R says œï
œï
.
3.3
Soundness of Logical Rules
The rules presented in Section 3.1 are merely rules for manipulating formulas. Up
to this point, we have no reason to believe in their logical consistency, which makes
using them a risky prospect. To show that the inference rules are consistent, we must
Ô¨Årst use the Kripke-structure semantics to prove their soundness.
SpeciÔ¨Åcally, an inference rule
H1
¬∑¬∑¬∑
Hk
C
is sound with respect to the Kripke semantics provided that, whenever a Kripke struc-
ture M satisÔ¨Åes all of the rule‚Äôs hypotheses (i.e., H1,...Hk), it also satisÔ¨Åes the rule‚Äôs
consequent. It follows that an inference rule is not sound if there exists even just a
single Kripke structure M = ‚ü®W,I,J‚ü©such that, for each Hi, M |= Hi and yet M Ã∏|=C.
We now use the Kripke semantics given in Figure 2.1 to prove the soundness of
the inference rules Taut, Modus Ponens, and Says; we also provide a proof sketch for
the soundness of Equivalence. The proofs of soundness for the remaining rules are
left as exercises for the reader.
Example 3.1 (Soundness of Taut)
Let œï be an instance of a propositional-logic tautology œà. We need to show that, for
every Kripke structure M = ‚ü®W,I,J‚ü©, œï is true in every world in W (i.e., EM [[œï]] =
W).
Let Ma = ‚ü®Wa,Ia,Ja‚ü©be an arbitrary Kripke structure. Because œï is an instance of
œà, there exist propositional variables p1,..., pk and modal formulas œï1,...,œïk such
that œï is obtained from œà by replacing each pi by œïi. Without loss of generality, we
assume that the variables p1,..., pk do not appear in œï.
We now construct a new model M ‚Ä≤ = ‚ü®Wa,I‚Ä≤,Ja‚ü©, where I‚Ä≤ is deÔ¨Åned as follows:
for each pi (1 ‚â§i ‚â§k), I‚Ä≤(pi) = EMa[[œïi]]; for all other propositional variables q,
I‚Ä≤(q) = I(q). Thus, for each 1 ‚â§i ‚â§k,
EM ‚Ä≤[[pi]] = EMa[[œïi]];

Reasoning about Access Control
51
likewise, for all other propositional variables q,
EM ‚Ä≤[[q]] = EMa[[q]].
From these facts, it is straightforward to show by induction on the structure of œà that
EM ‚Ä≤[[œà]] = EMa[[œï]].
Because œà is a propositional-logic tautology, œà is true in all worlds, independent of
the interpretation of its propositional variables. Therefore, EM ‚Ä≤[[œà]] = Wa, and thus
EMa[[œï]] = Wa as well.
Because Ma was arbitrary, we have shown that the Taut rule is sound.
‚ô¶
Example 3.2 (Soundness of Modus Ponens)
To prove that the Modus Ponens inference rule is sound, we must prove the following,
for all formulas œï and œï‚Ä≤, and for all Kripke structures M = ‚ü®W,I,J‚ü©:
If M |= œï and M |= œï ‚äÉœï‚Ä≤, then M |= œï‚Ä≤.
That is, we need to show that, whenever EM [[œï]] = W and EM [[œï ‚äÉœï‚Ä≤]] = W, it is
also the case that EM [[œï‚Ä≤]] = W.
Therefore, we consider an arbitrary model Ma = ‚ü®Wa,Ia,Ja‚ü©for which EMa[[œï]] =
Wa and EMa[[œï ‚äÉœï‚Ä≤]] =Wa, and we will show that EMa[[œï‚Ä≤]] =Wa necessarily follows.
Working straight from the deÔ¨Ånition of the evaluation functions EM [[‚àí]] given in
Figure 2.1, we see that
EMa[[œï ‚äÉœï‚Ä≤]] = (Wa ‚àíEMa[[œï]])‚à™EMa[[œï‚Ä≤]]
= (Wa ‚àíWa)‚à™EMa[[œï‚Ä≤]]
= EMa[[œï‚Ä≤]].
Thus, EMa[[œï‚Ä≤]] = EMa[[œï ‚äÉœï‚Ä≤]] = Wa. Because Ma was arbitrary, we have shown
that the Modus Ponens rule is sound.
‚ô¶
Example 3.3 (Soundness of Says)
To prove that the inference rule Says is sound, we must prove the following, for all
Kripke structures M = ‚ü®W,I,J‚ü©:
If M |= œï, then (for all principals P) M |= P says œï.
That is, we need to show that whenever EM [[œï]] = W, it is also the case that (for
every principal P) EM [[P says œï]] = W.

52
Access Control, Security, and Trust: A Logical Approach
As before, we start by considering an arbitrary Kripke structure Ma = ‚ü®Wa,Ia,Ja‚ü©
that satisÔ¨Åes the formula œï (i.e., for which EMa[[œï]] = Wa). We also let Q be an
arbitrary principal. From the semantic deÔ¨Ånitions of Figure 2.1, we see that
EMa[[Q says œï]] = {w | Ja(Q)(w) ‚äÜEMa[[œï]]} = {w | Ja(Q)(w) ‚äÜWa}.
Because Ja(Q) is by deÔ¨Ånition a subset of Wa √óWa, every w‚Ä≤ ‚ààWa satisÔ¨Åes the con-
straint that Ja(Q)(w‚Ä≤) ‚äÜWa, and thus EMa[[Q says œï]] = Wa. Because both Q and Ma
were arbitrary, we have shown that the Says inference rule is sound.
‚ô¶
Example 3.4 (Soundness of Equivalence)
To prove that the inference rule Equivalence is sound, we must prove the follow-
ing, for all formulas œï1,œï2,œà, propositional variables q, and Kripke structures M =
‚ü®W,I,J‚ü©:
If M |= œï1 ‚â°œï2 and M |= œà[œï1/q], then M |= œà[œï2/q].
That is, we need to show that whenever EM [[œï1 ‚â°œï2]] =W and EM [[œà[œï1/q]]] =W,
it is also the case that EM [[œà[œï2/q]]] = W.
The proof proceeds by a straightforward induction on the structure of œà, using the
fact from Exercise 2.3.9 that EM [[œï1]] = EM [[œï2]] whenever EM [[œï1 ‚â°œï2]] = W. ‚ô¶
Exercise 3.3.1
Prove the soundness of the Speaks For inference rule.
Exercise 3.3.2
Prove the soundness of the & Says inference rule. (Hint: Exer-
cise 2.3.9 is useful here.)
Exercise 3.3.3
Prove the soundness of the Quoting inference rule. (Hint: Exer-
cise 2.3.9 is useful here.)
Exercise 3.3.4
Prove the soundness of the Transitivity of ‚áíinference rule.
Exercise 3.3.5
Prove the soundness of the Monotonicity of ‚áíinference rule.
Exercise 3.3.6
Consider the following formula, which intuitively states that every
principal has the jurisdiction to select its own proxies:
(P says (Q ‚áíP)) ‚äÉ(Q ‚áíP).
Prove that this formula would not make for a sound axiom. That is, Ô¨Ånd a partic-
ular Kripke structure M = ‚ü®W,I,J‚ü©such that
M Ã∏|= (P says (Q ‚áíP)) ‚äÉ(Q ‚áíP).

Reasoning about Access Control
53
Exercise 3.3.7
Consider the following formula, which intuitively states that controls
distributes over conjunction:
(P controls (œï1 ‚àßœï2)) ‚â°((P controls œï1)‚àß(P controls œï2)).
Prove that this formula would not make for a sound axiom. That is, Ô¨Ånd particular
formulas œï1,œï2 and a particular Kripke structure M = ‚ü®W,I,J‚ü©such that
M Ã∏|= (P controls (œï1 ‚àßœï2)) ‚â°((P controls œï1)‚àß(P controls œï2)).
Exercise 3.3.8
Consider the following plausible inference rule:
P & Q controls œï
P says œï
Q controls œï
.
Determine whether or not this rule is sound, and justify your answer:
‚Ä¢ If you determine that the rule is sound, prove its soundness.
‚Ä¢ If you determine that the rule is not sound, give (and explain) a particular
Kripke structure, principals P and Q, and formula œï that demonstrate the lack
of soundness.
Exercise 3.3.9
Consider the following plausible inference rule:
P ‚áíR
R ‚áíQ
R says œï ‚â°P & Q says œï.
Determine whether or not this rule is sound, and justify your answer:
‚Ä¢ If you determine that the rule is sound, prove its soundness.
‚Ä¢ If you determine that the rule is not sound, give (and explain) a particular
Kripke structure, principals P,Q,R and formula œï that demonstrate the lack of
soundness.
Exercise 3.3.10
Consider the following plausible inference rule:
P ‚áíQ
Q ‚áíP
P says œï ‚â°Q says œï.
Determine whether or not this rule is sound, and justify your answer:
‚Ä¢ If you determine that the rule is sound, prove its soundness.
‚Ä¢ If you determine that the rule is not sound, give (and explain) a particular
Kripke structure, principals P,Q,R and formula œï that demonstrate the lack of
soundness.

54
Access Control, Security, and Trust: A Logical Approach
Exercise 3.3.11
Consider the following plausible inference rule:
P controls (œï1 ‚àßœï2)
P controls œï1
.
Determine whether or not this rule is sound, and justify your answer:
‚Ä¢ If you determine that the rule is sound, prove its soundness.
‚Ä¢ If you determine that the rule is not sound, give (and explain) a particular
Kripke structure, principals P,Q,R and formula œï that demonstrate the lack of
soundness.
3.4
Summary
As seen in the previous chapter, Kripke structures provide precise meanings for
the statements of the access-control logic. However, reasoning at the level of Kripke
structures is extremely cumbersome. Furthermore, it is not always possible to know
which Kripke structures accurately represent a particular situation.
To avoid the necessity of reasoning about particular structures, we introduced a
collection of inference rules that are sound. The Kripke-structure semantics provide
the basis for guaranteeing this soundness. The beneÔ¨Åts of reasoning with sound
inference rules include the convenience they provide as well as the guarantee they
provide: any situation that satisÔ¨Åes the initial assumptions is guaranteed to satisfy
the deduced consequences. We therefore have a basis for answering the question
‚ÄúShould this access-control request be granted?‚Äù, as well as an explicit accounting
for all assumptions on which the analysis depends.
The learning outcomes associated with this chapter appear in Figure 3.7.
3.5
Further Reading
The inference rules of the access-control logic are based on the semantics pre-
sented in the previous chapter. Our semantics is similar to the logic of Abadi, Lamp-
son, and colleagues (Lampson et al., 1992; Abadi et al., 1993). The notion of sound-
ness based on Kripke models is part of standard modal logic. The reader can consult
Hughes and Cresswell (Hughes and Cresswell, 1996) for more on this subject.

Reasoning about Access Control
55
FIGURE 3.7 Learning outcomes for Chapter 3
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Analysis
‚Ä¢ When given a set of assumptions and a goal that logically follows from
those assumptions, you should be able to give a formal proof of that goal.
Synthesis
‚Ä¢ When given a sound axiom or inference rule for the access-control logic,
you should be able to prove its soundness in the underlying Kripke
model.
‚Ä¢ When given a potential inference rule for the access-control logic that is
not sound, you should be able to construct a Kripke structure that demon-
strates its lack of soundness.
Evaluation
‚Ä¢ When given a theory, inference rules, and a proof, you should be able to
judge if the proof is correct.
‚Ä¢ When given a proposed inference rule, you should be able to judge
whether or not it is sound.


Chapter 4
Basic Concepts
In the preceding two chapters, we introduced a modal logic that we will use through-
out this book to describe and reason about various aspects of access control. This
access-control logic and its collection of inference rules provide a rigorous and for-
mal basis for answering the question ‚ÄúShould this request be granted?‚Äù
In learning any language for the Ô¨Årst time, it‚Äôs important to start using the lan-
guage to describe objects and concepts of interest to us. We begin this chapter by
introducing the concept of a reference monitor, which provides a context for the re-
mainder of this chapter. We then develop the fundamental access-control concepts of
tickets, capabilities, and access-control lists. We also discuss various authentication
mechanisms and how they relate to access control. Throughout this chapter, there
are numerous examples to illustrate how the access-control logic is used.
4.1
Reference Monitors
Access control is about guarding objects, assets, resources, services, communi-
cations, Ô¨Åles, databases, information, or people. In everyday life, physical locks of
various forms guard against unauthorized access to all sorts of resources, ranging
from house doors and high-school lockers to automobile ignitions and safety-deposit
boxes. In other situations, a person or machine may require us to provide an artifact
(e.g., subway token or bus pass) to gain access to particular resources. For example,
we need a boarding pass to get on a bus, train, or airplane. We need to have a ticket
before we can get in to see a movie. If we are invited to a formal dinner such as
a wedding reception, there may be a guest list and name cards at each seat in the
banquet hall.
In computer and information systems, the guards are called reference monitors,
and they protect resources such as memory, Ô¨Åles, databases, programs, and commu-
nication channels. Figure 4.1 provides an abstract view of how a reference monitor
Ô¨Åts into a system. Initially, a principal‚Äîeither a person, machine, or process‚Äîmakes
a request to the reference monitor to perform some operation on a protected object.
Principals that make requests are often referred to as subjects, and the right to per-
form an operation on an object is typically called an access right or privilege. The
reference monitor‚Äôs decision regarding whether or not to grant the request depends on
57

58
Access Control, Security, and Trust: A Logical Approach
FIGURE 4.1 Abstract view of reference monitors
several factors: what is being asked for, potentially the identity and other attributes
of the subject making the request, and Ô¨Ånally the access policy. When a decision
is made, it is recorded in an audit log where a record of all the access requests and
decisions are kept.
All reference monitors must satisfy three essential properties:
1. Completeness: The reference monitor cannot be bypassed.
Completeness means that there is no way to access a protected resource with-
out coming under the scrutiny of the guard or reference monitor. Physically,
completeness means that there is no ‚Äúhole in the fence.‚Äù In information sys-
tems, completeness means that all paths to a state in which access is granted
go through an access controller.
2. Isolation: The reference monitor is tamper proof.
Isolation is primarily satisÔ¨Åed by physical protection mechanisms. For ex-
ample, machines on which sensitive information is stored must be housed in
locked and guarded machine rooms where access to the machine rooms is con-
stantly watched.
3. VeriÔ¨Åable: The reference monitor is correctly implemented.
The property of being veriÔ¨Åed correct means that we must have fully and pre-
cisely speciÔ¨Åed what the reference monitor is supposed to do.
Conceptually, the desired access-control behavior of a system can be easily rep-
resented by an access-control matrix, as introduced by Lampson (Lampson, 1971).
The rows of an access-control matrix M are labeled by possible subjects (i.e., princi-
pals), while the columns are labeled by the protected objects. Each entry Ms,o in the
matrix indicates the access rights that subject s possesses with respect to the object
o. As an example, Table 4.1 contains a simple access-control matrix for a system
involving three subjects (with identiÔ¨Åers alice, bob, carol) and four protected
objects (Ô¨Åles numbered 1 through 4). According to this table, the principal alice
has the rights to read Ô¨Åle1, to read or write Ô¨Åle2, and to execute Ô¨Åle4; alice has no

Basic Concepts
59
Ô¨Åle1
Ô¨Åle2
Ô¨Åle3
Ô¨Åle4
alice
read
read, write
execute
bob
read
execute
carol
read, write
execute
execute
Table 4.1: Example of an access-control matrix
access rights for Ô¨Åle3. In contrast, bob has no access rights for Ô¨Åle1 or Ô¨Åle4, but can
read Ô¨Åle2 and execute Ô¨Åle3.
When assigning rights to principals, it is wise to follow the principle of least priv-
ilege (Saltzer and Schroeder, 1975): principals should be assigned only the rights
necessary for completing their tasks and no more.
Access-control matrices provide a very simple and straightforward way to de-
scribe systems, particularly those that have relatively small numbers of subjects and
objects. However, they alone are insufÔ¨Åcient for reasoning about access control in
more complex systems that involve large numbers of subjects and objects or that per-
mit proxies and delegation. In particular, they do not address issues related to how
subjects are authenticated or the trust assumptions that underlie the authentication
process.
The purpose of the access-control logic is to bridge this gap, to help us to deduce
and to specify what a reference monitor‚Äôs behavior should be for any given policy
and request. From a logical point of view, a reference monitor decides whether or
not to approve an access request based on the inference rules of the access-control
logic and within the context of an access policy and trust assumptions.
As we have already seen, we can use the logic to describe a subject‚Äôs request to
access a particular object as follows:
Subject says ‚ü®access right, object‚ü©.
To grant the request, the reference monitor must be able to determine that the access
policy authorizes the subject to perform the requested operation. In terms of the
logic, this determination amounts to using the inference rules of the logic to derive
the atomic proposition
‚ü®access right, object‚ü©
from assumptions about the given policy and any additional trust assumptions that
are relevant to the situation.
Policy statements state who or what has jurisdiction over certain statements, as
well as who or what is authorized with certain rights. These statements usually take
the form
P controls œï,
where P is the authority and œï is the statement over which P has jurisdiction.
Trust assumptions are usually assumptions about proxies or symbols of authority,
such as the queen‚Äôs seal, a principal‚Äôs public key, or tickets issued by an airline.

60
Access Control, Security, and Trust: A Logical Approach
These statements usually have the form
P ‚áíQ,
where P is the recognized proxy for principal Q. As we will see in the subsequent
sections, these statements express explicitly the reference monitor‚Äôs reliance on the
unforgeability of certiÔ¨Åcates and credentials.
A certiÔ¨Åcate is a signed statement made by a recognized authority, such as a
birth certiÔ¨Åcate (signed perhaps by a town‚Äôs health department) or a driver‚Äôs license
(signed by a state department of motor vehicles). A credential is a certiÔ¨Åcate that
asserts a particular principal‚Äôs authorization to perform some action; for example, a
driver‚Äôs license asserts a given driver‚Äôs authorization to drive a certain class of vehi-
cle.
The next section develops the speciÔ¨Åcs of requests, policy statements, and trust
assumptions within the context of tickets and access-control lists. Subsequent sec-
tions introduce the notions of certiÔ¨Åcates and credentials, particularly as they apply
to both identity-based and group-based authentication.
4.2
Access-Control Mechanisms: Tickets and Lists
As we have already seen, reference monitors guard those objects deemed neces-
sary to protect. There are two common protection schemes employed by reference
monitors: one is based on tickets (or capabilities), while the other depends on access-
control lists.
Tickets signify a capability to access a resource. For example, a ticket to a movie
theater grants the holder of a theater ticket access to a seat in a particular theater
at a particular time. Tickets are presumed to be unforgeable‚Äîonly the appropriate
controlling authorities are able to issue tickets. In ticket-oriented protection schemes,
principals possess tickets for the resources or objects to which they have access.
Principals gain access to protected resources or objects by presenting the appropriate
ticket to the reference monitors protecting those resources.
In list-oriented protection schemes, an access-control list has the names of princi-
pals who are allowed to access the resource being protected. An example application
of access-control lists is a restaurant where patrons are seated at dining tables only if
they have a reservation. The access-control list itself must also be protected: that is,
only certain principals are recognized to have the authority to adjust whose names
appear on the access-control list.
In this section, we formalize the concepts of tickets and access-control lists using
the access-control logic.

Basic Concepts
61
4.2.1
Tickets
Tickets are a common means to control access to protected objects. For example,
when airline passengers prepare to board an airplane, the gate agents check each pas-
senger‚Äôs ticket or boarding pass to see if the person presenting the ticket is authorized
to board the airplane. The gate agents are the reference monitors guarding access to
the airplane‚Äôs doors. The boarding pass signiÔ¨Åes the capability to access a particular
Ô¨Çight. Ideally, it is an unforgeable document issued by the airline (i.e., the controlling
authority) granting permission to board a particular airplane at a particular time.
In general, ticket-oriented access control requires the following four components,
which we express as statements in the logic:
1. The access policy:
authority controls (subject controls ‚ü®access right, object‚ü©)
The policy statement asserts that the controlling authority has jurisdiction over
which subjects can exercise an access right on an object.
2. The ticket:
ticket says (subject controls ‚ü®access right, object‚ü©)
A ticket is a credential stating that the subject has the right to access a partic-
ular object.
3. Additional trust assumption:
ticket ‚áíauthority
This assumption states that tickets are tokens of authority issued by the con-
trolling authority. As such, it is imperative that these tokens or tickets be
unforgeable.
4. The access request:
subject says ‚ü®access right, object‚ü©
The access request occurs when the subject actually presents the ticket. For
example, an airline passenger presenting a ticket is requesting to sit at her
assigned seat on the designated Ô¨Çight. Of course, a subject may request access
without actually presenting a ticket, but the reference monitor should not grant
access in that case. For example, a person in the air terminal who tries to board
an airplane without presenting a valid ticket should not be allowed to do so.
Together, these four components provide sufÔ¨Åcient information for the reference
monitor to determine that the requested action ‚ü®access right, object‚ü©should be per-
mitted. If we generalize the atomic proposition ‚ü®access right, object‚ü©to an arbitrary

62
Access Control, Security, and Trust: A Logical Approach
FIGURE 4.2 Formal proof of the Ticket Rule
1. subject says œï
Access request
2. authority controls (subject controls œï)
Access policy
3. ticket ‚áíauthority
Trust assumption
4. ticket says (subject controls œï)
Ticket
5. authority says (subject controls œï)
3, 4 speaks for
6. subject controls œï
2, 5 controls
7. œï
6, 1 controls
statement œï, we can derive the following useful inference rule:
Ticket
subject says œï
authority controls (subject controls œï)
ticket ‚áíauthority
ticket says (subject controls œï)
œï
.
The formal proof of this rule appears in Figure 4.2. The next example illustrates how
the Ticket Rule applies to the airline-ticket scenario.
Example 4.1
Suppose Tina has an airplane ticket assigning her to seat 25D on Smooth Air Flight
#1. When her row is called, she presents her ticket to the gate agents for Ô¨Çight #1,
and she is granted access.
The following analysis justiÔ¨Åes the decision by the gate agents to let Tina board
the airplane:
1. Tina says ‚ü®seat 25D, Ô¨Çight #1‚ü©
Tina‚Äôs request
2. Smooth Air controls (Tina controls ‚ü®seat 25D, Ô¨Çight #1‚ü©)
Access policy
3. ticket ‚áíSmooth Air
Trust assumption
4. ticket says (Tina controls ‚ü®seat 25D, Ô¨Çight #1‚ü©)
Tina‚Äôs ticket
5. ‚ü®seat 25D, Ô¨Çight #1‚ü©
1, 2, 3, 4 ticket rule
Line 1 represents Tina‚Äôs request to board Ô¨Çight #1 and sit in seat 25D, which hap-
pens when Tina walks up to the gate agent and presents her ticket. Line 2 indicates
that the gate agent recognizes Smooth Air‚Äôs authority over which subjects can access
Ô¨Çight #1. Line 3 states that tickets represent the authority of Smooth Air; that is, the
gate agent assumes that Smooth Air will issue tickets only to people who should be
passengers on Ô¨Çight #1. Implicit in this line is that the gate agent recognizes the ticket
as being a valid ticket issued by Smooth Air. For example, if Tina were to present a
piece of notebook paper that has her name and a seat assignment scribbled on it, the
gate agent would not accept that paper as representing the authority of Smooth Air.
Line 4 describes what the ticket says, namely that Tina is assigned to seat 25D
on Ô¨Çight #1. Line 5‚Äîwhich is obtained from the previous lines through the Ticket
Rule‚Äîdemonstrates that the gate agent‚Äôs decision to allow Tina to board Ô¨Çight #1 is
justiÔ¨Åed, given the previous conditions.
‚ô¶

Basic Concepts
63
In the preceding example, the boarding pass really is a ticket in the technical sense:
the gate agent checks only the validity of the boarding pass and does not conÔ¨Årm
Tina‚Äôs identity. Most likely, anyone who handed Tina‚Äôs valid boarding pass to the
gate agent would be allowed on the plane, regardless of their actual identity.
Exercise 4.2.1
Suppose a theater ticket is sold by the box ofÔ¨Åce to a patron to

see Gone with the Wind in Theater 5 at 7:30 p.m. Using the access-control logic,
describe the patron‚Äôs request, the access-control policy of the theater, the trust as-
sumptions, and movie ticket. Based on your descriptions, formally justify admitting
the patron to see the movie.
4.2.2
Lists
Another widely used access-control mechanism is a list of principals with their ac-
cess rights to protected objects. These lists are known as access-control lists (ACLs).
In access-control list schemes, reference monitors possess the list of authorized
users and their privileges. Principals identify themselves and make their requests to
access the protected resources. The reference monitor compares the request to the
access-control list: if there is a match, then access is granted. Unlike ticket-oriented
access control, principals do not possess a credential that says they have a right to
access a protected resource.
ACLs are a very common protection mechanism. For example, if a scientist wants
to visit a secure government or commercial facility, she must call ahead to ensure
that the guards at the gates of the facility have her name on a visitor‚Äôs list. When she
arrives at the gate, she identiÔ¨Åes herself to the guard, who checks her identity and
looks to see if she is on the guest list. If her name is on the list, then she is allowed
into the facility. A more benign but common example is a wedding or formal party
where a greeter (acting as the reference monitor) checks off guest names on a guest
list.
In general, protection schemes involving access-control lists require the following
components:
1. Access policy:
authority controls (subject controls ‚ü®access right, object‚ü©)
The guard or reference monitor recognizes the jurisdiction of some controlling
authority to decide who is allowed what access to the protected object.
2. Access-control list:
ACL says
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
subject1 controls ‚ü®access right1,object1‚ü©‚àß
subject2 controls ‚ü®access right2,object2‚ü©‚àß
¬∑¬∑¬∑‚àß
subjectn controls ‚ü®access rightn,objectn‚ü©

64
Access Control, Security, and Trust: A Logical Approach
An ACL is essentially a listing of subjects and their access rights.
3. Trust assumption:
ACL ‚áíauthority
This assumption states that the ACL always reÔ¨Çects the wishes of the authority.
As is the case with tickets, it is imperative that the ACL‚Äôs integrity be assured.
4. Access request:
subject says ‚ü®access right, object‚ü©
The request occurs when the subject presents herself to the guard (who can
establish her identity, as necessary) and makes the request.
With the exception of the form of the access-control list, these components are the
same as those for tickets. Authorities determine who has access, and trust assump-
tions are needed to interpret statements from authority. It is within this context that
subjects‚Äô requests are considered.
The only difference is in the statements directly associated with the ticket and
with the access-control list. A ticket associates a speciÔ¨Åc subject with a speciÔ¨Åc
authorization:
ticket says (subjecti controls œïi)
In contrast, an access-control list associates many subjects with many authoriza-
tions, having the following general form:
ACL says
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
subject1 controls œï1 ‚àß
¬∑¬∑¬∑
subjecti controls œïi ‚àß
¬∑¬∑¬∑
However, it is possible to extract from an access-control list the relevant autho-
rization, using the Says SimpliÔ¨Åcation rules introduced in Exercise 3.2.5:
P says (œï1 ‚àßœï2)
P says œï1
P says (œï1 ‚àßœï2)
P says œï2
Using a combination of these two rules, we can extract (for any i)
ACL says (subjecti controls œïi)
from the access-control list description.
It is therefore possible to use the Ticket Rule to also reason about access-control
lists. The following example is an illustration.

Basic Concepts
65
Example 4.2
Suppose Erika is invited to a dinner meeting where delicate negotiations are to be
held. Because of the sensitive nature of the talks, the meeting is being held in a
private dining room. At the door of the dining room is a maÀÜƒ±tre d‚Äô with a guest list of
all the attendees. The guest list, which was authorized by the restaurant‚Äôs manager,
has three names: Erika, Darnell, and Gina.
To gain entry to the dining room, Erika identiÔ¨Åes herself to the maÀÜƒ±tre d‚Äô and he lets
her in. For simplicity, we will assume that the maÀÜƒ±tre d‚Äô recognizes Erika, Darnell,
and Gina by sight, so that no further identiÔ¨Åcation is needed. (We will consider more
realistic and complicated situations later in this chapter.). The following analysis
justiÔ¨Åes the decision of the maÀÜƒ±tre d‚Äô to let Erika into the private dining room:
1. Erika says ‚ü®enter, dining room‚ü©
Erika‚Äôs request
2. Manager controls (Erika controls ‚ü®enter, dining room‚ü©)
Access policy
3. ACL ‚áíManager
Trust assumption
4. ACL says
Ô£±
Ô£¥
Ô£≤
Ô£¥
Ô£≥
Erika controls ‚ü®enter, dining room‚ü©‚àß
Darnell controls ‚ü®enter, dining room‚ü©‚àß
Gina controls ‚ü®enter, dining room‚ü©
Guest List
5. ACL says (Erika controls ‚ü®enter, dining room‚ü©)
4 simplify says
6. ‚ü®enter, dining room‚ü©
1, 2, 3, 5 ticket rule
Line 1 represents Erika‚Äôs spoken request to be let into the private dining room.
Line 2 reÔ¨Çects the restaurant‚Äôs policy that the manager in charge has authority over
who can enter the private dining room. Line 3 represents the maÀÜƒ±tre d‚Äôs belief that the
guest list is authorized by the manager and accurately reÔ¨Çects the manager‚Äôs wishes.
Line 4 describes the guest list‚Äîa list of subjects who can exercise entry rights to the
private dining room. Line 5 is obtained by repeated application of the Simplify Says
inference rule to Line 4, and Line 6 is obtained by the Ticket Rule.
‚ô¶
When expressing ACLs in the logic, it is important to remember that the two
formulas
(P controls œï1) ‚àß(P controls œï2)
and
P controls (œï1 ‚àßœï2)
are not equivalent (see Exercise 3.3.7). In the former case, P is trusted on each of œï1
and œï2; P‚Äôs request to perform either should be granted. In the latter case, however,
P is trusted on the conjunction of œï1 and œï2: P must request both in order for either
to be granted. For this reason, an ACL will often contain many separate clauses for
the same principal, detailing all of the individual access rights that principal has.
Similarly, the two formulas
(P controls œï) ‚àß(Q controls œï)
and
P & Q controls œï

66
Access Control, Security, and Trust: A Logical Approach
are not equivalent. The Ô¨Årst formula states that both P and Q are individually autho-
rized for œï. In contrast, the second formula states that both P and Q must request
œï for access to be granted. For instance, in Example 4.2, Erika, Darnell, and Gina
are each authorized to enter the private dining room, and each may enter the room
without the others. Suppose instead that the ACL were deÔ¨Åned in the following way:
ACL says Erika & Darnell & Gina controls ‚ü®enter, dining room‚ü©.
This case would correspond to restaurants that seat parties only after everyone has
arrived: Erika, Darnell, and Gina could enter the dining room only if all three make
the request to do so.
Exercise 4.2.2
In operating systems with discretionary access control, a user can
specify who else can read, write, or execute her Ô¨Åles. Suppose Carter creates a
program foo, and he wants Dan to be able to execute foo but neither read foo nor
write to it. Also, Carter wishes to grant read, write, and execute privileges to Jan.
Assume that Dan and Jan have userids dan and jan, respectively. Formalize the
above description and formally justify why Jan‚Äôs request to execute foo should be
granted.
Exercise 4.2.3
Prove that the formulas
P & Q controls œï,
(P controls œï) ‚àß(Q controls œï)
are not equivalent. That is, Ô¨Ånd a particular formula œï and a particular Kripke
structure M = ‚ü®W,I,J‚ü©such that
M Ã∏|= (P & Q controls œï) ‚â°((P controls œï) ‚àß(Q controls œï)).
4.2.3
Logical and Pragmatic Implications
We have looked in detail at ticket-oriented and list-oriented mechanisms for con-
trolling access to protected objects. As we have seen, tickets and lists differ in the
following two ways:
1. Tickets are typically possessed‚Äîor easily accessible‚Äîby principals wishing
to access a protected object. In contrast, ACLs are possessed by reference
monitors.
2. Typically, when tickets are used, the reference monitor does not need to iden-
tify the principal making the request. Instead, the principal possessing the
ticket needs only give it to the reference monitor to exercise the rights corre-
sponding to the ticket. In contrast, a reference monitor using an ACL must
somehow determine the identity of the principal requesting access to deter-
mine what rights, if any, the principal has.
Despite these differences, tickets and ACLs share similar logical meanings and
logical contexts:

Basic Concepts
67
1. Logically speaking, tickets and ACLs make statements of the same form:
ticket says (subject controls œï)
ACL says (subject controls œï)
In the case of tickets, the subject may simply be Bearer, in that anyone bearing
or possessing the ticket has the corresponding access right. In the case of
ACLs, speciÔ¨Åc principals are generally identiÔ¨Åed.
2. Both tickets and ACLs are used in the context of policies that specify which
authority has jurisdiction over access decisions. These policies regarding ju-
risdiction are expressed in the logic as follows:
authority controls (subject controls œï)
3. Both tickets and ACLs are the mechanisms for controlling access and thus
must be protected from fraud. As a result, both must faithfully represent the
desires of the authorities they represent. Statements regarding this faithful
representation of the authority are expressed in the logic as follows:
ticket ‚áíauthority
ACL ‚áíauthority
From a logical standpoint, it is no accident that reference monitors depend upon
the same inference rules to deduce whether an access request should be granted.
Although ticket-based access may appear on the surface to be very different from
list-based access, the information conveyed by tickets and lists is the same, as is the
larger context of policy statements and trust assumptions.
There is an important pragmatic consideration regarding the use of the derived
Ticket Rule in our examples. Recall that the ticket rule is as follows:
Ticket Rule
subject says œï
authority controls (subject controls œï)
ticket ‚áíauthority
ticket says (subject controls œï)
œï
From an implementation standpoint, an engineer designing a reference monitor
will not implement a general purpose inference engine or theorem prover based on
the inference rules of the access-control logic. Instead, what he or she will build
amounts to a mechanization of a derived inference rule speciÔ¨Åc to his or her situation.
In particular, many reference monitors are essentially implemented as checklists.
They determine whether the various necessary tickets and certiÔ¨Åcates are present; if
so, access is granted. Such approaches can be justiÔ¨Åed by derived inference rules
where the elements on the checklist are the premises and the desired action on an
object is the conclusion. Furthermore, there is usually a speciÔ¨Åc context within
which the reference monitor operates. These contexts will sometimes provide im-
plicit premises, rather than explicit checklist items.

68
Access Control, Security, and Trust: A Logical Approach
For example, suppose that Margaret is an engineer building a reference monitor
for a copy machine. The intention is that, if a person has a ticket‚Äîperhaps a card he
inserts into the copy machine‚Äîthen he can make copies. In the actual implementa-
tion, Margaret‚Äôs chief concern is to develop a way to check the validity of an inserted
copy card; if it is valid, then a copy operation should be performed whenever the
copy button is pressed. The veriÔ¨Åed validity of the copy card amounts to a statement
of the form
card says (bearer controls copy),
while each press of the copy button amounts to the following statement:
bearer says copy.
The Ticket rule allows Margaret to justify her design formally, but only by explicitly
stating the implicit assumptions about what card validity means. SpeciÔ¨Åcally, the
analysis requires her to acknowledge an authority that governs who can make copies
and that the copy card must speak for that authority:
auth controls (bearer controls copy),
card ‚áíauth.
The value of such an analysis is that potentially implicit assumptions are made
explicit. If there are future changes to the operating context (e.g., the card-making
equipment is stolen or compromised), then it is easier to determine whether or not
systems need to be reconÔ¨Ågured or even redesigned.
4.3
Authentication
Authentication is the task of identifying a subject who is making a request. More
abstractly, authentication is the task of associating one principal (e.g., the process
requesting access to a guarded resource or the face and person requesting access to a
guarded facility) with another principal (e.g., an individual or group included on an
ACL).
In this section we take a detailed look at several authentication scenarios and de-
scribe them in the access-control logic.
4.3.1
Two-Factor Authentication
Authentication is generally based on one or more kinds of information or factors:
‚Ä¢ Type 1: something you know, such as a PIN, or password.
‚Ä¢ Type 2: something you have, such as card, key, or token.
‚Ä¢ Type 3: something you are, such as your Ô¨Ångerprint, face, or voice.

Basic Concepts
69
Authentication can be performed using one or more items of information drawn
from each factor. Two-factor authentication uses information drawn from two of
the preceding types. An example of two-factor authentication based on information
drawn from types 1 and 2 is identifying a principal based on a personal identiÔ¨Åcation
number PIN (type 1) and a badge or certiÔ¨Åcate (type 2).
Many badges have the principal‚Äôs name and the PIN associated with the principal
in protected form on the badge. A typical security badge might have a person‚Äôs
name, face, and security clearance on the front of the badge, and have the name,
security clearance, and PIN stored in encrypted form accessible by a magnetic card
reader. Most cards have both physical and electronic security. Physical security
measures include the way the card was manufactured: the person‚Äôs picture is part
of the plastic, there is a stamp or signature written over visible information such as
security clearances and names, and so on. Electronic security can include encryption
and signing of electronic information such as names, PINs, and security clearances.
If a smart card is used, key information will be wiped out if physical tampering is
attempted.
For an older picture ID with a magnetic stripe that contains a protected PIN, we
can express the two-factor authentication as follows:
authority says ((facesubject & PINsubject) ‚áísubject).
We recognize that the above formula is exactly the form of a certiÔ¨Åcate (i.e., a
signed statement) issued by an authority: the certiÔ¨Åcate in this case associates a
particular face and PIN with a particular subject or principal. The ID badge is used
by the reference monitor as a certiÔ¨Åcate. Again, it is crucial that badges and other
certiÔ¨Åcates be impossible‚Äîor at least very difÔ¨Åcult‚Äîto counterfeit.
Figure 4.3 contains an analysis of two-factor authentication based on certiÔ¨Åcates.
Line 1 represents the subject using factor1 and factor2 to make a request œï. Line 2
is the access policy that states that authority1 is in charge of who has access. Line 3
corresponds to an access-control list entry stating that subject has access right œï.
Line 4 state that authority2 has jurisdiction over certiÔ¨Åcates identifying principals
using factor1 and factor2. Line 5 is the information contained in the certiÔ¨Åcate or
badge possessed by the subject. Lines 6 through 9 are derived using the inference
rules of the access-control logic.
The two-factor proof of Figure 4.3 corresponds to the following derived inference
rule:
Two Factor Auth
(factor1 & factor2) says œï
authority1 controls (subject controls œï)
authority1 says (subject controls œï)
authority2 controls ((factor1 & factor2) ‚áísubject)
authority2 says ((factor1 & factor2) ‚áísubject)
œï
.
The following example demonstrates the use of this derived rule for reasoning about
access based on two-factor authentication.

70
Access Control, Security, and Trust: A Logical Approach
FIGURE 4.3 Template for two-factor authentication
1. (factor1 & factor2) says œï
Access request
2. authority1 controls (subject controls œï)
Access policy
3. authority1 says (subject controls œï)
ACL entry
4. authority2 controls ((factor1 & factor2) ‚áísubject)
Jurisdiction
5. authority2 says ((factor1 & factor2) ‚áísubject)
CertiÔ¨Åcate
6. (factor1 & factor2) ‚áísubject
4, 5 Controls
7. subject says œï
6, 1 Says
8. subject controls œï
2, 3 Controls
9. œï
8, 7 Controls
Example 4.3
Omar works for a military research lab. The security ofÔ¨Åce has issued Omar a badge
(badgeOmar) and personal identiÔ¨Åcation number (PINOmar). The doors to the lab are
protected by card readers with keypads. To enter the lab, Omar must swipe his badge
in the card reader and then punch in his PIN. If Omar uses his card and punches in
his PIN, then the reference monitor controlling the door is able to identify him, look
him up on an electronic access-control list, and determine that he is an employee in
good standing and should be admitted.
This description corresponds to two-factor authentication. Omar must know his
PIN (a type-1 factor) and then he must possess a badge (a type-2 factor). The com-
bination of the two are used to identify Omar to the electronic reference monitor that
guards the lab doors.
As is the case with many badges, the magnetic stripe on Omar‚Äôs card contains
protected information, such as a badge identiÔ¨Åer and Omar‚Äôs PIN. The card reader
on the doors can identify both the badge identiÔ¨Åer and protected PIN.
The following proof justiÔ¨Åes letting Omar enter the lab when he presents his badge
and PIN.
1. (badgeOmar & PINOmar) says ‚ü®enter, lab‚ü©
Access request
2. security ofÔ¨Åce controls (Omar controls ‚ü®enter, lab‚ü©)
Access policy
3. security ofÔ¨Åce says (Omar controls ‚ü®enter, lab‚ü©)
ACL entry
4. security ofÔ¨Åce controls ((badgeOmar & PINOmar) ‚áíOmar)
Jurisdiction
5. security ofÔ¨Åce says ((badgeOmar & PINOmar) ‚áíOmar)
Info in badge
6. ‚ü®enter, lab‚ü©
Two Factor ACL
The preceding scenario is largely analogous to the use of bank cards and PINs by
automated teller machines. The important difference is that withdrawals also depend
on conditions related to the account balances.
4.3.2
Using Credentials from Other Authorities
So far we have described access scenarios that depend on credentials issued by the
same authority responsible for the protected resource. In many situations, however,

Basic Concepts
71
this need not be the case. For example, airlines routinely use state-issued driver‚Äôs
licenses with pictures and federally issued passports to identify passengers. Guards
at airports use a combination of government-issued documents with pictures (such
as passports and licenses) coupled with airline boarding passes.
The informal justiÔ¨Åcation that allows such credentials to be used is based on trust
in the process by which these credentials are issued. For example, government-
issued picture IDs are accepted based on the assumption that it is hard to dupe the
government into issuing a fake picture ID. That is, the government‚Äôs registration
process for issuing picture IDs is deemed to be sufÔ¨Åciently robust. Likewise, airline
tickets are accepted as credentials because airlines have strong economic interests
to control who can get on their airplanes: it is consequently difÔ¨Åcult to defraud an
airline and obtain a ticket without paying for it (either with money or frequent-Ô¨Çyer
miles).
A typical goal when making access-control decisions based on other credentials
is two-fold: Ô¨Årst, determine whether the subject making the request is who they say
they are, and then determine whether they have access rights to some other object
that is meaningful and relevant to you.
A template for this type of situation appears in Figure 4.4. Lines 1 through 9 are
the initial assumptions. Line 1 describes a subject with a face, voice, Ô¨Ångerprint, or
userid and password, making a request œï1. Lines 2 and 3 are the recognition of the
jurisdiction of authority A1 regarding œï1 and the stated access policy. Note that the
policy expressed in line 3 differs in form from previous examples: it states that, if
the subject has access to the object corresponding to œï2, then the subject also has
access to the object corresponding to œï1. For example, if Penny has permission to
board an airplane leaving from a given airport, then Penny has permission to enter
that airport. Lines 4 through 6 relate to recognizing the jurisdiction of authority A2
on associating a factor with a subject (factor ‚áísubject), the credential itself, and its
assumed integrity. Similarly lines 7 through 9 relate to recognizing the jurisdiction
of A3 to control œï2 and the integrity of the certiÔ¨Åcate authorizing subject to control
œï2.
The proof in Figure 4.4 yields the following derived inference rule:
ID & Ticket
factor says œï1
A1 controls ((subject controls œï2) ‚äÉsubject controls œï1)
A1 says ((subject controls œï2) ‚äÉsubject controls œï1)
A2 controls (factor ‚áísubject)
certiÔ¨Åcate says (factor ‚áísubject)
certiÔ¨Åcate ‚áíA2
A3 controls (subject controls œï2)
ticket says (subject controls œï2)
ticket ‚áíA3
œï1
.
To be explicit, this template illustrates one particular way that credentials from
multiple authorities might Ô¨Åt together as part of an access-control scheme. Not all

72
Access Control, Security, and Trust: A Logical Approach
FIGURE 4.4 Template for using credentials from multiple authorities
1. factor says œï1
Request
2. A1 controls ((subject controls œï2) ‚äÉsubject controls œï1)
Jurisdiction
3. A1 says ((subject controls œï2) ‚äÉsubject controls œï1)
Access policy
4. A2 controls (factor ‚áísubject)
Jurisdiction
5. certiÔ¨Åcate says (factor ‚áísubject)
ID card
6. certiÔ¨Åcate ‚áíA2
Trust assumption
7. A3 controls (subject controls œï2)
Jurisdiction
8. ticket says (subject controls œï2)
Ticket
9. ticket ‚áíA3
Trust assumption
10. subject controls œï2 ‚äÉsubject controls œï1
2, 3 Controls
11. A2 says (factor ‚áísubject)
6, 5 Derived speaks for
12. factor ‚áísubject
4, 11 Controls
13. subject says œï1
12, 1 Derived speaks for
14. A3 says (subject controls œï2)
9, 8 Speaks for
15. subject controls œï2
7, 14 Controls
16. subject controls œï1
15, 10 Modus ponens
17. œï1
16, 13 Controls
situations will necessarily Ô¨Åt into this speciÔ¨Åc template. The following example ap-
plies the ID & Ticket rule to a common situation that does Ô¨Åt this template: the
task of controlling access to an airport terminal where only ticketed passengers with
DMV-issued driver‚Äôs licenses can enter.
Example 4.4
Penny is a ticketed passenger on Ô¨Çight #1. To gain entry to the airport terminal, she
must show a state-issued picture ID along with her boarding pass. When she reaches
airport security, she presents her driver‚Äôs license. She also presents her boarding pass
for Ô¨Çight #1. The following formalization justiÔ¨Åes why Penny is allowed to enter the
airport terminal:
1. Face says ‚ü®enter, airport‚ü©
Request
2. Security controls ((Penny controls ‚ü®25D, Ô¨Çight #1‚ü©) ‚äÉ
Penny controls ‚ü®enter, airport‚ü©)
Jurisdiction
3. Security says ((Penny controls ‚ü®25D, Ô¨Çight #1‚ü©) ‚äÉ
Penny controls ‚ü®enter, airport‚ü©)
Access policy
4. DMV controls (Face ‚áíPenny)
Jurisdiction
5. license says (Face ‚áíPenny)
ID card
6. license ‚áíDMV
Trust assumption
7. Airline controls (Penny controls ‚ü®25D, Ô¨Çight #1‚ü©)
Jurisdiction
8. ticket says (Penny controls ‚ü®25D, Ô¨Çight #1‚ü©)
Ticket
9. ticket ‚áíAirline
Trust assumption
10. ‚ü®enter, airport‚ü©
1,2,3,4,5,6,7,8,9 ID
& Ticket Rule
Line 1 is the request as interpreted by airport security: a speciÔ¨Åc face is making
a request to enter the airport terminal. Line 2 corresponds to the security guard at

Basic Concepts
73
the airport recognizing the authority of airport security to make policy. Line 3 is the
actual policy stating that, if the person presenting the ticket is really Penny (i.e., her
face matches the face on the driver‚Äôs license), then Penny can enter the airport. Lines
4 through 6 correspond to the guard recognizing the authority of the state DMV,
seeing Penny‚Äôs driver‚Äôs license with her picture on it, and accepting the license as
authentically issued by the DMV. Lines 7 through 9 have a similar interpretation
with respect to the airline and Penny‚Äôs airplane ticket.
‚ô¶
Exercise 4.3.1
Meg keeps a safe-deposit box (box #1205) at her local bank, in
which she stores important documents. She can access those documents by visiting
the bank, getting the safe-deposit box, and then opening it with her bank-issued
keycard (cardM) and PIN (PINM).
At the bank, access to safe-deposit boxes is governed as follows:
‚Ä¢ As a strategy to prevent theft, safe-deposit boxes can be removed from the vault
only through the cooperation of the Vault OfÔ¨Åcer and the Teller Supervisor.
SpeciÔ¨Åcally, the Vault OfÔ¨Åcer and the Teller Supervisor must simultaneously
insert special keys (keyVO and keyTS, respectively) and enter the speciÔ¨Åc box‚Äôs
number.
‚Ä¢ Once removed from the vault, the safe-deposit box is brought to the account
holder (in this case, Meg), who may open the box by inserting her keycard and
entering the correct PIN number.
Let ‚ü®release,#1205‚ü©and ‚ü®open,#1205‚ü©be (respectively) the operations of releas-
ing box 1205 from the vault and of opening box 1205. Use expressions in the access-
control logic to answer the following questions regarding the certiÔ¨Åcations, creden-
tials, and access-control policies needed for the bank‚Äôs safe-deposit system.
a. What are the speciÔ¨Åc access requests that are placed to release box 1205 from
the vault and to open box 1205?
b. What are the ACL (access-control list) entries that govern the vault‚Äôs release
and Meg‚Äôs opening of box 1205?
c. What are the additional certiÔ¨Åcates, recognition of authority, and trust as-
sumptions that are necessary for determining whether to grant release or open
requests for box 1205?
d. Suppose that Meg‚Äôs safe-deposit box is brought to her, and that her keycard is
valid. Using the relevant assumptions from Parts (1), (2), and (3) above, give
a formal proof that the request ‚ü®open,#1205‚ü©will be granted.
Exercise 4.3.2
Suppose you are the chief operating ofÔ¨Åcer of a rental car company.
One of your primary objectives is to make sure you rent cars only to qualiÔ¨Åed drivers.

74
Access Control, Security, and Trust: A Logical Approach
a. Informally describe your method for assessing whether a domestic customer
is qualiÔ¨Åed to rent a car, what evidence you will use, how you will check the
authenticity of the evidence, and who you trust in the process.
b. Formally describe your method using the access-control logic.
c. Most domestic rental car companies rent cars to international customers but
require additional credentials.
Describe your process for establishing the
qualiÔ¨Åcations of international customers.
d. Formally describe your method using the access-control logic.
4.3.3
Groups
In many situations, access control is group based, rather than individual based.
That is, access-control decisions may be based on whether the requester belongs to
a speciÔ¨Åc group, rather than on the speciÔ¨Åc identity of the individual. As a common
example, access to the teacher‚Äôs lounge in a high school is typically granted to all
members of the faculty.
In reality, groups are simply another sort of principal, and group-based access
policies are deÔ¨Åned in the same way as identity-based policies. For example, the
following statement expresses the access policy for the high-school teacher‚Äôs lounge:
Faculty controls ‚ü®enter,lounge‚ü©.
Group membership can be expressed using the speaks-for relationship, as in the fol-
lowing statement:
Leslie ‚áíFaculty.
Using the Derived Controls rule, it is straightforward to deduce that any request from
Leslie to enter the teacher‚Äôs lounge should be granted.
Many everyday documents are certiÔ¨Åcates that assert group membership of one
kind or another. Alec‚Äôs United States passport is a certiÔ¨Åcate issued by the U.S. State
Department that asserts that Alec is a U.S. citizen:
US State Department says Alec ‚áíUS Citizen.
Miranda‚Äôs student identiÔ¨Åcation card, issued by New State University (NSU) when
she Ô¨Årst enrolled, is a certiÔ¨Åcate stating that she is an NSU student:
NSU says Miranda ‚áíNSU Student.
Exercise 4.3.3
Oliver has recently become a Premium member of the frequent-Ô¨Çyer
club of Fly-By-Night Airlines. The membership package he received from Fly-By-
Night included a membership card with his name and frequent-Ô¨Çyer number printed
on it. As a Premium member, Oliver is eligible to enter the Fly-By-Night lounges
at participating airports. On his next trip, Oliver decides to use this beneÔ¨Åt: he

Basic Concepts
75
heads to the Fly-By-Night lounge and presents his membership card and a photo
identiÔ¨Åcation (his driver‚Äôs license) to the staffer who is guarding the door.
Formally describe the access-policy of the lounge, Oliver‚Äôs request to enter, and
any other necessary trust assumptions required for Oliver to be granted access to
the lounge. Based on these descriptions, formally justify letting Oliver into the Fly-
By-Night lounge.
Exercise 4.3.4
Sam is a citizen and resident of the United States, returning home
after visiting Canada. She arrives at the US border with her passport in hand.
a. The act of driving her car to the border agent‚Äôs station can be interpreted as
a request to enter the United States. Express this request in the access-control
logic.
b. Sam‚Äôs passport can be interpreted as a certiÔ¨Åcate (i.e., a signed statement)
from the U.S. State Department that associates Sam‚Äôs face with her name. It
also identiÔ¨Åes Sam as belonging to the group of U.S. citizens.
Formalize Sam‚Äôs passport as an expression in the access-control logic.
c. The standard policy is that US citizens with proof of citizenship may enter the
country, provided the border agent deems them not to be a threat (i.e., the
border agent may exercise discretion).
Formalize this policy in the access-control logic.
d. Suppose that the border agent deems Sam not to be a threat.
Identify any other necessary assumptions (expressed as formulas in the access-
control logic) about certiÔ¨Åcates, jurisdiction, et cetera to justify letting Sam
enter the United States.
e. Using only those expressions given in the previous parts of the question, give
a formal proof justifying Sam‚Äôs entry into the United States.
4.4
Summary
Our goal is to be able to justify an answer to the question ‚ÄúShould this access-
control request be granted?‚Äù Answering this question correctly relies on sorting
through the maze of tickets, certiÔ¨Åcates, credentials, access-control lists, and various
means of authentication. This analysis further depends upon the authorities that are
recognized and the extent of their jurisdiction.
In this chapter, we demonstrated how to describe the mechanisms (e.g., tickets,
ACLs, certiÔ¨Åcates) and the broader context (e.g., recognized authorities and jurisdic-
tions) in our access-control logic. We also demonstrated several derived rules that
support reasoning about access requests. In the process, we developed templates for

76
Access Control, Security, and Trust: A Logical Approach
FIGURE 4.5 Learning outcomes for Chapter 4
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Application
‚Ä¢ Express requests, access-control policies, credentials, and notions of ju-
risdiction in the access-control logic.
‚Ä¢ Express tickets, access-control lists, authentication factors, and group
membership in the access-control logic.
Synthesis
‚Ä¢ When given a scenario that involves tickets, lists, and individual or
group-based authentication, formalize the scenario, identify all neces-
sary trust assumptions, and formally justify the granting of a request.
analyzing a wide variety of access-control scenarios. These templates demonstrate
the utility of derived rules that can be reused in a variety of situations. They also
illustrate how the language can be used to describe new mechanisms and concepts.
The learning outcomes associated with this chapter appear in Figure 4.5.
4.5
Further Reading
For classic papers on access control and protection, we suggest Butler Lampson‚Äôs
‚ÄúProtection‚Äù (Lampson, 1971) and Saltzer and Schroeder‚Äôs ‚ÄúThe Protection of In-
formation in Computer Systems‚Äù (Saltzer and Schroeder, 1975). In addition, Matt
Bishop‚Äôs textbook Computer Security: Art and Science (Bishop, 2003) provides a
compendium of access-control models and devotes a chapter to authentication.

Chapter 5
Security Policies
In Chapter 4, we focused on security mechanisms, which provide the means to en-
force security policies. We now turn to security policies themselves, which deÔ¨Åne
what is allowed and what is prohibited.
Typically, policies specify the permissible actions that subjects can perform on
objects. Subjects are active entities: they include the users, system processes, and
services that perform actions, such as sending messages, reading and writing Ô¨Åles,
and so on. Subjects correspond to the principals of our access-control logic. Objects
are the passive entities acted upon, such as databases, Ô¨Åles, etc. Subjects and objects
are mutually exclusive, but both are protected by access-control policies.
There are three aspects of security policies: conÔ¨Ådentiality, integrity, and avail-
ability. ConÔ¨Ådentiality addresses who may see what; integrity addresses who may
modify what; and availability addresses the required levels of service. In this chap-
ter, we give high-level deÔ¨Ånitions of each, with the understanding that more precise
deÔ¨Ånitions of each depend upon the speciÔ¨Åc system and the context in which it is
used.
Security policies are classiÔ¨Åed in several ways. One way to classify policies is
based on whether they are discretionary or mandatory. A second way is to classify
policies based on their use context: military or commercial. A third way is to describe
policies based on which aspect of security is addressed: conÔ¨Ådentiality, integrity, or
availability. We introduce these different classiÔ¨Åcations in this chapter, providing
examples of each.
5.1
ConÔ¨Ådentiality, Integrity, and Availability
The three core aspects of security policies are conÔ¨Ådentiality, integrity, and avail-
ability (collectively known as the C-I-A triad).
ConÔ¨Ådentiality
ConÔ¨Ådentiality boils down to the inaccessibility of information
or resources. For example, consider Lara‚Äôs grade in her Writing 101 course. The
conÔ¨Ådentiality (or privacy) rules for the university‚Äôs computer system might state
that only Lara, the university Registrar, and Lara‚Äôs Writing 101 instructor can read
Lara‚Äôs Writing 101 grade. As a consequence, the university‚Äôs computer systems are
77

78
Access Control, Security, and Trust: A Logical Approach
not authorized to release Lara‚Äôs Writing 101 grade to her parents. Secrecy is an aspect
of conÔ¨Ådentiality: it deÔ¨Ånes who may know or possess information. For example,
the general leading an army is permitted to know the date of when the army will
attack, whereas the general‚Äôs cook is not permitted to know this date.
Integrity
Integrity refers to the accuracy and credibility of information. Integrity
is a more subtle property than conÔ¨Ådentiality. In the case of conÔ¨Ådentiality, either the
information is compromised (leaked) or not. However, integrity has multiple aspects.
One aspect is content related: are the data correct and uncorrupted? The other aspect
is origin related: who or what is the source of the data?
For example, consider bonds issued by corporations. Bonds are used to raise
money with the promise to lenders that they will be repaid with interest. The simplest
aspect of a bond is the interest rate. Its accuracy is easily veriÔ¨Åed. The vastly more
difÔ¨Åcult aspect is evaluating how credible the promise of repayment is. To deal with
credibility and integrity, we often rely on a rating or grading system. Bonds are rated
as AAA, AA, A, BBB, BB, B, CCC, CC, and C, where AAA is the highest (and
presumably most risk free), C is the lowest (the highest likelihood of default), and
anything with a BB rating or lower is considered junk.
Mechanisms that support integrity fall into two classes: detection and prevention.
Detection mechanisms look for corruption or alteration of data. For example, parity
checking coupled with parity bits is used to detect the corruption of values stored in
computer memory. In contrast, prevention mechanisms deny unauthorized attempts
to change data. For example, students are not granted write access to their college
transcripts.
Lack of corruption alone is insufÔ¨Åcient to guarantee integrity, because integrity
also includes trust. For example, suppose that Trish receives a message to install
a speciÔ¨Åc program on her machine, and the message arrives intact and without any
corruption in its transmission. Trish‚Äôs actions depend on whether she trusts the orig-
inator of the message. If the originator is a system administrator who has authority
over her computer, then Trish will likely comply and install the program. On the
other hand, if the request is from an unknown or untrusted source, then she most
likely will not comply.
Assumptions about who or what is trusted affect the integrity and security of a
system directly. When these assumptions are implicit and unstated, a system may
become vulnerable as the result of changing circumstances. As seen throughout this
book, trust assumptions and statements of authority and jurisdiction are crucial to
virtually every analysis.
Availability
A system is available if it can be used with some minimum level of
performance or quality of service. For example, a personal computer system might
be considered to be available if all requests to access Ô¨Åles on the local hard drives are
satisÔ¨Åed within one second.
When resources or systems are unavailable or perform beneath expected levels of
service, it is difÔ¨Åcult to judge whether the system is suffering from a so-called denial

Security Policies
79
of service attack or from routine use. As an example, consider a highway that is
often jammed during rush hour. When a driver encounters bumper-to-bumper trafÔ¨Åc,
she typically might conclude that its cause is rush-hour trafÔ¨Åc. On the other hand,
she might come to a very different conclusion if she read about a planned protest
by local farmers who intended to drive at the legal minimum speed in all lanes of
the highway that day. The key difference is the intent behind accessing or using a
resource. In the case of rush-hour trafÔ¨Åc, the intent of most highway users is to get
to work. In the case of protesters, it is to deny, frustrate, or degrade the quality of
service to other highway users in order to draw attention to the issues of concern
to them. Assessing whether or not a denial of service attack is occurring can be
difÔ¨Åcult, because determining intent from observable behavior is difÔ¨Åcult.
5.2
Discretionary Security Policies
Discretionary policies are dynamic (i.e., they can be changed), and they are typi-
cally under user control. In particular, discretionary access control (DAC) policies
are policies where users control access to protected objects over which they have ju-
risdiction. Often, DAC policies are based on identity, providing examples of identity-
based access control (IBAC).
In the access-control logic, IBAC policies are recognizable by controls statements
that relate access rights (such as ‚ü®operation,object‚ü©) to a subject‚Äôs identity:
Subject controls ‚ü®operation,object‚ü©.
Such statements reÔ¨Çect the identity-based portion of an access-control policy. Fur-
thermore, in the case of DAC policies, there is some principal (typically the owner of
the object) who speciÔ¨Åes the policy, resulting in a statement of the following form:
Principal says (Subject controls ‚ü®operation,object‚ü©).
The discretionary nature of the policy‚Äîthat is, that a given principal has the authority
to set such policies‚Äîcan be expressed as follows:
Principal controls (Subject controls ‚ü®operation,object‚ü©).
The expected behavior or response to subjects‚Äô access requests can be expressed
as theorems or derived inference rules. Expressing behavior as theorems in the form
of derived inference rules enables our expectations to be checked for validity.
For example, given the above identity-based statements, we can express our ex-
pectations of behavior as a derivable inference rule of the following form:
Principal says (Subject controls ‚ü®operation,object‚ü©)
Principal controls (Subject controls ‚ü®operation,object‚ü©)
Subject says ‚ü®operation,object‚ü©
‚ü®operation,object‚ü©
.

80
Access Control, Security, and Trust: A Logical Approach
foo
April
read
Bill
read, write
Carla
execute
Table 5.1: Access-control matrix for foo under Susan‚Äôs control
Of course, depending on the actual situation, the inference rules may be more
complicated and include various conditions. The following example is typical of
discretionary access-control policies.
Example 5.1
Suppose Susan creates a program foo. Susan is allowed to specify that April can read
foo, Bill can read and write (modify) foo, and Carla can execute foo. The access-
control matrix for foo is shown in Table 5.1.
In the access-control logic, we represent the table as follows:
Susan says (April controls ‚ü®read, foo‚ü©)‚àß
Susan says (Bill controls ‚ü®read, foo‚ü©)‚àß
Susan says (Bill controls ‚ü®write, foo‚ü©)‚àß
Susan says (Carla controls ‚ü®execute, foo‚ü©).
This statement reÔ¨Çects the access policy on foo, as dictated by Susan.
We represent Susan‚Äôs discretion or authority to dictate access to foo as follows:
Susan controls (April controls ‚ü®read, foo‚ü©) ‚àß
Susan controls (Bill controls ‚ü®read, foo‚ü©) ‚àß
Susan controls (Bill controls ‚ü®write, foo‚ü©) ‚àß
Susan controls (Carla controls ‚ü®execute, foo‚ü©).
Both statements, taken together, describe the discretionary access-control policy for
the Ô¨Åle foo.
The behavior of the system that controls access to foo can be described by theo-
rems or derived inference rules. For example, given the above discretionary access-
control policy statements, we can derive the response to the request
Carla says ‚ü®execute, foo‚ü©.
In particular, the response to Carla‚Äôs request is given by the following derivable in-
ference rule:
Susan says (Carla controls ‚ü®execute, foo‚ü©)
Susan controls (Carla controls ‚ü®execute, foo‚ü©)
Carla says ‚ü®execute, foo‚ü©
‚ü®execute, foo‚ü©
.
‚ô¶

Security Policies
81
Exercise 5.2.1
Consider the following access-control matrix for subjects S1 ¬∑¬∑¬∑S3

and objects O1 ¬∑¬∑¬∑O4 (the owner of each object appears in parentheses):
Subject
O1(S1)
O2(S2)
O3(S3)
O4(S4)
S1 read,write
read
read
read
S2
read
read,write
read
read
S3
read
read
read,write
read,write
Use the access-control logic to formulate the access-control rules for each object.
Exercise 5.2.2
Sunita and her husband Anand are organizing their Ô¨Åles on their
home computer system, which is shared with their son Samuel. Sunita is a distin-
guished author who is working on her next book, and she keeps her draft on the
family‚Äôs home computer system in the Ô¨Åle bookDraft. Sunita routinely relies on
Anand and Samuel to read her draft. She keeps a running list of their comments in
the Ô¨Åles AnandsComments and SamuelsComments, which Anand and Samuel use to
record their reactions to her draft.
It is important to Sunita that she be the only one who can make changes to the book
draft. She wants to allow Anand and Samuel to read the draft and each other‚Äôs com-
ments, but it is important to her that Anand‚Äôs and Samuel‚Äôs comments be separate
from one another.
Devise answers to the following questions.
a. Create access-control matrices for the Ô¨Åles bookDraft, AnandsComments,
and SamuelsComments. Explain how your access-control matrices satisfy
Sunita‚Äôs concerns.
b. Write the policy statements for each Ô¨Åle in the access-control logic.
c. Describe in words what happens when given each of the following requests:
Anand says ‚ü®write,AnandsComments‚ü©,
Anand says ‚ü®write,SamuelsComments‚ü©,
Samuel says ‚ü®read,AnandsComments‚ü©.
For each request that is granted, give the derivable inference rule that justiÔ¨Åes
granting access.
d. Prove that your inference rules are valid.
5.3
Mandatory Security Policies
Mandatory security policies are policies that apply to everyone and everything all
the time: they are static and cannot be changed, and individuals have no discretion

82
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.1 Conceptual diagram of a virtual machine and monitor
or control over them. Mandatory access control (MAC) policies in computers are
typically implemented by the operating system or by the hardware. For resources
protected by a MAC policy, neither the owner nor the requester can inÔ¨Çuence the
access-control decision. Unlike DAC policies, MAC policies typically do not name
speciÔ¨Åc subjects or principals in policy statements. The rules for access usually
depend on considerations other than identity.
As an important example of a MAC policy, let us consider the rules for access-
ing physical memory in support of virtual memory for virtual machines. We study
virtual machines in detail in Chapter 10. For now, it sufÔ¨Åces to know that virtual ma-
chines provide self-contained operating environments that provide users the illusion
of working on a single-user system. To maintain this illusion successfully, the op-
erating environment must separate and isolate individual virtual machines from one
another, so that no virtual machine can interfere with the operation of another virtual
machine. Consequently, all virtual-machine attempts to access the host operating
system or hardware must be monitored and checked by a reference monitor called
the virtual machine monitor.
Figure 5.1 contains a simple conceptualization of a virtual machine and its rela-
tionship with physical memory and the virtual machine monitor (VMM). The virtual
machine has two registers:
1. The instruction register (IR) holds the current instruction being executed by
the virtual machine.
2. The accumulator register (ACC) holds intermediate values of computations.
Physical memory has q locations, with the addresses ranging from 0 through q ‚àí1.
The virtual machine monitor regulates virtual-machine access to physical memory.
As mentioned previously, the virtual machine provides to users the illusion that
they are operating in a single-user environment. For this reason, user programs use
logical (or virtual) addresses, which must then be translated into physical addresses.
For example, a program might contain an instruction to load the accumulator register
ACC with the contents of virtual address A (LDA @A). To execute LDA @A, the

Security Policies
83
virtual machine loads the instruction into the instruction register IR and requests
access to the virtual address A. The virtual machine monitor must determine whether
the request would cause a security violation. Whether or not a security violation
exists depends on the relationship between virtual and real memory addresses.
Conceptually, the VMM is implemented as part of a memory management unit
(MMU). The VMM‚Äôs job is twofold: (1) to isolate user programs from one another,
and (2) to provide an execution environment for each virtual machine. Isolation is
accomplished by partitioning physical memory into segments, where each segment
corresponds to the virtual-memory space available to its corresponding user. The
execution environment is provided by associating with each virtual machine VM
the physical location of the VM‚Äôs memory segment. Thus, Alice‚Äôs VM has one
segment, and Bob‚Äôs VM has another separate segment. Alice‚Äôs and Bob‚Äôs programs
may both execute the instruction LDA @A, but the VMM can distinguish the two
address references because Alice and Bob use different memory segments.
The physical location of a memory segment depends on two parameters: the base
and the bound. The base value is the segment‚Äôs starting address in physical mem-
ory. The bound value gives the segment‚Äôs size. These two values are stored in the
relocation register RR, as shown in Figure 5.1.
If there are no security violations (a situation we discuss shortly), then the physical
address associated with the virtual address A can be calculated relative to the base
value for the current segment:
PHYSICAL ADDRESS = base+ A.
If a user‚Äôs program accesses virtual address A and there are no security violations,
then the physical address base+ A is stored in the MMU‚Äôs memory address register
(MAR).
There are two possible security violations that may occur. The Ô¨Årst occurs when
the limit of physical memory (i.e., q) is exceeded:
base+ A ‚â•q.
The second occurs when the boundary of the individual VM‚Äôs segment is exceeded:
A ‚â•bound.
If either of these violations occur, then the instruction is trapped: user control over
the virtual machine is terminated, and control is passed on to a trusted supervisory
control program that can assess the situation and determine what to do next.
The following example illustrates the basic ideas of virtual memory, how MAC
policies are formally described, and how the behavior of virtual machine monitors
can be described as derived inference rules. We will discuss virtual machines and
virtual machine monitors in fuller detail in Chapter 10.
Example 5.2
Consider the virtual machine environment illustrated in Figure 5.1, with three distinct
memory segments.

84
Access Control, Security, and Trust: A Logical Approach
Suppose that the current instruction being executed by the virtual machine VM is
LDA @A (i.e., load the contents of virtual address A into ACC). We can formalize
this situation by the formula
IR says ‚ü®LDA @A‚ü©,
where ‚ü®LDA @A‚ü©is interpreted as ‚Äúit would be a good idea to execute LDA @A.‚Äù
The virtual machine‚Äîvia the instruction register IR‚Äîrequests that VMM permit the
execution of LDA @A.
The virtual machine monitor provides an execution environment for virtual ma-
chines in general, and for VM in this speciÔ¨Åc case. Because the execution environ-
ment is determined by the values base and bound in the relocation register RR, we
can describe it by the statement
RR says ‚ü®(base,bound)‚ü©,
where ‚ü®(base,bound)‚ü©is interpreted as ‚Äúthe pair (base,bound) describes the current
active memory segment.‚Äù
A primary distinction between mandatory and discretionary policies is that manda-
tory policies are enforced by the operating system or hardware, whereas users are in
control in discretionary policies. In the case of enforcing mandatory access control
for physical memory, the memory management unit MMU has the authority and re-
sponsibility for enforcing the MAC policy. This MAC policy imposes conditions
on the states of the VM and VMM registers and on the size of physical memory, as
stated by the following two formulas:
IR says ‚ü®LDA @A‚ü©‚äÉ(RR says ‚ü®(base,bound)‚ü©‚äÉ(((base+ A ‚â•q)‚à®(A ‚â•bound)) ‚äÉ‚ü®trap‚ü©)),
IR says ‚ü®LDA @A‚ü©‚äÉ(RR says ‚ü®(base,bound)‚ü©‚äÉ((base+ A < q) ‚äÉ((A < bound) ‚äÉ‚ü®LDA @A‚ü©))).
The Ô¨Årst formula expresses the conditions under which an address violation occurs
and is trapped. The second formula states that, when there is no address violation,
the instruction LDA @A is executed. In both formulas, the relevant states of VM and
VMM are expressed by the values in IR and RR.
The behavior of the MMU is expressed through derived inference rules. The fol-
lowing rule, which is provable using the inference rules of the access-control logic,
describes when the instruction LDA @A should be trapped:
LDA @A trap
IR says ‚ü®LDA @A‚ü©‚äÉ
(RR says ‚ü®(base,bound)‚ü©‚äÉ(((base+ A ‚â•q)‚à®(A ‚â•bound)) ‚äÉ‚ü®trap‚ü©))
IR says ‚ü®LDA @A‚ü©
RR says ‚ü®(base,bound)‚ü©
(A +base ‚â•q)‚à®(A ‚â•bound)
‚ü®trap‚ü©
The inference rule describing when LDA @A is approved for execution and the
proof of its validity are left as exercises for the reader.
‚ô¶

Security Policies
85
Exercise 5.3.1
Recall the LDA @A instruction illustrated in Figure 5.1. Devise and
prove valid an inference rule that describes when LDA @A is approved for execution.
Exercise 5.3.2
The instruction STO @A takes the contents of the accumulator ACC
and stores it in virtual address A. Devise and prove valid inference rules that de-
scribe when STO @A is trapped and when STO @A is executed.
5.4
Military Security Policies
Military security policies are focused on controlling the disclosure of information
(i.e., providing conÔ¨Ådentiality). These policies regulate the Ô¨Çow of information by
describing who is allowed to know what. Information is classiÔ¨Åed or labeled using
security levels, where the security classiÔ¨Åcation levels are ordered. Principals are
assigned a security clearance level using the same classiÔ¨Åcation levels. Reference
monitors can then control the Ô¨Çow of information by limiting access based on these
classiÔ¨Åcation levels.
For example, the classic classiÔ¨Åcation scheme involves the following four classi-
Ô¨Åcation levels, in increasing order: UNCLASSIFIED, CONFIDENTIAL, SECRET, and
TOP SECRET. To read a Ô¨Åle that has been classiÔ¨Åed at the SECRET level, one must
possess an appropriate clearance level (SECRET or TOP SECRET) and have an appro-
priate need to know.
To express and reason about military security policies, we must augment our logic
to support security classiÔ¨Åcation levels. Fortunately, these modiÔ¨Åcations are very
straightforward, as we shall see.
5.4.1
Extending the Logic with Security Levels
As we have seen in previous chapters, a logic has three important components: the
syntax (i.e., how formulas are written), the semantics (i.e., what the formulas mean),
and the logical rules (i.e., how the formulas are manipulated). In augmenting our
logic to support security levels, we address each of these components in turn.
Syntax
The Ô¨Årst step is to introduce syntax for describing and comparing security
levels. We deÔ¨Åne SecLabel to be the collection of simple security labels, which are
used as names for the various security levels associated with a military access policy;
possible elements of this set include TS, S, C, and UC, among others.
In addition to these speciÔ¨Åc security labels, we will often want to refer abstractly
to the security level assigned to a particular principal P. For this reason, we deÔ¨Åne
the larger set SecLevel of all possible security-level expressions:
SecLevel ::= SecLabel / slev(PName)

86
Access Control, Security, and Trust: A Logical Approach
That is, a security-level expression is either a simple security label or an expression
of the form slev(A), where A is a simple principal name.1 Informally, slev(A)
refers to the security level of principal A.
Finally, we extend our deÔ¨Ånition of well-formed formulas to support comparisons
of security levels:
Form ::= SecLevel ‚â§s SecLevel / SecLevel =s SecLevel
Informally, the formula C ‚â§s slev(Kate) states that Kate‚Äôs security level is greater
than or equal to the security level C. Similarly, the formula slev(Barry) =s slev(Joe)
states that Barry and Joe have been assigned the same security level.
Semantics
Providing formal and precise meanings for the newly added syntax
requires us to Ô¨Årst extend our Kripke structures with additional components that
describe security classiÔ¨Åcation levels. SpeciÔ¨Åcally, we introduce extended Kripke
structures of the form
M = ‚ü®W,I,J,K,L,‚™Ø‚ü©,
where:
‚Ä¢ W, I, and J are deÔ¨Åned as in DeÔ¨Ånition 2.1.
‚Ä¢ K is a non-empty set, which serves as the universe of security levels.
‚Ä¢ L : (SecLabel‚à™PName) ‚ÜíK is a function that maps each security label and
each simple principal name to a security level.
L is extended to work over arbitrary security-level expressions, as follows:
L( slev(A)) = L(A),
for every simple principal name A.
‚Ä¢ ‚™Ø‚äÜK √óK is a partial order on K: that is, ‚™Øis reÔ¨Çexive (for all k ‚ààK, k ‚™Øk),
transitive (for all k1,k2,k3 ‚ààK, if k1 ‚™Øk2 and k2 ‚™Øk3, then k1 ‚™Øk3), and
anti-symmetric (for all k1,k2,k3 ‚ààK, if k1 ‚™Øk2 and k2 ‚™Øk1, then k1 = k2).
Using these extended Kripke structures, we deÔ¨Åne the semantics for our new well-
formed expressions as follows:
EM [[‚Ñì1 ‚â§s ‚Ñì2]] =
(
W,
if L(‚Ñì1) ‚™ØL(‚Ñì2)
/0,
otherwise
EM [[‚Ñì1 =s ‚Ñì2]] = EM [[‚Ñì1 ‚â§s ‚Ñì2]]‚à©EM [[‚Ñì2 ‚â§s ‚Ñì1]].
As these deÔ¨Ånitions suggest, the expression ‚Ñì1 =s ‚Ñì2 is simply syntactic sugar for
(‚Ñì1 ‚â§s ‚Ñì2)‚àß(‚Ñì2 ‚â§s ‚Ñì1).
1This syntax precludes security-level expressions such as slev(P & Q) or slev(P | Q), because there is
no standard technique for associating security classiÔ¨Åcation labels with compound principals.

Security Policies
87
FIGURE 5.2 Inference rules for relating security levels
‚Ñì1 =s ‚Ñì2
def
= (‚Ñì1 ‚â§s ‚Ñì2)‚àß(‚Ñì2 ‚â§s ‚Ñì1)
ReÔ¨Çexivity of ‚â§s
‚Ñì‚â§s ‚Ñì
Transitivity of ‚â§s
‚Ñì1 ‚â§s ‚Ñì2
‚Ñì2 ‚â§s ‚Ñì3
‚Ñì1 ‚â§s ‚Ñì3
FIGURE 5.3 Proof of ‚â§s Subst
1. slev(P) =s ‚Ñì1
Assumption
2. slev(Q) =s ‚Ñì2
Assumption
3. ‚Ñì1 ‚â§s ‚Ñì2
Assumption
4. ( slev(P) ‚â§s ‚Ñì1) ‚àß(‚Ñì1 ‚â§s slev(P))
1, Defn =s
5. ( slev(Q) ‚â§s ‚Ñì2) ‚àß(‚Ñì2 ‚â§s slev(Q))
2, Defn =s
6. slev(P) ‚â§s ‚Ñì1
4, SimpliÔ¨Åcation
7. slev(P) ‚â§s ‚Ñì2
6, 3 Transitivity of ‚â§s
8. ‚Ñì2 ‚â§s slev(Q)
5, SimpliÔ¨Åcation
9. slev(P) ‚â§s slev(Q)
7,8 Transitivity of ‚â§s
Logical Rules
The last task is to introduce logical rules that support using security
levels to reason about access requests. SpeciÔ¨Åcally, the deÔ¨Ånition and two sound
inference rules in Figure 5.2 reÔ¨Çect that ‚â§s is a partial order. The antisymmetry of
‚â§s emerges directly from the deÔ¨Ånition of ‚Ñì1 =s ‚Ñì2.
The following useful rule (‚â§s Subst) is derivable, as shown by the proof in Fig-
ure 5.3:
‚â§s Subst
slev(P) =s ‚Ñì1
slev(Q) =s ‚Ñìs
‚Ñì1 ‚â§s ‚Ñì2
slev(P) ‚â§s slev(Q)
.
5.4.2
Expressing Military Security Policies
With these extensions to the logic in hand, we can now formulate a simple military
security policy for managing information Ô¨Çow. There are two parts: (1) the simple
security condition, which describes the conditions under which read access can be
granted; and (2) the *-property, which states when it is permissible to grant someone
write access. Our presentation is based on the Bell‚ÄìLa Padula model (Bell and La
Padula, 1973; Bell and La Padula, 1975). We provide a simpliÔ¨Åed discussion here,
leaving the full details to Chapter 13.
DeÔ¨Ånition 5.1 The simple security condition mandates that principal P can read
object O if and only if:

88
Access Control, Security, and Trust: A Logical Approach
1. P‚Äôs security level is at least as high as O‚Äôs (i.e., slev(O) ‚â§s slev(P)), and
2. P has discretionary read access to O (i.e., P controls ‚ü®read,O‚ü©).
The simple security condition reÔ¨Çects the thinking that people or processes should
have read access to information at their security level or below, provided that they
need to know. The Ô¨Årst component ( slev(O) ‚â§s slev(P)) indicates that P can read
O only if P‚Äôs security level is at least as high as O‚Äôs. The second component requires
P to have discretionary read access to O, reÔ¨Çecting P‚Äôs need to know (i.e., some con-
trolling authority has granted P read access to O). The purpose of the simple security
condition is to prevent a principal from reading information at a higher classiÔ¨Åcation
level than the principal possesses.
Unauthorized reading of Ô¨Åles, however, is not the only way that information might
improperly Ô¨Çow from a higher classiÔ¨Åcation level to a lower one. It is also neces-
sary to prevent the ‚Äúwrite-down‚Äù of information, whereby information from a Ô¨Åle
at one classiÔ¨Åcation level is then written to a Ô¨Åle with a lower classiÔ¨Åcation level.
The standard write policy that prevents such situations is known as the *-property
(pronounced ‚Äústar property‚Äù).
DeÔ¨Ånition 5.2 The *-property mandates that principal P can write to object O if
and only if:
1. O‚Äôs security level is at least as high as P‚Äôs (i.e., slev(P) ‚â§s slev(O)), and
2. P has discretionary write access to O (i.e., P controls ‚ü®write,O‚ü©).
Restricting write access to Ô¨Åles at or above a principal‚Äôs clearance level means
that information cannot Ô¨Çow downwards (i.e., to lower classiÔ¨Åcation levels) through
the writing of Ô¨Åles. Taken together, the simple security condition and the *-property
deÔ¨Åne the desired behavior of systems with a simple military security policy.
It is straightforward to formalize the behavior mandated by the simple security
condition and the *-property. Both the simple security condition and the *-property
have two parts: (1) a mandatory access-control condition ¬µ, and (2) a discretionary
statement œï. Thus, each can be expressed by a statement of the form ¬µ ‚äÉœï.
In the case of the simple security condition, the mandatory access-control condi-
tion is
slev(O) ‚â§s slev(P),
which mandates that the security level of object O must be no higher than the security
level of subject P. The corresponding discretionary access statement to grant P read
access to O is
P controls ‚ü®read,O‚ü©.
Thus, we can specify the simple security condition for subject P and object O as
follows:
( slev(O) ‚â§s slev(P)) ‚äÉ(P controls ‚ü®read,O‚ü©).

Security Policies
89
In such statements, P discretionary read access to O is conditioned upon P having
sufÔ¨Åcient clearance with respect to O.
Special note: It is important to realize that the simple security property really
mandates two properties of a system. The Ô¨Årst property states that it is per-
missible to grant read access to any principal who has both a sufÔ¨Åciently
high security level and discretionary read access. It is this property that is
expressed by a policy statement
( slev(O) ‚â§s slev(P)) ‚äÉ(P controls ‚ü®read,O‚ü©).
The second property asserts that read access should be granted only to
those principals who have both a sufÔ¨Åciently high security level and dis-
cretionary read access. In other words, any principal who is able to obtain
read access has sufÔ¨Åcient clearance and is authorized, without exception.
This second-property is in actuality a meta-property, in that one would
have to inspect every consequence of the collection of rules governing
access. It is impossible to express such a property in the logic itself.
A similar situation holds with respect to the *-property.
In the same manner, we can express the *-property as follows:
( slev(P) ‚â§s slev(O)) ‚äÉ(P controls ‚ü®write,O‚ü©).
That is, P‚Äôs discretionary write access to O is contingent upon O having a sufÔ¨Åciently
high security level (i.e., at least as high as P‚Äôs).
The intended behavior described by both the simple security condition and the
*-property can be captured by the following Conditional Controls derived inference
rule:
Conditional Controls
¬µ ‚äÉP controls œï
¬µ
P says œï
œï
.
In this rule, the statement ¬µ ‚äÉP controls œï is a military security policy statement
corresponding to either the simple security condition or the *-property: ¬µ is the
mandatory access-control policy condition with respect to the subject‚Äôs and object‚Äôs
security levels, and P controls œï is the discretionary access-control policy. The state-
ment P says œï reÔ¨Çects the actual access request from P, where œï is the action being
requested by P.
When writing military security policies for which we wish the simple security
condition and the *-property to hold, it is important to follow the form of the read and
write access-control statements presented earlier. SpeciÔ¨Åcally, all read authorizations

90
Access Control, Security, and Trust: A Logical Approach
Document
ClassiÔ¨Åcation
threat scenario
TS
status report
TS
requirements
S
design
S
artist renderings
C
press releases
UC
Table 5.2: ClassiÔ¨Åcation level of documents
on object O by subject P corresponding to the simple security condition should have
the form:
( slev(O) ‚â§s slev(P)) ‚äÉ(P controls ‚ü®read,O‚ü©).
Similarly, all write authorizations on object O by subject P corresponding to the
*-property should have the form:
( slev(P) ‚â§s slev(O)) ‚äÉ(P controls ‚ü®write,O‚ü©).
In some cases, it may seem simpler to adopt shortcuts, but they can produce un-
intended results when circumstances change. For example, suppose that Alice is
assumed to have the highest security level, which in this case is TOP SECRET. Under
the simple security condition, she has read access to all Ô¨Åles to which she also has
discretionary read access. One might be tempted to take a shortcut and simply state
Alice controls ‚ü®read,O‚ü©. When Alice makes a read request‚ÄîAlice says ‚ü®read,O‚ü©‚Äî
the Controls inference rule allows one to conclude that Alice should be allowed to
read O. However, now suppose that Alice‚Äôs security clearance is later downgraded
below TOP SECRET, and that O‚Äôs security level is TOP SECRET. Because the access
policy for Alice reading O does not mention her clearance level, she would still be
granted read permission, in direct violation of the simple security condition.
5.4.3
Military Security Policies: An Extended Example
As an extended example, consider the following scenario involving the (Ô¨Åctional)
defense contractor DefenseSystemsRUs:
DefenseSystemsRUs has defense contracts across several branches
of the military. In particular, they have separate contracts with both
the Air Force and the Navy to develop air superiority Ô¨Åghters. The Air
Force-funded project is called the FX-1, while the Navy-funded project
is called the FX-2.
The Defense Department wishes to order only one aircraft, and will
select either the FX-1 or FX-2 to serve the needs of both the Air Force
and Navy. To ensure a fair competition, the Defense Department has
mandated that DefenseSystemsRUs have two totally independent design

Security Policies
91
Function
Clearance
FX-1
FX-2
Team Leader
TS
Amy
Arlen
Engineer
S
Biao
Burt
Artist
C
Sonja
Suresh
Public Relations
UC
Jude
Jodi
Table 5.3: FX-1 and FX-2 project personnel, functions, and clearances
teams, each working closely with the military branch that is funding the
team.
As part of the FX-1 and FX-2 contracts, each design team has to
work with several documents at various security classiÔ¨Åcation levels.
All the documents are stored in DefenseSystemsRUs computers. Thus, a
security policy is required to control information Ô¨Çow within and be-
tween design teams. The policy relies on the four standard military
classiÔ¨Åcation levels, linearly ordered as follows (in increasing order):
UNCLASSIFIED, CONFIDENTIAL, SECRET, and TOP SECRET.
Table 5.2 lists each document and its classiÔ¨Åcation level. The threat
scenario is considered TOP SECRET to prevent adversary nations from
developing technologies to defeat the FX-1 or FX-2 before they are de-
veloped and deployed. The requirements document is at the SECRET
level to guide design engineers at the SECRET level. Both the threat
scenario and requirements documents are issued by the Department of
Defense and cannot be changed by anyone in DefenseSystemsRUs. The
artist renderings of the FX-1 and FX-2 are CONFIDENTIAL. Press re-
leases are UNCLASSIFIED. Weekly status reports, which go to the gov-
ernment, are TOP SECRET.
The FX-1 and FX-2 teams are organized as shown in Table 5.3. Each
team has a team leader. Team leaders hold TOP SECRET clearance and
are able to read all reports related to their projects. As team leaders, they
can write the weekly status reports. They cannot alter the threat scenario
or requirements document, as both come from the Defense Department.
Engineers are at the SECRET level. They have read access to the
requirements and design documents. They can modify the design docu-
ments, but not the requirements document.
Artists are at the CONFIDENTIAL level. They can create artist ren-
derings of their aircraft.
Finally, public relations staff are at the UNCLASSIFIED level. They
author press releases, which are available to the public.
Given the above scenario, we devise two access-control matrices, one for each
project team, with the understanding that members of each team have no access
rights to documents written by the other team. These matrices are shown in Ta-
bles 5.4 and 5.5. The threat scenario is accessible to both FX-1 and FX-2 team

92
Access Control, Security, and Trust: A Logical Approach
FX-1 Document
Amy (TS)
Biao (S)
Sonja (C)
Jude (UC)
threat scenario (TS)
read
status report (TS)
read, write
write
write
write
requirements (S)
read
read
design (S)
read
read,write
artist renderings (C)
read
read
read, write
press releases (UC)
read
read
read
read, write
Table 5.4: FX-1 access matrix
FX-2 Document
Arlen (TS)
Burt (S)
Suresh (C)
Jodi (UC)
threat scenario (TS)
read
status report (TS)
read, write
write
write
write
requirements (S)
read
read
design (S)
read
read,write
artist renderings (C)
read
read
read, write
press releases (UC)
read
read
read
read, write
Table 5.5: FX-2 access matrix
leaders; the requirements document is accessible to the team leaders and engineers
of both projects.
The access-control matrices adhere to the simple military security properties spec-
iÔ¨Åed by the simple security condition and the *-property. Access to Ô¨Åles is based on
a need to know. For example, while there would be no violation of information
Ô¨Çow policies if Jude in Public Relations were allowed to write to the FX-1 design
documents, there is no compelling need to grant her write access.
Formal description:
We translate the preceding informal descriptions into the
access-control logic. Doing so provides us with a precise description of the scenario
and enables us to do a rigorous analysis of behavior.
The Ô¨Årst task is to identify the security labels for this application (e.g., UC, C, S,
TS) and to specify the ordering among them. We can express the ordering as a series
of statements in the logic, as follows:
UC ‚â§s C,
C ‚â§s S,
S ‚â§s C.
Note that (for example) UC ‚â§s S can be deduced using the transitivity rule for ‚â§s.
The second task is to state in the logic the assignment of security levels to princi-
pals and objects, as speciÔ¨Åed in Tables 5.2 and 5.3. We include here the statements

Security Policies
93
related to the FX-1 project (those for the FX-2 project are similar):
slev(threatScenario) =s TS
slev(Amy) =s TS
slev(statusFX1) =s TS
slev(Biao) =s S
slev(reqs) =s S
slev(Sonja) =s C
slev(designFX1) =s S
slev(Jude) =s UC
slev(artistRendFX1) =s C
slev(pressRelFX1) =s UC
In our simpliÔ¨Åed analysis, we just accept these statements as assumptions of the
form slev(statusFX1) =s TS and slev(Amy) =s TS.
More detailed descriptions
would include the source and jurisdiction of these statements: for example, a se-
curity ofÔ¨Åcer SO might have the authority and responsibility to certify the secu-
rity levels of people and documents. This would be described in the usual way as
SO says ( slev(statusFX1) =s TS) and SO controls ( slev(statusFX1) =s TS). To keep
things simple for this example, we will just assume these levels as stated.
Finally, at the object level of the access-control logic, we consider the access-
control matrices in Tables 5.4 and 5.5. As we did with setting security levels, we
take the access-control statements as assumptions. More detailed descriptions would
include the source of these statements and the authority of the sources. For simplicity,
we assume the content of these statements is appropriate and has the backing of those
in authority. We can translate Table 5.4 into statements on who can access FX-1
status reports as follows (access to other documents can be similarly described):
slev(statusFX1) ‚â§s slev(Amy) ‚äÉ(Amy controls ‚ü®read,statusFX1‚ü©)
slev(Amy) ‚â§s slev(statusFX1) ‚äÉ(Amy controls ‚ü®write,statusFX1‚ü©)
slev(Biao) ‚â§s slev(statusFX1) ‚äÉ(Biao controls ‚ü®write,statusFX1‚ü©)
slev(Sonja) ‚â§s slev(statusFX1) ‚äÉ(Sonja controls ‚ü®write,statusFX1‚ü©)
slev(Jude) ‚â§s slev(statusFX1) ‚äÉ(Jude controls ‚ü®write,statusFX1‚ü©).
The *-property says that any FX-1 team member can submit their portion of the FX-1
status report. The simple security condition says that only Amy‚Äîas the team leader
with a TOP SECRET clearance‚Äîcan read the report.
If the read-access and write-access statements in the access-control matrices are
translated into formulas following the pattern for military security policies, then the
simpliÔ¨Åed security condition and the *-property describe the behavior of the system.
For example, if Jude wishes to write her status report, the following derived inference
rule describes what happens:

94
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.4 Proof justifying Jude‚Äôs access to status report
1.
slev(Jude) ‚â§s slev(statusFX1) ‚äÉ(Jude controls ‚ü®write,statusFX1‚ü©)
Assumption
2.
slev(Jude) =s UC
Assumption
3.
slev(statusFX1) =s TS
Assumption
4.
UC ‚â§s TS
Assumption
5.
Jude says ‚ü®write,statusFX1‚ü©
Assumption
6.
slev(Jude) ‚â§s slev(statusFX1)
2, 3, 4 ‚â§s Subst
7.
‚ü®write,statusFX1‚ü©
1, 6, 5 Cond‚Äôl Controls
slev(Jude) ‚â§s slev(statusFX1) ‚äÉ(Jude controls ‚ü®write,statusFX1‚ü©)
slev(Jude) =s UC
slev(statusFX1) =s TS
UC ‚â§s TS
Jude says ‚ü®write,statusFX1‚ü©
‚ü®write,statusFX1‚ü©
.
The above rule has as its Ô¨Årst hypothesis the write-access policy statement cor-
responding to the *-property: Jude can write to statusFX1 so long as the security
level of statusFX1 is the same or is higher than Jude‚Äôs security level. The next two
hypotheses are Jude‚Äôs and statusFX1‚Äôs security levels. The last hypothesis is Jude‚Äôs
write request.
Figure 5.4 contains a proof justifying that Jude be allowed to write the FX-1 status
report.
Exercise 5.4.1
Recall the access-control matrix in Table 5.5. Devise the derived
inference rules corresponding to Jodi‚Äôs access rights on press releases. Prove the
validity of your derived rules.
Exercise 5.4.2
Recall the access-control matrix in Table 5.4. Suppose that artists

renderings are now unclassiÔ¨Åed. Is the Bell‚ÄìLa Padula model still satisÔ¨Åed? If
not, suggest a change in Table 5.4 that restores compliance with the Bell‚ÄìLa Padula
model.
5.5
Commercial Policies
Commercial policies are primarily concerned with integrity, namely the level of
quality and trustworthiness. Businesses fail if the quality or trustworthiness of their
products or services fail to live up to consumer expectations. For example, consider
what happens to a restaurant that serves tainted food to its customers.

Security Policies
95
Commercial integrity policies use labels in many of the same ways that military
security policies use labels. Integrity labels often reÔ¨Çect a grading of quality or trust-
worthiness levels. For example, a grading scheme we encounter in daily life is the
grading of gasoline based on its octane level. The gasoline we pump into our cars is
usually one of three grades: REGULAR, MID-GRADE, or PREMIUM. From the con-
sumer standpoint, higher gasoline grades are more expensive. From the petroleum
reÔ¨Ånery‚Äôs standpoint, higher gasoline grades require more reÔ¨Ånement and are more
costly to produce. The public expects gasoline grades and labels to match and feels
defrauded when they do not.
Another grading scheme we encounter in daily life is level of customer service.
For example, banking customers with balances of $200,000 or more in their accounts
might qualify for PREMIUM customer service. Customers with lesser balances qual-
ify for STANDARD customer service. It may be that STANDARD service includes
24-hour automated support and 24-hour phone support. PREMIUM support might
include everything that STANDARD support has, plus the assignment of a personal
banker available during working hours during the work week. Because higher levels
of customer support are more costly to companies and meeting customer expecta-
tions is crucial to stay in business, well-run businesses establish and enforce service
(integrity) policies to meet customer expectations while controlling costs.
In this section we introduce basic policies aimed at preserving integrity.
5.5.1
Extending the Logic with Integrity Levels
As we did for security labels, we extend our logic to incorporate integrity labels
and a means to compare them. A variety of integrity labels are possible, and they are
partially ordered. The formalization is nearly identical to that for security labels: the
only difference is in our intention to distinguish integrity levels from security levels,
so as to permit analyses that involve both (as in Chapter 13).
Syntax
We deÔ¨Åne IntLabel to be the collection of simple integrity labels, and we
deÔ¨Åne IntLevel to be the set of all possible integrity-level expressions:
IntLevel ::= IntLabel / ilev(PName)
Informally, ilev(A) refers to the integrity‚Äîi.e., quality or trustworthiness‚Äîlevel of
principal A.
We then extend our deÔ¨Ånition of well-formed formulas to support comparisons of
security levels:
Form ::= IntLevel ‚â§i IntLevel / IntLevel =i IntLevel
The symbol ‚â§i denotes a partial ordering on integrity levels, in the same way that ‚â§s
denotes a partial ordering on security levels. In particular, ‚â§i is reÔ¨Çexive, transitive,
and antisymmetric.

96
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.5 Inference rules for relating integrity levels
‚Ñì1 =i ‚Ñì2
def
= (‚Ñì1 ‚â§i ‚Ñì2)‚àß(‚Ñì2 ‚â§i ‚Ñì1)
ReÔ¨Çexivity of ‚â§i
‚Ñì‚â§i ‚Ñì
Transitivity of ‚â§i
‚Ñì1 ‚â§i ‚Ñì2
‚Ñì2 ‚â§i ‚Ñì3
‚Ñì1 ‚â§i ‚Ñì3
Semantics
The semantics of integrity levels is added to the logic by extending
Kripke structures in precisely the same way we did for security levels. SpeciÔ¨Åcally,
we introduce extended Kripke structures of the form
M = ‚ü®W,I,J,K,L,‚™Ø‚ü©,
where:
‚Ä¢ W, I, and J are deÔ¨Åned as in DeÔ¨Ånition 2.1.
‚Ä¢ K is a non-empty set, which serves as the universe of integrity levels.
‚Ä¢ L : (IntLabel‚à™PName) ‚ÜíK is a function that maps each integrity label and
each simple principal name to a integrity level.
L can be extended to work over arbitrary security-level expressions, as follows:
L( ilev(A)) = L(A),
for every simple principal name A.
‚Ä¢ ‚™Ø‚äÜK √óK is a partial order on K: that is, ‚™Øis reÔ¨Çexive (for all k ‚ààK, k ‚™Øk),
transitive (for all k1,k2,k3 ‚ààK, if k1 ‚™Øk2 and k2 ‚™Øk3, then k1 ‚™Øk3), and
anti-symmetric (for all k1,k2,k3 ‚ààK, if k1 ‚™Øk2 and k2 ‚™Øk1, then k1 = k2).
Using these extended Kripke structures, we deÔ¨Åne the semantics for our new well-
formed expressions as follows:
EM [[‚Ñì1 ‚â§i ‚Ñì2]] =
(
W,
if L(‚Ñì1) ‚™ØL(‚Ñì2)
/0,
otherwise
EM [[‚Ñì1 =i ‚Ñì2]] = EM [[‚Ñì1 ‚â§i ‚Ñì2]]‚à©EM [[‚Ñì2 ‚â§i ‚Ñì1]].
As these deÔ¨Ånitions suggest, the expression ‚Ñì1 =i ‚Ñì2 is simply syntactic sugar for
(‚Ñì1 ‚â§i ‚Ñì2) ‚àß(‚Ñì2 ‚â§i ‚Ñì1). Given the semantics of integrity levels, it is straightforward
to prove the soundness of the inference rules in Figure 5.5, which show that ‚â§i is a
partial order.

Security Policies
97
Just as in the case of security levels, the following derived rule ‚â§i Subst holds true
for integrity levels:
‚â§i Subst
ilev(P) =s ‚Ñì1
ilev(Q) =i ‚Ñìi
‚Ñì1 ‚â§i ‚Ñì2
ilev(P) ‚â§i ilev(Q)
.
5.5.2
Protecting Integrity
The major concern of integrity policies is protecting the system and its resources
from damage, misappropriation, or corruption. In the case of petroleum reÔ¨Åneries,
integrity policies might ensure that REGULAR, MID-GRADE, and PREMIUM grades
have at least 87, 91, and 93 octane, respectively. In the case of banking services,
service policies might guarantee that big depositors have immediate (e.g., less than
one-minute wait times) access to a human banker during business hours while regular
customers have access to human bankers only after having used automated support.
Some of the earliest work on integrity in computer systems was done in the 1970‚Äôs
by Biba. Although this work appeared prior to the threat of worms and viruses, the
deÔ¨Ånitions and principles that informed the integrity policies then are still relevant
now. Biba deÔ¨Åned computer system integrity as follows (Biba, 1975):
The concern of computer system integrity is thus the guarantee that a
subsystem will perform as it was intended to perform by its creator. We
assume that a subsystem has been initially certiÔ¨Åed (by some system
external agency) to perform properly. We then wish to insure that the
subsystem cannot be corrupted to perform in a manner contrary to its
certiÔ¨Åcation. The integrity problem is the formulation of access control
policies and mechanisms that provide a subsystem with the isolation
necessary for protection from subversion. Based on an initial assump-
tion of proper behavior (according to some system external standard),
we are primarily concerned with protection from intentionally malicious
attack: unprivileged, intentionally malicious modiÔ¨Åcation.
Subjects and objects have the same function in both integrity and conÔ¨Ådentiality
policies: subjects are active in that they make requests, whereas objects are passive
and do not make requests.
We consider three kinds of access:
1. Observation: the viewing of information by a subject. The act of viewing also
embodies the notion of execution or changing the state of the observing subject
as a result of the observation. For example, program execution by a subject is
accomplished by a subject observing (fetching) a program from an object.
2. ModiÔ¨Åcation: a change in the object that is discernible by observation.
3. Invocation: a request for service by one subject of another. This is a form of
modiÔ¨Åcation, because the state of the invoked subject is changed by the request
for service, if the request is accepted.

98
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.6 Subjects, objects, domains, and access
Each subject has an associated domain, which deÔ¨Ånes the objects and kind of ac-
cess the subject has. The domain of a subject may overlap or be completely separate
from the domain of any other subject.
Figure 5.6 shows three subjects S1, S2, and S3, each with its own domain (access
to objects). Subjects are represented by squares, objects by circles, and the kind of
access allowed by labeled arcs connecting subjects and objects. The domains of S1
and S2 overlap; the domains of S1 and S3 do not.
For example, S1‚Äôs domain gives it access to three objects: O1,O2, and O3. S1
is able to observe objects O1 and O2, and is able to modify objects O2 and O3.
Figure 5.6 also show the invocation rights of subjects. S2 is able to invoke S3 to
modify O5 and S1 can invoke S2 to observe O4 and modify O5.
In the next section, we introduce Biba‚Äôs strict integrity property (Biba, 1975),
which protects the integrity of both subjects and objects. It does so by ensuring
that subjects observe or read only from objects of equal or greater integrity and that
objects are modiÔ¨Åed only by subjects of equal or greater integrity.
5.5.3
Strict Integrity
The objective of strict integrity is to preserve the integrity of objects and subjects,
which means that no access degrades the integrity of any object or subject. Infor-
mally, preserving integrity requires meeting the following conditions:
1. If there is a path by which the contents of O1 can migrate to On+1, then the
contents of On+1 must not be degraded by the contents of O1. Thus, the in-
tegrity of O1 must be at least as high as that of On+1:
ilev(On+1) ‚â§i ilev(O1).
2. If subject S can observe object O, then S must not be degraded by the contents

Security Policies
99
of O. Thus, the integrity of O must be at least as high as that of S:
ilev(S) ‚â§i ilev(O).
3. Degradation cannot occur due to a subject with greater integrity following the
directions (invocations) of a subject with lesser integrity (e.g., a user telling
the system administrator to turn off all anti-virus software).
The Ô¨Årst two conditions deal with direct access, which we discuss in the rest of this
chapter. The third condition deals with indirect access through invocation, which we
defer until Chapter 13.
To formalize strict integrity, we must Ô¨Årst introduce the notion of a transfer path.
DeÔ¨Ånition 5.3 A transfer path is a sequence of objects O1,O2,¬∑¬∑¬∑ ,On+1 and subjects
S1,S2,¬∑¬∑¬∑ ,Sn such that:
S1 can observe/take O1
S1 can modify/put
O2
S2 can observe/take O2
S2 can modify/put
O3
¬∑¬∑¬∑
Sn can observe/take On
Sn can modify/put
On+1.
A transfer path allows one or more subjects to collectively transfer or copy the con-
tents of one object to another. The following example demonstrates both the concept
of a transfer path and its expression in the logic.
Example 5.3
Suppose that the subject S1 has the authority to observe object O1 directly and to
modify object O2 directly. Representing the direct observation and modiÔ¨Åcation of
an object O by the primitive propositions ‚ü®o,O‚ü©and ‚ü®m,O‚ü©, respectively, we can
represent these two authorizations as follows:
S1 controls ‚ü®o,O1‚ü©,
S1 controls ‚ü®m,O2‚ü©.
Given S1‚Äôs authority, there is a transfer path (via S1) from O1 to O2. Similarly, if
subject S2 has the authority to observe O2 and modify O3, then there is a transfer
path from O2 to O3. Combining the transfer paths for S1 and S2 results in a transfer
path from O1 to O3.
‚ô¶
Strict integrity mandates that no subject or object along a transfer path can ever
possibly become degraded. That is, for any possible operation along a transfer path,
the contents of a destination must never be contaminated with the contents of a source

100
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.7 Preparation of vegetarian and non-vegetarian meals
with a lower level of integrity. Thus, under strict integrity, if we wish to grant subject
S the authority to observe an object O, we must Ô¨Årst ensure that O‚Äôs integrity level is
at least as high as S‚Äôs. In contrast, if we wish to grant S the authority to modify O, we
must Ô¨Årst ensure that S‚Äôs integrity level is at least as high as O‚Äôs.
Consequently, we can express strict integrity policies for direct observations and
direct modiÔ¨Åcations as follows:
( ilev(S) ‚â§i ilev(O)) ‚äÉ(S controls ‚ü®o,O‚ü©),
( ilev(O) ‚â§i ilev(S)) ‚äÉ(S controls ‚ü®m,O‚ü©).
If the strict integrity conditions are violated, then access requests should be trapped:
¬¨( ilev(S) ‚â§i ilev(O)) ‚äÉ(S says ‚ü®o,O‚ü©) ‚äÉ‚ü®trap‚ü©,
¬¨( ilev(O) ‚â§i ilev(S)) ‚äÉ(S says ‚ü®m,O‚ü©) ‚äÉ‚ü®trap‚ü©.
5.5.4
An Extended Example of a Strict Integrity Policy
As a simple example of the application of strict integrity, consider the preparation
of vegetarian and non-vegetarian meals. For the purpose of this example, we assume
that the preparation of vegetarian meals requires following certain rules and proce-
dures beyond the preparation of non-vegetarian meals. In other words, we assume
that people cooking vegetarian meals require additional training beyond the training
to cook non-vegetarian meals.
For the purposes of this example, we have two subjects (Alice and Bob) and Ô¨Åve
objects (starch, vegetables, meat, veg meal, and non-veg meal). Alice is a vegetarian
cook, and Bob is a non-vegetarian cook. As a vegetarian cook, Alice takes from
starch and vegetables to prepare items for veg meals. She can also provide veg dishes

Security Policies
101
Subject
Starch
Vegetables
Meat
Veg Meal
Non-veg meal
Alice
take
take
-
put
put
Bob
take
take
take
-
put
Table 5.6: Access matrix for vegetarian and non-vegetarian meals
Subject or Object
Integrity Level
Alice
V
Bob
NV
Starch
V
Vegetables
V
Meat
NV
Veg Meal
V
Non-veg Meal
NV
Table 5.7: Integrity levels for subjects and objects
for non-veg meals. In contrast, Bob (as a non-vegetarian cook) may draw upon all
possible ingredients. Because his level of training does not qualify him to prepare
certiÔ¨Åed vegetarian meals, however, he can only prepare items for non-veg meals.
Figure 5.7 shows Alice‚Äôs and Bob‚Äôs various capabilities, resulting in the following
transfer paths:
‚Ä¢ Starch and vegetables can be put into a veg meal through Alice.
‚Ä¢ Starch and vegetables can be put into a non-veg meal through either Alice or
Bob.
‚Ä¢ Meat can be put into a non-veg meal through Bob.
The information in this Ô¨Ågure corresponds to the access matrix in Table 5.6. How-
ever, the access matrix alone fails to capture the underlying certiÔ¨Åcations and rules
that assure the integrity of vegetarian meals.
While we could implement the access matrix in Table 5.6, we would fail to capture
the underlying certiÔ¨Åcations and rules that assure the integrity of vegetarian meals.
To capture these aspects, we introduce two integrity levels (V and NV, with NV ‚â§i V)
and assign them to the subjects and objects as in Table 5.7.

102
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.8 Partial access diagram for gasoline
The resulting strict-integrity policy for Alice and Bob can be expressed as follows:
ilev(Alice) ‚â§i ilev(Starch) ‚äÉAlice controls ‚ü®take,Starch‚ü©
ilev(Alice) ‚â§i ilev(Vegetables) ‚äÉAlice controls ‚ü®take,Vegetables‚ü©
ilev(veg meal) ‚â§i ilev(Alice) ‚äÉAlice controls ‚ü®put,veg meal‚ü©
ilev(non-veg meal) ‚â§i ilev(Alice) ‚äÉAlice controls ‚ü®put,non-veg meal‚ü©
ilev(Bob) ‚â§i ilev(Starch) ‚äÉBob controls ‚ü®take,Starch‚ü©
ilev(Bob) ‚â§i ilev(Vegetables) ‚äÉBob controls ‚ü®take,Vegetables‚ü©
ilev(Bob) ‚â§i ilev(Meat) ‚äÉBob controls ‚ü®take,Meat‚ü©
ilev(non-veg meal) ‚â§i ilev(Bob) ‚äÉBob controls ‚ü®put,non-veg meal‚ü©.
Exercise 5.5.1
Consider the following scenario. There are two grades of gaso-
line: P for premium gasoline and R for regular gasoline. There are also two pumps
(Pump1 and Pump2) and two cars: one car (RGC) uses regular gasoline and the
other car (PGC) requires premium gasoline.
Assume that the typical relation on gasoline grades holds: premium gasoline is
higher quality than regular gasoline. You may also assume that cars speciÔ¨Åed as
taking a particular grade of gasoline can safely take that grade of gasoline or higher.
a. Provide the additional arrows (along with their proper labels) necessary for
Figure 5.8 to satisfy the following:
(a) Biba‚Äôs strict integrity policy, and
(b) Fuel requirements on cars.

Security Policies
103
b. Based on your completed diagram, for each pump below, give their domains:
Pump1 =
Pump2 =
c. Using P for Premium grade and R for regular grade, Ô¨Åll out the level assign-
ments for the subject and objects in the following table:
Subjects and Objects
Integrity Level
Premium Gas Tank PGT
Regular Gas Tank RGT
Pump1
Pump2
Premium Gas Car PGC
Regular Gas Car RGC
d. Fill in the access-control matrix:
Subject
PGT
RGT
PGC
RGC
Pump1
Pump2
e. Express as a formula in the access-control logic the policy that states that
Pump1 can put into PGC.
f. Propose a derived inference rule that states that Pump1 can put gasoline into
PGC. Give a formal proof of the derived rule.
Exercise 5.5.2
A cleanroom is a room with a strictly maintained environment to
control the level of contaminants‚Äîsuch as dust particles or microbes‚Äîthat can con-
taminate manufacturing processes. Cleanrooms are classiÔ¨Åed according to the num-
ber and size of particles per volume of air.
The International Standards Organization standard ISO 14644-1 rates the clean-
liness of cleanrooms based on the log base 10 of the number of particles 0.1¬µm or
larger permitted per cubic meter of air. Based on ISO 14644-1, an ISO Class 5 clean-
room has at most 105 = 100,000 particles per m3 of air. The ordering of cleanliness
levels is
¬∑¬∑¬∑ < class 6 < class 5 < class 4 < ¬∑¬∑¬∑ .
The cleanliness of cleanrooms is maintained by a combination of (1) special air
handlers and Ô¨Ålters to control the Ô¨Çow of air between cleanrooms and (2) airlocks
using air showers to clean people to match the level of cleanliness of the cleanroom
they are entering. As to be expected, higher levels of cleanliness require longer times
in airlocks.
Suppose we have a semiconductor or biotechnology assembly line that starts in
a class 4 cleanroom and ends in a class 6 cleanroom. People and parts enter the
cleanrooms as follows:

104
Access Control, Security, and Trust: A Logical Approach
‚Ä¢ People enter cleanrooms from airlocks, and
‚Ä¢ People in cleanrooms take parts from bins.
The access-control rules on people entering cleanrooms from airlocks and people
in cleanrooms taking parts from bins must satisfy the following integrity concerns:
‚Ä¢ Parts must be protected from contamination or else the Ô¨Ånal product is con-
taminated or corrupted. These parts are rated class 4, class 5, and class 6.
‚Ä¢ The cleanrooms must be protected from contamination from both parts and
people.
The following are the integrity levels for cleanrooms, bins, and airlocks.
ilev(RoomA) = ilev(BinA) = ilev(AirlockA) = class 4
ilev(RoomB) = ilev(BinB) = ilev(AirlockB) = class 5
ilev(RoomC) = ilev(BinC) = ilev(AirlockC) = class 6
a. We describe in the access-control logic people in Roomi taking parts from Binj
as follows:
Roomi says ‚ü®take,Binj‚ü©.
(a) Fill in the access-control matrix showing which rooms are allowed to
take parts from which bins.
Objects
Subjects
BinA
BinB
BinC
RoomA
RoomB
RoomC
(b) Based on the preceding access-control matrix, formulate in the access-
control logic the integrity policy that states when Roomi is allowed to
take parts from Binj, for i, j = A,B,C.
b. We describe in the access-control logic people in Airlocki entering Roomj as
follows:
Airlocki says ‚ü®enter,Roomj‚ü©.
(a) Fill in the access-control matrix showing which airlocks are allowed to
enter which rooms.

Security Policies
105
Objects
Subjects
RoomA
RoomB
RoomC
AirlockA
AirlockB
AirlockC
(b) Based on the preceding access-control matrix, formulate in the access-
control logic the integrity policy that states when Airlocki is allowed to
enter Roomj, for i, j = A,B,C.
5.6
Summary
Security policies state what is allowed and what is not. We examined security
policies from several standpoints: mandatory policies, discretionary policies, conÔ¨Å-
dentiality policies, and integrity policies. Mandatory policies apply to all subjects
and objects. Discretionary policies typically are set by owners of objects.
We introduced Bell‚ÄìLa Padula simple security condition and *-property, as well
as Biba‚Äôs strict integrity property. We also augmented the access-control logic to
support reasoning about policies that depends on security and integrity labels.
The learning outcomes associated with this chapter appear in Figure 5.9.
5.7
Further Reading
The original work by the pioneers of secure computing are well worth reading.
Saltzer and Schroeder‚Äôs classis paper ‚ÄúThe Protection of Information in Computer
Systems‚Äù (Saltzer and Schroeder, 1975) provides an excellent overview of memory
protection for isolation and sharing. Bell and La Padula‚Äôs conÔ¨Ådentiality models are
published as technical reports (Bell and La Padula, 1973) and (Bell and La Padula,
1975). Biba‚Äôs original work on integrity also appears as a technical report (Biba,
1975). These are available electronically and should be read by serious students of
security.

106
Access Control, Security, and Trust: A Logical Approach
FIGURE 5.9 Learning outcomes for Chapter 5
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
‚Ä¢ When given an access-control matrix, interpret which requests are per-
mitted and which are not.
Application
‚Ä¢ When given an informal description, use the Bell‚ÄìLa Padula simple secu-
rity condition and the *-property as guides for developing conÔ¨Ådentiality
levels and access rules.
‚Ä¢ When given an informal description, use the Biba strict integrity model
for developing integrity levels and access rules.
Synthesis
‚Ä¢ When given an informal mandatory and discretionary access-control pol-
icy that is required to meet the simple security condition, *-property, and
strict integrity, formalize access rules for subject and objects meeting the
requirements.
‚Ä¢ When given speciÔ¨Åc integrity or security levels and a partial ordering of
those levels, devise an appropriate Kripke model supporting the seman-
tics of ‚â§i and ‚â§s.
Evaluation
‚Ä¢ When given formal access-control rules, security levels, integrity levels,
and trust assumptions, derive if an access request should be granted.

Part II
Distributed Access Control


Chapter 6
Digital Authentication
To make reasoned access-control decisions in a digital world, we need to explore
in more depth how statements are signed and authenticated digitally. The basis for
digital signatures rests on cryptographic keys and cryptographic hash functions in
general, and on public-key cryptography in particular. Digital authentication using
public-key infrastructure (PKI) is well suited for authentication in distributed envi-
ronments such as the Internet.
In this chapter, we describe the details of digital authentication using the access-
control calculus. Unlike traditional explanations, we do not go into the algorith-
mic details of any particular encryption or hash function. Rather, we describe the
relationship between public and private keys, encryption and decryption, keys and
principals, and certiÔ¨Åcates.
6.1
Public-Key Cryptography
At the core of public-key cryptography are two cryptographic keys: one is public
and is openly shared (much like a telephone number or email address), and the other
is private and must be known only by a single principal. In theory, the principal
who possesses the private key is the only principal who can decrypt messages that
were encrypted using the corresponding public key. This property is used to ensure
privacy: that is, only a principal with the correct private key can read a message.
Similarly, messages encrypted using a private key can be universally decrypted by
anyone who possesses the corresponding (and publicly disclosed) public key. This
property is used for authenticity: that is, one can establish the identity of the author
of a message.
The public key K and private key K‚àí1 together form a key pair (K,K‚àí1), and each
key can undo the actions of the other. That is, if encrypt and decrypt are particular
encryption and decryption functions, then the following properties must hold for all
messages m:
decrypt(K‚àí1,encrypt(K,m)) = m,
decrypt(K,encrypt(K‚àí1,m)) = m.
In addition, the public and private keys are typically distinct (i.e., K‚àí1 Ã∏= K). Conse-
109

110
Access Control, Security, and Trust: A Logical Approach
FIGURE 6.1 Process for using public-key encryption for privacy
quently, public-key cryptography is often referred to synonymously as asymmetric-
key cryptography. In the case of public-key algorithms such as RSA (Rivest et al.,
1978), the same algorithm serves both to encrypt and decrypt messages.
There are two additional properties about encryption and decryption that are im-
portant if a cryptosystem is to be useful:
1. It should be computationally infeasible to read an encrypted message without
knowing the correct decryption key.
That is, given an encrypted message encrypt(Ke,m), it should be computation-
ally infeasible to determine m without knowing the reciprocal key Kd. (Note
that Ke may be either a public or private key; Kd is the other half of the key
pair.)
2. It should be computationally infeasible to successfully forge an encryption
without knowing the encryption key Ke.
That is, given a message m but without knowing Ke, it should be computation-
ally infeasible to compute an x such that decrypt(Kd,x) = m.
These properties of public-key cryptography support privacy in the following way.
If Alice wishes to send a message to Bob that only Bob can read, she encrypts the
message with Bob‚Äôs public key KBob. Thus, Alice sends to Bob the following:
encrypt(KBob,message).
The idea here is that the resulting cipher text should be decipherable only with knowl-
edge of Bob‚Äôs private key K‚àí1
Bob. Upon receipt, Bob uses his private key to decrypt
the message:
decrypt(K‚àí1
Bob,encrypt(KBob,message)).
Figure 6.1 illustrates schematically how Alice and Bob use Bob‚Äôs keys to ensure
privacy of the message sent to Bob.
The use of public-key cryptography is not limited solely to achieving privacy.
More often, it is used to authenticate messages, such as website-connection requests,

Digital Authentication
111
FIGURE 6.2 Process for using private-key encryption for authenticity
public-key certiÔ¨Åcates, and so on. For example, suppose that Bob wishes to com-
municate a message to the world in such a way that anyone can deduce that Bob
authored the message. Such a situation might arise if Bob needs to establish his au-
thorship of an article or book for intellectual-property protection. In this case, Bob
can encrypt his book using his private key (which is known only by him):
encrypt(K‚àí1
Bob,book).
Anyone who cares to read Bob‚Äôs book can read it using Bob‚Äôs freely available public
key to decrypt his encrypted Ô¨Åle:
decrypt(KBob,encrypt(K‚àí1
Bob,book)).
The idea here is that only Bob could have created the original encrypted Ô¨Åle, be-
cause only Bob knows the key K‚àí1
Bob used to create it. This sort of use of public-key
cryptography for authenticity is illustrated in Figure 6.2.
One can combine these two approaches to achieve a combination of privacy and
authenticity. For example, suppose that Alice wants to send a message that only Bob
can read, while providing Bob assurance that the message is coming from her. To
achieve this goal, Alice employs the following three-step process:
1. Alice encrypts her message m with her private key:
encrypt(K‚àí1
Alice,m).
This step will allow Bob to deduce that Alice authored the message m, because
Bob can retrieve m using Alice‚Äôs public key.
2. Alice encrypts the result from step one with Bob‚Äôs public key, resulting in the
following:
encrypt(KBob,encrypt(K‚àí1
Alice,m)).
This step provides Alice with assurance that only Bob can read her message,
because only Bob‚Äôs private key will be able to decrypt this cipher text.

112
Access Control, Security, and Trust: A Logical Approach
3. Alice informs Bob in plain text that she is the author of the cipher text from
step two. This hint tells Bob to look up Alice‚Äôs public key so that he can
decipher the message in such a way as to be assured that Alice was the author.
6.2
EÔ¨Éciency Mechanisms
In theory, the approaches described in the previous section are sufÔ¨Åcient to handle
the needs of privacy and authenticity. In practice, however, the approaches described
for authenticity are rarely used, because public-key algorithms are relatively slow
and costly to use. In addition, the repeated use of a given key-pair on large amounts
of text can expose the keys and make them vulnerable to cryptographic analysis and
discovery. To address these pragmatic concerns, two additional steps can be taken:
(1) we can use a cryptographic hashing algorithm, and (2) we can use a session or
data-encryption key (DEK). We describe these steps‚Äîand their use in creating and
verifying digital signatures‚Äîin this section.
6.2.1
Cryptographic Hash Functions
Hash functions are commonly used in computing as a way to map large data spaces
to more compact spaces, substituting uniqueness for efÔ¨Åciency. A very simple ex-
ample of a hash function is a word-level parity check: all 32-bit words with an even
number of 1 bits are mapped to the bit 0, while all those with an odd number of 1
bits are mapped to the bit 1. In this case, a data space with 232 elements is mapped
to a data space of 2 elements, at the cost of uniqueness: 216 values are hashed to the
bit 0, and 216 values are hashed to the bit 1.
Cryptographic hash functions are hash functions with an additional property: given
any particular hash value, it must be computationally infeasible to determine an input
that, when hashed, produces the given value. This property is known as the one-way
property of cryptographic hashes: while hash values can be computed easily from
a given input, it is computationally infeasible to do the reverse (i.e., compute an
input that produces a particular hash value). Thus, for example, the parity check
clearly fails the one-way property (and hence would not make for a cryptographic
hash function), because it is trivial to Ô¨Ånd 32-bit words with parity 1.
The one-way property of cryptographic hashes makes it possible for Ô¨Åxed-length
cryptographic hash values to simulate unique identiÔ¨Åers for messages of arbitrary
length. For example, suppose Ellen wants to send a Ô¨Åle to Todd that is several
megabytes in length, and she wants Todd to be able to determine whether the Ô¨Åle
he receives has arrived unchanged and uncorrupted. To accomplish this goal, Ellen
sends a cryptographic hash of the Ô¨Åle‚Äîalso known as the cryptographic checksum‚Äî
along with the Ô¨Åle itself:
(Ô¨Åle,cryptographic checksum).

Digital Authentication
113
When Todd receives the Ô¨Åle and the cryptographic checksum, he applies the hash
algorithm to the received Ô¨Åle and compares the value he computed with the checksum
sent by Ellen. If they both match, then Todd concludes that the Ô¨Åle he received is
intact and free of any tampering or corruption.
6.2.2
Data-Encryption Keys
When privacy is also a concern for a large Ô¨Åle, the sender may elect to also employ
a symmetric-key encryption algorithm‚Äîsuch as the Advanced Encryption Standard
(AES) (National Institute of Standards and Technology, 2001)‚Äîto create a one-time
data-encryption key Kdek. Symmetric-key encryption algorithms use the same key
for both encryption and decryption (i.e., Kdek = K‚àí1
dek). In addition, symmetric-key
encryption algorithms are three orders of magnitude faster than typical public-key
algorithms, because of the underlying operations they employ (i.e., exclusive-or and
bit substitutions as opposed to modulo-n exponentiation).
If Ellen chooses this approach, she Ô¨Årst creates a key Kdek; she then encrypts the
Ô¨Åle with Kdek, computes the cryptographic checksum of the Ô¨Åle, and encrypts Kdek
with Todd‚Äôs public key KTodd. Ellen sends these three items to Todd:
encrypt(KTodd,Kdek),
hash(Ô¨Åle),
encrypt(Kdek,Ô¨Åle).
When Todd receives these three items, he Ô¨Årst retrieves the data-encryption key Kdek
by using his private key:
decrypt(K‚àí1
Todd,encrypt(KTodd,Kdek)) = Kdek.
He then uses the data-encryption key to retrieve the Ô¨Åle:
decrypt(Kdek,encrypt(Kdek,Ô¨Åle)) = Ô¨Åle.
Finally, he checks the integrity of the retrieved Ô¨Åle by determining whether it hashes
to the cryptographic checksum that Ellen sent.
Unfortunately, however, this scheme is subject to fraud, because none of the sent
items directly tie Ellen to the message that Todd receives. As a result, a third party
could send their own message (along with its cryptographic checksum) to Todd,
alleging that the message is from Ellen. Todd has no way of determining the truth of
such a claim. We can address this problem by introducing digital signatures in the
next subsection.
6.2.3
Digital Signatures
We can Ô¨Åx the potential fraud problem by one addition: Ellen creates a digital
signature by encrypting the cryptographic checksum with her private key. Figure 6.3
illustrates a process for creating digital signatures that supports both integrity and
authorship:
1. The message or Ô¨Åle is hashed by a one-way function. The resulting Ô¨Åxed-
length hash value identiÔ¨Åes the contents of the message.

114
Access Control, Security, and Trust: A Logical Approach
FIGURE 6.3 Process for creating a digital signature
FIGURE 6.4 Process for verifying a digital signature
2. The hash value is then encrypted using the private key of the sender. The
encrypted hash value identiÔ¨Åes the sender, because the hash value can be re-
trieved only by using the sender‚Äôs public key. Note that, because the hash value
is much smaller than the message itself, this approach is much more efÔ¨Åcient
than the process described in Section 6.1.
Unlike human signatures, which remain relatively unchanged over time, digital sig-
natures change depending on the contents of what is being signed.
Figure 6.4 illustrates the process of verifying a digital signature. The process sim-
ply compares the following two values: (1) the hash value obtained by decrypting the
received signature using the sender‚Äôs public key, and (2) the hash value obtained by
hashing the contents of the received message. If the values match, then the received
message is considered to be both intact and from the public key‚Äôs owner.
6.3
Reasoning about Cryptographic Communications
In the previous two sections, we have seen how cryptography can be used to pro-
vide assurances of privacy and authenticity in distributed digital systems. In this

Digital Authentication
115
FIGURE 6.5 Simple analysis of digital signatures
1. KEllen says m
Received message with signature veriÔ¨Åed using KEllen
2. KEllen ‚áíEllen
Ellen‚Äôs public key
3. Ellen says m
2, 1 Derived speaks for
section, we see how the access-control logic can be used to express and reason about
cryptographic communications.
As an example, suppose that Ellen uses the methods described in Section 6.2 to
send Todd a digitally signed message. For this Ô¨Årst case, let us imagine that Ellen
sends the message itself in the clear (i.e., in plaintext with no encryption), but she
digitally signs it. Thus, she sends to Todd the following combination of items:
m,
encrypt(K‚àí1
Ellen,hash(m)).
That is, she sends the message m, along with the encrypted hash (i.e., cryptographic
checksum) of m.
When Todd receives the message, he applies the signature-veriÔ¨Åcation protocol of
Figure 6.4. If this process is successful, then Todd has evidence that Ellen‚Äôs public
key KEllen has been used to verify that the message m arrived intact and uncorrupted.
This fact can be expressed in the access-control logic by recognizing m as a statement
made by KEllen:
KEllen says m.
If Todd believes that KEllen is Ellen‚Äôs public key‚Äîperhaps he looked her key up in
a public directory, or perhaps Ellen gave it to him sometime in the past‚Äîthen he is
willing to associate statements made (i.e., integrity-checked) by that key with Ellen.
Thus, the belief that Ellen‚Äôs public key is KEllen can be expressed in the logic as the
following statement:
KEllen ‚áíEllen.
Based on these two factors, Todd can deduce that Ellen originally sent the message
m. In terms of the logic, this analysis uses the Derived Speaks For rule, as shown in
the proof of Figure 6.5.
Having examined the simpler case, now let us consider the case where Ellen sends
Todd a digitally signed message, using a data-encryption key Kdek to encrypt the
message. Recall that Todd receives three items, along with a hint that Ellen is the
sender:
encrypt(KTodd,Kdek),
encrypt(Kdek,m),
encrypt(K‚àí1
Ellen,hash(m)).
Todd must decrypt the Ô¨Årst component of this package to obtain the key Kdek, which
he can then use to retrieve the message m. He then veriÔ¨Åes Ellen‚Äôs digital signature
by computing the cryptographic hash of m and comparing it with the checksum that
Ellen sent.

116
Access Control, Security, and Trust: A Logical Approach
If this process is successful, then Todd once again used Ellen‚Äôs public key to verify
that the message m he received was indeed the one initially signed by Ellen. As we
saw earlier, this situation can be expressed simply as follows:
KEllen says m.
Although the signature-veriÔ¨Åcation process itself may be more involved in this situ-
ation, it plays the same role in Todd‚Äôs interpretation of the received message. From
Todd‚Äôs perspective, the important aspect is deducing that the message was initially
signed by Ellen using the key K‚àí1
Ellen, which presumably only she possesses. Although
he initially had to decrypt other components of the message using the keys K‚àí1
Todd and
Kdek, those operations alone do not associate the message m to Ellen.
These two examples provide a general framework for expressing digitally signed
and veriÔ¨Åed statements in our logic. Whenever a digitally signed message m has
been veriÔ¨Åed using the key KP in a process such as the one shown in Figure 6.4, we
represent the message as
KP says m.
We can express the belief that the key KP is associated with the principal P as follows:
KP ‚áíP.
Finally, the Derived Speaks For rule allows us to conclude that m originated from the
principal P:
P says m.
The important aspect about this abstraction is that it moves beyond the details about
signature veriÔ¨Åcation and focuses our attention on who said what.
6.4
CertiÔ¨Åcates, CertiÔ¨Åcate Authorities, and Trust
In the examples of the previous section, Todd‚Äôs reasoning depended crucially upon
his assumption that KEllen was Ellen‚Äôs key:
KEllen ‚áíEllen.
What we have not directly addressed is how Todd‚Äîor anyone else, for that matter‚Äî
would come to believe that KEllen is Ellen‚Äôs public key.
In most cases, such a belief originates from a public-key certiÔ¨Åcate, which is issued
by an alleged authority called a certiÔ¨Åcate authority (or CA, for short). A public-key
certiÔ¨Åcate is a digitally signed statement that associates a given public key with a
given principal. A certiÔ¨Åcate that associates the public key KP with principal P can
be expressed in the logic as
KCA says (KP ‚áíP),

Digital Authentication
117
where KCA is the public key of the issuing certiÔ¨Åcate authority.
Although digital certiÔ¨Åcates such as these provide necessary information, they
alone are insufÔ¨Åcient to establish why Todd would believe that KEllen ‚áíEllen. Rather,
such beliefs also depend on recognition of the certiÔ¨Åcate authority and their jurisdic-
tion. For example, suppose that Cora issues a certiÔ¨Åcate signed with her private key
K‚àí1
Cora:
KCora says (KEllen ‚áíEllen).
Furthermore, suppose that Todd believes that KCora is Cora‚Äôs public key:
KCora ‚áíCora.
From these two pieces of information, Todd is able to conclude the following:
Cora says (KEllen ‚áíEllen).
At this point, Todd has a decision to make: does he trust in Cora‚Äôs integrity, author-
ity, and accuracy when Cora says that KEllen ‚áíEllen? That is, Todd must determine
whether or not he is willing to make the following assumption regarding Cora‚Äôs trust-
worthiness and jurisdiction:
Cora controls (KEllen ‚áíEllen).
As this example demonstrates, belief in a public key generally depends upon both
a public-key certiÔ¨Åcate and trust in the certiÔ¨Åcate authority that issued the certiÔ¨Åcate.
A formal analysis of such a situation typically has the following general form:
1. Kca ‚áíCertiÔ¨Åcate Authority
Trust assumption
2. Kca says KP ‚áíP
CertiÔ¨Åcate
3. CertiÔ¨Åcate Authority controls (KP ‚áíP)
Jurisdiction
4. CertiÔ¨Åcate Authority says KP ‚áíP
1, 2 Derived speaks for
5. KP ‚áíP
3, 4 Controls
This proof demonstrates a common pattern that we will observe in many deriva-
tions regarding digitally signed statements. In fact, it gives rise to a useful derived
rule, which we will see again in future chapters:
CertiÔ¨Åcate
VeriÔ¨Åcation
Kca ‚áíCertiÔ¨Åcate Authority
Kca says (KP ‚áíP)
CertiÔ¨Åcate Authority controls (KP ‚áíP)
KP ‚áíP
However, the proof also demonstrates a key aspect of public-key infrastructures,
which is that they ultimately depend upon trust in some public key. In the preceding
proof, we rely on the trust assumption Kca ‚áíCertiÔ¨Åcate Authority to deduce that
KP is P‚Äôs public key. If we were unwilling to make that assumption blindly, then
we would require a public-key certiÔ¨Åcate for the CertiÔ¨Åcate Authority, which in turn
would be signed by yet another CA. Ultimately, the process of evaluating crypto-
graphically signed statements hinges upon the existence of some initial or root key

118
Access Control, Security, and Trust: A Logical Approach
that is trusted (i.e., not derived from another signed certiÔ¨Åcate). This key typically
belongs to some top-level root certiÔ¨Åcate authority and is used to read other key
certiÔ¨Åcates. The following example demonstrates the role of the trusted root key.
Example 6.1
Sally purchases a new computer from a reputable company with the operating sys-
tem and applications such as web browsers already installed. Upon setting up her
computer, Sally types the web address of her favorite Internet bookstore (Good-
Books.com) into her web browser. She connects with the bookstore‚Äôs web site and
logs onto her account, which is handled by a secure portion of the web site that relies
on a private and public key pair (K‚àí1
GoodBooks,KGoodBooks). Because she is a careful
computer user, she veriÔ¨Åes that she has connected to the real GoodBooks.com site
by having her web browser authenticate the identity of the site. Her browser reports
the following:
The identity of this web site has been veriÔ¨Åed by TrueSignatures, Inc., a
certiÔ¨Åcate authority you trust for this purpose.
Using her browser, she looks at the public-key certiÔ¨Åcate of GoodBooks.com and
sees that it is signed by TrueSignatures, Inc. She then makes her selections, places
her order, enters her credit-card information, and then leaves the site.
We formalize Sally‚Äôs thinking using the access-control logic as shown below:
1. KTrueSignatures ‚áíTrueSignatures
Trust assumption
2. KTrueSignatures says KGoodBooks ‚áíGoodBooks
Public key certiÔ¨Åcate
3. TrueSignatures controls KGoodBooks ‚áíGoodBooks
Jurisdiction
4. TrueSignatures says KGoodBooks ‚áíGoodBooks
1, 2 speaks for
5. KGoodBooks ‚áíGoodBooks
3, 4 controls
‚ô¶
The preceding example provides a useful basis for exploring the basis of trust
assumptions regarding root certiÔ¨Åcate authorities. SpeciÔ¨Åcally, it is natural to ask
whether the trust assumption
KTrueSignatures ‚áíTrueSignatures
is appropriate and (if so) why. In Sally‚Äôs case, the assumption seems warranted,
because she is just an ordinary consumer who bought a computer with software from
a reputable Ô¨Årm. Because she is not a high-value target, it is extremely unlikely that
Sally is the target of an elaborate fraud scheme to plant a fraudulent key in place of
the authentic KTrueSignatures in her computer. The company that sold her the computer
is reputable and has safeguards against such fraud (e.g., they use legitimate copies
of operating systems and application software). It is therefore reasonable for her to
trust that the root certiÔ¨Åcation authority‚Äôs key KTrueSignatures is installed correctly on
her machine.

Digital Authentication
119
FIGURE 6.6 Network of certiÔ¨Åcate authorities
On the other hand, suppose that Sally were a military planner using the computer
in a secure military complex. In this case, ruling out an elaborate fraud is not a good
basis for trusting in a root-level certiÔ¨Åcation authority. Instead, the corresponding
trust assumption of the root certiÔ¨Åcation authority would likely be based on a much
more secure process in which a military security ofÔ¨Åcer oversaw the installation of
the root key.
Given the different needs of different entities, it is not unusual to have to account
for several certiÔ¨Åcate authorities. In fact, multiple certiÔ¨Åcate authorities are common
when two or more organizations collaborate. In such situations, one certiÔ¨Åcate au-
thority will vouch for the public key of another certiÔ¨Åcation authority by issuing a
public-key certiÔ¨Åcate for the other authority. Principals who recognize the jurisdic-
tion of the Ô¨Årst authority can use that knowledge to rely upon or trust the public key
of the second certiÔ¨Åcate authority. We illustrate this idea in the following example.
Example 6.2
Suppose that Alice wishes to Ô¨Ånd out Bob‚Äôs public key, either to send him a message
or to verify messages digitally signed by him. Let us also assume that Alice recog-
nizes and trusts the authority of her certiÔ¨Åcate authority CA1 and that she believes
that KCA1 ‚áíCA1. Figure 6.6 shows the network of certiÔ¨Åcate authorities: CA2 is the
certiÔ¨Åcate authority for both CA1 and for Bob.
Because Alice knows the public key of CA1, she is able to verify and willing
to believe the public-key certiÔ¨Åcate for CA2 signed by CA1. Such a certiÔ¨Åcate is
sometimes called a reverse certiÔ¨Åcate, because the signer is certifying a key for an
entity higher up in the hierarchy. Once Alice deduces that KCA2 ‚áíCA2, she is able
to verify and willing to believe the public-key certiÔ¨Åcate for Bob signed by CA2.
This certiÔ¨Åcate‚Äîin which the signer is certifying a key for an entity lower in the
hierarchy‚Äîis sometimes called a forward certiÔ¨Åcate.
The following proof shows the trust assumptions made by Alice, as well as the
authorities she chooses to recognize:

120
Access Control, Security, and Trust: A Logical Approach
1. KCA1 ‚áíCA1
trust assumption
2. KCA1 says (KCA2 ‚áíCA2)
Key CertiÔ¨Åcate
3. KCA2 says (KBob ‚áíBob)
Key CertiÔ¨Åcate
4. CA1 controls (KCA2 ‚áíCA2)
Jurisdiction
5. CA2 controls (KBob ‚áíBob)
Jurisdiction
6. CA1 says (KCA2 ‚áíCA2)
1,2 Derived Speaks For
7. KCA2 ‚áíCA2
4,6 Controls
8. CA2 says (KBob ‚áíBob)
7, 3 Derived Speaks For
9. KBob ‚áíBob
5,8 Controls
The Ô¨Årst line is Alice‚Äôs belief that she knows the public key of CA1. Lines two
and three are the public-key certiÔ¨Åcates for CA2 (signed by CA1) and Bob (signed
by CA2). Lines four and Ô¨Åve represent Alice‚Äôs trust in CA1‚Äôs reliability regarding
certifying that KCA2 is CA2‚Äôs key, and in CA2‚Äôs authority to say truthfully that KBob
is Bob‚Äôs public key. Lines six through nine are the steps necessary for Alice to deduce
that KBob is indeed Bob‚Äôs public key.
‚ô¶
Exercise 6.4.1
Suppose Toni sends Blake a message in the clear that is digitally

signed as follows:
(‚ÄòYou‚Äôre hired!‚Äù,encrypt(K‚àí1
Toni,hash(You‚Äôre hired!‚Äù)))
The above message is sent over an open channel, such as the Internet. Laura in-
tercepts the message and retransmits it to Blake with one crucial change: ‚ÄúYou‚Äôre
hired!‚Äù is changed to ‚ÄúYou‚Äôre Ô¨Åred!‚Äù. Because she wants Blake to think that the
message came from Toni, she reuses Toni‚Äôs digital signature from the message she
intercepted.
What happens when Blake receives the message? Is he able to detect the forgery
or not?
Exercise 6.4.2
Use the CertiÔ¨Åcate VeriÔ¨Åcation rule to give an alternate proof for

the situation in Example 6.2.
Exercise 6.4.3
This question relates to an online-purchase transaction, in which

Zoey logs into a previously established account with her userid-password combina-
tion. She then attempts to purchase an airline ticket, using her credit card. In what
follows, you should interpret CCZ as a principal representing a speciÔ¨Åc credit-card
number/account.
a. Give English translations for each of the following statements, using notions
such as authority (or jurisdiction), signatures, sessions, and access requests.
(a) PwdServer controls (‚ü®zoey,pwd‚ü©‚áíZoey)
(b) Visa controls ((Zoey | CCZ) controls buyticket)
(c) KP ‚áíPwdServer

Digital Authentication
121
(d) KV ‚áíVisa
(e) (‚ü®zoey,pwd‚ü©| CCZ) says buyticket
(f) KP says (‚ü®zoey,pwd‚ü©‚áíZoey)
(g) KV says ((Zoey | CCZ) controls buyticket)
b. Give a formal proof that buyticket can be deduced from the seven assumptions
listed above.
Exercise 6.4.4
Dana has registered her public key KD with the certiÔ¨Åcate authority
CA, whose public key is KCA. She wishes to send a message m to Earl, who does not
know Dana and does not recognize the authority of CA. Fortunately, Dana knows
Earl‚Äôs good friend Finn, who does recognize CA‚Äôs authority.
Furthermore, both Earl and Finn work at Gaggle and have registered their public
keys (KE and KF, respectively) with the company‚Äôs certiÔ¨Åcate authority; the public
key for Gaggle‚Äôs certiÔ¨Åcate authority is KG. (Note that both Earl and Finn have
access to Gaggle certiÔ¨Åcates, but they have never exchanged their public keys with
one another before.)
Dana signs her message using her private key, and sends it to Finn. She asks Finn
to forward it to Earl, which Finn does as follows:
‚Ä¢ Finn uses his private key to sign Dana‚Äôs signed message, and sends the result
to Earl.
‚Ä¢ Finn also gets a copy of Dana‚Äôs public-key certiÔ¨Åcate, and signs and sends that
to Earl.
‚Ä¢ Finn signs a message that indicates what CA‚Äôs public key is, and sends that to
Earl.
‚Ä¢ Finally, Finn signs and sends to Earl a message stating his conÔ¨Ådence in CA‚Äôs
jurisdiction over Dana‚Äôs public key.
a. Express in the access-control logic the four items that Finn sends to Earl.
b. Identify the additional certiÔ¨Åcates, recognitions of authority, and any other
trust assumptions that Earl requires in order to deduce that the message m
truly came from Dana. You should express each of these items in the access-
control logic.
c. Give a formal proof that m can be deduced from your answers to parts (a)
and (b).
d. In your opinion, which if any of the assumptions from part (b) are the most
suspect? Explain your answer.

122
Access Control, Security, and Trust: A Logical Approach
Exercise 6.4.5
Two high-tech companies‚ÄîNanoTech and PicoWare‚Äîare col-
laborating on a joint project called ILLUMINA. The project‚Äôs repository and web
page are housed at PicoWare‚Äôs facilities .
When the two companies initiated this project, they established a special certiÔ¨Å-
cate authority (CA) that both companies would trust with respect to high-level com-
pany keys for the purposes of this project only. The public-private key pairs for the
various entities are as follows:
(KCA,K‚àí1
CA)
CA‚Äôs key pair, trusted by both companies
(KN,K‚àí1
N )
NanoTech‚Äôs key pair, with KN registered with CA
(KP,K‚àí1
P )
PicoWare‚Äôs key pair, with KP registered with CA
This setup allows both NanoTech and PicoWare to maintain their existing company
key infrastructures, and neither company has to blindly trust the other‚Äôs infrastruc-
ture. SpeciÔ¨Åcally:
‚Ä¢ NanoTech‚Äôs public key has been installed on all machines at NanoTech;
therefore NanoTech machines do not need to fetch NanoTech‚Äôs certiÔ¨Åcate
from CA.
‚Ä¢ Likewise, PicoWare‚Äôs public key has been installed on all machines at Pi-
coWare; therefore PicoWare machines do not need to fetch PicoWare‚Äôs cer-
tiÔ¨Åcate from CA.
‚Ä¢ Neither company‚Äôs public key has been installed on any of the other company‚Äôs
machines. Therefore, certiÔ¨Åcates from CA are necessary for any secure cross-
company communication.
‚Ä¢ Each company certiÔ¨Åes its own internal keys: the keys for all NanoTech em-
ployees and NanoTech machines are certiÔ¨Åed by NanoTech, and the keys for
all PicoWare employees and PicoWare machines are certiÔ¨Åed by PicoWare.
(a) The access-control policy for the ILLUMINA project web page grants both
read access (‚ü®read,iwp‚ü©) and write access (‚ü®write,iwp‚ü©) to all members of the
ILLUMINA project. Express this access-control policy as an expression in the
access-control logic.
(b) Stan‚Äîa NanoTech employee‚Äîhas been issued the key pair (KS,K‚àí1
S ). Ex-
press Stan‚Äôs public-key certiÔ¨Åcate as an expression in the access-control logic.
(c) Stan has been assigned to the ILLUMINA project team, and the group database
at PicoWare has been updated to reÔ¨Çect this group membership. If ever re-
quested, the group-database server (GDS) can digitally sign a statement that
attests to Stan‚Äôs group membership; the group-database server‚Äôs key pair is
(KGDS,K‚àí1
GDS).
Express such a certiÔ¨Åcate as an expression in the access-control logic.

Digital Authentication
123
Stan logs into his NanoTech ofÔ¨Åce computer, using his NanoTech-issued smart
card. He then tries to establish an SSL connection to the ILLUMINA WEB PAGE at
PicoWare. To authenticate the SSL connection, Stan‚Äôs smart card uses his private
key to cryptographically sign a response to a challenge from the PicoWare server.
The SSL connection is secured with a session key Kssl. From the point of view of the
web server at PicoWare, the result of successfully establishing this SSL connection
is the following trust assumption:
Kssl ‚áíKS
(‚àó)
(d) PicoWare‚Äôs web server receives (via SSL) Stan‚Äôs request to read the ILLU-
MINA web page. Express this request as an expression in the access-control
logic.
(e) What are the additional certiÔ¨Åcates, recognition of authority, and trust as-
sumptions regarding keys that are necessary for the PicoWare web server to
determine that the read request should be granted?
(f) Give a formal proof that justiÔ¨Åes granting the request. Your only assump-
tions should be your answers to (a) through (e), plus the trust assumption (‚àó).
Exercise 6.4.6
Some access-control decisions are made based on attributes (prop-
erties such as one‚Äôs age or the amount of money in your checking account) rather
than identity. For example, consider what happens when you use a fare card like the
Metro card in Washington, D.C.‚Äôs Metro system:
‚Ä¢ From the standpoint of the rider, there are two resources to be accessed: the
train at the starting destination and the exit to the street at the ending destina-
tion.
‚Ä¢ The Metro card is a certiÔ¨Åcate issued by the individual Metro stations and
possessed by the rider. Note, however, that while the physical card may be the
same before entering and after leaving the Metro station, it is not the same
certiÔ¨Åcate:
‚Äì The information on the Metro card (particularly the amount of money left
on the card) must be updated.
‚Äì The authorities that sign or vouch for the amount on the card are dif-
ferent: the turnstile used at the beginning of a rider‚Äôs trip is different
from the turnstile used at the end of the trip. Distinguishing the separate
turnstiles (more accurately, the different stations) permits one to charge
different amounts for different trips: the exit turnstile can always deter-
mine where the trip originated and how much to deduct from the card.
To see how this works, consider a rider who wishes to travel from Station A to
Station B, which is a trip that costs $1. Let‚Äôs assume that the rider‚Äôs most recent exit
was at Station C, with a balance of $5 on the card.

124
Access Control, Security, and Trust: A Logical Approach
‚Ä¢ The rider inserts the Metro card into the turnstile at Station A. This card is
(ultimately) interpreted as:
Station C says (balance=$5‚àßexit= Station C).
Because the balance is greater than $0, access to the train is granted; the card
is returned to the rider saying (in effect):
Station A says (balance=$5‚àßentry= Station A).
‚Ä¢ The rider boards the train and travels to Station B.
‚Ä¢ The rider inserts his or her Metro card into the turnstile at Station B. Because
the balance is sufÔ¨Åcient to cover the cost of the trip, the turnstile returns the
Metro card with an updated balance of $4.
Assume that the turnstiles at the three stations (A, B, and C) form a private net-
work called MetroNet. There is a single certiÔ¨Åcation authority CA that issues pub-
lic/private key pairs and certiÔ¨Åes membership in MetroNet. The relevant keys are as
follows:
(KCA,K‚àí1
CA)
CA‚Äôs key pair
(KA,K‚àí1
A )
Station A‚Äôs key pair
(KB,K‚àí1
B )
Station B‚Äôs key pair
(KC,K‚àí1
C )
Station C‚Äôs key pair
Digital signatures (based on the public and private keys distributed by the CA) are
used to provide protection against fraudulent cards. Thus, the fare cards are actually
digitally signed certiÔ¨Åcates.
Use expressions in the access-control logic to answer the following questions
regarding the certiÔ¨Åcations, credentials, and access-control policies needed for a
rider to travel from Station A to Station B. Treat attributes such as ‚Äúbalance=$5,‚Äù
‚Äúbalance > $0,‚Äù and ‚Äúexit= Station C‚Äù as simple propositions. Use ‚ü®enter,A‚ü©and
‚ü®exit,B‚ü©to denote entering at Station A and leaving at Station B, respectively.
a. What are the necessary certiÔ¨Åcates, recognition of authority, and trust assump-
tions about keys for a turnstile at Station B to recognize Station A as belonging
to MetroNet?
b. What are the necessary certiÔ¨Åcates, recognition of authority, and trust assump-
tions about keys for a turnstile at Station B to interpret an arbitrary fare card
signed by Station A?
c. What is the access policy of turnstiles at Station A that guard entry to trains?
d. Assuming that a turnstile at Station A grants entry access to a rider, what
certiÔ¨Åcate is given back to the rider as he or she passes through the turnstile?
e. Assuming that a turnstile at Station B allows a rider to exit, what certiÔ¨Åcate is
given back to the rider as he or she passes through the turnstile?

Digital Authentication
125
6.5
Symmetric-Key Cryptography
As mentioned earlier in this chapter, an alternative to public-key cryptography
is shared-key cryptography, also known as symmetric-key cryptography because the
same key is used for both encryption and decryption. If encrypt and decrypt are
particular symmetric-key encryption and decryption functions, then the following
property must hold for all messages m and keys K:
decrypt(K,encrypt(K,m)) = m.
Symmetric-key encryption algorithms‚Äîsuch as the Advanced Encryption Stan-
dard (AES) (National Institute of Standards and Technology, 2001)‚Äîprovide a sig-
niÔ¨Åcant advantage over public-key algorithms in terms of efÔ¨Åciency. They are typi-
cally three orders of magnitude faster than public-key algorithms, because of the un-
derlying operations they employ (i.e., exclusive-or and bit substitutions as opposed
to modulo-n exponentiation). For this reason, it is often preferable to encrypt large
messages or Ô¨Åles using symmetric-key cryptography.
However, shared-key cryptography also has disadvantages that arise from its use
of a single key. The Ô¨Årst problem is one of planning: Alice can send a secure mes-
sage to Bob only if they have previously agreed upon a speciÔ¨Åc key to use. Such an
arrangement should happen via out-of-band communication, such as during a face-
to-face meeting or some other reasonably secure means. In contrast, Alice can send
a secure message to Bob via public-key cryptography even if they have never com-
municated before, as long as they have both published their public keys.
Another problem with shared-key cryptography is one of scale: each principal
must maintain a separate key for every other principal with whom he or she wishes
to communicate. Thus, a collection of n principals would require on the order of n2
keys to support all possible two-way conversations; public-key cryptography would
require only 2n keys (i.e., public and private keys for each principal).
Finally, shared-key cryptography does not support the separation of privacy and
authenticity concerns. For example, suppose that Alice wishes to publish a message
(i.e., allow it to be read by anyone) in a way that provides evidence of her authorship.
We have already seen how to accomplish this goal using public-key cryptography.
However, symmetric-key cryptography does not support this goal directly: anyone
who has knowledge of the key to decrypt the message could forge a new message or
even claim to be the original author.
These disadvantages can be somewhat mitigated through the use of relays to sim-
ulate a public-key infrastructure. We demonstrate how later in Examples 6.4 and 6.5,
but we must Ô¨Årst address one additional pragmatic difference between public-key
and shared-key cryptography. A sender using public-key cryptography can tell the
recipient the relevant key to use. For example, if Alice sends a digitally signed mes-
sage to Bob, she can also send along her public key for his convenience. Likewise,
if she sends a secret message to Bob using his public key, she can notify him which

126
Access Control, Security, and Trust: A Logical Approach
public key she used; that information is useless to anyone who does not know Bob‚Äôs
private key.
If Alice instead uses shared-key cryptography, then she obviously cannot directly
indicate which key Bob should use to decrypt the message. However, she can ac-
complish the same goal by sending along a key identiÔ¨Åer (or key hint) that will be
meaningful only to Bob. A key identiÔ¨Åer might be as simple as an index into a table
of keys that is kept on Bob‚Äôs machine. Alternatively, Bob may have a master key that
he uses to encrypt all his other keys: the key identiÔ¨Åer for a shared key K may be the
result of encrypting K with that master key. The point here is that, when Alice and
Bob set up their shared key, they must also have exchanged their individual key iden-
tiÔ¨Åers associated with that key. However, neither needs to know how the other selects
their key identiÔ¨Åers. The following example demonstrates the use of key identiÔ¨Åers.
Example 6.3
Both Richard and Janet use master keys (KR and KJ, respectively) to encrypt all their
important keys. They have recently set up a shared key KS that will allow them
to communicate with one another securely. At that time, they also exchanged key
identiÔ¨Åers:
‚Ä¢ Richard keeps a table of encrypted keys on his computer, and he likes to use
the indices of this table as his key identiÔ¨Åers. Richard adds the encrypted
shared key (i.e., encrypt(KR,KS)) to this table, and gives the corresponding
index (e.g., 213) to Janet as the key identiÔ¨Åer to use for this key.
‚Ä¢ Janet prefers to use encrypted keys themselves as key identiÔ¨Åers. She therefore
encrypts the shared key with her master key and gives the result to Richard as
her key identiÔ¨Åer for the key: encrypt(KJ,KS).
When Janet sends a secure message to Richard using the shared key KS, she also
sends him his key identiÔ¨Åer (i.e., 213) for the key. Richard is able to look up the
appropriate entry in his table (i.e., encrypt(KR,KS)), decrypt it with his master key
KR to obtain KS, and then use KS to decrypt Janet‚Äôs message.
Similarly, when Richard sends a secure message to Janet using the shared key KS,
he also includes her key identiÔ¨Åer (i.e., encrypt(KJ,KS)). Janet knows to decrypt
this hint with her master key KJ to obtain the key necessary to decrypt Richard‚Äôs
message.
‚ô¶
As the preceding example shows, two principals may have very different key iden-
tiÔ¨Åers for the same shared key. What is important is that each knows the other‚Äôs key
identiÔ¨Åer for the shared key, so that they can always clearly communicate which key
is being used. (In fact, two principals may share multiple keys, using each for a dif-
ferent purpose. The use of key identiÔ¨Åers allows them to indicate which is relevant
for the given communication.)

Digital Authentication
127
We adopt the notational convention1 of using principals‚Äô names or initials as super-
scripts on keys to denote a given principal‚Äôs key hint for a speciÔ¨Åc key. For example,
we use KR
S and KJ
S to denote Richard‚Äôs and Janet‚Äôs key hints for the shared key KS.
We can then associate the key hints with the statements that they make (i.e., with the
messages encrypted by the keys they hint at). Thus, if Janet sends Richard the mes-
sage m encrypted with the shared key Ks, we can express Richard‚Äôs interpretation as
follows:
KR
S says m.
If Richard trusts Janet to keep the shared key KS private, then he should be willing to
associate messages encrypted by KS with Janet. That is, he should be willing to view
the key hint KR
S (and the key it represents) as a proxy for Janet:
KR
S ‚áíJanet.
The following series of examples, inspired by a discussion in (Lampson et al.,
1992), demonstrates how a symmetric-key system can mimic a public-key infras-
tructure by judicious use of key identiÔ¨Åers. The approach hinges on the use of re-
lays, trusted agents that serve as encryption/decryption intermediaries. Just as any
principal can register a public-key with a certiÔ¨Åcate authority in a PKI, the relay
system requires that any principal Q be able to register with a special trusted agent.
Furthermore, the trusted agent must know the relay‚Äôs master key, which the relay
uses for creating its key identiÔ¨Åers (similar to Janet‚Äôs method for key identiÔ¨Åers in
Example 6.3).
Example 6.4
Quinton wishes to use symmetric encryption to send a message that Shelby can read
and verify as being from Quinton. Although Shelby and Quinton do not share a key,
they have both previously established secret-key channels with the trusted relay R by
registering with a trusted agent TA. When Quinton registered, TA created and sent
back to Quinton two items:
1. A secret key Kq to be shared between Quinton and the relay R
2. R‚Äôs key identiÔ¨Åer KR
q = encrypt(Km,Kq) for this key, which TA creates by
encrypting the new key Kq with R‚Äôs master key Km
TA also publishes a secret-key certiÔ¨Åcate for Quinton that contains two components
(we introduce the notation ‚ü®‚ü®m1,m2,...,mk‚ü©‚ü©to denote the concatenation of the k
items m1 through mk):
encrypt(Kta,‚ü®‚ü®KQ
q ,KR
q ,Quinton‚ü©‚ü©),
KR
ta.
1For convenience, we often blur the distinction between keys and key identiÔ¨Åers when using the logic,
such as in the network case studies of Chapter 8. The important point of this discussion is that key hints
can be used both in practice and in the logic to shroud secret keys.

128
Access Control, Security, and Trust: A Logical Approach
The Ô¨Årst item associates the pair (KQ
q ,KR
q ) of key identiÔ¨Åers with Quinton; this item
is signed by TA using the key Kta that it shares with the relay. The second item is
the relay‚Äôs key hint for the shared key Kta, so that anyone who wishes to read the
certiÔ¨Åcate can direct the relay which key to use for decryption. Unlike a public-key
certiÔ¨Åcate, the shared-key certiÔ¨Åcate is not readable by just anybody: only a principal
that knows the secret key Kta (such as TA and the relay) can read it.
Quinton encrypts his message m with the key Kq, sending both the encrypted mes-
sage and the key hint KR
q to Shelby:
encrypt(Kq,m),
KR
q .
Shelby cannot decrypt the message directly, because she does not know the shared
key that KR
q identiÔ¨Åes. However, because Shelby also registered with TA, she shares
a secret key Ks with the relay and knows the relay‚Äôs key hint KR
s for that key. She
therefore forwards these items on to the relay R, along with the key-identiÔ¨Åer pair
(KS
s ,KR
s ). In effect, Shelby is requesting a decryption of the encryption message and
telling the relay how to return the answer. (Shelby can also forward on Quinton‚Äôs
shared-key certiÔ¨Åcate for decryption, if she wishes to authenticate the message. We
leave this possibility for Example 6.5.)
The relay applies its master key Km to the key hint KR
q to obtain the key Kq:
decrypt(Km,KR
q ) = decrypt(Km,encrypt(Km,Kq)) = Kq.
With this key, the relay can decrypt the message encrypt(Kq,m) to obtain the original
message m:
decrypt(Kq,encrypt(Kq,m)) = m.
The relay then repackages this message for Shelby by encrypting m with the shared
key Ks = decrypt(Km,KR
s ), sending back the newly encrypted version as well as the
key hint KS
s :
encrypt(Ks,m),
KS
s .
Upon receiving this package, Shelby uses the key hint KS
s to identify the correct
key (Ks) to use to decrypt the message:
decrypt(Ks,encrypt(Ks,m)) = m.
As a result of this process, Shelby interprets this message as follows:
KS
s says KR
q says m.
That is, the key identiÔ¨Åer KS
s is claiming that the key hint KR
q was used to send
the message m. If Shelby trusts the relay to keep the key Ks secret (i.e., if she be-
lieves that KS
s ‚áíR) and to accurately translate messages (i.e., R controls KR
q says m),
then she will deduce that m was the original intended message she received (i.e.,
KR
q says m).
‚ô¶

Digital Authentication
129
The previous example demonstrated how secret-key cryptography can be used to
exchange messages between principals who have not previously established a shared
secret key. However, the example as it stands does not address authentication con-
cerns: neither the relay nor Shelby are able to associate the key identiÔ¨Åer KR
q with
Quinton at the end of the process described. To do so, they must also make use of
Quinton‚Äôs secret-key certiÔ¨Åcate as demonstrated in the next example.
Example 6.5
By the end of Example 6.4, Shelby had deduced that KR
q says m. If she wishes to
authenticate Quinton as the message‚Äôs author (i.e., to associate the key hint KR
q with
Quinton), then she forwards Quinton‚Äôs secret-key certiÔ¨Åcate to the relay along with
her key-hint pair (KS
s ,KR
s ). As with the original message, the relay can decrypt the
certiÔ¨Åcate, which can be expressed in the logic as follows:
KR
ta says ((KQ
q ,KR
q ) ‚áíQuinton).
That is, the key hint KR
ta claims that the key-hint pair (KQ
q ,KR
q ) is associated with
Quinton. If the relay trusts that the key hint KR
ta (i.e., the key Kta) represents TA and
that TA has jurisdiction over which keys represent which principals, then it will be
able to associate the key hint KR
q with Quinton.
At this point, the relay must create for Shelby a new certiÔ¨Åcate that associates the
key hint KR
q with Quinton. The relay sends the following items to Shelby:
encrypt(Ks,‚ü®‚ü®KR
q ,Quinton‚ü©‚ü©),
KS
s .
As before, Shelby can decrypt the message to interpret the certiÔ¨Åcate as follows:
KS
s says (KR
q ‚áíQuinton).
‚ô¶
As a Ô¨Ånal twist on this scenario, suppose that Shelby wishes to exchange a series
of messages with Quinton. She may wish to set up an authenticated channel between
herself and Quinton, so that they can exchange messages directly without having
to use the relay for continual translations. As the following example demonstrates,
Shelby can enlist the relay‚Äôs help to set up such a channel.
Example 6.6
To set up an authenticated channel with Quinton, Shelby proceeds in the same way
as at the beginning of Example 6.5. She forwards Quinton‚Äôs secret-key certiÔ¨Åcate to
the relay along with her key-hint pair (KS
s ,KR
s ). The relay decrypts this certiÔ¨Åcate
and interprets it as before:
KR
ta says ((KQ
q ,KR
q ) ‚áíQuinton).

130
Access Control, Security, and Trust: A Logical Approach
The relay now creates a new channel for direct communication between Quinton
and Shelby. SpeciÔ¨Åcally, the relay creates a new key K and creates key identiÔ¨Åers for
Quinton and Shelby by encrypting it with their individual keys:
KQ = encrypt(Kq,K),
KS = encrypt(Ks,K).
The relay then creates a new certiÔ¨Åcate for Shelby that associates this new key-hint
pair (KQ,KS) with Quinton:
encrypt(Ks,‚ü®‚ü®KQ,KS,Quinton‚ü©‚ü©),
KS
s .
Shelby can interpret this certiÔ¨Åcate as follows:
KS
s says ((KQ,KS) ‚áíQuinton).
(If Quinton wishes to also authenticate Shelby, then the relay will need to obtain
Shelby‚Äôs original secret-key certiÔ¨Åcate and then generate a new one for Quinton to
use in a similar fashion.)
‚ô¶
Exercise 6.5.1
Consider the certiÔ¨Åcate that Shelby receives at the end of Exam-
ple 6.5.
a. What additional trust assumptions must Shelby make in order to identify Quin-
ton as the sender of the original message from Example 6.4?
b. Another possible interpretation in the logic of that certiÔ¨Åcate is as follows:
KS
s says (KS
s | KR
q ‚áíQuinton).
Given this interpretation, what additional trust assumptions must Shelby make
in order to identify Quinton as the sender of the original message from Exam-
ple 6.4?
c. BrieÔ¨Çy describe the advantages and disadvantages of the two interpretations,
and identify the situations where each might be more appropriate.
Exercise 6.5.2
What additional trust assumptions must Shelby make to associate

the new key K (or its associated key-hint pair) with Quinton in Example 6.6?
Exercise 6.5.3
Suppose that two-way authentication is necessary in Example 6.6
(i.e., Quinton also wishes to authenticate Shelby).
a. What messages need to be sent between Quinton and the relay?
b. Express in the access-control logic how these messages are interpreted, along
with the trust assumptions necessary for the protocol to succeed.

Digital Authentication
131
6.6
Summary
We have already seen that authentication‚Äîthat is, identifying the originator of a
request‚Äîis critical to sound access control. In the digital world, authentication is
made more difÔ¨Åcult by the physical absence of the requesting principal: a request
arrives as a sequence of bits, with only hints of its origin.
In this chapter, we saw how public-key infrastructures support digital authentica-
tion through the use of public and private keys, digital signatures, public-key certiÔ¨Å-
cates, and certiÔ¨Åcate authorities. A PKI supports integrity, privacy, and authentica-
tion by providing the means to associate speciÔ¨Åc keys‚Äîand, subsequently, speciÔ¨Åc
statements‚Äîwith speciÔ¨Åc principals. We explored how to express and reason about
the fundamental concepts of PKI in the access-control logic. We also discussed the
main differences between symmetric and asymmetric cryptography.
The learning outcomes associated with this chapter appear in Figure 6.7.
6.7
Further Reading
The Handbook of Applied Cryptography (Menezes et al., 1997) is a very math-
ematical and authoritative reference for cryptographic algorithms, hash algorithms,
digital signatures, and cryptographic protocols. William Stallings‚Äô book Cryptogra-
phy and Network Security Principles and Practices (Stallings, 2003) is a less com-
prehensive but more accessible reference.

132
Access Control, Security, and Trust: A Logical Approach
FIGURE 6.7 Learning outcomes for Chapter 6
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
‚Ä¢ Describe the characteristics of private-key and secret-key cryptographic
systems.
‚Ä¢ Describe basic principles of trust topologies and networks of certiÔ¨Åcation
authorities.
Application
‚Ä¢ When given protocol descriptions and trust hierarchies, you should be
able to use the access-control logic to describe the protocol and trust
relationships.
‚Ä¢ When given a trust topology, you should be able to determine the neces-
sary certiÔ¨Åcates for establishing trust in a key.
Analysis
‚Ä¢ When given a set of certiÔ¨Åcates, you should be able to formally derive
whether a key is associated with a particular principal.
Synthesis
‚Ä¢ When given a description of a trust topology, you should be able to cre-
ate a formal description of the certiÔ¨Åcates and trust relationships for the
certiÔ¨Åcation authorities.

Chapter 7
Delegation
Central to the study of distributed access control is the notion of delegation. A fun-
damental property of distributed systems is their lack of locality: the originators of
requests, the principals that vouch for various forms of identity and authorizations,
and the reference monitors that guard resources are generally not in the same loca-
tion. Furthermore, requests are typically made by delegates, in many cases processes
operating on behalf of people. Likewise, credentials are signed not by human hands
but with digital signatures. A critical question naturally arises: how do we evaluate
access requests made by principals acting on behalf of others?
In this chapter, we explore how to reason about delegation and evaluate such re-
quests. We extend the access-control logic to formalize delegation, identifying some
important properties along the way.
7.1
Simple Delegations
The simplest of delegations arises when one principal simply passes on a request
from another principal. This situation occurs frequently in distributed systems, such
as when a computer or software application communicates a request on behalf of
a user. We can express such delegations in the access-control logic by quoting, as
demonstrated in the following simple example.
Example 7.1
Linda is running late at work. She, her daughter Emily, and her husband have dinner
reservations that evening for 7 p.m. Linda calls home and her daughter answers the
phone. Linda says, ‚ÄúEmily, I‚Äôm running late at work. Would you ask Dad to call
the restaurant to change our dinner reservations to 7:30 p.m.?‚Äù After hanging up
the phone, Emily tells her father, ‚ÄúDad, Mom just called to say she‚Äôs running late at
work. She wants you to call the restaurant to change the dinner reservations to 7:30.‚Äù
Dutifully, Emily‚Äôs father picks up the phone, calls the restaurant, and changes their
dinner reservation to 7:30 p.m.
‚ô¶
The above situation happens countless times in various forms every day. We can
systematically analyze the structure of the above example from an access-control
133

134
Access Control, Security, and Trust: A Logical Approach
perspective. Consider the following sequence of questions and answers.
‚Ä¢ Who is making the request and on whose behalf is the request being made?
Emily is making the request on behalf of her mother. Emily is quoting Linda
by passing on Linda‚Äôs request to her father.
‚Ä¢ To whom is the request made?
Emily‚Äôs father.
‚Ä¢ What is the request?
Change the dinner reservation to 7:30 p.m.
‚Ä¢ How do we represent Linda‚Äôs request relayed by Emily?
The compound principal Emily | Linda represents Emily quoting Linda. The
actual request is represented by the atomic proposition
‚ü®Change to 7:30 p.m., dinner reservation‚ü©.
Putting everything together, the request is represented as follows:
Emily | Linda says ‚ü®Change to 7:30 p.m., dinner reservation‚ü©
‚Ä¢ Emily‚Äôs father acted as if he believed Emily was correctly relaying Linda‚Äôs
request. How is this belief represented?
Emily‚Äôs father‚Äôs trust in what Emily tells him essentially means that, if Emily
says Linda says ‚Äúchange the dinner reservation to 7:30 p.m.,‚Äù then he con-
cludes Linda has indeed said ‚Äúchange the dinner reservation to 7:30 p.m.‚Äù
Emily‚Äôs father‚Äôs trust in her word is represented as:
(Emily | Linda says ‚ü®Change to 7:30 p.m., dinner reservation‚ü©) ‚äÉ
Linda says ‚ü®Change to 7:30 p.m., dinner reservation‚ü©.
‚Ä¢ Emily‚Äôs father changed the reservation once he heard what he interpreted to
be Linda‚Äôs request. What is the basis for this decision?
Emily‚Äôs father acted as if Linda had jurisdiction over changing the time of their
dinner reservation. This belief is represented as follows:
Linda controls ‚ü®Change to 7:30 p.m., dinner reservation‚ü©.
‚Ä¢ Given the above, is Emily‚Äôs father‚Äôs decision to change the dinner reservation
justiÔ¨Åed?
Yes, and we can prove it using the inference rules of our access-control logic.
Situations like the previous example occur often in many forms. Formally captur-
ing the logic behind this kind of situation enables us to reason about delegations. In
the next section we formalize the notion of delegation illustrated in the above exam-
ple, develop derived inference rules, and prove some important delegation properties.

Delegation
135
7.2
Delegation and Its Properties
Delegation is used frequently in networks of all kinds‚Äîhuman, computer, and
organization‚Äîwhere requests, authorities, and decisions are in different locations.
The idea is that, in many situations, one principal must make a request on behalf of
another principal. For example, an investor cannot directly sell her own stocks but
must instead rely on her stockbroker to do so. Similarly, when a banking customer
attempts to pay his bills online, the associated requests arrive not directly from him
but from his web browser acting on his behalf.
When a principal P makes a request on behalf of principal Q and we believe as a
result of P‚Äôs statement that Q is making the request, then we say that P is Q‚Äôs delegate
(or representative) on that statement. To represent such situations in our logic, we
extend the BNF speciÔ¨Åcation of our logic with the following production rule:
Form ::= (Princ reps Princ on œï)
The reps operator has the same binding precedence as says and controls. Hence, for
example, the formula Jane reps Paul on ‚ü®buy‚ü©‚àß‚ü®sell‚ü©is equivalent to
(Jane reps Paul on ‚ü®buy‚ü©)‚àß‚ü®sell‚ü©.
As with the controls operator in Chapter 2, the reps operator is simply syntactic
sugar for an implication:
P reps Q on œï def
= (P | Q says œï) ‚äÉQ says œï.
Using this deÔ¨Ånition, it is easy to prove the following equivalence:
P reps Q on œï ‚â°P controls (Q says œï).
Essentially, if we believe that P is Q‚Äôs representative on the statement œï, then we
trust P when she says that Q said œï.
There are three crucial properties of the delegation relationship that the formal
deÔ¨Ånition of delegation must capture:
1. A recognized delegate should in fact have the authority to act on behalf of the
principals they represent. That is, if a given policy allows principals to delegate
to others and recognizes that Barb is Holly‚Äôs delegate, then Barb should be able
to act on Holly‚Äôs behalf.
2. Delegates generally should not be able to restrict the scope of their duties as a
principal‚Äôs representative. For example, suppose that Garth delegates to Mary
the task of withdrawing $500 from his checking account and depositing it to
his savings account. Mary should not be able to withdraw the funds without
also depositing them; to do so would be a violation of her responsibilities, not
to mention theft.

136
Access Control, Security, and Trust: A Logical Approach
3. The delegation relationship generally is not transitive: a delegate should not
be able to pass on his responsibilities to someone else.
The Ô¨Årst property‚Äîthat recognized delegates should be able to act on behalf of
the principals they represent‚Äîis reÔ¨Çected by the Reps rule:
Reps
Q controls œï
P reps Q on œï
P | Q says œï
œï
This rule states that if Q is authorized to perform œï, P is recognized as Q‚Äôs delegate
on œï, and P requests œï on Q‚Äôs behalf, then the request for œï should be granted. This
rule can be derived from the deÔ¨Ånition of P reps Q on œï and our other inference rules,
and thus the rule is sound itself.
The second and third properties both state things that should not happen. For
that reason, it is necessary to verify that our deÔ¨Ånition of delegation prohibits the
reduction or passing on of delegation duties. The following two rules, which would
allow the undesired behavior, can easily be shown to be unsound with respect to the
Kripke semantics:
Unsound Rule!
P reps Q on (œï1 ‚àßœï2)
P reps Q on œï1
,
Unsound Rule!
P reps Q on œï
Q reps R on œï
P reps R on œï
.
The Ô¨Årst rule‚Äîif sound‚Äîwould allow a delegate to restrict the scope of his duties.
The second rule‚Äîif sound‚Äîwould allow a delegate to pass on her responsibilities
to someone else.
Figure 7.1 contains the formal deÔ¨Ånition of delegation, along with some useful
logical rules for delegation. Each of these rules can be derived from existing infer-
ence rules; the derivations themselves are left as exercises.
The following example illustrates how delegation can be used to represent com-
mon proxy situations, such as proxies at shareholder meetings. It also illustrates how
principals can adjust the scope of delegation to meet their different needs.
Example 7.2
MagnaProÔ¨Åt Incorporated is a publicly traded company that produces accounting
software. At this year‚Äôs upcoming shareholder‚Äôs meeting, there are three primary
issues to be voted upon:
1. MagnaProÔ¨Åt wishes to make a change to their employee stock-purchase pro-
gram (ESPP), which needs to be approved by a majority of shareholders.
2. MagnaProÔ¨Åt has selected Kim to Ô¨Åll a vacancy on the company‚Äôs board of
directors, a selection that must be ratiÔ¨Åed by a majority of shareholders.
3. A shareholder has proposed a revision to the selection procedure for Mag-
naProÔ¨Åt‚Äôs board of directors; the change will happen only if supported by a
majority of shareholders.

Delegation
137
FIGURE 7.1 Logical rules for delegation
P reps Q on œï def
= P | Q says œï ‚äÉQ says œï
Reps
Q controls œï
P reps Q on œï
P | Q says œï
œï
Rep Controls
A reps B on œï ‚â°(A controls (B says œï))
Rep Says
A reps B on œï
A|B says œï
B says œï
These votes all require a majority of all shareholders to approve them, regardless of
which shareholders actually attend the meeting. Realizing that many shareholders
cannot attend the meeting, MagnaProÔ¨Åt sent a proxy statement to each shareholder,
which included a ballot as well as a designated default proxy (Janet). Shareholders
may select a different proxy if they prefer. Shareholders can use the ballot to specify
precisely how the proxy must vote on their behalf. If a shareholder fails to specify
how the proxy should vote for a particular ballot issue, then the proxy may vote
however he or she pleases.
Aaron and Ruth are both shareholders in MagnaProÔ¨Åt who are unable to attend the
meeting. Aaron is in favor of all three proposals, and he is willing to accept Janet as
his proxy. He therefore records his desired votes, signs the form, and mails it back
to MagnaProÔ¨Åt. In effect, his signed form can be interpreted as follows:
Aaron says (Janet reps Aaron on (‚ü®yes,ESPP‚ü©‚àß‚ü®yes,Kim‚ü©‚àß‚ü®yes,revise‚ü©)).
In this case, Janet must vote yes on all three proposals; she does not have the author-
ity to cast other votes on Aaron‚Äôs behalf.
In contrast, Ruth prefers to have her cousin Lou represent her at the meeting. Ruth
is also in favor of both the stock-purchase plan and of Kim as a director, but she
is undecided on the remaining issue. Because she believes that discussion at the
meeting should be brought to bear on her vote, she decides to let Lou decide her vote
on the shareholder proposal. She therefore records her two votes, leaves the third
issue unmarked, designates Lou as her proxy, and mails the form back. Her signed
form can be interpreted as follows:
Ruth says (Lou reps Ruth on (‚ü®yes,ESPP‚ü©‚àß‚ü®yes,Kim‚ü©‚àß‚ü®yes,revise‚ü©)
‚àßLou reps Ruth on (‚ü®yes,ESPP‚ü©‚àß‚ü®yes,Kim‚ü©‚àß‚ü®no,revise‚ü©)).
In this case, Lou must vote yes on the Ô¨Årst two issues. However, Lou has the au-
thority to cast either a yes or no vote on the proposal to revise the director-selection

138
Access Control, Security, and Trust: A Logical Approach
procedure.
‚ô¶
It is important to note that the deÔ¨Ånition of P reps Q on œï and its properties describe
a particular kind of delegation, in which delegation deals only with statements œï
made by Q. In the following example, we analyze a situation that is often informally
thought of as delegation but that does not correspond to our deÔ¨Ånition.
Example 7.3
Consider the following scenario:
Beth is a director of a not-for-proÔ¨Åt organization that is running a rafÔ¨Çe
as part of a fund-raising event. Beth reaches into a basket of names and
draws Tanya‚Äôs name. Tanya is not present to receive her prize, but she is
still entitled to it. Dawn approaches Beth after the drawing and says that
she is Tanya‚Äôs friend and work colleague and that she will take Tanya‚Äôs
prize to her. Beth, for whatever reason, takes Dawn at her word and
gives Tanya‚Äôs prize to Dawn, with the expectation that Dawn will get
the prize to Tanya.
Here, Dawn is acting on Tanya‚Äôs behalf but not as her delegate; in fact, Tanya is
currently unaware that Dawn is acting on her behalf. Instead, Dawn has asked to
convey Tanya‚Äôs prize to Tanya, in effect making the following statement to Beth:
‚ÄúGive me Tanya‚Äôs prize, and if you give me Tanya‚Äôs prize I will give the
prize to Tanya.‚Äù
From Beth‚Äôs perspective, the request from Dawn corresponds to the following logical
statement:
Dawn says (‚ü®Give Dawn, prize‚ü©‚àß(‚ü®Give Dawn, prize‚ü©‚äÉ‚ü®Give Tanya, prize‚ü©)).
It is straightforward to see that Dawn‚Äôs request does not match the form of a delegate
acting on Tanya‚Äôs behalf. Therefore, Dawn is not Tanya‚Äôs delegate, even though she
is doing a favor for her.
If Beth trusts Dawn, then this trust can be represented as follows:
Dawn controls (‚ü®Give Dawn, prize‚ü©‚àß(‚ü®Give Dawn, prize‚ü©‚äÉ‚ü®Give Tanya, prize‚ü©)).
The Controls inference rule allows Beth to conclude
‚ü®Give Dawn, prize‚ü©‚àß(‚ü®Give Dawn, prize‚ü©‚äÉ‚ü®Give Tanya, prize‚ü©),
and the SimpliÔ¨Åcation and Modus Ponens rules further allow her to conclude
‚ü®Give Tanya, prize‚ü©.
That is, because Beth trusts Dawn, she concludes that she should give Tanya‚Äôs prize
to Dawn and that, by so doing, Tanya will receive her prize.
‚ô¶

Delegation
139
The previous example demonstrated an informal notion of delegation that did not
match our deÔ¨Ånition. However, the same scenario does involve a delegation that does
match our deÔ¨Ånition: Tanya serves as Beth‚Äôs delegate to deliver the prize to Tanya.
The following example highlights the relevant analysis.
Example 7.4
Recall the scenario from Exercise 7.3:
Tanya won a prize in Beth‚Äôs rafÔ¨Çe, but was not present at the rafÔ¨Çe draw-
ing and did not know she won. Thus, Tanya was not able to pick up her
prize herself. Tanya‚Äôs friend and colleague Dawn was there and offered
to take Tanya‚Äôs prize to Tanya, which Beth accepted. Beth asks Dawn to
take the prize to Tanya and give it to her, which Dawn does.
Here, Beth through Dawn is asking Tanya to accept the prize that she won at the raf-
Ô¨Çe. From Tanya‚Äôs perspective, the request she receives from Dawn can be expressed
as follows:
Dawn | Beth says ‚ü®accept, rafÔ¨Çe prize‚ü©.
In this case, we can see that Dawn is Beth‚Äôs delegate. Beth through Dawn is
asking Tanya to accept the prize. If Tanya trusts Dawn about the rafÔ¨Çe prize and also
regards Beth as being in charge of the rafÔ¨Çe, then she in effect believes the following
two statements:
Dawn reps Beth on ‚ü®accept, rafÔ¨Çe prize‚ü©,
Beth controls ‚ü®accept, rafÔ¨Çe prize‚ü©.
The Reps Rule allows Tanya to conclude that she should accept the prize:
‚ü®accept, rafÔ¨Çe prize‚ü©.
‚ô¶
Exercise 7.2.1
Using the formal deÔ¨Ånition of delegation and the Reps Rule, provide

a formal analysis of Example 7.1.
Exercise 7.2.2
Give a formal proof of the Reps inference rule from Figure 7.1.
Exercise 7.2.3
Give a formal proof of the Rep Controls rule from Figure 7.1.
Exercise 7.2.4
Give a formal proof of the Rep Says rule from Figure 7.1.
Exercise 7.2.5
Suppose Naresh and Sunita are both professors in the Department
of Electrical and Computer Engineering. Sunita will be away at a conference during
the department faculty meeting when votes for department chair will be cast. Sunita
wishes to cast her vote for Maja, and she trusts Naresh to be her proxy. Because it‚Äôs
an ofÔ¨Åcial vote, she writes on a piece of paper:

140
Access Control, Security, and Trust: A Logical Approach
FIGURE 7.2 Grace‚Äôs health-care proxy
Health-Care Proxy
1. I, Grace, hereby appoint Norm as my health care agent to
make decisions on whether or not to resuscitate me in the
event I am unable to make my own health care decisions.
2. Grace‚Äôs signature.
3. Witness: I, Kirstin‚Äôs signature, witnessed Grace sign this
proxy of her own free will in my presence.
Naresh is my proxy. I authorize him to cast my vote for Maja as depart-
ment chair.
Sunita turns the paper into George prior to the meeting. George is the faculty chair
who will count the votes at the department meeting.
At the faculty meeting, the vote is called. Naresh casts his vote and then he says,
‚ÄúAs Sunita‚Äôs proxy I am casting her vote for Maja as department chair.‚Äù George,
who is tabulating the votes, notes Naresh‚Äôs vote and then puts down one vote for
Maja cast by Sunita.
Represent Naresh‚Äôs casting of one vote for Maja by Sunita. What must the depart-
ment policies be regarding proxy votes? Give the inference rule that corresponds to
accepting Sunita‚Äôs vote by proxy and prove that it is sound.
Exercise 7.2.6
Consider Grace‚Äôs health-care proxy as shown in Figure 7.2. Her
proxy authorizes Norm to be her delegate in the event she is unable to make her own
health care decisions. Her proxy is witnessed by Kirstin.
In a tragic turn of events, Grace is involved in a car accident that leaves her in a
coma. Grace is taken to the hospital where she is cared for by Dan, her physician.
Norm tells Dan that Grace is not to be resuscitated. Dan, after reading Grace‚Äôs
health-care proxy, writes ‚ÄúDo Not Resuscitate‚Äù on Grace‚Äôs chart.
Answer the following questions:
a. Represent Grace‚Äôs health-care proxy in the access-control logic.
b. Express the hospital‚Äôs policy regarding delegation and the rights of patients
not to be resuscitated in the event they are unable to communicate for them-
selves.
c. Represent Dan‚Äôs decision not to resuscitate Grace as an inference rule.
d. Formally derive and prove Dan‚Äôs inference rule.
Exercise 7.2.7
Demonstrate the unsoundness of the following inference rule:

Delegation
141
Unsound Rule!
P reps Q on (œï1 ‚àßœï2)
P reps Q on œï1
.
Exercise 7.2.8
Demonstrate the unsoundness of the following inference rule:
Unsound Rule!
B controls œï1
A reps B on (œï1 ‚àßœï2)
A|B says œï1
œï1
.
Exercise 7.2.9
Demonstrate the unsoundness of the following inference rule:
Unsound Rule!
P reps Q on œï
Q reps R on œï
P reps R on œï
.
7.3
A Delegation Example: Simple Checking
Delegation is so commonplace that we often do not even recognize its relevance
to a given system. If we are responsible for designing or constructing a system that
uses delegation, however, then we are well served if we understand precisely how
delegation is being used, as well as the assurances that must be in place.
For example, the use of paper checks as payment instruments relies on notions
of delegation, although most check writers never think about that in the course of
conducting their daily business. In this section, we highlight the role of delegation in
the use of paper checks in a simple consumer scenario; we expand on this treatment
in Chapter 8.
For this example, we consider a situation where Alice wishes to purchase goods
from Bob, her local grocer. For simplicity, we assume that Alice and Bob are both
depositors of the same bank.1 Alice wishes to pay Bob, but she does not have sufÔ¨Å-
cient cash on hand (or she chooses not to use it). Instead, she writes a check to Bob,
which is in fact an order to the bank to debit the given amount from her account and
to pay it to Bob. Alice hands the check to Bob, as shown in Figure 7.3. Because Bob
wishes to be paid, he heads to the bank, endorses Alice‚Äôs check by signing the back
of it, and presents the check to the teller for payment.
Suppose that we are in charge of designing the operation of the bank used by Alice
and Bob. Our objective is to come up with a concept of operations that supports the
use of checks written and cashed by bank customers. We would like to know that
these operations are justiÔ¨Åed‚Äîthat is, the operations should be logically sound with
respect to bank policies set by relevant authorities. Accomplishing this goal requires
three steps, which we follow:
1. Give a formal deÔ¨Ånition in the access-control logic of checks and endorsed
checks.
1We address the more complicated situation where Alice and Bob are depositors of different banks in
Chapter 8.

142
Access Control, Security, and Trust: A Logical Approach
FIGURE 7.3 Simple checking
2. Formally describe the bank policies regarding checks.
3. Describe the bank‚Äôs operating rules with respect to checks as inference rules,
which we prove to be sound by deriving them using our existing inference
rules.
7.3.1
Formal DeÔ¨Ånitions of Checks
A check is a signed order from a principal (known as the payer) upon a bank to
draw upon the payer‚Äôs deposit of funds to pay a certain amount of money to another
principal (known as the payee). If P is the payer and Q is the payee, we can represent
a check written by P to Q as follows:
SignatureP says (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).
Here, SignatureP is the signature of P, and hence we must also be able to asso-
ciate the given signature with the payer. That is, we must be willing to believe that
SignatureP ‚áíP.
A check is endorsed when the payee signs the check‚Äîtypically on the back side
of the check‚Äîissued to him or her. If P is the payer and Q is the payee, we can
represent a check written by P and endorsed by Q as follows:
SignatureQ | SignatureP says (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).
Again, we must be willing to associate the given signatures with the relevant indi-
viduals:
SignatureP ‚áíP
SignatureQ ‚áíQ.

Delegation
143
Note that the form of the endorsed check is the same as a delegation-based request:
the payee (or his signature) is claiming that the payee (or her signature) wants her
account to be debited. However, the delegation relationship between the payer and
the payee exists only if the bank accepts checks as legal payment instruments.
7.3.2
Bank Policies on Checks
In our example, Alice and Bob use the same bank, and thus there is a single con-
trolling authority (i.e., the bank owner). The bank‚Äôs policies must allow the use of
checks, which means that it must recognize Alice‚Äôs right to write checks to Bob (i.e.,
Alice‚Äôs right to designate Bob as her representative for withdrawing funds from her
account). Taken together, the following pair of statements says that the bank allows
payer P to write checks to Q (i.e., the bank recognizes Q as P‚Äôs delegate on such
matters):
Bank Owner controls
(Q reps P on (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)),
Bank Owner says
(Q reps P on (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)).
Note that the Ô¨Årst statement refers to the bank owner‚Äôs jurisdiction on the matter;
the second statement indicates the actual bank policy. Using the Controls rule, it is
straightforward to deduce the following single policy statement:
Q reps P on (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).
At Ô¨Årst glance, these statements may appear too strong: the bank recognizes any
possible payee Q as a representative for the payer P. However, recognizing Q as P‚Äôs
delegate is insufÔ¨Åcient for allowing Q to withdraw money from P‚Äôs account. Instead,
Q must be able to present the check with P‚Äôs signature on it, which emphasizes the
importance of being able to associate SignatureP with P herself. We return to this
point in the next subsection.
Furthermore, the bank must have policies that state the conditions under which
Bob should be paid from Alice‚Äôs account. In particular, the bank will want to verify
that P has enough funds in her account (acctP) to cover the check she has written.
This policy along with the bank‚Äôs authority to set this policy can be represented by
the following two statements:
Bank Owner controls
(‚ü®amt covered,acctP‚ü©‚äÉP controls (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)),
Bank Owner says
(‚ü®amt covered,acctP‚ü©‚äÉP controls (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)).
Again, the Controls rule allows us to derive the bank‚Äôs policy that permits cashing
the check, provided that P has sufÔ¨Åcient funds in her account:
‚ü®amt covered,acctP‚ü©‚äÉP controls (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).

144
Access Control, Security, and Trust: A Logical Approach
7.3.3
Operating Rules for Checks
The bank has speciÔ¨Åc procedures‚Äîthat is, a concept of operations‚Äîfor handling
checks, designed to minimize processing costs (both time and money) and the likeli-
hood of fraud. In particular, the bank allows a payment to be made (and the payment
amount debited from the speciÔ¨Åed account) only if the following conditions are met:
1. A check is received with a payer‚Äôs signature SignatureP and endorsed with a
payee‚Äôs signature SignatureQ:
SignatureQ | SignatureP says (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).
2. The signatures on the check correspond to the payer and payee:
SignatureP ‚áíP,
SignatureQ ‚áíQ.
3. There are enough funds in the account to cover the amount of the check:
‚ü®amt covered,acctP‚ü©.
4. The policy allows that, if there are sufÔ¨Åcient funds in the account, then P can
authorize payment to Q and debit the payment from her account:
‚ü®amt covered,acctP‚ü©‚äÉP controls (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).
5. The policy allows Q to be P‚Äôs delegate:
Q reps P on (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).
This concept of operations can be captured by the following derivable (and there-
fore sound) inference rule:
Simple
Checking
SignatureQ | SignatureP says (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)
SignatureQ ‚áíQ
SignatureP ‚áíP
‚ü®amt covered,acctP‚ü©
‚ü®amt covered,acctP‚ü©‚äÉP controls (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)
Q reps P on (‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)
‚ü®pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©
.
The proof of this rule is left as an exercise for the reader.
This extended example illustrates the point we made at the beginning of this sec-
tion: an understanding of delegation can illuminate how existing systems work and
the assumptions or assurances upon which they depend. In the simple-checking sys-
tem, the delegates are the check payees. The analysis depends upon the delegation
instruments‚Äîthat is, the checks that must be integrity checked‚Äîand the statements
that specify which authorities have jurisdiction over which statements. For a sys-
tems designer, each of these items‚Äîsuch as the integrity checking or jurisdiction
statements‚Äîrepresents an assumption of fact. It is essential that we always ask our-
selves, ‚ÄúAre these assumptions reasonable?‚Äù These assumptions reÔ¨Çect the potential
risks to the system. If we deem them unreasonable, then our burden is to identify
assumptions that we consider reasonable and redo the analysis.

Delegation
145
Exercise 7.3.1
Give a formal proof of the Simple Checking Rule.
Exercise 7.3.2
Consider the concept of operations of simple checking as presented
in this section. What are the risks in simple checking as presented?
Exercise 7.3.3
This question concerns a university library and their policies that
allow students and faculty to check out books.
Let us adopt the notation ‚ü®checkout book,P‚ü©to denote a request to check out book
on person P‚Äôs account. Thus, for example,
‚ü®check out Animal Farm,George‚ü©
represents a request to check out the book Animal Farm on George‚Äôs account. An-
swer the following questions:
a. Toby decides to check out the book How Not to Write, using his own library
account. What is the speciÔ¨Åc request‚Äîexpressed as a statement in the access-
control logic‚Äîthat Toby makes when he brings the book to the check-out
counter? (Assume that the librarian recognizes Toby.)
b. Toby‚Äôs account is in good standing, and thus he is authorized to check out this
book. Express this authorization as a statement in the logic.
c. Sidney is a faculty member who wishes to have Toby (her graduate assistant)
check out books on her behalf. The library requires her to sign a proxy form
that states her willingness to be held responsible for all books that Toby checks
out on her behalf. This form, which amounts to a delegation certiÔ¨Åcate, is kept
on Ô¨Åle at the library.
Express both this proxy form and Sidney‚Äôs authority to sign it as statements in
the logic. (Note: In theory, this proxy form applies to all books in the library.
For this exercise, it sufÔ¨Åces to consider the speciÔ¨Åc book Proving Refutations.)
d. Toby heads to the library to check out Proving Refutations on Sidney‚Äôs behalf.
What is the speciÔ¨Åc request‚Äîexpressed as a statement in the access-control
logic‚Äîthat Toby makes when he brings the book to the check-out counter?
(Assume again that the librarian recognizes Toby.)
e. Sidney‚Äôs account is in good standing. What is the library‚Äôs policy (expressed
as a statement in the logic) that governs Toby‚Äôs request to check out Proving
Refutations for Sidney?
Exercise 7.3.4
In recent years, Utopia College has spent a lot of money and effort
in developing its computer infrastructure to support administrative operations in an
integrative fashion across the college. SpeciÔ¨Åcally, they have adopted a public-key
infrastructure, and provided all students and staff with personal key pairs. All public
keys in the college are certiÔ¨Åed by the IT department (ITD), with one exception:
ITD‚Äôs public key (KI) is stored on all ofÔ¨Åcial campus servers.

146
Access Control, Security, and Trust: A Logical Approach
The annual housing lottery for students occurs online, and is scheduled for next
week. Unfortunately, Vera will be participating in a workshop at that time, and so
she wishes to have her friend Henry select a room on her behalf. There are two
separate stages to this process:
Stage One In advance of the lottery, Vera and Henry must each Ô¨Åll out and sign (us-
ing their private keys) an electronic form requesting that Henry be authorized
as Vera‚Äôs room-selection proxy.
The OfÔ¨Åce of Residential Life (ORL) approves only those requests that are
signed by both individuals (i.e., the delegator and the delegate). If the request
is approved, then ORL sends to Henry a proxy certiÔ¨Åcate, signed with ORL‚Äôs
private key, stating that Henry is authorized to make room selections on Vera‚Äôs
behalf.
Stage Two When the lottery begins and it is Vera‚Äôs turn to select a room, Henry
sends the following two items to the Lottery server:
‚Ä¢ The proxy certiÔ¨Åcate obtained from ORL in Stage 1
‚Ä¢ His selection (on Vera‚Äôs behalf), signed with his private key
Let KV, KH, and KO be the public keys of Vera, Henry, and ORL, respectively. Let
‚ü®proxy,Henry,Vera‚ü©denote the request ‚Äúplease assign Henry as Vera‚Äôs proxy,‚Äù and
let ‚ü®select,422‚ü©denote the request ‚ÄúSelect Room 422.‚Äù Thus, for example, the two
electronic proxy-request forms sent in Stage 1 can be interpreted as follows:
KV says ‚ü®proxy,Henry,Vera‚ü©,
KH says ‚ü®proxy,Henry,Vera‚ü©.
Answer the following questions regarding the certiÔ¨Åcations, credentials, and access-
control policies needed for the housing lottery. All answers should be given as ex-
pressions (or formal proofs, where requested) in the access-control logic.
a. ORL will grant the Stage 1 request only if ‚ü®proxy,Henry,Vera‚ü©can be deduced.
What recognition of authority/jurisdiction is implicit in ORL‚Äôs requirement
that both proxy participants sign the form?
b. What additional certiÔ¨Åcates, recognition of authority, and trust assumptions
regarding keys are necessary for the ORL to determine that the proxy request
should be granted in Stage 1? Provide a formal justiÔ¨Åcation for granting the
request.
c. Assuming that ORL grants the proxy request, what is the speciÔ¨Åc proxy certiÔ¨Å-
cate sent to Henry at the end of Stage 1?
d. In Stage 2, Henry presents a room-selection request to the Housing Lottery
server. What is the speciÔ¨Åc form of this room-selection request?

Delegation
147
e. What additional certiÔ¨Åcates, recognition of authority, and trust assumptions
regarding keys are necessary for the Housing Lottery system to determine that
the room-selection request should be granted? Provide a formal justiÔ¨Åcation
for granting the request.
7.4
Summary
Delegation is an essential aspect of distributed systems, but it also complicates the
task of making access-control decisions. Under what circumstances should we act
upon a statement made by a purported delegate?
In this chapter, we examined that question and demonstrated how to describe and
reason about delegation in our access-control logic. We identiÔ¨Åed the role of juris-
diction in such decisions, as well as two important properties of delegation: nonre-
duction of responsibilities and nontransitivity of delegation.
The learning outcomes associated with this chapter appear in Figure 7.4.
7.5
Further Reading
The notion of delegation introduced in this chapter is statement-based: the scope
of a delegate‚Äôs responsibility is explicitly limited by detailing the statements that
the delegate is authorized to make. In contrast, the work of Abadi, Lampson, and
colleagues (Lampson et al., 1992; Abadi et al., 1993) uses a principal-based notion
of delegation that relies on a virtual delegation server. In this setting, the scope
of a delegate‚Äôs responsibility is limited only by the access policy enforced by the
reference monitor.

148
Access Control, Security, and Trust: A Logical Approach
FIGURE 7.4 Learning outcomes for Chapter 7
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
‚Ä¢ Describe the important properties of delegation relationships.
Application
‚Ä¢ When given a particular delegation relationship, you should be able to
use the access-control logic to describe that relationship.
Analysis
‚Ä¢ When given a scenario involving delegation, you should be able to per-
form a formal analysis of it using the access-control logic.
Synthesis
‚Ä¢ When given a scenario involving delegation, you should be able to ex-
press in the logic the primary principals (i.e., the originators of state-
ments), their proxies, and the statements over which the proxies have
jurisdiction.
Evaluation
‚Ä¢ Given a formal analysis of a scenario involving delegation, you should
be able to assess the possible risks associated with the analysis.

Chapter 8
Networks: Case Studies
The concepts of digital authentication and delegation‚Äîdeveloped in the previous
two chapters‚Äîare essential for analyzing networks, both public (such as the world
wide web) and private (such as those used by Ô¨Ånancial institutions). With networks,
the tasks of authenticating principals and authorizing access requests are made more
difÔ¨Åcult by the lack of locality among the policy makers (i.e., authorities), the access
controllers (i.e., reference monitors), and the originators of requests.
Network operations are usually described in terms of protocols, which are sets of
syntactic and semantic rules for exchanging information. These protocols are typi-
cally implemented by a series of requests and responses that produce a desired result,
such as establishing a secure communications channel by exchanging cryptographic
keys or transferring money from one person or account to another. In this chapter, we
demonstrate how to describe such protocols as collections of derived inference rules
using our access-control logic. The inference rules correspond to the operating rules
used by the decision makers within the various protocols. Capturing the underlying
logic of a protocol in this way makes explicit the context in which the protocol oper-
ates, revealing the inherent trust assumptions, jurisdiction of authority, certiÔ¨Åcations,
and delegations upon which it depends.
We apply this approach to develop three different network case studies:
1. SSL and TLS, which support authentication across the web;
2. Kerberos, a widely used authentication and delegation protocol for distributed
systems; and
3. ACH, a backbone of Ô¨Ånancial networks.
Taken together, these case studies illustrate how a formalization of a protocol can
help to explicate its concept of operations (CONOPS).
8.1
SSL and TLS: Authentication across the Web
For web-based transactions, the primary security concerns are authentication, in-
tegrity, and privacy. These concerns are addressed by protocols such as SSL (Secure
Sockets Layer) (Freier et al., 1996) and TLS (Transport Layer Security) (Dierks and
Rescorla, 2006).
149

150
Access Control, Security, and Trust: A Logical Approach
Both SSL and TLS rely on two protocols: a handshake protocol and a record
protocol. The handshake protocol, which we examine in Subsection 8.1.1, is used to
establish a communications channel between a client and a server. This channel may
or may not require the client and the server to authenticate each other. The purpose
of the handshake protocol is to establish a cryptographic key to secure the session.
The record protocol‚Äîwhich we describe in Subsection 8.1.2‚Äîis used to frag-
ment, compress, provide for integrity checking, and encrypt application data trans-
mitted on the web. The record protocol relies on the cryptographic keys and secrets
shared between the client and server during the handshake protocol. The record pro-
tocol relies on key certiÔ¨Åcates for authentication, nonces (i.e., values created for a
particular run of the protocol to guarantee freshness and to prevent replay attacks),
and cryptographic hash functions for generating keys and secrets.
Although the SSL and TLS protocols are very similar, they differ in several areas,
including their available cipher suites and how they compute message authentication
codes and shared secrets. At the level of detail we are considering here, however,
these differences are not signiÔ¨Åcant. As a result, we will not distinguish between
SSL and TLS in the discussion that follows.
8.1.1
Handshake Protocol
The handshake protocol used in SSL and TLS is the starting point for establishing
secure communications. The handshake protocol does the following:
‚Ä¢ Establishes agreement between the client and the server on cryptographic al-
gorithms.
‚Ä¢ Exchanges certiÔ¨Åcates between the client and the server allowing each to au-
thenticate the other.
‚Ä¢ Exchanges cryptographic parameters that enable the client and the server to
create the same master secret.
Figure 8.1 contains a diagram of the handshake protocol, which is read from the
top down. Messages and information‚Äîrepresented by arrows‚Äîare exchanged be-
tween the client and server. The direction of each arrow indicates the Ô¨Çow of the
message; the arrows are annotated by the information sent or the type of message.
The solid arrows represent messages that must appear as part of the SSL or TLS
protocols. The dotted arrows represent messages that are optional, depending upon
whether or not the client and/or server are to be authenticated.
8.1.1.1
Client and Server Hello Messages
To initiate a session, the client sends a client hello message to the server unen-
crypted and in the clear. This message, which initiates a negotiation between the
client and server, contains the client‚Äôs preferences: the version of the TLS/SSL proto-
col the client wishes to use (ideally, the most recent version that the client supports),
a random number randomC generated by the client, a list of the cipher suites that the

Networks: Case Studies
151
FIGURE 8.1 Handshake protocol: Establishing associations among principals and
keys
client supports, and a list of compression methods (if any) that the client supports.
Each cipher suite contains three components related to how the client and server will
exchange information:
1. The key-exchange method, which deÔ¨Ånes how the client and server will agree
upon the shared secret key for the session (e.g., using DifÔ¨Åe-Hellman or Rivest-
Shamir-Adelman (RSA)) and what sort of digital signatures to use (if any)
during the key exchange
2. The symmetric crypto-algorithm to use for data transfer, such as stream ci-
phers, cipher-block-chaining ciphers, or no encryption at all
3. The message-digest function (mdf) to use ‚Äîsuch as MD5 (Message Digest 5)
or SHA1 (Secure Hash Algorithm)‚Äîto create message authentication codes
(MACs) for integrity checking
The server responds by sending to the client a server hello message, also unen-
crypted in the clear. The server hello message contains the result of the negotiation:
the version of TLS/SSL to be used, which is the most recent version that both the
client and server can accommodate; a random number randomS generated by the

152
Access Control, Security, and Trust: A Logical Approach
server; the session identiÔ¨Åer corresponding to this connection; the cipher suite se-
lected by the server, which must be one of those initially supplied by the client; and
the compression method (if any) selected from the list supplied by the client.
None of the parameters exchanged in the client or server hello messages (e.g.,
protocol version number, random numbers, session ID, cipher suite, compression
method) has any interpretation within the calculus. Rather, these items establish the
computational infrastructure for messages, privacy, and integrity checking.
8.1.1.2
Server Authentication
After the client hello and server hello messages are exchanged, both the client
and server will use the same cryptographic algorithms and compression method
for the connection indexed by the session identiÔ¨Åer. At this point, the client and
server can authenticate themselves, if required, using certiÔ¨Åcates for RSA or DifÔ¨Åe-
Hellman public keys. In particular, the server must authenticate itself whenever the
key-exchange method agreed upon requires digital signatures.
If the server must authenticate itself, then it sends a server certiÔ¨Åcate message as
indicated in Figure 8.1. This message contains a list of certiÔ¨Åcates, with the server‚Äôs
RSA or DifÔ¨Åe-Hellman key certiÔ¨Åcate at the head; the remaining certiÔ¨Åcates (if any)
correspond to a chain of certiÔ¨Åcate authorities, each certifying the certiÔ¨Åcate directly
preceding it. Thus, for example, the second certiÔ¨Åcate in the list belongs to the
server‚Äôs certiÔ¨Åcate authority, while the third certiÔ¨Åcate belongs to the authority that
certiÔ¨Åed the server‚Äôs certiÔ¨Åcate authority. As we have seen before, the server‚Äôs cer-
tiÔ¨Åcate is a statement digitally signed by the server‚Äôs certiÔ¨Åcate authority CAS, whose
public key is KCAS:
KCAS says (KS ‚áíServer).
If the client directly recognizes both the public key and the jurisdiction of CAS re-
garding public-key certiÔ¨Åcates, then the client in effect accepts the following two
statements:
KCAS ‚áíCAS,
CAS controls (KS ‚áíServer).
In such a case, the client can conclude that KS is truly the server‚Äôs key:
KS ‚áíServer.
This analysis corresponds to the CertiÔ¨Åcate VeriÔ¨Åcation rule introduced in Chapter 6:
CertiÔ¨Åcate
VeriÔ¨Åcation
Kca ‚áíCertiÔ¨Åcate Authority
Kca says (KP ‚áíP)
CertiÔ¨Åcate Authority controls (KP ‚áíP)
KP ‚áíP
.
In those cases where the client does not recognize the authority of CAS, the ad-
ditional certiÔ¨Åcates in the certiÔ¨Åcate chain may provide the necessary support, as
previously described in Example 6.2.
In some cases, the server‚Äôs key KS might be certiÔ¨Åed only for signing but not for
encryption. In such situations, the server must also send a server key exchange mes-
sage, which conveys either a signed RSA public key that will be used to encrypt the

Networks: Case Studies
153
pre-master secret or signed DifÔ¨Åe-Hellman parameters that can be used to compute
the pre-master secret. In effect, the server creates a temporary public key Kt to be
used by the client, and the server key exchange message corresponds to the following
statement:
KS says ‚ü®use,Kt‚ü©.
Because the server signs the RSA public key or the DifÔ¨Åe-Hellman parameters, the
client has everything it needs to conclude that it in fact is communicating with the
server. It can now associate with the server any messages sent using the public key
KS or temporary public key Kt. The remainder of the handshake protocol deals with
the (optional) authentication of the client.
8.1.1.3
Client Authentication
When the cipher suite dictates, the server sends the client a certiÔ¨Åcate request
message, which contains a list of the types of certiÔ¨Åcates requested (in descending
order of preference) and a list of the distinguished names of acceptable certiÔ¨Åcate
authorities. If no names are speciÔ¨Åed, then the client may send any certiÔ¨Åcate that
matches the types of certiÔ¨Åcates speciÔ¨Åed by the server.
At this point, the server sends a mandatory server hello done message to indicate
that it is done sending messages to support the key exchange and the client can
proceed with its part to support the key exchange.
If the server requested a client certiÔ¨Åcate, the client sends a client certiÔ¨Åcate mes-
sage with a suitable certiÔ¨Åcate, signed by the client‚Äôs certiÔ¨Åcate authority CAC:
KCAC says (KC ‚áíclient).
If the server recognizes both the public key and jurisdiction of CAC, then the server
must accept the following two statements:
KCAC ‚áíCAC,
CAC controls (KC ‚áíclient).
It is straightforward for the server to then conclude that
KC ‚áíclient.
On the other hand, if the server does not request a client certiÔ¨Åcate, then the server
will possess no cryptographic key to authenticate the identity of the client. The client
in this case is anonymous.
8.1.1.4
Key Exchange
The next message is the mandatory client key exchange message. The client gen-
erates a random pre-master secret (typically 48 bits long), encrypts it with the public
key (either from the server‚Äôs certiÔ¨Åcate or the temporary key supplied in the key
exchange message), and sends it on to the server.
At this point, both the client and the server can compute the master secret and the
necessary keys for data encryption and message authentication. The master secret is

154
Access Control, Security, and Trust: A Logical Approach
computed by applying the message-digest function mdf speciÔ¨Åed in the server hello
message to the pre-master secret, randomC, and randomS:
master secret = mdf(pre-master secret,randomC,randomS).
The master secret is used to create the key block, which contains all the cryptographic
keys necessary for encryption and for computing message authentication codes. The
key block is also computed using the message-digest function:
key block = mdf(master secret,randomC,randomS).
The key block contains all of the various keys required by the cipher speciÔ¨Åcation;
each key can be extracted from a speciÔ¨Åed portion of the key block. SpeciÔ¨Åcally,
(client write MAC secret, server write MAC secret, client write key, server write key)
=
key block.
If the server requested a client certiÔ¨Åcate, then the client must also send a certiÔ¨Å-
cate verify message, which contains the digitally signed hash of all previous hand-
shake messages sent or received. This message provides assurance to the server that
it in fact has been communicating with the principal authenticated by the client‚Äôs
certiÔ¨Åcate.
At this point, both the client and the server possess the shared secret key K used
to compute MACs and the write key KW used to encrypt fragments if desired. The
client associates the shared secret MAC key with the server:
K ‚áíServer.
Likewise, the server associates the shared secret MAC key with the client, who may
or may not be authenticated:
K ‚áí
(
anonymoussession ID,
if the client is not authenticated
Authenticated Client,
if the client is authenticated.
Note that, in the case where clients are not authenticated (which is the case for many
business to consumer connections), the server can still identify which anonymous
client it is dealing with by the session identiÔ¨Åer session ID.
At this point, the client sends a change cipher spec message to indicate that sub-
sequent records will be sent using the newly negotiated cipher-suite parameters and
keys. The client then also sends a Ô¨Ånished message.
The handshake protocol ends when the server also sends a change cipher spec
message followed by a Ô¨Ånished message.

Networks: Case Studies
155
FIGURE 8.2 Record protocol: Payload construction
8.1.2
Record Protocol
After the completion of the Handshake Protocol, the client and server can ex-
change data relevant to the transaction (e.g., credit-card numbers or mailing ad-
dresses) using the Record Protocol. Figure 8.2 contains a Ô¨Çowchart of the record
protocol used by SSL and TLS. Application data are divided into fragments of 214
bytes or less, and then each fragment is compressed according to the agreed upon
compression method. The message authentication code (MAC) for a compressed
message fragment M is obtained by applying the agreed-upon message-digest func-
tion mdf as follows:
MAC = mdf(K,M,iopad,seq num),
where K is the shared secret key, iopad is a sequence of padding bits, and seq num
is the sequence number corresponding to the particular fragment being sent.
Figure 8.3 contains a diagram showing how payloads are integrity checked. To see
how it works, suppose that Alice sends Bob a message using the record protocol, and
Bob wants to check the integrity of a message fragment he receives. He Ô¨Årst decrypts
the message using the write key KW obtained through the handshake protocol. The

156
Access Control, Security, and Trust: A Logical Approach
FIGURE 8.3 Record protocol: Payload integrity checking
result has two components: the compressed fragment and the attached MAC. Bob
uses the shared secret key KW and the message-digest function mdf to compute a
new message authentication code, which he compares with the attached MAC. If the
two message authentication codes match, then Bob concludes that (because mdf is a
one-way function) the sender of the message must have possessed the shared secret
key K along with the correct sequence number. That is, Bob concludes that
K says M.
Due to the handshake protocol, Bob associates the shared key K with Alice, and
hence he concludes that the message originated with Alice:
Alice says M.
Exercise 8.1.1
Suppose that the server‚Äôs key KS is certiÔ¨Åed only for signing, and
hence the server sends a key exchange message as explained in the text to convey a
signed public key Kt. Formalize and prove the derived rule that captures the analysis
that allows the client to associate Kt with the server.

Networks: Case Studies
157
FIGURE 8.4 Overview of Kerberos protocol
8.2
Kerberos: Authentication for Distributed Systems
Kerberos (Neuman et al., 2005) is an industry-standard protocol that supports both
authentication and delegation on open networks. At the core of Kerberos lie two spe-
cial services‚Äîthe authentication server (AS) and the ticket-granting server (TGS).
Conceptually, these two services are distinct, but they are typically implemented on
the same machine and collectively referred to as the Key Distribution Center (KDC).
The KDC has access to an authentication database that stores the individual Ker-
beros encryption keys for all users and services in the system. For this reason, the
KDC should be in a physically secure location, as compromise of the authentication
database would compromise the security of the entire system. Both services provide
tickets to access other services: the AS provides ticket-granting tickets (TGTs) for
accessing the TGS, while the TGS offers service-speciÔ¨Åc tickets for other services
(such as a Ô¨Åle system or print server).
For example, consider a user who, throughout the day at work, needs to access the
(non-local) Ô¨Åle system to obtain various Ô¨Åles and also needs to print those Ô¨Åles on a
network printer. Both the Ô¨Åle system and the printer need to authenticate the user to
determine whether or not to provide access to the requested service. For reasons of
both security and practicality, it is undesirable to require the user to repeatedly type
in her password or rescan her smartcard each time she needs to access one of the
services. Kerberos provides for single signon, whereby the user authenticates herself
once and then receives an unforgeable token‚Äîcalled a ticket-grating ticket‚Äîthat she
can subsequently pass along to services to authenticate herself for a speciÔ¨Åed period
of time (e.g., eight hours).
This example provides a backdrop to our explanation of the Kerberos protocol. We
begin with simplest sort of case (i.e., not involving delegation), and then discuss how
the protocol also supports proxies. Figure 8.4 illustrates the sequence of messages
that comprise the Kerberos protocol. We examine these messages in turn.
8.2.1
Initial Authentication Requests
To understand how Kerberos works, let us consider what happens when Ursula
arrives at work in the morning. Having fetched her coffee, Ursula sits down at her
computer and types her userid (ursa) and password to log into the workstation. The

158
Access Control, Security, and Trust: A Logical Approach
Kerberos client on her workstation uses a one-way function to calculate her secret
(symmetric) encryption key Ku, which is the same key that the KDC associates with
ursa in its authentication database. The client also saves this encryption key in the
workstation‚Äôs credentials cache. (The credentials cache should be emptied when the
user logs out of the workstation, to prevent unauthorized use or replay of this key or
other credentials.)
At this point, the Kerberos client sends in the clear an authentication request to
the AS, which contains the following information: the user‚Äôs name (ursa), the name
of the service for which a ticket is being requested (i.e., the ticket-granting service
TGS), and a random nonce nu, which can be used to match requests with responses
and also to detect possible replay attacks. This request may also contain additional
information for the requested ticket-granting ticket (TGT), such as whether it should
be proxiable or forwardable; for now, we ignore those possibilities, but we shall
return to them later. This request corresponds to arrow 1 of Figure 8.4.
When the AS receives this request, it looks up the name ursa in its authentica-
tion database to verify that ursa is a registered user and to obtain Ursula‚Äôs secret
Kerberos key Ku. The AS also generates a session key Ku,tgs to be used between the
workstation‚Äôs Kerberos client and the TGS. The AS then sends two encrypted items
back to the Kerberos client, as represented by arrow 2 in Figure 8.4. The Ô¨Årst uses Ur-
sula‚Äôs secret key Ku to encrypt both the newly generated session key and the client‚Äôs
original nonce (recall that the notation ‚ü®‚ü®m1,m2,...,mk‚ü©‚ü©denotes the concatenation
of the k items m1 through mk):
encrypt(Ku,‚ü®‚ü®Ku,tgs,nu‚ü©‚ü©).
The second item is the ticket-granting ticket (TGT), which is encrypted with the
secret Kerberos key that the authentication database associates with the TGS (Ktgs):
encrypt(Ktgs,‚ü®‚ü®Ku,tgs,ursa,addu,ticket-validity period‚ü©‚ü©).
The TGT contains the newly generated session key, the userid ursa, the network
addresses that are allowed to use this ticket (in our case, only Ursula‚Äôs workstation
address addu), and the period of time for which the TGT is valid (a system-speciÔ¨Åc
value, which may be minutes or hours). Notice that, because the original request
was sent in the clear, the AS has no way to verify that the request really arrived from
a client representing ursa and not from an impostor. Rather, the AS constructs
a response that should be useful only to a client working on her behalf (i.e., with
knowledge of the secret key Ku), as described next.
Upon receiving these two items from the AS, Ursula‚Äôs Kerberos client uses the
shared secret key Ku to recover the original nonce (which associates the AS response
with the initial client response) and the session key to use with the TGS:
Ku says ‚ü®use session key,Ku,tgs‚ü©.
The client associates the shared secret key Ku with the AS and also trusts the AS with

Networks: Case Studies
159
respect to the assignment of session keys to use with the TGS:
Ku ‚áíAS,
AS controls ‚ü®use session key,Ku,tgs‚ü©.
The client stores both the session key and the TGT for future use in requesting spe-
ciÔ¨Åc network services, as described in the next subsection. The client‚Äôs decision to
accept and store the session key corresponds to an instance of an application of the
following derived rule:
Session Key
Receipt
Authority controls ‚ü®use session key,Ksession‚ü©
K says ‚ü®use session key,Ksession‚ü©
K ‚áíAuthority
‚ü®use session key,Ksession‚ü©
.
In fact, this derived rule could be generalized signiÔ¨Åcantly by replacing the action
‚ü®use session key,Ksession‚ü©with a generic statement œï, and in fact we have indirectly
used such a generic rule multiple times throughout this book. However, stating the
explicit rule in this context-speciÔ¨Åc way allows us to explicitly capture the operating
behavior of the Kerberos client.
8.2.2
Requests for Service-SpeciÔ¨Åc Tickets
A little later that morning, Ursula wants to make some changes to the Ô¨Åle book.tex.
When she selects the Ô¨Åle to open, her Kerberos client must obtain a ticket for the ap-
propriate Ô¨Åle server. To do so, the Kerberos client sends to the TGS a request for a
Ô¨Åle-server ticket. This request contains the following components, many of which
we have seen before: the user name ursa, the name of the requested service (in this
case, the Ô¨Åle server FS), a (new) random nonce nw, the TGT previously received and
stored, and an authenticator for the TGT. The authenticator is an encrypted package
designed to demonstrate to the TGS knowledge of the secret session key Ku,tgs:
encrypt(Ku,tgs,‚ü®‚ü®ursa,addu,timestamp‚ü©‚ü©).
The idea here is that only principals knowing the session key Ku,tgs could create
the authenticator. The timestamp indicates freshness and provides protection against
replay attacks, while the user name and address must match those that appear in the
TGT.
Upon receiving this request, the TGS Ô¨Årst veriÔ¨Åes that it is intact and well-formed.
The TGS must then use the TGT and authenticator to determine whether or not to
grant the request.
Recall that the TGT has been created as follows:
encrypt(Ktgs,‚ü®‚ü®Ku,tgs,ursa,addu,ticket-validity period‚ü©‚ü©).
The TGS uses its secret key Ktgs to decrypt the TGT, from which it then extracts
the session key Ku,tgs. In doing so, the TGS also veriÔ¨Åes that the ticket is within its

160
Access Control, Security, and Trust: A Logical Approach
validity period and that the network address in the ticket (addu) matches the network
address of the request itself. If the TGS deems the ticket to be valid, then it has in
effect interpreted the TGT as follows:
Ktgs says (Ku,tgs controls ‚ü®get ticket, FS, ursa@addu‚ü©)
That is, the TGT indicates which session key should be trusted with regards to re-
quests for tickets for user ursa at the network address addu. Furthermore, the TGS
must associate the secret key Ktgs with AS and trust in AS‚Äôs jurisdiction regarding
ticket requests:
Ktgs ‚áíAS,
AS controls (Ku,tgs controls ‚ü®get ticket, FS, ursa@addu‚ü©).
Having veriÔ¨Åed the validity of the TGT, the TGS then uses the extracted session
key Ku,tgs to decrypt the authenticator. For integrity checking, the TGS must verify
that the authenticator timestamp is sufÔ¨Åciently recent (typically, within 5 minutes),
that the user name is the same in both the ticket and authenticator, and that the net-
work address given in the authenticator is the same as that given in the TGT. If
everything checks out, then the TGS effectively interprets the authenticator as the
following statement:
Ku,tgs says ‚ü®get ticket, FS, ursa@addu‚ü©.
That is, the session key is certifying a request to create a ticket for ursa for the Ô¨Åle
server.
At this point, it should be straightforward to see why the TGS determines it ap-
propriate to grant the ticket request. In fact, the TGS decision to grant the request
corresponds to an instance of the following derivable operating rule:
Grant Service
Ticket
K says (Ksession controls ‚ü®get ticket, service, P@add‚ü©)
K ‚áíAS
AS controls (Ksession controls ‚ü®get ticket, service, P@add‚ü©)
Ksession says ‚ü®get ticket, service, P@add ‚ü©
‚ü®get ticket, service, P@add‚ü©
.
It is important to note that the TGS never directly associates the session key with
the apparent requesting user, because its decision is not identity-based. Instead, the
analysis depends on trust in the secrecy of the keys K (Ktgs) and Ksession (Ku,tgs). At
this stage of the protocol, the user‚Äôs name and network address exist only so that the
correct service ticket can be created.
To grant the request, TGS creates a random session key Ku,fs to be used between
Ursula‚Äôs Kerberos client and FS. The TGS sends the following two items back to the
Kerberos client, in much the same way that the AS responded to the initial request
for authentication:
encrypt(Ku,tgs,‚ü®‚ü®Ku,fs,nw‚ü©‚ü©),
encrypt(Kfs,‚ü®‚ü®Ku,fs,ursa,addu,ticket-validity period‚ü©‚ü©).

Networks: Case Studies
161
As before, the Kerberos client can decrypt the Ô¨Årst item to obtain the session key
and to associate this response with the appropriate request; this decision again cor-
responds to an application of the Session Key Receipt rule. The second item is the
Ô¨Åle-service ticket, where Kfs is the secret key that the KDC‚Äôs authentication database
associates with FS.
8.2.3
Requests for Services
The Kerberos client now sends to the Ô¨Åle server FS a request with the following
components: the user name ursa, the Ô¨Åle-service ticket previously received from the
TGS, an authenticator encrypt(Ku,fs,‚ü®‚ü®ursa,addu,timestamp‚ü©‚ü©) for the Ô¨Åle-service
ticket, and any necessary application-speciÔ¨Åc information (in this case, the actual
request to read the Ô¨Åle book.tex). This request corresponds to arrow 5 in Figure 8.4.
The FS proceeds in much the same fashion as the TGS did earlier. SpeciÔ¨Åcally,
the FS uses its secret key Kfs to decrypt the ticket and extract the session key Ku,fs.
However, the interpretation of this ticket is different, because‚Äîunlike the TGS‚Äîthe
Ô¨Åle server must make an identity-based access decision. The Ô¨Åle-service ticket must
therefore provide an association between the secret session key Ku,fs and the user
ursa:
Kfs says (Ku,fs ‚áíursa).
The Ô¨Åle server must also associate the secret key Kfs with the TGS and trust in the
TGS‚Äôs jurisdiction:
Kfs ‚áíTGS,
TGS controls (Ku,fs ‚áíursa).
As before, the FS uses the session key to decrypt the authenticator and performs
the basic integrity check: the authenticator timestamp must be sufÔ¨Åciently recent,
the user name must be the same in both the ticket and authenticator, and the network
address given in the authenticator is the same as that in the ticket. If the authenticator
checks out, then FS interprets the authenticator in the following way:
Ku,fs says ‚ü®read,book.tex‚ü©.
It is straightforward for the FS to conclude that
ursa says ‚ü®read,book.tex‚ü©.
Such a conclusion corresponds to an instance of the following derivable operating
rule:
Service
Request
TGS controls (Ksession ‚áíP)
K says (Ksession ‚áíP)
K ‚áíTGS
Ksession says servicerequest
P says servicerequest
.

162
Access Control, Security, and Trust: A Logical Approach
At this point, the Ô¨Åle server can perform its usual process‚Äîsuch as checking the
appropriate access-control list‚Äîto determine whether or not to grant the request.
8.2.4
Proxiable Tickets
Having described the basic Kerberos interactions, we return to an issue mentioned
brieÔ¨Çy earlier: Kerberos also supports proxiable and forwardable tickets. We de-
scribe the use of proxiable tickets in this section, and leave discussion of forwardable
tickets to Exercise 8.2.5.
In many situations, a network service must make a request of another service on
behalf of its client. For example, suppose our user Ursula decides that she wants to
print out the Ô¨Åle story.pdf. In such a scenario, the print service will need to contact
the Ô¨Åle server on Ursula‚Äôs behalf to obtain the Ô¨Åle. Kerberos has a mechanism for the
creation and use of proxiable tickets, which can be used for proxy requests.
The initial authentication (i.e., Steps 1 and 2) for Ursula is very similar to that
described in Subsection 8.2.1. However, the Ô¨Årst step must also include a request
for a proxiable TGT. Likewise, the TGT that the AS transmits back must have the
proxiable Ô¨Çag set, so that the TGT has the following form:
encrypt(Ktgs,‚ü®‚ü®Ku,tgs,ursa,addu,proxiable,ticket-validity period‚ü©‚ü©).
When Ursula decides to print her Ô¨Åle, her Kerberos client requests (and obtains) a
print-service ticket and session key, exactly as described in Subsection 8.2.2. How-
ever, the client must also request a proxy ticket that allows the print service to make
requests on Ursula‚Äôs behalf from the Ô¨Åle service.. Therefore, the Kerberos client
sends to the TGS a request containing the following components, most of which
we have seen before: the user name ursa, the name of the requested service (in
this case, the Ô¨Åle server FS), a speciÔ¨Åc request for a proxy ticket, the network ad-
dresses of the allowable proxies (in this case, the address addps of the print server),
a random nonce nv, the proxiable TGT previously received and stored, and an au-
thenticator encrypt(Ku,tgs,‚ü®‚ü®ursa,addu,timestamp‚ü©‚ü©) for the TGT. The request may
also include service-speciÔ¨Åc information: in this scenario, the client might include
the speciÔ¨Åc request(s) that the Ô¨Åle service should honor (e.g., ‚Äúread story.pdf‚Äù).
Upon receiving this request, TGS uses its secret key Ktgs to decrypt the TGT, from
which it then extracts the session key Ku,tgs. In doing so, TGS also veriÔ¨Åes that the
ticket is within its validity period and that the network address in the ticket (addu)
matches the network address of the request itself. If the TGS deems the ticket to be
valid, then it has in effect interpreted the TGT as follows:
Ktgs says (Ku,tgs controls
(ursa@addu controls
‚ü®get proxy ticket, FS,ursa@addu,addps,‚Äúread story.pdf‚Äù‚ü©)).
That is, the TGT indicates which session key should be trusted with regards to re-
quests for tickets for user ursa at network address addu.

Networks: Case Studies
163
TGS must also associate the secret key Ktgs with AS and trust in AS‚Äôs jurisdiction
regarding ticket requests:
Ktgs ‚áíAS
AS controls (Ku,tgs controls
(ursa@addu controls
‚ü®get proxy ticket, FS,ursa@addu,addps,‚Äúread story.pdf‚Äù‚ü©)).
The TGS then uses the extracted session key Ku,tgs to decrypt the authenticator.
For integrity checking, the TGS must verify that the authenticator timestamp is suf-
Ô¨Åciently recent (typically, within 5 minutes), that the user name is the same in both
the ticket and authenticator, and that the network address given in the authenticator is
the same as that given in the TGT. If everything checks out, then the TGS effectively
interprets the authenticator as the following statement:
Ku,tgs says (ursa@addu controls
‚ü®get proxy ticket, FS,ursa@addu,addps,‚Äúread story.pdf‚Äù‚ü©)).
That is, the session key is certifying that user ursa is authorized to receive the
requested ticket.
This analysis is identical to that performed for a regular (i.e., nonproxy) ticket,
and it is straightforward to see why TGS determines it appropriate to grant the ticket
request (the Grant Service Ticket rule still applies). To grant the request, as before,
TGS creates a random session key Ku,fs to be used with FS. The TGS sends the
following two items back to W:
encrypt(Ku,tgs,‚ü®‚ü®Ku,fs,nw‚ü©‚ü©),
encrypt(Kfs,‚ü®‚ü®Ku,fs,ursa,addu,addps,proxy,tkt-validity pd,‚Äúread story.pdf‚Äù‚ü©‚ü©).
As before, the Kerberos client can decrypt the Ô¨Årst portion to obtain the session key
and to associate this response with the appropriate request. The second portion is the
Ô¨Åle-service proxy ticket‚Äîin particular, the ticket contains all addresses that may be
used with this ticket, namely ursa‚Äôs workstation and the print server‚Äôs address.
The Kerberos client sends the previously obtained print-service ticket and authen-
ticator to PS as described in the previous subsection; the ticket includes a session
key Ku,ps. The Kerberos client also forwards the newly obtained proxy ticket, along
with the session key Ku,fs (encrypted with the key Ku,ps) to the print service PS. The
printer service PS is then able to make its proxy request to the Ô¨Åle server FS, using
this new session key. In particular, the printing service sends a request with the fol-
lowing components: the user‚Äôs name (ursa), the Ô¨Åle-service ticket that the TGS sent
in step 4, an authenticator encrypt(Ku,fs,‚ü®‚ü®ursa,addps,timestamp‚ü©‚ü©) for the ticket,
and necessary application-speciÔ¨Åc information (in this case, the actual request to read
the Ô¨Åle story.pdf).
The FS proceeds in much the same fashion as before: it uses its secret key Kfs to
decrypt the ticket and extract the session key Ku,fs. Because this ticket is a proxy

164
Access Control, Security, and Trust: A Logical Approach
ticket, however, its interpretation is slightly different:
Kfs says ((Ku,fs ‚áíaddps | ursa) ‚àß(addps reps ursa on ‚ü®read,story.pdf‚ü©)).
That is, the Ô¨Åle-service ticket associates the secret session key Ku,fs with the principal
addps | ursa; it also speciÔ¨Åes that addps is a valid proxy for ursa. To make use
of this credential, FS must also associate the secret key Kfs with TGS and trust in
TGS‚Äôs jurisdiction:
Kfs ‚áíTGS
TGS controls Ku,fs ‚áíaddps | ursa
TGS controls (addps reps ursa on ‚ü®read,story.pdf‚ü©).
As before, FS uses the session key to decrypt the authenticator and performs the
basic integrity checking: the authenticator timestamp must be sufÔ¨Åciently recent, the
user name must be the same in both the ticket and authenticator, and the network
address given in the authenticator must also appear in the ticket. If the authenticator
checks out, then FS interprets the authenticator in the following way:
Ku,fs says ‚ü®read,story.pdf‚ü©.
It‚Äôs straightforward for FS to conclude that
addps | ursa says ‚ü®read,story.pdf‚ü©,
and therefore that
ursa says ‚ü®read,story.pdf‚ü©.
This analysis corresponds to the following operating rule for the Ô¨Åle service:
Proxied
Service
Request
Authority controls (Ksession ‚áíQ | P)
Authority controls (Q reps P on servicerequest)
K says ((Ksession ‚áíQ | P) ‚àßQ reps P on servicerequest)
K ‚áíAuthority
Ksession says servicerequest
P says servicerequest
.
Exercise 8.2.1
Give a formal proof of the Session Key Receipt rule.
Exercise 8.2.2
Give a formal proof of the Grant Service Ticket rule.
Exercise 8.2.3
Give a formal proof of the Service Request rule.
Exercise 8.2.4
Give a formal proof of the Proxied Service Request rule.

Networks: Case Studies
165
Exercise 8.2.5
A disadvantage of proxy tickets in Kerberos is that the Kerberos
client must obtain the proxy tickets and then pass them on to the proxy service. Doing
so requires the client to know the names of all of the services that the proxy will need
to fulÔ¨Åll a speciÔ¨Åc request, which is not always possible on a distributed system.
For this reason, Kerberos also supports forwardable (and forwarded) tickets, which
the client can pass on (or forward) to the services that work on its behalf. The
responsibility for obtaining subsequent tickets falls to the given service.
The Kerberos client must request a forwardable TGT when initially authenticat-
ing a user to Kerberos; the request must indicate the addresses that will be able to
act on the user‚Äôs behalf. Likewise, the TGT that the AS sends back must have the
forwardable Ô¨Çag set. Thus, a forwardable TGT for user name ursa that allows the
addresses addu and addps to make service requests has the following form:
encrypt(Ktgs,‚ü®‚ü®Ku,tgs,ursa,addu,addps,forwardable,ticket-validity period‚ü©‚ü©)
When Ursula decides she wants to print the Ô¨Åle story.pdf, her Kerberos client
sends the forwardable TGT to the print service, along with the session key Ku,tgs. (As
in the case of proxiable tickets, the client must also obtain and pass along a print-
service ticket and authenticator; these together provide the necessary key by which
the session key Ku,tgs can be encrypted.)
The print service PS then has the responsibility for obtaining the necessary ticket
for the Ô¨Åle service, or any other necessary tickets. The request that the print service
sends to the TGS to obtain a Ô¨Åle-service ticket contains the standard information
(user name, requested service, forwardable TGT, and authenticator) plus a request
for a forwarded ticket. The TGS performs an analysis very similar to the previous
cases, while also ensuring that the print service‚Äôs address is one of the addresses
included in the forwardable TGT. If everything checks out, the TGS sends both an
encrypted session key and a forwarded Ô¨Åle-service ticket to the print service:
encrypt(Ku,tgs,‚ü®‚ü®Ku,fs,n‚Ä≤
w‚ü©‚ü©),
encrypt(Kfs,‚ü®‚ü®Ku,fs,U,addu,addps,forwarded,ticket-validity period‚ü©‚ü©).
The print service can now send a request to the Ô¨Åle service, containing the follow-
ing information: the user name, the forwarded Ô¨Åle-service ticket, an authenticator
for the ticket, and any necessary application-speciÔ¨Åc information. The Ô¨Åle service
proceeds in much the same fashion as for other Ô¨Åle-service tickets: it uses its secret
key Kfs to decrypt the ticket and extract the session key Ku,fs.
a. Give interpretations in the access-control logic of the forwarded Ô¨Åle-service
ticket and its authenticator. Note that the interpretation of forwarded tickets
will differ from those for standard or proxy tickets.
b. What additional assumptions‚Äîexpressed as statements in the logic‚Äîmust the
Ô¨Åle service make in order to process this request?
c. Formalize and prove a derivable rule that captures the operating rules of the
Ô¨Åle service in this context.

166
Access Control, Security, and Trust: A Logical Approach
8.3
Financial Networks
The previous two sections explored common protocols that are used to support au-
thentication and delegation across distributed computer systems, including the web.
In this section, we focus on Ô¨Ånancial networks, looking at how electronic clearing-
houses use electronic records and images to eliminate the passing of paper checks.
SpeciÔ¨Åcally, we describe a banking network that uses electronic credits and debits
and the Automated Clearing House (ACH) network, a trusted third-party settlement
service. Detailed descriptions of retail payment systems and the ACH network can
be found in the Federal Financial Institutions Examination Council‚Äôs (FFIEC) hand-
book on Retail Payment Systems (FFIEC, 2004) and the National Automated Clear-
ing House Association‚Äôs guide to rules and regulations governing the ACH network
(National Automated Clearing House Association, 2006). This section builds on the
simple-checking example of Section 7.3.
8.3.1
Electronic Clearinghouses
The banking system uses clearinghouses (or clearing corporations) to collect and
settle individual transactions while minimizing the number of actual payments made
between banks. The FFIEC deÔ¨Ånes a clearing corporation as follows (FFIEC, 2004,
page B-4):
A central processing mechanism whereby members agree to net,
clear, and settle transactions involving Ô¨Ånancial instruments. Clearing
corporations fulÔ¨Åll one or all of the following functions:
‚Ä¢ nets many trades so that the number and the amount of payments
that have to be made are minimized,
‚Ä¢ determines money obligations among traders, and
‚Ä¢ guarantees that trades will go through by legally assuming the risk
of payments not made or securities not delivered. This latter func-
tion is what is implied when it is stated that the clearing corpo-
ration becomes the ‚Äúcounter-party‚Äù to all trades entered into its
system. Also known as a clearinghouse or clearinghouse associa-
tion.
To understand how a clearinghouse works, suppose that the depositors of BankP
and BankQ exchange a total of two checks as follows during the day:
1. Bob (a BankQ depositor) deposits a $100 check from Alice, a BankP depositor.
2. Dan (a BankP depositor) deposits a $250 check from Carol, a BankQ depositor.
BankP and BankQ send the deposited checks to a clearinghouse to total up the trans-
actions between them. The clearinghouse will let each bank know how much it owes

Networks: Case Studies
167
to (or is owed from) other banks to settle their accounts each banking day. In this
example, BankP and BankQ settle by having BankQ transfer $150 to BankP. BankP
will credit $250 to Dan‚Äôs account and debit Alice‚Äôs account by $100. BankQ will
credit $100 to Bob‚Äôs account and debit $250 from Carol‚Äôs account. In the (one hopes
unlikely) event that BankQ is unable to cover its debts, the clearinghouse will pay
BankP what it is owed.
To allow faster check processing, banks typically employ check truncation, which
the FFIEC deÔ¨Ånes as follows (FFIEC, 2004, page B-3): ‚ÄúThe practice of holding a
check at the institution at which it was deposited (or at an intermediary institution)
and electronically forwarding the essential information on the check to the institution
on which it was written. A truncated check is not returned to the writer.‚Äù
To support check truncation, banks and other Ô¨Ånancial institutions use electronic
check conversion (ECC) to convert endorsed physical checks into legally equivalent
check images. During electronic check conversion, magnetic-ink character recogni-
tion (MICR) captures information from a check‚Äôs MICR line, including: the bank‚Äôs
routing number, account number, check number, check amount, and other informa-
tion printed near the bottom of the check in magnetic ink in accordance with gener-
ally applicable industry standards. We represent BankQ‚Äôs ECC of an endorsed check
as follows:
ECCBankQ says (Q | SignatureP) says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).
That is, the result of BankQ‚Äôs ECC process is an electronic claim that Q presented an
integrity-checked signed check for the speciÔ¨Åed amount.
The use of electronically converted checks in the context of check truncation
is known as electronic check presentment (ECP). The FFIEC deÔ¨Ånes ECP as fol-
lows (FFIEC, 2004, page B-6): ‚ÄúCheck truncation methodology in which the paper
check‚Äôs MICR line information is captured and stored electronically for presentment.
The physical checks may or may not be presented after the electronic Ô¨Åles are deliv-
ered, depending on the type of ECP service that is used.‚Äù
BankQ‚Äôs presentation of the electronic check image can be represented as:
BankQ says (ECCBankQ says
(Q | SignatureP) says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)).
That is, BankQ is relaying the output of its ECC process. Presumably, BankQ makes
this statement (i.e., vouches for its electronic check conversion process) only when
it believes the process is working correctly.
Figure 8.5 illustrates the use of a clearinghouse and check images. Bob, the payee,
deposits Alice‚Äôs check at his bank (arrow 2). Bob‚Äôs bank does not present the check
endorsed by Bob to Alice‚Äôs bank directly. Rather, Bob‚Äôs bank truncates the check,
credits Bob‚Äôs account (arrow 3), and sends an electronic version of the check‚Äî
usually in a batch with other orders‚Äîto an Automated Clearing House (ACH) opera-
tor (arrow 4), who sends the image and information to Alice‚Äôs bank (arrow 5) to debit
Alice‚Äôs account (arrow 6). The ACH operator settles the accounts between Alice‚Äôs
and Bob‚Äôs respective banks each day (arrows 7 and 8).

168
Access Control, Security, and Trust: A Logical Approach
FIGURE 8.5 Interbank checking using the ACH electronic clearinghouse
Clearing corporations such as the Federal Reserve Banks guarantee payments for
depository Ô¨Ånancial institutions using services such as FedACH. Consequently, the
Federal Reserve Banks take on the Ô¨Ånancial risk if a depository Ô¨Ånancial institution
(DFI) defaults and has insufÔ¨Åcient funds to settle. Hence, both the ACH and the DFIs
are signatories to transactions. Thus, the ACH is not merely relaying information but
also assuming liability.
We represent the presentation of a check image created by BankQ | ECCBankQ by
an ACH operator ACH as follows:
((ACH & BankQ) | ECCBankQ) | Q says
(SignatureP says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)).
The operator is functioning as a clearing corporation and countersigns the check
image. By so doing, the ACH operator assumes the risk of the transaction if BankP
defaults at settlement time.
In this system, checks are cleared immediately without Ô¨Årst checking payers‚Äô ac-
count balances. If there is an insufÔ¨Åcient balance to cover the amount of the check,
the check in question is returned to the depositor, and the amount is ultimately
charged back to his or her account as a separate transaction. In Figure 8.5, if Alice‚Äôs
check bounces, then Alice‚Äôs check (or a truncated version of her check) is returned
by her bank to the ACH operator to debit Bob‚Äôs bank the amount of the returned
check.

Networks: Case Studies
169
8.3.2
Bank Authorities, Jurisdiction, and Policies
The controlling authorities in this case include the bank owners as well as the
Automated Clearing House (ACH) association, whose rules all members agree to
follow as a condition of membership. We begin by formalizing the necessary poli-
cies for each of these authorities, starting with the paying bank BankP. In the next
subsection, we show how these policies are instrumental to the operating rules these
institutions use for making access-control decisions related to interbank checking.
8.3.2.1
Policies of the Paying Bank
At the individual account level, depositors are allowed to write checks. If there are
insufÔ¨Åcient funds in the account, another transaction will reverse the debit. There-
fore, the policy allowing depositor P to write checks to payee Q can be expressed by
the following two statements:
BankP Owner controls (P controls (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)),
BankP Owner says (P controls (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)).
That is, the owner of BankP not only has the authority to set this policy but also‚Äîas
a condition of ACH membership‚Äîindeed does set it. Furthermore, BankP must have
a policy that allows payees to be the delegates of the payers indicated on checks:
BankP Owner controls (Q reps P on (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)),
BankP Owner says (Q reps P on (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)).
Applying the Controls inference rules to the above statements produces the following
policy statements for BankP:
P controls (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)
Q reps P on (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).
Because BankP is part of the ACH network, it must recognize ACH as a counter-
signer with any ACH network bank that uses ECP:
BankP Owner controls
((ACH & BankQ) | ECCBankQ) reps (Q | SignatureP) on
(‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©),
BankP Owner says
((ACH & BankQ) | ECCBankQ) reps (Q | SignatureP) on
(‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).
Again, the Controls inference rule lets us derive BankP‚Äôs policy regarding check
images and information forwarded to it by ACH:
((ACH & BankQ) | ECCBankQ) reps (Q | SignatureP) on
(‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©).

170
Access Control, Security, and Trust: A Logical Approach
8.3.2.2
Policies of the ACH Operator
The ACH operator accepts transactions only from ACH members. In this example,
the policies for the ACH operator regarding BankP and BankQ are as follows:
BankP controls ‚ü®Pay amt,Q‚ü©.
That is, the ACH operator will accept a payment from BankP as part of the settlement
process. Furthermore, BankQ is allowed to present electronically converted checks
to the operator:
BankQ reps ECCBankQ on
(Q | SignatureP) says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)).
8.3.2.3
Policies of the Receiving Bank
The controlling authority for BankQ is BankQ‚Äôs owner. The following policies
result from recognizing BankP as a banking partner as part of the ACH network
(which can be determined from the MICR line). The Ô¨Årst policy states that checks
drawn upon accounts in BankP may be deposited in BankQ‚Äôs accounts:
BankQ Owner controls
((Q | SignatureP says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)) ‚äÉ
(‚ü®Pay amt,Q‚ü©‚àß(Q controls ‚ü®credit amt,acctQ‚ü©))),
BankQ Owner says
((Q | SignatureP says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)) ‚äÉ
(‚ü®Pay amt,Q‚ü©‚àß(Q controls ‚ü®credit amt,acctQ‚ü©))).
We are assuming here that funds are immediately available (i.e., there is no Ô¨Çoat
time). The second policy states that BankQ recognizes ACH‚Äôs settlement statement:
BankQ Owner controls (ACH controls ‚ü®Pay amt,Q‚ü©),
BankQ Owner says (ACH controls ‚ü®Pay amt,Q‚ü©).
As before, BankQ‚Äôs policies can be obtained by applying the Controls inference
rule to the previous statements:
(Q | SignatureP says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)) ‚äÉ
(‚ü®Pay amt,Q‚ü©‚àßQ controls ‚ü®credit amt,acctQ‚ü©),
ACH controls ‚ü®Pay amt,Q‚ü©.
8.3.3
Bank Operating Rules
There are Ô¨Åve access-control decisions to be made during the course of check pro-
cessing, corresponding to the arrows labeled 3‚Äì8 in Figure 8.5. The Ô¨Årst decision

Networks: Case Studies
171
(arrow 3) is made by the receiving bank BankQ in response to Bob‚Äôs request to de-
posit Alice‚Äôs check, credit his account by the same amount, and have the funds made
available to him. This decision is made by the ACH Check Deposit rule, whose proof
(like those for all other inference rules in this section) is left as an exercise:
ACH Check
Deposit
SignatureQ | SignatureP says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)
SignatureQ says ‚ü®credit amt,acctQ‚ü©
SignatureQ ‚áíQ
(Q | SignatureP says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)) ‚äÉ
‚ü®Pay amt,Q‚ü©‚àß(Q controls ‚ü®credit amt,acctQ‚ü©)
‚ü®Pay amt,Q‚ü©‚àß‚ü®credit amt,acctQ‚ü©
.
The Ô¨Årst two premises correspond to Q‚Äôs presentation of both the endorsed check
and a signed deposit slip, respectively. The third premise corresponds to a signature
check for Q, while the fourth premise reÔ¨Çects BankQ‚Äôs policy to accept checks drawn
on BankP accounts.
The second decision (arrow 4) is also made by BankQ, which must decide whether
or not to electronically present the check endorsed by Q to the ACH operator. If
the check is endorsed by a depositor Q of BankQ, SignatureQ is Q‚Äôs signature, and
the check itself passes whatever integrity check the bank uses, then the check is
converted to its electronic version and passed on to the ACH operator. This decision
is made by the ACH Check Presentation rule:
ACH Check
Presentation
SignatureQ | SignatureP says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)
SignatureQ ‚áíQ
BankQ says (ECCBankQ says (Q | SignatureP) says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)) .
The third decision (arrow 5) is made by the ACH operator to countersign the elec-
tronically converted check and present it to BankP. This decision uses the ACH
Countersign rule:
ACH
Countersign
BankQ says (ECCBankQ says (Q | SignatureP) says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©))
BankQ controls (ECCBankQ says (Q | SignatureP) says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©))
(ACH & BankQ) says (ECCBankQ says ((Q | SignatureP) says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©))) .
Here, the Ô¨Årst premise corresponds to BankQ‚Äôs presentation of the electronic check
image. The second premise indicates that the ACH operator must trust that BankQ is
correctly reporting the result of its ECC process.
The fourth decision (arrows 6 and 7) is made by the paying bank BankP, which
must determine whether to debit the appropriate account and pay toward settlement.
The ACH Check Funding rule reÔ¨Çects this decision:
ACH
Check
Funding
(ACH & BankQ) says (ECCBankQ says ((Q | SignatureP) says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)))
((ACH & BankQ) | ECCBankQ) reps Q on (SignatureP says (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©))
SignatureP ‚áíP
P controls (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)
Q reps P on (‚ü®Pay amt,Q‚ü©‚àß‚ü®debit amt,acctP‚ü©)
(BankP says ‚ü®Pay amt,Q‚ü©)‚àß‚ü®debit amt,acctP‚ü©
.

172
Access Control, Security, and Trust: A Logical Approach
The premises of this rule capture many necessary conditions: BankP‚Äôs must receive
the electronic check image countersigned by the ACH (premise 1), recognize the
countersigned check image as being relayed on behalf of the payee Q (premise 2),
verify the signature of the payer P (premise 3), recognize the payer P‚Äôs authority to
write checks on the account acctP (premise 4), and recognize that the payee Q is the
check writer P‚Äôs delegate in this context (premise 5).
The Ô¨Ånal decision (arrow 8) is made by the ACH operator using the ACH Check
Settlement rule:
ACH Check Settlement
BankP says ‚ü®Pay amt,Q‚ü©
BankP controls ‚ü®Pay amt,Q‚ü©
ACH says ‚ü®Pay amt,Q‚ü©
.
Exercise 8.3.1
Give a formal proof of the ACH Check Deposit rule.
Exercise 8.3.2
Give a formal proof of the ACH Check Presentation rule.
Exercise 8.3.3
Give a formal proof of the ACH Countersign rule.
Exercise 8.3.4
Give a formal proof of the ACH Check Funding rule.
Exercise 8.3.5
Give a formal proof of the ACH Check Settlement rule.
Exercise 8.3.6
Consider the concept of operations of ACH checking as presented
in Figure 8.5. What are the risks?
8.4
Summary
Taken together, the SSL/TLS protocols, Kerberos, and the ACH network all illus-
trate the complexity of providing authentication, authorization, and accountability in
both public and private networks. These industry-standard protocols all employ a
series of requests and responses carefully designed to provide speciÔ¨Åc assurances to
their different participants.
In this chapter, we showed how to use the access-control logic to help explicate
the policies, underlying trust assumptions, and concepts of operations employed by
the various protocol principals. These case studies demonstrate that it is possible to
take real industry standards and describe them concisely in our access-control logic.
The learning outcomes associated with this chapter appear in Figure 8.6.

Networks: Case Studies
173
FIGURE 8.6 Learning outcomes for Chapter 8
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Application
‚Ä¢ Given a derivable inference rule related to a network protocol, you should
be able to give a formal proof of the rule.
Synthesis
‚Ä¢ Given an informal but clear description of a network protocol, you should
be able to construct the derivable inference rules that capture the proto-
col‚Äôs concept of operations.
Evaluation
‚Ä¢ Given a formalization of a protocol in the access-control logic, you
should be able to evaluate the potential risks associated with the pro-
tocol‚Äôs concept of operations.
8.5
Further Reading
The IETF Internet Draft for SSL 3.0 (Freier et al., 1996) and the Requests for
Comments (RFC) for TLS version 1.1 (Dierks and Rescorla, 2006) and Kerberos
version 5 (Neuman et al., 2005) provide detailed descriptions of the messages and
data structures that comprise these protocols. For an informative but less technical
description of Kerberos, we refer interested readers to Bryant and Ts‚Äôo‚Äôs Ô¨Åctional
dialogue describing the design of a Kerberos-like system (Bryant, 1988).


Part III
Isolation and Sharing


Chapter 9
A Primer on Computer Hardware
At the core of cybersecurity lies the need for hardware security. The bits and bytes
that support cyberspace ultimately exist in physical memory and in the registers in-
side processors. These bits move from place to place via data paths guided by timing
and control logic. DeÔ¨Åciencies or discrepancies at the hardware level may signiÔ¨Å-
cantly compromise system security: even the most secure cryptographic protocol is
useless if someone can alter the keys stored in physical memory. As a consequence,
systems engineers must have at least an elementary knowledge of computer hardware
as it relates to security.
In this chapter, we provide a hardware primer for readers who may be unfamiliar
with the concepts and terminology of synchronous hardware design and microcode.
We introduce combinational logic circuits and registers, as well as their descrip-
tions using truth tables, block diagrams, and timing diagrams. We start with basic
deÔ¨Ånitions of components and then show how more complicated functions such as
arithmetic logic units (ALUs) are created.
This primer provides the necessary background for the rest of Part III, whose top-
ics include virtual machines, memory protection, process isolation and sharing, de-
scriptors, and capabilities.
9.1
Ones and Zeros
In hardware design, it is customary to blur the distinction between truth values
and binary values and therefore to use 0 (respectively, 1) to represent both the truth
value false (respectively, true) and the binary value 0 (respectively, 1). The particular
interpretation is determined by the context in which 0 or 1 is used.
Electrically, 0 and 1 are implemented as electrical voltages within a particular
range. For example, some designs consider 0 to be any voltage between 0 and 0.5
volts and 1 to be any voltage between 4.0 and 5.0 volts. Because the only allowed
values are 0 and 1, voltages between 0.5 and 4.0 volts would have no meaning.
All numerical values have a binary representation in hardware. How many differ-
ent values a given processor can represent ultimately depends upon its word length.
A processor‚Äôs word length is the number of bits (i.e., binary digits) in a word, which
is the smallest addressable unit of data in memory. For example, a data memory that
177

178
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.1 Timing diagram for synchronous registers and memory
consists of eight 4-bit words has a word length of four bits, and each 4-bit word has
16 possible values (0000 through 1111). Furthermore, each of the eight 4-bit words
in memory is addressable by a 3-bit address (000 through 111).
The number of bits in data registers is usually the same as the word length of data.
Typical microprocessors have word lengths of 16, 32, or 64 bits. Thus, data registers
and memory would be 16-, 32-, or 64-bits wide. Data buses‚Äîa collection of two or
more related signal lines‚Äîare typically 16-, 32-, or 64-bits wide. In our descriptions,
we are not concerned about the details of word length, so we omit from our diagrams
any mention of word lengths. We assume that words are wide enough to represent
all values of interest.
9.2
Synchronous Design
Most computer hardware is designed using synchronous design principles. That
is, the registers and memory that are used to store values are updated relative to a
central system clock. In this section, we describe the behavior of various kinds of
registers, including synchronous registers, registers with load control, and registers
with tri-state outputs. We also provide a description of combinational-logic design
that includes the development of a simple arithmetic logic unit (ALU). Registers
combined with ALUs are the core of synchronous central processing units (CPUs).
9.2.1
Synchronous Registers
Figure 9.1 contains two diagrams of a synchronous register. On the left of the
Ô¨Ågure, we see a register with two inputs (in and clock) and a single output (out). On
the right is a timing diagram that shows the values of clock, in, and out at different
cycles. At the top of the timing diagram is the clock signal‚Äîa square wave whose
value alternates from 1 to 0 and back again. In synchronous design, clock cycles are
bounded by what are called active clock edges. Figure 9.1 illustrates the case where
the active clock edges are the 0-to-1 clock transitions.

A Primer on Computer Hardware
179
A single clock cycle corresponds to one cycle of the system clock signal that is
bounded by two active clock edges. The simplest approach to synchronous design is
single-phase clocking, in which there is only one system clock and one active clock
edge (i.e., either 0-to-1 or 1-to-0). Our descriptions assume that the active edges are
the 0-to-1 clock edges, and thus the 1-to-0 clock transitions are of no consequence.
Figure 9.1 displays three clock cycles, labeled 1, 2, and 3. In practice, multiple
clocks and phases may be used (e.g., two- and four-phased clocks). However, for
our purposes, single-phase clocking is sufÔ¨Åcient.
The third row in the timing diagram shows the values that appear on the input line
in during cycles 1, 2, and 3: the values v1,v2, and v3, respectively. At the beginning
of each clock cycle, the values may change as a result of an active clock edge. These
delays correspond to the time necessary for circuits to charge or discharge. For
correct design behavior, all input signals to registers and memory must be stable prior
to the next active clock edge. The maximum time to charge or discharge determines
the maximum clock frequency.
The fourth row of the timing diagram shows the values that appear on the output
line out during cycles 1, 2, and 3. In cycle 1, the value of out is labeled X: that is,
the value is either unknown or a don‚Äôt-care value. The values of out in cycles 2 and
3 are v1 and v2, respectively.
The behavior of the register in Figure 9.1 is best described as a delayed repetition:
its output value at cycle n+1 is the same as its input value at cycle n. The relationship
between out and in is described by
out(n+1) = in(n),
where, for any given cycle number i, out(i) and in(i) respectively denote the output
value and input value at cycle i.
9.2.2
Registers with Load Control
The simple register shown in Figure 9.1 stores a potentially different value each
clock cycle. In contrast, Figure 9.2 shows a register with an additional control signal
LD, which controls whether or not values on in are loaded into the register. In this
register, input values are loaded precisely when the value of LD is active (i.e., when
LD‚Äôs value is 1). When LD‚Äôs value is 1 during cycle n, the value of out during cycle
n + 1 will be in(n); when LD‚Äôs value is 0 during cycle n, the value of out in cycle
n+1 remains unchanged. This relationship is shown below:
out(n+1) =
(
out(n),
if LD(n) is 0,
in(n),
if LD(n) is 1.
9.2.3
Registers with Tri-State Outputs
All digital circuits in computer hardware have electrical values corresponding to 0
and 1. Some digital circuits also have the additional capability to have a third value

180
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.2 Timing diagram for synchronous registers with load control
FIGURE 9.3 Tri-state buffer
called a high-impedance state, which is denoted by Hi-Z. Circuits that have three
values‚Äî0, 1, and Hi-Z‚Äîare known as tri-state circuits. Electrically, Hi-Z corre-
sponds to an open switch‚Äîthat is, a path of inÔ¨Ånite resistance (hence the term ‚Äúhigh
impedance‚Äù).
One of the simplest digital circuits with a tri-state capability is a tri-state buffer.
Figure 9.3 shows the standard symbol for a tri-state buffer with two inputs (in and
EN) and a single output (out). Logically, buffers are analogous to switches, and
Figure 9.3 also shows the analogous switch states associated with the output-enable
signal EN. When EN is 0, the tri-state buffer output behaves like an open switch,
where out is electrically isolated from in. When EN is 1, the tri-state buffer output
behaves like a closed switch, where out is electrically connected to in and out = in.
We can express the relationship between inputs and outputs for a tri-state buffer in
clock cycle n as follows:
out(n) =
(
in(n),
if EN = 1,
Hi-Z,
if EN = 0.
Notice that tri-state buffers are effectively instantaneous: the value on the output line
out depends upon the values of in and EN for the very same cycle.
Circuits with tri-state outputs are useful when inputs and outputs come from mul-
tiple sources, as in the following example.

A Primer on Computer Hardware
181
FIGURE 9.4 Tri-state bus example
ENA
ENB
A
B
Data Bus
1
1
X
X
Not permitted: electrical conÔ¨Çict
1
0
valA
X
valA
0
1
X
valB
valB
0
0
X
X
Hi-Z
Table 9.1: Tri-state data-bus values
Example 9.1
Consider a data bus in a processor with two input sources, A and B. These sources are
electrically connected to the data bus using tri-state buffers with the enable controls
ENA and ENB, as shown in Figure 9.4.
Table 9.1 shows the values that appear on the data bus for all combinations of val-
ues for the tri-state buffer controls ENA and ENB. As before, X denotes an unknown
or a don‚Äôt-care value. Note that enabling both tri-state buffer outputs simultaneously
results in an electrical conÔ¨Çict‚Äîfor example, A might be at 0 volts at the same time
that B is at 5 volts‚Äîand must therefore be avoided. When both control signals are
0, then both A and B inputs are electrically isolated from the data bus; if they are
the only inputs, the data bus itself is in a Hi-Z state. The other two cases correspond
to exactly one of A and B being enabled, and the data bus takes on the value of the
enabled input.
‚ô¶
Registers are frequently equipped with tri-state buffers so that their outputs can
be enabled in sequence onto a data- or memory-address bus. Figure 9.5 shows an
implementation of a register with load control and tri-state outputs. SpeciÔ¨Åcally,
the Ô¨Ågure contains a register with load control whose output is fed into a tri-state
buffer. Note that we have left out the clock line: for synchronous designs, the clock
is assumed to go to all registers and is omitted from high-level descriptions.
Table 9.2 fully describes a register with load control and tri-state outputs under
control of a control input EN. When EN is active (i.e., has the value 1), then the
register behaves exactly like the register with load control from Figure 9.2: either
0 or 1 appears on the output, as appropriate. When EN is inactive, however, the
corresponding value of out is Hi-Z, regardless of any other input.

182
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.5 Register with tri-state output
EN(n+1)
LD(n)
in(n)
out(n)
out(n+1)
1
1
1
X
1
1
1
0
X
0
1
0
X
1
1
1
0
X
0
0
0
X
X
X
Hi-Z
Table 9.2: Register with high-impedance output
9.2.4
Combinational Logic and Functions
In addition to registers and memory components, computer hardware consists of
combinational logic and arithmetic functions. Table 9.3 gives the truth-table deÔ¨Å-
nitions of inverters (i.e., not gates), and gates, nand gates, or gates, nor gates, and
exclusive-or gates, while Figure 9.6 shows their symbolic representation.
These
gates are the basic components for all digital computer hardware. Like the tri-state
buffers, pure combinational-logic circuits are effectively instantaneous: their output
values for a given cycle depend on the input values of the very same cycle.
The following example illustrates how the combinational-logic gates are used to
implement a full adder, which serves as a basic building block for developing more
complex hardware components. SpeciÔ¨Åcally, a full adder is a circuit that adds three
single bits to compute a 2-bit result.
Example 9.2
A full adder has three inputs: x, y, and the carry input cin. It also has two outputs: the
carry output cout and the sum s. Table 9.4 provides an arithmetic interpretation for a
x
y
not(x)
and(x,y)
nand(x,y)
or(x,y)
nor(x,y)
xor(x,y)
1
1
0
1
0
1
0
0
1
0
0
0
1
1
0
1
0
1
1
0
1
1
0
1
0
0
1
0
1
0
1
0
Table 9.3: Simple combinational-logic functions

A Primer on Computer Hardware
183
FIGURE 9.6 Basic combinational-logic gates
Inputs
Outputs
Arithmetic Interpretation
x
y
cin
cout
s
2√ócout +s = x+y+cin
1
1
1
1
1
2√ó1+1 = 1+1+1
1
1
0
1
0
2√ó1+0 = 1+1+0
1
0
1
1
0
2√ó1+0 = 1+0+1
1
0
0
0
1
2√ó0+1 = 1+0+0
0
1
1
1
0
2√ó1+0 = 0+1+1
0
1
0
0
1
2√ó0+1 = 0+1+0
0
0
1
0
1
2√ó0+1 = 0+0+1
0
0
0
0
0
2√ó0+0 = 0+0+0
Table 9.4: An arithmetic interpretation of a full adder
full adder, showing all eight combination of inputs, the corresponding carry and sum
outputs, and the arithmetic relationship that holds between the inputs and outputs.
For example, the Ô¨Årst line of the table corresponds to the case where x, y, and cin
all have the value 1. Their base-10 sum x + y + cin = 1 + 1 + 1 is 310, which can
be expressed in base-2 as 112; this result is identical to that obtained by looking at
the bit sequence couts = 11.1 Similarly, the Ô¨Åfth line of the table covers the case
where x has the value 0, while both y and cin have the value 1. Their base-10 sum
x + y + cin = 0 + 1 + 1 is 210, which can be expressed in base-2 as 102; again, this
result is identical to that obtained by looking at the bit sequence couts = 10.
Using this table, one can verify that the carry and sum output functions are cor-
rectly deÔ¨Åned using combinational logic as follows:2
cout = or(and(x,y),and(x,cin),and(y,cin)),
s = xor(xor(x,y),cin).
Figure 9.7(a) contains a block-diagram representation of a full adder with inputs
Xi, Yi, and Ci, and outputs Ci+1 and Si. Figure 9.7(b) contains a gate-level imple-
mentation of the full adder, which comes directly from the sum and carry output
functions deÔ¨Åned above.
‚ô¶
1Following standard conventions, we use the notation bn‚àí1bn‚àí2 ¬∑¬∑¬∑b0 to indicate a sequence of bits that
starts with bn‚àí1 and terminates with b0.
2In our diagrams and notations, we often make use of n-ary operators and gates‚Äîsuch as or(x,y,z)‚Äî
rather than expanding them to a series of binary operators or gates, such as or(x,(or(y,z))). Such presen-
tations are more concise, and are common among hardware designers.

184
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.7 Full-adder implementation
Basic combinational-logic gates and full adders are used to create more complex
arithmetic and logic units, as we show in the next subsection.
9.2.5
Arithmetic Logic Units
In the previous example, we showed how combinational-logic gates are combined
to form basic arithmetic building blocks such as full adders. These basic arithmetic
building blocks are combined with combinational-logic functions to create arithmetic
logic units (ALUs), which are the computational core of central processing units
(CPUs).
A simple example is an n-bit adder, which adds two n-bit words (X and Y) and
a carry-input bit C0 to produce an (n + 1)-bit result that consists of a carry-output
bit Cn and an n-bit sum S. Figure 9.8(a) contains a block diagram of an n-bit adder.
The n-bit adder is easily implemented by a linear array of full adders called a ripple
adder (so called because the carry bits ‚Äúripple‚Äù from the least signiÔ¨Åcant bit to the
most signiÔ¨Åcant bit). Figure 9.8(b) shows an implementation of an n-bit adder by an
n-bit ripple adder.
The arithmetic relationship between the inputs (X = Xn‚àí1 ...Xo, Y = Yn‚àí1 ...Y0,
and C0) and the outputs (Cn and S = Sn‚àí1 ...S0) is as follows:
2n √óCn +2n‚àí1 √óSn‚àí1 +¬∑¬∑¬∑+20 √óS0 =
2n‚àí1 √óXn‚àí1 +2n‚àí2 √óXn‚àí2 +¬∑¬∑¬∑+20 √óX0+
2n‚àí1 √óYn‚àí1 +2n‚àí2 √óYn‚àí2 +¬∑¬∑¬∑+20 √óY0 +20 √óC0.
That is, the binary value of the (n+1)-bit sequence CnSn‚àí1 ¬∑¬∑¬∑S0 is equal to the sum
of the inputs‚Äô binary values.
The following example illustrates this relationship between inputs and outputs.

A Primer on Computer Hardware
185
FIGURE 9.8 An n-bit adder and ripple adder
Example 9.3
Suppose we have a 4-bit adder with the following inputs:
X = X3X2X1X0 = 10002 = 810,
Y = Y3Y2Y1Y0 = 10012 = 910,
C0 = 12 = 110.
The result of adding X,Y, and C0 together is 1810, which in binary is given by the
carry output and sum bits as follows:
C4S3S2S1S0 = 100102 = 1810.
One can also verify that the ripple-adder implementation computes this sum cor-

186
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.9 ALU implementation
rectly, as follows:
C1S0 = X0 +Y0 +C0 = 0+1+1 = 102,
C2S1 = X1 +Y1 +C1 = 0+0+1 = 012,
C3S2 = X2 +Y2 +C2 = 0+0+0 = 002,
C4S3 = X3 +Y3 +C3 = 1+1+0 = 102.
‚ô¶
Finally, we show how to construct a simple ALU with four operations from an
n-bit adder and combinational-logic gates that control the inputs to the adder.
Example 9.4
Figure 9.9 shows a high-level block diagram of a four-function ALU. The ALU has
two n-bit data inputs (A and B), an n-bit data output (out), and two one-bit ALU-
function inputs (F1 and F0). Taken together, the function inputs F1 and F0 specify
which one of four operations the ALU is to perform: Inc B (i.e., pass the value of
B+1 to out), Pass A (i.e., pass A to out), Pass B (i.e., pass B to out), and ADD A B
(i.e., pass the value of A+B to out). The ALU output at clock cycle i is described as
follows:
out(i) =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
B(i)+1,
if F1(i)F0(i) = 00,
A(i),
if F1(i)F0(i) = 01,
B(i),
if F1(i)F0(i) = 10,
A(i)+B(i)
if F1(i)F0(i) = 11.
Recall that the inputs and outputs of the n-bit adder obey the following relation:
CnSn‚àí1 ¬∑¬∑¬∑S0 = (Xn‚àí1 ¬∑¬∑¬∑X0)+(Yn‚àí1 ¬∑¬∑¬∑Y0)+C0.
The four ALU operations are implemented by an n-bit adder by appropriately con-
trolling the ALU inputs, as shown in Figure 9.9. Note that the X and Y inputs to

A Primer on Computer Hardware
187
ALU Operation & Code
Internal Control Signals
Function
F1
F0
PassB
PassA
C0
out
Inc B
0
0
1
0
1
B + 1
Pass A
0
1
0
1
0
A
Pass B
1
0
1
0
0
B
ADD A B
1
1
1
1
0
A+B
Table 9.5: ALU functions
the adder both come from and gates: each one of the n input lines to the n-bit X
(respectively, Y) adder input comes from an and gate that combines an individual
bit of A (respectively, B) with PassA (respectively, PassB). That is, the following
relationships hold:
Xi = and(Ai,PassA),
Yi = and(Bi,PassB).
The net result is that X is 0 whenever PassA is set to 0, and otherwise X is A
(and similarly for Y, PassB, and B).
For simplicity, we write and(A,PassA) to
mean the bit-wise ‚Äúand‚Äùing of PassA with the individual bits of A, and similarly
for and(B,PassB):
and(A,PassA) = and(An‚àí1,PassA)¬∑¬∑¬∑and(A0,PassA),
and(B,PassB) = and(Bn‚àí1,PassA)¬∑¬∑¬∑and(B0,PassA).
Thus, provided that PassA,PassB, and C0 are appropriately controlled, the n-bit
adder produces each of the four ALU functions, as follows:
CnSn‚àí1 ¬∑¬∑¬∑S0 = (Xn‚àí1 ¬∑¬∑¬∑X0)+(Yn‚àí1 ¬∑¬∑¬∑Y0)+C0 =
Ô£±
Ô£¥
Ô£¥
Ô£¥
Ô£≤
Ô£¥
Ô£¥
Ô£¥
Ô£≥
and(A,0)+and(B,1)+1 = B+1,
if PassA = 0,PassB = 1,C0 = 1,
and(A,1)+and(B,0)+0 = A,
if PassA = 1,PassB = 0,C0 = 0,
and(A,0)+and(B,1)+0 = B,
if PassA = 0,PassB = 1,C0 = 0,
and(A,1)+and(B,1)+0 = A+B,
if PassA = 1,PassB = 1,C0 = 0.
The task of controlling PassA, PassB, and C0 falls to the control-logic compo-
nent on the righthand side of Figure 9.9. Using both this Ô¨Ågure and Table 9.5, it is
straightforward to verify that the following logic will properly control each signal:
PassA = F0,
PassB = nand(not(F1),F0),
C0 = and(not(F1),not(F0)).
‚ô¶

188
Access Control, Security, and Trust: A Logical Approach
Exercise 9.2.1
Consider the following logical expression:

out = and(or(A,B),C)
a. Devise a truth table showing the value of out for all possible input combina-
tions.
b. Draw a schematic diagram of gates showing a hardware implementation of
out.
Exercise 9.2.2
Consider the following truth table:
x
y
z
out
0
0
0
0
0
0
1
0
0
1
0
0
0
1
1
1
1
0
0
0
1
0
1
0
1
1
0
1
1
1
1
1
a. Give a logical formula that expresses out as a function of x,y, and z.
b. Implement out as deÔ¨Åned above using only inverters, and gates, and or gates.
Try to use as few gates as possible.
c. Implement out as deÔ¨Åned above using only nand gates. As a hint, the following
relationship (known as DeMorgan‚Äôs Law) is helpful:
not(and(A,B)) = or(not(A),not(B)).
Exercise 9.2.3
Positive and negative numbers are typically represented using a
two‚Äôs-complement representation. In a two‚Äôs-complement representation, the most
signiÔ¨Åcant bit xn‚àí1 has a weight of ‚àí2n‚àí1; in contrast, an unsigned-binary repre-
sentation gives the most signiÔ¨Åcant bit a weight of 2n‚àí1. Thus, the following rela-
tionships hold, depending upon whether the representation is two‚Äôs complement or
unsigned binary:
(xn‚àí1xn‚àí2 ¬∑¬∑¬∑x0)2
=
(
2n‚àí1 √óxn‚àí1 +2n‚àí2 √óxn‚àí2 +¬∑¬∑¬∑+20 √óx0,
in unsigned binary,
(‚àí2n‚àí1 √óxn‚àí1)+2n‚àí2 √óxn‚àí2 +¬∑¬∑¬∑+20 √óx0,
in two‚Äôs complement.
a. Fill in the following table, giving both the unsigned binary and the two‚Äôs-
complement interpretations of all 4-bit binary numbers.

A Primer on Computer Hardware
189
x3
x2
x1
x0
unsigned binary
two‚Äôs-complement
0
0
0
0
0
0
0
1
0
0
1
0
0
0
1
1
0
1
0
0
0
1
0
1
0
1
1
0
0
1
1
1
1
0
0
0
1
0
0
1
1
0
1
0
1
0
1
1
1
1
0
0
1
1
0
1
1
1
1
0
1
1
1
1
b. Using the table from part (a), devise a method that uses only logical negation
and addition to negate any two‚Äôs-complement number. That is, devise a method
that, when given a two‚Äôs-complement bit sequence
xn‚àí1xn‚àí2 ¬∑¬∑¬∑x0
whose base-10 value is X10, produces a two‚Äôs-complement bit sequence
yn‚àí1yn‚àí2 ¬∑¬∑¬∑y0
whose base-10 value is ‚àíX10.
c. What is the maximum positive number that can be represented in n bits, using
two‚Äôs-complement representation? What is the most negative number that can
be represented in n bits, using two‚Äôs-complement representation?
d. Using only full adders and basic combinational-logic gates as components,
devise a 4-function ALU with the following speciÔ¨Åcations:
F1
F0
Function
0
0
A+B
0
1
A‚àíB
1
0
‚àíA+B
1
1
‚àíA‚àíB
You should assume a two‚Äôs-complement interpretation.

190
Access Control, Security, and Trust: A Logical Approach
9.3
Microcode
In the previous section, we described several forms of synchronous registers, in-
troduced basic combinational logic gates, showed how n-bit adders are built, and de-
signed a simple ALU. Taken together, these components are sufÔ¨Åcient for describing
the operation of processors in terms of their data paths and control paths. A cen-
tral processing unit‚Äôs (CPU) data and control paths determine which computations
are performed and in what order. In this section, we discuss how a CPU‚Äôs data and
control paths are controlled using instructions called microcode or microinstructions.
9.3.1
Data Paths and Control Paths
Data paths consist of registers and memory that store data, the paths that data can
follow, and the arithmetic and logic functions on those paths. Control paths consist
of the control lines that direct the timing and Ô¨Çow of data, the control lines that direct
which functions are performed by arithmetic and logic units, and the sequence in
time that values appear on the control lines.
Figure 9.10 shows a simple data path and control path. The data path consists of
the following seven components:
‚Ä¢ The ALU
‚Ä¢ Three registers (Reg1, Reg2, and ACC, which is known as the accumulator)
‚Ä¢ A bus (Data Bus) used to supply inputs to the a input of the ALU from the
outputs of Reg1 and Reg2
‚Ä¢ A data line connecting the output of ACC to the b input of the ALU.
The control path consists of the seven control signals (LD1,LD2,LDACC,EN1,EN2,
F0, and F1) and the timing and control unit (TCU).
The TCU accepts a machine-language instruction and converts it to a sequence
of microinstructions, which are very Ô¨Åne-grained instructions that correspond to the
CPU‚Äôs data path. Microinstructions typically take only a single clock cycle to exe-
cute, whereas machine-language instructions usually take two or more clock cycles.
For example, the data and control paths in Figure 9.10 might have the following
microinstructions, all of which are executable in a single clock cycle:
‚Ä¢ ACC ‚ÜêReg1: load ACC with the contents of Reg1
‚Ä¢ ACC ‚ÜêReg2: load ACC with the contents of Reg2
‚Ä¢ ACC ‚ÜêReg1 + ACC: add the contents of Reg1 to the contents of ACC and
store the result in the ACC
‚Ä¢ ACC ‚ÜêReg2 + ACC: add the contents of Reg2 to the contents of ACC and
store the result in the ACC

A Primer on Computer Hardware
191
FIGURE 9.10 Simple data- and control-path example
‚Ä¢ Reg1 ‚ÜêACC: store the contents of ACC into Reg1
‚Ä¢ Reg2 ‚ÜêACC: store the contents of ACC into Reg2
‚Ä¢ Reg1 ‚ÜêReg2: store the contents of Reg2 into Reg1
‚Ä¢ Reg2 ‚ÜêReg1: store the contents of Reg1 into Reg2
The TCU implements each microinstruction by appropriately setting the values of the
control-path signals to perform the speciÔ¨Åed operation, as illustrated in the following
example.
Example 9.5
Suppose that the ALU of Figure 9.10 is the 4-function ALU described by Table 9.5,
and further suppose that we wish to implement the microinstruction
ACC ‚ÜêReg1,
which should load ACC with the contents of Reg1.
The following table shows the control-path settings necessary to implement this
microinstruction:
Microinstruction
LD1
EN1
LD2
EN2
F1
F0
LDACC
ACC ‚ÜêReg1
0
1
0
0
0
1
1
The control-path settings have the following effects upon the data-path operations:
‚Ä¢ LD1 = 0: The contents of Reg1 are unchanged by the microinstruction.
‚Ä¢ EN1 = 1: The contents of Reg1 are enabled onto the Data Bus.
‚Ä¢ LD2 = 0: The contents of Reg2 are unchanged by the microinstruction.
‚Ä¢ EN2 = 0: The contents of Reg2 are isolated from the Data Bus.

192
Access Control, Security, and Trust: A Logical Approach
‚Ä¢ F1F0 = 01: The ALU operation is PASS A, which in this case passes the con-
tents of Reg1 to the ALU output.
‚Ä¢ LDACC = 1: The contents of the accumulator ACC are updated with the ALU
output, namely the contents of Reg1.
‚ô¶
9.3.2
Microprogramming
Microprograms are sequences of microcode instructions.
Although micropro-
grams can be implemented directly in hardware, they are typically implemented in
Ô¨Årmware: that is, the microinstructions are stored in read-only memory and are ac-
cessed by hardware known as a microprogram sequencer. The advantage of Ô¨Årmware
over hardware is that Ô¨Årmware is changeable (although not by users), which allows
a machine‚Äôs behavior to be altered without modifying the hardware. In particular,
altering the contents of the read-only memory containing microinstructions permits
the same hardware to emulate different machine instruction sets. A disadvantage of
Ô¨Årmware, however, is that it is slower than instructions implemented by hardware
alone.
Microprograms are the interface between hardware and software. In our descrip-
tions, we will frequently describe operations at the microprogram level using exam-
ple data and control paths.
Example 9.6
Suppose we wish to implement a machine instruction that swaps the contents of
registers Reg1 and Reg2, using the data and control path shown in Figure 9.10. The
following microprogram implements the swap instruction:
1. ACC ‚ÜêReg1
2. Reg1 ‚ÜêReg2
3. Reg2 ‚ÜêACC
‚ô¶
Exercise 9.3.1
Using Figure 9.9 as a starting point, design the control logic for the

internal control signals shown in Table 9.5.
That is, determine the logical formulas for each control signal, and then draw the
schematic of a 4-bit ALU using full adders and the basic combinational logic gates
shown in Figure 9.6.
Exercise 9.3.2
For each of the following machine instructions, draw a timing dia-
gram and a sequence of microinstructions that show the sequence of control signals
for the data path in Figure 9.10 that implements the given instruction:

A Primer on Computer Hardware
193
a. ACC ‚ÜêReg2: load ACC with the contents of Reg2
b. ACC ‚ÜêReg1 + ACC: add the contents of Reg1 to the contents of ACC and
store the result in the ACC
c. ACC ‚ÜêReg2 + ACC: add the contents of Reg2 to the contents of ACC and
store the result in the ACC
d. Reg1 ‚ÜêACC: store the contents of ACC into Reg1
e. Reg2 ‚ÜêACC: store the contents of ACC into Reg2
f. Reg1 ‚ÜêReg2: store the contents of Reg2 into Reg1
g. Reg2 ‚ÜêReg1: store the contents of Reg1 into Reg2
Exercise 9.3.3
Devise a sequence of microinstructions that implements the assem-
bly language instruction ADD Reg1 Reg2 (i.e., ACC is updated with the sum of the
contents of Reg1 and Reg2).
Exercise 9.3.4
Draw the timing diagram for the operation ADD Reg1 Reg2 (see the
previous exercise), using the following template:
clock
cycle
1
2
3
4
LD1
EN1
Reg1
LD2
EN2
Reg2
F1
F0
out
LDACC
ACC
9.4
Summary
In this chapter, we introduced the fundamentals of synchronous hardware design
and microprogramming: understanding these fundamentals is essential to under-
standing hardware security, which we will be exploring in the next few chapters.
Common to all synchronous hardware is a central clock that synchronizes all state
changes: registers can change their values only on active clock edges.

194
Access Control, Security, and Trust: A Logical Approach
FIGURE 9.11 Learning outcomes for Chapter 9
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
‚Ä¢ When given a block diagram of combinational-logic gates, you are able
to write the output of any logic gate as a function of the inputs to the
block diagram.
‚Ä¢ When given a data-path diagram and a microinstruction, you are able to
describe the computation performed and the next state of the system.
Application
‚Ä¢ When given a logical function, you are able to translate that function into
a block diagram of combinational-logic gates.
Analysis
‚Ä¢ When given a data path and control path description, a microprogram,
and the values in all relevant registers and memory, you are able to de-
termine what computation or operation is performed and the values in all
relevant registers and memory.
Synthesis
‚Ä¢ When given a description of a machine-level instruction and a block di-
agram of a processor with its control lines, devise microprograms that
implement the machine-level instruction.
Synchronous hardware designs are described by a combination of data and control
paths. Data paths are the paths along which data travel and on which data operations
are performed. Control paths comprise the control signals that direct both data and
the operations on data. During any clock cycle, the values of the control signals are
viewed as a microinstruction. Sequences of microinstructions form microprograms,
which are used to implement the instruction set of a processor.
In this chapter, we described the operation of real processor hardware. In the next
chapter, we show how simulations of processors‚Äîvirtual machines‚Äîare created.
Virtual machines provide the illusion to users of controlling an entire processor when
in fact the same processor and memory are safely and securely shared among many
users.
The learning outcomes associated with this chapter appear in Figure 9.11.

A Primer on Computer Hardware
195
9.5
Further Reading
There are numerous books on digital hardware design, microprogramming, and
computer architecture. Wakerly‚Äôs book‚ÄîDigital Design: Principles and Practices
(Wakerly, 2006)‚Äîcovers combinational-logic design and sequential logic design.
Comer‚Äôs book‚ÄîEssentials of Computer Architecture (Comer, 2005)‚Äîcovers CPUs,
instruction sets, microcode, and memory management.


Chapter 10
Virtual Machines and Memory
Protection
Controlling access to memory is critical for sharing one machine among many users
and processes. Security is impossible without the capabilities to isolate one process
from another and to control how data are shared. For example, if Alice and Bob
are running their programs on the same machine, their programs should be protected
from one another. Such protection requires that Alice‚Äôs calculations should not spill
over into the memory used by Bob. Furthermore, unless Alice has authorization
from Bob or other appropriate controlling authorities, Alice should not be able to
read from or write to Bob‚Äôs data stored in memory.
Many of the basic concepts of memory protection‚Äîincluding virtual memory and
access control‚Äîwere worked out in the era of time-shared mainframe computers.
These mainframes were viewed as centralized resources to be shared among many
users simultaneously. With the introduction of minicomputers and microprocessors‚Äî
which later evolved into personal computers‚Äîmany memory-protection mechanisms
were discarded for efÔ¨Åciency reasons, under the assumption that personal computers
were ultimately controlled by single users and not shared. Widespread network-
ing has since falsiÔ¨Åed this assumption, making memory-protection mechanisms and
virtual-machine technologies relevant once again.
A virtual machine (VM) is a simulation of a physical machine. This simulation
provides the illusion to operating systems and programs that they are running di-
rectly on the hardware. Protecting the real machine, processes, and one user from
another is the task of the virtual machine monitor (VMM), which is a layer (often
implemented in microcode or software) separating virtual machines and hardware.
Figure 10.1 illustrates this idea, showing three operating systems supported by a
VMM that operates on a single hardware platform. Each operating system executes
within an environment created by the VMM; each of these environments is a VM.
The VMM manages the hardware (memory, processor, and input/output) with
three objectives in mind:
1. Creation of an identical execution environment to that of the hardware being
emulated
2. EfÔ¨Åcient execution of instructions to maintain acceptable program execution
speeds
3. Intervention by the VMM in any attempt to alter the allocation of resources
197

198
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.1 Virtual machine monitor protecting memory
As Figure 10.1 shows, VMMs are reference monitors that check the instructions
issued by VMs in order to guard the integrity of the physical system. Virtual ma-
chine monitors are also called hypervisors or security kernels: we use the virtual
machine monitor nomenclature in this text to emphasize the VMM‚Äôs role as a ref-
erence monitor. In this chapter, we analyze the details of memory protection and
the basic structure and operation of VMMs. Because VMs operate at the boundary
between hardware and software, we start with a description of a simple processor
that provides a concrete context for the movement and control of data. We then look
at memory protection based on memory segmentation. After that, we develop the
mandatory access control policies for accessing memory. Finally, we look at the
structure and function of VMMs.
10.1
A Simple Processor
In this section, we introduce a concrete example of a simple processor, shown in
Figure 10.2. By specifying a speciÔ¨Åc processor, we can give concrete examples for
the underlying mechanisms that implement memory protection and the structure and
function of virtual machine monitors. The ideas we present, however, are applica-
ble to any standard processor. We start by describing the processor‚Äôs components,
its data path, and its control path. We follow with a description of the processor‚Äôs
machine-language instructions.

Virtual Machines and Memory Protection
199
FIGURE 10.2 A simple processor
10.1.1
Processor Components
Our simple processor has the following major components, which we describe in
turn.
Data Buses:
There are three data buses: the A BUS, the B BUS, and the F BUS, all
of which are used to convey data to the components of the processor.
Arithmetic Logic Unit:
The arithmetic logic unit (ALU) has data input ports A
and B, control input FUNCTION, and output port F. The ALU performs operations
on inputs A and B as speciÔ¨Åed by FUNCTION, and the output appears on F. Typical
operations of the ALU include: PASS A, PASS B, and ADD A B, which correspond
respectively to ‚ÄúF = A,‚Äù ‚ÄúF = B,‚Äù and ‚ÄúF = A+B.‚Äù
Accumulator:
The accumulator (ACC) register is used to store intermediate val-
ues produced by the ALU during calculations that extend over multiple instructions.
When LD is asserted, the ACC updates its contents from the F BUS at the beginning
of the next cycle. When EN is asserted, the output of ACC is immediately put onto
the B BUS.

200
Access Control, Security, and Trust: A Logical Approach
Program Counter:
The program counter (PC) register is used to point to the ad-
dress of the next machine instruction stored in the program memory. The PC register
updates its contents from the F BUS in exactly the same way as the ACC. Likewise,
the output of the PC is immediately put onto the B BUS when the EN input to the PC
is asserted.
Memory Address Register:
The memory address register (MAR) is used to point
to the address of instructions and data to be fetched or stored in program memory.
The control line LD functions in the same way for the MAR as it does for the ACC
and PC registers. In our processor, the MAR has two outputs: the Ô¨Årst is always
enabled and connected to the ADDRESS input of the program memory, and the second
is connected to the B BUS under the control of EN.
Instruction Register:
The instruction register (IR) holds the machine instruction
currently being executed by the processor. The input to the IR is the A BUS.
We assume that all instructions have two Ô¨Åelds: an op-code Ô¨Åeld and an address
Ô¨Åeld. The op-code is a numerical encoding of the instruction to be performed, while
the address speciÔ¨Åes the memory address. For example, the operation that loads
the accumulator register with the contents of memory address 100 would be LDA
100. Operations for which memory addresses are irrelevant (e.g., CLA: clear the
accumulator) ignore the contents of the address Ô¨Åeld. Operations for which two
memory addresses are needed (e.g., LD 100 200: load the contents of location 100
with the contents of 200) actually take up two locations in memory: LD 100 followed
by 200, where the op-code Ô¨Åeld of the instruction containing address 200 is ignored.
Program Memory:
The program memory (PM) holds machine instructions and
data for user programs. PM has DATA IN and DATA OUT ports. The DATA IN port
gets its value from the B BUS, while the DATA OUT port is the sole input to the A BUS.
PM behaves like a linearly addressable array of q registers, starting at location 0 and
ending at location q-1. At the start of each cycle, the contents of location ADDRESS
of PM, which we denote by PM[ADDRESS], appear on the DATA OUT port, where the
value of ADDRESS comes from the MAR during the previous cycle. If a memory-read
operation is speciÔ¨Åed by the R/W control, then value PM[ADDRESS] is unchanged.
However, if a memory-write operation is speciÔ¨Åed by R/W, then the value on the B
BUS from the prior cycle is placed into PM at location ADDRESS and that value also
appears on DATA OUT.
Timing and Control Unit:
The timing and control unit (TCU) is responsible for
fetching instructions from the PM, sequencing the Ô¨Çow of data in the processor, and
sequencing the operations performed by the ALU. The TCU controls the processor
operations and data Ô¨Çow by the following control lines:
‚Ä¢ FUNCTION: this line controls the function performed by the ALU.

Virtual Machines and Memory Protection
201
‚Ä¢ LD: this bus controls which registers (ACC, PC, or MAR) are loaded from the
F BUS at the start of the next cycle. Any combination of these registers (i.e.,
ranging from none to all of them) is possible.
‚Ä¢ EN: this bus controls which register‚Äôs output (ACC, PC, or MAR) is placed on
the B BUS. At most one register‚Äôs output should be placed on the bus.
‚Ä¢ R/W: this line speciÔ¨Åes whether the contents of PM is a read or write.
Having described the processor‚Äôs data and control paths, we turn our attention to
the processor‚Äôs instructions at the machine level and microcode level.
10.1.2
Machine Instructions
All the registers (ACC, PC, IR, and MAR) and state-holding devices (such as PM)
are synchronous‚Äîthat is, they change state on active clock edges.
We now consider a concrete example. Suppose we wish to execute the program-
level operation PM[B] ‚ÜêPM[A] + PM[B], which should add the contents of program
memory at addresses A and B and store the result back into address B. To execute
this program-level operation requires three machine-level instructions to be executed
in the following order:
1. LDA @A: load the ACC with the contents of PM at address A, ACC ‚ÜêPM[A].
2. ADD @B: load the ACC with the results of adding the contents of the ACC
with the contents of PM at address B, ACC ‚ÜêACC +PM[B].
3. STO @B: store the contents of the ACC into the PM at address B, PM[B] ‚Üê
ACC.
To execute each of the machine-level instructions requires a third and Ô¨Ånal level of
programming at the microprogram level. The TCU executes microprograms that set
and sequence the appropriate values on the various control lines (e.g., FUNCTION,
LD, and EN).
All three levels of programming‚Äîprogram level, machine level, and microcode‚Äî
are shown in Table 10.1. The left column corresponds to a high-level operation that is
compiled into the machine-level instructions in the middle column. Each machine-
level instruction is realized by a series of micro-coded operations as shown in the
rightmost column. We use the notation MAR ‚ÜêPM[MAR].addr to denote that MAR
is loaded with the address portion of the instruction found in PM[MAR].
The remainder of this section is targeted towards hardware designers, who typi-
cally must account for all of the calculations that occur during each and every clock
cycle. Readers who are not interested in such low-level details may safely skip to
Section 10.2.
Table 10.2 details the values for each control line for each microprogram operation
listed in Table 10.1. Because microprograms are intricate, we examine in detail what
happens with the instruction LDA @A. To help us keep track of all the details, we
use a timing diagram, as shown in Figure 10.3.

202
Access Control, Security, and Trust: A Logical Approach
Program Level
Operation
Instructions
Stored in Program
Memory
Microcode Operations
Sequenced by Timing and Control Unit
@B ‚Üê@A+@B
i.
LDA @A
1.
MAR ‚ÜêPC
2.
PC ‚ÜêPC +1
3.
IR ‚ÜêPM[MAR],MAR ‚ÜêPM[MAR].addr
4.
ACC ‚ÜêPM[MAR]
i+1.
ADD @B
5.
MAR ‚ÜêPC
6.
PC ‚ÜêPC +1
7.
IR ‚ÜêPM[MAR],MAR ‚ÜêPM[MAR].addr
8.
ACC ‚ÜêPM[MAR]+ACC
i+2.
STO @B
9.
MAR ‚ÜêPC
10.
PC ‚ÜêPC +1
11.
IR ‚ÜêPM[MAR],MAR ‚ÜêPM[MAR].addr
12.
PM[MAR] ‚ÜêACC
Table 10.1: Operations for @B ‚Üê@A+@B
Microcode Operations
LD
EN
ALU
R/W
A Bus
B Bus
F Bus
1.
MAR ‚ÜêPC
MAR
PC
Pass B
R
X
PC
PC
2.
PC ‚ÜêPC +1
PC
PC
Inc B
R
X
PC
PC+1
3.
IR ‚ÜêPM[MAR]
MAR ‚ÜêPM[MAR].addr
IR, MAR
X
Pass A
R
PM[MAR]
X
PM[MAR]
4.
ACC ‚ÜêPM[MAR]
ACC
X
Pass A
R
PM[MAR]
X
PM[MAR]
5.
MAR ‚ÜêPC
MAR
PC
Pass B
R
X
PC
PC
6.
PC ‚ÜêPC +1
PC
PC
Inc B
R
X
PC
PC+1
7.
IR ‚ÜêPM[MAR]
MAR ‚ÜêPM[MAR].addr
IR, MAR
X
Pass A
R
PM[MAR]
X
PM[MAR]
8.
ACC ‚Üê
PM[MAR]+ACC
ACC
ACC
Add
R
PM[MAR]
X
PM[MAR]
+ ACC
9.
MAR ‚ÜêPC
MAR
PC
Pass B
R
X
PC
PC
10.
PC ‚ÜêPC +1
PC
PC
Inc B
R
X
PC
PC+1
11.
IR ‚ÜêPM[MAR]
MAR ‚ÜêPM[MAR].addr
IR, MAR
X
Pass A
R
PM[MAR]
X
PM[MAR]
12.
PM[MAR] ‚ÜêACC
X
ACC
X
W
PM[MAR]
ACC
X
Table 10.2: Timing and control unit operations
Conceptually, the Ô¨Årst machine operation‚Äîi.e., loading ACC with the contents of
PM at location A‚Äîrequires the following three steps:
1. Fetch the LDA @A instruction from location i in PM and store it the instruction
register IR. We assume that the program counter PC has the address i of the
machine instruction to be executed.
2. Fetch the contents of PM at address A.
3. Store the contents of PM at address A into the accumulator ACC.
Completing these three conceptual steps requires a total of four micro-operations
(i.e., four clock cycles), as follows:
Cycle 1 objective: Copy the contents of the PC into the MAR.
Initial state: PC = i. The values of the MAR, ACC, IR, and PM[MAR] de-
pend on previous operations and are unimportant at this point.
Control lines: EN = PC, LD = MAR, FUNCTION = PASS B.

Virtual Machines and Memory Protection
203
FIGURE 10.3 Timing of CPU operations
Setting EN to PC puts the output of the PC onto the B BUS. Setting
FUNCTION to PASS B passes the values on the B BUS through the ALU
to the F BUS. Setting LD to MAR causes the F BUS values to be loaded
into the MAR at the start of the next clock cycle.
Cycle 2 objective: Increment the PC to point to the next instruction.
Initial state: PC = i, MAR = i. The values of ACC, IR, and PM[MAR] are
due to previous operations.
Control lines: EN = PC, LD = PC, FUNCTION = INC B.
Setting EN to PC puts the output of the PC onto the B BUS. Setting
FUNCTION to INC B increments the current value stored in the PC, which
is i, to i+1. This is the ALU output and is put on to the B BUS. Setting
LD to PC loads i+1 into the PC at the start of the next cycle.
Cycle 3 objective: Fetch the instruction at location i‚ÄîLDA @A‚Äîand store the in-
struction in the instruction register IR and memory address A in the MAR.
State: PC = i+1, MAR = i, PM[MAR] = PM[i] = LDA @A. The values of
ACC and IR are due to previous operations.
Control lines: EN = X, LD = IR, MAR, FUNCTION = PASS A.
The MAR contains i from the previous cycle. So, the output of PM is
PM[i], namely the ith instruction LDA @A. We enable the instruction
register IR and the MAR to load LDA @A and address A respectively in
the next cycle by arranging the ALU function to pass the output of PM
on the A BUS through to the ALU output by setting FUNCTION to PASS
A.
Cycle 4 objective: Load the contents of address A in PM into the ACC.
State: PC = i+1, MAR = A, IR = LDA @A, and PM[MAR] = A. The contents
of the ACC are due to previous operations.

204
Access Control, Security, and Trust: A Logical Approach
Control lines: EN = X, LD = ACC, FUNCTION = PASS A.
The values of IR and MAR are set to the value LDA @A. Setting LD
to ACC puts the value of the ALU output (PM[A]) into the ACC at the
beginning of the next cycle. As the ALU is passing through the output
of PM, it is unimportant which register (ACC, PC, or MAR) output is
enabled onto the B BUS.
Cycles 5 through 12 can be described in a similar fashion and their descriptions are
left as an exercise for the reader.
Next, we turn our attention to the virtual machine monitor, which protects physical
memory and user processes.
Exercise 10.1.1
In a similar fashion to the description of the four microcode cycles

implementing LDA @A in the text based on Tables 10.1, 10.2, and Figure 10.3, de-
scribe the micro-cycle objectives, state, and control line values for cycles 5 through
8 implementing ADD @B.
Exercise 10.1.2
In a fashion similar to Table 10.1, work out the details of a CLA

(i.e., clear accumulator ACC) instruction. Assume that the ALU has a function ZERO,
which outputs 0 regardless of the inputs on A and B. Work out the microcode cycles
and draw the timing diagram.
Exercise 10.1.3
In a similar fashion to the description of the four microcode cycles

implementing LDA @A in the text based on Tables 10.1, 10.2, and Figure 10.3, de-
scribe the micro-cycle objectives, state, and control line values for cycles 8 through
12 implementing STO @B.
10.2
Processors with Memory Segmentation
The simple processor developed thus far gives user programs access to all loca-
tions in program memory PM. When multiple user programs are stored in program
memory, the simple processor is unable to protect one program from reading or writ-
ing over the programs or data of other users. In this section, we examine how pro-
cessors can provide this protection through memory segmentation.
10.2.1
Segmentation Using a Relocation Register
To protect the programs and data of each user, physical memory is segmented:
that is, memory is partitioned to simultaneously isolate one user from another while
giving each user the illusion that he or she controls the entire processor memory. Seg-
mentation is accomplished by the addition of a relocation register (RR) that speciÔ¨Åes
the starting address of each memory segment (i.e., the base address) and the size of
the memory segment (i.e., its bound).

Virtual Machines and Memory Protection
205
FIGURE 10.4 Memory segmentation
Figure 10.4 shows a physical memory with q locations partitioned into four seg-
ments: one segment each for Alice, Bob, Carol, and the Supervisor. The base and
bound values stored in the relocation register uniquely locate each memory seg-
ment. The idea is that, within each segment, users access virtual memory locations 0
through bound - 1, when in fact they are really accessing physical memory locations
base through base + bound - 1. Thus, the physical-memory location that corre-
sponds to the virtual address in the program counter PC is determined by adding the
base-address value to the virtual-address value in the PC. If the virtual address a is
less than bound and a + base < q, then the address being read from or written to is
within the user‚Äôs memory segment in physical memory. Otherwise, the operation is
trapped‚Äîthat is, control is passed on to the Supervisor.
This policy for controlling access to physical memory is enforced for all programs
and users. Thus, it is a mandatory access-control policy.
The following example illustrates the use of the base and bound values in the
relocation register.
Example 10.1
Suppose that the value in the program counter is 57, the base-bound value in the
relocation register is (128,256), and the size of physical memory is 1024: thus, the
virtual address is deÔ¨Åned relative to a base value of 128 and a bound of 256. The
location of the next instruction is found in absolute address 128 + 57 = 185. The
absolute address falls within the bounds of physical memory (i.e., 185 ‚â§1023) and
within the bound set by the relocation bound register (i.e., 57 ‚â§255). Therefore, the
address falls within the user‚Äôs allotted memory segment, and the operation should be
allowed to proceed.
‚ô¶

206
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.5 Processor model for virtual machines
To accommodate memory segmentation, we must add a relocation register RR
and a mode bit M to our simple processor. Figure 10.5 contains a diagram of our
enhanced processor that supports virtual machines.
The mode bit M is used to indicate whether the processor is operating in user
mode or supervisor mode (denoted by u and s, respectively). In user mode, access
to security-sensitive instructions (such as writing values into RR) is trapped, which
means that the processor is limited to the segment identiÔ¨Åed by RR. In supervisor
mode, all instructions are available, including the security-sensitive instructions that
alter the values in RR. This capability to alter the values in RR gives the processor
access to all segments in PM.
Normally, the processor operates in user mode. When a program operating in user
mode either wishes to transfer control to the Supervisor or has an instruction trapped,
the mode bit is changed from u to s and control is passed to the Supervisor program.
The next three subsections develop the details of traps and how control is trans-
ferred to the Supervisor program.

Virtual Machines and Memory Protection
207
10.2.2
Processor State and Instructions
The state S of a processor is given by the values stored in its state-holding ele-
ments: program memory PM, accumulator ACC, program counter PC, memory ad-
dress register MAR, instruction register IR, mode bit M, and relocation register RR.
We denote these values in lower case by pm, acc, pc, mar, ir, m, and (base,bound),
and we represent the state S by a seven-tuple:
S = (pm,acc, pc,mar,ir,m,(base,bound))
Note that pm is an array of values pm[0] through pm[q‚àí1], where q‚àí1 is the largest
available address in physical program memory PM.
Because each component of S is Ô¨Ånite, there are a Ô¨Ånite number of processor
states S; we call this set of states PStates. Each machine instruction i can therefore
be viewed as a total function from PStates to PStates.
Example 10.2
Suppose the machine instruction LDA @15 is stored in address 0 of program memory
PM and S = (pm,acc, pc,mar,ir,u,(256,128)) is the current state of the processor.
Because the mode bit is u and the relocation register RR is (256,128), the virtual
machine is in user mode and the memory segment starts at location 256 in PM and
ends at location 256 + 127 = 383. Consulting Table 10.1, we can check the result
of the Ô¨Årst four micro-cycles to calculate the state that results from applying LDA
@15. If we assume that PM[256 + 15] contains the value 12 and that the values of
ACC, MAR, and IR are acc, mar, and ir respectively, then the state that results from
applying the instruction LDA @15 is as follows:
(LDA @15)(pm,acc,0,mar,ir,u,(256,128)) =
(pm,12,1,15, LDA @15,u,(256,128)).
‚ô¶
10.2.3
Program Status Word
The context of a program is given by the program status word (PSW). As Fig-
ure 10.5 shows, the PSW consists of the accumulator (ACC), memory address reg-
ister (MAR), mode bit (M), program counter (PC), relocation register (RR), and
instruction register (IR). The PSW value psw is a 6-tuple consisting of the contents
of the respective registers:
psw = (acc, pc,mar,ir,m,(base,bound)).
Thus, the PSW speciÔ¨Åes the current state of the processor, modulo the contents of
program memory.

208
Access Control, Security, and Trust: A Logical Approach
Example 10.3
Suppose the value of the PSW is (25,42,56, CLA,s,(0,65536)), where the underly-
ing executable memory size is 216 = 65536 locations. From the PSW, we see that
the current program is operating in supervisor mode, the accumulator value is 25,
the MAR is pointing to relative address 56, the program counter is pointing to rela-
tive address 42 in executable memory (which is also the absolute address as the base
value of R is 0), the current program is able to access the entire executable memory,
and the instruction in IR is clear accumulator.
‚ô¶
The PSW is a mechanism by which the program in control of the processor can
by changed. Changing programs or execution threads is essential for supporting
multiprocessing. By loading the PSW registers with values corresponding to the Su-
pervisor or other programs, the processor‚Äôs execution context is changed or switched.
For simplicity, we assume that the value of the PSW Ô¨Åts entirely into one storage
location in PM. We also assume that PM[1] (i.e., the second location in memory) has
a Ô¨Åxed value that never changes and that corresponds to the default state in which the
Supervisor program always starts.1 This state corresponds to a PSW value of form
pswsuper = (accsuper,addrsuper,marsuper,irsuper,s,(0,q)),
where addrsuper is the starting address of the Supervisor program, and accsuper,
marsuper, and irsuper are the default starting values for the ACC, MAR, and IR reg-
isters. Note that the mode bit is set to s (for supervisor), and the relocation register
speciÔ¨Åes that the full address range of memory is accessible.
When it is necessary to transfer control to the Supervisor program, the content of
the currently executing program (i.e., the current PSW value) is stored in memory
location PM[0]. The Ô¨Åxed value pswsuper stored in location PM[1] is then copied
to the PSW, which allows the Supervisor program to take control of the processor.
When the Supervisor has completed its execution, control can be returned to the
original program by copying the value temporarily stored in PM[0] back to the PSW.
Now that we have explained the concept and use of the PSW, we can introduce
some of the details of traps.
10.2.4
Traps
Trapping an instruction transfers control to the Supervisor program. If all security-
sensitive instructions are trapped (i.e., inspected or executed under the supervision of
the Supervisor control program), then the privacy and integrity of program memory
can be maintained, provided that the Supervisor is itself correct and trustworthy.
1Correct implementation of memory segmentation will ensure that user programs can never change the
value of location PM[1]. The Supervisor program must also be implemented in a way that prevents
alterations to this location.

Virtual Machines and Memory Protection
209
An instruction i is said to trap if the contents of program memory are unchanged
except for the contents of PM[0]; the program status word prior to trapping instruc-
tion i is stored in PM[0]; and the new program status word takes its value from
PM[1].
Presumably, the updated value of PSW is the starting address and values of the
Supervisor program. During its execution, the Supervisor program can look at the
contents of PM[0] to Ô¨Ånd the previous value of psw and to see the virtual machine
state just before instruction i trapped.
For now, we defer the description of the structure and contents of the Supervisor
program until later. In the next section we look at the access-control policy for
accessing program memory.
Exercise 10.2.1
Consider Table 10.1. To which states are ADD @B and STO @B

applied? Which states are the result of these function applications?
Exercise 10.2.2
Consider the application of machine instruction LDA @200 stored

in address 300 of program memory PM. Let S = (pm,acc, pc,mar,ir,u,(256,128))
be the state of the processor to which LDA @200 is applied. What is the next state?
10.3
Controlling Access to Memory and Segmentation
Registers
Having introduced the idea and mechanisms behind memory segmentation, we
now turn to describing the mandatory access control policy for program memory.
We treat the processor registers IR, M, and RR as principals who make statements
about their values. For example, suppose that the instruction register IR has the value
LDA @A. We can model this situation as
IR says ‚ü®LDA @A‚ü©,
where we interpret ‚ü®LDA @A‚ü©as the proposition ‚Äúit would be a good idea to execute
the instruction LDA @A.‚Äù Likewise, we can express the situation where RR contains
the value (base,bound) as
RR says ‚ü®(base,bound)‚ü©,
where we interpret ‚ü®(base,bound)‚ü©as the proposition ‚Äúthe base value is base, and
the bound value is bound.‚Äù We can similarly model the situation where M has the
value u by
M says ‚ü®u‚ü©,
where ‚ü®u‚ü©is interpreted as ‚Äúthe virtual machine is in user mode.‚Äù

210
Access Control, Security, and Trust: A Logical Approach
As Figure 10.5 illustrates, the virtual machine monitor comprises the timing and
control unit (TCU) and the program memory. Virtual machines provide privacy and
integrity protection for users so long as all resource allocation is controlled by the
Supervisor (i.e., by the virtual machine monitor). Thus, only the Supervisor should
be able to alter the relocation register RR; user programs should not have that capa-
bility.
Throughout the remainder of this chapter, we assume that all supervisory functions
are implemented by the TCU. In other words, the TCU is designed to reÔ¨Çect the
wishes of the Supervisor,
TCU ‚áíSupervisor.
10.3.1
Access to Program Memory
Access to program memory PM is governed by the value of RR and the size of
PM, which we represent by q. For each instruction that involves accessing PM, such
as LDA @A, we have an access-control policy.
In general, for an instruction OP @A that involves an operation OP on relative
address A, we have the following considerations:
‚Ä¢ The absolute address must fall within the addressable locations of PM. That
is, we must have A +base < q, where q is the size of PM. If this condition is
violated, then a memory trap occurs.
‚Ä¢ The relative address must fall within the bound of the memory segment as-
signed to the virtual machine (i.e., A + base < bound). If this condition is
violated, then a memory trap occurs.
‚Ä¢ If the above two conditions are satisÔ¨Åed, then the memory operation is per-
formed.
To be explicit, there are certain classes of instructions‚Äîprivileged and sensitive in-
structions, which we will deÔ¨Åne more formally in Section 10.4‚Äîfor which additional
conditions may force traps. For now, we consider only innocuous instructions, whose
only traps are memory traps.
For an innocuous instruction OP @A, the behavior of the VMM when executing
OP @A is speciÔ¨Åed by the three derived inference rules in Figure 10.6: Memory
Trap 1, Memory Trap 2, and OP @A. Memory Trap 1 states that, if the address
A + base exceeds the physical limits of memory, then OP @A should be trapped.
Similarly, Memory Trap 2 states that an operation involving relative address A should
be trapped if A exceeds the segment boundary bound. The OP @A rule states that, if
the operation OP @A does not require a memory trap, then it should be performed.
In the simple processor model we have introduced (Figures 10.2 and 10.5), the in-
struction register IR is under control of the virtual machine. Hence, for our processor
model, we have
IR ‚áíVM.

Virtual Machines and Memory Protection
211
FIGURE 10.6 Derived inference rules for an operation OP @A
Memory
Trap 1
TCU controls (VM says ‚ü®OP @A‚ü©
‚äÉRR says ‚ü®(base,bound)‚ü©‚äÉA +base ‚â•q ‚äÉ‚ü®trap‚ü©)
TCU says (VM says ‚ü®OP @A‚ü©
‚äÉRR says ‚ü®(base,bound)‚ü©‚äÉA +base ‚â•q ‚äÉ‚ü®trap‚ü©)
VM says ‚ü®OP @A‚ü©
RR says ‚ü®(base,bound)‚ü©
A +base ‚â•q
‚ü®trap‚ü©
Memory
Trap 2
TCU controls (VM says ‚ü®OP @A‚ü©
‚äÉRR says ‚ü®(base,bound)‚ü©‚äÉA ‚â•bound ‚äÉ‚ü®trap‚ü©)
TCU says (VM says ‚ü®OP @A‚ü©
‚äÉRR says ‚ü®(base,bound)‚ü©‚äÉA ‚â•bound ‚äÉ‚ü®trap‚ü©)
VM says ‚ü®OP @A‚ü©
RR says ‚ü®(base,bound)‚ü©
A ‚â•bound
‚ü®trap‚ü©
OP @A
TCU controls (RR says ‚ü®(base,bound)‚ü©
‚äÉA +base < q ‚äÉA < bound ‚äÉVM controls ‚ü®OP @A‚ü©)
TCU says (RR says ‚ü®(base,bound)‚ü©
‚äÉA +base < q ‚äÉA < bound ‚äÉVM controls ‚ü®OP @A‚ü©)
VM says ‚ü®OP @A‚ü©
RR says ‚ü®(base,bound)‚ü©
A +base < q
A < bound
‚ü®OP @A‚ü©
Thus, whenever IR says ‚ü®OP @A‚ü©, we can deduce that VM says ‚ü®OP @A‚ü©as well.
Example 10.4
Consider the case where the machine instruction is LDA @A (load ACC with the
contents of PM[base +A]), where A is a relative address within a segment in PM
given by (base,bound) in RR, and the maximum address in PM is q‚àí1. The relevant
access-control rules for LDA @A are as follows:
LDA mem-
ory trap 1
TCU controls
(VM says ‚ü®LDA @A‚ü©‚äÉRR says ‚ü®(base,bound)‚ü©‚äÉA +base ‚â•q ‚äÉ‚ü®trap‚ü©)
TCU says
(VM says ‚ü®LDA @A‚ü©‚äÉRR says ‚ü®(base,bound)‚ü©‚äÉA +base ‚â•q ‚äÉ‚ü®trap‚ü©)
VM says ‚ü®LDA @A‚ü©
RR says ‚ü®(base,bound)‚ü©
A +base ‚â•q
‚ü®trap‚ü©
LDA mem-
ory trap 2
TCU controls
(VM says ‚ü®LDA @A‚ü©‚äÉRR says ‚ü®(base,bound)‚ü©‚äÉA ‚â•bound ‚äÉ‚ü®trap‚ü©)
TCU says
(VM says ‚ü®LDA @A‚ü©‚äÉRR says ‚ü®(base,bound)‚ü©‚äÉA ‚â•bound ‚äÉ‚ü®trap‚ü©)
VM says ‚ü®LDA @A‚ü©
RR says ‚ü®(base,bound)‚ü©
A ‚â•bound
‚ü®trap‚ü©

212
Access Control, Security, and Trust: A Logical Approach
LDA @A
TCU controls (RR says ‚ü®(base,bound)‚ü©‚äÉ
A +base < q ‚äÉA < bound ‚äÉVM controls ‚ü®LDA @A‚ü©)
TCU says (RR says ‚ü®(base,bound)‚ü©‚äÉ
A +base < q ‚äÉA < bound ‚äÉVM controls ‚ü®LDA @A‚ü©)
VM says ‚ü®LDA @A‚ü©
RR says ‚ü®(base,bound)‚ü©
A +base < q
A < bound
‚ü®LDA @A‚ü©
These three rules follow directly from the general template presented for OP @A in
Figure 10.6. The Ô¨Årst two rules check for a memory trap caused by either attempting
to access an address beyond the maximum size of program memory or beyond the
segment boundary. If a memory trap does not occur, then the virtual machine can
load the accumulator with the contents of program memory at address A.
‚ô¶
The preceding approach sufÔ¨Åces as a simple memory access-control policy. In
Section 10.3.3, we reÔ¨Åne this policy to cover what are known as privileged and sen-
sitive instructions, some of which include memory operations. In the remainder of
this section, however, we describe the hardware-level details of implementing this
simple memory-access policy. Readers uninterested in these details may safely skip
ahead to Section 10.3.3.
10.3.2
Implementation Details
We now turn to some implementation details for protecting physical memory
based on the processor model for virtual machines given in Figure 10.5. That model
extends the simple processor shown in Figure 10.2 by adding a relocation register
and a mode bit. In this model, all memory accesses make use of the memory address
register (MAR).
We can enforce the mandatory access control policy for memory addresses by
checking all addresses prior to loading them into the MAR. If the memory address
exceeds the boundary of either physical memory or the assigned segment, then a trap
occurs. Otherwise, the MAR is loaded with the address.
Based on Table 10.1, we can modify the Timing and Control Unit (TCU) mi-
crocode that generates the timing and control signals for the processor. The modiÔ¨Å-
cations are straightforward, in that the MAR is loaded with a new value only if the
new value does not cause a memory trap. We can specify this using a conditional
‚Äúif-then-else‚Äù expression:
condition ‚Üía | b =
(
a,
if condition is true
b,
if condition is false
.
For the LDA @A, ADD @B, and STO @B instructions shown in Table 10.1, the
following changes are made:

Virtual Machines and Memory Protection
213
Instructions
Stored in Program
Memory
Microcode Operations
Sequenced by Timing and Control Unit
i.
LDA @A
1.
(((base + PC) ‚â•q) or (PC ‚â•bound)) ‚Üítrap |
(MAR ‚Üêbase+PC)
2.
PC ‚ÜêPC +1
3.
IR ‚ÜêPM[MAR],
(((base+PM[MAR].addr) ‚â•q) or (PM[MAR].addr ‚â•
bound)) ‚Üítrap | (MAR ‚Üêbase+PM[MAR].addr)
4.
ACC ‚ÜêPM[MAR]
i+1.
ADD @B
5.
(((base + PC) ‚â•q) or (PC ‚â•bound)) ‚Üítrap |
(MAR ‚Üêbase+PC)
6.
PC ‚ÜêPC +1
7.
IR ‚ÜêPM[MAR],
(((base+PM[MAR].addr) ‚â•q) or (PM[MAR].addr ‚â•
bound)) ‚Üítrap | (MAR ‚Üêbase+PM[MAR].addr)
8.
ACC ‚ÜêPM[MAR]+ACC
i+2.
STO @B
9.
(((base + PC) ‚â•q) or (PC ‚â•bound)) ‚Üítrap |
(MAR ‚Üêbase+PC)
10.
PC ‚ÜêPC +1
11.
IR ‚ÜêPM[MAR],
(((base+PM[MAR].addr) ‚â•q) or (PM[MAR].addr ‚â•
bound)) ‚Üítrap | (MAR ‚Üêbase+PM[MAR].addr)
12.
PM[MAR] ‚ÜêACC
Table 10.3: Memory-protected operations for LDA @A, ADD @A, and STO @B
‚Ä¢ The simple assignment MAR ‚ÜêPC is changed to the conditional expression
(((base+PC) ‚â•q) or (PC ‚â•bound)) ‚Üítrap | (MAR ‚Üêbase+PC).
‚Ä¢ The simple assignment MAR ‚ÜêPM[MAR].addr is changed to the conditional
expression
(((base+PM[MAR].addr) ‚â•q) or (PM[MAR].addr ‚â•bound))
‚Üítrap | (MAR ‚Üêbase+PM[MAR].addr).
Notice that the addresses are treated as relative addresses in the virtual processor and
must be converted to real addresses by adding the value base. The resulting changes
to Table 10.1 are shown in Table 10.3.
10.3.3
Access to the Relocation Register
Memory segmentation depends directly on the values stored in the relocation reg-
ister RR, because the integrity of physical memory depends on the integrity of RR.
Therefore, only the Supervisor should be able to read from (or write to) RR: users
should never have read or write access to RR.
Access to RR is protected by the mode bit M: the VM is able read or write RR
only when M is set to s (i.e., supervisor mode). Otherwise, any attempt to access RR
is trapped.

214
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.7 Simple processor with relocation register
As a concrete illustration, consider the instruction LDARR (i.e., load the accumu-
lator ACC with the contents of RR). Assume that the simple processor is extended in
the following ways, as shown in Figure 10.7:
‚Ä¢ The relocation register RR uses the same data path as the ACC, PC, and MAR
registers with similar timing and control for LD and EN. The timing and control
unit has the mode bit M added to it.
‚Ä¢ The contents of RR are always available both to the timing and control unit
and to the MAR.
‚Ä¢ The MAR is enhanced so that the address it stores is the summation of the
inputs from the F BUS and the base value stored in RR.
For a LDARR instruction stored at relative address i, the following four steps must
be taken:
1. Convert the address i in PC to its read address base+i and check for a memory
trap. If the address is not trapped, then pass i through the ALU and set up the
LD control on MAR to load i off the F BUS at the beginning of the next cycle.
2. Increment i in PC. The control lines on PC are set to enable its output on to the
B BUS and load the contents of the F BUS at the start of the next cycle. ALU
FUNCTION is set to INC B.
3. The instruction LDARR at address i of PM is fetched and stored in IR. The
output of RR is enabled onto the B BUS, ALU FUNCTION is set to PASS B, and
the ACC input is set to load the ALU output at the beginning of the next cycle,
which is the contents of RR.

Virtual Machines and Memory Protection
215
Instructions
Stored in Program
Memory
Microcode Operations
Sequenced by Timing and Control Unit
i.
LDARR
1.
(((base+PC) ‚â•q) or (PC ‚â•bound)) ‚Üítrap | (MAR ‚Üêbase+PC)
2.
PC ‚ÜêPC +1
3.
IR ‚ÜêPM[MAR]
4.
M = u ‚Üítrap | ACC ‚ÜêRR
Table 10.4: LDARR: Reading the contents of relocation register
Microcode Operations
LD
EN
ALU
1.
(((base + PC) ‚â•q) or (PC ‚â•bound)) ‚Üítrap |
(MAR ‚Üêbase+PC)
MAR
PC
Pass B
2.
PC ‚ÜêPC +1
PC
PC
Inc B
3.
IR ‚ÜêPM[MAR]
IR
X
Pass A
4.
M = s ‚ÜíACC ‚ÜêRR | trap
ACC
RR
Pass B
Table 10.5: Timing and control unit operations for LDARR
4. If the mode bit M is set to u, then the instruction is trapped. Otherwise, the
contents of RR are loaded into ACC.
The details of the above operations are illustrated in Tables 10.4 and 10.5, as well as
in the timing diagram found in Figure 10.8.
There are two derived inference rules that specify the operation of LDARR:
LDARR trap
TCU controls (M says ‚ü®u‚ü©‚äÉVM says ‚ü®LDARR‚ü©‚äÉ‚ü®trap‚ü©)
TCU says (M says ‚ü®u‚ü©‚äÉVM says ‚ü®LDARR‚ü©‚äÉ‚ü®trap‚ü©)
M says ‚ü®u‚ü©
VM says ‚ü®LDARR‚ü©
‚ü®trap‚ü©
,
LDARR
TCU controls (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®LDARR‚ü©)
TCU says (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®LDARR‚ü©)
M says ‚ü®s‚ü©
VM says ‚ü®LDARR‚ü©
‚ü®LDARR‚ü©
.
The Ô¨Årst rule indicates that the instruction LDARR is trapped if the processor is in user
mode. The second rule indicates that LDARR is allowed to execute if the processor is
in supervisor mode.
A similar set of derived inference rules and implementation details exist for LDRR
@A (i.e., store the memory contents found in relative address A into RR). The details
are left as an exercise for the reader.
10.3.4
Setting the Mode Bit
The Ô¨Ånal area to consider in the simple memory-segmentation design is the control
of the mode bit M. Improper control of the privileged state bit M could cause a user
program to be incorrectly labeled as the Supervisor. It is therefore essential to trap
any introduction by a user program that attempts to set M to supervisor mode.

216
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.8 Timing diagram for loading RR into ACC
As a concrete illustration, suppose we have an instruction SSM (set supervisor
mode), which is trapped if the processor is in user mode and otherwise executed.
The resulting rules are as follows:
SSM trap
TCU controls (M says ‚ü®u‚ü©‚äÉVM says ‚ü®SSM‚ü©‚äÉ‚ü®trap‚ü©)
TCU says (M says ‚ü®u‚ü©‚äÉVM says ‚ü®SSM‚ü©‚äÉ‚ü®trap‚ü©)
M says ‚ü®u‚ü©
VM says ‚ü®SSM‚ü©
‚ü®trap‚ü©
,
SSM
TCU controls (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®SSM‚ü©
TCU says (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®SSM‚ü©
M says ‚ü®s‚ü©
VM says ‚ü®SSM‚ü©
‚ü®SSM‚ü©
.
It turns out that the ability to set a processor‚Äôs operating mode to user mode should
also be limited to the Supervisor. In particular, user programs are restricted from any
knowledge of supervisory functions. Put another way, only the Supervisor should
be able to transfer control of a processor to a user program. This policy prevents a
user program from downgrading the Supervisor program to user mode and thereby
denying it access to supervisory functions.
For example, suppose we have an instruction SUM (set user mode). Letting ‚ü®SUM‚ü©
denote ‚Äúit is a good idea to set the mode bit M to u (i.e., enter user mode),‚Äù we can

Virtual Machines and Memory Protection
217
identify the derived inference rules corresponding to SUM as follows:
SUM trap
TCU controls (M says ‚ü®u‚ü©‚äÉVM says ‚ü®SUM‚ü©‚äÉ‚ü®trap‚ü©)
TCU says (M says ‚ü®u‚ü©‚äÉVM says ‚ü®SUM‚ü©‚äÉ‚ü®trap‚ü©)
M says ‚ü®u‚ü©
VM says ‚ü®SUM‚ü©
‚ü®trap‚ü©
,
SUM
TCU controls (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®SUM‚ü©)
TCU says (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®SUM‚ü©)
M says ‚ü®s‚ü©
VM says ‚ü®SUM‚ü©
‚ü®SUM‚ü©
.
The implementation details in terms of microcode operations and timing diagrams
can be devised in a similar fashion as for previous machine instructions. These tasks
are left as exercises for the reader.
Exercise 10.3.1
Work out the inference rules for the operation LDRR @A (i.e.,
loading RR with the values found in relative address A in PM). Prove the validity of
the inference rules.
Exercise 10.3.2
Work out the table of timing and control unit operations for the
operation LDRR @A (similar to Tables 10.4 and 10.5). Draw the corresponding
timing diagram for LDRR @A.
Exercise 10.3.3
Work out the table of timing and control unit operations for the
operation SSM. Draw the corresponding timing diagram for SSM.
Exercise 10.3.4
Work out the table of timing and control unit operations for the
operation SUM. Draw the corresponding timing diagram for SUM.
Exercise 10.3.5
Suppose a new machine instruction is required of the simple pro-
cessor with relocation register RR, as shown in Figure 10.7. The new instruction is
LD @A @B (i.e., load the contents of virtual address B into virtual address A).
a. Show the microcode implementation of LD @A @B as a sequence of mi-
crocode instructions, similar to what is found in Table 10.4.
b. Devise and prove valid the access-control rules for LD @A @B.
10.4
Design of the Virtual Machine Monitor
We now take a more detailed look at the design of virtual machine monitors. An
instruction-set architecture is said to be virtualizable if it is possible to build a virtual
machine monitor that satisÔ¨Åes the following three objectives:

218
Access Control, Security, and Trust: A Logical Approach
1. The VMM creates an execution environment that is essentially identical to the
execution environment of the hardware being emulated.
2. The VMM efÔ¨Åciently executes instructions, so that program execution times
are not seriously degraded.
3. The VMM maintains control over all system resources, so that any attempt
by users to change the resources allocated to them will cause the VMM to
intervene.
The virtualization techniques we present in this section were originally developed
in the context of mainframe computers (such as the IBM 360) and continued into
the design of minicomputers (such as the Digital Equipment Corporation‚Äôs VAX
architecture). These machines had to support multiple users executing programs
on the same hardware. Today‚Äôs systems face the same problem, whereby (due to
widespread networking) multiple processes and potentially multiple users use the
same hardware.
It turns out that not all instruction-set architectures are virtualizable. Because
virtualization is a key security capability, it is important to understand the condi-
tions under which virtualization is possible and how a VMM is constructed under
such conditions. These details were worked out by Popek and Goldberg (Popek and
Goldberg, 1974).
Virtualization depends on identifying three types of instructions:
1. Privileged instructions: instructions that are trapped in user mode but not
trapped in supervisor mode.
2. Sensitive instructions: instructions whose behavior depends on the value of
relocation register RR or mode bit M (i.e., behavior that is location or mode
sensitive, or instructions that attempt to alter the allocation of resources, such
as memory).
3. Innocuous instructions: instructions that are neither privileged nor sensitive.
The key property for an instruction-set architecture to be virtualizable is that all
sensitive instructions must be privileged instructions. If this property holds, then
all sensitive instructions are automatically trapped by the VMM‚Äôs control program,
which is deliberately constructed to control any change in allocated resources and
to faithfully simulate the execution of privileged instructions. This construction ad-
dresses the Ô¨Årst and third objectives of VMMs: creation of an identical execution
environment to the hardware being emulated, and control over all resources allo-
cated to VMs. Innocuous instructions, which are neither privileged nor sensitive,
are passed on for direct execution by hardware. This approach satisÔ¨Åes the second
objective of VMMs, namely the efÔ¨Åcient execution of instructions.
In the discussion that follows, we assume that the speciÔ¨Åc instruction-set architec-
ture is virtualizable (i.e., all sensitive instructions are also privileged instructions).

Virtual Machines and Memory Protection
219
FIGURE 10.9 Top-level operation of virtual machine monitor
The VMM‚Äôs operation at the conceptual level is illustrated by Figure 10.9. Instruc-
tions that are not privileged (i.e., innocuous instructions) are passed unchanged di-
rectly to the hardware for execution. Instructions that are privileged are trapped and
handled by the control program CP. The three objectives for VMMs are met as fol-
lows:
1. Identical execution environment: The execution environment for innocuous
instructions is identical, because innocuous instructions are passed unchanged
to the hardware. Privileged instructions are trapped by the control program
CP. If CP accurately simulates the execution of instructions, then the VMM
will provide an identical execution environment.
2. EfÔ¨Åcient execution of instructions: Passing on innocuous instructions (instead
of simulating them) provides the same execution speed as executing them on
hardware.
3. Control over all system resources: Because all sensitive instructions that at-
tempt to change the allocation of system resources are trapped, the VMM is in
control.
We now focus on how trapped instructions are handled by the VMM. Conceptu-
ally, the control program CP has three main components:
1. A dispatcher D, which is the top-level module called when a trap occurs. The
value of the program status word PSW‚Äîin particular, the instruction register
IR and the program counter PC‚Äîenables D to decide which module to call,
based on the instruction that was trapped.

220
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.10 Graphic representation of control program for handling trapped in-
structions
2. An allocator A, which is called every time a privileged instruction attempts
to change resources associated with the computing environment, such as the
value of the relocation register RR. The main task of the allocator is to decide
what system resources are provided to the various programs running on the
machine.
3. A set V of interpreters, which contains a distinct interpreter routine vi for each
privileged instruction i. These interpreters simulate the effect of running the
instruction that was trapped. In practice, each interpreter might be nothing
more than a table that looks up the state of the machine when the instruction i
was called and returns the state the machine should be in after executing i.
Figure 10.10 illustrates the function of the control program CP. Each trapped in-
struction i is examined by the dispatcher D, which determines whether to invoke the
allocator A or a speciÔ¨Åc interpreter vi ‚ààV.
We now examine more closely how privileged, sensitive, and innocuous instruc-
tions are classiÔ¨Åed.
10.4.1
Privileged Instructions
Privileged instructions are those that are intended to be executed only in supervisor
mode: they always trap when executed in user mode, but never when executed in
supervisor mode (unless, of course, there is a memory trap due to out-of-bounds
addressing). An instruction i is said to be privileged if (in the absence of memory
traps) it exhibits the following behavior, for all states:
i(pm,acc, pc,mar,ir,m,(base,bound))
(
traps, if m = u
doesn‚Äôt trap, if m = s.

Virtual Machines and Memory Protection
221
Example 10.5
Loading the contents (base,bound) of the relocation register RR into the accumulator
ACC is a privileged instruction, which only the Supervisor program can execute.
Recall the deÔ¨Ånition of LDARR, which is expressed as the following two derived
inference rules:
LDARR trap
TCU controls (M says ‚ü®u‚ü©‚äÉVM says ‚ü®LDARR‚ü©‚äÉ‚ü®trap‚ü©)
TCU says (M says ‚ü®u‚ü©‚äÉVM says ‚ü®LDARR‚ü©‚äÉ‚ü®trap‚ü©)
M says ‚ü®u‚ü©
VM says ‚ü®LDARR‚ü©
‚ü®trap‚ü©
,
LDARR
TCU controls (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®LDARR‚ü©)
TCU says (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®LDARR‚ü©)
M says ‚ü®s‚ü©
VM says ‚ü®LDARR‚ü©
‚ü®LDARR‚ü©
.
From these rules we can see that, if M says ‚ü®u‚ü©, then LDARR is trapped. If M says ‚ü®s‚ü©,
then LDARR is executed. Thus, LDARR is a privileged instruction.
‚ô¶
10.4.2
Sensitive Instructions
Informally, there are two types of sensitive instructions:
1. Control-sensitive instructions, which attempt to change either the processor
mode (i.e., user or supervisor) or the memory resources allocated to the pro-
cessor (via the relocation register)
2. Behavior-sensitive instructions, whose behavior is affected by either the pro-
cessor mode or the physical location in real memory speciÔ¨Åed by the relocation
register
To deÔ¨Åne these notions more precisely, we must Ô¨Årst introduce two simple deÔ¨Åni-
tions.
DeÔ¨Ånition 10.1 Let S = (pm,acc, pc,mar,ir,m,(base,bound)) be a processor state.
‚Ä¢ An instruction i is mode preserving on S if i(S) either address traps or has the
same mode bit as S (i.e., the value m).
‚Ä¢ An instruction i is resource preserving on S if i(S) either address traps or has
the same (base,bound) value as S.
Intuitively, control-sensitive instructions are instructions that attempt to change
either the contents of the relocation register RR, the mode bit M, or both. That is, an
instruction i is control sensitive if there exists at least one state S such that i is not
mode preserving on S, not resource preserving on S, or both.

222
Access Control, Security, and Trust: A Logical Approach
Example 10.6
Consider the SUM (i.e., set user mode) instruction. Its behavior is deÔ¨Åned by the
following two derived inference rules:
SUM trap
TCU controls (M says ‚ü®u‚ü©‚äÉVM says ‚ü®SUM‚ü©‚äÉ‚ü®trap‚ü©)
TCU says (M says ‚ü®u‚ü©‚äÉVM says ‚ü®SUM‚ü©‚äÉ‚ü®trap‚ü©)
M says ‚ü®u‚ü©
VM says ‚ü®SUM‚ü©
‚ü®trap‚ü©
,
SUM
TCU controls (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®SUM‚ü©)
TCU says (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®SUM‚ü©)
M says ‚ü®s‚ü©
VM says ‚ü®SUM‚ü©
‚ü®SUM‚ü©
.
From these rules, we can see that SUM is a privileged instruction that is trapped
in user mode but not in supervisor mode. However, the second rule states that an
attempt to execute SUM in supervisor mode should be granted: the result of the
instruction would be to change the mode bit from s to u. As a result, the instruction
SUM is also control sensitive.
‚ô¶
In contrast, behavior-sensitive instructions are either location sensitive (i.e., de-
pend on the value of the relocation register) or mode sensitive (i.e., depend on the
value of the mode bit). Recall that a virtual-machine program accesses memory
through the use of virtual addresses. In theory, a program‚Äôs behavior should be in-
dependent of the physical locations in which its segment resides. SpeciÔ¨Åcally, its
behavior should be governed by what is stored within the memory segment it can
access (whose boundaries are deÔ¨Åned by RR), what is stored in location PM[1] be-
cause of the way we deÔ¨Åned traps, and the value stored in the program status word
PSW, namely the values stored in ACC, PC, MAR, IR, M, and RR. What is stored in
other segments should not affect the behavior of the processor under the control of
the segment referenced by (base, bound) in RR.
We therefore introduce the following notion of equivalence, which equates states
whose contents of accessible memory segments are identical (independent of their
physical locations in memory).
DeÔ¨Ånition 10.2 Let x be an integer. Two states S1 and S2 are equivalent modulo x
(written S1 ‚âàx S2) if and only if there exist values pm1, pm2,acc, pc,mar,ir,m,base,
and bound such that the following conditions all hold:
‚Ä¢ S1 = (pm1,acc, pc,mar,ir,m,(base,bound)),
‚Ä¢ S2 = (pm2,acc, pc,mar,ir,m,(base+x,bound)),
‚Ä¢ pm1[1] = pm2[1],
‚Ä¢ For all j such that base ‚â§j < base+bound, pm1[j] = pm2[j +x].

Virtual Machines and Memory Protection
223
Intuitively, when S1 ‚âàx S2 (for whatever value of x), S1 and S2 differ only in the
physical locations of their segments and in the contents of memory outside of their
respective segments. In particular, they share the same values for the ACC, PC,
MAR, IR, and M registers; in the contents of memory location PM[1]; and in the
logical locations associated with their segments.
With this deÔ¨Ånition in hand, we can deÔ¨Åne behavior sensitivity precisely. An in-
struction i is behavior sensitive if there exists states S1 and S2 such that the following
conditions all hold:
‚Ä¢ S1 ‚âàx S2,
‚Ä¢ i is mode preserving and resource preserving on both S1 and S2,
‚Ä¢ i does not address trap on S1 or S2,
‚Ä¢ i(S1) Ã∏‚âàx i(S2).
Intuitively, i is behavior sensitive when it fails to behave similarly on two equivalent
states.
Example 10.7
Consider the instruction LDARR, which loads the value of RR into ACC if the pro-
cessor is in supervisor mode and traps if the processor is in user mode. (Notice that
this behavior makes LDARR a privileged instruction.)
Consider the following two states, where S1 ‚âàx S2 for some x Ã∏= 0:
S1 = (pm1,acc, pc,mar,ir,s,(base,bound)),
S2 = (pm2,acc, pc,mar,ir,s,(base+x,bound)).
Because the values of the mode bits are both s, executing LDARR from either state
will load the relevant RR value into the accumulator. Thus, the accumulator value for
i(S1) will be (base,bound), whereas the accumulator value for i(S2) will be (base+
x,bound). It follows that i(S1) Ã∏‚âàx i(S2), and hence LDARR is behavior sensitive.
‚ô¶
10.4.3
Virtualizable Processor Architectures
A key result of Popek and Goldberg‚Äôs work (Popek and Goldberg, 1974) is that a
virtual machine monitor can be constructed for processors that have relocation mech-
anisms, user and supervisor modes, and traps, if the set of sensitive instructions is a
subset of the set of privileged instructions. In other words, an instruction-set archi-
tecture is virtualizable if every sensitive instruction is also a privileged instruction.
The proof of this property can be found in the original paper.
Exercise 10.4.1
Consider the machine instructions of the simple processor de-
scribed in Chapter 9‚ÄîA Primer on Computer Hardware.

224
Access Control, Security, and Trust: A Logical Approach
a. Classify each of the following instructions as privileged, control sensitive, be-
havior sensitive, or innocuous:
LDA @A
LDA @A @B
ADD @B
STO @B
LDARR
SSM
SUM
LDRR @A
b. Assume that the above list of machine instructions fully enumerates the in-
struction set of the processor. Is the processor virtualizable? Justify your
answer.
Exercise 10.4.2
Suppose the processor state for a program in a segment described
by (base,bound) = (256,1024) is given by
S = (pm,acc,0,mar,ir,m,(256,1024)).
Answer the following questions.
a. What locations in physical memory does the active memory segment occupy?
b. Suppose the active segment is to be relocated to base address = 128 in physical
memory and executed. This new state is S‚Ä≤:
S‚Ä≤ = (pm,acc,0,mar,ir,m,(128,1024)).
What must be true for the program in the relocated segment to behave exactly
like the program in the original segment, where ‚Äúbehave exactly‚Äù means all
PSW values in the relocated segment will match for each execution state with
the exception of base = 256 and base‚Ä≤ = 128? Justify your answer.
10.5
Summary
The goal of this chapter was to introduce the fundamentals of memory protection,
virtual machines, and virtual machine monitors. Central to protecting memory is
segmentation as supported by a relocation register and a mode bit to protect the
relocation register.
Central to virtual machine monitors is the control program that consists of a dis-
patcher, resource allocator, and interpreters of privileged instructions. The equiv-
alence of machines with and without a virtual machine monitor was demonstrated

Virtual Machines and Memory Protection
225
using a mapping function that mapped states of one machine to equivalent states in
the other. A key result is that those instruction-set architectures where all sensitive
instructions are also privileged instructions are virtualizable.
The learning outcomes associated with this chapter appear in Figure 10.11.
10.6
Further Reading
Saltzer and Schroeder‚Äôs paper, The Protection of Information in Computer Systems
(Saltzer and Schroeder, 1975), is a classic paper on memory protection mechanisms.
The protection methods described in this chapter and following chapters are devel-
oped based on this paper. Popek and Goldberg‚Äôs paper Formal Requirements for
Virtualizable Third Generation Architectures (Popek and Goldberg, 1974) is a clas-
sic paper on virtual machine monitors. Its Ô¨Åndings are still relevant in current virtual
machines.

226
Access Control, Security, and Trust: A Logical Approach
FIGURE 10.11 Learning outcomes for Chapter 10
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
‚Ä¢ When given a block diagram of a processor using memory segmentation,
describe how virtual memory addresses are related to physical memory
addresses.
Application
‚Ä¢ When given a block diagram of a processor and control lines to registers
and arithmetic logic units, interpret timing diagrams describing processor
behavior.
Analysis
‚Ä¢ When given the value of the program status word and an instruction,
determine if the instruction will be address trapped or passed on for exe-
cution.
‚Ä¢ When given the deÔ¨Ånition of an instruction, be able to determine if it is
innocuous, privileged, behavior sensitive, or control sensitive.
‚Ä¢ When given a request to access a virtual (logical) address, determine if
accessing the address is permitted.
Synthesis
‚Ä¢ When given a description of a machine-level instruction and a block di-
agram of a processor with its control lines, devise microprograms that
implement the machine-level instruction.
‚Ä¢ When given a description of a machine-level instruction, be able to de-
scribe when the operation is performed or trapped through the use of
derived inference rules.

Chapter 11
Access Control Using Descriptors
and Capabilities
In Chapter 10, we introduced a simple memory-segmentation scheme based on a
single relocation register guarded by a mode bit. Under this scheme, each virtual
machine has access to precisely one memory segment, whose location is determined
by the base-address value base stored in the relocation register RR. The relocation
register thus serves as a single mechanism to support both segment addressing and
memory protection.
In this chapter, we introduce a more sophisticated and Ô¨Çexible scheme, known as a
capability system. This scheme separates the notions of addressing and protection by
distinguishing address descriptors (which serve as pointers to memory segments in
physical memory) from capabilities (unforgeable tickets that specify permitted oper-
ations on a given memory segment). In this scheme, each machine is given a catalog
(i.e., collection) of capabilities that together deÔ¨Åne its privileges to access various
memory segments. As a consequence, a single virtual machine may be granted a va-
riety of access privileges (e.g., read, write, or execute) to multiple memory segments.
11.1
Address Descriptors and Capabilities
When writing programs, programmers typically do not know the precise location
of memory segments. Thus, it is convenient to refer to memory segments by name
or by a unique identiÔ¨Åer, as opposed to a speciÔ¨Åc (base,bound) value. Address
descriptors provide the means to associate segment names (or unique identiÔ¨Åers)
with physical locations in memory. In turn, capabilities bind access rights (such as
read or write) to object identiÔ¨Åers or names. In the case of segments, this binding
is made possible by appending read and write permissions to segment names and
storing the results in capability registers.
Figure 11.1 shows how address descriptors and capabilities Ô¨Åt together. Each ad-
dress descriptor has the form (Ns,(baseNs,boundNs)), where Ns is a segment name
(i.e., a unique identiÔ¨Åer) and (baseNs,boundNs) point to the physical location of that
segment in memory. Taken together, these descriptors form a memory map that is
stored in the address descriptor segment (ADS). To provide protection, the ADS
must be under supervisory control, inaccessible to users, and trustworthy.
227

228
Access Control, Security, and Trust: A Logical Approach
FIGURE 11.1 Memory protection with address descriptors and capabilities
Figure 11.1 also includes a processor with two capability registers (CR[1] and
CR[2]), which are both protected by mode bit M. Each capability register holds a
capability, which comprises a read bit (R), a write bit (W), and a unique segment
id. The value of the read (respectively, write) bit indicates whether or not read (re-
spectively, write) access to the given segment is permitted.1 In this Ô¨Ågure, CR[1]
contains a capability that points to the address descriptor for Alice‚Äôs program seg-
ment; in contrast, CR[2] contains a capability that points to the address descriptor
for a shared-data segment.
We can express in the logic the contents of the ADS and of the capability registers
in the same way that we previously expressed the contents of other registers: that is,
we can treat ADS, CR[1], and CR[2] as principals who make statements about their
values. For example, the formula
ADS says (‚ü®(N1,(baseN1,boundN1))‚ü©‚àß¬∑¬∑¬∑ ‚àß‚ü®(Nn,(baseNn,boundNn))‚ü©)
expresses that there are n address descriptors stored in the ADS, with the jth descrip-
tor associating the segment name Nj with the physical locations given by the pair
(baseNj,boundNj). In a similar vein, Table 11.1 provides the possible interpretations
for a given capability stored in capability register CR[i].
1In many systems, there will also be an execute bit (X), which governs the ability to execute a segment. To
keep our discussion and diagrams simpler, we adopt the convention that read access also provides execute
access. Alternatively, one could incorporate the additional bit into capabilities and the capability registers.

Access Control Using Descriptors and Capabilities
229
Contents of CR[i]
Interpretation
(read,write,Ns)
CR[i] says (‚ü®read,Ns‚ü©‚àß‚ü®write,Ns‚ü©)
(read,not write,Ns)
CR[i] says (‚ü®read,Ns‚ü©‚àß¬¨‚ü®write,Ns‚ü©)
(not read,write,Ns)
CR[i] says (¬¨‚ü®read,Ns‚ü©‚àß‚ü®write,Ns‚ü©)
(not read,not write,Ns)
CR[i] says (¬¨‚ü®read,Ns‚ü©‚àß¬¨‚ü®write,Ns‚ü©)
Table 11.1: Interpretation of capability-register values
Example 11.1
Suppose capability register CR[1] has the value (read,not write,Math Program).
The contents of CR[1] can be expressed in the logic as follows:
CR[1] says (‚ü®read,Math Program‚ü©‚àß¬¨‚ü®write,Math Program‚ü©).
The process possessing the capability in CR[1] is able to read (and therefore also
execute) the shared math program, but the process is not allowed to modify it.
‚ô¶
Instructions previously deÔ¨Åned using the simple memory protection scheme of
Chapter 10 must be redeÔ¨Åned to incorporate the use of capabilities and address de-
scriptors. At a minimum, the new deÔ¨Ånitions must accommodate the use of capabil-
ity registers and the introduction of segment names Ns into virtual addresses. The
following example shows how the instruction STO (which stores the contents of the
ACC into a particular memory address) is altered to accommodate the new protection
scheme.
Example 11.2
Consider the instruction STO @(Ns,A), which stores the contents of the accumulator
ACC into address A of segment Ns. This instruction should be allowed to execute
only when the following conditions are all satisÔ¨Åed (otherwise, a trap should occur):
‚Ä¢ One of the capability registers CR[i] possesses a write capability for the seg-
ment Ns:
CR[i] says ‚ü®write,Ns‚ü©.
‚Ä¢ The ADS contains an address descriptor for Ns that associates it with some
particular physical segment (baseNs,boundNs):
ADS says (Ns,(baseNs,boundNs)).
‚Ä¢ The virtual address A is within bounds of the segment Ns and thus will not
trigger an address trap:
(A +baseNs) < q,
A < boundNs.

230
Access Control, Security, and Trust: A Logical Approach
These conditions are collectively summarized by the following derived inference
rule:
STO
@(Ns,A)
Supervisor controls (CR[i] says ‚ü®(writeNs,Ns)‚ü©‚äÉADS says ‚ü®(Ns,(baseNs,boundNs))‚ü©‚äÉ
(A +baseNs < q) ‚äÉ(A < boundNs) ‚äÉVM controls ‚ü®STO @(Ns,A)‚ü©)
Supervisor says (CR[i] says ‚ü®(writeNs,Ns)‚ü©‚äÉADS says ‚ü®(Ns,(baseNs,boundNs))‚ü©‚äÉ
(A +baseNs < q) ‚äÉ(A < boundNs) ‚äÉVM controls ‚ü®STO @(Ns,A)‚ü©)
VM says ‚ü®STO @(Ns,A)‚ü©
CR[i] says ‚ü®(writeNs,Ns)‚ü©
ADS says ‚ü®(Ns,(baseNs,boundNs))‚ü©
A +baseNs < q
A < boundNs
‚ü®STO @(Ns,A)‚ü©
.
‚ô¶
The memory protection scheme shown in Figure 11.1 protects the contents of the
two capability registers CR[1] and CR[2] using the mode bit M. In particular, the
capability registers CR[i] are only accessible if the processor is in supervisor mode.
The following example illustrates how this protection works.
Example 11.3
Consider the instruction LDACR[i], which loads the ACC with the contents of CR[i],
provided that the virtual machine is in supervisor mode; when the virtual machine
is in user mode, the instruction is trapped. (This instruction is the capability-register
analogue of the LDARR instruction introduced in Chapter 10.)
The following derived inference rules deÔ¨Åne the behavior of LDACR[i]:
LDACR[i]
Supervisor controls (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®LDACR[i]‚ü©)
Supervisor says (M says ‚ü®s‚ü©‚äÉVM controls ‚ü®LDACR[i]‚ü©)
M says ‚ü®s‚ü©
VM says ‚ü®LDACR[i]‚ü©
‚ü®LDACR[i]‚ü©
,
LDACR[i] trap
Supervisor controls (M says ‚ü®u‚ü©‚äÉVM says ‚ü®LDACR[i]‚ü©‚äÉ‚ü®trap‚ü©)
Supervisor says (M says ‚ü®u‚ü©‚äÉVM says ‚ü®LDACR[i]‚ü©‚äÉ‚ü®trap‚ü©)
M says ‚ü®u‚ü©
VM says ‚ü®LDACR[i]‚ü©
‚ü®trap‚ü©
.
In particular, LDACR[i] is executed only when the mode bit M indicates that the VM
is in supervisor mode. If M indicates that the VM is in user mode, then a trap occurs
instead.
‚ô¶
In the next section, we look at an alternative way to protect the capability regis-
ters, without using a mode bit. This alternative‚Äîknown as a tagged architecture‚Äî
permits user programs to load the capability registers, but only with values that are
known to be authorized descriptor values.
Exercise 11.1.1
Recall the LDA @A instruction in Chapter 10, which loads the

accumulator ACC with the contents of PM[base + A]]. Revise the inference rules
deÔ¨Åning the behavior of LDA @A (LDA memory trap 1, LDA memory trap 1, and
LDA @A), using address descriptors and capabilities.

Access Control Using Descriptors and Capabilities
231
Exercise 11.1.2
Consider the instruction STOCR[i], which loads the contents of the
accumulator ACC into capability register CR[i] provided the virtual machine is in
supervisor mode. Letting ‚ü®STOCR[i]‚ü©denote that the ACC contents should be loaded
into CR[i], propose and formally prove a derived inference rule for STOCR[i].
11.2
Tagged Architectures
The security of segmented memory depends on the integrity of relocation registers
and capability registers: if users are able to load these registers with arbitrary or
unauthorized permissions, then the security and integrity of physical and program
memory are lost. There are at least two ways to achieve integrity of these registers.
The Ô¨Årst approach is to grant access to capability registers only to those principals
trusted to create legitimate capabilities (e.g., the Supervisor). This approach can
be accommodated through the use of mode bits that indicate when the processor is
under supervisory control. To maintain the integrity of the capability registers, user
processes are denied access to the registers; only the Supervisor is granted access
to the capability registers. While secure, this approach is inÔ¨Çexible. For example,
suppose that there are two capability registers, but more than two memory segments
that a particular user‚Äôs program is allowed to access. If users lack the ability to load
capability values into capability registers, then they can access only the two memory
segments that the Supervisor loaded into the capability registers. Such a situation is
both limiting and inconvenient.
The second (and more Ô¨Çexible) approach is to limit the values that can be loaded
into capability registers, rather than to limit the principals who can alter the ca-
pability registers. This approach can be accommodated through the use of tagged
architectures. Essentially, the contents of memory are classiÔ¨Åed into one of two
types: capabilities (i.e., permissions) and non-capabilities (i.e., data). The type of
information stored at any memory location is indicated by a tag bit, which is part of
the word stored at any memory location.
To be explicit, tagged architectures do not necessarily eliminate the need for mode
bits. Although the mode bits are not used for determining whether to grant access to
capability registers, they are still useful for determining whether or not the Supervi-
sor is in control of the processor.
In tagged architectures, each memory location has an associated tag bit that is
either on or off. Thus, each memory location PM[(Ns, A)] stores a pair comprising
both a value and a tag. When the tag bit is on, the value in address A of segment Ns is
interpreted as a capability. When the tag bit is off, the value of address A in segment
Ns is interpreted as ordinary data and cannot be used as a capability. Table 11.2
shows how the values of these pairs can be interpreted in the logic.
The mandatory access control policy for reading from and writing to memory and
capability registers CR[i] is as follows:

232
Access Control, Security, and Trust: A Logical Approach
Contents of PM[(Ns, A)]
Interpretation
(value,off)
PM[(Ns, A)] says (‚ü®value‚ü©‚àß‚ü®data‚ü©)
(value,on)
PM[(Ns, A)] says (‚ü®value‚ü©‚àß‚ü®capability‚ü©)
Table 11.2: Interpretation of tag bit
1. Instructions that load a capability register with the value stored in a partic-
ular memory location are executed only if the tag bit in that location is on.
Otherwise, the instruction is trapped.
2. Instructions that store values from capability registers into memory are permit-
ted provided that there are no address traps and the virtual machine possesses
write permission on the relevant memory segment. The corresponding tag bit
is turned on as a result of storing a capability value.
3. All other memory store instructions turn the tag bit off.
The following example illustrates the mandatory access control policy for an in-
struction that attempts to update a capability register.
Example 11.4
Consider the instruction LDCR(i,@(Ns, A)) that attempts to load CR[i] with the value
in address A in segment Ns. The operation is successful provided that address A does
not address trap, the associated tag bit indicates the value is a capability value, and
the virtual machine has read permission on Ns.
Let ‚ü®LDCR (i,@(Ns,a))‚ü©denote that the operation should be permitted. The fol-
lowing inference rule describes the conditions under which the operation is allowed:
LDCR
(i,@(Ns, A))
Supervisor controls (CR[j] says ‚ü®readNs‚ü©‚äÉPM[(Ns, A)] says ‚ü®capability‚ü©‚äÉ
ADS says ‚ü®(Ns,(baseNs,boundNs))‚ü©‚äÉA +baseNs < q ‚äÉA < boundNs ‚äÉ
VM controls ‚ü®LDCR (i,@(Ns,a))‚ü©)
Supervisor says (CR[j] says ‚ü®readNs‚ü©‚äÉPM[(Ns, A)] says ‚ü®capability‚ü©‚äÉ
ADS says ‚ü®(Ns,(baseNs,boundNs))‚ü©‚äÉA +baseNs < q ‚äÉA < boundNs ‚äÉ
VM controls ‚ü®LDCR (i,@(Ns,a))‚ü©)
VM says ‚ü®LDCR (i,@(Ns,a))‚ü©
CR[i] says ‚ü®readNs‚ü©
PM[(Ns, A)] says ‚ü®capability‚ü©
ADS says ‚ü®(Ns,(baseNs,boundNs))‚ü©
A +baseNs < q
A < boundNs
‚ü®LDCR (i,@(Ns,a))‚ü©
‚ô¶
In the next section, we show how tagged architectures are used to support capabil-
ity systems, which provide secure and Ô¨Çexible sharing of data and programs among
users.

Access Control Using Descriptors and Capabilities
233
Exercise 11.2.1
Propose derived inference rules for LDCR (i,@(Ns,a)) that indi-
cate the conditions under which the LDCR instruction will trap. Provide an informal
argument justifying your inference rules and formally derive them.
Exercise 11.2.2
Consider the instruction STOCR (i,@(Ns,A)), which attempts to
store the contents of capability register CR[i] into address A of segment Ns. This
operation should be allowed, provided address A does not cause an address trap and
the virtual machine has write permission on Ns; otherwise, the instruction is trapped.
Let ‚ü®STOCR (i,@(Ns,A))‚ü©denote that the operation should be permitted. Propose
and formally prove a derived inference rule called STOCR (i,@(Ns,A)) that formally
deÔ¨Ånes the behavior of STOCR when there is no address trap and the user has write
permission on Ns.
Exercise 11.2.3
As we have seen, tagged architectures depend on the trustworthy
segregation of capability values from ordinary data. Identify the crucial elements of
micro-coded timing and control operations necessary for any implementation of a
tagged architecture to be trustworthy.
11.3
Capability Systems
The concepts of descriptors, capabilities, and tagged architectures combine to sup-
port capability systems. Capability systems are organized around the principle that
access to protected objects and services is granted using capabilities, which incorpo-
rate descriptors that refer to the protected objects and services. Because they incor-
porate tags, capability systems can grant user programs the ability to load and store
capabilities to and from the capability registers.
11.3.1
Catalogs
Capability systems are more Ô¨Çexible than the simple segmented memory system
described in Chapter 10, which used a mode bit for protecting descriptors. The ability
of user programs to read and store valid capabilities allows the use of catalogs of
capabilities, which simpliÔ¨Åes the task of securely sharing memory.
To see how catalogs work in capability systems, consider the processor and phys-
ical memory shown in Figure 11.2, which shows a processor with four capability
registers (CR[1] through CR[4]). The contents of these registers are as follows:
‚Ä¢ CR[1] contains a capability (read,write,Alice‚Äôs Program), which gives read
and write access to the segment in physical memory that is Alice‚Äôs program.
‚Ä¢ CR[2] contains a capability (read,write,Alice‚Äôs Catalog), which gives both
read and write access to Alice‚Äôs catalog of capabilities.

234
Access Control, Security, and Trust: A Logical Approach
FIGURE 11.2 Catalogs and capabilities
‚Ä¢ CR[3] contains capability C2, which is a capability in Alice‚Äôs catalog that
allows Alice to read to or write from the Shared Data segment she shares with
Bob.
‚Ä¢ CR[4] contains capability C4, which is a capability in Alice‚Äôs catalog that gives
her read access to the Math Program segment, which she also shares with Bob.
The capabilities in Figure 11.2 serve as both pointers and privileges to memory
segments that contain data and/or programs. Because the capabilities exist within
the context of a tagged architecture, they can be read from memory into capability
registers and written from capability registers into memory. For each user‚Äôs virtual
machine, the Supervisor provides a memory segment (i.e., the user‚Äôs catalog) that
contains the capabilities that the user is authorized to use.
The following example illustrates the use of catalogs to provide Ô¨Çexible storage
and use of user capabilities.
Example 11.5
Consider the situation shown in Figure 11.2. Suppose we wish to have the contents
of CR[1] point to the math program, while still preserving the processor‚Äôs access to
Alice‚Äôs program. We can see that the math-program capability C4 is in Alice‚Äôs capa-
bility catalog; the capability for Alice‚Äôs program is not. For simplicity, let us refer to
the capability for Alice‚Äôs program as C6 and the virtual address of any capability Ci
as addrCi.

Access Control Using Descriptors and Capabilities
235
The contents of Alice‚Äôs catalog are as follows:
PM[(Alice‚Äôs Catalog,addrC2)] = (read,write,Shared Data),
PM[(Alice‚Äôs Catalog,addrC3)] = (read,write,Alice‚Äôs Data),
PM[(Alice‚Äôs Catalog,addrC4)] = (read,not write,Math Program).
If C4 is loaded immediately into the register CR[1], then the processor will lose
the capability to access Alice‚Äôs program. Hence, two steps are necessary: the Ô¨Årst
stores C6 into addrC6 of Alice‚Äôs Catalog, and the second loads C4 into CR[1]. In
this way, access to Alice‚Äôs program can be maintained through Alice‚Äôs catalog.
‚ô¶
11.3.2
Creating New Segments
Recall from Chapter 10 that one of the requirements for virtualizability is that all
security-sensitive instructions are trapped and executed under supervisory control. In
particular, any attempt to alter the allocation of memory must be done under control
of the Supervisor. Therefore, user requests for additional memory resources in the
form of new memory segments must be handled by the Supervisor control program,
because they reÔ¨Çect requests to change the resources allocated to users.
To illustrate how new memory segments could be created in ways that support
virtualization, suppose that the instruction CREATESEG Ns is employed by users to
create a memory segment Ns. When invoked by a user, CREATESEG Ns is trapped,
and control is turned over to the control program‚Äôs resource allocation module.
When CREATESEG Ns is trapped, the program status word PSW‚Äîwhich includes
the contents of the instruction register IR containing CREATESEG Ns‚Äîis stored in
PM[0]. In this way, the control program‚Äôs dispatcher module can detect that the
user is attempting to create a new memory segment with the name Ns and then pass
control to the allocation module.
The allocation module then decides whether or not to honor the request CREATE-
SEG Ns. If the allocation module decides to honor the request and Ns is not already
being used as a segment name in the ADS, then the allocation module assigns re-
sources to the user by (1) allocating a memory segment (baseNs,boundNs) for Ns, (2)
placing the address descriptor (Ns,(baseNs,boundNs)) into the ADS at some loca-
tion addrNs, and (3) writing the capability (read,write,Ns) into one of the capability
registers.
The above process can be performed with either hardware or microcode support
by another instruction MKSEG (i,Ns,addrNs,read,write,(baseNs,boundNs)). MKSEG
should only be executed by a processor in supervisor mode, and only when the new
segment is within the bounds of physical memory.
Figure 11.3 contains three inference rules, which collectively deÔ¨Åne the operation
of MKSEG. The Ô¨Årst rule (MKSEG user trap) states that MKSEG traps when called
from a virtual machine in user mode. The second rule (MKSEG address trap) states
that MKSEG also traps when the base and bound address parameters of segment Ns

236
Access Control, Security, and Trust: A Logical Approach
FIGURE 11.3 Inference rules for MKSEG instruction
MKSEG
user
trap
Supervisor controls (M says ‚ü®u‚ü©‚äÉ
VM says ‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©‚äÉ‚ü®trap‚ü©)
Supervisor says (M says ‚ü®u‚ü©‚äÉ
VM says ‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©‚äÉ‚ü®trap‚ü©)
M says ‚ü®u‚ü©
VM says ‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©
‚ü®trap‚ü©
MKSEG
address
trap
Supervisor controls (baseNs +boundNs ‚â•q ‚äÉ
VM says ‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©‚äÉ‚ü®trap‚ü©)
Supervisor says (baseNs +boundNs ‚â•q ‚äÉ
VM says ‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©‚äÉ‚ü®trap‚ü©)
baseNs +boundNs ‚â•q
VM says ‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©
‚ü®trap‚ü©
MKSEG
Supervisor controls (M says ‚ü®s‚ü©‚äÉbaseNs +boundNs < q ‚äÉ
VM says ‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©‚äÉ
‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©)
Supervisor says (M says ‚ü®s‚ü©‚äÉbaseNs +boundNs < q ‚äÉ
VM says ‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©‚äÉ
‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©)
M says ‚ü®s‚ü©
baseNs +boundNs < q
VM says ‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©
‚ü®MKSEG(i,Ns,addrNs,read,write,(baseNs,boundNs))‚ü©
exceed the size of physical memory q. The third rule (MKSEG) captures the non-
trapping behavior of the instruction: a new memory segment is formed when the
virtual machine is in supervisor mode and segment Ns falls within the bounds of
physical memory (i.e., baseNs +boundNs < q).
The following example illustrates how a user‚Äôs request to create a new memory
segment Ns is handled.
Example 11.6
Suppose Alice wishes to create a new memory segment Ns over which she will have
read and write access. Let us assume that, in this particular capability system, the
capability (read,write,Ns) will always be placed in CR[3]‚Äîthe thinking here is that
CR[1] holds the capability for Alice‚Äôs program and CR[2] holds the capability for
Alice‚Äôs capability catalog.
The following sequence of events creates Alice‚Äôs new segment:
1. Alice requests creation of a new memory segment Ns via the instruction CRE-
ATESEG Ns.

Access Control Using Descriptors and Capabilities
237
2. CREATESEG Ns is trapped, and control is passed to the Supervisor control pro-
gram.
3. The resource allocation module of the control program decides whether or not
to grant the request. If the request is granted, then memory is allocated for
the new segment Ns: that is, (baseNs,boundNs) is set, and the control program
executes the instruction MKSEG (i,Ns,addrNs,read,write,(baseNs,boundNs)).
4. The address descriptor (Ns,(baseNs,boundNs)) is added to the address descrip-
tor segment ADS, and the capability (read,write,Ns) is placed into CR[3].
5. Control of the processor is returned to Alice by restoring the values of the
PSW (except for CR[3]).
‚ô¶
11.3.3
Dynamic Sharing
The description of catalogs presented so far support only static sharing: the autho-
rizations for memory segments are determined prior to program execution. In many
cases, however, it is desirable to support dynamic sharing, whereby users may share
segments that are created during a program‚Äôs execution. As we shall see, dynamic
sharing of memory segments requires trusted communication of capabilities among
principals.
For example, recall Example 11.6, where Alice created a new memory segment
(let us call it SharedAlice). As a result of this action, she possesses the capability
(read,write,SharedAlice).
Now, suppose that Alice wishes to make this newly created memory segment ac-
cessible to Bob as well. Because users can copy capabilities, granting access to
SharedAlice is possible by creating a set of circumstances whereby Bob can both (1)
copy the capability (read,write,SharedAlice) and (2) know and trust that SharedAlice
is the named segment intended for him to share with Alice. There are at least two ap-
proaches for accomplishing this task. Both are based on the idea of communication
segments, which are memory segments set up to exchange capabilities among users.
The Ô¨Årst approach, which is illustrated in Figure 11.4, uses two communication
segments for each pair of users. When Alice wishes to share a capability with Bob,
she writes the capability into the segment Alice2Bob, to which she has read and write
access. Bob‚Äîwho has read access to Alice2Bob‚Äîcan then copy the capability into
one of his capability registers or to a memory location to which he has write access.
Bob will believe that the capability came from Alice if he believes that Alice is
the only user (other than the trusted Supervisor) who can write into the Alice2Bob
segment. That is, Bob must believe the following:
Alice2Bob ‚áíAlice.

238
Access Control, Security, and Trust: A Logical Approach
FIGURE 11.4 Pairwise shared communications segments
FIGURE 11.5 Formal analysis for pairwise shared communication segment
1. Alice2Bob ‚áíAlice
Trust assumption
2. Alice controls œïcapability
Alice‚Äôs jurisdiction over her capabilities
3. Alice2Bob says œïcapability
Capability in Alice2Bob
4. Alice says œïcapability
1, 3 Derived speaks for
5. œïcapability
2, 4 Controls
If Bob believes that the segment Alice2Bob is in fact under Alice‚Äôs sole control,
then the analysis of the situation from Bob‚Äôs perspective can be summarized as in
Figure 11.5, where œïcapability is the expression that represents the capability Alice
passed to Bob.
The downside to this approach is that it is relatively expensive in terms of commu-
nication segments: if there are N users, then each user requires N ‚àí1 communication
segments (i.e., one for each other user), resulting in a total of N √ó(N ‚àí1) communi-
cation segments.
A more economical approach is illustrated in Figure 11.6. This approach uses a
single communication segment for each user, resulting in a need for only N com-
munication segments for N users. Each user‚Äôs communication segment serves as a
mailbox: when users wish to send capabilities to other users, they ask the Supervisor
to place the capabilities in the recipient‚Äôs mailbox on their behalf. When doing so,
the Supervisor indicates the sender of each capability.
To understand the idea behind this approach, suppose that Alice sends a capability
(read,write,SharedAlice) to Bob via the Supervisor: the Supervisor places something
like (Alice,(read,write,SharedAlice)) into Bob‚Äôs mailbox ForBob. From Bob‚Äôs per-
spective, this state of affairs can be expressed in the logic as follows, where œïcapability
is the expression that represents the passed capability:
ForBob | Alice says œïcapability.
That is, Bob‚Äôs mailbox ForBob asserts that Alice sent the capability. Because the
Supervisor is in control of Bob‚Äôs mailbox, Bob believes that anything in his mailbox

Access Control Using Descriptors and Capabilities
239
FIGURE 11.6 Mailbox segments
FIGURE 11.7 Formal analysis for mailbox communication segment
1. Supervisor | Alice ‚áíAlice
Trust assumption
2. Alice controls œïcapability
Alice‚Äôs jurisdiction over her capabilities
3. ForBob | Alice says œïcapability
Capability in Bob‚Äôs mailbox
4. ForBob ‚áíSupervisor
Trust assumption
5. Alice ‚áíAlice
Idempotency of ‚áí
6. ForBob | Alice ‚áíSupervisor | Alice
4, 5 Monotonicity of ‚áí
7. Supervisor | Alice says œïcapability
6, 3 Derived Speaks For
8. Alice says œïcapability
1, 7 Derived speaks for
9. œïcapability
2, 8 Controls
is placed there by the Supervisor:
ForBob ‚áíSupervisor.
From the idempotency and monotonicity of ‚áí, Bob can conclude that
ForBob | Alice ‚áíSupervisor | Alice.
Bob relies upon the trust assumption that the Supervisor correctly authenticates mes-
sages from each user (in this case, Alice). This assumption is expressed as
Supervisor | Alice ‚áíAlice.
The complete formal justiÔ¨Åcation for Bob‚Äôs interpretation of the situation appears in
Figure 11.7. This justiÔ¨Åcation depends upon two essential trust assumptions that Bob
must make: (1) that only the Supervisor can place items in the mailbox segments,
and (2) that the Supervisor is trustworthy.
11.3.4
Revocation of Capabilities
As we have seen, capability systems allow users to load, store, and share capabili-
ties. However, it is sometimes necessary to revoke previously granted privileges. As

240
Access Control, Security, and Trust: A Logical Approach
we shall see, capability revocation can be difÔ¨Åcult, unless restrictions are imposed
on how capabilities are stored or shared.
For example, consider what happens when Alice sends Bob a capability to access
one of her segments. After receiving the capability, Bob can load it into one of his
capability registers and then freely store it in any location in memory that he can
access. In particular, he can store it in an arbitrary number of arbitrary locations‚Äî
even the Supervisor will not necessarily know how many copies exist and where they
are located.
If Alice later decides to revoke Bob‚Äôs access to her segment, then the system must
be able to invalidate all of Bob‚Äôs copies of the capability. To accomplish this task,
either (1) the Supervisor must perform an exhaustive search of all memory accessible
to Bob, or (2) Alice must move the contents of the segment she shared with Bob to
a new segment, destroy the original segment, and issue capabilities to the new one.
Both options are unattractive from an efÔ¨Åciency perspective.
To reduce the task of locating revoked capabilities, we can institute a mandatory
access-control policy that restricts capability storing to a relatively small number
of known locations. For example, each user might be restricted to having just one
segment for storing capabilities. If the policy is enforced without exception, then
searching for revoked capabilities is greatly simpliÔ¨Åed: only a small number of loca-
tions must be checked.
Another approach is to use indirect references to intermediate objects. For exam-
ple, if Alice wishes to grant Bob access to segment A, she creates a new object B that
points to A, and gives Bob a capability to access B. She follows a similar approach
for each principal to whom she wishes to grant access to A; in each case, she creates
a different intermediate object. If Alice wishes to revoke Bob‚Äôs capability to access
A, she simply removes B, thus destroying Bob‚Äôs link to A. All other principals with
capabilities to access A still retain their privileges.
A third approach is to limit which capabilities can be copied. The simplest way
to accomplish this task is to include a copy bit with each capability. When a user
program attempts to store a capability from the capability register into memory, the
capability‚Äôs copy bit is checked. If the copy bit is on, then the attempt succeeds
and the capability is stored; if the copy bit is off, then the attempt is trapped and
blocked. This type of use of a copy bit amounts to an all-or-nothing approach: more
sophisticated schemes can be developed to permit a capability to be copied a speciÔ¨Åc
number of times.
Exercise 11.3.1
Figure 11.2 illustrated the use of descriptors as capabilities while

omitting the use of address descriptors for simplicity. Redraw the Ô¨Ågure explicitly
showing the application of address descriptors.
Exercise 11.3.2
Consider the following speciÔ¨Åcations for a small system:
‚Ä¢ Ali and Boris run separate programs to which only they as program owners
have access.

Access Control Using Descriptors and Capabilities
241
‚Ä¢ Ali trusts Boris to be careful about not overwriting her data. She wishes to
share data with Boris that she creates, and she needs to access data that Boris
creates for her.
‚Ä¢ Ali has mistakenly overwritten Boris‚Äôs data in the past, so he is more careful
about granting write access to Ali. He needs to share data that he creates for
her, and he needs to see data that Ali creates for him.
‚Ä¢ Ali has private data that only she should see.
‚Ä¢ Ali has data that she needs to share only with Clancy.
‚Ä¢ Ali, Boris, and Clancy need to read a common database under the control of
the Supervisor.
Devise a collection of capabilities that satisfy the above, and distribute the capabil-
ities into catalogs for Ali, Boris, and Clancy. Draw a diagram of physical memory
(similar to Figure 11.2) illustrating the contents of the catalogs and referenced mem-
ory segments. You may omit address descriptors.
Exercise 11.3.3
The deÔ¨Ånition of MKSEG (Ns,(baseNs,boundNs)) in the text as-
sumed direct hardware support. Assume that there is no direct hardware support
for MKSEG. Devise methods, instructions, and derived inference rules that add the
descriptor (Ns,(baseNs,boundNs)) to the address descriptor segment ADS and place
the capability (read,write,Ns) into CR[3].
11.4
Summary
In this chapter, we introduced capability systems, which provide a Ô¨Çexible scheme
that supports both segment addressing and memory protection. In capability sys-
tems, descriptors (i.e., protected physical addresses) provide the means for segment
addressing, while capabilities (i.e., unforgeable tickets that authorize access to mem-
ory) provide for memory protection. Capability systems employ tagged architectures
to distinguish capabilities from other data, thereby safely allowing user programs the
Ô¨Çexibility to load, store, and share authorized capability values.
Although capabilities are easy to issue and to distribute, they can be challenging to
revoke. Typical revocation solutions impose some limitations on capabilities, either
in terms of where they may be stored, whether they may be copied, or how they may
be shared with other users.
Figure 11.8 summarizes the learning outcomes for this chapter.

242
Access Control, Security, and Trust: A Logical Approach
11.5
Further Reading
In their classic paper (Saltzer and Schroeder, 1975), Saltzer and Schroeder pro-
vide a thorough overview of the underlying concepts of capabilities and descriptors.
They also describe access-control list (ACL) systems as an alternative to capability
systems: instead of distributing capabilities to principals, each protected object has
its own access controller. Each access controller has the form of an address descrip-
tor with an access-control list that enumerates principals and their access rights on
that object.
The use of capabilities is not limited to granting access rights to memory seg-
ments. If address descriptors are used for other objects‚Äîsuch as communications
channels, printers, I/O devices, and applications in general‚Äîthen capabilities can be
used for the relevant operations on those objects. This more extensive use of capa-
bilities is discussed in Levy‚Äôs book (Levy, 1984), which provides details of thirteen
experimental and commercial computer systems that use descriptors and capabilities.

Access Control Using Descriptors and Capabilities
243
FIGURE 11.8 Learning outcomes for Chapter 11
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
‚Ä¢ When given a diagram of address descriptors and capabilities, be able to
interpret or describe how objects are identiÔ¨Åed and physically addressed,
as well as how operations on objects are authorized.
Application
‚Ä¢ When given the protection goals of a system, apply the concepts of capa-
bilities and address descriptors to devise protection policies and mecha-
nisms.
Analysis
‚Ä¢ When given an access request, determine if the capabilities at hand are
sufÔ¨Åcient to grant access.
Synthesis
‚Ä¢ When given the protections goals of a system, design appropriate poli-
cies, architectures, instructions, and mechanisms for implementing ad-
dress descriptors and capabilities.
‚Ä¢ When given descriptions of policies, architectures, instructions, and
mechanisms, create formal descriptions of each.
Evaluation
‚Ä¢ When given formal descriptions of policies, architectures, instructions,
and mechanisms, express access-control decisions as formally derived
theorems in the access-control logic.


Chapter 12
Access Control Using Lists and
Rings
In Chapter 11, we introduced capability systems, which separate the notions of ad-
dressing and protection by distinguishing address descriptors from capabilities. Each
virtual machine associated with a process was given a catalog of capabilities that
identiÔ¨Åed the segments the virtual machine could access as well as the associated
access rights on those segments. The alternative to capabilities is access-control lists
(ACLs), which grant privileges to principals by name.
In this chapter, we describe how to use ACLs to provide discretionary access con-
trol for memory segments. When a principal requests access to a segment, the seg-
ment‚Äôs ACL is checked to determine whether the principal‚Äôs request is allowed. The
advantage of using ACLs is that removing or changing access to a resource does not
entail searching through each and every memory segment for a capability. As we
will see, however, the price for adding access-control lists to segment protection is
another layer of indirection.
We also introduce protection rings, which support mandatory access control for
memory segments. Protection rings are a generalization of the user and supervisor
modes, which provides a Ô¨Åner-grained approach to protecting system integrity. Ring-
based access control is a layered approach to mandatory access control: processes
operating in the innermost rings have the greatest access, and data stored in the in-
nermost rings are afforded the greatest protection. This Ô¨Åner-grained approach to
access control provides greater Ô¨Çexibility and speciÔ¨Åcity on how data and processes
are accessed and by whom.
12.1
Generalized Addresses
Figure 12.1 illustrates a sample user‚Äôs conceptual view of memory that contains
four segments: each segment has a name, an associated access-control list, and data
contents. From the user‚Äôs point of view, the segments do not have speciÔ¨Åc addresses
associated with them. Instead, the user refers to the given segments and their con-
tents through the use of symbolic names, allowing her to write programs that are
independent of the segments‚Äô physical locations in memory. For example, the pro-
gram may have a data segment named AliceInformation from which it attempts to
245

246
Access Control, Security, and Trust: A Logical Approach
FIGURE 12.1 User view of memory
extract the contents of the symbolic location phoneNumber.
Of course, the actual layout of physical memory is quite different, and requests
to access memory ultimately must be converted into physical addresses. The Ô¨Årst
step in the conversion process occurs when the program is linked. The linker trans-
lates the location-within-a-segment reference into a generalized address (s,i), where
s and i are both natural numbers. The value s serves to identify the particular mem-
ory segment, while the value i identiÔ¨Åes a particular location within that segment.
Conceptually, the generalized address (s,i) refers to location i within the segment
numbered s. The details of the linker‚Äôs translation process are beyond the scope of
this text. Instead, in this section and the next, we turn to the details of how the
generalized address (s,i) is converted into a physical address in memory.
As we saw in Chapter 11, address descriptors‚Äîwhich give the starting location
and the size of segments‚Äîare stored in a catalog called the address descriptor seg-
ment (ADS). Each virtual machine has a descriptor base register (DBR) that points
to the base location of the ADS and contains the bound of the ADS.
The address descriptor for a segment numbered s is stored in the sth location of
the ADS. Thus, for example, if the linker associates the natural number K with the
symbolic segment name Ns, then the address descriptor for Ns is stored in location K
of the ADS. Furthermore, the value K is stored in one of two registers, depending on
the contents of the segment:1 (1) the procedure base register, if the segment is a pro-
cedure segment (i.e., contains instructions only), or (2) the temporary base register,
if the segment contains data.
Figure 12.2 illustrates how the relevant registers of a virtual machine are used to
determine the location of segment Ns‚Äôs address descriptor. The DBR contains the
1A common design discipline is to separate executable instructions from alterable data by keeping them
in separate segments. This approach supports pure procedures (i.e., executable code kept separate from
writable data), which reduce the likelihood of mistakenly executing data as instructions. The mechanisms
we describe in this chapter support pure procedures.

Access Control Using Lists and Rings
247
FIGURE 12.2 Address formation for address descriptor
address descriptor for the ADS. Depending upon whether Ns is a procedure or data
segment, either the procedure base register (PBR) or temporary base register (TBR)
contains the location of Ns‚Äôs address descriptor, relative to the base address of the
ADS. This value is the natural number that the linker associated with Ns (i.e., K in
our example). The physical address of Ns‚Äôs address descriptor is therefore
DBR.base+K,
provided that K < DBR.bound and DBR.base+K < q, where q is the size of physical
memory. That is, the value K must be within bounds of both the ADS and physical
memory.
Throughout the remainder of this chapter, we will refer to segments and their
locations via their linker-associated numbers, rather than their symbolic names.
12.2
Segment Access Controllers
In the capability systems of Chapter 11, address descriptors to generic memory
segments were (base,bound) values that corresponded directly to that segment‚Äôs
starting address and its number of locations in physical memory. For systems that use
access-control lists to protect memory segments, address descriptors instead point to
the access-controller segment that guards the named memory segment.
Access controllers have two parts: (1) the (base,bound) pair that points to the
segment being guarded, and (2) a list of authorized principal identiÔ¨Åers paired with
their access rights to the guarded memory segment. Whenever a processor running
a process with a particular principal identiÔ¨Åer wishes to access a speciÔ¨Åc memory
segment, the access controller for that segment is consulted to determine whether the
principal identiÔ¨Åer is listed with the requested access right. If so, then the operation
is permitted; otherwise, the request is trapped.

248
Access Control, Security, and Trust: A Logical Approach
FIGURE 12.3 Segment access controller
FIGURE 12.4 Data or instruction segment
Figure 12.3 illustrates an access controller for a memory segment K. The Ô¨Årst
location of the access controller contains the address descriptor (baseK,boundK),
which points to the block of addresses in physical memory that deÔ¨Åne the guarded
memory segment K. The remaining locations contain the ACL entries, which consist
of principal identiÔ¨Åers and their associated permissions. For example, if the principal
identiÔ¨Åer register contains the principal identiÔ¨Åer Carol, then both read and write
access to the protected memory segment is permitted.
Figure 12.4 shows the segment K itself. The contents of the instruction counter
IC (if K is a procedure segment) or the segment address register SAR (if K is a data
segment) indicates the speciÔ¨Åc location within K to be accessed. Thus, for example,
an attempt to access generalized address (K,‚Ñì) would have ‚Ñìas the value of the IC
or SAR.
Figure 12.5 summarizes the four steps of converting a generalized address (s,i)
into an actual location in physical memory:
1. The address descriptor for the ADS assigned to the processor is located in the
descriptor base register (DBR); its value is (base1,bound1).
2. The address descriptor for s‚Äôs access controller is located in address s of the
ADS (i.e., in physical address base1 + s). Its value is (base2,bound2), which
points to the access controller that guards segment s.

Access Control Using Lists and Rings
249
FIGURE 12.5 Generalized address formation and access control
3. Location 0 of the access controller for segment s (i.e., physical location base2)
contains the address-descriptor value (base3,bound3), which points to the ac-
tual segment s. The remaining locations of the access controller contain the
ACL entries for principals who have access rights to segment s.
4. Location i within the segment s can be found at physical address base3 +i.
Exercise 12.2.1
Suppose George creates a data segment that he wishes to share
with Karen. George as the creator of the segment has read and write permission; he
wishes Karen to have read permission only.
Draw a diagram similar to Figure 12.5 that contains the explicit values for the
registers in Karen‚Äôs processor, the address descriptors, and the access-control list
entries to describe the following situation:
‚Ä¢ The generalized address for the relevant location within the shared segment is
(5,12).
‚Ä¢ Karen‚Äôs address descriptor segment is located in physical addresses 100
through 199.
‚Ä¢ The access controller for the shared segment is located in physical addresses
200 through 250.
‚Ä¢ The shared segment is located in physical addresses 300 through 400.
12.3
ACL-Based Access Policy for Memory Accesses
In the previous section, we fully described the process for calculating a physical
address from a generalized one. We now introduce a simple virtual machine (VM)
and virtual machine monitor (VMM) that supports generalized addresses and uses
ACLs to control memory accesses.

250
Access Control, Security, and Trust: A Logical Approach
FIGURE 12.6 Virtual machine and virtual machine monitor for ACL system
Figure 12.6 shows a partitioning of registers and segments that support VM and
VMM functions. The VM has the following Ô¨Åve registers: (1) the instruction regis-
ter (IR); (2) the accumulator (ACC); (3) the generalized instruction address, which
comprises the procedure base register (PBR) and instruction counter (IC); (4) the
generalized data address, which comprises the temporary base register (TBR) and
data address register (DAR); and (5) the principal ID register (PID).
The VMM has one register: the descriptor base register (DBR), which points to the
catalog of address descriptors in the ADS available to the VM. The ADS and access
controller segment for segment s are also used by the VMM to evaluate instruction
requests made by the VM.
Based on this structure, we develop the access-control policy for a generic instruc-
tion OP [s : i], which involves the operation OP on the generalized address (s,i). As
we will see, three memory fetches are required. For each memory fetch, we iden-
tify both the relevant address descriptor and the required mandatory access-control
(MAC) condition:
1. Look up the location of the ADS
As we saw in the previous section, the segment parameters for the ADS are
found in the base and bound Ô¨Åelds of the DBR:
base1 = DBR.base,
bound1 = DBR.bound.
The necessary MAC conditions are that the address value base1 + s must be
within the limits of physical memory and that s is within the segment boundary
of the ADS:
(base1 +s < q)‚àß(s < bound1).

Access Control Using Lists and Rings
251
2. Look up the location of the access-controller segment
The parameters for the access-controller segment are found in the base and
bound Ô¨Åelds of the sth location of the ADS segment:
base2 = PM[base1 +s].base,
bound2 = PM[base1 +s].bound.
We assume that, within the access-controller segment for s, indexid gives the
location of the permissions associated with id. The MAC address conditions
therefore check to determine whether base2 + indexid and indexid are within
the bounds of physical memory and of the access-controller segment, respec-
tively:
(base2 +indexid < q)‚àß(indexid < bound2).
In addition to the MAC conditions are the discretionary access control con-
ditions. In this case, the permission Ô¨Åeld corresponding to OP is checked to
determine whether the principal requesting to perform OP has permission to
do so:
PM[base2 +indexid].op says ‚ü®OP‚ü©.
For example, when OP corresponds to a memory-write operation, the write
permission Ô¨Åeld of PM[base2 +indexid] (i.e., PM[base2 +indexid].write) must
be checked.
3. Look up the generalized address (s,i)
The parameters for segment s are given by the following descriptor values:
base3 = PM[base2].base,
bound3 = PM[base2].bound.
The MAC conditions are as expected, namely that base3 +i must be within the
limits of physical memory and i must be within segment boundaries:
(base3 +i < q)‚àß(i < bound3).
We can combine these three sets of requirements to form the access-control policy
for approving execution of the operation OP [s : i]. The access-control policy consists
of the typical three parts:
1. The state of the machine, as given by the contents of IR, DBR, PID, and the
contents of physical memory PM
2. Discretionary access permission, as indicated by the statement
PM[base2 +indexid].op says ‚ü®OP‚ü©.

252
Access Control, Security, and Trust: A Logical Approach
3. The absence of any addressing problems, as indicated by the following three
conditions:
(base1 +s < q)‚àß(s < bound1),
(base2 +indexid < q)‚àß(indexid < bound2),
(base3 +i < q)‚àß(i < bound3).
The entire policy can be expressed in the logic as follows:
IR says ‚ü®OP[s : i]‚ü©‚äÉ(DBR says ‚ü®(base1,bound1)‚ü©) ‚äÉ(PID says ‚ü®indexid‚ü©) ‚äÉ
(PM[base2 +indexid].op says ‚ü®OP‚ü©) ‚äÉ((base1 +s < q)‚àß(s < bound1)) ‚äÉ
((base2 +indexid < q)‚àß(indexid < bound2)) ‚äÉ((base3 +i < q)‚àß(i < bound3)) ‚äÉ
(IR controls ‚ü®OP[s : i]‚ü©)
IR says ‚ü®OP[s : i]‚ü©
DBR says ‚ü®(base1,bound1)‚ü©
PID says ‚ü®indexid‚ü©
PM[base2 +indexid].op says ‚ü®OP‚ü©
(base1 +s < q)‚àß(s < bound1)
(base2 +indexid < q)‚àß(indexid < bound2)
(base3 +i < q)‚àß(i < bound3)
‚ü®OP[s : i]‚ü©
The following example shows how this general access-control policy can be in-
stantiated for a speciÔ¨Åc operation on a given generalized address.
Example 12.1
Consider a request to execute the operation LDA[s : i], which loads the accumulator
ACC with the contents of generalized address (s,i). The derivable inference rule that
captures the conditions under which the request should be granted is as follows:
IR says ‚ü®LDA[s : i]‚ü©‚äÉ(DBR says ‚ü®(base1,bound1)‚ü©) ‚äÉ(PID says ‚ü®indexid‚ü©) ‚äÉ
(PM[base2 +indexid].read says ‚ü®LDA‚ü©) ‚äÉ((base1 +s < q)‚àß(s < bound1)) ‚äÉ
((base2 +indexid < q)‚àß(indexid < bound2)) ‚äÉ((base3 +i < q)‚àß(i < bound3)) ‚äÉ
(IR controls ‚ü®LDA[s : i]‚ü©)
IR says ‚ü®LDA[s : i]‚ü©
DBR says ‚ü®(base1,bound1)‚ü©
PID says ‚ü®indexid‚ü©
PM[base2 +indexid].read says ‚ü®LDA‚ü©
(base1 +s < q)‚àß(s < bound1)
(base2 +indexid < q)‚àß(indexid < bound2)
(base3 +i < q)‚àß(i < bound3)
‚ü®LDA[s : i]‚ü©
‚ô¶
Exercise 12.3.1
Prove the soundness of the behavioral theorem of Example 12.1.
Exercise 12.3.2
Devise and prove sound the access-control policy for STO [s : i]
(i.e., storing the contents of generalized address (s,i) into the accumulator ACC).

Access Control Using Lists and Rings
253
FIGURE 12.7 Protection rings
12.4
Ring-Based Access Control
Protection rings are a means to provide layered protection: the most trusted and
trustworthy procedures and data segments are in the innermost rings, while less
trusted and trustworthy processes and data are in the outermost rings.
Figure 12.7 illustrates the notions behind protection rings. The innermost ring is
ring 0: it lies at the center of concentric rings of privilege. The most trusted (and,
one hopes, most trustworthy) supervisory procedures and data segments are in ring
0: ring-0 procedures have the greatest access privileges, and ring-0 data have the
most protection. Subsequent rings are labeled 1 through n, with each successive
ring having fewer access rights than the ring that precedes it. User processes and
data, located in the outermost rings, have relatively few access privileges and are
accessible by more privileged processes.
Protection rings were introduced in the Multics operating system (Schroeder and
Saltzer, 1972). Although Multics segments were guarded by both rings and ACLs,
neither ACLs or capabilities are necessary for ring-based protection. Instead, associ-
ated with each segment are the following permission-related components: privilege
bits to indicate which types of access (i.e., read, write, or execute) are permitted on
the segment;2 a ring bracket, speciÔ¨Åed by three ring numbers (r1,r2,r3); and (for
2Note that, if the system does use ACLs or capabilities, then these privilege bits are unnecessary. In such
cases, read/write/execute access for a given segment is determined on a principal-by-principal basis. In

254
Access Control, Security, and Trust: A Logical Approach
procedure segments) an integer Ô¨Åeld gate that indicates its number of call gates. We
describe the purpose of the ring-bracket and gate Ô¨Åelds in the following subsections.
12.4.1
Access Brackets
Each segment‚Äôs ring bracket (r1,r2,r3) must satisfy the following condition:
r1 ‚â§r2 ‚â§r3.
Collectively, the three ring numbers deÔ¨Åne two additional brackets: an access bracket
(which we describe here) and a call bracket (which we defer to Subsection 12.4.2).
Intuitively, a segment‚Äôs access and call brackets constrain the ring levels from which
processes can access that segment.
Consider a segment with ring bracket (r1,r2,r3). We say that ring r is within the
target segment‚Äôs access bracket if the following condition holds:
r1 ‚â§r ‚â§r2.
That is, ring r must be within the inclusive bounds set by r1 and r2. We say that ring
r is below the segment‚Äôs access bracket if r < r1.
Being within or below a segment‚Äôs access bracket is a necessary but insufÔ¨Åcient
condition for being granted read, write, or execute access to that segment. The fol-
lowing mandatory access-control policy applies to any process in ring r that attempts
to access a segment s having ring bracket (r1,r2,r3):
‚Ä¢ Read access is granted if and only if s has read access turned on and r ‚â§r2.
‚Ä¢ Execute access is granted if and only if s has execute access turned on and
r ‚â§r2.
‚Ä¢ Write access is granted if and only if s has write access turned on and one of
the following conditions holds: (1) s is a procedure segment and r ‚â§r2, or (2)
s is a data segment and r ‚â§r1.
Thus, read and execute access are granted to any processes within or below a target
segment‚Äôs access bracket, provided that the segment has the corresponding access
turned on. The policy for write access to procedure segments is the same. However,
only processes at more privileged ring levels are allowed to alter a data segment‚Äôs
contents: write access to data segments is limited to processes operating at the lower
bound of (or below) the segment‚Äôs access bracket, and only when the segment has
write access turned on.
The following example illustrates how the ring-based access-control policy applies
to both procedure and data segments.
the rest of this chapter, we use the terminology ‚Äúaccess turned on‚Äù to indicate either that the appropriate
privilege bit is set (for systems without ACLs and capabilities) or that the particular principal making the
request possesses the appropriate permission for that segment.

Access Control Using Lists and Rings
255
Example 12.2
Consider three processes Q1,Q4,Q6, which are executing in rings 1, 4, and 6, re-
spectively. Furthermore, let PS and DS both be segments with ring bracket (2,5,8)
and with read, write, and execute access turned on. Assume that PS is a procedure
segment and that DS is a data segment.
The following table shows the results of attempts by the processes to access the
two segments under the ring-based access policy:
Access to PS
Access to DS
Access type
Q1
Q4
Q6
Q1
Q4
Q6
Read
Granted
Granted
Denied
Granted
Granted
Denied
Execute
Granted
Granted
Denied
Granted
Granted
Denied
Write
Granted
Granted
Denied
Granted
Denied
Denied
This table highlights two important points. First, the ring requirements prevent Q6
from accessing either segment, despite that read, write, and execute access are turned
on for both segments: Q6 is neither within nor below the segments‚Äô access bracket.
Second, Q4 is granted write access to the procedure segment PS but not to the data
segment DS: despite being within the appropriate access bracket, Q4‚Äôs ring level is
too high to write to the data segment.
‚ô¶
12.4.2
Call Brackets
In addition to an access bracket, segments may also have a call bracket. The
purpose of a call bracket is to allow procedure segments to be called under con-
trolled conditions by processes at ring levels above the segment‚Äôs access bracket.
This mechanism allows less trusted user processes to call more trusted (i.e., at a
lower ring level) system processes.
Let s be a segment with ring bracket (r1,r2,r3). We say that a ring r is within s‚Äôs
call bracket if the following condition holds:
r2 +1 ‚â§r ‚â§r3.
That is, r must be strictly greater than r2 and less than or equal to r3. Thus, in the
case where r2 = r3, the segment has no call bracket, because there is no r such that
r2 +1 ‚â§r ‚â§r2.
Conceptually, a procedure segment s has a list of n entry points‚Äîknown as call
gates‚Äîassociated with it. As illustrated in Figure 12.8, these call gates appear at the
beginning of s and are pointers to locations within the segment s. The purpose of
call gates is to limit the available entry points for processes that might be within s‚Äôs
call bracket but outside of s‚Äôs access bracket. The pointers ensure that s cannot be
executed at some incorrect, inappropriate, or arbitrary location.
Recall that each procedure segment has a permission Ô¨Åeld named gate, which
contains the number of call gates for s. A process in the calling bracket that wishes

256
Access Control, Security, and Trust: A Logical Approach
FIGURE 12.8 Call gates
to call the ith location in s requests that the operation CALL [s : i] be executed. If
i ‚â•gate, then the calling process is attempting to enter s at an unauthorized point
and execute permission is denied. Otherwise, if i < gate, then the procedure call is
within the stated gate list, and permission to execute the CALL [s : i] instruction is
granted, provided other necessary conditions related to s‚Äôs call bracket and execute
permissions are met.
Suppose a process running in ring r requests that the operation CALL [s : i] be
executed on a segment s that has ring bracket (r1,r2,r3). The following mandatory
access-control policy applies:
‚Ä¢ CALL [s : i] access is granted if and only if s has execute access turned on
and one of the following situations holds: (1) r is below or within s‚Äôs access
bracket, or (2) s is a procedure segment, r is within s‚Äôs calling bracket, and
i < gate.
Note that, for rings within or below a segment‚Äôs access bracket, CALL access corre-
sponds exactly to the standard execute access described in the previous section.
The following example illustrates how the ring-based access-control policy applies
to CALL requests.
Example 12.3
Consider four processes Q1,Q4,Q6,Q10, which are executing in rings 1, 4, 6, and 10,
respectively. Furthermore, let PS3, PS9, and DS all be segments with ring bracket
(2,5,8) and with read, write, and execute access turned on. Assume that DS is a
data segment; PS3 and PS9 are procedure segments with gate values of 3 and 9,
respectively.

Access Control Using Lists and Rings
257
The following table shows the results of attempts by the processes to execute the
CALL [s : 7] instruction on the various segments s:
Segment s
Q1
Q4
Q6
Q10
PS3
Granted
Granted
Denied
Denied
PS9
Granted
Granted
Granted
Denied
DS
Granted
Granted
Denied
Denied
This table highlights several important aspects of how ring protection works in the
context of call brackets. First, CALL access reverts to standard execute access for
processes operating within or below a segment‚Äôs access bracket: for this reason, all
of Q1‚Äôs and Q4‚Äôs attempts are granted. Second, a process operating in a ring level
higher than a segment‚Äôs call bracket is ineligible to use the CALL instruction: for this
reason, all requests by Q10 are denied. Finally, a process operating within a speciÔ¨Åc
call bracket can call only procedure segments, and only at locations i permitted by
the segment‚Äôs gate value: for this reason, Q6‚Äôs attempts to access the seventh location
of data segment DS and of procedure segment PS3 are denied.
Finally, all attempts to execute CALL [PS2 : 7] are denied, because the requested
entry point 7 exceeds the gate value 2.
‚ô¶
The following series of exercises is inspired by exercises in (Organick, 1972).
Exercise 12.4.1
For each of the following situations, give the 3-tuple (r1,r2,r3)

that deÔ¨Ånes the requested access and call brackets.
a. A single ring of access, r.
b. An access bracket (k,l), but no call bracket.
c. An access bracket (k,l) and a call bracket (l +1,m).
d. A single ring of access r and a call bracket (r +1,m).
Exercise 12.4.2
For each of the following situations, give the set of rings r from
which read, write, and execute access to the data segment s would be granted.
a. s has ring bracket (0,63,63), with read and execute access turned on.
b. s has ring bracket (0,1,63), with read and execute access turned on.
c. s has ring bracket (1,1,63), with read and execute access turned on.
d. s has ring bracket (0,0,1), with read and execute access turned on.
e. s has ring bracket (48,48,48), with read and write access turned on.
f. s has ring bracket (5,48,48), with read and write access turned on.

258
Access Control, Security, and Trust: A Logical Approach
Exercise 12.4.3
For each of the following situations, give the set of rings r from
which read, write, and execute access to the procedure segment s would be granted.
a. s has ring bracket (0,1,63), with read and execute access turned on.
b. s has ring bracket (1,1,63), with read and execute access turned on.
c. s has ring bracket (48,48,48), with read and write access turned on.
d. s has ring bracket (5,48,48), with read and write access turned on.
Exercise 12.4.4
For each of the following situations, give the set of rings r and set
of values i for which CALL [s : i] access to the segment s would be granted.
a. s is a data segment with ring bracket (0,1,63) and read and execute access
turned on.
b. s is a procedure segment with ring bracket (0,1,63), gate value 5, and read
and execute access turned on.
c. s is a procedure segment with ring bracket (0,0,1), gate value 5, and read and
execute access turned on.
d. s is a procedure segment with ring bracket (0,0,1), gate value 5, and read and
write access turned on.
12.5
Summary
In this chapter, we introduced both access-control list (ACL) systems and ring-
based access control, which extend the memory-protection mechanisms we saw in
previous chapters. These schemes can be used together (as in the Multics operating
system) or separately. In ACL systems, access-controller segments specify the lo-
cation of speciÔ¨Åc memory segments, as well as the access capabilities for individual
principals on that segment. ACL systems more easily support permission revocation
than capability systems, but at the cost of an additional level of indirection. Pro-
tection rings provide a layered protection approach that offers both Ô¨Çexibility and
speciÔ¨Åcity on how data and processes are accessed. Processes in the innermost rings
are the most trusted processes and have the greatest access privileges. Processes in
the outer rings are typically less trusted user processes with fewer privileges.
Both segmentation and protection rings were Ô¨Årst developed for use in the 1960s
for mainframe computing systems such as Multics. When mainframes and mini-
computers gave way to personal computers, protection schemes such as rings and
segmentation were dropped for economic reasons. At the time, personal comput-
ers were physically isolated machines with single users, which obviated the need
to isolate one user‚Äôs processes from those of other users. The additional cost of

Access Control Using Lists and Rings
259
FIGURE 12.9 Learning outcomes for Chapter 12
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Comprehension
‚Ä¢ When given an address descriptor, access controller with ACL entries,
and ring brackets for a segment, determine when access to a target seg-
ment is granted or denied.
Application
‚Ä¢ When given the protection goals of a system, apply the concepts of
access-control lists and ring brackets to devise protection policies and
mechanisms.
Analysis
‚Ä¢ When given an access request, determine if the permissions at hand are
sufÔ¨Åcient to grant access.
Synthesis
‚Ä¢ When given descriptions of policies, architectures, instructions, and
mechanisms, create formal descriptions of each.
Evaluation
‚Ä¢ When given formal descriptions of policies, architectures, instructions,
and mechanisms, express access-control decisions as formally derived
theorems in the access-control logic.
having hardware dedicated to security and isolation seemed impractical. No one
anticipated the global internetworking and unprecedented access and vulnerability
the Internet would bring. Decades later, security and access control are now essen-
tial, and support for segmentation and protection rings is now routinely included in
microprocessors.
Figure 12.9 summarizes the learning outcomes for this chapter.
12.6
Further Reading
The ideas behind the use of access-control lists and rings to control access to
segments were developed in the 1960s and 1970s on mainframe computer systems

260
Access Control, Security, and Trust: A Logical Approach
such as Multics. Numerous papers chart the development of generalized addressing
(Daley and Dennis, 1968), virtualized memory (Bensoussan et al., 1972), access-
control lists (Saltzer and Schroeder, 1975), and rings (Schroeder and Saltzer, 1972;
Organick, 1972).

Part IV
Access Policies


Chapter 13
ConÔ¨Ådentiality and Integrity Policies
In Chapter 5, we introduced military security policies, which are primarily concerned
with controlling the disclosure of classiÔ¨Åed information. We also introduced com-
mercial security policies, which focus on preserving the integrity (e.g., quality or
trustworthiness) of information. In both cases, the policies depend on assigning clas-
siÔ¨Åcation levels‚Äîeither security levels or integrity levels, as appropriate‚Äîto both
subjects and objects; access-control decisions are then based in part on the relation-
ships among those levels.
In this chapter, we expand on those ideas to support more Ô¨Ånely tuned conÔ¨Åden-
tiality and integrity policies. The Ô¨Årst step is to introduce a somewhat richer notion
of conÔ¨Ådentiality and integrity levels, which is based on categories of information in
addition to the classiÔ¨Åcation of information. As we will see, the underlying Kripke
semantics introduced in Chapter 5 also work for these richer levels. We then recap
both the Bell‚ÄìLa Padula model for conÔ¨Ådentiality and Biba‚Äôs strict-integrity model,
highlighting how they apply to the more reÔ¨Åned notions of conÔ¨Ådentiality and in-
tegrity levels. We also address how Biba‚Äôs notion of strict integrity applies to invo-
cation access requests, a topic previously deferred. Finally, we introduce Lipner‚Äôs
integrity model, which applies a combination of the Bell‚ÄìLa Padula and Biba mod-
els to develop an integrity policy for production systems. His approach serves as a
useful case study for designers and veriÔ¨Åers who must develop their own policies to
meet speciÔ¨Åc design requirements.
13.1
ClassiÔ¨Åcations and Categories
So far, we have considered only simple conÔ¨Ådentiality and integrity levels. It is
also possible to construct compound levels for both conÔ¨Ådentiality and integrity by
combining a classiÔ¨Åcation level (as we saw in Chapter 5) with a set of categories
to which it applies. SpeciÔ¨Åcally, each compound level is a pair (L,C), where L is a
classiÔ¨Åcation level and C ‚äÜCat is a (possibly empty) set of categories drawn from
the set Cat.
As we saw in Chapter 5, the classiÔ¨Åcation levels used for conÔ¨Ådentiality or in-
tegrity are related to one another by a partial order ‚â§. Recall that partial orders
are binary relations that are reÔ¨Çexive, transitive, and anti-symmetric. Therefore, the
263

264
Access Control, Security, and Trust: A Logical Approach
classiÔ¨Åcation-level relation ‚â§must satisfy the following properties:
‚Ä¢ ReÔ¨Çexivity: For all classiÔ¨Åcation levels L, L ‚â§L.
‚Ä¢ Transitivity: For all classiÔ¨Åcation levels L1,L2, and L3, if L1 ‚â§L2 and L2 ‚â§L3,
then L1 ‚â§L3.
‚Ä¢ Anti-symmetry: For all classiÔ¨Åcation levels L1 and L2, if L1 ‚â§L2 and L2 ‚â§L1,
then L1 = L2 (i.e., no cycles exist in the relation ‚â§).
The sets of categories are also related to one another by a partial order, namely the
subset relation ‚äÜ. It is therefore possible to deÔ¨Åne a partial order dom over compound
levels (L,C) as follows:
(L1,C1) dom (L2,C2) if and only if (L2 ‚â§L1) and C2 ‚äÜC1.
That is, the compound level (L1,C1) dominates (L2,C2) if and only L1 and C1 are
each at least as high as L2 and C2, respectively. The proof that dom is indeed a partial
order is left as an exercise.
The following two examples provide concrete illustrations of compound levels and
how the dom relation works.
Example 13.1
Consider the set of classiÔ¨Åcation levels L = {HI, LO}, with the partial order ‚â§deÔ¨Åned
over L as follows:
LO ‚â§HI.
Furthermore, suppose the set of categories is the set Cat = {BIN1, BIN2}.
This combination of sets L and Cat yields eight possible compound levels:
(HI,{BIN1, BIN2})
(HI,{BIN1})
(HI,{BIN2})
(HI,{})
(LO,{BIN1, BIN2})
(LO,{BIN1})
(LO,{BIN2})
(LO,{})
The dom ‚äÜL √óCat relation is presented graphically by the Hasse diagram in Fig-
ure 13.1. Each compound level appears as a node in the Hasse diagram, but not every
element of dom shows up as an explicit edge in the diagram. Each edge (i.e., down-
ward line) from a compound level ‚Ñì1 = (L1,C1) to a level ‚Ñì2 = (L2,C2) corresponds
to an element (‚Ñì1,‚Ñì2) ‚ààdom that cannot be deduced through reÔ¨Çexivity or transitivity.
Thus, for example, no edge is necessary to indicate that (HI,{BIN1}) dom (LO,{}):
that relationship can be deduced by recognizing that (HI,{BIN1}) dom (HI,{}) and
(HI,{}) dom (LO,{}).
‚ô¶

ConÔ¨Ådentiality and Integrity Policies
265
FIGURE 13.1 Partial ordering on conÔ¨Ådentiality levels
Example 13.2
Consider the compound levels and dom relation given in Example 13.1, and suppose
that Arthur has conÔ¨Ådentiality level (HI,{BIN1}). His conÔ¨Ådentiality level dominates
documents with the following levels:
(HI,{BIN1}), (LO,{BIN1}), (HI,{}), (LO,{}).
‚ô¶
As a Ô¨Ånal note, we point out that the compound levels introduced here still Ô¨Åt
within the syntax and semantics introduced for conÔ¨Ådentiality and integrity policies
in Chapter 5. In particular, compound levels can be viewed as additional elements of
the syntactic sets SecLabel and IntLabel, and the dom relation can be represented
by the ‚â§s and ‚â§i operators of the logic. The following example reinforces this point.
Example 13.3
Suppose that Jane‚Äôs conÔ¨Ådentiality level is (LO,{BIN1, BIN2}) and that object O has
conÔ¨Ådentiality level (LO,{BIN2}). We can express these facts in the logic as follows:
slev(Jane) = (LO,{BIN1, BIN2}) ‚àßslev(O) = (LO,{BIN2}).
We can also express the following consequence, namely that Jane‚Äôs conÔ¨Ådentiality
level dominates O‚Äôs level:
slev(O) ‚â§s slev(Jane).
‚ô¶
Exercise 13.1.1
Prove that the subset relation ‚äÜis a partial order.
Exercise 13.1.2
Prove that the dom relation is a partial order.

266
Access Control, Security, and Trust: A Logical Approach
13.2
Bell‚ÄìLa Padula Model, Revisited
We saw in Chapter 5 that the Bell‚ÄìLa Padula model for conÔ¨Ådentiality mandates
that the following two properties be satisÔ¨Åed:
1. Simple security condition: A principal P can read object O if and only if
P‚Äôs conÔ¨Ådentiality level is at least as high as O‚Äôs and P has discretionary read
access to O.
2. *-property: A principal P can write to object O if and only if O‚Äôs conÔ¨Åden-
tiality level is at least as high as P‚Äôs and P has discretionary write access to
O.
As pointed out previously, the ‚Äúif and only if‚Äù requirements in these properties
mean that each property actually involves two implications. For example, the simple
security condition contains the following two implications:
1. If P‚Äôs conÔ¨Ådentiality level is at least as high as O‚Äôs and P has discretionary
read access to O, then P can read O.
2. If P can read O, then P‚Äôs conÔ¨Ådentiality level is at least as high as O‚Äôs and P
has discretionary read access to O.
The Ô¨Årst implication represents an access-control condition: it states the conditions
under which a reference monitor should grant access to O (i.e., the access policy).
Such conditions will often occur in the context of a formal justiÔ¨Åcation for granting
access to a speciÔ¨Åc object. In contrast, the second implication represents a safety
condition for the entire system: this sort of property cannot be assumed, but instead
must be veriÔ¨Åed by checking every potential consequence of the rules and mecha-
nisms that govern access. Proving such a property‚Äîwhich relies on the complete-
ness, isolation, and veriÔ¨Åability of every reference monitor in the system‚Äîis beyond
the scope of the access-control logic. In the discussion that follows, we focus on the
access-control conditions.
We can formulate the access-control policies that correspond to the simple security
condition and the *-property as follows:
1. Simple-security access-control policy:
( slev(O) ‚â§s slev(P)) ‚äÉ(P controls ‚ü®read,O‚ü©)
2. *-property access-control policy:
( slev(P) ‚â§s slev(O)) ‚äÉ(P controls ‚ü®write,O‚ü©)
The following example illustrates the speciÔ¨Åcation of access policies associated
with the Bell‚ÄìLa Padula model.

ConÔ¨Ådentiality and Integrity Policies
267
Subject or Object
ConÔ¨Ådentiality Level
Carol
(HI,{BIN1, BIN2})
Kate
(LO,{BIN2})
O1
(HI,{BIN1, BIN2})
O2
(LO,{BIN2})
O3
(LO,{BIN1})
O4
(LO,{})
Table 13.1: ConÔ¨Ådentiality levels
Object
Subject
O1
O2
O3
O4
Carol
read
-
read
-
Kate
write
-
-
read
Table 13.2: Discretionary access-control matrix
Example 13.4
Consider Tables 13.1 and 13.2: the former lists the conÔ¨Ådentiality levels of two
subjects (Carol and Kate) and four objects (O1,O2,O3,O4), while the latter speciÔ¨Åes
the discretionary access of subjects to objects.
Together, these two tables satisfy the Bell‚ÄìLa Padula model: read permission for
object Oi is granted only to subjects whose conÔ¨Ådentiality level is at least as high
as Oi‚Äôs, and write permission for object Oi is granted only to subjects whose conÔ¨Å-
dentiality level is no higher than Oi‚Äôs. Note that, although both Carol and Kate have
conÔ¨Ådentiality levels high enough to read object O2, neither has been granted that
discretionary access.
The access-control policy can be expressed in the access-control logic by the fol-
lowing statements:
( slev(O1) ‚â§s slev(Carol)) ‚äÉ(Carol controls ‚ü®read,O1‚ü©),
( slev(O1) ‚â§s slev(Carol)) ‚äÉ(Carol controls ‚ü®read,O3‚ü©),
( slev(Kate) ‚â§s slev(O1)) ‚äÉ(Kate controls ‚ü®write,O1‚ü©),
( slev(O4) ‚â§s slev(Kate)) ‚äÉ(Kate controls ‚ü®read,O4‚ü©).
‚ô¶
We conclude this section with an example that illustrates why the safety aspects of
Bell‚ÄìLa Padula must be veriÔ¨Åed through system audit, rather than within the system
itself.
Example 13.5
Dexter is a corrupt system administrator who has supervisory privileges in an orga-
nization‚Äôs information system. Because Dexter is trusted (but not trustworthy), he is
in a position to bypass the security of the system.

268
Access Control, Security, and Trust: A Logical Approach
Prior to being Ô¨Åred, Dexter installs a back door so that he can rewrite personnel
records. He alters the reference monitor that guards the personnel records so that it
accepts a secret identiÔ¨Åer:
secretID controls ‚ü®write,personnel records‚ü©.
By doing so, Dexter has bypassed all the mandatory access-control checks. This
back door can be detected only by auditing Dexter‚Äôs actions and by inspecting the
subsystems accessed by Dexter: no reference monitor can possibly detect that Dex-
ter‚Äôs change violates the access policy.
‚ô¶
Exercise 13.2.1
Consider the conÔ¨Ådentiality-level assignments and discretionary
access speciÔ¨Åed in Example 13.4. Provide formal proofs in the access-control logic
that, when requested, Carol can read objects O1 and O3 and Kate can write O1 and
read O4.
Exercise 13.2.2
Reformulate the extended example in Section 5.4.3 for the FX-1
and FX-2 Ô¨Åghter projects. In particular, reformulate all the level assignments to
subjects and objects using both clearance levels and the categories FX-1 and FX-2.
Make sure that you assign the clearance levels to preserve the privileges as described
in Section 5.4.3.
In addition, suppose General Jane in the Department of Defense heads the entire
Next Generation Fighter project. Thus, both the FX-1 and FX-2 projects are in her
jurisdiction. What should her conÔ¨Ådentiality level be, and how are the FX-1 and
FX-2 access matrices altered so that she has read access to all documents?
Exercise 13.2.3
Correctional facilities (prisons) have several classiÔ¨Åcations of di-
rectives.1 These classiÔ¨Åcations are as follows:
‚Ä¢ Level A: accessible to anyone, including inmates. Level A documents are
public information.
‚Ä¢ Level B: not accessible by inmates but accessible to ofÔ¨Åcers, sergeants, and
management. Level B documents generally deal with normal facility opera-
tions.
‚Ä¢ Level C: conÔ¨Ådential (but not emergency-related) information. Level C doc-
uments generally deal with conÔ¨Ådential personnel information. These docu-
ments are available to sergeants and above, but not to ofÔ¨Åcers and inmates.
‚Ä¢ Level D: emergency-response information.
Level D documents are highly
conÔ¨Ådential that, if possessed by inmates, would threaten the security of the
prison. These documents are available only to management.
1This exercise was inspired by a conversation with Gary Tyler, Lieutenant (retired), New York State
Department of Corrections.

ConÔ¨Ådentiality and Integrity Policies
269
a. Fill in the following table with classiÔ¨Åcation levels for each subject and object:
Subject or Object
ClassiÔ¨Åcation Level
Inmate
OfÔ¨Åcer
Sergeant
Chief of Security
Prison Visiting Hours
Vacation Policy for OfÔ¨Åcers
ConÔ¨Ådential Performance Reviews of OfÔ¨Åcers
Emergency Response Plans for Riots
b. Fill in the following access-control matrix so that it meets the following con-
ditions:
‚Ä¢ It satisÔ¨Åes the Bell‚ÄìLa Padula conÔ¨Ådentiality model.
‚Ä¢ It follows the intent of the classiÔ¨Åcation levels described in the scenario.
‚Ä¢ Only the Chief of Security can write or read the Emergency Response
Plan for Riots.
‚Ä¢ Sergeants write the conÔ¨Ådential performance review for ofÔ¨Åcers, which
is then readable by the Chief of Security.
Subject
Visiting
Hours (A)
Vacation
Policy (B)
Performance
Reviews
(C)
Emergency
Response
Plans (D)
Inmate (A)
OfÔ¨Åcer (B)
Sergeant (C)
Chief of Security (D)
c. Express in the access-control logic the policy that the Chief of Security (CS)
has the capability to read and write Emergency Response Plans (ERPs).
d. Propose a derived inference rule that states that the Chief of Security (CS) can
read emergency response plans (ERPs). Give a formal proof of the derived
rule.
13.3
ConÔ¨Ådentiality Levels: Some Practical Considera-
tions
Our discussion of conÔ¨Ådentiality levels so far has ignored one pragmatic point:
people, processes, and information are rarely fully isolated in wholly separate en-
claves. Instead, people and information move across boundaries, be they physical

270
Access Control, Security, and Trust: A Logical Approach
or virtual. For example, within secure facilities, people with high security clear-
ances communicate with people who have lower or no security clearances. Those
who work in secure facilities head home and spend time with family and friends in
unsecured environments. Similarly, sensitive government documents are routinely
released to the public under freedom-of-information laws.
Several approaches are used to prevent sensitive information from being leaked
as people and information move across boundaries. These approaches include max-
imum and current conÔ¨Ådentiality levels, sanitization and conÔ¨Ånement. We describe
these approaches in turn, illustrating each with one or more examples.
The Bell‚ÄìLa Padula model imposes constraints on how subjects with high con-
Ô¨Ådentiality levels may communicate with subjects at lower levels. For example,
consider the case of Kamal, a Senior Scientist at a military research lab who has a
conÔ¨Ådentiality level of (TS,{NUC, US, EUR, ASIA}). Kamal needs to send a mes-
sage (i.e., a Ô¨Åle) to one of his staff engineers (Sarah), whose conÔ¨Ådentiality level is
(TS,{NUC, US}).
As presented so far, Bell‚ÄìLa Padula prohibits Sarah from reading any Ô¨Åle that Ka-
mal writes, because his conÔ¨Ådentiality level is higher than Sarah‚Äôs. However, in fact
the Bell‚ÄìLa Padula model makes the following allowance: subjects have both a max-
imum conÔ¨Ådentiality level and a current conÔ¨Ådentiality level. For understandable
reasons, the maximum level must always dominate the current level. When two sub-
jects need to communicate with one another, the subject with the higher level reduces
his or her current level appropriately, so that they can communicate as needed.
In practice, the adoption of different conÔ¨Ådentiality levels often requires a subject
to switch systems to effectively lower his or her level. For example, a person with
top-secret clearance will log on to a secret system to communicate with people at the
secret level. The following example illustrates how this approach lets Kamal send a
message that Sarah can read.
Example 13.6
Kamal has a conÔ¨Ådentiality level of (TS,{NUC, US, EUR, ASIA}), and his staff en-
gineer Sarah has a conÔ¨Ådentiality level of (TS,{NUC, US}). To communicate with
Sarah, Kamal logs onto a TS system that has only NUC,US as allowable categories,
and sends his message. Having reduced his current conÔ¨Ådentiality level to Sarah‚Äôs,
any message he writes will be readable by Sarah.
‚ô¶
In many cases‚Äîsuch as the public release of documents through freedom-of-
information requests‚Äîit is an object‚Äôs conÔ¨Ådentiality level that is problematic, rather
than a subject‚Äôs level. Sanitization is the process of removing classiÔ¨Åed or restricted
information from a document or Ô¨Åle, thereby allowing the remaining information to
be released at a lower classiÔ¨Åcation level. In essence, the document in question is
edited or rewritten without the sensitive information. The following two examples
illustrate the types of contexts in which sanitization is used.

ConÔ¨Ådentiality and Integrity Policies
271
Example 13.7
Court proceedings are typically considered public records. However, in cases that
involve crimes against children, there are often requirements that prevent disclosure
of the identities of child victims. In such situations, the court records released to the
public have the names of children removed or hidden.
‚ô¶
Example 13.8
Before government reports are released to the public, classiÔ¨Åed information (e.g., the
names of spies) is removed, hidden, or blocked out. The resulting document is then
reclassiÔ¨Åed to a lower level before being released.
‚ô¶
A third pragmatic concern is the potential leaking of protected information from
a protected environment to a less protected environment. ConÔ¨Ånement is the means
by which such leaks are prevented, be they in the virtual or physical environments.
For example, a secure server may be placed on a secure network, to prevent inse-
cure processes from accessing it and then transmitting information insecurely. The
following example illustrates an approach to conÔ¨Ånement in the physical world.
Example 13.9
A Sensitive Compartmented Information Facility (SCIF, pronounced ‚Äúskiff‚Äù) is a se-
cure location with appropriate access controls to ensure that classiÔ¨Åed information
can be accessed securely. Recently, Tom listened to a secret brieÔ¨Ång in his labo-
ratory‚Äôs SCIF, which is contained within a larger ofÔ¨Åce building that has a lower
clearance level. Being a new and inexperienced employee, Tom took his engineering
notebook with him into the SCIF without considering the consequences.
Before exiting the SCIF, Tom had to surrender his engineering notebook to the
SCIF security ofÔ¨Åce, who will inspect the notebook to verify that it does not contain
any classiÔ¨Åed information. This inspection may take up to a week or more.
‚ô¶
Exercise 13.3.1
Consider the situation described in Example 13.6. In whom and in
what must we trust to prevent the leakage of information downwards to Sarah from
Kamal? What are some things that can be done to mitigate some of the risks and
vulnerabilities?
Exercise 13.3.2
Consider the situation described in Example 13.7. Suppose that
Mary is in charge of court records, including the censoring of sensitive court docu-
ments for public release. Furthermore, suppose that the court employs two computer
systems, one for sensitive unsanitized (U) documents and another for publicly re-
leasable sanitized (S) documents.
Devise a concept of operations that describes how Mary takes a Ô¨Åle CaseSensitive
(where slev(CaseSensitive) = U), censors it, and produces a publicly releasable Ô¨Åle
CaseReleasable (where slev(CaseReleasable) = S).
Your concept of operations should satisfy the Bell‚ÄìLa Padula model and include
the following:

272
Access Control, Security, and Trust: A Logical Approach
‚Ä¢ The partial order that relates U and S
‚Ä¢ The discretionary access-control matrix that shows Mary‚Äôs privileges
‚Ä¢ The formulation in the access-control logic of the access-control policy for
Mary‚Äôs reading and writing of CaseSensitive and CaseReleasable
‚Ä¢ The formulation in the access-control logic of the assignment of conÔ¨Ådentiality
levels to Mary and to the two case Ô¨Åles
‚Ä¢ Theorems in the access-control logic that justify letting Mary read and write
the relevant case Ô¨Åles
Exercise 13.3.3
Consider the situation in Example 13.9, and suppose that Tom‚Äôs
notebook can be released only if it contains only unclassiÔ¨Åed information. John, as
the SCIF security ofÔ¨Åcer, determines whether or not Tom‚Äôs notebook can be released.
Describe in the access-control logic John‚Äôs jurisdiction over whether or not the
notebook is released or held. Using your formulation, formally justify in the access-
control logic the release of Tom‚Äôs notebook if John says so.
13.4
Biba‚Äôs Strict Integrity, Revisited
Biba‚Äôs strict-integrity model is the dual of the Bell‚ÄìLa Padula model: it uses the
same structure as Bell‚ÄìLa Padula (i.e., partially order levels), but the direction of
information Ô¨Çow is reversed. Whereas the catch phrase for Bell‚ÄìLa Padula is ‚Äúno
read up and no write down,‚Äù the catch phrase for Biba is ‚Äúno read down and no write
up.‚Äù
We introduced the underlying concepts and deÔ¨Ånitions for Biba‚Äôs strict-integrity
model in Chapter 5, where we focused on direct-access operations (i.e., observations
and modiÔ¨Åcations). In this section, we brieÔ¨Çy review the requirements for Biba‚Äôs
strict integrity, but we focus on Ô¨Ålling in the details as they apply to indirect access
(i.e., the invocation of one subject by another).
As we saw previously, strict integrity requires that the following three properties
be met:
1. Simple integrity condition: A subject S can observe O if and only if
ilev(S) ‚â§ilev(O) and S has discretionary observe access to O.
2. Integrity *-property: A subject S can modify O if and only if
ilev(O) ‚â§ilev(S) and S has discretionary modify access to O.
3. Invocation condition: A subject S1 and invoke subject S2 if and only if
ilev(S2) ‚â§ilev(S1) and S1 has discretionary invocation rights to S2.

ConÔ¨Ådentiality and Integrity Policies
273
In effect, strict integrity requires that the integrity level of each object and subject
in a transfer path must be at least as high as that of the subject or object that imme-
diately follows it. Thus, for example, in a transfer path where (for each i such that
1 ‚â§i ‚â§n) subject Si can read from object Oi and write to object Oi+1, the following
condition must hold:
ilev(On+1) ‚â§i ilev(Sn) ‚â§i ilev(On) ‚â§i ¬∑¬∑¬∑ ‚â§i ilev(O2) ‚â§ilev(S1) ‚â§i ilev(O1).
In the case of indirect operations, strict integrity requires that the integrity level of
the invoking subject S1 must be at least as high as that of the invoked subject S2.
Otherwise, less trustworthy subjects could direct the actions of more trustworthy
subjects: for example, users could direct system administrators to disable system
safeguards such as malware detectors and perimeter defenses.
As we saw with the Bell‚ÄìLa Padula properties, the three strict-integrity properties
implicitly involve two implications: an access-control condition and a system-wide
safety (or audit) property. The access-control conditions, which represent the access
policies used by reference monitors, can be expressed in the logic as follows:
1. Simple-integrity access-control condition:
( ilev(S) ‚â§ilev(O)) ‚äÉ(S controls ‚ü®observe,O‚ü©)
2. Integrity *-property access-control condition:
( ilev(O) ‚â§ilev(S)) ‚äÉ(S controls ‚ü®modify,O‚ü©)
3. Invocation access-control conditions:
( ilev(S2) ‚â§i ilev(S1)) ‚äÉ(S1 controls ‚ü®i,S2,o,O‚ü©),
( ilev(S2) ‚â§i ilev(S1)) ‚äÉ(S1 controls ‚ü®i,S2,m,O‚ü©),
where ‚ü®i,S,o,O‚ü©and ‚ü®i,S,m,O‚ü©respectively denote the propositions ‚Äúinvoke
subject S to observe O‚Äù and ‚Äúinvoke subject S to modify O.‚Äù
The following example demonstrates how such conditions can be used to express
strict-integrity policies.
Example 13.10
Consider Tables 13.3 and 13.4, which respectively (1) list the integrity levels for
two subjects (Carol and Kate) and four objects (O1,O2,O3,O4) and (2) specify the
subjects‚Äô discretionary observe, modify, and invocation rights. Together, these two
tables satisfy Biba‚Äôs strict-integrity properties.
The access-policy regarding Carol‚Äôs discretionary modify access to O3 can be ex-
pressed as follows:
( ilev(O3) ‚â§i ilev(Carol)) ‚äÉ(Carol controls ‚ü®modify,O3‚ü©).
‚ô¶

274
Access Control, Security, and Trust: A Logical Approach
Subject or Object
Integrity Level
Carol
(LEV1,{CAT1, CAT2})
Kate
(LEV2,{CAT2})
O1
(LEV1,{CAT1, CAT2})
O2
(LEV2,{CAT2})
O3
(LEV2,{CAT1})
O4
(LEV2,{})
Table 13.3: Integrity levels
Object
Subject
Subject
O1
O2
O3
O4
Carol
Kate
Carol
observe
-
modify
-
-
invoke observe,
invoke modify
Kate
observe
-
-
modify
-
-
Table 13.4: Discretionary access-control matrix
Table 13.4 includes indirect access rights for Carol: she has invocation rights on
Kate for both observe and modify actions. Consequently, Kate must have a policy on
how she will respond to Carol‚Äôs requests. There are three possibilities when Carol
invokes Kate:
1. Kate denies Carol‚Äôs request.
Because the request is denied, there is nothing further to analyze: strict in-
tegrity is preserved.
2. Kate honors Carol‚Äôs request, and Kate has discretionary access to the object
she must access on Carol‚Äôs behalf.
We explore this case in Example 13.11.
3. Kate honors Carol‚Äôs request, and Kate does not have discretionary access to
the object she must access on Carol‚Äôs behalf.
This case is left for Exercise 13.4.3.
Example 13.11
Consider the integrity levels given in Table 13.3 and the discretionary access-control
matrix of Table 13.4. Furthermore, suppose that Carol requests that Kate modify
object O4 on her behalf.
Figure 13.2 illustrates the three steps that must occur for Carol‚Äôs indirect request
for the modiÔ¨Åcation of O4 to be granted:
1. Kate‚Äôs reference monitor must conclude that Carol‚Äôs invocation request should
be honored by Kate.

ConÔ¨Ådentiality and Integrity Policies
275
FIGURE 13.2 Carol‚Äôs request and Kate‚Äôs response
The reference monitor must ensure that Carol‚Äôs integrity level is at least as high
as Kate‚Äôs integrity level and that Carol has discretionary invocation access on
Kate.
Letting ‚ü®i,Kate,modify,O4‚ü©denote the proposition ‚Äúit is good to grant the
invocation request for Kate to modify object O4,‚Äù the reference monitor‚Äôs be-
havior can be represented by the following derived inference rule:
ilev(Carol) =i (LEV1,{CAT1, CAT2})
ilev(Kate) =i (LEV2,{CAT2})
(LEV2,{CAT2}) ‚â§i (LEV1,{CAT1, CAT2})
ilev(Kate) ‚â§i ilev(Carol) ‚äÉ(Carol controls ‚ü®i,Kate,modify,O4‚ü©)
Carol says ‚ü®i,Kate,modify,O4‚ü©
‚ü®i,Kate,modify,O4‚ü©.
2. Kate‚Äôs behavior is governed by her policy regarding invocations that are passed
on to her by her reference monitor.
If her policy is to make any direct-access request that the reference monitor
passes on to her, then her behavior upon receiving Carol‚Äôs request can be ex-
pressed by the following derived inference rule:
‚ü®i,Kate,modify,O4‚ü©‚äÉ‚ü®modify,O4‚ü©
‚ü®i,Kate,modify,O4‚ü©
Kate says ‚ü®modify,O4‚ü©.
3. Ultimately, Kate‚Äôs request on Carol‚Äôs behalf must be approved by the reference
monitor for object O4.
The reference monitor must ensure that Kate‚Äôs integrity level is at least as
high as O4‚Äôs integrity level. The reference monitor‚Äôs eventual decision can be

276
Access Control, Security, and Trust: A Logical Approach
expressed by the following derived inference rule:
ilev(O4) =i (LEV2,{})
ilev(Kate) =i (LEV2,{CAT2})
(LEV2,{}) ‚â§i (LEV2,{CAT2})
ilev(O4) ‚â§i ilev(Kate) ‚äÉ(Kate controls ‚ü®modify,O4‚ü©)
Kate says ‚ü®modify,O4‚ü©
‚ü®modify,O4‚ü©.
‚ô¶
Exercise 13.4.1
Consider the discretionary-access control matrix in Table 13.4.
For each object O1 through O4, write the strict-integrity access-control policy for
the reference monitor that guards the object. Given the integrity levels in Table 13.3,
formally prove that (1) Carol can observe and modify objects O1 and O3, respec-
tively, and (2) that Kate can observe and modify objects O1 and O4, respectively.
Exercise 13.4.2
Formally derive the inference rules from Example 13.11 that de-
scribe the behavior of Kate‚Äôs reference monitor, Kate, and O4‚Äôs reference monitor.
Exercise 13.4.3
Suppose everything is the same as described in Example 13.11
with the following exception: Carol invokes Kate to modify O3 instead of O4. Show
how the direct access policies work to prevent Kate from accessing O3. Be as precise,
detailed, and formal as possible.
13.5
Lipner‚Äôs Integrity Model
In the previous sections, we reviewed the Bell‚ÄìLa Padula and Biba models for
conÔ¨Ådentiality and integrity. In this section, we introduce Lipner‚Äôs integrity model
(Lipner, 1982), which builds upon both models. Lipner developed his model for so-
called production systems, which depend on software to deliver commercial services
(e.g., banking, data processing, or inventory control). In such systems, data integrity
is paramount: imagine the chaos that would ensue if a bank‚Äôs account data became
corrupted or if an inventory-control program had incorrect information on what was
on store shelves and in warehouses.
Lipner‚Äôs model serves as an interesting case study for designers and veriÔ¨Åers who
must develop their own policies to meet design requirements. To be explicit, Lip-
ner does not provide an abstract model such as Bell‚ÄìLa Padula or Biba. Rather, he
speciÔ¨Åes a collection of requirements for commercial integrity and then employs a
combination of the Bell‚ÄìLa Padula and Biba models to identify speciÔ¨Åc conÔ¨Ådential-
ity and integrity levels suitable for meeting those requirements.

ConÔ¨Ådentiality and Integrity Policies
277
13.5.1
Commercial Integrity Requirements
To understand the type of commercial scenario that Lipner‚Äôs work addresses, con-
sider a Ô¨Ånancial-services company that offers online banking and credit services.
Bank customers use programs supplied by the bank to access and manipulate their
accounts online: they can view account information, pay bills, transfer funds be-
tween accounts, and so on. Thus, customers are users of bank services, and the data
and programs they use comprise the production domain. Unseen by customers are
the application developers and system programmers, who develop (respectively) the
applications software and the underlying IT infrastructure used by customers. These
people work in the development domain.
Experience shows that it is essential to isolate the production domain from the
development domain. Otherwise, programs under development could destroy or
corrupt customer account information, thereby destroying the bank‚Äôs reputation and
business. Before programs can be moved from the development domain into the
production domain, they must be thoroughly reviewed and tested. The decisions
and processes used to move a program out of development and into production are
managed by system controllers, managers, and auditors.
Lipner proposed the following commercial-integrity requirements for such pro-
duction systems:
1. Users can use production programs and data, but they cannot write their own
programs to operate on the production databases.
2. Application developers and system programmers perform their development
and testing in a special test environment; they have no access to the production
programs or databases. Should they require such access, a special process will
provide them with copies of the information they need.
3. Only system controllers may change the status of a program from development
to production.
4. The act of changing a program‚Äôs status from development to production is
audited.
5. Managers and auditors have access to the system state and audit logs.
In the next two subsections, we examine Lipner‚Äôs two approaches to meeting these
Ô¨Åve requirements: the Ô¨Årst approach uses only the Bell‚ÄìLa Padula model, while the
second uses a combination of both Bell‚ÄìLa Padula and Biba‚Äôs strict integrity.
13.5.2
Commercial Integrity via Bell‚ÄìLa Padula
Lipner‚Äôs requirements for commercial integrity explicitly identify Ô¨Åve classes of
subjects: (1) production-environment users (e.g., customers); (2) application pro-
grammers; (3) system programmers; (4) system controllers; and (5) system man-
agers or auditors. The requirements also implicitly identify seven classes of objects:

278
Access Control, Security, and Trust: A Logical Approach
(1) production data (e.g., customer account information); (2) production code (i.e.,
application software available to users); (3) system programs in the production envi-
ronment (e.g., database query engines or key-distribution software invoked by pro-
duction code); (4) application programs in development; (5) system programs and
data in development; (6) software tools used by development personnel (i.e., pro-
grammer support software, such as compilers); and (7) audit logs.
Table 13.5 gives the desired discretionary access-control matrix for this situation.
It is straightforward to verify that the matrix meets the imposed commercial-integrity
requirements:
1. Users may use production code and system programs, and they may modify
production data. However, they are unable to create or modify production
code, system programs, or anything in the development environment.
2. Application programmers and system programmers have no access to produc-
tion data and code; however, they may use (but not modify) system programs
in the application environment, as well as software tools. Application pro-
grammers can read and modify application programs within the development
environment, and system programmers can read and modify system programs
within the development environment.
3. System controllers‚Äîwho need to be able to install programs within both the
production and development environments‚Äîhave read and write permissions
for all programs and data in both environments.
4. All subjects‚Äô actions are written to the audit logs, including system controllers‚Äô
installations of programs.
5. System managers and auditors‚Äîwho must monitor the entire system‚Äôs state‚Äî
have read access to all aspects of the system, including the audit logs.
Lipner‚Äôs goal was to identify conÔ¨Ådentiality levels for the subjects and objects
in such a way that (1) the desired access-control matrix is consistent with Bell‚ÄìLa
Padula and (2) subjects have as many permissions as possible. Towards this end, he
used compound levels built from the set of classiÔ¨Åcation levels L = {AM, SL} and
the set of categories Cat = {PD, PC, D, SD, T}.
Intuitively, the classiÔ¨Åcation level AM (short for ‚Äúaudit manager‚Äù) is for system
managers and auditors; the classiÔ¨Åcation level SL (‚Äúsystem low‚Äù) is for all other
subjects in the system. Because system managers and auditors have greater privileges
than the other subjects, the partial order ‚â§on L is deÔ¨Åned such that SL ‚â§AM.
The categories reÔ¨Çect the types of data and programs that can be used or modiÔ¨Åed
by users, application programmers, and system programmers: production data (PD),
production code (PC), development code and test data (D), system programs in de-
velopment (SD), and software tools (T). Categories are assigned to subjects based on
the types of data and code that they can modify, as well as the programs that they
can use. For example, ordinary users should be able to modify production data and
to use production code; therefore, they are assigned the set of categories {PD, PC}.

ConÔ¨Ådentiality and Integrity Policies
279
Object
Subject
Prod
Data
Prod
Code
System
Prog
Dev
App
Prog
Dev
Sys
Prog
SW
Tools
Audit
Logs
Production
Users
read,
write
read
read
-
-
-
write
Application
Prgmers
-
-
read
read,
write
-
read
write
System
Prgmers
-
-
read
-
read,
write
read
write
System
Controllers
read,
write
read,
write
read,
write
read,
write
read,
write
read,
write
write
Managers
& Auditors
read
read
read
read
read
read
read,
write
Table 13.5: Access-control matrix for commercial integrity via Bell‚ÄìLa Padula
Subject Class
ConÔ¨Ådentiality Level
Ordinary user
(SL,{PD,PC})
Application programmer
(SL,{D,T})
System programmer
(SL,{SD,T})
System controller
(SL,{PD,PC,D,SD,T}) and downgrade privilege
System management or auditor
(AM,{PD,PC,D,SD,T})
Table 13.6: ConÔ¨Ådentiality levels for subjects
Similarly, system managers and auditors should be able to read every sort of data and
code; they are therefore assigned the full set of categories {PD, PC, D, SD, T}.
Table 13.6 shows the conÔ¨Ådentiality levels assigned to each kind of subject. Note
that subjects with classiÔ¨Åcation level SL‚Äîsuch as ordinary users and application
programmers‚Äîhave the lowest possible classiÔ¨Åcation level. Therefore, under Bell‚Äì
La Padula‚Äôs *-property (i.e., ‚Äúno write down‚Äù), their ability to modify a given object
is limited only by the set of categories associated with that object. For example,
application programmers are assigned the set of categories {D, T}: they can modify
any object whose associated categories include both D and T. System managers and
auditors are further limited by their classiÔ¨Åcation level AM: they cannot write to
objects whose classiÔ¨Åcation level is SL.
In contrast, conÔ¨Ådentiality levels are assigned to objects based on the levels of
those subjects who must be able to read them. In particular, objects are assigned
conÔ¨Ådentiality levels as high as possible, subject to the constraint imposed by the
simple security condition of Bell‚ÄìLa Padula (i.e., ‚Äúno read up‚Äù): no object‚Äôs con-
Ô¨Ådentiality level can be higher than that of a subject who must be able to read

280
Access Control, Security, and Trust: A Logical Approach
Object Class
ConÔ¨Ådentiality Level
Production data
(SL,{PD,PC})
Production code
(SL,{PC})
Systems programs
(SL,{})
Development code & test data
(SL,{D,T})
System programs in modiÔ¨Åcation
(SL,{SD,T})
Software tools
(SL,{T})
System and application audit logs
(AM,{PD,PC,D,SD,T})
Table 13.7: ConÔ¨Ådentiality levels for objects
it. For example, software tools must be readable by both application programmers
(who have conÔ¨Ådentiality level (SL,{D, T})) and system programmers (who have
conÔ¨Ådentiality level (SL,{SD, T})). Consequently, the highest conÔ¨Ådentiality level
that can be assigned to software tools is (SL,{T})). Similarly, audit logs must be
readable by system managers and auditors, and hence they are assigned the level
(AM,{PD, PC, D, SD, T}). Table 13.7 shows the conÔ¨Ådentiality levels for each class
of object.
One class of subjects requires special mention: system controllers have the spe-
cial role of moving programs out of development and into production. Consequently,
they need full read and write access to all Ô¨Åles except audit logs. Satisfying both the
simple security condition and the *-property, however, means that a subject can have
both read and write access to a given object only if the subject‚Äôs level exactly matches
the object‚Äôs level. We have already seen that production data, production code, devel-
opment code, software tools, and system programs in modiÔ¨Åcation all have different
conÔ¨Ådentiality levels. How, then, do we assign an appropriate level to system con-
trollers? The answer is that system controllers‚Äîand only system controllers‚Äîmust
be allowed to downgrade their classiÔ¨Åcation level. Therefore, system controllers are
assigned the conÔ¨Ådentiality level (SL,{PD,PC,D,SD,T}), along with the privilege to
downgrade their levels as necessary.
It is straightforward to translate the contents of the Table 13.5 access-control ma-
trix into the access-control logic. For example, the ability of users to read and write
production data is given by the following two statements:
slev(Production Data) ‚â§s slev(User) ‚äÉ(User controls ‚ü®read,Production Data‚ü©),
slev(User) ‚â§s slev(Production Data) ‚äÉ(User controls ‚ü®write,Production Data‚ü©).
Although this scheme satisÔ¨Åes the Ô¨Åve commercial-integrity requirements initially
proposed, Lipner points out a potential drawback: it is unclear how special-purpose
software (such as a database-repair program) Ô¨Åts into this scheme. Such programs
must be able to operate on production data but cannot be under the control of users.
He addresses this shortcoming via a combination of Bell‚ÄìLa Padula conÔ¨Ådentiality
and Biba strict integrity, which we explore in the next subsection.

ConÔ¨Ådentiality and Integrity Policies
281
Objects
Subjects
Prod
Data
Prod
Code
System
Prog
Dev
App
Prog
Dev
Sys
Prog
SW
Tools
Audit
Logs
Repair
Code
Production
Users
read,
write
read
read
-
-
-
write
-
Application
Prgmrs
-
-
read
read,
write
-
read
write
-
System
Prgmrs
-
-
read
-
read,
write
read
write
-
System Con-
trollers
read,
write
read,
write
read,
write
read,
write
read,
write
read,
write
write
read,
write
Managers &
Auditors
read
read
read
read
read
read
read,
write
read
Repair
read,
write
read
read
-
-
-
write
read
Table 13.8: Access-control matrix for both conÔ¨Ådentiality and strict integrity
13.5.3
Commercial Integrity via Bell‚ÄìLa Padula and Strict
Integrity
A major objective for combining the Bell‚ÄìLa Padula model with Biba‚Äôs strict in-
tegrity is to allow the use of special-purpose software to repair errors or inconsis-
tencies in production databases. Lipner states this additional requirement as follows
(Lipner, 1982, page 7):
Special-purpose application software shall be provided to effect ‚Äúdata
base repair‚Äù on the production data base. This software may be used by
members of the application programmer or system control population
under special circumstances.
The desired discretionary access-control matrix for this new situation appears in
Table 13.8. Note that this table is identical to Table 13.5, except for the addition of
a new class of subjects (Repair) and a new class of objects (Repair Code). Lipner
identiÔ¨Åed a combination of conÔ¨Ådentiality and integrity levels that, when suitably
assigned to subjects and objects, permits this access-control matrix to be consistent
with both Biba‚Äôs strict integrity and the Bell‚ÄìLa Padula conÔ¨Ådentiality policies.
For example, Table 13.8 shows that system managers have both read and write
access to the audit logs. Granting read access to system managers requires that the
following two properties hold:
slev(Audit Log) ‚â§s slev(System Manager),
ilev(System Manager) ‚â§i ilev(Audit Log).
The Ô¨Årst property is the mandatory access-control condition for read operations in
Bell‚ÄìLa Padula (i.e., ‚Äúno read up‚Äù), while the second condition is the ‚Äúno read down‚Äù

282
Access Control, Security, and Trust: A Logical Approach
condition for strict integrity. The combination of these two requirements can be
expressed in the logic as follows:
slev(Audit Log) ‚â§s slev(System Manager) ‚äÉ
ilev(System Manager) ‚â§i ilev(Audit Log) ‚äÉ
(System Manager controls ‚ü®read,Audit Log‚ü©).
Similarly, for write operations, we have a combination of Bell‚ÄìLa Padula‚Äôs ‚Äúno write
down‚Äù and Biba‚Äôs ‚Äúno write up‚Äù requirements:
slev(System Manager) ‚â§s slev(Audit Log) ‚äÉ
ilev(Audit Log) ‚â§i ilev(System Manager) ‚äÉ
(System Manager controls ‚ü®write,Audit Log‚ü©).
Lipner‚Äôs revised model handles conÔ¨Ådentiality levels in a very similar manner to
the original model. In particular, the conÔ¨Ådentiality classiÔ¨Åcations from the Ô¨Årst
model are retained: Lcon f = {AM, SL}, with SL ‚â§s AM. However, only three conÔ¨Å-
dentiality categories are required: Catconf = {Pc, Dc, SDc}. These categories loosely
correspond to the types of code and the domains in which they may reside: pro-
duction data and code (Pc), application code under development (Dc), and system
programs under development (SDc).
As in the initial model, the higher level AM (‚Äúaudit manager‚Äù) is assigned to man-
agers and auditors, while the lower level SL (‚Äúsystem low‚Äù) is assigned to all other
subjects. Likewise, categories are assigned to subjects based on the types of data and
programs that they can modify and use. Thus, for example, application program-
mers are assigned the category Dc, because they can modify application code under
development. In contrast, system managers and auditors are assigned the set of cat-
egories {Pc, Dc, SDc}, because they require read access to production data and code,
application code under development, and system programs under development.
ConÔ¨Ådentiality levels are then assigned to objects based on the conÔ¨Ådentiality lev-
els of the subjects who can read them. In particular, objects are assigned as high a
level as possible, subject to the ‚Äúno read up‚Äù constraint imposed by Bell‚ÄìLa Padula.
For example, software tools should be readable by everyone, regardless of their con-
Ô¨Ådentiality level: thus software tools must be assigned the lowest possible conÔ¨Åden-
tiality level (i.e., (SL,{})). In contrast, audit logs should be read only by system
managers and auditors, and hence audit logs are assigned the conÔ¨Ådentiality level
(AM,{Pc, Dc, SDc}).
The new aspect of this model is the assignment of integrity levels that prevent
users and programs from corrupting data and programs at higher integrity levels.
The set of integrity classiÔ¨Åcations is Lint = {SPi, OI, SLi}, where
SLi ‚â§i OI ‚â§i SPi.
System programs and repair programs are given the highest integrity classiÔ¨Åcation
(SPi), and thus can be modiÔ¨Åed only by subjects with sufÔ¨Åciently high levels. Produc-
tion code and software tools‚Äîwhich can be modiÔ¨Åed by a certain segment of users‚Äî
are given the intermediate OI (‚Äúoperational infrastructure‚Äù) classiÔ¨Åcation. Audit logs

ConÔ¨Ådentiality and Integrity Policies
283
Level
Object
ConÔ¨Ådentiality
Integrity
Production Data
(SL,{Pc})
(SLi,{Pi})
Production Code
(SL,{Pc})
(OI,{Pi})
System Programs
(SL,{})
(SPi,{Pi, Di})
Development Code & Test Data
(SL,{Dc})
(SLi,{Di})
System Programs in ModiÔ¨Åcation
(SL,{SDc})
(SLi,{Di})
Software Tools
(SL,{})
(OI,{Di})
Audit Logs
(AM,{Pc, Dc, SDc})
(SLi,{})
Repair
(SL,{Pc})
(SPi,{Pi})
Table 13.9: ConÔ¨Ådentiality and integrity levels for objects
are given the lowest integrity classiÔ¨Åcation (SLi), because they must be writable by
everyone. Similarly, production data is classiÔ¨Åed at the SLi level, because data can
be written by users at the lowest level.
The set of integrity categories is Catint = {Pi, Di}, where the categories are used
to distinguish between the production and development domains. The assignment of
objects to these integrity categories is intuitive. Production code, production data,
and repair programs are assigned the Pi (‚Äúproduction integrity‚Äù) category, while ap-
plication code, system programs in modiÔ¨Åcation, and software tools are assigned
the Di category (‚Äúdevelopment integrity‚Äù). The result is that programs in develop-
ment are effectively isolated from production code and production data. System
programs‚Äîwhich are used by both users and developers‚Äîare assigned both cate-
gories. In contrast, audit logs (which must be writable by everyone) are assigned the
lowest integrity level (SLi,{}). Table 13.9 summarizes both the conÔ¨Ådentiality and
integrity levels for objects.
Integrity levels are then granted to subjects based on the objects that they must be
able to modify: subjects are granted as low an integrity level as possible, subject to
the ‚Äúno write up‚Äù constraint imposed by Biba‚Äôs strict-integrity policy. For example,
application programmers must be able to write to both development code (integrity
level (SLi,{Di})) and the audit logs (integrity level (SLi,{})): the lowest integrity
level that allows write access to both items is (SLi,{Di}).
Table 13.10 shows the assignment of conÔ¨Ådentiality and integrity levels to sub-
jects. Given this assignment and that of Table 13.9, it is straightforward to check that
all operations in Table 13.8 are permitted by the Bell‚ÄìLa Padula model. However,
two aspects of this assignment deserve special mention.
First, system controllers alone have the ability to downgrade their integrity or con-
Ô¨Ådentiality level by altering their categories, which enables them to match integrity
and conÔ¨Ådentiality levels with any objects other than audit logs. Consequently, sys-
tem controllers are able to read and write to objects as necessary to move them from
development to production.
Second, people operating in the repair function have exactly the same integrity
and conÔ¨Ådentiality levels as production users. However, the discretionary access

284
Access Control, Security, and Trust: A Logical Approach
Level
Subject
ConÔ¨Ådentiality
Integrity
Production Users
(SL,{Pc})
(SLi,{Pi})
Application Programmers
(SL,{Dc})
(SLi,{Di})
System Programmers
(SL,{SDc})
(SLi,{Di})
Sys Controllers (incl. downgrade privilege)
(SL,{Pc, Dc, SDc})
(SPi,{Pi, Di})
Managers & Auditors
(AM,{Pc, Dc, SDc})
(SLi, {})
Repair
(SL,{Pc})
(SLi,{Pi})
Table 13.10: ConÔ¨Ådentiality and integrity levels for subjects
matrix in Table 13.8 shows that production users and repair people do not have the
same privileges: repair people have the additional privilege of reading repair code.
Although the mandatory access-control policies would permit production users to
read repair code, doing so is not a requirement for the system: users have no need to
have that privilege. Thus, in accordance with the principle of least privilege, users
are denied discretionary read access to repair code.
Exercise 13.5.1
Consider the access-control matrix in Table 13.5.
a. Using the access-control logic, formulate access-control policy statements for
production users that reÔ¨Çect both the mandatory access-control requirements
of Bell‚ÄìLa Padula and the discretionary access-control matrix.
b. Using the access-control logic, devise a theorem that shows under what con-
ditions access is granted for production users. Give a formal proof for each
theorem.
Exercise 13.5.2
Consider the access rights of system controllers, as given by the
access-control matrix in Table 13.5.
a. Describe precisely how system controllers are able to read and write to all
objects (with the exception of audit logs).
b. Downgrading clearance levels is limited to system controllers only. Describe
what undesirable effects would occur if other subjects could downgrade their
clearance level.
Exercise 13.5.3
a. Consider the assignment of conÔ¨Ådentiality levels to subjects and objects given
in Tables 13.10 and 13.9. Using the access-control matrix in Table 13.8 as a
guide, devise an access-control matrix that shows the maximum privileges al-
lowable to subjects when only the conÔ¨Ådentiality levels of subjects and objects
are considered.

ConÔ¨Ådentiality and Integrity Policies
285
b. Consider the assignment of integrity levels to subjects and objects given in Ta-
bles 13.10 and 13.9. Using the access-control matrix in Table 13.8 as a guide,
devise an access-control matrix that shows the maximum privileges allowable
to subjects when only the integrity levels of subjects and objects are considered
c. Consider the privileges of subjects obtained by intersecting the above two ta-
bles. How and why does this table differ from the access-control matrix in
Table 13.8?
Exercise 13.5.4
Extend the access-control logic to accommodate the use of both
conÔ¨Ådentiality and integrity levels. Devise the extensions to both the syntax and
semantics.
13.6
Summary
In this chapter, we introduced compound levels for both conÔ¨Ådentiality and in-
tegrity, based on the combined notions of classiÔ¨Åcation levels and categories. We
also introduced the related notions of level downgrading, sanitization, and contain-
ment.
We reviewed both the Bell‚ÄìLa Padula model for conÔ¨Ådentiality and Biba‚Äôs strict-
integrity model. We also described in detail Lipner‚Äôs approach to commercial in-
tegrity: this approach relies on a combination of Bell‚ÄìLa Padula‚Äôs conÔ¨Ådentiality
model and Biba‚Äôs strict-integrity model. Lipner‚Äôs work provides a good illustration
of how levels can be assigned to subjects and objects to satisfy a model‚Äôs policy
constraints.
The learning outcomes for this chapter appear in Figure 13.3.
13.7
Further Reading
Bell and La Padula‚Äôs work on conÔ¨Ådentiality is contained in two reports (Bell
and La Padula, 1975) and (Bell and La Padula, 1973). An alternate approach for
conÔ¨Ådentiality in commercial contexts is given by the Chinese Wall model (Brewer
and Nash, 1989), which is used to handle conÔ¨Çicts of interests in situations where
staff members in the same business (e.g., a law Ô¨Årm or accounting Ô¨Årm) work for
clients who are competitors. The Chinese Wall policy places competitors in the
same conÔ¨Çict-of-interest class. Employees are then restricted to accessing at most
one member in the same conÔ¨Çict-of-interest class: once an employee has accessed
one member of a conÔ¨Çict-of-interest class, all other members of the class are off

286
Access Control, Security, and Trust: A Logical Approach
limits. In this way, access restrictions in the Chinese Wall become more restrictive
over time, whereas restrictions in Bell‚ÄìLa Padula do not.
In addition to strict integrity, Biba‚Äôs original report (Biba, 1975) proposed two re-
lated and alternative policies: the low-watermark policy and the ring policy. Both
policies impose the same restrictions on modiÔ¨Åcations and invocations that strict
integrity does, but they impose different restrictions upon observations. The low-
water-mark policy allows a subject to observe objects below its integrity level, pro-
vides that the subject‚Äôs integrity level is reduced to that of the observed object. The
ring policy imposes no mandatory access controls on subjects‚Äô observations of ob-
jects; as a result, there are fewer built-in integrity protections if a subject unwisely
chooses to read an untrustworthy source.

ConÔ¨Ådentiality and Integrity Policies
287
FIGURE 13.3 Learning outcomes for Chapter 13
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Application
‚Ä¢ When given the conÔ¨Ådentiality- or integrity-level assignments and the
discretionary access-control matrix of a system, you should be able to
calculate whether or not a read or write access request should be permit-
ted.
‚Ä¢ Express conÔ¨Ådentiality policies consistent with the Bell‚ÄìLa Padula
model in the access-control logic.
‚Ä¢ Express integrity policies consistent with the Biba strict-integrity model
in the access-control logic.
Synthesis
‚Ä¢ When given an access-control scenario involving conÔ¨Ådentiality and/or
integrity, you should be able to deÔ¨Åne conÔ¨Ådentiality and/or integrity
levels for subjects and objects, and devise a discretionary access-control
matrix that accurately reÔ¨Çects the scenario.
‚Ä¢ When given an access-control scenario that involves the Bell‚ÄìLa Padula
model, you should be able to formalize the scenario, identify all neces-
sary trust assumptions, and formally justify the granting of a request.
‚Ä¢ When given an access-control scenario that involves the Biba strict-
integrity model, you should be able to formalize the scenario, identify
all necessary trust assumptions, and formally justify the granting of a
request.
Evaluation
‚Ä¢ When given a set of policy deÔ¨Ånitions alleged to be consistent with the
Bell‚ÄìLa Padula or strict-integrity models, you should be able to judge
whether they are consistent and (if not) identify the inconsistencies.


Chapter 14
Role-Based Access Control
In the previous chapter, we examined conÔ¨Ådentiality and integrity policies, which
permit access based in part on principals‚Äô and objects‚Äô classiÔ¨Åcation levels. In many
organizations, however, access-control policies are based instead upon employees‚Äô
job functions: for example, programmers must have both read and write access to
relevant source code, while salespeople may require access to internal marketing
reports. Role-based access control (RBAC) was designed to simplify the task of
managing such policies by explicitly introducing a notion of roles, which serve as in-
termediate links between users (i.e., employees) and permissions (i.e., their required
access rights).
In this chapter, we introduce the basic components of RBAC, which are based on
the related but independent notions of users, roles, and permissions. We also discuss
notions of separation of duty, which provide constraints that can limit the potential
for fraud or conÔ¨Çicts of interest. Finally, we show how the access-control logic can
be used to reason about access-control decisions within RBAC systems.
14.1
RBAC Fundamentals
The key idea behind RBAC is that an organization comprises many people (i.e.,
users), each of whom perform several job functions (i.e., roles). In turn, a person
performing a certain job function requires a collection of permissions to successfully
complete their tasks (e.g., an accountant must have the ability to read and modify the
accounting ledger). Therefore, at its core, an RBAC system can be described by the
following Ô¨Åve sets:
‚Ä¢ Users, the set of users
‚Ä¢ Roles, the set of roles
‚Ä¢ Perms, the set of permissions
‚Ä¢ UA ‚äÜPerms√óRoles, the user-to-role assignment (or simply user assignment)
‚Ä¢ PA ‚äÜPerms√óRoles, the permission-to-role assignment (or simply permission
assignment)
289

290
Access Control, Security, and Trust: A Logical Approach
The user-to-role assignment catalogs the associations between users and the roles
they perform: a pair (u,r) ‚ààUA indicates that user u has been explicitly assigned to
role r. Similarly, the permission-to-role assignment indicates which permissions are
associated with which roles: a pair (p,r) ‚ààUA indicates that permission p has been
explicitly assigned to role r.
The following example provides a simple RBAC description of a small restaurant.
Example 14.1
Consider a local restaurant with Ô¨Åve employees (Pat, Mel, Lee, Sam, and Kim) and
four identiÔ¨Åed job functions (host, server, chef, and manager). At present, the restau-
rant management has assigned responsibilities as follows: hosts may seat customers
and take reservations; servers may take orders, serve food to customers, clear tables,
and accept payment from customers; chefs may cook food; and managers may offer
customers a free meal, in addition to anything that a host or server can do.
The restaurant‚Äôs operations can be described by the following RBAC components:
Users = {Pat,Mel,Lee,Sam,Kim},
Roles = {Host,Server,Chef,Manager},
Perms = {‚ü®seat,customer‚ü©, ‚ü®take,reservation‚ü©, ‚ü®take,order‚ü©, ‚ü®cook,food‚ü©,
‚ü®serve,food‚ü©, ‚ü®clear, table‚ü©, ‚ü®accept,payment‚ü©, ‚ü®offer,free meal‚ü©},
UA = {(Pat,Host), (Pat,Server), (Lee,Server), (Mel,Chef), (Mel,Server),
(Kim,Manager), (Sam,Manager), (Sam,Chef)},
PA = {(‚ü®seat,customer‚ü©,Host), (‚ü®take,reservation‚ü©,Host), (‚ü®cook,food‚ü©,Chef),
(‚ü®take,order‚ü©,Server), (‚ü®serve,food‚ü©,Server), (‚ü®clear,table‚ü©,Server),
(‚ü®accept,payment‚ü©,Server), (‚ü®offer,free meal‚ü©,Manager),
(‚ü®seat,customer‚ü©,Manager), (‚ü®take,reservation‚ü©,Manager),
(‚ü®take,order‚ü©,Manager), (‚ü®serve,food‚ü©,Manager),
(‚ü®clear,table‚ü©,Manager), (‚ü®accept,payment‚ü©,Manager) }.
‚ô¶
As the previous example illustrates, the same user (e.g., Pat) may be assigned
to multiple roles. Similarly, the same permission (e.g., ‚ü®seat,customer‚ü©) may be
assigned to multiple roles.
14.1.1
Role Inheritance
RBAC also introduces a notion of role inheritance, which reÔ¨Çects the fact that
some job functions subsume others. For example, an accounting supervisor may
need to perform the standard accountant duties, in addition to her supervisory duties.
To express such situations more concisely, RBAC incorporates a role-inheritance
(also known as a role-hierarchy or role-dominance) relation
‚™∞‚äÜRoles√óRoles,

Role-Based Access Control
291
FIGURE 14.1 Sample Hasse diagram
T
@
@
@
   Y
   W
X
Z
@
@
@
M
U
which is a partial order on roles. Thus, the role-inheritance relation ‚™∞necessarily
satisÔ¨Åes the following properties:
‚Ä¢ ReÔ¨Çexivity: For all roles r, r ‚™∞r (i.e., every role inherits itself).
‚Ä¢ Transitivity: For all roles r1,r2,r3, if r1 ‚™∞r2 and r2 ‚™∞r3, then r1 ‚™∞r3.
‚Ä¢ Anti-symmetry: For all roles r1,r2, if r1 ‚™∞r2 and r2 ‚™∞r1, then r1 = r2 (i.e.,
no cycles exist in the inheritance relation).
As with other partial orders, it is often convenient to present the role-inheritance
relation graphically as a Hasse diagram. As an example, Figure 14.1 presents a Hasse
diagram for the relation
‚™∞= {(r,r) | r ‚àà{M,T,U,X,Y,W,Z}}
‚à™{(X,Y),(X,W),(X,T),(X,Z),(M,Z),(M,T)}.
The primary purpose of the role-inheritance relation is to allow more succinct
descriptions of user and permission assignments. Towards this end, we introduce
two functions
auth users : Roles ‚ÜíP(Users),
auth perms : Roles ‚ÜíP(Perms),
which, when given a speciÔ¨Åc role, respectively return the sets of authorized users and
authorized permissions for that role:
auth users(r) = {u | ‚àÉr‚Ä≤ ‚ààRoles.(r‚Ä≤ ‚™∞r and (u,r‚Ä≤) ‚ààUA)},
auth perms(r) = {p | ‚àÉr‚Ä≤ ‚ààRoles.(r ‚™∞r‚Ä≤ and (p,r‚Ä≤) ‚ààPA)}.
Thus, the authorized users of a role r are all those users explicitly assigned (through
the user-to-role assignment) to a role that inherits r. In contrast, the authorized
permissions of a role r are all those permissions explicitly assigned (through the
permission-to-role assignment) to a role that r inherits.
It is straightforward to show that the following properties hold:

292
Access Control, Security, and Trust: A Logical Approach
‚Ä¢ If r1 ‚™∞r2, then auth users(r1) ‚äÜauth users(r2).
‚Ä¢ If r1 ‚™∞r2, then auth perms(r2) ‚äÜauth perms(r1).
In this sense, users ‚ÄúÔ¨Çow down‚Äù the role hierarchy while permissions ‚ÄúÔ¨Çow up.‚Äù
The following example shows how the judicious choice of a role-inheritance rela-
tion simpliÔ¨Åes the permission assignment for the restaurant of Example 14.1.
Example 14.2
Recall the restaurant from Example 14.1, and suppose the role-inheritance relation
‚™∞is given by the following Hasse diagram:
Host
   Server
Manager
Chef
The restaurant‚Äôs operations can be described by the following RBAC components
(the sets Users, Roles, Perms, and UA remain unchanged from Example 14.1):
Users = {Pat,Mel,Lee,Sam,Kim},
Roles = {Host,Server,Chef,Manager},
Perms = {‚ü®seat,customer‚ü©, ‚ü®take,reservation‚ü©, ‚ü®take,order‚ü©, ‚ü®cook,food‚ü©,
‚ü®serve,food‚ü©, ‚ü®clear, table‚ü©, ‚ü®accept,payment‚ü©, ‚ü®offer,free meal‚ü©},
UA = {(Pat,Host), (Pat,Server), (Lee,Server), (Mel,Chef), (Mel,Server),
(Kim,Manager), (Sam,Manager), (Sam,Chef)},
PA = {(‚ü®seat,customer‚ü©,Host), (‚ü®take,reservation‚ü©,Host), (‚ü®cook,food‚ü©,Chef),
(‚ü®take,order‚ü©,Server), (‚ü®serve,food‚ü©,Server), (‚ü®clear,table‚ü©,Server),
(‚ü®accept,payment‚ü©,Server), (‚ü®offer,free meal‚ü©,Manager)}.
Given these base sets, we can calculate the authorized users for the roles Host and
Manager as follows:
auth users(Host) = {u | ‚àÉr‚Ä≤ ‚ààRoles.(r‚Ä≤ ‚™∞Host and (u,r‚Ä≤) ‚ààUA)}
= {u | (u,Host) ‚ààUA}‚à™{u | (u,Manager) ‚ààUA}
= {Pat,Kim,Sam},
auth users(Manager) = {u | ‚àÉr‚Ä≤ ‚ààRoles.(r‚Ä≤ ‚™∞Manager and (u,r‚Ä≤) ‚ààUA)}
= {u | (u,Manager) ‚ààUA}
= {Kim,Sam}.

Role-Based Access Control
293
Both Kim and Sam are authorized users of the role Host by virtue of role inheritance.
Similarly, we can calculate the authorized permissions for the same two roles:
auth perms(Host) = {p | ‚àÉr‚Ä≤ ‚ààRoles.(Host ‚™∞r‚Ä≤ and (p,r‚Ä≤) ‚ààPA)}
= {p | (p,Host) ‚ààPA}
= {‚ü®seat,customer‚ü©,‚ü®take,reservation‚ü©}
auth perms(Manager) = {p | ‚àÉr‚Ä≤ ‚ààRoles.(Manager ‚™∞r‚Ä≤ and (p,r‚Ä≤) ‚ààPA)}
= {p | (p,Host) ‚ààPA}‚à™{p | (p,Server) ‚ààPA}
‚à™{p | (p,Manager) ‚ààPA}
= {‚ü®seat,customer‚ü©,‚ü®take,reservation‚ü©,‚ü®take,order‚ü©,
‚ü®serve,food‚ü©,‚ü®clear, table‚ü©,
‚ü®accept,payment‚ü©,‚ü®offer,free meal‚ü©}.
In particular, the Manager role inherits all of the permissions associated with the
Host and Server roles.
‚ô¶
Exercise 14.1.1
Consider the following collection of RBAC deÔ¨Ånitions:

Users = {Ian, Moe, Sal, Pam, Rob}
Perms = {a,b,c,d,e,g,h,k,w,y}
Roles = {A,B,C,D,E,G,H,K,W,Y}
UA = {(Ian,A),(Moe,C),(Moe,E),(Pam,Y),
(Pam,H),(Rob,B),(Sal,K)}
PA = {(a,A),(b,B),(c,C),(d,D),(e,E),(f,F),
(g,G),(h,H),(k,K),(w,W),(y,Y)}
The relation ‚™∞is given by the following Hasse diagram:
K
  C
D
  @
@
G
B
E
A
A
A
A
AA
Y
A
  W
H
Calculate the following:

294
Access Control, Security, and Trust: A Logical Approach
a. auth users(A).
b. auth users(E).
c. auth users(W).
d. auth perms(A).
e. auth perms(E).
f. auth perms(W).
Exercise 14.1.2
A small construction Ô¨Årm has the following staff:
‚Ä¢ Alex, a carpenter who sometimes also Ô¨Ålls in as supervisor
‚Ä¢ Blake, an electrician
‚Ä¢ Cal, a plumber‚Äôs apprentice who sometimes also Ô¨Ålls in as ofÔ¨Åce manager
‚Ä¢ Dana, a plumber
‚Ä¢ Eddy, a carpenter‚Äôs apprentice and sometimes electrician‚Äôs apprentice
‚Ä¢ Fran, a supervisor and sometimes electrician
Due to a combination of factors (city licensing statutes, ongoing labor negotia-
tions, and the like), the Ô¨Årm has developed a collection of guidelines to govern the
permissible work-related activities of its employees. No permissions other than those
explicitly stated in the following rules are granted:
‚Ä¢ Carpenters may lay Ô¨Çooring (Ô¨Çoor), hang drywall (drywall), and install cabi-
nets (cabs).
‚Ä¢ Electricians may install or reconÔ¨Ågure wiring (wire), test wiring, and install
outlets.
‚Ä¢ Plumbers may lay pipes and install Ô¨Åxtures.
‚Ä¢ Apprentices are there to assist, plus they have speciÔ¨Åc duties related to their
basic trades: a carpenter‚Äôs apprentice may lay Ô¨Çooring (Ô¨Çoor), an electri-
cian‚Äôs apprentice may test wiring, and a plumber‚Äôs apprentice may lay pipes.
‚Ä¢ Electricians and plumbers are also able to help design their respective sys-
tems.
‚Ä¢ The supervisor can naturally supervise work, and assist when needed.
‚Ä¢ The ofÔ¨Åce manager may schedule jobs, order supplies, and bill customers.
‚Ä¢ All employees can review the current work plan.

Role-Based Access Control
295
Thus, for example, a carpenter may neither schedule jobs nor assist, but she can
always review the current work plan.
For the purposes of this question, let Users and Perms be deÔ¨Åned as follows, with
the obvious correspondence with the Ô¨Årm‚Äôs staff and work-related activities:
Users = {A,B,C,D,E,F}
Perms = {Ô¨Çoor,drywall,cabs,wire,test,outlets,pipes,Ô¨Åxtures,assist,
design,supervise,schedule,order,bill,review}
a. DeÔ¨Åne a set Roles of roles, along with a permission-to-role assignment PA,
a user-role assignment UA, and a role-dominance relation ‚™∞to accurately
capture the above guidelines.
To fully appreciate the implications of the role hierarchy, specify PA in such a
way that it maps each permission to exactly one role (i.e., PA should contain
exactly 15 pairs). Therefore, you will need to include roles that do not directly
correspond to particular job titles.
b. Using your deÔ¨Ånitions from Part a, calculate the following:
(a) The set of authorized permissions (i.e., auth perms(r)) for each role r ‚ààR
(b) The set of authorized users (i.e., auth users(r)) for each role r ‚ààR
14.1.2
Sessions
Although a speciÔ¨Åc user might be authorized for many roles, there are times that
she may wish to activate only a subset of roles, perhaps to avoid a conÔ¨Çict of interest
or simply to minimize unnecessary privileges. For example, suppose that Annie
is authorized for both the User and Superuser roles on her department‚Äôs computer
system. She may purposely elect to activate only the User role when she plans to
edit her personal Ô¨Åles, to avoid the possibility of accidentally altering system Ô¨Åles
that can be accessed only through the Superuser role.
RBAC provides the ability to activate only a subset of authorized roles through the
abstract notion of sessions. Intuitively, a session represents a period of time during
which a certain subset of a user‚Äôs authorized roles is activated. In the digital world,
a session might correspond to a user‚Äôs login session or to a speciÔ¨Åc open window or
shell. In the physical world‚Äîsuch as the restaurant described in Example 14.2‚Äîa
session might correspond to a speciÔ¨Åc work shift for an employee.
More formally, an RBAC system includes a set Sessions of sessions, in the same
way that it includes sets of users, roles, and permissions. Each session has precisely
one user associated with it, but zero or more roles associated with it.
When given a session s, the functions
user : Sessions ‚ÜíUsers,
roles : Sessions ‚ÜíP(Roles)

296
Access Control, Security, and Trust: A Logical Approach
respectively return the user and the roles associated with s. As an important note,
these two functions must be consistent with one another: for any speciÔ¨Åc session s,
user(s) must be an authorized user for each of the roles in roles(s). That is, for all
sessions s, the following constraint must hold:
roles(s) ‚äÜ{r ‚ààRoles | user(s) ‚ààauth users(r)}.
The following example extends the earlier restaurant example by introducing some
sessions that correspond to work shifts for particular employees.
Example 14.3
Recall the restaurant from Example 14.2, and consider the following work schedule:
‚Ä¢ On Monday night, Pat works as a server, and Kim works as manager.
‚Ä¢ On Tuesday afternoon, Kim works as both host and server; Pat has the day off,
and does not work at all.
This situation allows for the four sessions {s1,s2,s3,s4}, where s1 is Pat‚Äôs Monday-
night shift, s2 is Kim‚Äôs Monday-night shift, s3 is Pat‚Äôs Tuesday-afternoon shift, and
s4 is Kim‚Äôs Tuesday-afternoon shift.
The users and roles associated with each shift are as follows:
user(s1) = Pat,
roles(s1) = {Server},
user(s2) = Kim,
roles(s2) = {Manager},
user(s3) = Pat,
roles(s3) = /0,
user(s4) = Kim,
roles(s4) = {Host,Server}.
One can easily verify that, for each shift si,
roles(si) ‚äÜ{r ‚ààRoles | user(si) ‚ààauth users(r)}.
‚ô¶
Exercise 14.1.3
Which of the following proposed sessions would be permitted un-
der the RBAC descriptions from Example 14.2? Explain your answers.
a. Session sa, where user(sa) = Mel and roles(sa) = {Chef,Server}
b. Session sb, where user(sb) = Lee and roles(sb) = {Server,Manager}
c. Session sc, where user(sc) = Sam and roles(sc) = {Chef,Server}

Role-Based Access Control
297
14.2
Separation of Duty
When developing security policies, a common approach is to divide critical op-
erations among two or more principals, so that no single principal can compromise
security. For example, banks often require two people to be present when ATM de-
posits are opened and processed. Likewise, health-insurance companies may require
multiple doctors to assert that a medical procedure is necessary before the company
will pay for the procedure.
RBAC provides a means for enforcing such divisions by introducing separation-
of-duty constraints. RBAC recognizes the following two forms of separation of duty:
1. Static separation of duty enforces constraints on the user assignment, so that
users are prevented from ever becoming authorized for conÔ¨Çicting roles.
2. Dynamic separation of duty enforces constraints on the roles that may be acti-
vated concurrently (i.e., within a single session): a user may be authorized for
conÔ¨Çicting roles, but he will be unable to activate them simultaneously.
We look at these two types of constraints in sequence.
14.2.1
Static Separation of Duty
To support static separation of duty, RBAC introduces a relation
SSD ‚äÜP(Roles)√ó(N‚àí{0,1}),
whose purpose is to constrain how the user assignment UA is constructed.
Note that each element of SSD has the form (rs,n), where rs is a set of roles and
n is an integer greater than 1. Each such element reÔ¨Çects a particular separation-of-
duty constraint. In particular, for each (rs,n) ‚ààSSD, the following condition must
hold: for every subset rs‚Ä≤ of rs that contains at least n elements,
\
r‚ààrs‚Ä≤
auth users(r) = /0.
That is, each constraint (rs,n) ‚ààSSD mandates that no user may be authorized for n
or more roles from the set rs.
The following example illustrates how the SSD relation can be used to impose
static separation of duty.
Example 14.4
A small business is concerned about the possibility of embezzlement, so they have
split up the Ô¨Ånancial duties among three separate job functions: only a manager can
approve expenses, only an account clerk can record expenses, and only a cashier can

298
Access Control, Security, and Trust: A Logical Approach
actually release funds. Thus, they have adopted the following roles, permissions, and
permission assignment:
Roles = {Manager,AccountClerk,Cashier},
Perms = {‚ü®approve,expense‚ü©,‚ü®record,expense‚ü©,‚ü®release,funds‚ü©},
PA = {(‚ü®approve,expense‚ü©,Manager),(‚ü®record,expense‚ü©,AccountClerk),
(‚ü®release,funds‚ü©,Cashier)}.
A signiÔ¨Åcant danger of embezzlement exists if a single person becomes autho-
rized for all three roles (Manager, AccountClerk, and Cashier). The following SSD
relation imposes a constraint to prevent such an occurrence:
SSD = {({AccountClerk,Cashier},2)}.
This relation prevents anyone from being an authorized user for more than one of
the roles AccountClerk and Cashier. Thus, for example, any user authorized for the
Cashier role could not also be authorized for the AccountClerk role.
‚ô¶
It is important to realize that a SSD constraint constrains the number of roles for
which a given user may be authorized. As a result, to verify that static separation-of-
duty constraints are met, it is important to consider not only the user-to-role assign-
ment but also the role hierarchy. The following example illustrates how the combi-
nation of role hierarchy and SSD constraints may introduce unintended effects.
Example 14.5
Recall the RBAC descriptions for the small business in Example 14.4, and suppose
that the role-inheritance relation ‚™∞has been deÔ¨Åned such that
Manager ‚™∞AccountClerk,
Manager ‚™∞Cashier.
It therefore follows that
auth users(Manager) ‚äÜauth users(AccountClerk),
auth users(Manager) ‚äÜauth users(Cashier),
and thus anyone authorized for the Manager role is also authorized for both the
AccountClerk and Cashier roles. Because the SSD constraint requires that
auth users(AccountClerk)‚à©auth users(Cashier) = /0,
it must also be the case that auth users(Manager) = /0, even though Manager does
not explicitly show up in the SSD relation.
‚ô¶
The next example elaborates on the possible interactions between role inheritance
and static separation of duty.

Role-Based Access Control
299
FIGURE 14.2 Role hierarchy for a CS/CE department
CS Fac
CE Fac UnTenured
Tenured
A
A
A
A



Chair
P&T VM
Fac
A
A
A
A
    H
H
H
H
H
H
H
H
P
P
P
P
P
P
P
P
P
P
P
Example 14.6
Consider a hypothetical academic department that houses both Computer Science
(CS) and Computer Engineering (CE) programs.
The department includes both
tenured and untenured faculty, and every faculty member is associated with at least
one of the two academic programs. In addition, the department has a chairperson
and a collection of voting members for a Promotion & Tenure (P& T) committee.
Thus, there are seven relevant roles for this example:
Roles = {Fac,Tenured,UnTenured,CS Fac,CE Fac,Chair,P&T VM}.
The role hierarchy appears in Figure 14.2: note that the roles Chair and P&T VM
both inherit the tenured-faculty role Tenured.
The standard academic situation is that no one can be both tenured and untenured,
and hence the roles Tenured and UnTenured should be mutually exclusive. Fur-
thermore, the department‚Äôs bylaws mandate that the department chair cannot be a
P&T voting member. These constraints can be represented by the following static
separation-of-duty relation:
SSD = {({Tenured,UnTenured},2),({P&T VM,Chair},2)}.
Note that these two constraints also prevent untenured faculty from being department
chair and from being voting members of the P&T committee, because the roles Chair
and P&T VM both inherit the Tenured role.
‚ô¶
14.2.2
Dynamic Separation of Duty
Although static separation of duty is easy to administer, in many cases it is not
Ô¨Çexible enough. There are many situations in which a person may need to be autho-
rized for different roles (e.g., parent and doctor) that nevertheless should never be

300
Access Control, Security, and Trust: A Logical Approach
activated at the same time due to conÔ¨Çict-of-interest concerns. Dynamic separation
of duty provides a way to manage such constraints.
As with static separation of duty, RBAC supports dynamic separation of duty
through a relation
DSD ‚äÜP(Roles)√ó(N‚àí{0,1}).
This relation imposes constraints on sessions and the roles activated therein. In par-
ticular, each element (rs,n) ‚ààDSD requires that no session can have n or more roles
from the set rs simultaneously activated. That is, for each (rs,n) ‚ààDSD, the follow-
ing condition must hold: for any session s, every subset rs‚Ä≤ of rs‚à©roles(s) contains
fewer than n elements.
The following example illustrates how the DSD relation can be used to impose
dynamic separation of duty.
Example 14.7
Recall the academic department from Example 14.6. As it turns out, the department‚Äôs
bylaws also require the P&T Committee to contain a Ô¨Åxed number of representatives
from each of the CS and CE programs. Thus, for the purposes of P&T deliberations,
no faculty member can simultaneously represent both the CS and CE programs, al-
though she may be associated with both programs. This constraint can be represented
by the following dynamic separation-of-duty relation:
DSD = {({CS Fac,CE Fac,P&T VM},3)}.
Thus, no one may simultaneously act as CS faculty, CE faculty, and a P&T voting
member, although they may authorized for all three roles and may act in any two of
those roles simultaneously.
‚ô¶
Because the deÔ¨Ånition of dynamic separation constrains activated roles instead of
authorized roles, it does not implicitly incorporate the role hierarchy in the same
way that static separation of duty does. The difference is that the RBAC notion of
sessions permits a role to be activated without the roles that it inherits being activated,
whereas the same does not hold of role authorization. As a result, a DSD relation may
allow a role to be activated, even if it inherits two roles that cannot be simultaneously
activated. The following example illustrates this important difference.
Example 14.8
Recall the RBAC descriptions for the small business in Example 14.4, along with the
role-inheritance relation ‚™∞introduced in Example 14.5:
Manager ‚™∞AccountClerk,
Manager ‚™∞Cashier.
Suppose the SSD relation from Example 14.4 is instead replaced by the corre-
sponding DSD relation:
DSD = {({AccountClerk,Cashier},2)}.

Role-Based Access Control
301
In this case, it is permissible for a session s to be deÔ¨Åned such that
roles(s) = {Manager},
because every subset of roles(s) ‚à©{AccountClerk,Cashier} has fewer than 2 ele-
ments (in fact, the only such subset is the empty set).
Note, however, that the role Manager still possesses all of the permissions of
AccountClerk and Cashier. As a result, there is still a potential conÔ¨Çict on per-
missions, even though there is no conÔ¨Çict on roles.
‚ô¶
As the previous example illustrates, both static and dynamic separation of duty are
deÔ¨Åned in terms of roles. When avoiding potential conÔ¨Çicts of interests, however, the
real concern lies in the sets of permissions that a given user possesses. Therefore,
when designing an RBAC system, it is important to verify that permissions assigned
to non-considered roles do not create loopholes.
Exercise 14.2.1
A small accounting Ô¨Årm has recently Ô¨Åred their security admin-
istrator for gross incompetence. They‚Äôre now looking to you to help them identify
some fundamental Ô¨Çaws‚Äîsuch as session or separation-of-duty violations‚Äîin the
access-control system he set up.
Identify all of the various RBAC violations and inconsistencies inherent in the
following set of RBAC deÔ¨Ånitions:
Users = {Lyn,Mike,Nell,Opal,Per}
Perms = {p0, p1, p2, p3, p4, p5, p6, p7, p8, p9}
Roles = {A,B,C,D,E,F,G,H,J}
UA = {(Lyn,D),(Lyn,B),(Mike,F),(Nell,D),(Nell,F),(Mike,J),
(Per,H),(Per,J),(Opal,A),(Opal,H)}
PA = {(p1,E),(p2,E),(p3,D),(p4,C),(p5,G),(p6,J),
(p7,A),(p8,B),(p9,F),(p0,H)}
SSD = { ({B,C},2), ({G,A,F},3), ({B,J},2) }
DSD = { ({H,C,J},3), ({F,C,J},2) }
‚™∞
given by :
E
A
A
A
A
A
   B
@
@
@
   F
H
A
C
@
@
@
D   G
J

302
Access Control, Security, and Trust: A Logical Approach
The system‚Äôs current implementation also allows the sessions s1, s2, and s3 as
follows:
user(s1) = Opal, roles(s1) = {B,D}
user(s2) = Nell, roles(s2) = {G,F}
user(s3) = Per,
roles(s3) = {J,H}
Exercise 14.2.2
A small engineering Ô¨Årm has decided to purchase a system to help
manage their R&D projects. Here are the key features the company wants the system
to handle (do not assume any permissions or restrictions other than those explicitly
mentioned):
‚Ä¢ Every project has some number of design engineers (DEs), test engineers
(TEs), and project managers (PMs) assigned to it.
‚Ä¢ A project‚Äôs DEs can build prototypes, and the TEs can write tests for the
project.
‚Ä¢ Project managers (PMs) can revise requirements at any time during the course
of their project.
‚Ä¢ No engineer (i.e., DE or TE) may be assigned to more than two projects.
‚Ä¢ No one can be both a DE and a TE on the same project. However, it is perfectly
acceptable for someone to be a DE for one project and a TE for a different
project.
‚Ä¢ Because of the need for focus, PMs are prohibited from working on other
projects; however, a PM is allowed to work as an engineer on the same project
for which (s)he is PM.
‚Ä¢ When project members log into the system, they should be prompted to indicate
which project they‚Äôll be working on during that session; they are not allowed
to work on more than one project during a single session.
As part of the system‚Äôs initial test run, the Ô¨Årm is going to focus on its three major
projects: ALPHA, BRAVO, and CHARLIE. Thus, they‚Äôve identiÔ¨Åed the following
roles and permissions for this system:
Roles = {PMA,PMB,PMC,DEA,DEB,DEC,TEA,TEB,TEC}
Perms = {buildA,buildB,buildC,testA,testB,testC,revA,revB,revC}
For example, buildA, testA, and revA are the permissions to (respectively) build a
prototype, write tests, and revise requirements for project ALPHA.
Your task:
Provide the following RBAC components to accurately meet and fulÔ¨Åll
all of the company‚Äôs desired features/criteria:
a. A role-hierarchy relation ‚™∞and a permission-assignment relation PA
You may add additional roles if you like.

Role-Based Access Control
303
b. A static separation-of-duty relation to capture static constraints
c. A dynamic separation-of-duty relation to capture dynamic constraints
Exercise 14.2.3
A local law Ô¨Årm wants to buy a system to help them keep track
of their legal cases. Listed below are the key features the practice wants the system
to handle (do not assume any permissions or restrictions other than those explicitly
mentioned):
‚Ä¢ The legal staff includes lawyers (both partners and associates) and paralegals.
The three groups are mutually disjoint.
‚Ä¢ All members of the legal staff can research cases.
‚Ä¢ Associates can depose witnesses.
‚Ä¢ All lawyers can present cases in court.
‚Ä¢ Partners can reject prospective clients.
‚Ä¢ The law Ô¨Årm handles both criminal defense and civil cases.
‚Ä¢ The law Ô¨Årm occasionally handles pro bono cases (i.e., cases taken on for the
public good for which no payment is required). Pro bono cases may be either
criminal defense or civil cases.
‚Ä¢ No partner may be assigned to more than one pro bono case.
‚Ä¢ No paralegal may be assigned to more than three cases.
‚Ä¢ Lawyers cannot double-bill their time to multiple cases: that is, at any moment
in time (i.e., session), lawyers can work on at most one case.
As part of the system‚Äôs initial test run, the Ô¨Årm is going to focus on its four major
cases:
‚Ä¢ The Anderson case is a pro bono, civil case.
‚Ä¢ The Brady case is a pro bono, criminal defense case.
‚Ä¢ The Cortland case is a civil case (but not pro bono).
‚Ä¢ The Derby case is a criminal defense case (but not pro bono).
To date, the following roles and permissions have been identiÔ¨Åed:
Roles = {Partner, Assoc, ParaLegal, ProBono, Civil, CrimDef, A, B, C, D}
Perms = {research, depose, present, reject }

304
Access Control, Security, and Trust: A Logical Approach
Your task:
Provide the following RBAC components to accurately meet and fulÔ¨Åll
all of the company‚Äôs desired features/criteria:
a. A role-hierarchy relation ‚™∞and a permission-assignment relation PA.
You may add additional roles if you like.
b. A static separation-of-duty relation to capture static constraints
c. A dynamic separation-of-duty relation to capture dynamic constraints
14.3
Representing RBAC Systems in the Logic
Given an RBAC description of a system, it is useful to be able to justify for-
mally the resulting access-control decisions that occur. Fortunately, it is straightfor-
ward to translate the relevant aspects of an RBAC description into our access-control
logic. In this section, we introduce small extensions to the logic‚Äîalong with a proce-
dure for translating RBAC descriptions into the logic‚Äîthat support reasoning about
RBAC systems.
14.3.1
RBAC Extensions to the Logic
We start by extending the syntax of the logic to accommodate statements that
express equality among principals:
Form ::= (Princ = Princ).
As with controls and reps , this new syntax is syntactic sugar for a construction that
already exists in the logic:
P = Q def
= P ‚áíQ ‚àßQ ‚áíP.
Consequently, its Kripke semantics is given as follows, for any Kripke structure M :
EM [[P = Q]] = EM [[P ‚áíQ ‚àßQ ‚áíP]]
= EM [[P ‚áíQ]] ‚à©EM [[Q ‚áíP]]
=
(
W,
if J(P) = J(Q)
/0,
otherwise.
Figure 14.3 presents three rules that are particularly useful for reasoning about
principal equality in the context of RBAC. The Ô¨Årst rule (Principal Equality) states
that, if P = Q, then any occurrences of P in a formula œï can safely be replaced by Q.
This rule is the principal-speciÔ¨Åc analogue to the Equivalence rule that allows one to
safely replace one formula in a larger expression by an equivalent one.

Role-Based Access Control
305
FIGURE 14.3 Logical rules regarding principal equality
Principal Equality
P = Q
œï[P/A]
œï[Q/A]
Distributivity of |
P | (R1 & ¬∑¬∑¬∑ & Rk) = (P | R1) & ¬∑¬∑¬∑ & (P | Rk) (k ‚â•1)
Quoting SimpliÔ¨Åcation
P | (Q & R) says œï
P | Q says œï
The second rule (Distributivity of | ) states that quoting ( | ) distributes over prin-
cipal conjunction ( & ). All three rules are sound, the proofs of which are left as
exercises; in fact, the third rule is derivable from existing rules. There are additional
rules related to principal equality that can be useful in general; because they are less
directly applicable to the discussion of RBAC, however, we leave them for exercises.
Exercise 14.3.1
Prove the soundness of the Principal Equality inference rule.
Exercise 14.3.2
Prove the soundness of the Distributivity of | inference rule.
Exercise 14.3.3
Give a formal proof for the derived inference rule Quoting Simpli-
Ô¨Åcation.
Exercise 14.3.4
Prove the soundness of the following inference rule:
Commutativity of &
P & Q = Q & P
Exercise 14.3.5
Prove the soundness of the following inference rule:
Associativity of &
P & (Q & R) = (P & Q) & R
Exercise 14.3.6
Prove the soundness of the following inference rule:
Associativity of |
P | (Q | R) = (P | Q) | R
14.3.2
Translating RBAC into the Logic
We are now ready to consider the procedure for translating RBAC descriptions into
the logic. In particular, we are interested in those aspects of RBAC that provide the
basis for determining whether or not an access-control decision should be granted:
roles and role inheritance, the permission and user assignments, and sessions. We
consider each of these items in turn.

306
Access Control, Security, and Trust: A Logical Approach
Roles
In the logic, roles are simply principals, and role inheritance is represented
by the speaks-for (‚áí) relation. SpeciÔ¨Åcally, we translate R2 ‚™∞R1 into the logical
statement R2 ‚áíR1.
Recall that an important aspect of role inheritance is that it is a partial order; the
logic must support reasoning about consequences of this property. Fortunately, the
speaks-for relation is well suited for this purpose: the Idempotency and Transitivity
of ‚áíinference rules account for role-inheritance reÔ¨Çexivity and transitivity, and the
deÔ¨Ånition of principal equality accurately accounts for the anti-symmetry of role
inheritance.
Permission and user assignments
Each element (p,R) in the permission assign-
ment reÔ¨Çects a statement that the permission p is associated with role R (i.e., that R
is authorized to perform p). Consequently, each such (p,R) ‚ààPA can be translated
into the logical statement
R controls p.
Furthermore, every authorized user of R (as determined by the combination of the
user assignment and role inheritance) is authorized to use role R to request to perform
p (i.e., an authorized user can be viewed as an ofÔ¨Åcial representative of the role
R). Consequently, the single element (p,R) also induces, for each user X in the set
auth users(R), the following statement:
X reps R on p.
Sessions
Every access request in an RBAC system occurs in the context of a par-
ticular session, which in turn has an associated set of roles activated. For example,
consider a particular session s, in which
user(s) = P,
roles(s) = {R1,¬∑¬∑¬∑ ,Rk}.
Any request œï made during session s can be expressed as follows:
P | (R1 & ¬∑¬∑¬∑ & Rk) says œï.
That is, P asserts the activated roles R1,...,Rk when making the request œï.
The following small example illustrates the translation procedure, along with the
resulting formal justiÔ¨Åcation to grant an RBAC request.
Example 14.9
A small academic department has set up the following RBAC deÔ¨Ånitions for an elec-
tronic system to support two users (Dora and Liu), two roles (department chair and
faculty member), and two permissions (the abilities to read student grades and to
assign a course instructor):
UA = {(Dora,Chair),(Liu,Faculty)},
PA = {(readGrades,Faculty),(assignInstructor,Chair)},
‚™∞= {(Chair,Chair),(Chair,Faculty),(Faculty,Faculty)}.

Role-Based Access Control
307
Furthermore, let sD be a login session in which Dora activates the single role Chair.
This RBAC system can be expressed in the logic as follows:
1. Role hierarchy
The role Chair inherits the role Faculty, which results in the following logical
expression:
Chair ‚áíFaculty.
Note that, technically speaking, one should also specify the following two
statements:
Chair ‚áíChair,
Faculty ‚áíFaculty.
However, these statements are both instances of the Idempotency of ‚áírule.
Hence, neither statement would ever have to appear as an assumption in any
formal analysis.
2. Permission and user assignments
The pair (readGrades,Faculty) ‚ààPA gives rise to the following collection of
statements:
Faculty controls readGrades,
Liu reps Faculty on readGrades,
Dora reps Faculty on readGrades.
Note that Dora ‚ààauth users(Faculty), even though the pair (Dora,Faculty)
does not explicitly appear in the user assignment UA.
Likewise, the pair (assignInstructor,Faculty) ‚ààPA gives rise to the following
collection of statements:
Chair controls assignInstructor,
Dora reps Chair on assignInstructor.
3. Sessions
Suppose Dora requests to read student grades as part of her session sD. This
request can be represented as follows:
Dora | Chair says readGrades.
Because the session sD has only one activated role (i.e., Chair), only one role
is quoted in the access request.
The proof in Figure 14.4 then provides a formal justiÔ¨Åcation for granting Dora‚Äôs
request to read student grades. Similar justiÔ¨Åcations are possible if one changes the
set of activated roles for session sD to either {Faculty} (resulting in a shorter proof)
or {Faculty,Chair} (resulting in a slightly longer proof).
‚ô¶

308
Access Control, Security, and Trust: A Logical Approach
FIGURE 14.4 Formal justiÔ¨Åcation to allow Dora to read student grades
1. Chair ‚áíFaculty
Chair ‚™∞Faculty
2. Faculty controls readGrades
(readGrades,Faculty) ‚ààPA
3. Dora reps Faculty on readGrades
(readGrades,Faculty) ‚ààPA,
Dora ‚ààauth users(Faculty)
4. Dora | Chair says readGrades
Request in session sD
5. Dora ‚áíDora
Idempotency of ‚áí
6. Dora | Chair ‚áíDora | Faculty
5,1 Monotonicity of |
7. Dora | Faculty says readGrades
6,4 Derived speaks for
8. readGrades
2,3,7 Reps
As a Ô¨Ånal note, we point out that the separation-of-duty relations SSD and DSD do
not enter explicitly into the translation from RBAC to the access-control logic. Both
relations impose constraints on an RBAC policy, limiting either the user assignment
(via SSD) or allowable sessions (via DSD). Those constraints must be checked when
the user assignment is created or modiÔ¨Åed, or when a new session is activated. In
contrast, the logic is used to formally justify an access-control decision that occurs
due to a speciÔ¨Åc request in the context of a particular session: such requests neces-
sarily occur after the point in time when separation-of-duty constraints are relevant.
Exercise 14.3.7
Consider the following collection of RBAC deÔ¨Ånitions:
Users = {Del, Earl, Fred, Guy, Hal}
Perms = {write loan, read balance, approve loan, sell loan, accept deposit,
cash check, close acct, open acct, void transaction, Ô¨Åre staff}
Roles = {Emp, Teller, AcctOfÔ¨Åcer, LoanOfÔ¨Åcer, MortgageOfÔ¨Åcer,
LoanSupervisor, TellerSupervisor, BranchManager}
UA = {(Del,Teller), (Earl,MortgageOfÔ¨Åcer), (Fred,TellerSupervisor),
(Fred,AcctOfÔ¨Åcer), (Guy,LoanSupervisor), (Hal,BranchManager)}
PA = {(read balance,Emp), (open acct,AcctOfÔ¨Åcer), (close acct,AcctOfÔ¨Åcer),
(cash check,Teller), (accept deposit,Teller), (write loan,LoanOfÔ¨Åcer),
(void transaction,TellerSupervisor), (sell loan,MortgageOfÔ¨Åcer),
(approve loan,LoanSupervisor), (Ô¨Åre staff,BranchManager)}
‚™∞= {(r,r),(BranchManager,r),(r,Emp) | r ‚ààR}
‚à™{(LoanSupervisor,MortgageOfÔ¨Åcer), (LoanSupervisor,LoanOfÔ¨Åcer)}
‚à™{(MortgageOfÔ¨Åcer,LoanOfÔ¨Åcer), (TellerSupervisor,Teller)}
Furthermore, let sH be a session such that
user(sH) = Hal,
roles(sH) = {Teller, MortgageOfÔ¨Åcer}.

Role-Based Access Control
309
a. Suppose you were to translate the entire RBAC description into the access-
control logic. List all of the translations that have the form
Fred reps X on p,
for some role X and some permission p.
b. Suppose that, during session sH, Hal requests to write loan.
Enumerate as formulas in the access-control logic precisely the portions of
the RBAC description necessary for determining that Hal‚Äôs request should be
granted; do not include anything that is unnecessary for the formal justiÔ¨Åca-
tion. (These formulas will serve as the only assumptions for your justiÔ¨Åcation
in part (c).)
c. Give a formal proof that justiÔ¨Åes granting Hal‚Äôs request.
Exercise 14.3.8
The company Industria has recently updated their entire comput-
ing infrastructure. These updates include the following:
‚Ä¢ A public-key infrastructure
Industria assigned public/private keypairs to all employees and servers. These
public keys are certiÔ¨Åed by Industria itself. In addition, Industria‚Äôs own public
key (KI) is installed directly on all company machines.
‚Ä¢ An RBAC-based project-reporting system
This system currently involves three projects (A, B, and C). Members of each
project are authorized to read and write that project‚Äôs report; only a project‚Äôs
manager(s) can ofÔ¨Åcially submit that project‚Äôs report. In addition, the com-
pany‚Äôs research director(s) can read the reports from any project. Thus, the
following RBAC components have been identiÔ¨Åed:
Roles = {MemberA,MemberB,MemberC,PMA,PMB,PMC,Director}
‚™∞= {(r,r) | r ‚ààR} ‚à™{(PMX,MemberX) | X ‚àà{A,B,C}}
Perms = {readX, writeX, submitX | X ‚àà{A,B,C}}
PA = {(readX,MemberX), (writeX,MemberX) | X ‚àà{A,B,C}}
‚à™{(submitX,PMX), (readX,Director) | X ‚àà{A,B,C}}
In the context of this new infrastructure, Cam has been assigned the public/private
keypair (Kc,K‚àí1
c ). Cam has also been assigned (in the UA) as both a project man-
ager for project B and as a research director for Industria.
Over the weekend, Cam decides to log in from home to catch up on work. There
are two phases that happen:
Phase 1 (Initiate Session) Cam sends a digitally signed message to the RBAC server,
requesting to initiate a session in which the roles PMB and Director are both
activated.

310
Access Control, Security, and Trust: A Logical Approach
For the purpose of this phase, let
‚ü®session,Cam,{PMB,Director}‚ü©
denote the proposition ‚Äúit is good to start a session with user Cam and acti-
vated roles {PMB,Director}.‚Äù
If this phase is successful, then the RBAC server will start up the requested
session, with Cam as the identiÔ¨Åed user and the requested roles active. Cam
can then proceed to the next phase.
Phase 2 (Do Work) In the context of the newly created session, Cam can now make
various requests of the system, including reading, writing, and submitting
project reports. These requests are interpreted as coming directly from Cam‚Äôs
session (as opposed to some cryptographic key).
Answer the following questions regarding the relevant certiÔ¨Åcations, credentials,
trust assumptions, and so on needed to analyze this situation. All answers should
be expressions in the access-control logic.
a. Give the collection of statements that capture the permission assignment PA
with respect to which roles are authorized to read the reports of project B.
b. Give the (minimal) collection of statements that capture the permission as-
signment and the implicit user assignment with respect to all of Cam‚Äôs various
role-based authorizations.
c. Suppose that, in Phase 1, the RBAC server recognizes Cam‚Äôs right to initiate
the requested session. Express this recognition of authority as an expression
in the access-control logic.
d. Express Cam‚Äôs Phase 1 request as an expression in the access-control logic.
e. What additional certiÔ¨Åcates, recognition of authority, and trust assumptions
regarding keys are necessary for the RBAC server to determine that the session-
initiation request of Phase 1 should be granted?
f. As part of Phase 2, Cam makes a request to write to project B‚Äôs report. Express
this request as an expression in the access-control logic.
g. What additional assumptions regarding policies, recognition of authority, and
the role hierarchy are necessary for the system to determine that Cam‚Äôs write
request in Phase 2 should be permitted?
14.4
Summary
In this chapter, we introduced role-based access control (RBAC), in which a user‚Äôs
access rights (i.e., permissions) are determined based on the roles for which that user

Role-Based Access Control
311
FIGURE 14.5 Learning outcomes for Chapter 14
After completing this chapter, you should be able to achieve the following learning
outcomes at several levels of knowledge:
Application
‚Ä¢ When given an RBAC description of a system, you should be able to
calculate the authorized users or authorized permissions of any role in
that system.
‚Ä¢ Express RBAC permission assignments, user authorizations, and role hi-
erarchies in the access-control logic.
Synthesis
‚Ä¢ When given an access-control scenario, you should be able to deÔ¨Åne a set
of roles, role hierarchy, user and permission assignments, and separation-
of-duty relations to accurately reÔ¨Çect the scenario.
‚Ä¢ When given an access-control scenario that involves RBAC, you should
be able to formalize the scenario, identify all necessary trust assump-
tions, and formally justify the granting of a request.
Evaluation
‚Ä¢ When given a set of RBAC deÔ¨Ånitions, you should be able to judge
whether they are consistent and (if not) identify the inconsistencies.
is authorized. RBAC was designed to reduce the administrative complexity asso-
ciated with large organizations‚Äô access-control needs: the roles for which a user is
authorized can be adjusted as that user acquires or sheds different job responsibilities,
and permissions associated with a particular role can also be adjusted as necessary.
The essential RBAC components include user assignments, permission assign-
ments, and the role-inheritance relation. RBAC also supports static and dynamic
separation-of-duty constraints, which can be used to limit potential sources of collu-
sion or conÔ¨Çicts of interest.
We also showed how the access-control logic can be used to reason about access
requests in RBAC systems. We introduced a notion of principal equality and then
deÔ¨Åned a procedure for translating RBAC descriptions into the logic. The resulting
translation speciÔ¨Åes access-control policies in terms of roles and speciÔ¨Åes users‚Äô role
authorizations in terms of delegation. The speaks-for relation is used to capture role
inheritance.
The learning outcomes associated with this chapter appear in Figure 14.5.

312
Access Control, Security, and Trust: A Logical Approach
14.5
Further Reading
RBAC was Ô¨Årst introduced by Ferraiolo and Kuhn (Ferraiolo and Kuhn, 1992),
and then expanded upon by Sandhu and colleagues (Sandhu et al., 1996). These
two frameworks were integrated and proposed as a NIST standard in 2001 (Ferraiolo
et al., 2001); the standard was adopted in 2004.
The access-control logic introduced by Lampson and colleagues (Lampson et al.,
1992; Abadi et al., 1993) included a notion of roles that differs signiÔ¨Åcantly from
the RBAC notion of roles. In the Lampson setting, roles are used to limit a princi-
pal‚Äôs privileges, whereas RBAC roles provide a mechanism for users to gain privi-
leges. Thumrongsak Kosiyatrakul was the Ô¨Årst to use a notion of delegation to cap-
ture RBAC role authorization in an access-control logic (Kosiyatrakul et al., 2005;
Kosiyatrakul, 2010). Due to his interpretation of delegation, his semantics is more
complicated than the version presented here.

Appendix A
Summary of the Access-Control
Logic
This appendix provides a summary of the syntax and inference rules of the access-
control logic. It is intended as a canonical reference for the syntax and rules (both
core and derived) introduced throughout the book.
A.1
Syntax
We deÔ¨Åne PName to be the collection of all simple principal names. The set Princ
of all principal expressions is given by the following BNF speciÔ¨Åcation:
Princ ::= PName / Princ & Princ / Princ | Princ
The convention for compound principals is that & binds more tightly than | .
We let PropVar be the collection of all propositional variables. The set Form of
all well-formed expressions is given by the following BNF speciÔ¨Åcation:
Form ::= PropVar / ¬¨ Form / (Form‚à®Form) /
(Form‚àßForm) / (Form ‚äÉForm) / (Form ‚â°Form) /
(Princ ‚áíPrinc) / (Princ says Form) / (Princ controls Form)
(Princ reps Princ on œï)
Parentheses can be omitted according to the following conventions for operator prece-
dence, in decreasing tightness of bindings:
¬¨
says
controls
reps
‚àß
‚à®
‚äÉ
‚â°
313

314
Access Control, Security, and Trust: A Logical Approach
The deÔ¨Ånition of Form deÔ¨Ånes the core syntax of the logic. Three extensions are
made to describe conÔ¨Ådentiality, integrity, and role-based access control policies.
ConÔ¨Ådentiality
We deÔ¨Åne SecLabel to be the collection of simple security labels,
which are used as names for the various levels associated with conÔ¨Ådentiality. In
addition to these speciÔ¨Åc security labels, we will often want to refer abstractly to
the security level assigned to a particular principal P. For this reason, we deÔ¨Åne the
larger set SecLevel of all possible security-level expressions:
SecLevel ::= SecLabel / slev(PName)
That is, a security-level expression is either a simple security label or an expression
of the form slev(A), where A is a simple principal name.1 Informally, slev(A)
refers to the security level of principal A.
Finally, we extend our deÔ¨Ånition of well-formed formulas to support comparisons
of security levels:
Form ::= SecLevel ‚â§s SecLevel / SecLevel =s SecLevel
Integrity
We deÔ¨Åne IntLabel to be the collection of simple integrity labels, and
we deÔ¨Åne IntLevel to be the set of all possible integrity-level expressions:
IntLevel ::= IntLabel / ilev(PName)
Informally, ilev(A) refers to the integrity‚Äîi.e., quality or trustworthiness‚Äîlevel of
principal A.
We then extend our deÔ¨Ånition of well-formed formulas to support comparisons of
security levels:
Form ::= IntLevel ‚â§i IntLevel / IntLevel =i IntLevel
The symbol ‚â§i denotes a partial ordering on integrity levels, in the same way that ‚â§s
denotes a partial ordering on security levels. In particular, ‚â§i is reÔ¨Çexive, transitive,
and antisymmetric.
Role-based access control
We extend the syntax of the logic to accommodate
statements that express equality among principals:
Form ::= (Princ = Princ)
1This syntax precludes security-level expressions such as slev(P & Q) or slev(P | Q), because there is
no standard technique for associating security classiÔ¨Åcation labels with compound principals.

Summary of the Access-Control Logic
315
A.2
Core Rules, Derived Rules, and Extensions
The following Ô¨Ågures summarize the core rules, derived rules, and extensions for
the access-control logic:
‚Ä¢ Figure A.1 is a summary of the core inference rules.
‚Ä¢ Figure A.2 is a summary of frequently used derived inference rules.
‚Ä¢ Figure A.3 is a summary of inference rules for delegation.
‚Ä¢ Figure A.4 is a summary of inference rules for relating security levels.
‚Ä¢ Figure A.5 is a summary of inference rules for relating integrity levels.
‚Ä¢ Figure A.6 is a summary of inference rules regarding principal equality.

316
Access Control, Security, and Trust: A Logical Approach
FIGURE A.1 Summary of core rules for the access-control logic
Taut
œï
if œï is an instance of a prop-logic tautology
Modus Ponens
œï
œï ‚äÉœï‚Ä≤
œï‚Ä≤
Says
œï
P says œï
MP Says
(P says (œï ‚äÉœï‚Ä≤)) ‚äÉ(P says œï ‚äÉP says œï‚Ä≤)
Speaks For
P ‚áíQ ‚äÉ(P says œï ‚äÉQ says œï)
& Says
(P & Q says œï) ‚â°((P says œï)‚àß(Q says œï))
Quoting
(P | Q says œï) ‚â°(P says Q says œï)
Idempotency of ‚áí
P ‚áíP
Transitivity
of ‚áí
P ‚áíQ
Q ‚áíR
P ‚áíR
Monotonicity
of ‚áí
P ‚áíP‚Ä≤
Q ‚áíQ‚Ä≤
P | Q ‚áíP‚Ä≤ | Q‚Ä≤
Equivalence
œï1 ‚â°œï2
œà[œï1/q]
œà[œï2/q]
P controls œï
def
=
(P says œï) ‚äÉœï

Summary of the Access-Control Logic
317
FIGURE A.2 Summary of useful derived rules
Conjunction
œï1
œï2
œï1 ‚àßœï2
SimpliÔ¨Åcation (1)
œï1 ‚àßœï2
œï1
SimpliÔ¨Åcation (2)
œï1 ‚àßœï2
œï2
Disjunction (1)
œï1
œï1 ‚à®œï2
Disjunction (2)
œï2
œï1 ‚à®œï2
Modus Tollens
œï1 ‚äÉœï2
¬¨œï2
¬¨œï1
Double negation
¬¨¬¨œï
œï
Disjunctive
Syllogism
œï1 ‚à®œï2
¬¨œï1
œï2
Hypothetical
Syllogism
œï1 ‚äÉœï2
œï2 ‚äÉœï3
œï1 ‚äÉœï3
Controls
P controls œï
P says œï
œï
Derived
Speaks For
P ‚áíQ
P says œï
Q says œï
Derived
Controls
P ‚áíQ
Q controls œï
P controls œï
Says
SimpliÔ¨Åcation (1)
P says (œï1 ‚àßœï2)
P says œï1
Says
SimpliÔ¨Åcation (2)
P says (œï1 ‚àßœï2)
P says œï2

318
Access Control, Security, and Trust: A Logical Approach
FIGURE A.3 Summary of rules for delegation
P reps Q on œï def
= P | Q says œï ‚äÉQ says œï
Reps
Q controls œï
P reps Q on œï
P | Q says œï
œï
Rep Controls
A reps B on œï ‚â°(A controls (B says œï))
Rep Says
A reps B on œï
A|B says œï
B says œï
FIGURE A.4 Inference rules for relating security levels
‚Ñì1 =s ‚Ñì2
def
= (‚Ñì1 ‚â§s ‚Ñì2)‚àß(‚Ñì2 ‚â§s ‚Ñì1)
ReÔ¨Çexivity of ‚â§s
‚Ñì‚â§s ‚Ñì
Transitivity of ‚â§s
‚Ñì1 ‚â§s ‚Ñì2
‚Ñì2 ‚â§s ‚Ñì3
‚Ñì1 ‚â§s ‚Ñì3
FIGURE A.5 Inference rules for relating integrity levels
‚Ñì1 =i ‚Ñì2
def
= (‚Ñì1 ‚â§i ‚Ñì2)‚àß(‚Ñì2 ‚â§i ‚Ñì1)
ReÔ¨Çexivity of ‚â§i
‚Ñì‚â§i ‚Ñì
Transitivity of ‚â§i
‚Ñì1 ‚â§i ‚Ñì2
‚Ñì2 ‚â§i ‚Ñì3
‚Ñì1 ‚â§i ‚Ñì3

Summary of the Access-Control Logic
319
FIGURE A.6 Logical rules regarding principal equality
P = Q def
= P ‚áíQ ‚àßQ ‚áíP
Principal Equality
P = Q
œï[P/A]
œï[Q/A]
Distributivity of |
P | (R1 & ¬∑¬∑¬∑ & Rk) = (P | R1) & ¬∑¬∑¬∑ & (P | Rk) (k ‚â•1)
Quoting SimpliÔ¨Åcation
P | (Q & R) says œï
P | Q says œï


Bibliography
Abadi, M., Burrows, M., Lampson, B., and Plotkin, G. (1993). A calculus for access
control in distributed systems. ACM Transactions on Programming Languages
and Systems, 15(4):706‚Äì734.
Bell, D. E. and La Padula, L. J. (1973). Secure computer systems: Mathematical
foundations. Technical Report Technical Report MTR-2547, Vol. I, MITRE
Corporation, Bedford, MA.
Bell, D. E. and La Padula, L. J. (1975). Secure computer system: UniÔ¨Åed exposi-
tion and Multics interpretation. Technical Report MTR-2997 Rev. 1, MITRE
Corporation, Bedford, MA.
Bensoussan, A., Clingen, C. T., and Daley, R. C. (1972). The Multics virtual mem-
ory: Concepts and design. Communications of the ACM, 15(5):308‚Äì318.
Biba, K. (1975). Integrity considerations for secure computer systems. Technical
Report MTR-3153, MITRE Corporation, Bedford, MA.
Bishop, M. (2003). Computer Security: Art and Science. Addison-Wesley Profes-
sional.
Brewer, D. F. and Nash, M. J. (1989). The Chinese wall security policy. In Proceed-
ings of the 1989 IEEE Symposium on Security and Privacy, pages pp. 206‚Äì214.
Bryant, B. (1988). Designing an authentication system: a dialogue in four scenes.
(Afterword by Theodore Ts‚Äôo, 1997).
Comer, D. E. (2005). Essentials of Computer Architecture. Prentice Hall, New York.
Daley, R. C. and Dennis, J. B. (1968). Virtual memory, processes, and sharing in
MULTICS. Communications of the ACM, 11(5):306‚Äì312.
Dierks, T. and Rescorla, E. (2006).
The transport layer security (TLS) protocol
version 1.1. RFC 4346 (Proposed Standard).
Ferraiolo, D. and Kuhn, R. (1992). Role-Based Access Control. In 15th NIST-NCSC
National Computer Security Conference, pages 554‚Äì563, Gaithersburg, MD.
Ferraiolo, D. F., Sandhu, R. S., Gavrila, S. I., Kuhn, D. R., and Chandramouli, R.
(2001). Proposed NIST Standard for Role-Based Access Control. ACM Trans-
action on Information and System Security, 4(3):224‚Äì274.
321

322
Access Control, Security, and Trust: A Logical Approach
FFIEC (2004). Retail payment systems booklet: IT examination handbook. Avail-
able at http://www.fÔ¨Åec.gov/ under IT Booklets on the FFIEC IT Handbook
InfoBase web page.
Freier, A., Karlton, P., and Kocher, P. C. (1996). The SSL protocol version 3.0. IETF
Internet Draft, Transport Layer Security Working Group.
Hughes, G. and Cresswell, M. (1996). A New Introduction to Modal Logic. Rout-
ledge, New York.
Kosiyatrakul, T. (2010). A Modal Logic for Role-Based Access Control within the
Higher-Order Logic (HOL) Theorem Prover. PhD thesis, Syracuse University.
Kosiyatrakul, T., Older, S., and Chin, S.-K. (2005). A modal logic for role-based
access control. In Gorodetsky, V., Kotenko, I. V., and Skormin, V. A., editors,
MMM-ACNS, volume 3685 of Lecture Notes in Computer Science, pages 179‚Äì
193. Springer.
Lampson, B. (1971). Protection. In Proceedings of the 5th Princeton Conference on
Information Sciences and Systems.
Lampson, B., Abadi, M., Burrows, M., and Wobber, E. (1992). Authentication in
distributed systems: Theory and practice. ACM Transactions on Computer Sys-
tems, 10(4):265‚Äì310.
Levy, H. M. (1984). Capability-Based Computer Systems. Digital Press Inc., Day-
tona Beach, FL.
Lipner, S. B. (1982). Non-discretionary controls for commercial applications. In
Proceedings of the 1982 IEEE Symposium on Privacy and Security, pages pp.
2‚Äì10.
Menezes, A. J., van Oorschot, P. C., and Vanstone, S. A. (1997).
Handbook of
Applied Cryptography. CRC Press, Boca Raton, FL.
National Automated Clearing House Association (2006). 2006 ACH Rules: A Com-
plete Guide to Rules and Regulations Governing the ACH Network.
13665
Dulles Technology Drive, Suite 300, Herndon, VA 20171.
National Institute of Standards and Technology (2001). Advanced encryption stan-
dard. FIPS Publication 197.
Neuman, C., Yu, T., Hartman, S., and Raeburn, K. (2005). The Kerberos Network
Authentication Service (V5).
RFC 4120 (Proposed Standard).
Updated by
RFCs 4537, 5021.
Organick, E. (1972). The Multics System: An Examination of Its Structure. MIT
Press, Cambridge, MA.

Popek, G. J. and Goldberg, R. P. (1974). Formal requirements for virtualizable third
generation architectures. Communications of the ACM, 17(7):412‚Äì421.
Rivest, R. L., Shamir, A., and Adelman, L. (1978). A method for obtaining dig-
ital signatures and public-key cryptosystems. Communications of the ACM,
21:120‚Äì126.
Rosen, K. H. (2003).
Discrete Mathematics and its Applications, 5th edition.
McGraw-Hill, New York.
Ross, K. A. and Wright, C. R. (2002). Discrete Mathematics, 5th edition. Prentice
Hall, New York.
Saltzer, J. and Schroeder, M. (1975). The Protection of Information in Computer
Systems. Proceedings IEEE.
Sandhu, R. S., Coyne, E. J., Feinstein, H. L., and Youman, C. E. (1996). Role-based
access control models. IEEE Computer, 29(2):38‚Äì47.
Schroeder, M. D. and Saltzer, J. H. (1972). A hardware architecture for implementing
protection rings. Communications of the ACM, 15(3):157‚Äì170.
Stallings, W. (2003). Cryptography and Network Security Principles and Practices,
3rd edition. Prentice Hall, New York.
Wakerly, J. F. (2006). Digital Design: Principles and Practices. Prentice Hall, New
York.

Notation Index
‚àí: set difference, 12
<< m1,m2,...,mk >>: concatenation, 127
R(a): image of R under a, 12
‚à©: set intersection, 12
‚ó¶: relational composition, 12
‚à™: set union, 12
/0: empty set, 12
‚áí: speaks for, 20
‚âàx: equivalence modulo x, 222
P(S): power set, 12
idA: identity relation on set A, 12
{}: empty set, 12
‚â°: equivalence, 20
‚àà: set membership, 12
|: quoting, 18
|=: satisfaction, 28, 33
¬¨: negation, 20
Ã∏‚àà: set non-membership, 12
Ã∏|=: nonsatisfaction of model, 28
‚äÜ: subset, 12
‚™∞: role-inheritance relation, 290
‚äÉ: implication, 20
√ó: Cartesian product, 12
‚à®: disjunction, 20
‚àß: conjunction, 20
=: principal equality, 304
œà[œï/q]: substitution, 45



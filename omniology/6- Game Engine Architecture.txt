Game Engine
Architecture 
S E C O N D  E D I T I O N
J a s o n  G r e g o r y  
F O R E W O R D  B Y Richard Lemarchand

Game Engine
Architecture 
S E C O N D  E D I T I O N

This page intentionally left blank
This page intentionally left blank

CRC Press is an imprint of the
Taylor & Francis Group, an informa business
Boca Raton   London   New York
Game Engine
Architecture 
S E C O N D  E D I T I O N
J a s o n  G r e g o r y
 L e a d  P r o g r a m m e r,  N a u g h t y  D o g  I n c .
F O R E W O R D  B Y Richard Lemarchand

CRC Press
Taylor & Francis Group
6000 Broken Sound Parkway NW, Suite 300
Boca Raton, FL 33487-2742
© 2015 by Taylor & Francis Group, LLC
CRC Press is an imprint of Taylor & Francis Group, an Informa business
No claim to original U.S. Government works
Version Date: 20140624
International Standard Book Number-13: 978-1-4665-6006-2 (eBook - PDF)
This book contains information obtained from authentic and highly regarded sources. Reasonable efforts have been made to publish reliable data and information, but 
the author and publisher cannot assume responsibility for the validity of all materials or the consequences of their use. The authors and publishers have attempted to 
trace the copyright holders of all material reproduced in this publication and apologize to copyright holders if permission to publish in this form has not been obtained. 
If any copyright material has not been acknowledged please write and let us know so we may rectify in any future reprint.
Except as permitted under U.S. Copyright Law, no part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, mechanical, 
or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information storage or retrieval system, without 
written permission from the publishers.
For permission to photocopy or use material electronically from this work, please access www.copyright.com (http://www.copyright.com/) or contact the Copyright 
Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, MA 01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a 
variety of users. For organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.
Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for identification and explanation without intent to 
infringe.
Visit the Taylor & Francis Web site at
http://www.taylorandfrancis.com
and the CRC Press Web site at
http://www.crcpress.com

Dedicated to
Trina, Evan and Quinn Gregory,
in memory of our heroes,
Joyce Osterhus, Kenneth Gregory and Erica Gregory.

This page intentionally left blank
This page intentionally left blank

Contents
Foreword to the First Edition
xiii
Foreword to the Second Edition
xvii
Preface to the First Edition
xxi
Preface to the Second Edition
xxiii
Acknowledgements
xxiv
I
Foundations
1
1
Introduction
3
1.1
Structure of a Typical Game Team
5
1.2
What Is a Game?
8
1.3
What Is a Game Engine?
11
1.4
Engine Differences Across Genres
13
1.5
Game Engine Survey
26
vii

viii
CONTENTS
1.6
Runtime Engine Architecture
32
1.7
Tools and the Asset Pipeline
54
2 Tools of the Trade
63
2.1
Version Control
63
2.2
Microsoft Visual Studio
73
2.3
Proﬁling Tools
91
2.4
Memory Leak and Corruption Detection
93
2.5
Other Tools
94
3 Fundamentals of Software Engineering for Games
97
3.1
C++ Review and Best Practices
97
3.2
Data, Code and Memory
112
3.3
Catching and Handling Errors
144
3.4
Pipelines, Caches and Optimization
152
4 3D Math for Games
165
4.1
Solving 3D Problems in 2D
165
4.2
Points and Vectors
166
4.3
Matrices
181
4.4
Quaternions
200
4.5
Comparison of Rotational Representations
209
4.6
Other Useful Mathematical Objects
213
4.7
Hardware-Accelerated SIMD Math
218
4.8
Random Number Generation
227
II
Low-Level Engine Systems
229
5 Engine Support Systems
231
5.1
Subsystem Start-Up and Shut-Down
231
5.2
Memory Management
239
5.3
Containers
254
5.4
Strings
274
5.5
Engine Conﬁguration
290

CONTENTS
ix
6 Resources and the File System
297
6.1
File System
298
6.2
The Resource Manager
308
7 The Game Loop and Real-Time Simulation
339
7.1
The Rendering Loop
339
7.2
The Game Loop
340
7.3
Game Loop Architectural Styles
343
7.4
Abstract Timelines
346
7.5
Measuring and Dealing with Time
348
7.6
Multiprocessor Game Loops
361
7.7
Networked Multiplayer Game Loops
375
8 Human Interface Devices (HID)
381
8.1
Types of Human Interface Devices
381
8.2
Interfacing with a HID
383
8.3
Types of Inputs
385
8.4
Types of Outputs
391
8.5
Game Engine HID Systems
392
8.6
Human Interface Devices in Practice
409
9 Tools for Debugging and Development
411
9.1
Logging and Tracing
411
9.2
Debug Drawing Facilities
416
9.3
In-Game Menus
423
9.4
In-Game Console
426
9.5
Debug Cameras and Pausing the Game
427
9.6
Cheats
427
9.7
Screenshots and Movie Capture
428
9.8
In-Game Proﬁling
429
9.9
In-Game Memory Stats and Leak Detection
436
III
Graphics, Motion and Sound
441
10 The Rendering Engine
443

x
CONTENTS
10.1
Foundations of Depth-Buffered Triangle Rasterization
444
10.2
The Rendering Pipeline
489
10.3
Advanced Lighting and Global Illumination
519
10.4
Visual Effects and Overlays
532
10.5
Further Reading
541
11 Animation Systems
543
11.1
Types of Character Animation
543
11.2
Skeletons
548
11.3
Poses
551
11.4
Clips
556
11.5
Skinning and Matrix Palette Generation
570
11.6
Animation Blending
575
11.7
Post-Processing
594
11.8
Compression Techniques
597
11.9
Animation System Architecture
604
11.10
The Animation Pipeline
605
11.11
Action State Machines
621
11.12
Animation Controllers
646
12 Collision and Rigid Body Dynamics
647
12.1
Do You Want Physics in Your Game?
648
12.2
Collision/Physics Middleware
653
12.3
The Collision Detection System
655
12.4
Rigid Body Dynamics
684
12.5
Integrating a Physics Engine into Your Game
722
12.6
Advanced Physics Features
740
13 Audio
743
13.1
The Physics of Sound
744
13.2
The Mathematics of Sound
756
13.3
The Technology of Sound
774
13.4
Rendering Audio in 3D
786
13.5
Audio Engine Architecture
806
13.6
Game-Speciﬁc Audio Features
828

CONTENTS
xi
IV
Gameplay
845
14 Introduction to Gameplay Systems
847
14.1
Anatomy of a Game World
848
14.2
Implementing Dynamic Elements: Game Objects
853
14.3
Data-Driven Game Engines
856
14.4
The Game World Editor
857
15 Runtime Gameplay Foundation Systems
869
15.1
Components of the Gameplay Foundation System
869
15.2
Runtime Object Model Architectures
873
15.3
World Chunk Data Formats
892
15.4
Loading and Streaming Game Worlds
899
15.5
Object References and World Queries
909
15.6
Updating Game Objects in Real Time
916
15.7
Events and Message-Passing
933
15.8
Scripting
954
15.9
High-Level Game Flow
978
V
Conclusion
979
16 You Mean There’s More?
981
16.1
Some Engine Systems We Didn’t Cover
981
16.2
Gameplay Systems
982
Bibliography
987
Index
991

This page intentionally left blank
This page intentionally left blank

Foreword to the First Edition
T
he very ﬁrst video game was built entirely out of hardware, but rapid ad-
vancements in microprocessors have changed all that. These days, video
games are played on versatile PCs and specialized video game consoles that
use soft ware to make it possible to off er a tremendous variety of gaming
experiences. It’s been 50 years since those ﬁrst primitive games, but the in-
dustry is still considered by many to be immature. It may be young, but when
you take a closer look, you will ﬁnd that things have been developing rapidly.
Video games are now a multi-billion-dollar industry covering a wide range of
demographics.
Video games come in all shapes and sizes, falling into categories or
“genres” covering everything from solitaire to massively multiplayer online
role-playing games, and these games are played on virtually anything with a
microchip in it. These days, you can get games for your PC, your cell phone,
as well as a number of diff erent specialized gaming consoles—both handheld
and those that connect to your home TV. These specialized home consoles
tend to represent the cutt ing edge of gaming technology, and the pattern of
these platforms being released in cycles has come to be called console “gen-
erations.” The powerhouses of this latest generation are Microsoft’s Xbox 360
and Sony’s PlayStation 3, but the ever-present PC should never be overlooked,
and the extremely popular Nintendo Wii represents something new this time
around.
xiii

xiv
Foreword to the First Edition
The recent explosion of downloadable and casual games has added even
more complexity to the diverse world of commercial video games. Even so,
big games are still big business. The incredible computing power available
on today’s complicated platforms has made room for increased complexity in
the software. Naturally, all this advanced soft ware has to be created by some-
one, and that has driven up the size of development teams—not to mention
development costs. As the industry matures, we’re always looking for better,
more efﬁcient ways to build our products, and development teams have be-
gun compensating for the increased complexity by taking advantage of things
like reusable software and middleware.
With so many different styles of game on such a wide array of platforms,
there cannot be any single ideal software solution. However, certain patterns
have developed, and there is a vast menu of potential solutions out there. The
problem today is choosing the right solution to ﬁt the needs of the particular
project. Going deeper, a development team must consider all the different
aspects of a project and how they ﬁt together. It is rare to ﬁnd any one software
package that perfectly suits every aspect of a new game design.
Those of us who are now veterans of the industry found ourselves pio-
neering unknown territory. Few programmers of our generation have Com-
puter Science degrees (Matt’s is in Aeronautical Engineering, and Jason’s is
in Systems Design Engineering), but these days many colleges are starting to
programs and degrees in video games. The students and developers of today
need a good place to turn to for solid game development information. For
pure high-end graphics, there are a lot of sources of very good information
from research to practical jewels of knowledge. However, these sources are
often not directly applicable to production game environments or suffer from
not having actual production-quality implementations. For the rest of game
development, there are so-called beginner books that so gloss over the details
and act as if they invented everything without giving references that they are
just not useful or often even accurate. Then there are high-end specialty books
for various niches like physics, collision, AI, etc. But these can be needlessly
obtuse or too high level to be understood by all, or the piecemeal approach just
doesn’t all ﬁt together. Many are even so directly tied to a particular piece of
technology as to become rapidly dated as the hardware and software change.
Then there is the Internet, which is an excellent supplementary tool for
knowledge gathering. However, broken links, widely inaccurate data and
variable-to-poor quality often make it not useful at all unless you know ex-
actly what you are after.
Enter Jason Gregory, himself an industry veteran with experience at
Naughty Dog—one of the most highly regarded video game studios in the

Foreword to the First Edition
xv
world. While teaching a course in game programming at USC, Jason found
himself facing a shortage of textbooks covering the fundamentals of video-
game architecture. Luckily for the rest of us, he has taken it upon himself to
ﬁll that gap.
What Jason has done is pull together production-quality knowledge ac-
tually used in shipped game projects and bring together the entire game-
development picture. His experience has allowed him to bring together not
only the ideas and techniques but also actual code samples and implementa-
tion examples to show you how the pieces come together to actually make a
game. The references and citations make it a great jumping-off point to dig
deeper into any particular aspect of the process. The concepts and techniques
are the actual ones we use to create games, and while the examples are often
grounded in a technology, they extend way beyond any particular engine or
API.
This is the kind of book we wanted when we were getting started, and we
think it will prove very instructive to people just starting out as well as those
with experience who would like some exposure to the larger context.
Jeff Lander
Matthew Whiting

This page intentionally left blank
This page intentionally left blank

Foreword to the Second Edition
G
ames and computing are deeply intertwined. From the advent of the
ﬁrst digital computer game, Spacewar, in 1962, to the state-of-the-art
gaming systems of the present day, the procedural aspects of games dove-
tail perfectly with the logical and mathematical nature of computers. Digi-
tal games beckon us toward a future world where systems thinking and the
foundational literacies of interaction and programming promise a new era of
human invention, discovery and imagination. This future is a complex one—
we’re all going to need good guidebooks.
Let’s cut to the chase: In my opinion, this book is the best of its kind,
and you’re lucky to have found it. It covers the huge ﬁeld of game engine
architecture in a succinct, clear way, and expertly balances the breadth and the
depth of its coverage, offering enough detail that even a beginner can easily
understand the concepts it presents. The author, Jason Gregory, is not only a
world expert in his ﬁeld; he’s a working programmer with production-quality
knowledge and many shipped game projects under his belt. He works among
the game engineers of Naughty Dog, one of the foremost game studios in the
world, on what are widely regarded to be some of the best videogames ever
made. To cap things off, Jason is also an experienced educator, who has taught
in the top-ranked university game program in North America.
xvii

xviii
Foreword to the Second Edition
Why should you take my word for the fact that you’re looking at a rare
gem of a book, and one that will become an essential part of your game de-
velopment reference library? Let me do my best to give you some conﬁdence
in my claims.
I’ve worked as a professional game designer for all my adult life. For much
of that time I worked as a Lead Game Designer at Naughty Dog, the Sony-
owned studio that created the Crash Bandicoot and Jak and Daxter series of
games. It was at Naughty Dog that I ﬁrst met Jason Gregory, who I’ve known
for a long time now, and it was there that he and I were honored to participate
in the creation of all three games in the hugely critically and commercially
successful Uncharted series. Jason would go on to work on The Last of Us, the
next of Naughty Dog’s enormously successful storytelling action games.
I got my start as a game designer at MicroProse in the UK, and before
joining Naughty Dog I worked at Crystal Dynamics where I helped to create
game series like Gex and Legacy of Kain: Soul Reaver. I learned an immense
amount during the eight amazing years that I worked at Naughty Dog, and I
have now joined the faculty of the Interactive Media and Games Division of
the School of Cinematic Arts at the University of Southern California, where
I teach in the USC Games program and design games as part of USC’s Game
Innovation Lab. The bonds between USC and Naughty Dog are strong; Jason
has also taught programming in the USC Games program, as part of USC’s
Viterbi School of Engineering.
When I ﬁrst met Jason, he was newly arriving at Naughty Dog from our
neighbors at Electronic Arts, where he had done great work on the highly
technical and artistically driven ﬁeld of game animation, among other things.
We were able to work together almost immediately. Along with the many
other complex tasks he took on, Jason helped to develop the scripting lan-
guage and proprietary authoring environment that my fellow game designers
and I would use to tie together elements of art, animation, audio, visual effects
and code into the set pieces that wowed our audience of Uncharted players.
This means that I have ﬁrst-hand experience of how Jason can take complex
concepts and make them clear. The tools that he helped develop are the best
I’ve ever used, and I know from our other work together that he brings this
same technical horsepower and clarity of communication to every one of the
many game systems he has worked on in the course of his professional life, as
well as to this book.
Contemporary videogame development is a big subject. From design to
development, from triple-A to indie hit, from rendering to collision to tools
programming, there’s a lot to say about the interlocking sets of systems and
skills that go into making a game. The game-making tools that we now have

Foreword to the Second Edition
xix
at our disposal are unparalleled in their power and complexity, and the many
detailed code samples and implementation examples in this book will help
you understand just how the pieces come together in a great game. By helping
you in this way, Jason’s book might just empower you to outstrip even the
most audacious dreams of history’s best game designers and developers.
This book is a survey, but not just of the surface; it also digs deeply enough
into each subject to give us a chance to understand everything it covers. In the
colorful language of my friend Ian Dallas, creative director at Giant Sparrow,
creators of The Unﬁnished Swan, and a person who swears by this book: It
gives us a chance “to eat a piece of the elephant”—to start to wrap our heads
around the “big picture” of a giant subject that could otherwise seem too vast
to begin to understand.
This is a great time to be approaching game software engineering. Schools
all around the world are offering high-quality programs staffed by experi-
enced game creators who bring a wealth of technical and artistic skill to bear
on their subject. The incredible renaissance that is taking place in games, due
in part to the inﬂuence of independent games and art games, is opening up
our world to new voices and new perspectives, all the while strengthening
the very healthy and innovative mainstream of computer, console and mobile
game development.
Our ﬁeld is only going to become more interesting, more culturally im-
portant, in terms of entertainment, art and business, and more innovative, as
we head into the many exciting, uncharted futures of digital gaming. You
couldn’t ask for a better springboard than this book, or for a better, wiser
guide than Jason Gregory, as you begin what I hope will be a lifelong process
of learning about the fascinating world of game development.
Richard Lemarchand
14th November, 2013

This page intentionally left blank
This page intentionally left blank

Preface to the First Edition
W
elcome to Game Engine Architecture. This book aims to present a com-
plete discussion of the major components that make up a typical com-
mercial game engine. Game programming is an immense topic, so we have a
lot of ground to cover. Nevertheless, I trust you’ll ﬁnd that the depth of our
discussions is sufﬁcient to give you a solid understanding of both the theory
and the common practices employed within each of the engineering disci-
plines we’ll cover. That said, this book is really just the beginning of a fasci-
nating and potentially lifelong journey. A wealth of information is available
on all aspects of game technology, and this text serves both as a foundation-
laying device and as a jumping-off point for further learning.
Our focus in this book will be on game engine technologies and architec-
ture. This means we’ll cover both the theory underlying the various subsys-
tems that comprise a commercial game engine and also the data structures,
algorithms and software interfaces that are typically used to implement them.
The line between the game engine and the game is rather blurry. We’ll fo-
cus primarily on the engine itself, including a host of low-level foundation
systems, the rendering engine, the collision system, the physics simulation,
character animation and an in-depth discussion of what I call the gameplay
foundation layer. This layer includes the game’s object model, world editor,
event system and scripting system. We’ll also touch on some aspects of game-
play programming, including player mechanics, cameras and AI. However,
xxi

xxii
Preface to the First Edition
by necessity, the scope of these discussions will be limited mainly to the ways
in which gameplay systems interface with the engine.
This book is intended to be used as a course text for a two- or three-course
college-level series in intermediate game programming. Of course, it can also
be used by amateur software engineers, hobbyists, self-taught game program-
mers and existing members of the game industry alike. Junior engineers can
use this text to solidify their understanding of game mathematics, engine ar-
chitecture and game technology. And some senior engineers who have de-
voted their careers to one particular specialty may beneﬁt from the bigger
picture presented in these pages as well.
To get the most out of this book, you should have a working knowledge
of basic object-oriented programming concepts and at least some experience
programming in C++. Although a host of new and exciting languages are be-
ginning to take hold within the game industry, industrial-strength 3D game
engines are still written primarily in C or C++, and any serious game pro-
grammer needs to know C++. We’ll review the basic tenets of object-oriented
programming in Chapter 3, and you will no doubt pick up a few new C++
tricks as you read this book, but a solid foundation in the C++ language is
best obtained from [41], [31] and [32]. If your C++ is a bit rusty, I recommend
you refer to these or similar books to refresh your knowledge as you read this
text. If you have no prior C++ experience, you may want to consider read-
ing at least the ﬁrst few chapters of [41] and/or working through a few C++
tutorials online, before diving into this book.
The best way to learn computer programming of any kind is to actually
write some code. As you read through this book, I strongly encourage you
to select a few topic areas that are of particular interest to you and come up
with some projects for yourself in those areas. For example, if you ﬁnd char-
acter animation interesting, you could start by installing OGRE and explor-
ing its skinned animation demo. Then you could try to implement some of
the animation blending techniques described in this book, using OGRE. Next
you might decide to implement a simple joypad-controlled animated charac-
ter that can run around on a ﬂat plane. Once you have something relatively
simple working, expand upon it! Then move on to another area of game tech-
nology. Rinse and repeat. It doesn’t particularly matter what the projects are,
as long as you’re practicing the art of game programming, not just reading
about it.
Game technology is a living, breathing thing that can never be entirely
captured within the pages of a book. As such, additional resources, errata, up-
dates, sample code and project ideas will be posted from time to time on this
book’s website at http://www.gameenginebook.com and on the book’s blog
at http://gameenginebook.blogspot.com. You can also follow me on Twitter
@jqgregory.

Preface to the Second Edition
I
n this, the second edition of Game Engine Architecture, my goal was three-
fold. First, I wanted to update the book to include information on some
new and exciting topics, including that latest variant of the C++ program-
ming language, C++11, and the architecture of the eighth generation of gam-
ing consoles—the Xbox One and the PlayStation 4.
Second, I wanted to ﬁll in some gaps in the content of the original book.
Most notably, I decided to include a brand new chapter on audio technology.
This decision was based in part on requests from you, my loyal and always
helpful readers. It was also based in part on the fact that, to my knowledge,
no book currently exists that covers the fundamentals of the physics, mathe-
matics and technology that go into the creation of a AAA game audio engine.
Audio plays a crucially important role in any great game, and it is my sincere
hope that the audio chapter in this book will help at least a little to open up
the ﬁeld of game audio technology to a wider audience.
Third, I wanted to repair the various errata that were brought to my atten-
tion by my readers. Thank you! I hope you’ll ﬁnd that the mistakes you found
have all been ﬁxed—and replaced by a whole new batch of mistakes that you
can tell me about for the third edition!
Of course, as I’ve said before, the ﬁeld of game engine programming is
almost unimaginably broad and deep. There’s no way to cover every topic
xxiii

xxiv
Preface to the Second Edition
in one book. As such, the primary purpose of this book remains to serve as
an awareness-building tool and a jumping-off point for further learning. I
hope you ﬁnd this edition helpful on your journey through the fascinating
and multifaceted landscape of game engine architecture.
Acknowledgements
No book is created in a vacuum, and this one is certainly no exception. This
book—and its second edition, which you hold in your hands now—would
not have been possible without the help of my family, friends and colleagues
in the game industry, and I’d like to extend warm thanks to everyone who
helped me to bring this project to fruition.
Of course, the ones most impacted by a project like this are invariably the
author’s family. So I’d like to start by offering for a second time a special thank-
you to my wife Trina. She was a pillar of strength during the writing of the
original book, and this time around she was as supportive and invaluably
helpful as ever. While I’m busy tapping away on my keyboard, Trina is always
there to take care of our two boys, Evan (now age 10) and Quinn (age 7), day
after day and night after night, often forgoing her own plans, doing my chores
as well as her own (more often than I’d like to admit), and always giving me
kind words of encouragement when I needed them the most. I’d also like to
thank my sons, Evan and Quinn, for being patient with me, especially when
my writing schedule interfered with their burning desires to download the
latest Minecraft mod or Gmod add-on, and for offering me unconditional love
and affection despite their quite understandable frustration with my lack of
availability.
I would also like to extend special thanks to my editors for the ﬁrst edition,
Matt Whiting and Jeff Lander. Their insightful, targeted and timely feedback
was always right on the money, and their vast experience in the game industry
helped to give me conﬁdence that the information presented in these pages
is as accurate and up-to-date as humanly possible. Matt and Jeff were both
a pleasure to work with, and I am honored to have had the opportunity to
collaborate with such consummate professionals on this project. I’d like to
thank Jeff in particular for putting me in touch with Alice Peters and helping
me to get this project off the ground in the ﬁrst place.
A number of my colleagues at Naughty Dog also contributed to this book,
either by providing feedback or by helping me with the structure and topic
content of one of the chapters. I’d like to thank Marshall Robin and Carlos

Preface to the Second Edition
xxv
Gonzalez-Ochoa for their guidance and tutelage as I wrote the rendering chap-
ter, and Pål-Kristian Engstad for his excellent and insightful feedback on the
content of that chapter. My thanks go to Christian Gyrling for his feedback
on various sections of the book, including the chapter on animation (which
is one of his many specialties). And I want to extend a special thank-you to
Jonathan Lanier, Naughty Dog’s resident senior audio programmer extraor-
dinaire, for providing me with a great deal of the raw information you’ll ﬁnd
in the new audio chapter, for always being available to chat when I had ques-
tions, and for providing laser-focused and invaluable feedback after reading
the initial draft. My thanks also go to the entire Naughty Dog engineering
team for creating all of the incredible game engine systems that I highlight in
this book.
My thanks go to Keith Schaeffer of Electronic Arts for providing me with
much of the raw content regarding the impact of physics on a game, found in
Section 12.1. I’d also like to extend a warm thank-you to Paul Keet, who was
a lead engineer on the Medal of Honor franchise during my time at Electronic
Arts, and Steve Ranck, the lead engineer on the Hydro Thunder project at Mid-
way San Diego, for their mentorship and guidance over the years. While they
did not contribute to the book directly, they did help to make me the engineer
that I am today, and their inﬂuences are echoed on virtually every page in one
way or another.
This book arose out of the notes I developed for a course entitled ITP-
485: Programming Game Engines, which I taught under the auspices of the
Information Technology Program at the University of Southern California for
approximately four years. I would like to thank Dr. Anthony Borquez, the
director of the ITP department at the time, for hiring me to develop the ITP-
485 course curriculum in the ﬁrst place.
My extended family and friends also deserve thanks, in part for their
unwavering encouragement, and in part for entertaining my wife and our
two boys on so many occasions while I was working. I’d like to thank my
sister- and brother-in-law, Tracy Lee and Doug Provins, my cousin-in-law
Matt Glenn, and all of our incredible friends, including Kim and Drew Clark,
Sherilyn and Jim Kritzer, Anne and Michael Scherer and Kim and Mike Warner.
My father Kenneth Gregory wrote a book on investing in the stock market
when I was a teenager, and in doing so he inspired me to write this book. For
this and so much more, I am eternally grateful to him. I’d also like to thank my
mother Erica Gregory, in part for her insistence that I embark on this project,
and in part for spending countless hours with me when I was a child, beating
the art of writing into my cranium—I owe my writing skills, my work ethic,
and my rather twisted sense of humor entirely to her!

xxvi
Preface to the Second Edition
I’d like to thank Alice Peters and Kevin Jackson-Mead, as well as the entire
A K Peters staff, for their Herculean efforts in publishing the ﬁrst edition of
this book. Since that time, A K Peters has been acquired by the CRC Press, the
principal science and technology book division of the Taylor & Francis Group.
I’d like to wish Alice and Klaus Peters all the best in their future endeavors.
I’d also like to thank Rick Adams and Jennifer Ahringer of Taylor & Francis for
their patient support and help throughout the process of creating the second
edition of Game Engine Architecture, and Jonathan Pennell for his work on the
cover for the second edition.
Since the ﬁrst edition was published, I was thrilled to learn that it had
been translated into the Japanese language. I would like to extend my sincere
thanks to Kazuhisa Minato and his team at Namco Bandai Games for taking
on this incredibly daunting task, and doing such a great job with it. I’d also
like to thank the folks at Softbank Creative, Inc. for publishing the Japanese
version of the book. I have also learned that the book has been recently trans-
lated into Chinese. I would like to thank Milo Yip for his hard work and
dedication to this project.
Many of my readers took the time to send me feedback and alert me to er-
rors in the ﬁrst edition, and for that I’d like to extend my sincere thanks to all
of you who contributed. I’d like to give a special thank-you to Milo Yip and
Joe Conley for going above and beyond the call of duty in this regard. Both of
you provided me with many-page documents chock full of errata and incred-
ibly valuable and insightful suggestions. I’ve tried my best to incorporate all
of this feedback into the second edition. Please keep it coming!
Jason Gregory
September 2013

Part I
Foundations

This page intentionally left blank
This page intentionally left blank

1
Introduction
W
hen I got my ﬁrst game console in 1979—a way-cool Intellivision sys-
tem by Mattel—the term “game engine” did not exist. Back then, video
and arcade games were considered by most adults to be nothing more than
toys, and the software that made them tick was highly specialized to both
the game in question and the hardware on which it ran. Today, games are a
multi-billion-dollar mainstream industry rivaling Hollywood in size and pop-
ularity. And the software that drives these now-ubiquitous three-dimensional
worlds—game engines like id Software’s Quake and Doom engines, Epic
Games’ Unreal Engine 4, Valve’s Source engine and the Unity game engine—
have become fully featured reusable software development kits that can be
licensed and used to build almost any game imaginable.
While game engines vary widely in the details of their architecture and im-
plementation, recognizable coarse-grained patterns are emerging across both
publicly licensed game engines and their proprietary in-house counterparts.
Virtually all game engines contain a familiar set of core components, including
the rendering engine, the collision and physics engine, the animation system,
the audio system, the game world object model, the artiﬁcial intelligence sys-
tem and so on. Within each of these components, a relatively small number of
semi-standard design alternatives are also beginning to emerge.
There are a great many books that cover individual game engine subsys-
tems, such as three-dimensional graphics, in exhaustive detail. Other books
3

4
1. Introduction
cobble together valuable tips and tricks across a wide variety of game tech-
nology areas. However, I have been unable to ﬁnd a book that provides its
reader with a reasonably complete picture of the entire gamut of components
that make up a modern game engine. The goal of this book, then, is to take
the reader on a guided hands-on tour of the vast and complex landscape of
game engine architecture.
In this book you will learn:
•
how real industrial-strength production game engines are architected;
•
how game development teams are organized and work in the real world;
•
which major subsystems and design patterns appear again and again in
virtually every game engine;
•
the typical requirements for each major subsystem;
•
which subsystems are genre- or game-agnostic, and which ones are typ-
ically designed explicitly for a speciﬁc genre or game; and
•
where the engine normally ends and the game begins.
We’ll also get a ﬁrst-hand glimpse into the inner workings of some popular
game engines, such as Quake and Unreal, and some well-known middleware
packages, such as the Havok Physics library, the OGRE rendering engine and
Rad Game Tools’ Granny 3D animation and geometry management toolkit.
Before we get started, we’ll review some techniques and tools for large-
scale software engineering in a game engine context, including:
•
the difference between logical and physical software architecture;
•
conﬁguration management, revision control and build systems; and
•
some tips and tricks for dealing with one of the common development
environments for C and C++, Microsoft Visual Studio.
In this book I assume that you have a solid understanding of C++ (the
language of choice among most modern game developers) and that you un-
derstand basic software engineering principles. I also assume you have some
exposure to linear algebra, three-dimensional vector and matrix math and
trigonometry (although we’ll review the core concepts in Chapter 4). Ideally,
you should have some prior exposure to the basic concepts of real time and
event-driven programming. But never fear—I will review these topics brieﬂy,
and I’ll also point you in the right direction if you feel you need to hone your
skills further before we embark.

1.1. Structure of a Typical Game Team
5
1.1
Structure of a Typical Game Team
Before we delve into the structure of a typical game engine, let’s ﬁrst take a
brief look at the structure of a typical game development team. Game stu-
dios are usually composed of ﬁve basic disciplines: engineers, artists, game
designers, producers and other management and support staff (marketing,
legal, information technology/technical support, administrative, etc.). Each
discipline can be divided into various subdisciplines. We’ll take a brief look
at each below.
1.1.1
Engineers
The engineers design and implement the software that makes the game, and
the tools, work. Engineers are often categorized into two basic groups: runtime
programmers (who work on the engine and the game itself) and tools pro-
grammers (who work on the ofﬂine tools that allow the rest of the develop-
ment team to work effectively). On both sides of the runtime/tools line, engi-
neers have various specialties. Some engineers focus their careers on a single
engine system, such as rendering, artiﬁcial intelligence, audio or collision and
physics. Some focus on gameplay programming and scripting, while others
prefer to work at the systems level and not get too involved in how the game
actually plays. Some engineers are generalists—jacks of all trades who can
jump around and tackle whatever problems might arise during development.
Senior engineers are sometimes asked to take on a technical leadership
role. Lead engineers usually still design and write code, but they also help to
manage the team’s schedule, make decisions regarding the overall technical
direction of the project, and sometimes also directly manage people from a
human resources perspective.
Some companies also have one or more technical directors (TD), whose job
it is to oversee one or more projects from a high level, ensuring that the teams
are aware of potential technical challenges, upcoming industry developments,
new technologies and so on. The highest engineering-related position at a
game studio is the chief technical ofﬁcer (CTO), if the studio has one. The
CTO’s job is to serve as a sort of technical director for the entire studio, as well
as serving a key executive role in the company.
1.1.2
Artists
As we say in the game industry, “Content is king.” The artists produce all of
the visual and audio content in the game, and the quality of their work can
literally make or break a game. Artists come in all sorts of ﬂavors:

6
1. Introduction
•
Concept artists produce sketches and paintings that provide the team
with a vision of what the ﬁnal game will look like. They start their
work early in the concept phase of development, but usually continue
to provide visual direction throughout a project’s life cycle. It is com-
mon for screenshots taken from a shipping game to bear an uncanny
resemblance to the concept art.
•
3D modelers produce the three-dimensional geometry for everything in
the virtual game world. This discipline is typically divided into two
subdisciplines: foreground modelers and background modelers. The
former create objects, characters, vehicles, weapons and the other objects
that populate the game world, while the latter build the world’s static
background geometry (terrain, buildings, bridges, etc.).
•
Texture artists create the two-dimensional images known as textures,
which are applied to the surfaces of 3D models in order to provide detail
and realism.
•
Lighting artists lay out all of the light sources in the game world, both
static and dynamic, and work with color, intensity and light direction to
maximize the artfulness and emotional impact of each scene.
•
Animators imbue the characters and objects in the game with motion.
The animators serve quite literally as actors in a game production, just
as they do in a CG ﬁlm production. However, a game animator must
have a unique set of skills in order to produce animations that mesh
seamlessly with the technological underpinnings of the game engine.
•
Motion capture actors are often used to provide a rough set of motion
data, which are then cleaned up and tweaked by the animators before
being integrated into the game.
•
Sound designers work closely with the engineers in order to produce and
mix the sound effects and music in the game.
•
Voice actors provide the voices of the characters in many games.
•
Many games have one or more composers, who compose an original score
for the game.
As with engineers, senior artists are often called upon to be team lead-
ers. Some game teams have one or more art directors—very senior artists who
manage the look of the entire game and ensure consistency across the work of
all team members.

1.1. Structure of a Typical Game Team
7
1.1.3
Game Designers
The game designers’ job is to design the interactive portion of the player’s
experience, typically known as gameplay. Different kinds of designers work
at different levels of detail. Some (usually senior) game designers work at
the macro level, determining the story arc, the overall sequence of chapters
or levels, and the high-level goals and objectives of the player. Other de-
signers work on individual levels or geographical areas within the virtual
game world, laying out the static background geometry, determining where
and when enemies will emerge, placing supplies like weapons and health
packs, designing puzzle elements and so on. Still other designers operate
at a highly technical level, working closely with gameplay engineers and/or
writing code (often in a high-level scripting language). Some game design-
ers are ex-engineers, who decided they wanted to play a more active role in
determining how the game will play.
Some game teams employ one or more writers. A game writer’s job can
range from collaborating with the senior game designers to construct the story
arc of the entire game, to writing individual lines of dialogue.
As with other disciplines, some senior designers play management roles.
Many game teams have a game director, whose job it is to oversee all aspects
of a game’s design, help manage schedules, and ensure that the work of indi-
vidual designers is consistent across the entire product. Senior designers also
sometimes evolve into producers.
1.1.4
Producers
The role of producer is deﬁned differently by different studios. In some game
companies, the producer’s job is to manage the schedule and serve as a hu-
man resources manager. In other companies, producers serve in a senior game
design capacity. Still other studios ask their producers to serve as liaisons be-
tween the development team and the business unit of the company (ﬁnance,
legal, marketing, etc.). Some smaller studios don’t have producers at all. For
example, at Naughty Dog, literally everyone in the company, including the
two co-presidents, play a direct role in constructing the game; team man-
agement and business duties are shared between the senior members of the
studio.
1.1.5
Other Staff
The team of people who directly construct the game is typically supported by
a crucial team of support staff. This includes the studio’s executive manage-
ment team, the marketing department (or a team that liaises with an external

8
1. Introduction
marketing group), administrative staff and the IT department, whose job is
to purchase, install and conﬁgure hardware and software for the team and to
provide technical support.
1.1.6
Publishers and Studios
The marketing, manufacture and distribution of a game title are usually han-
dled by a publisher, not by the game studio itself. A publisher is typically
a large corporation, like Electronic Arts, THQ, Vivendi, Sony, Nintendo, etc.
Many game studios are not afﬁliated with a particular publisher. They sell
each game that they produce to whichever publisher strikes the best deal with
them. Other studios work exclusively with a single publisher, either via a
long-term publishing contract or as a fully owned subsidiary of the publishing
company. For example, THQ’s game studios are independently managed, but
they are owned and ultimately controlled by THQ. Electronic Arts takes this
relationship one step further, by directly managing its studios. First-party de-
velopers are game studios owned directly by the console manufacturers (Sony,
Nintendo and Microsoft). For example, Naughty Dog is a ﬁrst-party Sony de-
veloper. These studios produce games exclusively for the gaming hardware
manufactured by their parent company.
1.2
What Is a Game?
We probably all have a pretty good intuitive notion of what a game is. The
general term “game” encompasses board games like chess and Monopoly, card
games like poker and blackjack, casino games like roulette and slot machines,
military war games, computer games, various kinds of play among children,
and the list goes on. In academia we sometimes speak of game theory, in which
multiple agents select strategies and tactics in order to maximize their gains
within the framework of a well-deﬁned set of game rules. When used in
the context of console or computer-based entertainment, the word “game”
usually conjures images of a three-dimensional virtual world featuring a hu-
manoid, animal or vehicle as the main character under player control. (Or for
the old geezers among us, perhaps it brings to mind images of two-dimensional
classics like Pong, Pac-Man, or Donkey Kong.) In his excellent book, A Theory
of Fun for Game Design, Raph Koster deﬁnes a game to be an interactive expe-
rience that provides the player with an increasingly challenging sequence of
patterns which he or she learns and eventually masters [26]. Koster’s asser-
tion is that the activities of learning and mastering are at the heart of what we
call “fun,” just as a joke becomes funny at the moment we “get it” by recog-
nizing the pattern.

1.2. What Is a Game?
9
For the purposes of this book, we’ll focus on the subset of games that com-
prise two- and three-dimensional virtual worlds with a small number of play-
ers (between one and 16 or thereabouts). Much of what we’ll learn can also
be applied to Flash games on the Internet, pure puzzle games like Tetris, or
massively multiplayer online games (MMOG). But our primary focus will be
on game engines capable of producing ﬁrst-person shooters, third-person ac-
tion/platform games, racing games, ﬁghting games and the like.
1.2.1
Video Games as Soft Real-Time Simulations
Most two- and three-dimensional video games are examples of what com-
puter scientists would call soft real-time interactive agent-based computer simu-
lations. Let’s break this phrase down in order to better understand what it
means.
In most video games, some subset of the real world—or an imaginary
world—is modeled mathematically so that it can be manipulated by a com-
puter. The model is an approximation to and a simpliﬁcation of reality (even
if it’s an imaginary reality), because it is clearly impractical to include every
detail down to the level of atoms or quarks. Hence, the mathematical model
is a simulation of the real or imagined game world. Approximation and sim-
pliﬁcation are two of the game developer’s most powerful tools. When used
skillfully, even a greatly simpliﬁed model can sometimes be almost indistin-
guishable from reality—and a lot more fun.
An agent-based simulation is one in which a number of distinct entities
known as “agents” interact. This ﬁts the description of most three-dimensional
computer games very well, where the agents are vehicles, characters, ﬁreballs,
power dots and so on. Given the agent-based nature of most games, it should
come as no surprise that most games nowadays are implemented in an object-
oriented, or at least loosely object-based, programming language.
All interactive video games are temporal simulations, meaning that the vir-
tual game world model is dynamic—the state of the game world changes over
time as the game’s events and story unfold. A video game must also respond
to unpredictable inputs from its human player(s)—thus interactive temporal
simulations. Finally, most video games present their stories and respond to
player input in real time, making them interactive real-time simulations. One
notable exception is in the category of turn-based games like computerized
chess or non-real-time strategy games. But even these types of games usually
provide the user with some form of real-time graphical user interface. So for
the purposes of this book, we’ll assume that all video games have at least some
real-time constraints.

10
1. Introduction
At the core of every real-time system is the concept of a deadline. An ob-
vious example in video games is the requirement that the screen be updated
at least 24 times per second in order to provide the illusion of motion. (Most
games render the screen at 30 or 60 frames per second because these are mul-
tiples of an NTSC monitor’s refresh rate.) Of course, there are many other
kinds of deadlines in video games as well. A physics simulation may need
to be updated 120 times per second in order to remain stable. A character’s
artiﬁcial intelligence system may need to “think” at least once every second to
prevent the appearance of stupidity. The audio library may need to be called
at least once every 1/60 second in order to keep the audio buffers ﬁlled and
prevent audible glitches.
A “soft” real-time system is one in which missed deadlines are not catas-
trophic. Hence, all video games are soft real-time systems—if the frame rate
dies, the human player generally doesn’t! Contrast this with a hard real-time
system, in which a missed deadline could mean severe injury to or even the
death of a human operator. The avionics system in a helicopter or the control-
rod system in a nuclear power plant are examples of hard real-time systems.
Mathematical models can be analytic or numerical. For example, the ana-
lytic (closed-form) mathematical model of a rigid body falling under the in-
ﬂuence of constant acceleration due to gravity is typically written as follows:
y(t) = 1
2gt2 + v0t + y0.
(1.1)
An analytic model can be evaluated for any value of its independent variables,
such as the time t in the above equation, given only the initial conditions v0
and y0 and the constant g. Such models are very convenient when they can be
found. However, many problems in mathematics have no closed-form solu-
tion. And in video games, where the user’s input is unpredictable, we cannot
hope to model the entire game analytically.
A numerical model of the same rigid body under gravity might be
y(t + ∆t) = F(y(t), ˙y(t), ¨y(t), . . .).
(1.2)
That is, the height of the rigid body at some future time (t + ∆t) can be found
as a function of the height and its ﬁrst and second time derivatives at the
current time t. Numerical simulations are typically implemented by running
calculations repeatedly, in order to determine the state of the system at each
discrete time step. Games work in the same way. A main “game loop” runs
repeatedly, and during each iteration of the loop, various game systems such
as artiﬁcial intelligence, game logic, physics simulations and so on are given
a chance to calculate or update their state for the next discrete time step. The
results are then “rendered” by displaying graphics, emitting sound and pos-
sibly producing other outputs such as force-feedback on the joypad.

1.3. What Is a Game Engine?
11
1.3
What Is a Game Engine?
The term “game engine” arose in the mid-1990s in reference to ﬁrst-person
shooter (FPS) games like the insanely popular Doom by id Software. Doom
was architected with a reasonably well-deﬁned separation between its core
software components (such as the three-dimensional graphics rendering sys-
tem, the collision detection system or the audio system) and the art assets,
game worlds and rules of play that comprised the player’s gaming experi-
ence. The value of this separation became evident as developers began li-
censing games and retooling them into new products by creating new art,
world layouts, weapons, characters, vehicles and game rules with only min-
imal changes to the “engine” software. This marked the birth of the “mod
community”—a group of individual gamers and small independent studios
that built new games by modifying existing games, using free toolkits pro-
vided by the original developers.
Towards the end of the 1990s, some games like Quake III Arena and Un-
real were designed with reuse and “modding” in mind. Engines were made
highly customizable via scripting languages like id’s Quake C, and engine li-
censing began to be a viable secondary revenue stream for the developers who
created them. Today, game developers can license a game engine and reuse
signiﬁcant portions of its key software components in order to build games.
While this practice still involves considerable investment in custom software
engineering, it can be much more economical than developing all of the core
engine components in-house.
The line between a game and its engine is often blurry. Some engines
make a reasonably clear distinction, while others make almost no attempt to
separate the two. In one game, the rendering code might “know” speciﬁ-
cally how to draw an orc. In another game, the rendering engine might pro-
vide general-purpose material and shading facilities, and “orc-ness” might
be deﬁned entirely in data. No studio makes a perfectly clear separation be-
tween the game and the engine, which is understandable considering that
the deﬁnitions of these two components often shift as the game’s design so-
lidiﬁes.
Arguably a data-driven architecture is what differentiates a game engine
from a piece of software that is a game but not an engine. When a game
contains hard-coded logic or game rules, or employs special-case code to ren-
der speciﬁc types of game objects, it becomes difﬁcult or impossible to reuse
that software to make a different game. We should probably reserve the term
“game engine” for software that is extensible and can be used as the founda-
tion for many different games without major modiﬁcation.

12
1. Introduction
Can be “modded” to 
build any game in a 
specific genre
Can be used to build any 
game imaginable
Cannot be used to build 
more than one game
Can be customized to 
make very similar games
Unity,
Unreal Engine 4,
Source Engine, ...
Hydro Thunder 
Engine
Probably 
impossible
PacMan
Quake III 
Engine
Figure 1.1. Game engine reusability gamut.
Clearly this is not a black-and-white distinction. We can think of a gamut
of reusability onto which every engine falls. Figure 1.1 takes a stab at the
locations of some well-known games/engines along this gamut.
One would think that a game engine could be something akin to Apple
QuickTime or Microsoft Windows Media Player—a general-purpose piece of
software capable of playing virtually any game content imaginable. However,
this ideal has not yet been achieved (and may never be). Most game engines
are carefully crafted and ﬁne-tuned to run a particular game on a particular
hardware platform. And even the most general-purpose multiplatform en-
gines are really only suitable for building games in one particular genre, such
as ﬁrst-person shooters or racing games. It’s safe to say that the more general-
purpose a game engine or middleware component is, the less optimal it is for
running a particular game on a particular platform.
This phenomenon occurs because designing any efﬁcient piece of software
invariably entails making trade-offs, and those trade-offs are based on as-
sumptions about how the software will be used and/or about the target hard-
ware on which it will run. For example, a rendering engine that was designed
to handle intimate indoor environments probably won’t be very good at ren-
dering vast outdoor environments. The indoor engine might use a binary
space partitioning (BSP) tree or portal system to ensure that no geometry is
drawn that is being occluded by walls or objects that are closer to the camera.
The outdoor engine, on the other hand, might use a less-exact occlusion mech-
anism, or none at all, but it probably makes aggressive use of level-of-detail
(LOD) techniques to ensure that distant objects are rendered with a minimum
number of triangles, while using high-resolution triangle meshes for geome-
try that is close to the camera.
The advent of ever-faster computer hardware and specialized graphics
cards, along with ever-more-efﬁcient rendering algorithms and data struc-
tures, is beginning to soften the differences between the graphics engines of
different genres. It is now possible to use a ﬁrst-person shooter engine to
build a real-time strategy game, for example. However, the trade-off between

1.4. Engine Differences Across Genres
13
generality and optimality still exists. A game can always be made more im-
pressive by ﬁne-tuning the engine to the speciﬁc requirements and constraints
of a particular game and/or hardware platform.
1.4
Engine Differences Across Genres
Game engines are typically somewhat genre speciﬁc. An engine designed
for a two-person ﬁghting game in a boxing ring will be very different from a
massively multiplayer online game (MMOG) engine or a ﬁrst-person shooter
(FPS) engine or a real-time strategy (RTS) engine. However, there is also a
great deal of overlap—all 3D games, regardless of genre, require some form
of low-level user input from the joypad, keyboard and/or mouse, some form
of 3D mesh rendering, some form of heads-up display (HUD) including text
rendering in a variety of fonts, a powerful audio system, and the list goes
on. So while the Unreal Engine, for example, was designed for ﬁrst-person
shooter games, it has been used successfully to construct games in a number
of other genres as well, including the wildly popular third-person shooter
franchise Gears of War by Epic Games and the smash hits Batman: Arkham
Asylum and Batman: Arkham City by Rocksteady Studios.
Let’s take a look at some of the most common game genres and explore
some examples of the technology requirements particular to each.
1.4.1
First-Person Shooters (FPS)
The ﬁrst-person shooter (FPS) genre is typiﬁed by games like Quake, Unreal
Tournament, Half-Life, Counter-Strike and Battleﬁeld (see Figure 1.2).
These
games have historically involved relatively slow on-foot roaming of a po-
tentially large but primarily corridor-based world. However, modern ﬁrst-
person shooters can take place in a wide variety of virtual environments in-
cluding vast open outdoor areas and conﬁned indoor areas.
Modern FPS
traversal mechanics can include on-foot locomotion, rail-conﬁned or free-
roaming ground vehicles, hovercraft, boats and aircraft. For an overview of
this genre, see http://en.wikipedia.org/wiki/First-person_shooter.
First-person games are typically some of the most technologically chal-
lenging to build, probably rivaled in complexity only by third-person shooter/
action/platformer games and massively multiplayer games. This is because
ﬁrst-person shooters aim to provide their players with the illusion of being
immersed in a detailed, hyperrealistic world. It is not surprising that many of
the game industry’s big technological innovations arose out of the games in
this genre.

14
1. Introduction
Figure 1.2. Battleﬁeld 4 by Electronic Arts/DICE (PC, Xbox 360, PlayStation 3, Xbox One, PlaySta-
tion 4). (See Color Plate I.)
First-person shooters typically focus on technologies such as:
•
efﬁcient rendering of large 3D virtual worlds;
•
a responsive camera control/aiming mechanic;
•
high-ﬁdelity animations of the player’s virtual arms and weapons;
•
a wide range of powerful handheld weaponry;
•
a forgiving player character motion and collision model, which often
gives these games a “ﬂoaty” feel;
•
high-ﬁdelity animations and artiﬁcial intelligence for the non-player
characters (NPCs)—the player’s enemies and allies; and
•
small-scale online multiplayer capabilities (typically supporting up to
64 simultaneous players), and the ubiquitous “death match” gameplay
mode.
The rendering technology employed by ﬁrst-person shooters is almost al-
ways highly optimized and carefully tuned to the particular type of environ-
ment being rendered. For example, indoor “dungeon crawl” games often em-
ploy binary space partitioning trees or portal-based rendering systems. Out-
door FPS games use other kinds of rendering optimizations such as occlusion
culling, or an ofﬂine sectorization of the game world with manual or auto-
mated speciﬁcation of which target sectors are visible from each source sector.

1.4. Engine Differences Across Genres
15
Of course, immersing a player in a hyperrealistic game world requires
much more than just optimized high-quality graphics technology. The charac-
ter animations, audio and music, rigid body physics, in-game cinematics and
myriad other technologies must all be cutting-edge in a ﬁrst-person shooter.
So this genre has some of the most stringent and broad technology require-
ments in the industry.
1.4.2
Platformers and Other Third-Person Games
“Platformer” is the term applied to third-person character-based action games
where jumping from platform to platform is the primary gameplay mechanic.
Typical games from the 2D era include Space Panic, Donkey Kong, Pitfall! and
Super Mario Brothers. The 3D era includes platformers like Super Mario 64,
Crash Bandicoot, Rayman 2, Sonic the Hedgehog, the Jak and Daxter series (Fig-
ure 1.3), the Ratchet & Clank series and Super Mario Galaxy. See http://en.
wikipedia.org/wiki/Platformer for an in-depth discussion of this genre.
In terms of their technological requirements, platformers can usually be
lumped together with third-person shooters and third-person action/adven-
Figure 1.3.
Jak II by Naughty Dog (Jak, Daxter, Jak and Daxter, and Jak II © 2003, 2013/™SCEA.
Created and developed by Naughty Dog, PlayStation 2). (See Color Plate II.)

16
1. Introduction
Figure 1.4. Gears of War 3 by Epic Games (Xbox 360). (See Color Plate III.)
ture games like Dead Space 2, Gears of War 3 (Figure 1.4), Red Dead Remption,
the Uncharted series, the Resident Evil series, The Last of Us, and the list goes on.
Third-person character-based games have a lot in common with ﬁrst-per-
son shooters, but a great deal more emphasis is placed on the main character’s
abilities and locomotion modes. In addition, high-ﬁdelity full-body character
animations are required for the player’s avatar, as opposed to the somewhat
less-taxing animation requirements of the “ﬂoating arms” in a typical FPS
game. It’s important to note here that almost all ﬁrst-person shooters have
an online multiplayer component, so a full-body player avatar must be ren-
dered in addition to the ﬁrst-person arms. However, the ﬁdelity of these FPS
player avatars is usually not comparable to the ﬁdelity of the non-player char-
acters in these same games; nor can it be compared to the ﬁdelity of the player
avatar in a third-person game.
In a platformer, the main character is often cartoon-like and not particu-
larly realistic or high-resolution. However, third-person shooters often fea-
ture a highly realistic humanoid player character. In both cases, the player
character typically has a very rich set of actions and animations.
Some of the technologies speciﬁcally focused on by games in this genre
include:

1.4. Engine Differences Across Genres
17
•
moving platforms, ladders, ropes, trellises and other interesting locomo-
tion modes;
•
puzzle-like environmental elements;
•
a third-person “follow camera” which stays focused on the player char-
acter and whose rotation is typically controlled by the human player via
the right joypad stick (on a console) or the mouse (on a PC—note that
while there are a number of popular third-person shooters on a PC, the
platformer genre exists almost exclusively on consoles); and
•
a complex camera collision system for ensuring that the view point never
“clips” through background geometry or dynamic foreground objects.
1.4.3
Fighting Games
Fighting games are typically two-player games involving humanoid charac-
ters pummeling each other in a ring of some sort. The genre is typiﬁed by
games like Soul Calibur and Tekken 3 (see Figure 1.5). The Wikipedia page
http://en.wikipedia.org/wiki/Fighting_game provides an overview of this
genre.
Traditionally games in the ﬁghting genre have focused their technology
efforts on:
Figure 1.5. Tekken 3 by Namco (PlayStation). (See Color Plate IV.)

18
1. Introduction
•
a rich set of ﬁghting animations;
•
accurate hit detection;
•
a user input system capable of detecting complex button and joystick
combinations; and
•
crowds, but otherwise relatively static backgrounds.
Since the 3D world in these games is small and the camera is centered
on the action at all times, historically these games have had little or no need
for world subdivision or occlusion culling. They would likewise not be ex-
pected to employ advanced three-dimensional audio propagation models, for
example.
State-of-the-art ﬁghting games like EA’s Fight Night Round 4 (Figure 1.6)
have upped the technological ante with features like:
•
high-deﬁnition character graphics, including realistic skin shaders with
subsurface scattering and sweat effects;
•
high-ﬁdelity character animations; and
•
physics-based cloth and hair simulations for the characters.
It’s important to note that some ﬁghting games like Heavenly Sword take
place in a large-scale virtual world, not a conﬁned arena. In fact, many people
consider this to be a separate genre, sometimes called a brawler. This kind of
Figure 1.6. Fight Night Round 4 by EA (PlayStation 3). (See Color Plate V.)

1.4. Engine Differences Across Genres
19
ﬁghting game can have technical requirements more akin to those of a third-
person shooter or real-time strategy game.
1.4.4
Racing Games
The racing genre encompasses all games whose primary task is driving a
car or other vehicle on some kind of track.
The genre has many subcat-
egories. Simulation-focused racing games (“sims”) aim to provide a driv-
ing experience that is as realistic as possible (e.g., Gran Turismo).
Arcade
racers favor over-the-top fun over realism (e.g., San Francisco Rush, Cruis’n
USA, Hydro Thunder). One subgenre explores the subculture of street rac-
ing with tricked out consumer vehicles (e.g., Need for Speed, Juiced).
Kart
racing is a subcategory in which popular characters from platformer games
or cartoon characters from TV are re-cast as the drivers of whacky vehicles
(e.g., Mario Kart, Jak X, Freaky Flyers).
Racing games need not always in-
volve time-based competition. Some kart racing games, for example, offer
modes in which players shoot at one another, collect loot or engage in a va-
riety of other timed and untimed tasks. For a discussion of this genre, see
http://en.wikipedia.org/wiki/Racing_game.
A racing game is often very linear, much like older FPS games. However,
travel speed is generally much faster than in an FPS. Therefore, more focus is
placed on very long corridor-based tracks, or looped tracks, sometimes with
various alternate routes and secret short-cuts. Racing games usually focus all
their graphic detail on the vehicles, track and immediate surroundings. How-
ever, kart racers also devote signiﬁcant rendering and animation bandwidth
to the characters driving the vehicles. Figure 1.7 shows a screenshot from
the next installment in the well-known Gran Turismo racing game series, Gran
Turismo 6, developed by Polyphony Digital and published by Sony Computer
Entertainment.
Some of the technological properties of a typical racing game include the
following techniques:
•
Various “tricks” are used when rendering distant background elements,
such as employing two-dimensional cards for trees, hills and mountains.
•
The track is often broken down into relatively simple two-dimensional
regions called “sectors.”
These data structures are used to optimize
rendering and visibility determination, to aid in artiﬁcial intelligence
and path ﬁnding for non-human-controlled vehicles, and to solve many
other technical problems.
•
The camera typically follows behind the vehicle for a third-person per-
spective, or is sometimes situated inside the cockpit ﬁrst-person style.

20
1. Introduction
Figure 1.7. Gran Turismo 6 by Polyphony Digital (PlayStation 3). (See Color Plate VI.)
•
When the track involves tunnels and other “tight” spaces, a good deal
of effort is often put into ensuring that the camera does not collide with
background geometry.
1.4.5
Real-Time Strategy (RTS)
The modern real-time strategy (RTS) genre was arguably deﬁned by Dune II:
The Building of a Dynasty (1992). Other games in this genre include Warcraft,
Command & Conquer, Age of Empires and Starcraft. In this genre, the player
deploys the battle units in his or her arsenal strategically across a large playing
ﬁeld in an attempt to overwhelm his or her opponent. The game world is
typically displayed at an oblique top-down viewing angle. For a discussion
of this genre, see http://en.wikipedia.org/wiki/Real-time_strategy.
The RTS player is usually prevented from signiﬁcantly changing the view-
ing angle in order to see across large distances. This restriction permits de-
velopers to employ various optimizations in the rendering engine of an RTS
game.
Older games in the genre employed a grid-based (cell-based) world con-
struction, and an orthographic projection was used to greatly simplify the ren-
derer. For example, Figure 1.8 shows a screenshot from the classic RTS Age of
Empires.
Modern RTS games sometimes use perspective projection and a true 3D
world, but they may still employ a grid layout system to ensure that units and
background elements, such as buildings, align with one another properly. A
popular example, Command & Conquer 3, is shown in Figure 1.9.

1.4. Engine Differences Across Genres
21
Figure 1.8. Age of Empires by Ensemble Studios (PC). (See Color Plate VII.)
Figure 1.9. Command & Conquer 3 by EA Los Angeles (PC, Xbox 360). (See Color Plate VIII.)

22
1. Introduction
Some other common practices in RTS games include the following tech-
niques:
•
Each unit is relatively low-res, so that the game can support large num-
bers of them on-screen at once.
•
Height-ﬁeld terrain is usually the canvas upon which the game is de-
signed and played.
•
The player is often allowed to build new structures on the terrain in
addition to deploying his or her forces.
•
User interaction is typically via single-click and area-based selection of
units, plus menus or toolbars containing commands, equipment, unit
types, building types, etc.
1.4.6
Massively Multiplayer Online Games (MMOG)
The massively multiplayer online game (MMOG or just MMO) genre is typ-
iﬁed by games like Guild Wars 2 (AreaNet/NCsoft), EverQuest (989 Studios/
SOE), World of Warcraft (Blizzard) and Star Wars Galaxies (SOE/Lucas Arts), to
name a few. An MMO is deﬁned as any game that supports huge numbers of
simultaneous players (from thousands to hundreds of thousands), usually all
playing in one very large, persistent virtual world (i.e., a world whose internal
state persists for very long periods of time, far beyond that of any one player’s
gameplay session). Otherwise, the gameplay experience of an MMO is often
similar to that of their small-scale multiplayer counterparts. Subcategories
of this genre include MMO role-playing games (MMORPG), MMO real-time
strategy games (MMORTS) and MMO ﬁrst-person shooters (MMOFPS). For
a discussion of this genre, see http://en.wikipedia.org/wiki/MMOG. Fig-
ure 1.10 shows a screenshot from the hugely popular MMORPG World of War-
craft.
At the heart of all MMOGs is a very powerful battery of servers. These
servers maintain the authoritative state of the game world, manage users sign-
ing in and out of the game, provide inter-user chat or voice-over-IP (VoIP) ser-
vices and more. Almost all MMOGs require users to pay some kind of regular
subscription fee in order to play, and they may offer micro-transactions within
the game world or out-of-game as well. Hence, perhaps the most important
role of the central server is to handle the billing and micro-transactions which
serve as the game developer’s primary source of revenue.
Graphics ﬁdelity in an MMO is almost always lower than its non-massively
multiplayer counterparts, as a result of the huge world sizes and extremely
large numbers of users supported by these kinds of games.

1.4. Engine Differences Across Genres
23
Figure 1.10. World of Warcraft by Blizzard Entertainment (PC). (See Color Plate IX.)
Figure 1.11 shows a screen from Bungie’s latest highly anticipated FPS
game, Destiny. This game has been called an MMOFPS because it incorpo-
rates some aspects of the MMO genre. However, Bungie prefers to call it a
“shared world” game because unlike a traditional MMO, in which a player
can see and interact with literally any other player on a particular server, Des-
tiny provides “on-the-ﬂy match-making.” This permits the player to interact
Figure 1.11.
Destiny by Bungie (Xbox 360, PlayStation 3, Xbox One, PlayStation 4). (See Color
Plate X.)

24
1. Introduction
only with the other players with whom they have been matched by the server.
Also unlike a traditional MMO, the graphics ﬁdelity in Destiny promises to be
among the best of its generation.
1.4.7
Player-Authored Content
As social media takes off, games are becoming more and more colaborative in
nature. A recent trend in game design is toward player-authored content. For
example, Media Molecule’s Little Big Planet and Little Big Planet 2 (Figure 1.12)
are technically puzzle platformers, but their most notable and unique feature
is that they encourage players to create, publish and share their own game
worlds. Media Molecule’s latest instalment in this up-and-coming genre is
Tearaway for the PlayStation Vita (Figure 1.13).
Perhaps the most popular game today in the player-created content genre
is Minecraft (Figure 1.14). The brilliance of this game lies in its simplicity:
Minecraft game worlds are constructed from simple cubic voxel-like elements
mapped with low-resolution textures to mimic various materials. Blocks can
be solid, or they can contain items such as torches, anvils, signs, fences and
panes of glass. The game world is populated with one or more player charac-
ters, animals such as chickens and pigs, and various “mobs”—good guys like
villagers and bad guys like zombies and the ubiquitous creepers who sneak up
on unsuspecting players and explode (only scant moments after warning the
player with the “hiss” of a burning fuse).
Figure 1.12. Little Big Planet 2 by Media Molecule, © 2014 Sony Computer Entertainment Europe
(PlayStation 3). (See Color Plate XI.)

1.4. Engine Differences Across Genres
25
Figure 1.13. Tearaway by Media Molecule, © 2014 Sony Computer Entertainment Europe (PlaySta-
tion Vita). (See Color Plate XII.)
Players can create a randomized world in Minecraft and then dig into the
generated terrain to create tunnels and caverns. They can also construct their
own structures, ranging from simple terrain and foliage to vast and complex
buildings and machinery. Perhaps the biggest stroke of genious in Minecraft
is redstone. This material serves as “wiring,” allowing players to lay down
Figure 1.14. Minecraft by Markus “Notch” Persson / Mojang AB (PC, Mac, Xbox 360, PlayStation 3,
PlayStation Vita, iOS). (See Color Plate XIII.)

26
1. Introduction
circuitry that controls pistons, hoppers, mine carts and other dynamic ele-
ments in the game. As a result, players can create virtually anything they can
imagine, and then share their worlds with their friends by hosting a server
and inviting them to play online.
1.4.8
Other Genres
There are of course many other game genres which we won’t cover in depth
here. Some examples include:
•
sports, with subgenres for each major sport (football, baseball, soccer,
golf, etc.);
•
role-playing games (RPG);
•
God games, like Populous and Black & White;
•
environmental/social simulation games, like SimCity or The Sims;
•
puzzle games like Tetris;
•
conversions of non-electronic games, like chess, card games, go, etc.;
•
web-based games, such as those offered at Electronic Arts’ Pogo site;
and the list goes on.
We have seen that each game genre has its own particular technological re-
quirements. This explains why game engines have traditionally differed quite
a bit from genre to genre. However, there is also a great deal of technological
overlap between genres, especially within the context of a single hardware
platform. With the advent of more and more powerful hardware, differences
between genres that arose because of optimization concerns are beginning to
evaporate. It is therefore becoming increasingly possible to reuse the same en-
gine technology across disparate genres, and even across disparate hardware
platforms.
1.5
Game Engine Survey
1.5.1
The Quake Family of Engines
The ﬁrst 3D ﬁrst-person shooter (FPS) game is generally accepted to be Castle
Wolfenstein 3D (1992). Written by id Software of Texas for the PC platform, this
game led the game industry in a new and exciting direction. Id Software went
on to create Doom, Quake, Quake II and Quake III. All of these engines are very
similar in architecture, and I will refer to them as the Quake family of engines.
Quake technology has been used to create many other games and even other

1.5. Game Engine Survey
27
engines. For example, the lineage of Medal of Honor for the PC platform goes
something like this:
•
Quake III (Id);
•
Sin (Ritual);
•
F.A.K.K. 2 (Ritual);
•
Medal of Honor: Allied Assault (2015 & Dreamworks Interactive); and
•
Medal of Honor: Paciﬁc Assault (Electronic Arts, Los Angeles).
Many other games based on Quake technology follow equally circuitous
paths through many different games and studios. In fact, Valve’s Source en-
gine (used to create the Half-Life games) also has distant roots in Quake tech-
nology.
The Quake and Quake II source code is freely available, and the original
Quake engines are reasonably well architected and “clean” (although they are
of course a bit outdated and written entirely in C). These code bases serve
as great examples of how industrial-strength game engines are built. The
full source code to Quake and Quake II is available at https://github.com/
id-Software/Quake-2.
If you own the Quake and/or Quake II games, you can actually build the
code using Microsoft Visual Studio and run the game under the debugger us-
ing the real game assets from the disk. This can be incredibly instructive. You
can set breakpoints, run the game and then analyze how the engine actually
works by stepping through the code. I highly recommend downloading one
or both of these engines and analyzing the source code in this manner.
1.5.2
The Unreal Family of Engines
Epic Games, Inc. burst onto the FPS scene in 1998 with its legendary game Un-
real. Since then, the Unreal Engine has become a major competitor to Quake
technology in the FPS space. Unreal Engine 2 (UE2) is the basis for Unreal
Tournament 2004 (UT2004) and has been used for countless “mods,” university
projects and commercial games. Unreal Engine 4 (UE4) is the latest evolution-
ary step, boasting some of the best tools and richest engine feature sets in the
industry, including a convenient and powerful graphical user interface for
creating shaders and a graphical user interface for game logic programming
called Kismet. Many games are being developed with UE4 lately, including of
course Epic’s popular Gears of War.
The Unreal Engine has become known for its extensive feature set and
cohesive, easy-to-use tools. The Unreal Engine is not perfect, and most devel-
opers modify it in various ways to run their game optimally on a particular

28
1. Introduction
hardware platform. However, Unreal is an incredibly powerful prototyping
tool and commercial game development platform, and it can be used to build
virtually any 3D ﬁrst-person or third-person game (not to mention games in
other genres as well).
The Unreal Developer Network (UDN) provides a rich set of documenta-
tion and other information about all released versions of the Unreal Engine
(see http://udn.epicgames.com/Main/WebHome.html). Some documenta-
tion is freely available. However, access to the full documentation for the
latest version of the Unreal Engine is generally restricted to licensees of the
engine. There are plenty of other useful websites and wikis that cover the Un-
real Engine. One popular one is http://www.beyondunreal.com.
Thankfully, Epic now offers full access to Unreal Engine 4, source code and
all, for a low monthly subscription fee plus a cut of your game’s proﬁts if it
ships. This makes UE4 a viable choice for small independent game studios.
1.5.3
The Half-Life Source Engine
Source is the game engine that drives the smash hit Half-Life 2 and its sequels
HL2: Episode One nad HL2: Episode Two, Team Fortress 2 and Portal (shipped
together under the title The Orange Box). Source is a high-quality engine, ri-
valing Unreal Engine 4 in terms of graphics capabilities and tool set.
1.5.4
DICE’s Frostbite
The Frostbite engine grew out of DICE’s efforts to create a game engine for
Battleﬁeld Bad Company in 2006. Since then, the Frostbite engine has become
the most widely adopted engine within Electronic Arts (EA); it is used by
many of EA’s key franchises including Mass Effect, Battleﬁeld, Need for Speed
and Dragon Age. Frostbite boasts a powerful uniﬁed asset creation tool called
FrostEd, a powerful tools pipeline known as Backend Services, and a powerful
runtime game engine. At the time this was written, the latest version of the
engine is Frostbite 3, which is being used on DICE’s popular title Battleﬁeld 4
for the PC, Xbox 360, Xbox One, PlayStation 3 and PlayStation 4, along with
new games in the Command & Conquer, Dragon Age and Mass Effect franchises.
1.5.5
CryENGINE
Crytek originally developed their powerful game engine known as CryEN-
GINE as a tech demo for Nvidia. When the potential of the technology was
recognized, Crytek turned the demo into a complete game and Far Cry was
born. Since then, many games have been made with CryENGINE including
Crysis, Codename Kingdoms, Warface and Ryse: Son of Rome. Over the years the

1.5. Game Engine Survey
29
engine has evolved into what is now Crytek’s latest offering, CryENGINE 3.
This powerful game development platform offers a powerful suite of asset-
creation tools and a feature-rich runtime engine featuring high-quality real-
time graphics. CryENGINE 3 can be used to make games targeting a wide
range of platforms including Xbox One, Xbox 360, PlayStation 4, PlayStation 3,
Wii U and PC.
1.5.6
Sony’s PhyreEngine
In an effort to make developing games for Sony’s PlayStation 3 platform more
accessible, Sony introduced PhyreEngine at the Game Developer’s Confer-
ence (GDC) in 2008. As of 2013, PhyreEngine has evolved into a powerful and
full-featured game engine, supporting an impressive array of features includ-
ing advanced lighting and deferred rendering. It has been used by many stu-
dios to build over 90 published titles, including thatgamecompany’s hits ﬂOw,
Flower and Journey, VectorCell’s AMY, and From Software’s Demon’s Souls and
Dark Souls. PhyreEngine now supports Sony’s PlayStation 4, PlayStation 3,
PlayStation 2, PlayStation Vita and PSP platforms. PhyreEngine 3.5 gives de-
velopers access to the power of the highly parallel Cell architecture on PS3
and the advanced compute capabilities of the PS4, along with a streamlined
new world editor and other powerful game development tools. It is available
free of charge to any licensed Sony developer as part of the PlayStation SDK.
1.5.7
Microsoft’s XNA Game Studio
Microsoft’s XNA Game Studio is an easy-to-use and highly accessible game
development platform aimed at encouraging players to create their own games
and share them with the online gaming community, much as YouTube encour-
ages the creation and sharing of home-made videos.
XNA is based on Microsoft’s C# language and the Common Language
Runtime (CLR). The primary development environment is Visual Studio or
its free counterpart, Visual Studio Express. Everything from source code to
game art assets are managed within Visual Studio. With XNA, developers
can create games for the PC platform and Microsoft’s Xbox 360 console. After
paying a modest fee, XNA games can be uploaded to the Xbox Live network
and shared with friends. By providing excellent tools at essentially zero cost,
Microsoft has brilliantly opened the ﬂoodgates for the average person to cre-
ate new games.
1.5.8
Unity
Unity is a powerful cross-platform game development environment and run-
time engine supporting a wide range of platforms. Using Unity, developers

30
1. Introduction
can deploy their games on mobile platforms (Apple iOS, Google Android,
Windows phone and BlackBerry 10 devices), consoles (Microsoft Xbox 360
and Xbox One, Sony PlayStation 3 and PlayStation 4, and Nintendo Wii and
Wii U) and desktop computers (Microsoft Windows, Apple Macintosh and
Linux). It even supports a Webplayer for deployment on all the major web
browsers.
Unity’s primary design goals are ease of development and cross-platform
game deployment. As such, Unity provides an easy-to-use integrated editor
environment, in which you can create and manipulate the assets and entities
that make up your game world and quickly preview your game in action right
there in the editor, or directly on your target hardware. Unity also provides
a powerful suite of tools for analyzing and optimizing your game on each
target platform, a comprehensive asset conditioning pipeline, and the ability
to manage the performance-quality trade-off uniquely on each deployment
platform. Unity supports scripting in JavaScript, C# or Boo; a powerful ani-
mation system supporting animation retargeting (the ability to play an anima-
tion authored for one character on a totally different character); and support
for networked multiplayer games.
Unity has been used to create a wide variety of published games, including
Deus Ex: The Fall by N-Fusion/Eidos Montreal, Chop Chop Runner by Gameri-
zon and Zombieville USA by Mika Mobile, Inc.
1.5.9
2D Game Engines for Non-programmers
Two-dimensional games have become incredibly popular with the recent ex-
plosion of casual web gaming and mobile gaming on platforms like Apple
iPhone/iPad and Google Android. A number of popular game/multimedia
authoring toolkits have become available, enabling small game studios and
independent developers to create 2D games for these platforms.
These
toolkits emphasize ease of use and allow users to employ a graphical user
interface to create a game rather than requiring the use of a programming
language. Check out this YouTube video to get a feel for the kinds of games
you can create with these toolkits:
https://www.youtube.com/watch?v=
3Zq1yo0lxOU
•
Multimedia Fusion 2 (http://www.clickteam.com/website/world is a 2D
game/multimedia authoring toolkit developed by Clickteam. Fusion
is used by industry professionals to create games, screen savers and
other multimedia applications. Fusion and its simpler counterpart, The
Games Factory 2, are also used by educational camps like PlanetBravo
(http://www.planetbravo.com) to teach kids about game development

1.5. Game Engine Survey
31
and programming/logic concepts. Fusion supports iOS, Android, Flash,
Java and XNA platforms.
•
Game Salad Creator (http://gamesalad.com/creator) is another graphical
game/multimedia authoring toolkit aimed at non-programmers, simi-
lar in many respects to Fusion.
•
Scratch (http://scratch.mit.edu) is an authoring toolkit and graphical pro-
gramming language that can be used to create interactive demos and
simple games. It is a great way for young people to learn about pro-
gramming concepts such as conditionals, loops and event-driven pro-
gramming. Scratch was developed in 2003 by the Lifelong Kindergarten
group, led by Mitchel Resnick at the MIT Media Lab.
1.5.10
Other Commercial Engines
There are lots of other commercial game engines out there. Although indie
developers may not have the budget to purchase an engine, many of these
products have great online documentation and/or wikis that can serve as a
great source of information about game engines and game programming in
general. For example, check out the C4 Engine by Terathon Software (http://
www.terathon.com), a company founded by Eric Lengyel in 2001. Documen-
tation for the C4 Engine can be found on Terathon’s website, with additional
details on the C4 Engine wiki.
1.5.11
Proprietary In-House Engines
Many companies build and maintain proprietary in-house game engines. Elec-
tronic Arts built many of its RTS games on a proprietary engine called Sage,
developed at Westwood Studios. Naughty Dog’s Crash Bandicoot and Jak and
Daxter franchises were built on a proprietary engine custom tailored to the
PlayStation and PlayStation 2. For the Uncharted series, Naughty Dog devel-
oped a brand new engine custom tailored to the PlayStation 3 hardware. This
engine evolved and was ultimately used to create Naughty Dog’s latest hit,
The Last of Us, and it will continue to evolve as Naughty Dog transitions onto
the PlayStation 4. And of course, most commercially licensed game engines
like Quake, Source, Unreal Engine 3, CryENGINE 3 and Frostbite 2 all started
out as proprietary in-house engines.
1.5.12
Open Source Engines
Open source 3D game engines are engines built by amateur and professional
game developers and provided online for free. The term “open source” typi-

32
1. Introduction
cally implies that source code is freely available and that a somewhat open de-
velopment model is employed, meaning almost anyone can contribute code.
Licensing, if it exists at all, is often provided under the Gnu Public License
(GPL) or Lesser Gnu Public License (LGPL). The former permits code to be
freely used by anyone, as long as their code is also freely available; the latter
allows the code to be used even in proprietary for-proﬁt applications. Lots of
other free and semi-free licensing schemes are also available for open source
projects.
There are a staggering number of open source engines available on the
web. Some are quite good, some are mediocre and some are just plain awful!
The list of game engines provided online at http://en.wikipedia.org/wiki/
List_of_game_engines will give you a feel for the sheer number of engines
that are out there.
OGRE is a well-architected, easy-to-learn and easy-to-use 3D rendering
engine. It boasts a fully featured 3D renderer including advanced lighting
and shadows, a good skeletal character animation system, a two-dimensional
overlay system for heads-up displays and graphical user interfaces, and a
post-processing system for full-screen effects like bloom.
OGRE is, by its
authors’ own admission, not a full game engine, but it does provide
many of the foundational components required by pretty much any game
engine.
Some other well-known open source engines are listed here:
•
Panda3D is a script-based engine. The engine’s primary interface is the
Python custom scripting language. It is designed to make prototyping
3D games and virtual worlds convenient and fast.
•
Yake is a game engine built on top of OGRE.
•
Crystal Space is a game engine with an extensible modular architecture.
•
Torque and Irrlicht are also well-known game engines.
1.6
Runtime Engine Architecture
A game engine generally consists of a tool suite and a runtime component.
We’ll explore the architecture of the runtime piece ﬁrst and then get into tool
architecture in the following section.
Figure 1.15 shows all of the major runtime components that make up a
typical 3D game engine. Yeah, it’s big! And this diagram doesn’t even account
for all the tools. Game engines are deﬁnitely large software systems.
Like all software systems, game engines are built in layers. Normally up-
per layers depend on lower layers, but not vice versa. When a lower layer

1.6. Runtime Engine Architecture
33
Gameplay Foundations
Event/Messaging 
System
Dynamic Game 
Object Model
Scripting System
World Loading / 
Streaming
Static World 
Elements
Real-Time Agent-
Based Simulation
High-Level Game Flow System/FSM
Skeletal Animation
Animation 
Decompression
Inverse 
Kinematics (IK)
Game-Specific 
Post-Processing
Sub-skeletal 
Animation
LERP and 
Additive Blending
Animation 
Playback
Animation State 
Tree & Layers
Profiling & Debugging
Memory & 
Performance Stats
In-Game Menus 
or Console
Recording & 
Playback
Hierarchical 
Object Attachment
3rd Party SDKs
Havok, PhysX, 
ODE etc.
DirectX, OpenGL, 
libgcm, Edge, etc.
Boost++
STL / STLPort
etc.
Kynapse
Euphoria
Granny, Havok 
Animation, etc.
OS
Drivers
Hardware (PC, XBOX360, PS3, etc.)
Platform Independence Layer
Atomic Data 
Types
Platform Detection
Collections and 
Iterators
Threading Library
Hi-Res Timer
File System
Network Transport 
Layer (UDP/TCP)
Graphics 
Wrappers
Physics/Coll. 
Wrapper
Core Systems
Module Start-Up 
and Shut-Down
Parsers (CSV, 
XML, etc.)
Assertions
Unit Testing
Math Library
Strings and 
Hashed String Ids
Debug Printing 
and Logging
Memory Allocation
Engine Config
(INI files etc.)
Profiling / Stats 
Gathering
Object Handles / 
Unique Ids
RTTI / Reflection 
& Serialization
Curves & 
Surfaces Library
Random Number 
Generator
Localization 
Services
Asynchronous
File I/O
Movie Player
Memory Card I/O 
(Older Consoles)
Resources (Game Assets)
Resource Manager
Texture 
Resource
Material 
Resource
3D Model 
Resource
Font 
Resource
Collision 
Resource
Physics 
Parameters
Game
World/Map
etc.
Skeleton 
Resource
Human Interface 
Devices (HID)
Physical Device
I/O
Game-Specific 
Interface
Audio
Audio Playback / 
Management
DSP/Effects
3D Audio Model
Online Multiplayer
Match-Making & 
Game Mgmt.
Game State 
Replication
Object Authority 
Policy
Scene Graph / Culling Optimizations
LOD System
Occlusion & PVS
Spatial Hash (BSP 
Tree, kd-Tree, …)
Visual Effects
Particle & Decal 
Systems
Post Effects
HDR Lighting
PRT Lighting, 
Subsurf. Scatter
Environment 
Mapping
Light Mapping & 
Dynamic Shadows
Front End
Heads-Up Display 
(HUD)
Full-Motion Video 
(FMV)
In-Game Menus
In-Game GUI
Wrappers / Attract 
Mode
In-Game Cinematics 
(IGC)
Collision & Physics
Shapes/
Collidables
Rigid Bodies
Phantoms
Ray/Shape 
Casting (Queries)
Forces & 
Constraints
Physics/Collision 
World
Ragdoll 
Physics
GAME-SPECIFIC SUBSYSTEMS
Game-Specific Rendering
Terrain Rendering
Water Simulation 
& Rendering
etc.
Player Mechanics
Collision Manifold
Movement
State Machine & 
Animation
Game Cameras
Player-Follow 
Camera
Debug Fly-
Through Cam
Fixed Cameras
Scripted/Animated 
Cameras
AI
Sight Traces & 
Perception
Path Finding (A*)
Goals & Decision-
Making
Actions
(Engine Interface)
Camera-Relative 
Controls (HID)
Weapons
Power-Ups
etc.
Vehicles
Puzzles
Low-Level Renderer
Primitive 
Submission
Viewports & 
Virtual Screens
Materials & 
Shaders
Texture and 
Surface Mgmt.
Graphics Device Interface
Static & Dynamic 
Lighting
Cameras
Text & Fonts
Debug Drawing
(Lines etc.)
Skeletal Mesh 
Rendering
Figure 1.15. Runtime game engine architecture.

34
1. Introduction
depends upon a higher layer, we call this a circular dependency. Dependency
cycles are to be avoided in any software system, because they lead to unde-
sirable coupling between systems, make the software untestable and inhibit
code reuse. This is especially true for a large-scale system like a game engine.
What follows is a brief overview of the components shown in the diagram
in Figure 1.15. The rest of this book will be spent investigating each of these
components in a great deal more depth and learning how these components
are usually integrated into a functional whole.
1.6.1
Target Hardware
The target hardware layer, shown in isolation in Figure 1.16, represents the
computer system or console on which the game will run. Typical platforms
include Microsoft Windows, Linux and MacOS-based PCs; mobile platforms
like the Apple iPhone and iPad, Android smart phones and tablets, Sony’s
PlayStation Vita and Amazon’s Kindle Fire (among others); and game con-
soles like Microsoft’s Xbox, Xbox 360 and Xbox One, Sony’s PlayStation, Play-
Station 2, PlayStation 3 and PlayStation 4, and Nintendo’s DS, GameCube, Wii
and Wii U. Most of the topics in this book are platform-agnostic, but we’ll also
touch on some of the design considerations peculiar to PC or console devel-
opment, where the distinctions are relevant.
Hardware (PC, XBOX360, PS3, etc.)
Figure 1.16. Hardware layer.
1.6.2
Device Drivers
As depicted in Figure 1.17, device drivers are low-level software components
provided by the operating system or hardware vendor. Drivers manage hard-
ware resources and shield the operating system and upper engine layers from
the details of communicating with the myriad variants of hardware devices
available.
Drivers
Figure 1.17. Device driver layer.

1.6. Runtime Engine Architecture
35
OS
Figure 1.18. Operating system layer.
1.6.3
Operating System
On a PC, the operating system (OS) is running all the time. It orchestrates the
execution of multiple programs on a single computer, one of which is your
game. The OS layer is shown in Figure 1.18. Operating systems like Microsoft
Windows employ a time-sliced approach to sharing the hardware with multi-
ple running programs, known as preemptive multitasking. This means that a
PC game can never assume it has full control of the hardware—it must “play
nice” with other programs in the system.
On a console, the operating system is often just a thin library layer that is
compiled directly into your game executable. On a console, the game typically
“owns” the entire machine. However, with the introduction of the Xbox 360
and PlayStation 3, this was no longer strictly the case. The operating sys-
tem on these consoles and their successors, the Xbox One and PlayStation 4
respectively, can interrupt the execution of your game, or take over certain
system resources, in order to display online messages, or to allow the player
to pause the game and bring up the PS3’s Xross Media Bar or the Xbox 360’s
dashboard, for example. So the gap between console and PC development is
gradually closing (for better or for worse).
1.6.4
Third-Party SDKs and Middleware
Most game engines leverage a number of third-party software development
kits (SDKs) and middleware, as shown in Figure 1.19. The functional or class-
based interface provided by an SDK is often called an application program-
ming interface (API). We will look at a few examples.
3rd Party SDKs
Havok, PhysX, 
ODE etc.
DirectX, OpenGL, 
libgcm, Edge, etc.
Boost++
STL / STLPort
etc.
Kynapse
Euphoria
Granny, Havok 
Animation, etc.
Figure 1.19. Third-party SDK layer.
1.6.4.1
Data Structures and Algorithms
Like any software system, games depend heavily on collection data structures
and algorithms to manipulate them. Here are a few examples of third-party
libraries which provide these kinds of services:

36
1. Introduction
•
STL. The C++ standard template library provides a wealth of code and
algorithms for managing data structures, strings and stream-based I/O.
•
STLport. This is a portable, optimized implementation of STL.
•
Boost. Boost is a powerful data structures and algorithms library, de-
signed in the style of STL. (The online documentation for Boost is also a
great place to learn a great deal about computer science!)
•
Loki. Loki is a powerful generic programming template library which is
exceedingly good at making your brain hurt!
Game developers are divided on the question of whether to use template
libraries like STL in their game engines. Some believe that the memory alloca-
tion patterns of STL, which are not conducive to high-performance program-
ming and tend to lead to memory fragmentation (see Section 5.2.1.4), make
STL unusable in a game. Others feel that the power and convenience of STL
outweigh its problems and that most of the problems can in fact be worked
around anyway. My personal belief is that STL is all right for use on a PC, be-
cause its advanced virtual memory system renders the need for careful mem-
ory allocation a bit less crucial (although one must still be very careful). On
a console, with limited or no virtual memory facilities and exorbitant cache-
miss costs, you’re probably better off writing custom data structures that have
predictable and/or limited memory allocation patterns. (And you certainly
won’t go far wrong doing the same on a PC game project either.)
1.6.4.2
Graphics
Most game rendering engines are built on top of a hardware interface library,
such as the following:
•
Glide is the 3D graphics SDK for the old Voodoo graphics cards. This
SDK was popular prior to the era of hardware transform and lighting
(hardware T&L) which began with DirectX 7.
•
OpenGL is a widely used portable 3D graphics SDK.
•
DirectX is Microsoft’s 3D graphics SDK and primary rival to OpenGL.
•
libgcm is a low-level direct interface to the PlayStation 3’s RSX graphics
hardware, which was provided by Sony as a more efﬁcient alternative
to OpenGL.
•
Edge is a powerful and highly efﬁcient rendering and animation engine
produced by Naughty Dog and Sony for the PlayStation 3 and used by
a number of ﬁrst- and third-party game studios.

1.6. Runtime Engine Architecture
37
1.6.4.3
Collision and Physics
Collision detection and rigid body dynamics (known simply as “physics”
in the game development community) are provided by the following well-
known SDKs:
•
Havok is a popular industrial-strength physics and collision engine.
•
PhysX is another popular industrial-strength physics and collision en-
gine, available for free download from NVIDIA.
•
Open Dynamics Engine (ODE) is a well-known open source physics/col-
lision package.
1.6.4.4
Character Animation
A number of commercial animation packages exist, including but certainly
not limited to the following:
•
Granny. Rad Game Tools’ popular Granny toolkit includes robust 3D
model and animation exporters for all the major 3D modeling and ani-
mation packages like Maya, 3D Studio MAX, etc., a runtime library for
reading and manipulating the exported model and animation data, and
a powerful runtime animation system. In my opinion, the Granny SDK
has the best-designed and most logical animation API of any I’ve seen,
commercial or proprietary, especially its excellent handling of time.
•
Havok Animation. The line between physics and animation is becoming
increasingly blurred as characters become more and more realistic. The
company that makes the popular Havok physics SDK decided to create
a complimentary animation SDK, which makes bridging the physics-
animation gap much easier than it ever has been.
•
Edge. The Edge library produced for the PS3 by the ICE team at Naughty
Dog, the Tools and Technology group of Sony Computer Entertainment
America, and Sony’s Advanced Technology Group in Europe includes
a powerful and efﬁcient animation engine and an efﬁcient geometry-
processing engine for rendering.
1.6.4.5
Biomechanical Character Models
•
Endorphin and Euphoria.
These are animation packages that produce
character motion using advanced biomechanical models of realistic hu-
man movement.
As we mentioned previously, the line between character animation and
physics is beginning to blur. Packages like Havok Animation try to marry

38
1. Introduction
physics and animation in a traditional manner, with a human animator pro-
viding the majority of the motion through a tool like Maya and with physics
augmenting that motion at runtime. But recently a ﬁrm called Natural Motion
Ltd. has produced a product that attempts to redeﬁne how character motion
is handled in games and other forms of digital media.
Its ﬁrst product, Endorphin, is a Maya plug-in that permits animators to
run full biomechanical simulations on characters and export the resulting an-
imations as if they had been hand animated. The biomechanical model ac-
counts for center of gravity, the character’s weight distribution, and detailed
knowledge of how a real human balances and moves under the inﬂuence of
gravity and other forces.
Its second product, Euphoria, is a real-time version of Endorphin intended
to produce physically and biomechanically accurate character motion at run-
time under the inﬂuence of unpredictable forces.
1.6.5
Platform Independence Layer
Most game engines are required to be capable of running on more than one
hardware platform. Companies like Electronic Arts and ActivisionBlizzard
Inc., for example, always target their games at a wide variety of platforms be-
cause it exposes their games to the largest possible market. Typically, the only
game studios that do not target at least two different platforms per game are
ﬁrst-party studios, like Sony’s Naughty Dog and Insomniac studios. There-
fore, most game engines are architected with a platform independence layer,
like the one shown in Figure 1.20. This layer sits atop the hardware, drivers,
operating system and other third-party software and shields the rest of the
engine from the majority of knowledge of the underlying platform.
By wrapping or replacing the most commonly used standard C library
functions, operating system calls and other foundational application program-
ming interfaces (APIs), the platform independence layer ensures consistent
behavior across all hardware platforms. This is necessary because there is a
good deal of variation across platforms, even among “standardized” libraries
like the standard C library.
Platform Independence Layer
Atomic Data 
Types
Platform Detection
Collections and 
Iterators
Threading Library
Hi-Res Timer
File System
Network Transport 
Layer (UDP/TCP)
Graphics 
Wrappers
Physics/Coll. 
Wrapper
Figure 1.20. Platform independence layer.

1.6. Runtime Engine Architecture
39
Core Systems
Module Start-Up 
and Shut-Down
Parsers (CSV, 
XML, etc.)
Assertions
Unit Testing
Math Library
Strings and 
Hashed String Ids
Debug Printing 
and Logging
Memory Allocation
Engine Config
(INI files etc.)
Profiling / Stats 
Gathering
Object Handles / 
Unique Ids
RTTI / Reflection 
& Serialization
Curves & 
Surfaces Library
Random Number 
Generator
Localization 
Services
Asynchronous
File I/O
Movie Player
Memory Card I/O 
(Older Consoles)
Figure 1.21. Core engine systems.
1.6.6
Core Systems
Every game engine, and really every large, complex C++ software application,
requires a grab bag of useful software utilities. We’ll categorize these under
the label “core systems.” A typical core systems layer is shown in Figure 1.21.
Here are a few examples of the facilities the core layer usually provides:
•
Assertions are lines of error-checking code that are inserted to catch log-
ical mistakes and violations of the programmer’s original assumptions.
Assertion checks are usually stripped out of the ﬁnal production build
of the game.
•
Memory management. Virtually every game engine implements its own
custom memory allocation system(s) to ensure high-speed allocations
and deallocations and to limit the negative effects of memory fragmen-
tation (see Section 5.2.1.4).
•
Math library. Games are by their nature highly mathematics-intensive.
As such, every game engine has at least one, if not many, math libraries.
These libraries provide facilities for vector and matrix math, quaternion
rotations, trigonometry, geometric operations with lines, rays, spheres,
frusta, etc., spline manipulation, numerical integration, solving systems
of equations and whatever other facilities the game programmers re-
quire.
•
Custom data structures and algorithms. Unless an engine’s designers de-
cided to rely entirely on a third-party package such as STL, a suite of
tools for managing fundamental data structures (linked lists, dynamic
arrays, binary trees, hash maps, etc.) and algorithms (search, sort, etc.)
is usually required. These are often hand coded to minimize or elimi-
nate dynamic memory allocation and to ensure optimal runtime perfor-
mance on the target platform(s).
A detailed discussion of the most common core engine systems can be
found in Part II.

40
1. Introduction
1.6.7
Resource Manager
Present in every game engine in some form, the resource manager provides a
uniﬁed interface (or suite of interfaces) for accessing any and all types of game
assets and other engine input data. Some engines do this in a highly cen-
tralized and consistent manner (e.g., Unreal’s packages, OGRE’s Resource-
Manager class). Other engines take an ad hoc approach, often leaving it up
to the game programmer to directly access raw ﬁles on disk or within com-
pressed archives such as Quake’s PAK ﬁles. A typical resource manager layer
is depicted in Figure 1.22.
Resources (Game Assets)
Resource Manager
Texture 
Resource
Material 
Resource
3D Model 
Resource
Font
Resource
Collision 
Resource
Physics
Parameters
Game
World/Map
etc.
Skeleton 
Resource
Figure 1.22. Resource manager.
1.6.8
Rendering Engine
The rendering engine is one of the largest and most complex components of
any game engine. Renderers can be architected in many different ways. There
is no one accepted way to do it, although as we’ll see, most modern rendering
engines share some fundamental design philosophies, driven in large part by
the design of the 3D graphics hardware upon which they depend.
One common and effective approach to rendering engine design is to em-
ploy a layered architecture as follows.
1.6.8.1
Low-Level Renderer
The low-level renderer, shown in Figure 1.23, encompasses all of the raw ren-
dering facilities of the engine. At this level, the design is focused on rendering
a collection of geometric primitives as quickly and richly as possible, without
much regard for which portions of a scene may be visible. This component is
broken into various subcomponents, which are discussed below.
Graphics Device Interface
Graphics SDKs, such as DirectX and OpenGL, require a reasonable amount of
code to be written just to enumerate the available graphics devices, initialize
them, set up render surfaces (back-buffer, stencil buffer, etc.) and so on. This

1.6. Runtime Engine Architecture
41
Low-Level Renderer
Primitive
Submission
Viewports & 
Virtual Screens
Materials & 
Shaders
Texture and 
Surface Mgmt.
Graphics Device Interface
Static & Dynamic 
Lighting
Cameras
Text & Fonts
Debug Drawing
(Lines etc.)
Skeletal Mesh 
Rendering
Figure 1.23. Low-level rendering engine.
is typically handled by a component that I’ll call the graphics device interface
(although every engine uses its own terminology).
For a PC game engine, you also need code to integrate your renderer with
the Windows message loop. You typically write a “message pump” that ser-
vices Windows messages when they are pending and otherwise runs your
render loop over and over as fast as it can. This ties the game’s keyboard
polling loop to the renderer’s screen update loop. This coupling is undesir-
able, but with some effort it is possible to minimize the dependencies. We’ll
explore this topic in more depth later.
Other Renderer Components
The other components in the low-level renderer cooperate in order to collect
submissions of geometric primitives (sometimes called render packets), such as
meshes, line lists, point lists, particles, terrain patches, text strings and what-
ever else you want to draw, and render them as quickly as possible.
The low-level renderer usually provides a viewport abstraction with an as-
sociated camera-to-world matrix and 3D projection parameters, such as ﬁeld
of view and the location of the near and far clip planes. The low-level renderer
also manages the state of the graphics hardware and the game’s shaders via
its material system and its dynamic lighting system. Each submitted primitive
is associated with a material and is affected by n dynamic lights. The mate-
rial describes the texture(s) used by the primitive, what device state settings
need to be in force, and which vertex and pixel shader to use when rendering
the primitive. The lights determine how dynamic lighting calculations will
be applied to the primitive. Lighting and shading is a complex topic, which
is covered in depth in many excellent books on computer graphics, includ-
ing [14], [44] and [1].

42
1. Introduction
k
Figure 1.24. A typical scene graph/spatial subdivision layer, for culling optimization.
1.6.8.2
Scene Graph/Culling Optimizations
The low-level renderer draws all of the geometry submitted to it, without
much regard for whether or not that geometry is actually visible (other than
back-face culling and clipping triangles to the camera frustum). A higher-
level component is usually needed in order to limit the number of primitives
submitted for rendering, based on some form of visibility determination. This
layer is shown in Figure 1.24.
For very small game worlds, a simple frustum cull (i.e., removing objects
that the camera cannot “see”) is probably all that is required. For larger game
worlds, a more advanced spatial subdivision data structure might be used to
improve rendering efﬁciency by allowing the potentially visible set (PVS) of
objects to be determined very quickly. Spatial subdivisions can take many
forms, including a binary space partitioning tree, a quadtree, an octree, a kd-
tree or a sphere hierarchy. A spatial subdivision is sometimes called a scene
graph, although technically the latter is a particular kind of data structure and
does not subsume the former. Portals or occlusion culling methods might also
be applied in this layer of the rendering engine.
Ideally, the low-level renderer should be completely agnostic to the type
of spatial subdivision or scene graph being used. This permits different game
teams to reuse the primitive submission code but to craft a PVS determination
system that is speciﬁc to the needs of each team’s game. The design of the
OGRE open source rendering engine (http://www.ogre3d.org) is a great ex-
ample of this principle in action. OGRE provides a plug-and-play scene graph
architecture. Game developers can either select from a number of preimple-
mented scene graph designs, or they can provide a custom scene graph im-
plementation.
1.6.8.3
Visual Effects
Modern game engines support a wide range of visual effects, as shown in
Figure 1.25, including:

1.6. Runtime Engine Architecture
43
Figure 1.25. Visual effects.
•
particle systems (for smoke, ﬁre, water splashes, etc.);
•
decal systems (for bullet holes, foot prints, etc.);
•
light mapping and environment mapping;
•
dynamic shadows; and
•
full-screen post effects, applied after the 3D scene has been rendered to
an off-screen buffer.
Some examples of full-screen post effects include:
•
high dynamic range (HDR) tone mapping and bloom;
•
full-screen anti-aliasing (FSAA); and
•
color correction and color-shift effects, including bleach bypass, satura-
tion and desaturation effects, etc.
It is common for a game engine to have an effects system component that
manages the specialized rendering needs of particles, decals and other visual
effects. The particle and decal systems are usually distinct components of the
rendering engine and act as inputs to the low-level renderer. On the other
hand, light mapping, environment mapping and shadows are usually han-
dled internally within the rendering engine proper. Full-screen post effects
are either implemented as an integral part of the renderer or as a separate
component that operates on the renderer’s output buffers.
1.6.8.4
Front End
Most games employ some kind of 2D graphics overlaid on the 3D scene for
various purposes. These include:
•
the game’s heads-up display (HUD);
•
in-game menus, a console and/or other development tools, which may or
may not be shipped with the ﬁnal product; and

44
1. Introduction
Front End
Heads-Up Display 
(HUD)
Full-Motion Video 
(FMV)
In-Game Menus
In-Game GUI
Wrappers / Attract 
Mode
In-Game Cinematics 
(IGC)
Figure 1.26. Front end graphics.
•
possibly an in-game graphical user interface (GUI), allowing the player to
manipulate his or her character’s inventory, conﬁgure units for battle or
perform other complex in-game tasks.
This layer is shown in Figure 1.26. Two-dimensional graphics like these are
usually implemented by drawing textured quads (pairs of triangles) with an
orthographic projection. Or they may be rendered in full 3D, with the quads
bill-boarded so they always face the camera.
We’ve also included the full-motion video (FMV) system in this layer. This
system is responsible for playing full-screen movies that have been recorded
earlier (either rendered with the game’s rendering engine or using another
rendering package).
A related system is the in-game cinematics (IGC) system. This component
typically allows cinematic sequences to be choreographed within the game it-
self, in full 3D. For example, as the player walks through a city, a conversation
between two key characters might be implemented as an in-game cinematic.
IGCs may or may not include the player character(s). They may be done as a
deliberate cut-away during which the player has no control, or they may be
subtly integrated into the game without the human player even realizing that
an IGC is taking place.
1.6.9
Proﬁling and Debugging Tools
Figure 1.27. Proﬁling
and debugging tools.
Games are real-time systems and, as such, game engineers often need to pro-
ﬁle the performance of their games in order to optimize performance. In ad-
dition, memory resources are usually scarce, so developers make heavy use
of memory analysis tools as well. The proﬁling and debugging layer, shown
in Figure 1.27, encompasses these tools and also includes in-game debugging
facilities, such as debug drawing, an in-game menu system or console and
the ability to record and play back gameplay for testing and debugging pur-
poses.

1.6. Runtime Engine Architecture
45
There are plenty of good general-purpose software proﬁling tools avail-
able, including:
•
Intel’s VTune,
•
IBM’s Quantify and Purify (part of the PurifyPlus tool suite), and
•
Compuware’s Bounds Checker.
However, most game engines also incorporate a suite of custom proﬁling
and debugging tools. For example, they might include one or more of the
following:
•
a mechanism for manually instrumenting the code, so that speciﬁc sec-
tions of code can be timed;
•
a facility for displaying the proﬁling statistics on-screen while the game
is running;
•
a facility for dumping performance stats to a text ﬁle or to an Excel
spreadsheet;
•
a facility for determining how much memory is being used by the en-
gine, and by each subsystem, including various on-screen displays;
•
the ability to dump memory usage, high water mark and leakage stats
when the game terminates and/or during gameplay;
•
tools that allow debug print statements to be peppered throughout the
code, along with an ability to turn on or off different categories of debug
output and control the level of verbosity of the output; and
•
the ability to record game events and then play them back. This is tough
to get right, but when done properly it can be a very valuable tool for
tracking down bugs.
The PlayStation 4 provides a powerful core dump facility to aid program-
mers in debugging crashes. The PlayStation 4 is always recording the last 15
seconds of gameplay video, to allow players to share their experiences via the
Share button on the controller. Because of this, the PS4’s core dump facility
automatically provides programmers not only with a complete call stack of
what the program was doing when it crashed, but also with a screenshot of
the moment of the crash and 15 seconds of video footage showing what was
happening just prior to the crash. Core dumps can be automatically uploaded
to the game developer’s servers whenever the game crashes, even after the
game has shipped. These facilities revolutionize the tasks of crash analysis
and repair.

46
1. Introduction
Figure 1.28. Collision and physics subsystem.
1.6.10
Collision and Physics
Collision detection is important for every game. Without it, objects would
interpenetrate, and it would be impossible to interact with the virtual world
in any reasonable way. Some games also include a realistic or semi-realistic
dynamics simulation. We call this the “physics system” in the game industry,
although the term rigid body dynamics is really more appropriate, because we
are usually only concerned with the motion (kinematics) of rigid bodies and
the forces and torques (dynamics) that cause this motion to occur. This layer
is depicted in Figure 1.28.
Collision and physics are usually quite tightly coupled. This is because
when collisions are detected, they are almost always resolved as part of the
physics integration and constraint satisfaction logic. Nowadays, very few
game companies write their own collision/physics engine. Instead, a third-
party SDK is typically integrated into the engine.
•
Havok is the gold standard in the industry today. It is feature-rich and
performs well across the boards.
•
PhysX by NVIDIA is another excellent collision and dynamics engine.
It was integrated into Unreal Engine 4 and is also available for free as
a stand-alone product for PC game development. PhysX was originally
designed as the interface to Ageia’s new physics accelerator chip. The
SDK is now owned and distributed by NVIDIA, and the company has
adapted PhysX to run on its latest GPUs.
Open source physics and collision engines are also available. Perhaps the
best-known of these is the Open Dynamics Engine (ODE). For more informa-

1.6. Runtime Engine Architecture
47
tion, see http://www.ode.org. I-Collide, V-Collide and RAPID are other pop-
ular non-commercial collision detection engines. All three were developed
at the University of North Carolina (UNC). For more information, see http://
www.cs.unc.edu/~geom/I_COLLIDE/index.html and http://www.cs.unc.
edu/∼geom/V_COLLIDE/index.html.
1.6.11
Animation
Any game that has organic or semi-organic characters (humans, animals, car-
toon characters or even robots) needs an animation system. There are ﬁve
basic types of animation used in games:
•
sprite/texture animation,
•
rigid body hierarchy animation,
•
skeletal animation,
•
vertex animation, and
•
morph targets.
Skeletal animation permits a detailed 3D character mesh to be posed by an
animator using a relatively simple system of bones. As the bones move, the
vertices of the 3D mesh move with them. Although morph targets and vertex
animation are used in some engines, skeletal animation is the most prevalent
animation method in games today; as such, it will be our primary focus in this
book. A typical skeletal animation system is shown in Figure 1.29.
You’ll notice in Figure 1.15 that the skeletal mesh rendering component
bridges the gap between the renderer and the animation system. There is a
tight cooperation happening here, but the interface is very well deﬁned. The
Skeletal Animation
Animation
Decompression
Inverse
Kinematics (IK)
Game-Specific
Post-Processing
Sub-skeletal
Animation
LERP and 
Additive Blending
Animation
Playback
Animation State 
Tree & Layers
Figure 1.29. Skeletal animation subsystem.

48
1. Introduction
animation system produces a pose for every bone in the skeleton, and then
these poses are passed to the rendering engine as a palette of matrices. The
renderer transforms each vertex by the matrix or matrices in the palette, in
order to generate a ﬁnal blended vertex position. This process is known as
skinning.
There is also a tight coupling between the animation and physics systems
when rag dolls are employed. A rag doll is a limp (often dead) animated char-
acter, whose bodily motion is simulated by the physics system. The physics
system determines the positions and orientations of the various parts of the
body by treating them as a constrained system of rigid bodies. The animation
system calculates the palette of matrices required by the rendering engine in
order to draw the character on-screen.
1.6.12
Human Interface Devices (HID)
Figure 1.30.
The
player input/output
system, also known
as the human in-
terface device (HID)
layer.
Every game needs to process input from the player, obtained from various
human interface devices (HIDs) including:
•
the keyboard and mouse,
•
a joypad, or
•
other specialized game controllers, like steering wheels, ﬁshing rods,
dance pads, the Wiimote, etc.
We sometimes call this component the player I/O component, because
we may also provide output to the player through the HID, such as force-
feedback/ rumble on a joypad or the audio produced by the Wiimote. A typ-
ical HID layer is shown in Figure 1.30.
The HID engine component is sometimes architected to divorce the low-
level details of the game controller(s) on a particular hardware platform from
the high-level game controls.
It massages the raw data coming from the
hardware, introducing a dead zone around the center point of each joypad
stick, debouncing button-press inputs, detecting button-down and button-
up events, interpreting and smoothing accelerometer inputs (e.g., from the
PlayStation Dualshock controller) and more. It often provides a mechanism
allowing the player to customize the mapping between physical controls and
logical game functions. It sometimes also includes a system for detecting
chords (multiple buttons pressed together), sequences (buttons pressed in se-
quence within a certain time limit) and gestures (sequences of inputs from the
buttons, sticks, accelerometers, etc.).

1.6. Runtime Engine Architecture
49
1.6.13
Audio
Figure 1.31.
Audio
subsystem.
Audio is just as important as graphics in any game engine. Unfortunately, au-
dio often gets less attention than rendering, physics, animation, AI and game-
play. Case in point: Programmers often develop their code with their speak-
ers turned off! (In fact, I’ve known quite a few game programmers who didn’t
even have speakers or headphones.) Nonetheless, no great game is complete
without a stunning audio engine. The audio layer is depicted in Figure 1.31.
Audio engines vary greatly in sophistication. Quake’s audio engine is
pretty basic, and game teams usually augment it with custom functionality
or replace it with an in-house solution. Unreal Engine 4 provides a reasonably
robust 3D audio rendering engine (discussed in detail in [40]), although its fea-
ture set is limited and many game teams will probably want to augment and
customize it to provide advanced game-speciﬁc features. For DirectX plat-
forms (PC, Xbox 360, Xbox One), Microsoft provides an excellent audio tool
suite called XACT, supported at runtime by their feature-rich XAudio2 and
X3DAudio APIs. Electronic Arts has developed an advanced, high-powered
audio engine internally called SoundR!OT. In conjunction with ﬁrst-party stu-
dios like Naughty Dog, Sony Computer Entertainment America (SCEA) pro-
vides a powerful 3D audio engine called Scream, which has been used on a
number of PS3 titles including Naughty Dog’s Uncharted 3: Drake’s Deception
and The Last of Us. However, even if a game team uses a preexisting audio
engine, every game requires a great deal of custom software development,
integration work, ﬁne-tuning and attention to detail in order to produce high-
quality audio in the ﬁnal product.
1.6.14
Online Multiplayer/Networking
Many games permit multiple human players to play within a single virtual
world. Multiplayer games come in at least four basic ﬂavors:
•
Single-screen multiplayer. Two or more human interface devices (joypads,
keyboards, mice, etc.) are connected to a single arcade machine, PC or
console. Multiple player characters inhabit a single virtual world, and a
single camera keeps all player characters in frame simultaneously. Ex-
amples of this style of multiplayer gaming include Smash Brothers, Lego
Star Wars and Gauntlet.
•
Split-screen multiplayer. Multiple player characters inhabit a single vir-
tual world, with multiple HIDs attached to a single game machine, but
each with its own camera, and the screen is divided into sections so that
each player can view his or her character.

50
1. Introduction
•
Networked multiplayer. Multiple computers or consoles are networked
together, with each machine hosting one of the players.
•
Massively multiplayer online games (MMOG). Literally hundreds of thou-
sands of users can be playing simultaneously within a giant, persistent,
online virtual world hosted by a powerful battery of central servers.
Figure 1.32.
On-
line multiplayer net-
working subsystem.
The multiplayer networking layer is shown in Figure 1.32.
Multiplayer games are quite similar in many ways to their single-player
counterparts. However, support for multiple players can have a profound
impact on the design of certain game engine components. The game world
object model, renderer, human input device system, player control system
and animation systems are all affected. Retroﬁtting multiplayer features into
a preexisting single-player engine is certainly not impossible, although it can
be a daunting task. Still, many game teams have done it successfully. That
said, it is usually better to design multiplayer features from day one, if you
have that luxury.
It is interesting to note that going the other way—converting a multiplayer
game into a single-player game—is typically trivial. In fact, many game en-
gines treat single-player mode as a special case of a multiplayer game, in
which there happens to be only one player. The Quake engine is well known
for its client-on-top-of-server mode, in which a single executable, running on a
single PC, acts both as the client and the server in single-player campaigns.
1.6.15
Gameplay Foundation Systems
The term gameplay refers to the action that takes place in the game, the rules
that govern the virtual world in which the game takes place, the abilities of the
player character(s) (known as player mechanics) and of the other characters and
objects in the world, and the goals and objectives of the player(s). Gameplay
is typically implemented either in the native language in which the rest of the
engine is written or in a high-level scripting language—or sometimes both. To
bridge the gap between the gameplay code and the low-level engine systems
that we’ve discussed thus far, most game engines introduce a layer that I’ll
call the gameplay foundations layer (for lack of a standardized name). Shown
in Figure 1.33, this layer provides a suite of core facilities, upon which game-
speciﬁc logic can be implemented conveniently.
1.6.15.1
Game Worlds and Object Models
The gameplay foundations layer introduces the notion of a game world, con-
taining both static and dynamic elements. The contents of the world are usu-
ally modeled in an object-oriented manner (often, but not always, using an

1.6. Runtime Engine Architecture
51
Gameplay Foundations
Event/Messaging 
System
Dynamic Game 
Object Model
Scripting System
World Loading / 
Streaming
Static World 
Elements
Real-Time Agent-
Based Simulation
High-Level Game Flow System/FSM
Hierarchical 
Object Attachment
Figure 1.33. Gameplay foundation systems.
object-oriented programming language). In this book, the collection of object
types that make up a game is called the game object model. The game object
model provides a real-time simulation of a heterogeneous collection of objects
in the virtual game world.
Typical types of game objects include:
•
static background geometry, like buildings, roads, terrain (often a spe-
cial case), etc.;
•
dynamic rigid bodies, such as rocks, soda cans, chairs, etc.;
•
player characters (PC);
•
non-player characters (NPC);
•
weapons;
•
projectiles;
•
vehicles;
•
lights (which may be present in the dynamic scene at runtime, or only
used for static lighting ofﬂine);
•
cameras;
and the list goes on.
The game world model is intimately tied to a software object model, and
this model can end up pervading the entire engine. The term software object
model refers to the set of language features, policies and conventions used to
implement a piece of object-oriented software. In the context of game engines,
the software object model answers questions, such as:

52
1. Introduction
•
Is your game engine designed in an object-oriented manner?
•
What language will you use? C? C++? Java? OCaml?
•
How will the static class hierarchy be organized? One giant monolithic
hierarchy? Lots of loosely coupled components?
•
Will you use templates and policy-based design, or traditional polymor-
phism?
•
How are objects referenced?
Straight old pointers?
Smart pointers?
Handles?
•
How will objects be uniquely identiﬁed? By address in memory only?
By name? By a global unique identiﬁer (GUID)?
•
How are the lifetimes of game objects managed?
•
How are the states of the game objects simulated over time?
We’ll explore software object models and game object models in consider-
able depth in Section 15.2.
1.6.15.2
Event System
Game objects invariably need to communicate with one another. This can
be accomplished in all sorts of ways. For example, the object sending the
message might simply call a member function of the receiver object. An event-
driven architecture, much like what one would ﬁnd in a typical graphical user
interface, is also a common approach to inter-object communication. In an
event-driven system, the sender creates a little data structure called an event
or message, containing the message’s type and any argument data that are to
be sent. The event is passed to the receiver object by calling its event handler
function. Events can also be stored in a queue for handling at some future
time.
1.6.15.3
Scripting System
Many game engines employ a scripting language in order to make devel-
opment of game-speciﬁc gameplay rules and content easier and more rapid.
Without a scripting language, you must recompile and relink your game ex-
ecutable every time a change is made to the logic or data structures used in
the engine. But when a scripting language is integrated into your engine,
changes to game logic and data can be made by modifying and reloading the
script code. Some engines allow script to be reloaded while the game contin-
ues to run. Other engines require the game to be shut down prior to script
recompilation. But either way, the turnaround time is still much faster than it
would be if you had to recompile and relink the game’s executable.

1.6. Runtime Engine Architecture
53
1.6.15.4
Artiﬁcial Intelligence Foundations
Traditionally, artiﬁcial intelligence has fallen squarely into the realm of game-
speciﬁc software—it was usually not considered part of the game engine per
se. More recently, however, game companies have recognized patterns that
arise in almost every AI system, and these foundations are slowly starting to
fall under the purview of the engine proper.
A company called Kynogon developed a middleware SDK named Ky-
napse, which provided much of the low-level technology required to build
commercially viable game AI. This technology was purchased by Autodesk
and has been superseded by a totally redesigned AI middleware package
called Gameware Navigation, designed by the same engineering team that in-
vented Kynapse. This SDK provides low-level AI building blocks such as nav
mesh generation, path ﬁnding, static and dynamic object avoidance, identiﬁ-
cation of vulnerabilities within a play space (e.g., an open window from which
an ambush could come) and a well-deﬁned interface between AI and anima-
tion. Autodesk also offers a visual programming system and runtime engine
called Gameware Cognition, which together with Gameware Navigation aims
to make building ambitious game AI systems easier than ever.
1.6.16
Game-Speciﬁc Subsystems
On top of the gameplay foundation layer and the other low-level engine com-
ponents, gameplay programmers and designers cooperate to implement the
features of the game itself. Gameplay systems are usually numerous, highly
varied and speciﬁc to the game being developed. As shown in Figure 1.34,
these systems include, but are certainly not limited to the mechanics of the
player character, various in-game camera systems, artiﬁcial intelligence for
the control of non-player characters, weapon systems, vehicles and the list
goes on. If a clear line could be drawn between the engine and the game, it
GAME-SPECIFIC SUBSYSTEMS
Game-Specific Rendering
Terrain Rendering
Water Simulation 
& Rendering
etc.
Player Mechanics
Collision Manifold
Movement
State Machine & 
Animation
Game Cameras
Player-Follow 
Camera
Debug Fly-
Through Cam
Fixed Cameras
Scripted/Animated 
Cameras
AI
Sight Traces & 
Perception
Path Finding (A*)
Goals & Decision-
Making
Actions
(Engine Interface)
Camera-Relative 
Controls (HID)
Weapons
Power-Ups
etc.
Vehicles
Puzzles
Figure 1.34. Game-speciﬁc subsystems.

54
1. Introduction
would lie between the game-speciﬁc subsystems and the gameplay founda-
tions layer. Practically speaking, this line is never perfectly distinct. At least
some game-speciﬁc knowledge invariably seeps down through the gameplay
foundations layer and sometimes even extends into the core of the engine
itself.
1.7
Tools and the Asset Pipeline
Any game engine must be fed a great deal of data, in the form of game assets,
conﬁguration ﬁles, scripts and so on. Figure 1.35 depicts some of the types of
game assets typically found in modern game engines. The thicker dark-grey
arrows show how data ﬂows from the tools used to create the original source
assets all the way through to the game engine itself. The thinner light-grey
arrows show how the various types of assets refer to or use other assets.
1.7.1
Digital Content Creation Tools
Games are multimedia applications by nature. A game engine’s input data
comes in a wide variety of forms, from 3D mesh data to texture bitmaps to
animation data to audio ﬁles. All of this source data must be created and
manipulated by artists. The tools that the artists use are called digital content
creation (DCC) applications.
A DCC application is usually targeted at the creation of one particular type
of data—although some tools can produce multiple data types. For example,
Autodesk’s Maya and 3ds Max are prevalent in the creation of both 3D meshes
and animation data. Adobe’s Photoshop and its ilk are aimed at creating and
editing bitmaps (textures). SoundForge is a popular tool for creating audio
clips. Some types of game data cannot be created using an off-the-shelf DCC
app. For example, most game engines provide a custom editor for laying
out game worlds. Still, some engines do make use of preexisting tools for
game world layout. I’ve seen game teams use 3ds Max or Maya as a world
layout tool, with or without custom plug-ins to aid the user. Ask most game
developers, and they’ll tell you they can remember a time when they laid
out terrain height ﬁelds using a simple bitmap editor, or typed world layouts
directly into a text ﬁle by hand. Tools don’t have to be pretty—game teams
will use whatever tools are available and get the job done. That said, tools
must be relatively easy to use, and they absolutely must be reliable, if a game
team is going to be able to develop a highly polished product in a timely
manner.

1.7. Tools and the Asset Pipeline
55
Digital Content Creation (DCC) Tools
Game World
Game 
Object
Mesh
Skeletal Hierarchy 
Exporter
Skel. 
Hierarchy
Animation 
Exporter
Animation 
Curves
TGA
Texture
DXT Compression
DXT 
Texture
World Editor
Game Object 
Definition Tool
Material
Game Obj. 
Template
Animation 
Set
Animation Tree 
Editor
Animation 
Tree
Game 
Object
Game 
Object
Asset 
Conditioning 
Pipeline
GAME
WAV
sound
Audio Manager 
Tool
Sound 
Bank
Mesh Exporter
Photoshop
Photoshop
Sound Forge or Audio Tool
Sound Forge or Audio Tool
Game 
Object
Maya, 3DSMAX, etc.
Maya, 3DSMAX, etc.
Custom Material 
Plug-In
Houdini/Other Particle Tool
Houdini/Other Particle Tool
Particle 
System
Particle Exporter
Figure 1.35. Tools and the asset pipeline.
1.7.2
The Asset Conditioning Pipeline
The data formats used by digital content creation (DCC) applications are rarely
suitable for direct use in-game. There are two primary reasons for this.
1.
The DCC app’s in-memory model of the data is usually much more com-
plex than what the game engine requires. For example, Maya stores a di-
rected acyclic graph (DAG) of scene nodes, with a complex web of inter-
connections. It stores a history of all the edits that have been performed
on the ﬁle. It represents the position, orientation and scale of every ob-
ject in the scene as a full hierarchy of 3D transformations, decomposed
into translation, rotation, scale and shear components. A game engine

56
1. Introduction
typically only needs a tiny fraction of this information in order to render
the model in-game.
2.
The DCC application’s ﬁle format is often too slow to read at runtime,
and in some cases it is a closed proprietary format.
Therefore, the data produced by a DCC app is usually exported to a more
accessible standardized format, or a custom ﬁle format, for use in-game.
Once data has been exported from the DCC app, it often must be further
processed before being sent to the game engine. And if a game studio is ship-
ping its game on more than one platform, the intermediate ﬁles might be pro-
cessed differently for each target platform. For example, 3D mesh data might
be exported to an intermediate format, such as XML, JSON or a simple binary
format. Then it might be processed to combine meshes that use the same ma-
terial, or split up meshes that are too large for the engine to digest. The mesh
data might then be organized and packed into a memory image suitable for
loading on a speciﬁc hardware platform.
The pipeline from DCC app to game engine is sometimes called the asset
conditioning pipeline (ACP). Every game engine has this in some form.
1.7.2.1
3D Model/Mesh Data
The visible geometry you see in a game is typically constructed from triangle
meshes. Some older games also make use of volumetric geometry known as
brushes. We’ll discuss each type of geometric data brieﬂy below. For an in-
depth discussion of the techniques used to describe and render 3D geometry,
see Chapter 10.
3D Models (Meshes)
A mesh is a complex shape composed of triangles and vertices. Renderable
geometry can also be constructed from quads or higher-order subdivision sur-
faces. But on today’s graphics hardware, which is almost exclusively geared
toward rendering rasterized triangles, all shapes must eventually be trans-
lated into triangles prior to rendering.
A mesh typically has one or more materials applied to it in order to deﬁne
visual surface properties (color, reﬂectivity, bumpiness, diffuse texture, etc.).
In this book, I will use the term “mesh” to refer to a single renderable shape,
and “model” to refer to a composite object that may contain multiple meshes,
plus animation data and other metadata for use by the game.
Meshes are typically created in a 3D modeling package such as 3ds Max,
Maya or SoftImage. A powerful and popular tool by Pixologic called ZBrush

1.7. Tools and the Asset Pipeline
57
allows ultra high-resolution meshes to be built in a very intuitive way and
then down-converted into a lower-resolution model with normal maps to ap-
proximate the high-frequency detail.
Exporters must be written to extract the data from the digital content cre-
ation (DCC) tool (Maya, Max, etc.) and store it on disk in a form that is di-
gestible by the engine. The DCC apps provide a host of standard or semi-
standard export formats, although none are perfectly suited for game devel-
opment (with the possible exception of COLLADA). Therefore, game teams
often create custom ﬁle formats and custom exporters to go with them.
Brush Geometry
Brush geometry is deﬁned as a collection of convex hulls, each of which is de-
ﬁned by multiple planes. Brushes are typically created and edited directly in
the game world editor. This is essentially an “old school” approach to creating
renderable geometry, but it is still used in some engines.
Pros:
•
fast and easy to create;
•
accessible to game designers—often used to “block out” a game level for
prototyping purposes;
•
can serve both as collision volumes and as renderable geometry.
Cons:
•
low-resolution;
•
difﬁcult to create complex shapes;
•
cannot support articulated objects or animated characters.
1.7.2.2
Skeletal Animation Data
A skeletal mesh is a special kind of mesh that is bound to a skeletal hierarchy for
the purposes of articulated animation. Such a mesh is sometimes called a skin
because it forms the skin that surrounds the invisible underlying skeleton.
Each vertex of a skeletal mesh contains a list of indices indicating to which
joint(s) in the skeleton it is bound. A vertex usually also includes a set of joint
weights, specifying the amount of inﬂuence each joint has on the vertex.
In order to render a skeletal mesh, the game engine requires three distinct
kinds of data:
1.
the mesh itself,
2.
the skeletal hierarchy (joint names, parent-child relationships and the
base pose the skeleton was in when it was originally bound to the mesh),
and

58
1. Introduction
3.
one or more animation clips, which specify how the joints should move
over time.
The mesh and skeleton are often exported from the DCC application as a
single data ﬁle. However, if multiple meshes are bound to a single skeleton,
then it is better to export the skeleton as a distinct ﬁle. The animations are usu-
ally exported individually, allowing only those animations which are in use
to be loaded into memory at any given time. However, some game engines
allow a bank of animations to be exported as a single ﬁle, and some even lump
the mesh, skeleton and animations into one monolithic ﬁle.
An unoptimized skeletal animation is deﬁned by a stream of 4 × 3 matrix
samples, taken at a frequency of at least 30 frames per second, for each of the
joints in a skeleton (of which there can be 500 or more for a realistic humanoid
character). Thus, animation data is inherently memory-intensive. For this
reason, animation data is almost always stored in a highly compressed format.
Compression schemes vary from engine to engine, and some are proprietary.
There is no one standardized format for game-ready animation data.
1.7.2.3
Audio Data
Audio clips are usually exported from Sound Forge or some other audio pro-
duction tool in a variety of formats and at a number of different data sam-
pling rates. Audio ﬁles may be in mono, stereo, 5.1, 7.1 or other multi-channel
conﬁgurations. Wave ﬁles (.wav) are common, but other ﬁle formats such as
PlayStation ADPCM ﬁles (.vag) are also commonplace. Audio clips are often
organized into banks for the purposes of organization, easy loading into the
engine, and streaming.
1.7.2.4
Particle Systems Data
Modern games make use of complex particle effects. These are authored by
artists who specialize in the creation of visual effects. Third-party tools, such
as Houdini, permit ﬁlm-quality effects to be authored; however, most game
engines are not capable of rendering the full gamut of effects that can be cre-
ated with Houdini. For this reason, many game companies create a custom
particle effect editing tool, which exposes only the effects that the engine ac-
tually supports. A custom tool might also let the artist see the effect exactly as
it will appear in-game.
1.7.3
The World Editor
The game world is where everything in a game engine comes together. To my
knowledge, there are no commercially available game world editors (i.e., the

1.7. Tools and the Asset Pipeline
59
game world equivalent of Maya or Max). However, a number of commercially
available game engines provide good world editors:
•
Some variant of the Radiant game editor is used by most game engines
based on Quake technology.
•
The Half-Life 2 Source engine provides a world editor called Hammer.
•
UnrealEd is the Unreal Engine’s world editor. This powerful tool also
serves as the asset manager for all data types that the engine can con-
sume.
Writing a good world editor is difﬁcult, but it is an extremely important
part of any good game engine.
1.7.4
The Resource Database
Game engines deal with a wide range of asset types, from renderable geom-
etry to materials and textures to animation data to audio. These assets are
deﬁned in part by the raw data produced by the artists when they use a tool
like Maya, Photoshop or SoundForge. However, every asset also carries with
it a great deal of metadata. For example, when an animator authors an anima-
tion clip in Maya, the metadata provides the asset conditioning pipeline, and
ultimately the game engine, with the following information:
•
A unique id that identiﬁes the animation clip at runtime.
•
The name and directory path of the source Maya (.ma or .mb) ﬁle.
•
The frame range—on which frame the animation begins and ends.
•
Whether or not the animation is intended to loop.
•
The animator’s choice of compression technique and level. (Some assets
can be highly compressed without noticeably degrading their quality,
while others require less or no compression in order to look right in-
game.)
Every game engine requires some kind of database to manage all of the
metadata associated with the game’s assets. This database might be imple-
mented using an honest-to-goodness relational database such as MySQL or
Oracle, or it might be implemented as a collection of text ﬁles, managed by
a revision control system such as Subversion, Perforce or Git. We’ll call this
metadata the resource database in this book.
No matter in what format the resource database is stored and managed,
some kind of user interface must be provided to allow users to author and
edit the data. At Naughty Dog, we wrote a custom GUI in C# called Builder
for this purpose. For more information on Builder and a few other resource
database user interfaces, see Section 6.2.1.3.

60
1. Introduction
OS
Drivers
Hardware (PC, XBOX360, PS3, etc.)
3rd Party SDKs
Platform Independence Layer
Core Systems
Run-Time Engine
Tools and World Builder
Figure 1.36. Stand-alone tools architecture.
1.7.5
Some Approaches to Tool Architecture
A game engine’s tool suite may be architected in any number of ways. Some
tools might be stand-alone pieces of software, as shown in Figure 1.36. Some
tools may be built on top of some of the lower layers used by the runtime en-
gine, as Figure 1.37 illustrates. Some tools might be built into the game itself.
For example, Quake- and Unreal-based games both boast an in-game console
that permits developers and “modders” to type debugging and conﬁguration
commands while running the game. Finally, web-based user interfaces are
becoming more and more popular for certain kinds of tools.
As an interesting and unique example, Unreal’s world editor and asset
manager, UnrealEd, is built right into the runtime game engine. To run the
editor, you run your game with a command-line argument of “editor.” This
unique architectural style is depicted in Figure 1.38. It permits the tools to
have total access to the full range of data structures used by the engine and
avoids a common problem of having to have two representations of every
data structure—one for the runtime engine and one for the tools. It also means
that running the game from within the editor is very fast (because the game
is actually already running). Live in-game editing, a feature that is normally
very tricky to implement, can be developed relatively easily when the editor
is a part of the game. However, an in-engine editor design like this does have
its share of problems. For example, when the engine is crashing, the tools

1.7. Tools and the Asset Pipeline
61
OS
Drivers
Hardware (PC, XBOX360, PS3, etc.)
3rd Party SDKs
Platform Independence Layer
Core Systems
Run-Time Engine
Tools and World Builder
Figure 1.37. Tools built on a framework shared with the game.
become unusable as well. Hence a tight coupling between engine and asset
creation tools can tend to slow down production.
OS
Drivers
Hardware (PC, XBOX360, PS3, etc.)
3rd Party SDKs
Platform Independence Layer
Core Systems
Run-Time Engine
Other Tools
World Builder
Figure 1.38. UnrealEngine’s tool architecture.

62
1. Introduction
1.7.5.1
Web-Based User Interfaces
Web-based user interfaces are quickly becoming the norm for certain kinds of
game development tools. At Naughty Dog, we use a number of web-based
UIs. Naughty Dog’s localization tool serves as the front-end portal into our
localization database. Tasker is the web-based interface used by all Naughty
Dog employees to create, manage, schedule, communicate and collaborate on
game development tasks during production. A web-based interface known
as Connector also serves as our window into the various streams of debugging
information that are emitted by the game engine at runtime. The game spits
out its debug text into various named channels, each associated with a differ-
ent engine system (animation, rendering, AI, sound, etc.) These data streams
are collected by a lightweight Redis database. The browser-based Connector
interface allows users to view and ﬁlter this information in a convenient way.
Web-based UIs offer a number of advantages over stand-alone GUI appli-
cations. For one thing, web apps are typically easier and faster to develop
and maintain than a stand-alone app written in a language like Java, C# or
C++. Web apps require no special installation—all the user needs is a com-
patible web browser. Updates to a web-based interface can be pushed out to
the users without the need for an installation step—they need only refresh or
restart their browser to receive the update. Web interfaces also force us to de-
sign our tools using a client-server architecture. This opens up the possibility
of distributing our tools to a wider audience. For example, Naughty Dog’s
localization tool is available directly to outsourcing partners around the globe
who provide language translation services to us. Stand-alone tools still have
their place of course, especially when specialized GUIs such as 3D visualiza-
tion are required. But if your tool only needs to present the user with editable
forms and tabular data, a web-based tool may be your best bet.

2
Tools of the Trade
B
efore we embark on our journey across the fascinating landscape of game
engine architecture, it is important that we equip ourselves with some ba-
sic tools and provisions. In the next two chapters, we will review the software
engineering concepts and practices that we will need during our voyage. In
Chapter 2, we’ll explore the tools used by the majority of professional game
engineers. Then in Chapter 3, we’ll round out our preparations by reviewing
some key topics in the realms of object-oriented programming, design pat-
terns and large-scale C++ programming.
Game development is one of the most demanding and broad areas of soft-
ware engineering, so believe me, we’ll want to be well equipped if we are to
safely navigate the sometimes-treacherous terrain we’ll be covering. For some
readers, the contents of this chapter and the next will be very familiar. How-
ever, I encourage you not to skip these chapters entirely. I hope that they will
serve as a pleasant refresher; and who knows, you might even pick up a new
trick or two.
2.1
Version Control
A version control system is a tool that permits multiple users to work on a
group of ﬁles collectively. It maintains a history of each ﬁle so that changes
63

64
2. Tools of the Trade
can be tracked and reverted if necessary. It permits multiple users to mod-
ify ﬁles—even the same ﬁle—simultaneously, without everyone stomping on
each other’s work. Version control gets its name from its ability to track the
version history of ﬁles. It is sometimes called source control, because it is pri-
marily used by computer programmers to manage their source code. How-
ever, version control can be used for other kinds of ﬁles as well. Version
control systems are usually best at managing text ﬁles, for reasons we will
discover below. However, many game studios use a single version control
system to manage both source code ﬁles (which are text) and game assets like
textures, 3D meshes, animations and audio ﬁles (which are usually binary).
2.1.1
Why Use Version Control?
Version control is crucial whenever software is developed by a team of multi-
ple engineers. Version control
•
provides a central repository from which engineers can share source
code;
•
keeps a history of the changes made to each source ﬁle;
•
provides mechanisms allowing speciﬁc versions of the code base to be
tagged and later retrieved; and
•
permits versions of the code to be branched off from the main devel-
opment line, a feature often used to produce demos or make patches to
older versions of the software.
A source control system can be useful even on a single-engineer project.
Although its multiuser capabilities won’t be relevant, its other abilities, such
as maintaining a history of changes, tagging versions, creating branches for
demos and patches, tracking bugs, etc., are still invaluable.
2.1.2
Common Version Control Systems
Here are the most common source control systems you’ll probably encounter
during your career as a game engineer.
•
SCCS and RCS. The Source Code Control System (SCCS) and the Revi-
sion Control System (RCS) are two of the oldest version control systems.
Both employ a command-line interface. They are prevalent primarily on
UNIX platforms.
•
CVS. The Concurrent Version System (CVS) is a heavy-duty professional-
grade command-line-based source control system, originally built on

2.1. Version Control
65
top of RCS (but now implemented as a stand-alone tool). CVS is preva-
lent on UNIX systems but is also available on other development plat-
forms such as Microsoft Windows. It is open source and licensed under
the Gnu General Public License (GPL). CVSNT (also known as WinCVS)
is a native Windows implementation that is based on, and compatible
with, CVS.
•
Subversion. Subversion is an open source version control system aimed
at replacing and improving upon CVS. Because it is open source and
hence free, it is a great choice for individual projects, student projects
and small studios.
•
Git. This is an open source revision control system that has been used for
many venerable projects, including the Linux kernel. In the git develop-
ment model, the programmer makes changes to ﬁles and commits the
changes to a branch. The programmer can then merge his changes into
any other code branch quickly and easily, because git “knows” how to
rewind a sequence of diffs and reapply them onto a new base revision—
a process git calls rebasing. The net result is a revision control system that
is highly efﬁcient and fast when dealing with multiple code branches.
More information on git can be found at http://git-scm.com/.
•
Perforce. Perforce is a professional-grade source control system, with
both text-based and GUI interfaces. One of Perforce’s claims to fame is
its concept of change lists. A change list is a collection of source ﬁles that
have been modiﬁed as a logical unit. Change lists are checked into the
repository atomically—either the entire change list is submitted, or none
of it is. Perforce is used by many game companies, including Naughty
Dog and Electronic Arts.
•
NxN Alienbrain. Alienbrain is a powerful and feature-rich source control
system designed explicitly for the game industry. Its biggest claim to
fame is its support for very large databases containing both text source
code ﬁles and binary game art assets, with a customizable user interface
that can be targeted at speciﬁc disciplines such as artists, producers or
programmers.
•
ClearCase. ClearCase is a professional-grade source control system aimed
at very large-scale software projects. It is powerful and employs a unique
user interface that extends the functionality of Windows Explorer.
I
haven’t seen ClearCase used much in the game industry, perhaps be-
cause it is one of the more expensive version control systems.
•
Microsoft Visual SourceSafe. SourceSafe is a lightweight source control
package that has been used successfully on some game projects.

66
2. Tools of the Trade
2.1.3
Overview of Subversion and TortoiseSVN
I have chosen to highlight Subversion in this book for a few reasons. First off,
it’s free, which is always nice. It works well and is reliable, in my experience.
A Subversion central repository is quite easy to set up, and as we’ll see, there
are already a number of free repository servers out there if you don’t want
to go to the trouble of setting one up yourself. There are also a number of
good Windows and Mac Subversion clients, such as the freely available Tor-
toiseSVN for Windows. So while Subversion may not be the best choice for a
large commercial project (I personally prefer Perforce for that purpose), I ﬁnd
it perfectly suited to small personal and educational projects. Let’s take a look
at how to set up and use Subversion on a Microsoft Windows PC development
platform. As we do so, we’ll review core concepts that apply to virtually any
version control system.
Subversion, like most other version control systems, employs a client-server
architecture. The server manages a central repository, in which a version-
controlled directory hierarchy is stored. Clients connect to the server and re-
quest operations, such as checking out the latest version of the directory tree,
committing new changes to one or more ﬁles, tagging revisions, branching the
repository and so on. We won’t discuss setting up a server here; we’ll assume
you have a server, and instead we will focus on setting up and using the client.
You can learn how to set up a Subversion server by reading Chapter 6 of [38].
However, you probably will never need to do so, because you can always ﬁnd
free Subversion servers. For example, Google provides free Subversion code
hosting at http://code.google.com/.
2.1.4
Setting up a Code Repository on Google
The easiest way to get started with Subversion is to visit http://code.google.
com/ and set up a free Subversion repository. Create a Google user name
and password if you don’t already have one, then navigate to Project Host-
ing under Developer Resources (see Figure 2.1). Click “Create a new project,”
then enter a suitable unique project name, like “mygoogleusername-code.” You
can enter a summary and/or description if you like, and you can even pro-
vide tags so that other users all over the world can search for and ﬁnd your
repository. Click the “Create Project” button and you’re off to the races.
Once you’ve created your repository, you can administer it on the Google
Code website. You can add and remove users, control options and perform
a wealth of advanced tasks. But all you really need to do next is set up a
Subversion client and start using your repository.

2.1. Version Control
67
Figure 2.1. Google Code home page, Project Hosting link.
2.1.5
Installing TortoiseSVN
TortoiseSVN is a popular front end for Subversion. It extends the function-
ality of the Microsoft Windows Explorer via a convenient right-click menu
and overlay icons to show you the status of your version-controlled ﬁles and
folders.
To get TortoiseSVN, visit http://tortoisesvn.tigris.org/. Download the lat-
est version from the download page. Install it by double-clicking the .msi ﬁle
that you’ve downloaded and following the installation wizard’s instructions.
Once TortoiseSVN is installed, you can go to any folder in Windows Ex-
plorer and right-click—TortoiseSVN’s menu extensions should now be visi-
ble. To connect to an existing code repository (such as one you created on
Google Code), create a folder on your local hard disk and then right-click and
select “SVN Checkout....” The dialog shown in Figure 2.2 will appear. In
the “URL of repository” ﬁeld, enter your repository’s URL. If you are using
Google Code, it should be https://myprojectname.googlecode.com/svn/trunk,
where myprojectname is whatever you named your project when you ﬁrst cre-
ated it (e.g., “mygoogleusername-code”).
If you forget the URL of your repository, just log in to http://code.google.
com/, go to “Project Hosting” as before, sign in by clicking the “Sign in” link
in the upper right-hand corner of the screen, and then click the Settings link,
also found in the upper right-hand corner of the screen. Click the “My Pro-
ﬁle” tab, and you should see your project listed there. Your project’s URL
is https://myprojectname.googlecode.com/svn/trunk, where myprojectname is
whatever name you see listed on the “My Proﬁle” tab.

68
2. Tools of the Trade
Figure 2.2. TortoiseSVN initial check-out dialog.
Figure 2.3. TortoiseSVN user authentication dialog.
You should now see the dialog shown in Figure 2.3. The user name should
be your Google login name. The password is not your Google login pass-
word—it is an automatically generated password that can be obtained by
signing in to your account on Goggle’s “Project Hosting” page and clicking
on the “Settings” link. (See above for details.) Checking the “Save authenti-
cation” option on this dialog allows you to use your repository without ever
having to log in again. Only select this option if you are working on your own
personal machine—never on a machine that is shared by many users.
Once you’ve authenticated your user name, TortoiseSVN will download
(“check out”) the entire contents of your repository to your local disk. If you
have just set up your repository, this will be ...nothing! The folder you cre-
ated will still be empty. But now it is connected to your Subversion repository
on Google (or wherever your server is located). If you refresh your Windows
Explorer window (hit F5), you should now see a little green and white check-
mark on your folder. This icon indicates that the folder is connected to a Sub-
version repository via TortoiseSVN and that the local copy of the repository is
up to date.
2.1.6
File Versions, Updating and Committing
As we’ve seen, one of the key purposes of any source control system like Sub-
version is to allow multiple programmers to work on a single software code
base by maintaining a central repository or “master” version of all the source
code on a server. The server maintains a version history for each ﬁle, as shown

2.1. Version Control
69
Figure 2.4. File version histories.
Figure 2.5. Editing the local copy of a version-controlled ﬁle.
in Figure 2.4. This feature is crucial to large-scale multiprogrammer software
development. For example, if someone makes a mistake and checks in code
that “breaks the build,” you can easily go back in time to undo those changes
(and check the log to see who the culprit was!). You can also grab a snap-
shot of the code as it existed at any point in time, allowing you to work with,
demonstrate or patch previous versions of the software.
Each programmer gets a local copy of the code on his or her machine. In
the case of TortoiseSVN, you obtain your initial working copy by “checking
out” the repository, as described above. Periodically you should update your
local copy to reﬂect any changes that may have been made by other program-
mers. You do this by right-clicking on a folder and selecting “SVN Update”
from the pop-up menu.
You can work on your local copy of the code base without affecting the
other programmers on the team (Figure 2.5). When you are ready to share
your changes with everyone else, you commit your changes to the repository
(also known as submitting or checking in). You do this by right-clicking on the
folder you want to commit and selecting “SVN Commit...” from the pop-up
menu. You will get a dialog like the one shown in Figure 2.6, asking you to
conﬁrm the changes.
During a commit operation, Subversion generates a diff between your lo-
cal version of each ﬁle and the latest version of that same ﬁle in the repository.
The term “diff” means difference, and it is typically produced by performing a
line-by-line comparison of the two versions of the ﬁle. You can double-click on
any ﬁle in the TortoiseSVN Commit dialog (Figure 2.6) to see the diffs between
your version and the latest version on the server (i.e., the changes you made).
Files that have changed (i.e., any ﬁles that “have diffs”) are committed. This
replaces the latest version in the repository with your local version, adding a

70
2. Tools of the Trade
Figure 2.6. TortoiseSVN Commit dialog.
new entry to the ﬁle’s version history. Any ﬁles that have not changed (i.e.,
your local copy is identical to the latest version in the repository) are ignored
by default during a commit. An example commit operation is shown in Fig-
ure 2.7.
Figure 2.7.
Com-
mitting local edits to
the repository.
If you created any new ﬁles prior to the commit, they will be listed as “non-
versioned” in the Commit dialog. You can check the little check boxes beside
them in order to add them to the repository. Any ﬁles that you deleted locally
will likewise show up as “missing”—if you check their check boxes, they will
be deleted from the repository. You can also type a comment in the Commit
dialog. This comment is added to the repository’s history log, so that you and
others on your team will know why these ﬁles were checked in.
2.1.7
Multiple Check-Out, Branching and Merging
Some version control systems require exclusive check-out. This means that you
must ﬁrst indicate your intentions to modify a ﬁle by checking it out and lock-
ing it. The ﬁle(s) that are checked out to you are writable on your local disk
and cannot be checked out by anyone else. All other ﬁles in the repository
are read-only on your local disk. Once you’re done editing the ﬁle, you can

2.1. Version Control
71
check it in, which releases the lock and commits the changes to the repository
for everyone else to see. The process of exclusively locking ﬁles for editing
ensures that no two people can edit the same ﬁle simultaneously.
Subversion, CVS, Perforce and many other high-quality version control
systems also permit multiple check-out, i.e., you can edit a ﬁle while someone
else is editing that same ﬁle. Whichever user’s changes are committed ﬁrst
become the latest version of the ﬁle in the repository. Any subsequent com-
mits by other users require that programmer to merge his or her changes with
the changes made by the programmer(s) who committed previously.
Because more than one set of changes (diffs) have been made to the same
ﬁle, the version control system must merge the changes in order to produce a
ﬁnal version of the ﬁle. This is often not a big deal, and in fact many conﬂicts
can be resolved automatically by the version control system. For example, if
you changed function f() and another programmer changed function g(),
then your edits would have been to a different range of lines in the ﬁle than
those of the other programmer. In this case, the merge between your changes
and his or her changes will usually resolve automatically without any con-
ﬂicts. However, if you were both making changes to the same function f(),
then the second programmer to commit his or her changes will need to do a
three-way merge (see Figure 2.8).
For three-way merges to work, the version control server has to be smart
enough to keep track of which version of each ﬁle you currently have on your
local disk. That way, when you merge the ﬁles, the system will know which
version is the base version (the common ancestor, such as version 4 in Fig-
ure 2.8).
Subversion permits multiple check-out, and in fact it doesn’t require you
to check out ﬁles explicitly at all. You simply start editing the ﬁles locally—all
ﬁles are writable on your local disk at all times. (By the way, this is one reason
that Subversion doesn’t scale well to large projects, in my opinion. To deter-
mine which ﬁles you have changed, Subversion must search the entire tree of
source ﬁles, which can be slow. Version control systems like Perforce, which
explicitly keep track of which ﬁles you have modiﬁed, are usually easier to
work with when dealing with large amounts of code. But for small projects,
Subversion’s approach works just ﬁne.)
When you perform a commit operation by right-clicking on any folder and
selecting “SVN Commit...” from the pop-up menu, you may be prompted to
merge your changes with changes made by someone else. But if no one has
changed the ﬁle since you last updated your local copy, then your changes
will be committed without any further action on your part. This is a very
convenient feature, but it can also be dangerous. It’s a good idea to always

72
2. Tools of the Trade
Foo.cpp (joe_b)
Foo.cpp (suzie_q)
joe_b and suzie_q both 
start editing Foo.cpp at 
the same time
Foo.cpp (version 4)
Foo.cpp (joe_b)
Foo.cpp (version 5)
suzie_q commits her 
changes first
joe_b must now do a 3-way 
merge, which involves 2 sets
of diffs:
Foo.cpp (version 6)
Foo.cpp (joe_b)
Foo.cpp (version 5)
Foo.cpp (version 4)
Foo.cpp (version 4)
version 4 to his local version
version 4 to version 5
Figure 2.8. Three-way merge due to local edits by two different users.
check your commits carefully to be sure you aren’t committing any ﬁles that
you didn’t intend to modify. When TortoiseSVN displays its Commit Files
dialog, you can double-click on an individual ﬁle in order to see the diffs you
made prior to hitting the “OK” button.
2.1.8
Deleting Files
When a ﬁle is deleted from the repository, it’s not really gone. The ﬁle still ex-
ists in the repository, but its latest version is simply marked “deleted” so that
users will no longer see the ﬁle in their local directory trees. You can still see
and access previous versions of a deleted ﬁle by right-clicking on the folder in
which the ﬁle was contained and selecting “Show log” from the TortoiseSVN
menu.
You can undelete a deleted ﬁle by updating your local directory to the
version immediately before the version in which the ﬁle was marked deleted.
Then simply commit the ﬁle again. This replaces the latest deleted version of
the ﬁle with the version just prior to the deletion, effectively undeleting the
ﬁle.

2.2. Microsoft Visual Studio
73
2.2
Microsoft Visual Studio
Compiled languages, such as C++, require a compiler and linker in order to
transform source code into an executable program. There are many compil-
ers/linkers available for C++, but for the Microsoft Windows platform, the
most commonly used package is probably Microsoft Visual Studio. The fully
featured Professional Edition of the product can be purchased online from
the Microsoft store.
And Visual Studio Express, its lighter-weight cousin,
is available for free download at http://www.microsoft.com/visualstudio/
eng/products/visual-studio-express-products. Documentation on Visual Stu-
dio and the standard C and C++ libraries is available online at the Microsoft
Developer Network (MSDN) site (http://msdn.microsoft.com/en-us/default.
aspx).
Visual Studio is more than just a compiler and linker. It is an integrated
development environment (IDE), including a slick and fully featured text editor
for source code and a powerful source-level and machine-level debugger. In
this book, our primary focus is the Windows platform, so we’ll investigate
Visual Studio in some depth. Much of what you learn below will be applicable
to other compilers, linkers and debuggers, so even if you’re not planning on
ever using Visual Studio, I suggest you skim this section for useful tips on
using compilers, linkers and debuggers in general.
2.2.1
Source Files, Headers and Translation Units
A program written in C++ is comprised of source ﬁles. These typically have
a .c, .cc, .cxx or .cpp extension, and they contain the bulk of your program’s
source code. Source ﬁles are technically known as translation units, because the
compiler translates one source ﬁle at a time from C++ into machine
code.
A special kind of source ﬁle, known as a header ﬁle, is often used in order
to share information, such as type declarations and function prototypes, be-
tween translation units. Header ﬁles are not seen by the compiler. Instead,
the C++ preprocessor replaces each #include statement with the contents of
the corresponding header ﬁle prior to sending the translation unit to the com-
piler. This is a subtle but very important distinction to make. Header ﬁles exist
as distinct ﬁles from the point of view of the programmer—but thanks to the
preprocessor’s header ﬁle expansion, all the compiler ever sees are translation
units.

74
2. Tools of the Trade
2.2.2
Libraries, Executables and Dynamic Link Libraries
When a translation unit is compiled, the resulting machine code is placed in
an object ﬁle (ﬁles with a .obj extension under Windows or .o under UNIX-
based operating systems). The machine code in an object ﬁle is:
•
relocatable, meaning that the memory addresses at which the code resides
have not yet been determined, and
•
unlinked, meaning that any external references to functions and global
data that are deﬁned outside the translation unit have not yet been re-
solved.
Object ﬁles can be collected into groups called libraries. A library is simply
an archive, much like a ZIP or tar ﬁle, containing zero or more object ﬁles.
Libraries exist merely as a convenience, permitting a large number of object
ﬁles to be collected into a single easy-to-use ﬁle.
Object ﬁles and libraries are linked into an executable by the linker. The
executable ﬁle contains fully resolved machine code that can be loaded and
run by the operating system. The linker’s jobs are:
•
to calculate the ﬁnal relative addresses of all the machine code, as it will
appear in memory when the program is run, and
•
to ensure that all external references to functions and global data made
by each translation unit (object ﬁle) are properly resolved.
It’s important to remember that the machine code in an executable ﬁle is
still relocatable, meaning that the addresses of all instructions and data in the
ﬁle are still relative to an arbitrary base address, not absolute. The ﬁnal abso-
lute base address of the program is not known until the program is actually
loaded into memory, just prior to running it.
A dynamic link library (DLL) is a special kind of library that acts like a hy-
brid between a regular static library and an executable. The DLL acts like a
library, because it contains functions that can be called by any number of dif-
ferent executables. However, a DLL also acts like an executable, because it
can be loaded by the operating system independently, and it contains some
start-up and shut-down code that runs much the way the main() function in
a C++ executable does.
The executables that use a DLL contain partially linked machine code. Most
of the function and data references are fully resolved within the ﬁnal exe-
cutable, but any references to external functions or data that exist in a DLL re-
main unlinked. When the executable is run, the operating system resolves the

2.2. Microsoft Visual Studio
75
addresses of all unlinked functions by locating the appropriate DLLs, load-
ing them into memory if they are not already loaded, and patching in the
necessary memory addresses. Dynamically linked libraries are a very useful
operating system feature, because individual DLLs can be updated without
changing the executable(s) that use them.
2.2.3
Projects and Solutions
Now that we understand the difference between libraries, executables and
dynamic link libraries (DLLs), let’s see how to create them. In Visual Studio,
a project is a collection of source ﬁles which, when compiled, produce a library,
an executable or a DLL. In Visual Studio 2010 and 2012, projects are stored in
project ﬁles with a .vcxproj extension. These ﬁles are in XML format, so they
are reasonably easy for a human to read and even edit by hand if necessary.
All versions of Visual Studio since version 7 (Visual Studio 2003) employ
solution ﬁles (ﬁles with a .sln extension) as a means of containing and manag-
ing collections of projects. A solution is a collection of dependent and/or inde-
pendent projects intended to build one or more libraries, executables and/or
DLLs. In the Visual Studio graphical user interface, the Solution Explorer is
usually displayed along the right or left side of the main window, as shown
in Figure 2.9.
The Solution Explorer is a tree view. The solution itself is at the root, with
the projects as its immediate children. Source ﬁles and headers are shown as
children of each project. A project can contain any number of user-deﬁned
folders, nested to any depth. Folders are for organizational purposes only
and have nothing to do with the folder structure in which the ﬁles may reside
on-disk. However, it is common practice to mimic the on-disk folder structure
when setting up a project’s folders.
Figure 2.9. The VisualStudio Solution Explorer window.

76
2. Tools of the Trade
2.2.4
Build Conﬁgurations
The C/C++ preprocessor, compiler and linker offer a wide variety of options
to control how your code will be built. These options are normally speciﬁed
on the command line when the compiler is run. For example, a typical com-
mand to build a single translation unit with the Microsoft compiler might look
like this:
C:\> cl /c foo.cpp /Fo foo.obj /Wall /Od /Zi
This tells the compiler/linker to compile but not link (/c) the translation
unit named foo.cpp, output the result to an object ﬁle named foo.obj (/Fo
foo.obj), turn on all warnings (/Wall), turn off all optimizations (/Od) and
generate debugging information (/Zi).
Modern compilers provide so many options that it would be impractical
and error prone to specify all of them every time you build your code. That’s
where build conﬁgurations come in. A build conﬁguration is really just a collec-
tion of preprocessor, compiler and linker options associated with a particular
project in your solution. You can deﬁne any number of build conﬁgurations,
name them whatever you want, and conﬁgure the preprocessor, compiler and
linker options differently in each conﬁguration. By default, the same options
are applied to every translation unit in the project, although you can override
the global project settings on an individual translation unit basis. (I recom-
mend avoiding this if at all possible, because it becomes difﬁcult to tell which
.cpp ﬁles have custom settings and which do not.)
Most projects have at least two build conﬁgurations, typically called “De-
bug” and “Release.” The release build is for the ﬁnal shipping software, while
the debug build is for development purposes.
A debug build runs more
slowly than a release build, but it provides the programmer with invaluable
information for developing and debugging the program.
2.2.4.1
Common Build Options
This section lists some of the most common options you’ll want to control via
build conﬁgurations for a game engine project.
Preprocessor Settings
The C++ preprocessor handles the expansion of #included ﬁles and the deﬁ-
nition and substitution of #defined macros. One extremely powerful feature
of all modern C++ preprocessors is the ability to deﬁne preprocessor macros
via command-line options (and hence via build conﬁgurations). Macros de-
ﬁned in this way act as though they had been written into your source code

2.2. Microsoft Visual Studio
77
with a #define statement. For most compilers, the command line option for
this is -D or /D, and any number of these directives can be used.
This feature allows you to communicate various build options to your
code, without having to modify the source code itself. As a ubiquitous exam-
ple, the symbol _DEBUG is always deﬁned for a debug build, while in release
builds, the symbol NDEBUG is deﬁned instead. The source code can check
these ﬂags and in effect “know” whether it is being built in debug or release
mode. This is known as conditional compilation. For example,
void f()
{
#ifdef _DEBUG
printf("Calling function f()\n");
#endif
// ...
}
The compiler is also free to introduce “magic” preprocessor macros into
your code, based on its knowledge of the compilation environment and target
platform. For example, the macro __cplusplus is deﬁned by most C/C++
compilers when compiling a C++ ﬁle. This allows code to be written that
automatically adapts to being compiled for C or C++.
As another example, every compiler identiﬁes itself to the source code via
a “magic” preprocessor macro. When compiling code under the Microsoft
compiler, the macro _MSC_VER is deﬁned; when compiling under the GNU
compiler (gcc), the macro __GNUC__ is deﬁned instead and so on for the
other compilers. The target platform on which the code will be run is like-
wise identiﬁed via macros. For example, when building for a 32-bit Windows
machine, the symbol _WIN32 is always deﬁned. These key features permit
cross-platform code to be written, because they allow your code to “know”
what compiler is compiling it and on which target platform it is destined to
be run.
Compiler Settings
One of the most common compiler options controls whether or not the com-
piler should include debugging information with the object ﬁles it produces.
This information is used by debuggers to step through your code, display the
values of variables and so on. Debugging information makes your executa-
bles larger on disk and also opens the door for hackers to reverse-engineer
your code, so it is always stripped from the ﬁnal shipping version of your
executable. However, during development, debugging information is invalu-
able and should always be included in your builds.

78
2. Tools of the Trade
The compiler can also be told whether or not to expand inline functions.
When inline function expansion is turned off, every inline function appears
only once in memory, at a distinct address. This makes the task of tracing
through the code in the debugger much simpler, but obviously comes at the
expense of the execution speed improvements normally achieved by inlining.
Inline function expansion is but one example of generalized code transfor-
mations known as optimizations. The aggressiveness with which the compiler
attempts to optimize your code, and the kinds of optimizations its uses, can
be controlled via compiler options. Optimizations have a tendency to reorder
the statements in your code, and they also cause variables to be stripped out
of the code altogether, or moved around, and can cause CPU registers to be
reused for new purposes later in the same function. Optimized code usually
confuses most debuggers, causing them to “lie” to you in various ways, and
making it difﬁcult or impossible to see what’s really going on. As a result,
all optimizations are usually turned off in a debug build. This permits every
variable and every line of code to be scrutinized as it was originally coded.
But, of course, such code will run much more slowly than its fully optimized
counterpart.
Linker Settings
The linker also exposes a number of options. You can control what type of
output ﬁle to produce—an executable or a DLL. You can also specify which
external libraries should be linked into your executable, and which directory
paths to search in order to ﬁnd them. A common practice is to link with de-
bug libraries when building a debug executable and with optimized libraries
when building in release mode.
Linker options also control things like stack size, the preferred base ad-
dress of your program in memory, what type of machine the code will run on
(for machine-speciﬁc optimizations), and a host of other minutia with which
we will not concern ourselves here.
2.2.4.2
Typical Build Conﬁgurations
Game projects often have more than just two build conﬁgurations. Here are a
few of the common conﬁgurations I’ve seen used in game development.
•
Debug. A debug build is a very slow version of your program, with all
optimizations turned off, all function inlining disabled, and full debug-
ging information included. This build is used when testing brand new
code and also to debug all but the most trivial problems that arise during
development.

2.2. Microsoft Visual Studio
79
•
Release. A release build is a faster version of your program, but with de-
bugging information and assertions still turned on. (See Section 3.3.3.3
for a discussion of assertions.) This allows you to see your game run-
ning at a speed representative of the ﬁnal product, but it still gives you
some opportunity to debug problems.
•
Production. A production conﬁguration is intended for building the ﬁ-
nal game that you will ship to your customers. It is sometimes called a
“Final” build or “Disk” build. Unlike a release build, all debugging in-
formation is stripped out of a production build, all assertions are usually
turned off, and optimizations are cranked all the way up. A production
build is very tricky to debug, but it is the fastest and leanest of all build
types.
•
Tools. Some game studios utilize code libraries that are shared between
ofﬂine tools and the game itself. In this scenario, it often makes sense
to deﬁne a “Tools” build, which can be used to conditionally compile
shared code for use by the tools. The tools build usually deﬁnes a pre-
processor macro (e.g., TOOLS_BUILD) that informs the code that it is be-
ing built for use in a tool. For example, one of your tools might require
certain C++ classes to expose editing functions that are not needed by
the game. These functions could be wrapped in an #ifdef TOOLS_
BUILD directive. Since you usually want both debug and release ver-
sions of your tools, you will probably ﬁnd yourself creating two tools
builds, named something like “ToolsDebug” and “ToolsRelease.”
Hybrid Builds
A hybrid build is a build conﬁguration in which the majority of the translation
units are built in release mode, but a small subset of them is built in debug
mode. This permits the segment of code that is currently under scrutiny to be
easily debugged, while the rest of the code continues to run at full speed.
With a text-based build system like make, it is quite easy to set up a hybrid
build that permits users to specify the use of debug mode on a per-translation-
unit basis. In a nutshell, we deﬁne a make variable called something like
$HYBRID_SOURCES, which lists the names of all translation units (.cpp ﬁles)
that should be compiled in debug mode for our hybrid build. We set up build
rules for compiling both debug and release versions of every translation unit,
and arrange for the resulting object ﬁles (.obj/.o) to be placed into two differ-
ent folders, one for debug and one for release. The ﬁnal link rule is set up to
link with the debug versions of the object ﬁles listed in $HYBRID_SOURCES
and with the release versions of all other object ﬁles. If we’ve set it up prop-
erly, make’s dependency rules will take care of the rest.

80
2. Tools of the Trade
Unfortunately, this is not so easy to do in Visual Studio, because its build
conﬁgurations are designed to be applied on a per-project basis, not per-trans-
lation unit. The crux of the problem is that we cannot easily deﬁne a list of the
translation units that we want to build in debug mode. However, if your
source code is already organized into libraries, you can set up a “Hybrid”
build conﬁguration at the solution level, which picks and chooses between
debug and release builds on a per-project (and hence per-library) basis. This
isn’t as ﬂexible as having control on a per-translation-unit basis, but it does
work reasonably well if your libraries are sufﬁciently granular.
Build Conﬁgurations and Testability
The more build conﬁgurations your project supports, the more difﬁcult test-
ing becomes. Although the differences between the various conﬁgurations
may be slight, there’s a ﬁnite probability that a critical bug may exist in one
of them but not in the others. Therefore, each build conﬁguration must be
tested equally thoroughly. Most game studios do not formally test their de-
bug builds, because the debug conﬁguration is primarily intended for internal
use during initial development of a feature and for the debugging of problems
found in one of the other conﬁgurations. However, if your testers spend most
of their time testing your release conﬁguration, then you cannot simply make
a production build of your game the night before Gold Master and expect it
to have an identical bug proﬁle to that of the release build. Practically speak-
ing, the test team must test both your release and production builds equally
throughout alpha and beta to ensure that there aren’t any nasty surprises lurk-
ing in your production build. In terms of testability, there is a clear advantage
to keeping your build conﬁgurations to a minimum, and in fact some stu-
dios have no production build for this reason—they simply ship their release
build once it has been thoroughly tested (but with the debugging information
stripped out).
2.2.4.3
Project Conﬁguration Tutorial
Right-clicking on any project in the Solution Explorer and selecting “Proper-
ties...” from the menu brings up the project’s “Property Pages” dialog. The
tree view on the left shows various categories of settings. Of these, the four
we will use most are:
•
Conﬁguration Properties/General,
•
Conﬁguration Properties/Debugging,
•
Conﬁguration Properties/C++, and
•
Conﬁguration Properties/Linker.

2.2. Microsoft Visual Studio
81
Conﬁgurations Drop-Down Combo Box
Notice the drop-down combo box labeled “Conﬁguration:” at the top-left cor-
ner of the window. All of the properties displayed on these property pages
apply separately to each build conﬁguration. If you set a property for the de-
bug conﬁguration, this does not necessarily mean that the same setting exists
for the release conﬁguration.
If you click on the combo box to drop down the list, you’ll ﬁnd that you can
select a single conﬁguration or multiple conﬁgurations, including “All conﬁg-
urations.” As a rule of thumb, try to do most of your build conﬁguration edit-
ing with “All conﬁgurations” selected. That way, you won’t have to make the
same edits multiple times, once for each conﬁguration—and you don’t risk
setting things up incorrectly in one of the conﬁgurations by accident. How-
ever, be aware that some settings need to be different between the debug and
release conﬁgurations. For example, function inlining and code optimization
settings should, of course, be different between debug and release builds.
General Property Page
On the General property page, shown in Figure 2.10, the most useful ﬁelds are
the following:
•
Output directory. This deﬁnes where the ﬁnal product(s) of the build
will go—namely, the executable, library or DLL that the compiler/linker
ultimately outputs.
•
Intermediate directory. This deﬁnes where intermediate ﬁles, primarily
object ﬁles (.obj extension), are placed during a build. Intermediate ﬁles
are never shipped with your ﬁnal program—they are only required dur-
ing the process of building your executable, library or DLL. Hence, it is
a good idea to place intermediate ﬁles in a different directory than the
ﬁnal products (.exe, .lib or .dll ﬁles).
Note that VisualStudio provides a macro facility, which may be used when
specifying directories and other settings in the “Project Property Pages” dia-
log. A macro is essentially a named variable that contains a global value and
that can be referred to in your project conﬁguration settings.
Macros are invoked by writing the name of the macro enclosed in paren-
theses and preﬁxed with a dollar sign (e.g., $(ConfigurationName)). Some
commonly used macros are listed below.
•
$(TargetFileName). The name of the ﬁnal executable, library or DLL
ﬁle being built by this project.

82
2. Tools of the Trade
Figure 2.10. Visual Studio project property pages—General page.
•
$(TargetPath). The full path of the folder containing the ﬁnal exe-
cutable, library or DLL.
•
$(ConfigurationName). The name of the build conﬁg, typically “De-
bug” or “Release.”
•
$(OutDir). The value of the “Output Directory” ﬁeld speciﬁed in this
dialog.
•
$(IntDir). The value of the “Intermediate Directory” ﬁeld in this dia-
log.
•
$(VCInstallDir). The directory in which Visual Studio’s standard C
library is currently installed.
The beneﬁt of using macros instead of hard-wiring your conﬁguration set-
tings is that a simple change of the global macro’s value will automatically af-
fect all conﬁguration settings in which the macro is used. Also, some macros
like $(ConfigurationName) automatically change their values depending
on the build conﬁguration, so using them can permit you to use identical set-
tings across all your conﬁgurations.
To see a complete list of all available macros, click in either the “Output
Directory” ﬁeld or the “Intermediate Directory” ﬁeld on the “General” prop-
erty page, click the little arrow to the right of the text ﬁeld, select “Edit...”
and then click the “Macros” button in the dialog that comes up.
Debugging Property Page
The “Debugging” property page is where the name and location of the exe-
cutable to debug is speciﬁed. On this page, you can also specify the command-

2.2. Microsoft Visual Studio
83
line argument(s) that should be passed to the program when it runs. We’ll
discuss debugging your program in more depth below.
C/C++ Property Page
The C/C++ property page controls compile-time language settings—things
that affect how your source ﬁles will be compiled into object ﬁles (.obj exten-
sion). The settings on this page do not affect how your object ﬁles are linked
into a ﬁnal executable or DLL.
You are encouraged to explore the various subpages of the C/C++ page
to see what kinds of settings are available. Some of the most commonly used
settings include the following:
•
General Property Page/Additional Include Directories. This ﬁeld lists the
on-disk directories that will be searched when looking for #included
header ﬁles.
Important: It is always best to specify these directories using relative
paths and/or with Visual Studio macros like $(OutDir) or $(IntDir).
That way, if you move your build tree to a different location on disk or to
another computer with a different root folder, everything will continue
to work properly.
•
General Property Page/Debug Information Format.
This ﬁeld controls
whether or not debug information is generated and in what format. Typ-
ically both debug and release conﬁgurations include debugging infor-
mation so that you can track down problems during the development
of your game. The ﬁnal production build will have all the debug info
stripped out to prevent hacking.
•
Preprocessor Property Page/Preprocessor Deﬁnitions. This very handy ﬁeld
lists any number of C/C++ preprocessor symbols that should be de-
ﬁned in the code when it is compiled. See Preprocessor Settings in Section
2.2.4.1 for a discussion of preprocessor-deﬁned symbols.
Linker Property Page
The “Linker” property page lists properties that affect how your object code
ﬁles will be linked into an executable or DLL. Again, you are encouraged to
explore the various subpages. Some commonly used settings follow:
•
General Property Page/Output File. This setting lists the name and location
of the ﬁnal product of the build, usually an executable or DLL.

84
2. Tools of the Trade
•
General Property Page/Additional Library Directories. Much like the C/C++
Additional Include Directories ﬁeld, this ﬁeld lists zero or more directo-
ries that will be searched when looking for libraries and object ﬁles to
link into the ﬁnal executable.
•
Input Property Page/Additional Dependencies. This ﬁeld lists external li-
braries that you want linked into your executable or DLL. For example,
the OGRE libraries would be listed here if you are building an OGRE-
enabled application.
Note that Visual Studio employs various “magic spells” to specify libraries
that should be linked into an executable. For example, a special #pragma in-
struction in your source code can be used to instruct the linker to automati-
cally link with a particular library. For this reason, you may not see all of the
libraries you’re actually linking to in the “Additional Dependencies” ﬁeld.
(In fact, that’s why they are called additional dependencies.) You may have
noticed, for example, that Direct X applications do not list all of the DirectX
libraries manually in their “Additional Dependencies” ﬁeld. Now you know
why.
2.2.5
Debugging Your Code
One of the most important skills any programmer can learn is how to effec-
tively debug code. This section provides some useful debugging tips and
tricks. Some are applicable to any debugger and some are speciﬁc to Microsoft
Visual Studio. However, you can usually ﬁnd an equivalent to Visual Studio’s
debugging features in other debuggers, so this section should prove useful
even if you don’t use Visual Studio to debug your code.
2.2.5.1
The Start-Up Project
A Visual Studio solution can contain more than one project. Some of these
projects build executables, while others build libraries or DLLs. It’s possible
to have more than one project that builds an executable in a single solution.
Visual Studio provides a setting known as the “Start-Up Project.” This is the
project that is considered “current” for the purposes of the debugger. Typi-
cally a programmer will debug one project at a time by setting a single start-
up project. However, it is possible to debug multiple projects simultaneously
(see http://msdn.microsoft.com/en-us/library/0s590bew(v=vs.100).aspx for
details).
The start-up project is highlighted in bold in the Solution Explorer. By de-
fault, hitting F5 will run the .exe built by the start-up project, if the start-up

2.2. Microsoft Visual Studio
85
project builds an executable. (Technically speaking, F5 runs whatever com-
mand you type into the Command ﬁeld in the Debugging property page, so
it’s not limited to running the .exe built by your project.)
2.2.5.2
Breakpoints
Breakpoints are the bread and butter of code debugging. A breakpoint instructs
the program to stop at a particular line in your source code so that you can
inspect what’s going on.
In Visual Studio, select a line and hit F9 to toggle a breakpoint. When you
run your program and the line of code containing the breakpoint is about to
be executed, the debugger will stop the program. We say that the breakpoint
has been “hit.” A little arrow will show you which line of code the CPU’s
program counter is currently on. This is shown in Figure 2.11.
Figure 2.11. Setting a breakpoint in Visual Studio.
2.2.5.3
Stepping through Your Code
Once a breakpoint has been hit, you can single-step your code by hitting the
F10 key. The yellow program-counter arrow moves to show you the lines
as they execute. Hitting F11 steps into a function call (i.e., the next line of
code you’ll see is the ﬁrst line of the called function), while F10 steps over that
function call (i.e., the debugger calls the function at full speed and then breaks
again on the line right after the call).
2.2.5.4
The Call Stack
The call stack window, shown in Figure 2.12, shows you the stack of functions
that have been called at any given moment during the execution of your code.
To display the call stack (if it is not already visible), go to the “Debug” menu
on the main menu bar, select “Windows” and then “Call Stack.”

86
2. Tools of the Trade
Figure 2.12. The call stack window.
Once a breakpoint has been hit (or the program is manually paused), you
can move up and down the call stack by double-clicking on entries in the “Call
Stack” window. This is very useful for inspecting the chain of function calls
that were made between main() and the current line of code. For example,
you might trace back to the root cause of a bug in a parent function that has
manifested itself in a deeply nested child function.
2.2.5.5
The Watch Window
As you step through your code and move up and down the call stack, you will
want to be able to inspect the values of the variables in your program. This
is what watch windows are for. To open a watch window, go to the “Debug”
menu, select “Windows...,” then select “Watch...,” and ﬁnally select one of
“Watch 1” through “Watch 4.” (Visual Studio allows you to open up to four
watch windows simultaneously.) Once a watch window is open, you can type
the names of variables into the window or drag expressions in directly from
your source code.
As you can see in Figure 2.13, variables with simple data types are shown
with their values listed immediately to the right of their names. Complex
data types are shown as little tree views that can be easily expanded to “drill
down” into virtually any nested structure. The base class of a class is always
shown as the ﬁrst child of an instance of a derived class. This allows you to
inspect not only the class’ data members, but also the data members of its base
class(es).
You can type virtually any valid C/C++ expression into the watch window,
and Visual Studio will evaluate that expression and attempt to display the re-
sulting value. For example, you could type “5 + 3” and Visual Studio will
display “8.” You can cast variables from one type to another by using C or C++
casting syntax. For example, typing “(float)intVar1/(float)intVar2”
in the watch window will display the ratio of two integer variables as a ﬂoating-
point value.

2.2. Microsoft Visual Studio
87
Figure 2.13. Visual Studio’s watch window.
You can even call functions in your program from within the watch window.
Visual Studio reevaluates the expressions typed into the watch window(s)
automatically, so if you invoke a function in the watch window, it will be
called every time you hit a breakpoint or single-step your code. This allows
you to leverage the functionality of your program in order to save yourself
work when trying to interpret the data that you’re inspecting in the debug-
ger. For example, let’s say that your game engine provides a function called
quatToAngleDeg(), which converts a quaternion to an angle of rotation in
degrees. You can call this function in the watch window in order to easily
inspect the rotation angle of any quaternion within the debugger.
You can also use various sufﬁxes on the expressions in the watch window
in order to change the way Visual Studio displays the data, as shown in Fig-
ure 2.14.
•
The “,d” sufﬁx forces values to be displayed in decimal notation.
•
The “,x” sufﬁx forces values to be displayed in hexadecimal notation.
•
The “,n” sufﬁx (where n is any positive integer) forces Visual Studio to
treat the value as an array with n elements. This allows you to expand
array data that is referenced through a pointer.
Figure 2.14. Comma sufﬁxes in the Visual Studio watch window.

88
2. Tools of the Trade
Be careful when expanding very large data structures in the watch win-
dow, because it can sometimes slow the debugger down to the point of being
unusable.
2.2.5.6
Data Breakpoints
Regular breakpoints trip when the CPU’s program counter hits a particular
machine instruction or line of code. However, another incredibly useful fea-
ture of modern debuggers is the ability to set a breakpoint that trips when-
ever a speciﬁc memory address is written to (i.e., changed). These are called
data breakpoints, because they are triggered by changes to data, or sometimes
hardware breakpoints, because they are implemented via a special feature of the
CPU’s hardware—namely, the ability to raise an interrupt when a predeﬁned
memory address is written to.
Here’s how data breakpoints are typically used. Let’s say you are tracking
down a bug that manifests itself as a zero (0.0f) value mysteriously appear-
ing inside a member variable of a particular object called m_angle that should
always contain a nonzero angle. You have no idea which function might be
writing that zero into your variable. However, you do know the address of
the variable. (You can just type “&object.m_angle” into the watch window
to ﬁnd its address.) To track down the culprit, you can set a data breakpoint
on the address of object.m_angle, and then simply let the program run.
When the value changes, the debugger will stop automatically. You can then
inspect the call stack to catch the offending function red-handed.
To set a data breakpoint in Visual Studio, take the following steps.
•
Bring up the “Breakpoints” window found on the “Debug” menu under
“Windows” and then “Breakpoints” (Figure 2.15).
•
Select the “New” drop-down button in the upper-left corner of the win-
dow.
•
Select “New Data Breakpoint.”
•
Type in the raw address or an address-valued expression, such as
“&myVariable” (Figure 2.16).
2.2.5.7
Conditional Breakpoints
You’ll also notice in the “Breakpoints” window that you can set conditions
and hit counts on any type breakpoint—data breakpoints or regular line-of-
code breakpoints.
A conditional breakpoint causes the debugger to evaluate the C/C++ ex-
pression you provide every time the breakpoint is hit. If the expression is

2.2. Microsoft Visual Studio
89
Figure 2.15. The Visual Studio breakpoints window.
Figure 2.16. Deﬁning a data breakpoint.
true, the debugger stops your program and gives you a chance to see what’s
going on. If the expression is false, the breakpoint is ignored and the pro-
gram continues.
This is very useful for setting breakpoints that only trip
when a function is called on a particular instance of a class. For example,
let’s say you have a game level with 20 tanks on-screen, and you want to stop
your program when the third tank, whose memory address you know to be
0x12345678, is running. By setting the breakpoint’s condition expression to
something like “(uintptr_t)this == 0x12345678”, you can restrict the
breakpoint only to the class instance whose memory address (this pointer)
is 0x12345678.
Specifying a hit count for a breakpoint causes the debugger to decrement a
counter every time the breakpoint is hit, and only actually stop the program
when that counter reaches zero. This is really useful for situations where your
breakpoint is inside a loop, and you need to inspect what’s happening during
the 376th iteration of the loop (e.g., the 376th element in an array). You can’t
very well sit there and hit the F5 key 375 times! But you can let the hit count
feature of Visual Studio do it for you.
One note of caution: conditional breakpoints cause the debugger to eval-
uate the conditional expression every time the breakpoint is hit, so they can
bog down the performance of the debugger and your game.
2.2.5.8
Debugging Optimized Builds
I mentioned above that it can be very tricky to debug problems using a release
build, due primarily to the way the compiler optimizes the code. Ideally, ev-
ery programmer would prefer to do all of his or her debugging in a debug
build. However, this is often not possible. Sometimes a bug occurs so rarely
that you’ll jump at any chance to debug the problem, even if it occurs in a
release build on someone else’s machine. Other bugs only occur in your re-
lease build, but they magically disappear whenever you run the debug build.
These dreaded release-only bugs are sometimes caused by uninitialized vari-
ables, because variables and dynamically allocated memory blocks are often

90
2. Tools of the Trade
set to zero in debug mode but are left containing garbage in a release build.
Other common causes of release-only bugs include code that has been acci-
dentally omitted from the release build (e.g., when important code is erro-
neously placed inside an assertion statement), data structures whose size or
data member packing changes between debug and release builds, bugs that
are only triggered by inlining or compiler-introduced optimizations, and (in
rare cases) bugs in the compiler’s optimizer itself, causing it to emit incorrect
code in a fully optimized build.
Clearly, it behooves every programmer to be capable of debugging prob-
lems in a release build, unpleasant as it may seem. The best ways to reduce
the pain of debugging optimized code is to practice doing it and to expand
your skill set in this area whenever you have the opportunity. Here are a few
tips.
•
Learn to read and step through disassembly in the debugger. In a release
build, the debugger often has trouble keeping track of which line of
source code is currently being executed. Thanks to instruction reorder-
ing, you’ll often see the program counter jump around erratically within
the function when viewed in source code mode. However, things be-
come sane again when you work with the code in disassembly mode
(i.e., step through the assembly language instructions individually). Ev-
ery C/C++ programmer should be at least a little bit familiar with the
architecture and assembly language of their target CPU(s). That way,
even if the debugger is confused, you won’t be.
•
Use registers to deduce variables’ values or addresses. The debugger will
sometimes be unable to display the value of a variable or the contents of
an object in a release build. However, if the program counter is not too
far away from the initial use of the variable, there’s a good chance its ad-
dress or value is still stored in one of the CPU’s registers. If you can trace
back through the disassembly to where the variable is ﬁrst loaded into
a register, you can often discover its value or its address by inspecting
that register. Use the register window, or type the name of the register
into a watch window, to see its contents.
•
Inspect variables and object contents by address. Given the address of a vari-
able or data structure, you can usually see its contents by casting the
address to the appropriate type in a watch window. For example, if we
know that an instance of the Foo class resides at address 0x1378A0C0,
we can type “(Foo*)0x1378A0C0” in a watch window, and the debug-
ger will interpret that memory address as if it were a pointer to a Foo
object.

2.3. Proﬁling Tools
91
•
Leverage static and global variables. Even in an optimized build, the debug-
ger can usually inspect global and static variables. If you cannot deduce
the address of a variable or object, keep your eye open for a static or
global that might contain its address, either directly or indirectly. For
example, if we want to ﬁnd the address of an internal object within the
physics system, we might discover that it is in fact stored in a member
variable of the global PhysicsWorld object.
•
Modify the code. If you can reproduce a release-only bug relatively eas-
ily, consider modifying the source code to help you debug the problem.
Add print statements so you can see what’s going on. Introduce a global
variable to make it easier to inspect a problematic variable or object in
the debugger. Add code to detect a problem condition or to isolate a
particular instance of a class.
2.3
Proﬁling Tools
Games are typically high-performance real-time programs. As such, game
engine programmers are always looking for ways to speed up their code.
There is a well-known, albeit rather unscientiﬁc, rule of thumb known as the
Pareto principle (see http://en.wikipedia.org/wiki/Pareto_principle). It is also
known as the 80-20 rule, because it states that in many situations, 80% of the
effects of some event come from only 20% of the possible causes. In computer
science, we often use a variant of this principle known as the 90-10 rule, which
states that 90% of the wall clock time spent running any piece of software is
accounted for by only 10% of the code. In other words, if you optimize 10% of
your code, you can potentially realize 90% of all the gains in execution speed
you’ll ever realize.
So, how do you know which 10% of your code to optimize? For that, you
need a proﬁler. A proﬁler is a tool that measures the execution time of your
code. It can tell you how much time is spent in each function. You can then
direct your optimizations toward only those functions that account for the
lion’s share of the execution time.
Some proﬁlers also tell you how many times each function is called. This
is an important dimension to understand. A function can eat up time for
two reasons: (a) it takes a long time to execute on its own, or (b) it is called
frequently. For example, a function that runs an A* algorithm to compute the
optimal paths through the game world might only be called a few times each
frame, but the function itself may take a signiﬁcant amount of time to run. On

92
2. Tools of the Trade
the other hand, a function that computes the dot product may only take a few
cycles to execute, but if you call it hundreds of thousands of times per frame,
it might drag down your game’s frame rate.
Even more information can be obtained if you use the right proﬁler. Some
proﬁlers report the call graph, meaning that for any given function, you can
see which functions called it (these are known as parent functions) and which
functions it called (these are known as child functions or descendants). You can
even see what percentage of the function’s time was spent calling each of its
descendants and the percentage of the overall running time accounted for by
each individual function.
Proﬁlers fall into two broad categories.
1.
Statistical proﬁlers. This kind of proﬁler is designed to be unobtrusive,
meaning that the target code runs at almost the same speed, whether or
not proﬁling is enabled. These proﬁlers work by sampling the CPU’s
program counter register periodically and noting which function is cur-
rently running. The number of samples taken within each function yields
an approximate percentage of the total running time that is eaten up
by that function. Intel’s VTune is the gold standard in statistical proﬁl-
ers for Windows machines employing Intel Pentium processors, and it
is now also available for Linux. See http://software.intel.com/en-us/
intel-vtune-ampliﬁer-xe for details.
2.
Instrumenting proﬁlers. This kind of proﬁler is aimed at providing the
most accurate and comprehensive timing data possible, but at the ex-
pense of real-time execution of the target program—when proﬁling is
turned on, the target program usually slows to a crawl. These proﬁlers
work by preprocessing your executable and inserting special prologue
and epilogue code into every function. The prologue and epilogue code
calls into a proﬁling library, which in turn inspects the program’s call
stack and records all sorts of details, including which parent function
called the function in question and how many times that parent has
called the child. This kind of proﬁler can even be set up to monitor
every line of code in your source program, allowing it to report how
long each line is taking to execute. The results are stunningly accurate
and comprehensive, but turning on proﬁling can make a game virtu-
ally unplayable. IBM’s Rational Quantify, available as part of the Ra-
tional Purify Plus tool suite, is an excellent instrumenting proﬁler. See
http://www.ibm.com/developerworks/rational/library/957.html
for
an introduction to proﬁling with Quantify.

2.4. Memory Leak and Corruption Detection
93
Microsoft has also published a proﬁler that is a hybrid between the two
approaches. It is called LOP, which stands for low-overhead proﬁler. It uses
a statistical approach, sampling the state of the processor periodically, which
means it has a low impact on the speed of the program’s execution. However,
with each sample, it analyzes the call stack, thereby determining the chain of
parent functions that resulted in each sample. This allows LOP to provide
information normally not available with a statistical proﬁler, such as the dis-
tribution of calls across parent functions.
2.3.1
List of Proﬁlers
There are a great many proﬁling tools available. See http://en.wikipedia.org/
wiki/List_of_performance_analysis_tool for a reasonably comprehensive list.
2.4
Memory Leak and Corruption Detection
Two other problems that plague C and C++ programmers are memory leaks
and memory corruption. A memory leak occurs when memory is allocated
but never freed. This wastes memory and eventually leads to a potentially fa-
tal out-of-memory condition. Memory corruption occurs when the program
inadvertently writes data to the wrong memory location, overwriting the im-
portant data that was there—while simultaneously failing to update the mem-
ory location where that data should have been written. Blame for both of these
problems falls squarely on the language feature known as the pointer.
A pointer is a powerful tool. It can be an agent of good when used prop-
erly—but it can also be all-too-easily transformed into an agent of evil. If a
pointer points to memory that has been freed, or if it is accidentally assigned
a nonzero integer or ﬂoating-point value, it becomes a dangerous tool for cor-
rupting memory, because data written through it can quite literally end up
anywhere. Likewise, when pointers are used to keep track of allocated mem-
ory, it is all too easy to forget to free the memory when it is no longer needed.
This leads to memory leaks.
Clearly, good coding practices are one approach to avoiding pointer-related
memory problems. And it is certainly possible to write solid code that essen-
tially never corrupts or leaks memory. Nonetheless, having a tool to help you
detect potential memory corruption and leak problems certainly can’t hurt.
Thankfully, many such tools exist.
My personal favorite is IBM’s Rational Purify, which comes as part of the
Purify Plus toolkit. Purify instruments your code prior to running it, in order
to hook into all pointer dereferences and all memory allocations and deallo-

94
2. Tools of the Trade
cations made by your code. When you run your code under Purify, you get
a live report of the problems—real and potential—encountered by your code.
And when the program exits, you get a detailed memory leak report. Each
problem is linked directly to the source code that caused the problem, mak-
ing tracking down and ﬁxing these kinds of problems relatively easy. You
can ﬁnd more information on Purify at http://www-306.ibm.com/software/
awdtools/purify.
Another popular tool is Bounds Checker by CompuWare. It is similar
to Purify in purpose and functionality. You can ﬁnd more information on
Bounds Checker at https://www.microfocus.com/store/devpartner/bounds
checker.aspx.
2.5
Other Tools
There are a number of other commonly used tools in a game programmer’s
toolkit. We won’t cover them in any depth here, but the following list will
make you aware of their existence and point you in the right direction if you
want to learn more.
•
Difference tools. A difference tool, or diff tool, is a program that compares
two versions of a text ﬁle and determines what has changed between
them. (See http://en.wikipedia.org/wiki/Diff for a discussion of diff
tools.) Diffs are usually calculated on a line-by-line basis, although mod-
ern diff tools can also show you a range of characters on a changed
line that have been modiﬁed. Most version control systems come with
a diff tool. Some programmers like a particular diff tool and conﬁg-
ure their version control software to use the tool of their choice. Popu-
lar tools include ExamDiff (http://www.prestosoft.com/edp_examdiff.
asp), AraxisMerge (http://www.araxis.com), WinDiff (available in the
Options Packs for most Windows versions and available from many in-
dependent websites as well), and the GNU diff tools package (http://
www.gnu.org/software/diffutils/diffutils.html).
•
Three-way merge tools. When two people edit the same ﬁle, two inde-
pendent sets of diffs are generated. A tool that can merge two sets of
diffs into a ﬁnal version of the ﬁle that contains both person’s changes is
called a three-way merge tool. The name “three-way” refers to the fact
that three versions of the ﬁle are involved: the original, user A’s version
and user B’s version. (See http://en.wikipedia.org/wiki/3-way_merge
#Three-way_merge for a discussion of two-way and three-way merge
technologies.) Many merge tools come with an associated diff tool. Some

2.5. Other Tools
95
popular merge tools include AraxisMerge (http://www.araxis.com) and
WinMerge (http://winmerge.org). Perforce also comes with an excellent
three-way merge tool (http://www.perforce.com/perforce/products/
merge.html).
•
Hex editors. A hex editor is a program used for inspecting and mod-
ifying the contents of binary ﬁles. The data are usually displayed as
integers in hexadecimal format, hence the name. Most good hex editors
can display data as integers from one byte to 16 bytes each, in 32- and
64-bit ﬂoating-point format and as ASCII text. Hex editors are particu-
larly useful when tracking down problems with binary ﬁle formats or
when reverse-engineering an unknown binary format—both of which
are relatively common endeavors in game engine development circles.
There are quite literally a million different hex editors out there; I’ve had
good luck with HexEdit by Expert Commercial Software (http://www.
expertcomsoft.com/index.html), but your mileage may vary.
As a game engine programmer you will undoubtedly come across other
tools that make your life easier, but I hope this chapter has covered the main
tools you’ll use on a day-to-day basis.

This page intentionally left blank
This page intentionally left blank

3
Fundamentals of Software
Engineering for Games
I
n this chapter, we’ll brieﬂy review the basic concepts of object-oriented pro-
gramming (OOP) and then delve into some advanced topics that should
prove invaluable in any software engineering endeavor (and especially when
creating games). As with Chapter 2, I hope you will not to skip this chapter
entirely; it’s important that we all embark on our journey with the same set of
tools and supplies.
3.1
C++ Review and Best Practices
Because C++ is arguably the most commonly used language in the game
industry, we will focus primarily on C++ in this book. However, most of
the concepts we’ll cover apply equally well to any object-oriented program-
ming language. Certainly a great many other languages are used in the game
industry—imperative languages like C; object-oriented languages like C# and
Java; scripting languages like Python, Lua and Perl; functional languages like
Lisp, Scheme and F#, and the list goes on. I highly recommend that every
programmer learn at least two high-level languages (the more the merrier), as
97

98
3. Fundamentals of Software Engineering for Games
well as learning at least some assembly language programming. Every new lan-
guage that you learn further expands your horizons and allows you to think in
a more profound and proﬁcient way about programming overall. That being
said, let’s turn our attention now to object-oriented programming concepts in
general, and C++ in particular.
3.1.1
Brief Review of Object-Oriented Programming
Much of what we’ll discuss in this book assumes you have a solid under-
standing of the principles of object-oriented design. If you’re a bit rusty, the
following section should serve as a pleasant and quick review. If you have
no idea what I’m talking about in this section, I recommend you pick up a
book or two on object-oriented programming (e.g., [5]) and C++ in particular
(e.g., [41] and [31]) before continuing.
3.1.1.1
Classes and Objects
A class is a collection of attributes (data) and behaviors (code) that together
form a useful, meaningful whole. A class is a speciﬁcation describing how
individual instances of the class, known as objects, should be constructed. For
example, your pet Rover is an instance of the class “dog.” Thus, there is a
one-to-many relationship between a class and its instances.
3.1.1.2
Encapsulation
Encapsulation means that an object presents only a limited interface to the out-
side world; the object’s internal state and implementation details are kept hid-
den. Encapsulation simpliﬁes life for the user of the class, because he or she
need only understand the class’ limited interface, not the potentially intricate
details of its implementation. It also allows the programmer who wrote the
class to ensure that its instances are always in a logically consistent state.
3.1.1.3
Inheritance
Inheritance allows new classes to be deﬁned as extensions to preexisting classes.
The new class modiﬁes or extends the data, interface and/or behavior of the
existing class. If class Child extends class Parent, we say that Child in-
herits from or is derived from Parent. In this relationship, the class Parent is
known as the base class or superclass, and the class Child is the derived class
or subclass. Clearly, inheritance leads to hierarchical (tree-structured) relation-
ships between classes.

3.1. C++ Review and Best Practices
99
Figure 3.1. UML static class diagram depicting a simple class hierarchy.
Inheritance creates an “is-a” relationship between classes. For example,
a circle is a type of shape. So, if we were writing a 2D drawing application,
it would probably make sense to derive our Circle class from a base class
called Shape.
We can draw diagrams of class hierarchies using the conventions deﬁned
by the Uniﬁed Modeling Language (UML). In this notation, a rectangle rep-
resents a class, and an arrow with a hollow triangular head represents inheri-
tance. The inheritance arrow points from child class to parent. See Figure 3.1
for an example of a simple class hierarchy represented as a UML static class
diagram.
Multiple Inheritance
Some languages support multiple inheritance (MI), meaning that a class can
have more than one parent class. In theory MI can be quite elegant, but in
practice this kind of design usually gives rise to a lot of confusion and techni-
cal difﬁculties (see http://en.wikipedia.org/wiki/Multiple_inheritance). This
is because multiple inheritance transforms a simple tree of classes into a poten-
tially complex graph. A class graph can have all sorts of problems that never
plague a simple tree—for example, the deadly diamond (http://en.wikipedia.
org/wiki/Diamond_problem), in which a derived class ends up containing
two copies of a grandparent base class (see Figure 3.2). (In C++, virtual inheri-
tance allows one to avoid this doubling of the grandparent’s data.) Multiple
inheritance also complicates casting, because the actual address of a pointer
may change depending on which base class it is cast to. This happens because
of the presence of multiple vtable pointers within the object.
Most C++ software developers avoid multiple inheritance completely or
only permit it in a limited form. A common rule of thumb is to allow only
simple, parentless classes to be multiply inherited into an otherwise strictly
single-inheritance hierarchy. Such classes are sometimes called mix-in classes
because they can be used to introduce new functionality at arbitrary points in
a class tree. See Figure 3.3 for a somewhat contrived example of a mix-in class.

100
3. Fundamentals of Software Engineering for Games
ClassA
ClassB
ClassC
ClassD
ClassA
ClassA
ClassB
ClassB’s
memory layout:
ClassA’s
memory layout:
ClassA
ClassC
ClassC’s
memory layout:
ClassA
ClassB
ClassD’s
memory layout:
ClassA
ClassC
ClassD
Figure 3.2. “Deadly diamond” in a multiple inheritance hierarchy.
+Draw()
Shape
+Draw()
Circle
+Draw()
Rectangle
+Draw()
Triangle
+Animate()
Animator
Animator is a hypothetical mix-in
class that adds animation
functionality to whatever class it
is inherited by.
Figure 3.3. Example of a mix-in class.

3.1. C++ Review and Best Practices
101
3.1.1.4
Polymorphism
Polymorphism is a language feature that allows a collection of objects of differ-
ent types to be manipulated through a single common interface. The common
interface makes a heterogeneous collection of objects appear to be homoge-
neous, from the point of view of the code using the interface.
For example, a 2D painting program might be given a list of various shapes
to draw on-screen. One way to draw this heterogeneous collection of shapes
is to use a switch statement to perform different drawing commands for each
distinct type of shape.
void drawShapes(std::list<Shape*> shapes)
{
std::list<Shape*>::iterator pShape = shapes.begin();
std::list<Shape*>::iterator pEnd = shapes.end();
for ( ; pShape != pEnd; pShape++)
{
switch (pShape->mType)
{
case CIRCLE:
// draw shape as a circle
break;
case RECTANGLE:
// draw shape as a rectangle
break;
case TRIANGLE:
// draw shape as a triangle
break;
//...
}
}
}
The problem with this approach is that the drawShapes() function needs
to “know” about all of the kinds of shapes that can be drawn. This is ﬁne in a
simple example, but as our code grows in size and complexity, it can become
difﬁcult to add new types of shapes to the system. Whenever a new shape
type is added, one must ﬁnd every place in the code base where knowledge
of the set of shape types is embedded—like this switch statement—and add a
case to handle the new type.

102
3. Fundamentals of Software Engineering for Games
The solution is to insulate the majority of our code from any knowledge of
the types of objects with which it might be dealing. To accomplish this, we can
deﬁne classes for each of the types of shapes we wish to support. All of these
classes would inherit from the common base class Shape. A virtual function—
the C++ language’s primary polymorphism mechanism—would be deﬁned
called Draw(), and each distinct shape class would implement this function
in a different way. Without “knowing” what speciﬁc types of shapes it has
been given, the drawing function can now simply call each shape’s Draw()
function in turn.
struct Shape
{
virtual void Draw() = 0; // pure virtual function
};
struct Circle : public Shape
{
virtual void Draw()
{
// draw shape as a circle
}
};
struct Rectangle : public Shape
{
virtual void Draw()
{
// draw shape as a rectangle
}
};
struct Triangle : public Shape
{
virtual void Draw()
{
// draw shape as a triangle
}
};
void drawShapes(std::list<Shape*> shapes)
{
std::list<Shape*>::iterator pShape = shapes.begin();
std::list<Shape*>::iterator pEnd = shapes.end();
for ( ; pShape != pEnd; pShape++)
{

3.1. C++ Review and Best Practices
103
pShape->Draw(); // call virtual function
}
}
3.1.1.5
Composition and Aggregation
Composition is the practice of using a group of interacting objects to accomplish
a high-level task. Composition creates a “has-a” or “uses-a” relationship be-
tween classes. (Technically speaking, the “has-a” relationship is called com-
position, while the “uses-a” relationship is called aggregation.) For example, a
spaceship has an engine, which in turn has a fuel tank. Composition/aggrega-
tion usually results in the individual classes being simpler and more focused.
Inexperienced object-oriented programmers often rely too heavily on inheri-
tance and tend to underutilize aggregation and composition.
As an example, imagine that we are designing a graphical user interface
for our game’s front end. We have a class Window that represents any rectan-
gular GUI element. We also have a class called Rectangle that encapsulates
the mathematical concept of a rectangle. A naïve programmer might derive
the Window class from the Rectangle class (using an “is-a” relationship).
But in a more ﬂexible and well-encapsulated design, the Window class would
refer to or contain a Rectangle (employing a “has-a” or “uses-a” relationship).
This makes both classes simpler and more focused and allows the classes to
be more easily tested, debugged and reused.
3.1.1.6
Design Patterns
When the same type of problem arises over and over, and many different pro-
grammers employ a very similar solution to that problem, we say that a design
pattern has arisen. In object-oriented programming, a number of common de-
sign patterns have been identiﬁed and described by various authors. The most
well-known book on this topic is probably the “Gang of Four” book [17].
Here are a few examples of common general-purpose design patterns.
•
Singleton. This pattern ensures that a particular class has only one in-
stance (the singleton instance) and provides a global point of access to it.
•
Iterator. An iterator provides an efﬁcient means of accessing the individ-
ual elements of a collection, without exposing the collection’s underly-
ing implementation. The iterator “knows” the implementation details
of the collection so that its users don’t have to.
•
Abstract factory. An abstract factory provides an interface for creating
families of related or dependent classes without specifying their con-
crete classes.

104
3. Fundamentals of Software Engineering for Games
The game industry has its own set of design patterns for addressing prob-
lems in every realm from rendering to collision to animation to audio. In a
sense, this book is all about the high-level design patterns prevalent in mod-
ern 3D game engine design.
3.1.2
Coding Standards: Why and How Much?
Discussions of coding conventions among engineers can often lead to heated
“religious” debates. I do not wish to spark any such debate here, but I will go
so far as to suggest that following at least a minimal set of coding standards
is a good idea. Coding standards exist for two primary reasons.
1.
Some standards make the code more readable, understandable and main-
tainable.
2.
Other conventions help to prevent programmers from shooting them-
selves in the foot. For example, a coding standard might encourage the
programmer to use only a smaller, more testable and less error-prone
subset of the whole language. The C++ language is rife with possibili-
ties for abuse, so this kind of coding standard is particularly important
when using C++.
In my opinion, the most important things to achieve in your coding conven-
tions are the following.
•
Interfaces are king. Keep your interfaces (.h ﬁles) clean, simple, minimal,
easy to understand and well-commented.
•
Good names encourage understanding and avoid confusion. Stick to intuitive
names that map directly to the purpose of the class, function or vari-
able in question. Spend time up-front identifying a good name. Avoid
a naming scheme that requires programmers to use a look-up table in
order to decipher the meaning of your code. Remember that high-level
programming languages like C++ are intended for humans to read. (If
you disagree, just ask yourself why you don’t write all your software
directly in machine language.)
•
Don’t clutter the global namespace. Use C++ namespaces or a common
naming preﬁx to ensure that your symbols don’t collide with symbols
in other libraries. (But be careful not to overuse namespaces, or nest
them too deeply.) Name #defined symbols with extra care; remember
that C++ preprocessor macros are really just text substitutions, so they
cut across all C/C++ scope and namespace boundaries.
•
Follow C++ best practices. Books like the Effective C++ series by Scott Mey-
ers [31,32], Meyers’ Effective STL [33] and Large-Scale C++ Software Design

3.1. C++ Review and Best Practices
105
by John Lakos [27] provide excellent guidelines that will help keep you
out of trouble.
•
Be consistent. The rule I try to use is as follows: If you’re writing a body of
code from scratch, feel free to invent any convention you like—then stick
to it. When editing preexisting code, try to follow whatever conventions
have already been established.
•
Make errors stick out. Joel Spolsky wrote an excellent article on coding
conventions, which can be found at http://www.joelonsoftware.com/
articles/Wrong.html. Joel suggests that the “cleanest” code is not neces-
sarily code that looks neat and tidy on a superﬁcial level, but rather the
code that is written in a way that makes common programming errors
easier to see. Joel’s articles are always fun and educational, and I highly
recommend this one.
3.1.3
C++11
C++11 is the most-recent variant of the C++ programming language standard.
It was approved by the ISO on August 12, 2011, replacing C++03 (which itself
replaced the ﬁrst standardized version of the language, C++98). C++11 was
formerly known as C++0x.
C++11 introduces a number of new powerful language features. There
are plenty of great online resources and books that describe these features in
detail, so we won’t attempt to cover them here. Instead, we’ll just survey the
key features to serve as a jumping-off point for further reading. However, we
will cover move semantics in some depth because the concepts are a bit tricky
to understand.
3.1.3.1
auto
The auto keyword is not new to the C++ language, but its meaning has
changed for C++11. In C++03 it is a storage class speciﬁer, along with static,
register and extern. Only one of these four speciﬁers can be used on a
given varaible, but the default storage class is auto, meaning that the vari-
able has local scope and should be allocated in a register (if one is available)
or else on the program stack. In C++11, the auto keyword is now used for
variable type inference, meaning it can be used in place of a type speciﬁer—the
compiler infers the type from the right-hand side of the variable’s initializer
expression.
// C++03
float f = 3.141592f;
__m128 acc = _mm_setzero_ps();

106
3. Fundamentals of Software Engineering for Games
std::map<std::string, std::int32_t>::const_iterator it
= myMap.begin();
// C++11
auto f = 3.141592f;
auto acc = _mm_setzero_ps();
auto it = myMap.begin();
3.1.3.2
nullptr
In prior versions of C and C++, a NULL pointer was speciﬁed by using the
literal 0, sometimes cast to (void*) or (char*). This lacked type safety
and could cause problems because of C/C++’s implicit integer conversions.
C++11 introduces the type-safe explicit literal value nullptr to represent a
null pointer; it is an instance of the type std::nullptr_t.
3.1.3.3
Range-Based for Loops
C++11 extends the for statement to support a short-hand “foreach” declara-
tion style. This allows you to iterate over C-style arrays and any other data
structure for which the non-member begin() and end() functions are de-
ﬁned.
// C++03
for (std::map<std::string, std::int32_t>::const_iterator it
= myMap.begin();
it != myMap.end();
it++)
{
printf("%s\n", it->first.c_str());
}
// C++11
for (const auto& pair : myMap)
{
printf("%s\n", pair.first.c_str());
}
3.1.3.4
override and final
The virtual keyword in C++ can lead to confusing and possibly erroneous
code, because the language makes no distinction between:
•
introducing a new virtual function into a class,
•
overriding an inherited virtual function, and
•
implementing a leaf virtual function that is not intended to be overrid-
den by derived classes.

3.1. C++ Review and Best Practices
107
Also, C++ does not require the programmer to use the virtual keyword
on overridden virtual functions at all. To partially rectify this state of affairs,
C++11 introduces two new identiﬁers which can be tacked on to the end of
virtual function declarations, thereby making the programmer’s intent known
to both the compiler and other readers of the code. The override identiﬁer
indicates that this function is an override of a preexisting virtual inherited
from a base class. The final identiﬁer marks the virtual function so it cannot
be overridden by derived classes.
3.1.3.5
Strongly Typed enums
In C++03, an enum exports its enumerators to the surrounding scope, and
the type of its enumerators is determined by the compiler based on the val-
ues present in the enumeration. C++11 introduces a new kind of strongly
typed enumerator, declared using the keywords enum class, which scopes
its enumerators just like a class or struct scopes its members, and permits the
programmer to specify the underlying type.
// C++11
enum class Color : std::int8_t { Red, Green, Blue, White, Black };
Color c = Color::Red;
3.1.3.6
Standardized Smart Pointers
In C++11, std::unique_ptr, std::shared_ptr and std::weak_ptr
provide all the facilities we have come to expect from a solid smart pointer fa-
cility (much like the Boost library’s smart pointer system). std::unique_ptr
is used when we want to maintain sole “ownership” over the object being
pointed to. If we need to maintain multiple pointers to a single object, reference-
counted std::shared_ptrs should be used. A std::weak_ptr acts like a
shared pointer, but it does not contribute to the reference count of the pointed-
to object. As such, weak pointers are generally used as “back pointers” or in
other situations where the pointer “graph” contains cycles.
3.1.3.7
Lambdas
A lambda is an anonymous function.
It can be used anywhere a function
pointer, functor or std::function can be used. The term lambda is bor-
rowed from functional languages like Lisp and Scheme.
Lambdas allow you to write the implementation of a functor inline, rather
than having to declare a named function externally and pass it in. For exam-
ple:

108
3. Fundamentals of Software Engineering for Games
void SomeFunction(const std::vector& v)
{
auto pos = std::find_if(std::begin(v),
std::end(v),
[](int n) { return (n % 2 == 1); });
}
3.1.3.8
Move Semantics and Rvalue References
Prior to C++11, one of the less-efﬁcient aspects of the C++ language was the
way it dealt with copying objects. As an example, consider a function that
multiplies each value within a std::vector by a ﬁxed multiplier and re-
turns a new vector containing the results.
std::vector<float>
MultiplyAllValues(const std::vector<float>& input,
float multiplier)
{
std::vector<float> output(input.size());
for (std::vector<float>::const_iterator
it = input.begin();
it != input.end();
it++)
{
output.push_back(*it * multiplier);
}
return output;
}
void Test()
{
std::vector<float> v;
// fill v with some values...
v = MultiplyAllValues(v, 2.0f);
// use v for something...
}
Any seasoned C++ programmer would balk at this implementation, be-
cause this code makes at least one if not two copies of the std::vector be-
ing returned by the function. The ﬁrst copy happens when we return the local
variable output to the calling code—this copy will probably be optimized
away by the compiler via the return value optimization. But the second copy
cannot be avoided: It happens when the return value is copied back into the
vector v.

3.1. C++ Review and Best Practices
109
Sometimes copying data is necessary and desirable. But in this (rather
contrived) example, the copying is totally unnecessary because the source ob-
ject (i.e., the vector returned by the function) is a temporary object. It will be
thrown away immediately after being copied into v. Most good C++ pro-
grammers (again, prior to C++11) would probably suggest that we rewrite
the function as follows to avoid the unnecessary copying:
void MultiplyAllValues(std::vector<float>& output,
const std::vector<float>& input,
float multiplier)
{
output.resize(0);
output.reserve(input.size());
for (std::vector<float>::const_iterator it = input.begin();
it != input.end();
it++)
{
output.push_back(*it * multiplier);
}
}
Or we might consider making the function less general-purpose by having it
modify its input in place.
C++11 provides a mechanism that allows us to rectify these kinds of copy-
ing problems without having to change the function signature to pass the out-
put object into the function by pointer or reference. This mechanism is known
as move semantics, and it depends on being able to tell the difference between
copying an lvalue object and copying an rvalue (temporary) object.
In C and C++, an lvalue represents an actual storage location in the com-
puter’s registers or memory. An rvalue is a temporary data object that ex-
ists logically but doesn’t necessarily occupy any memory. When we write
int a = 7; the variable a is an lvalue, but the literal 7 is an rvalue. You can
assign to an lvalue, but you can’t assign to an rvalue.
In C++03 and prior, there was no way to handle copying of rvalues differ-
ently from copying lvalues. Therefore, the copy constructor and assignment
operator had to assume the worst and treat everything like an lvalue. In the
case of copying a container object like a std::vector, the copy construc-
tor and assignment operator would have to perform a deep copy—copying not
only the container object itself but all of the data it contains.
In C++11, we can declare a variable to be an rvalue reference by using a
double ampersand instead of a single ampersand (e.g., int&& rvalueRef
instead of int& lvalueRef). This in turn allows us to write two overloaded

110
3. Fundamentals of Software Engineering for Games
variants of both the copy constructor and the assignment operator—one for
lvalues and one for rvalues. When we copy an lvalue, we do a full deep copy
as always. But when we copy an rvalue (i.e., a temporary object), we needn’t
perform a deep copy. Instead, we can simply “steal” the contents of the tem-
porary object and move them directly into the destination object—hence the
term move semantics. For example, the copy constructors and assignment op-
erators for a simpliﬁed implementation of std::vector could be written
something like this:
namespace std
{
template<typename T>
class vector
{
private:
T* m_array;
int m_count;
public:
// lvalue copy ctor
vector<T>(const vector<T>& original)
: m_array(nullptr)
, m_count(original.size())
{
if (m_count != 0)
{
m_array = new T[m_count];
if (m_array != nullptr)
memcpy(m_array, original.m_array,
m_count * sizeof(T));
else
m_count = 0;
}
}
// rvalue "move" ctor
vector<T>(vector<T>&& original)
: m_array(original.m_array) // steal the data
, m_count(original.m_count)
{
original.m_array = nullptr; // stolen goods!
original.m_count = 0;
}

3.1. C++ Review and Best Practices
111
// lvalue assignment operator
vector<T>& operator=(const vector<T>& original)
{
if (this != &original)
{
m_array = nullptr;
m_count = original.size();
if (m_count != 0)
{
m_array = new T[m_count];
if (m_array != nullptr)
memcpy(m_array, original.m_array,
m_count * sizeof(T));
else
m_count = 0;
}
}
return *this;
}
// rvalue "move" assignment operator
vector<T>& operator=(vector<T>&& original)
{
if (this != &original)
{
m_array = original.m_array; // steal the data
m_count = original.m_count;
original.m_array = nullptr; // stolen goods!
original.m_count = 0;
}
return *this;
}
// ...
};
}
There is one additional subtlety here. An rvalue reference is itself an lvalue
(not an rvalue as one might think). In other words, you can assign to or modify
an rvalue reference variable. That’s what allows us to set original.m_array
to nullptr in the example code above. As such, if you want to explicitly in-
voke a move constructor or move assignment operator on an rvalue reference
variable, you have to wrap it in a call to std::move() to force the compiler

112
3. Fundamentals of Software Engineering for Games
into thinking your rvalue reference is an rvalue. Confused yet? Never fear,
with a bit of practice it will all make sense. For more information on move se-
mantics, see http://www.cprogramming.com/c++11/rvalue-references-and
-move-semantics-in-c++11.html.
3.2
Data, Code and Memory
3.2.1
Numeric Representations
Numbers are at the heart of everything that we do in game engine develop-
ment (and software development in general). Every software engineer should
understand how numbers are represented and stored by a computer. This
section will provide you with the basics you’ll need throughout the rest of the
book.
3.2.1.1
Numeric Bases
People think most naturally in base ten, also known as decimal notation. In
this notation, ten distinct digits are used (0 through 9), and each digit from
right to left represents the next highest power of 10. For example, the number
7803 = (7 × 103) + (8 × 102) + (0 × 101) + (3 × 100) = 7000 + 800 + 0 + 3.
In computer science, mathematical quantities such as integers and real-
valued numbers need to be stored in the computer’s memory. And as we
know, computers store numbers in binary format, meaning that only the two
digits 0 and 1 are available. We call this a base-two representation, because
each digit from right to left represents the next highest power of 2. Com-
puter scientists sometimes use a preﬁx of “0b” to represent binary numbers.
For example, the binary number 0b1101 is equivalent to decimal 13, because
0b1101 = (1 × 23) + (1 × 22) + (0 × 21) + (1 × 20) = 8 + 4 + 0 + 1 = 13.
Another common notation popular in computing circles is hexadecimal, or
base 16. In this notation, the 10 digits 0 through 9 and the six letters A through
F are used; the letters A through F replace the decimal values 10 through 15,
respectively. A preﬁx of “0x” is used to denote hex numbers in the C and C++
programming languages. This notation is popular because computers gener-
ally store data in groups of 8 bits known as bytes, and since a single hexadec-
imal digit represents 4 bits exactly, a pair of hex digits represents a byte. For
example, the value 0xFF = 0b11111111 = 255 is the largest number that can
be stored in 8 bits (1 byte). Each digit in a hexadecimal number, from right
to left, represents the next power of 16. So, for example, 0xB052 = (11 × 163)
+ (0×162)+(5×161)+(2×160) = (11×4096)+(0×256)+(5×16)+(2×1) =
45,138.

3.2. Data, Code and Memory
113
3.2.1.2
Signed and Unsigned Integers
In computer science, we use both signed and unsigned integers. Of course,
the term “unsigned integer” is actually a bit of a misnomer—in mathematics,
the whole numbers or natural numbers range from 0 (or 1) up to positive inﬁnity,
while the integers range from negative inﬁnity to positive inﬁnity. Neverthe-
less, we’ll use computer science lingo in this book and stick with the terms
“signed integer” and “unsigned integer.”
Most modern personal computers and game consoles work most easily
with integers that are 32 bits or 64 bits wide (although 8- and 16-bit integers
are also used a great deal in game programming as well). To represent a
32-bit unsigned integer, we simply encode the value using binary notation
(see above).
The range of possible values for a 32-bit unsigned integer is
0x00000000 (0) to 0xFFFFFFFF (4,294,967,295).
To represent a signed integer in 32 bits, we need a way to differentiate be-
tween positive and negative vales. One simple approach called the sign and
magnitude encoding reserves the most signiﬁcant bit as a sign bit. When this
bit is zero, the value is positive, and when it is one, the value is negative. This
leaves us 31 bits to represent the magnitude of the value, effectively cutting
the range of possible magnitudes in half (but allowing both positive and neg-
ative forms of every distinct magnitude, including zero).
Most microprocessors use a slightly more efﬁcient technique for encoding
negative integers, called two’s complement notation. This notation has only
one representation for the value zero, as opposed to the two representations
possible with simple sign bit (positive zero and negative zero). In 32-bit two’s
complement notation, the value 0xFFFFFFFF is interpreted to mean −1, and
negative values count down from there. Any value with the most signiﬁcant
bit set is considered negative. So values from 0x00000000 (0) to 0x7FFFFFFF
(2,147,483,647) represent positive integers, and 0x80000000 (−2,147,483,648)
to 0xFFFFFFFF (−1) represent negative integers.
3.2.1.3
Fixed-Point Notation
Integers are great for representing whole numbers, but to represent fractions
and irrational numbers we need a different format that expresses the concept
of a decimal point.
One early approach taken by computer scientists was to use ﬁxed-point no-
tation. In this notation, one arbitrarily chooses how many bits will be used
to represent the whole part of the number, and the rest of the bits are used
to represent the fractional part. As we move from left to right (i.e., from the
most signiﬁcant bit to the least signiﬁcant bit), the magnitude bits represent

114
3. Fundamentals of Software Engineering for Games
31
15
0
magnitude (16 bits)
fraction (15 bits)
1
= –173.25
sign
0x80
0x56
0xA0
0x00
1 0 0 0 0 0 0 0 0 1 0 1 0 1 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0
Figure 3.4. Fixed-point notation with 16-bit magnitude and 16-bit fraction.
decreasing powers of two (..., 16, 8, 4, 2, 1), while the fractional bits represent
decreasing inverse powers of two ( 1
2, 1
4, 1
8, 1
16, . . .). For example, to store the
number −173.25 in 32-bit ﬁxed-point notation with one sign bit, 16 bits for the
magnitude and 15 bits for the fraction, we ﬁrst convert the sign, the whole part
and the fractional part into their binary equivalents individually (negative =
0b1, 173 = 0b0000000010101101 and 0.25 = 1
4 = 0b010000000000000). Then we
pack those values together into a 32-bit integer. The ﬁnal result is 0x8056A000.
This is illustrated in Figure 3.4.
The problem with ﬁxed-point notation is that it constrains both the range
of magnitudes that can be represented and the amount of precision we can
achieve in the fractional part. Consider a 32-bit ﬁxed-point value with 16 bits
for the magnitude, 15 bits for the fraction and a sign bit. This format can
only represent magnitudes up to ±65,535, which isn’t particularly large. To
overcome this problem, we employ a ﬂoating-point representation.
3.2.1.4
Floating-Point Notation
In ﬂoating-point notation, the position of the decimal place is arbitrary and
is speciﬁed with the help of an exponent. A ﬂoating-point number is broken
into three parts: the mantissa, which contains the relevant digits of the number
on both sides of the decimal point, the exponent, which indicates where in that
string of digits the decimal point lies, and a sign bit, which of course indicates
whether the value is positive or negative. There are all sorts of different ways
to lay out these three components in memory, but the most common standard
is IEEE-754. It states that a 32-bit ﬂoating-point number will be represented
with the sign in the most signiﬁcant bit, followed by 8 bits of exponent and
ﬁnally 23 bits of mantissa.
The value v represented by a sign bit s, an exponent e and a mantissa m is
v = s × 2(e−127) × (1 + m).
The sign bit s has the value +1 or −1. The exponent e is biased by 127 so
that negative exponents can be easily represented. The mantissa begins with
an implicit 1 that is not actually stored in memory, and the rest of the bits are

3.2. Data, Code and Memory
115
0
31
23
0
exponent (8 bits)
0 1 1 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
mantissa (23 bits)
sign
= 0.15625
Figure 3.5. IEEE-754 32-bit ﬂoating-point format.
interpreted as inverse powers of two. Hence the value represented is really
1 + m, where m is the fractional value stored in the mantissa.
For example, the bit pattern shown in Figure 3.5 represents the value
0.15625, because s = 0 (indicating a positive number), e = 0b01111100 = 124
and m = 0b0100... = 0 × 2−1 + 1 × 2−2 = 1
4. Therefore,
v = s × 2(e−127) × (1 + m)
= (+1) × 2(124−127) × (1 + 1
4)
= 2−3 × 5
4
= 1
8 × 5
4
= 0.125 × 1.25 = 0.15625.
The Trade-Off between Magnitude and Precision
The precision of a ﬂoating-point number increases as the magnitude decreases,
and vice versa. This is because there are a ﬁxed number of bits in the mantissa,
and these bits must be shared between the whole part and the fractional part
of the number. If a large percentage of the bits are spent representing a large
magnitude, then a small percentage of bits are available to provide fractional
precision. In physics the term signiﬁcant digits is typically used to describe this
concept (http://en.wikipedia.org/wiki/Signiﬁcant_digits).
To understand the trade-off between magnitude and precision, let’s look
at the largest possible ﬂoating-point value, FLT_MAX ≈3.403 × 1038, whose
representation in 32-bit IEEE ﬂoating-point format is 0x7F7FFFFF. Let’s break
this down:
•
The largest absolute value that we can represent with a 23-bit mantissa
is 0x00FFFFFF in hexadecimal, or 24 consecutive binary ones—that’s 23
ones in the mantissa, plus the implicit leading one.
•
An exponent of 255 has a special meaning in the IEEE-754 format—it is
used for values like not-a-number (NaN) and inﬁnity—so it cannot be
used for regular numbers. Hence the maximum eight-bit exponent is
actually 254, which translates into 127 after subtracting the implicit bias
of 127.

116
3. Fundamentals of Software Engineering for Games
So FLT_MAX is 0x00FFFFFF × 2127 = 0xFFFFFF00000000000000000000000000.
In other words, our 24 binary ones were shifted up by 127 bit positions, leav-
ing 127 −23 = 104 binary zeros (or 104/4 = 26 hexadecimal zeros) after the
least signiﬁcant digit of the mantissa. Those trailing zeros don’t correspond to
any actual bits in our 32-bit ﬂoating-point value—they just appear out of thin
air because of the exponent. If we were to subtract a small number (where
“small” means any number composed of fewer than 26 hexadecimal digits)
from FLT_MAX, the result would still be FLT_MAX, because those 26 least sig-
niﬁcant hexadecimal digits don’t really exist!
The opposite effect occurs for ﬂoating-point values whose magnitudes are
much less than one. In this case, the exponent is large but negative, and the
signiﬁcant digits are shifted in the opposite direction. We trade the ability to
represent large magnitudes for high precision. In summary, we always have
the same number of signiﬁcant digits (or really signiﬁcant bits) in our ﬂoating-
point numbers, and the exponent can be used to shift those signiﬁcant bits
into higher or lower ranges of magnitude.
Another subtlety to notice is that there is a ﬁnite gap between zero and the
smallest nonzero value we can represent with any ﬂoating-point notation. The
smallest nonzero magnitude we can represent is FLT_MIN = 2−126 ≈1.175 ×
10−38, which has a binary representation of 0x00800000 (i.e., the exponent is
0x01, or −126 after subtracting the bias, and the mantissa is all zeros except
for the implicit leading one). The next smallest valid value is zero, so there is
a ﬁnite gap between the values -FLT_MIN and +FLT_MIN. Put another way,
the real number line is quantized when using a ﬂoating-point representation.
The gap around zero can be ﬁlled by employing an extension to the ﬂoating-
point representation known as denormalized values, also known as subnormal
values. When this extension is used, any ﬂoating-point value with a biased
exponent of 0 is interpreted as a subnormal number. The exponent is treated
as if it had been a 1 instead of a 0, and the implicit leading 1 that normally
sits in front of the bits of the mantissa is changed to a 0. This has the effect of
ﬁlling the gap between -FLT_MIN and +FLT_MIN with a linear sequence of
subnormal values. However, the real number line is still quantized of course.
The beneﬁt of using subnormal values is merely that it provides greater pre-
cision near zero by ﬁlling the gap between -FLT_MIN and +FLT_MIN with a
ﬁnite sequence of discrete values.
For a particular ﬂoating-point representation, the machine epsilon is deﬁned
to be the smallest ﬂoating-point value ε that satisﬁes the equation, 1 + ε ̸= 1.
For an IEEE-754 ﬂoating-point number, with its 23 bits of precision, the value
of ε is 2−23, which is approximately 1.192 × 10−7. The most signiﬁcant digit
of ε falls just inside the range of signiﬁcant digits in the value 1.0, so adding

3.2. Data, Code and Memory
117
any value smaller than ε to 1.0 has no effect. In other words, any new bits
contributed adding a value smaller than ε will get “chopped off” when we try
to ﬁt the sum into a mantissa with only 23 bits.
The concepts of limited precision and the machine epsilon have real im-
pacts on game software. For example, let’s say we use a ﬂoating-point vari-
able to track absolute game time in seconds. How long can we run our game
before the magnitude of our clock variable gets so large that adding 1/30th of
a second to it no longer changes its value? The answer is roughly 12.9 days.
That’s longer than most games will be left running, so we can probably get
away with using a 32-bit ﬂoating-point clock measured in seconds in a game.
But clearly it’s important to understand the limitations of the ﬂoating-point
format so that we can predict potential problems and take steps to avoid them
when necessary.
IEEE Floating-Point Bit Tricks
See [7, Section 2.1] for a few really useful IEEE ﬂoating-point “bit tricks” that
can make ﬂoating-point calculations lightning fast.
3.2.1.5
Atomic Data Types
As you know, C and C++ provide a number of atomic data types. The C
and C++ standards provide guidelines on the relative sizes and signedness of
these data types, but each compiler is free to deﬁne the types slightly differ-
ently in order to provide maximum performance on the target hardware.
•
char. A char is usually 8 bits and is generally large enough to hold an
ASCII or UTF-8 character (see Section 5.4.4.1). Some compilers deﬁne
char to be signed, while others use unsigned chars by default.
•
int, short, long. An int is supposed to hold a signed integer value
that is the most efﬁcient size for the target platform; it is usually deﬁned
to be 32 bits wide on a 32-bit CPU architecture, such as Pentium 4 or
Xeon, and 64 bits wide on a 64-bit architecture, such as Intel Core i7,
although the size of an int is also dependent upon other factors such as
compiler options and the target operating system. A short is intended
to be smaller than an int and is 16 bits on many machines. A long is
as large as or larger than an int and may be 32 or 64 bits wide, or even
wider, again depending on CPU architecture, compiler options and the
target OS.
•
float. On most modern compilers, a float is a 32-bit IEEE-754 ﬂoating-
point value.

118
3. Fundamentals of Software Engineering for Games
•
double. A double is a double-precision (i.e., 64-bit) IEEE-754 ﬂoating-
point value.
•
bool. A bool is a true/false value. The size of a bool varies widely
across different compilers and hardware architectures. It is never imple-
mented as a single bit, but some compilers deﬁne it to be 8 bits while
others use a full 32 bits.
Compiler-Speciﬁc Sized Types
The standard C/C++ atomic data types were designed to be portable and
therefore nonspeciﬁc. However, in many software engineering endeavors,
including game engine programming, it is often important to know exactly
how wide a particular variable is. The Visual Studio C/C++ compiler deﬁnes
the following extended keywords for declaring variables that are an explicit
number of bits wide: __int8, __int16, __int32 and __int64.
SIMD Types
The CPUs on many modern computers and game consoles have a specialized
type of arithmetic logic unit (ALU) referred to as a vector processor or vector
unit. A vector processor supports a form of parallel processing known as sin-
gle instruction, multiple data (SIMD), in which a mathematical operation is per-
formed on multiple quantities in parallel, using a single machine instruction.
In order to be processed by the vector unit, two or more quantities are packed
into a 64- or 128-bit CPU register. In game programming, the most commonly
used SIMD register format packs four 32-bit IEEE-754 ﬂoating-point quanti-
ties into a 128-bit SIMD register. This format allows us to perform calculations
such as vector dot products and matrix multiplications much more efﬁciently
than would be possible with a SISD (single instruction, single data) ALU.
Each microprocessor has a different name for its SIMD instruction set, and
the compilers that target those microprocessors use a custom syntax to declare
SIMD variables. For example, on a Pentium class CPU, the SIMD instruction
set is known as SSE (streaming SIMD extensions), and the Microsoft Visual
Studio compiler provides the built-in data type __m128 to represent a four-
ﬂoat SIMD quantity. The PowerPC class of CPUs used on the PlayStation 3
and Xbox 360 calls its SIMD instruction set Altivec, and the Gnu C++ compiler
uses the syntax vector float to declare a packed four-ﬂoat SIMD variable.
We’ll discuss how SIMD programming works in more detail in Section 4.7.
Portable Sized Types
Most other compilers have their own “sized” data types, with similar seman-
tics but slightly different syntax. Because of these differences between compil-

3.2. Data, Code and Memory
119
ers, most game engines achieve source code portability by deﬁning their own
custom atomic data types. For example, at Naughty Dog we use the following
atomic types:
•
F32 is a 32-bit IEEE-754 ﬂoating-point value.
•
U8, I8, U16, I16, U32, I32, U64 and I64 are unsigned and signed 8-,
16-, 32- and 64-bit integers, respectively.
•
VF32 represents a packed four-ﬂoat SIMD value.
<cstdint>
The C++11 standard library introduces a set of standardized sized integer
types.
They are declared in the <cstdint> header, and they include
the signed types std::int8_t, std::int16_t, std::int32_t and
std::int64_t and the unsigned types std::uint8_t, std::uint16_t,
std::uint32_t and std::uint64_t.
OGRE’s Atomic Data Types
OGRE deﬁnes a number of atomic types of its own. Ogre::uint8, Ogre::
uint16 and Ogre::uint32 are the basic unsigned sized integral types.
Ogre::Real deﬁnes a real ﬂoating-point value. It is usually deﬁned to be
32 bits wide (equivalent to a float), but it can be redeﬁned globally to be 64
bits wide (like a double) by deﬁning the preprocessor macro OGRE_DOUBLE
_PRECISION to 1. This ability to change the meaning of Ogre::Real is
generally only used if one’s game has a particular requirement for double-
precision math, which is rare. Graphics chips (GPUs) always perform their
math with 32-bit or 16-bit ﬂoats, the CPU/FPU is also usually faster when
working in single-precision, and SIMD vector instructions operate on 128-bit
registers that contain four 32-bit ﬂoats each. Hence, most games tend to stick
to single-precision ﬂoating-point math.
The data types Ogre::uchar, Ogre::ushort, Ogre::uint and Ogre
::ulong are just shorthand notations for C/C++’s unsigned char,
unsigned short and unsigned long, respectively. As such, they are no
more or less useful than their native C/C++ counterparts.
The types Ogre::Radian and Ogre::Degree are particularly interest-
ing. These classes are wrappers around a simple Ogre::Real value. The
primary role of these types is to permit the angular units of hard-coded literal
constants to be documented and to provide automatic conversion between
the two unit systems. In addition, the type Ogre::Angle represents an angle
in the current “default” angle unit. The programmer can deﬁne whether the
default will be radians or degrees when the OGRE application ﬁrst starts up.

120
3. Fundamentals of Software Engineering for Games
Perhaps surprisingly, OGRE does not provide a number of sized atomic
data types that are commonplace in other game engines. For example, it de-
ﬁnes no signed 8-, 16- or 64-bit integral types. If you are writing a game engine
on top of OGRE, you will probably ﬁnd yourself deﬁning these types manu-
ally at some point.
3.2.1.6
Multibyte Values and Endianness
Values that are larger than eight bits (one byte) wide are called multibyte quan-
tities. They’re commonplace on any software project that makes use of in-
tegers and ﬂoating-point values that are 16 bits or wider. For example, the
integer value 4660 = 0x1234 is represented by the two bytes 0x12 and 0x34.
We call 0x12 the most signiﬁcant byte (MSB) and 0x34 the least signiﬁcant
byte (LSB). In a 32-bit value, such as 0xABCD1234, the MSB is 0xAB and the
LSB is 0x34. The same concepts apply to 64-bit integers and to 32- and 64-bit
ﬂoating-point values as well.
Multibyte integers can be stored into memory in one of two ways, and
different microprocessors may differ in their choice of storage method (see
Figure 3.6).
•
Little-endian. If a microprocessor stores the least signiﬁcant byte (LSB) of
a multibyte value at a lower memory address than the most signiﬁcant
byte (MSB), we say that the processor is little-endian. On a little-endian
machine, the number 0xABCD1234 would be stored in memory using
the consecutive bytes 0x34, 0x12, 0xCD, 0xAB.
•
Big-endian. If a microprocessor stores the most signiﬁcant byte of a multi-
byte value at a lower memory address than the least signiﬁcant byte,
U32 value = 0xABCD1234;
U8* pBytes = (U8*)&value;
Figure 3.6. Big- and little-endian representations of the value 0xABCD1234.

3.2. Data, Code and Memory
121
we say that the processor is big-endian. On a big-endian machine, the
number 0xABCD1234 would be stored in memory using the bytes 0xAB,
0xCD, 0x12, 0x34.
Most programmers don’t need to think much about endianness. However,
when you’re a game programmer, endianness can become a bit of a thorn in
your side. This is because games are usually developed on a Windows or Linux
machine running an Intel Pentium processor (which is little-endian), but run
on a console such as the Wii, Xbox 360 or PlayStation 3—all three of which
utilize a variant of the PowerPC processor (which can be conﬁgured to use
either endianness, but is big-endian by default). Now imagine what happens
when you generate a data ﬁle for consumption by your game engine on an
Intel processor and then try to load that data ﬁle into your engine running on
a PowerPC processor. Any multibyte value that you wrote out into that data
ﬁle will be stored in little-endian format. But when the game engine reads
the ﬁle, it expects all of its data to be in big-endian format. The result? You’ll
write 0xABCD1234, but you’ll read 0x3412CDAB, and that’s clearly not what
you intended!
There are at least two solutions to this problem.
1.
You could write all your data ﬁles as text and store all multibyte num-
bers as sequences of decimal or hexadecimal digits, one character (one
byte) per digit. This would be an inefﬁcient use of disk space, but it
would work.
2.
You can have your tools endian-swap the data prior to writing it into
a binary data ﬁle. In effect, you make sure that the data ﬁle uses the
endianness of the target microprocessor (the game console), even if the
tools are running on a machine that uses the opposite endianness.
Integer Endian-Swapping
Endian-swapping an integer is not conceptually difﬁcult. You simply start at
the most signiﬁcant byte of the value and swap it with the least signiﬁcant
byte; you continue this process until you reach the halfway point in the value.
For example, 0xA7891023 would become 0x231089A7.
The only tricky part is knowing which bytes to swap. Let’s say you’re writ-
ing the contents of a C struct or C++ class from memory out to a ﬁle. To
properly endian-swap this data, you need to keep track of the locations and
sizes of each data member in the struct and swap each one appropriately
based on its size. For example, the structure

122
3. Fundamentals of Software Engineering for Games
struct Example
{
U32
m_a;
U16
m_b;
U32
m_c;
};
might be written out to a data ﬁle as follows:
void writeExampleStruct(Example& ex, Stream& stream)
{
stream.writeU32(swapU32(ex.m_a));
stream.writeU16(swapU16(ex.m_b));
stream.writeU32(swapU32(ex.m_c));
}
and the swap functions might be deﬁned like this:
inline U16 swapU16(U16 value)
{
return ((value & 0x00FF) << 8)
| ((value & 0xFF00) >> 8);
}
inline U32 swapU32(U32 value)
{
return ((value & 0x000000FF) << 24)
| ((value & 0x0000FF00) << 8)
| ((value & 0x00FF0000) >> 8)
| ((value & 0xFF000000) >> 24);
}
You cannot simply cast the Example object into an array of bytes and
blindly swap the bytes using a single general-purpose function. We need to
know both which data members to swap and how wide each member is, and each
data member must be swapped individually.
Floating-Point Endian-Swapping
Let’s take a brief look at how ﬂoating-point endian-swapping differs from in-
teger endian-swapping. As we’ve seen, an IEEE-754 ﬂoating-point value has
a detailed internal structure involving some bits for the mantissa, some bits
for the exponent and a sign bit. However, you can endian-swap it just as if
it were an integer, because bytes are bytes. You can reinterpret ﬂoats as inte-
gers by using C++’s reinterpret_cast operator on a pointer to the ﬂoat;
this is known as type punning. But punning can lead to optimization bugs

3.2. Data, Code and Memory
123
when strict aliasing is enabled. (See http://www.cocoawithlove.com/2008/
04/using-pointers-to-recast-in-c-is-bad.html for an excellent description of
this problem.) One convenient approach is to use a union, as follows:
union U32F32
{
U32
m_asU32;
F32
m_asF32;
};
inline F32 swapF32(F32 value)
{
U32F32 u;
u.m_asF32 = value;
// endian-swap as integer
u.m_asU32 = swapU32(u.m_asU32);
return u.m_asF32;
}
3.2.2
Declarations, Deﬁnitions and Linkage
3.2.2.1
Translation Units Revisited
As we saw in Chapter 2, a C or C++ program is comprised of translation units.
The compiler translates one .cpp ﬁle at a time, and for each one it generates
an output ﬁle called an object ﬁle (.o or .obj). A .cpp ﬁle is the smallest unit of
translation operated on by the compiler; hence, the name “translation unit.”
An object ﬁle contains not only the compiled machine code for all of the func-
tions deﬁned in the .cpp ﬁle, but also all of its global and static variables.
In addition, an object ﬁle may contain unresolved references to functions and
global variables deﬁned in other .cpp ﬁles.
The compiler only operates on one translation unit at a time, so whenever
it encounters a reference to an external global variable or function, it must
“go on faith” and assume that the entity in question really exists, as shown
in Figure 3.7. It is the linker’s job to combine all of the object ﬁles into a
ﬁnal executable image. In doing so, the linker reads all of the object ﬁles and
attempts to resolve all of the unresolved cross-references between them. If it
is successful, an executable image is generated containing all of the functions,
global variables and static variables, with all cross-translation-unit references
properly resolved. This is depicted in Figure 3.8.
The linker’s primary job is to resolve external references, and in this ca-
pacity it can generate only two kinds of errors:

124
3. Fundamentals of Software Engineering for Games
foo.cpp
U32 gGlobalA;
U32 gGlobalB;
void f()
{
    // ...
    gGlobalC = 5.3f;
    // ...
}
extern U32 gGlobalC;
bar.cpp
F32 gGlobalC;
void g()
{
    // ...
    U32 a = gGlobalA;
    // ...
    f();
    // ...
    gGlobalB = 0;
}
extern U32 gGlobalA;
extern U32 gGlobalB;
extern void f();
Figure 3.7. Unresolved external references in two translation units.
foo.cpp
U32 gGlobalA;
U32 gGlobalB;
void f()
{
    // ...
    gGlobalC = 5.3f;
    // ...
}
extern U32 gGlobalC;
bar.cpp
F32 gGlobalC;
void g()
{
    // ...
    U32 a = gGlobalA;
    // ...
    f();
    // ...
    gGlobalB = 0;
}
extern U32 gGlobalA;
extern U32 gGlobalB;
extern void f();
Figure 3.8. Fully resolved external references after successful linking.
???
Unresolved Reference
???
Multiply-Defined Symbol
???
foo.cpp
U32 gGlobalA;
U32 gGlobalB;
void f()
{
    // ...
    gGlobalC = 5.3f;
    gGlobalD = -2;
    // ...
}
extern U32 gGlobalC;
bar.cpp
F32 gGlobalC;
void g()
{
    // ...
    U32 a = gGlobalA;
    // ...
    f();
    // ...
    gGlobalB = 0;
}
extern U32 gGlobalA;
extern U32 gGlobalB;
extern void f();
spam.cpp
U32 gGlobalA;
void h()
{
    // ...
}
Figure 3.9. The two most common linker errors.

3.2. Data, Code and Memory
125
1.
The target of an extern reference might not be found, in which case the
linker generates an “unresolved symbol” error.
2.
The linker might ﬁnd more than one variable or function with the same
name, in which case it generates a “multiply deﬁned symbol” error.
These two situations are shown in Figure 3.9.
3.2.2.2
Declaration versus Deﬁnition
In the C and C++ languages, variables and functions must be declared and
deﬁned before they can be used. It is important to understand the difference
between a declaration and a deﬁnition in C and C++.
•
A declaration is a description of a data object or function. It provides the
compiler with the name of the entity and its data type or function signature
(i.e., return type and argument type(s)).
•
A deﬁnition, on the other hand, describes a unique region of memory in
the program. This memory might contain a variable, an instance of a
struct or class or the machine code of a function.
In other words, a declaration is a reference to an entity, while a deﬁnition
is the entity itself. A deﬁnition is always a declaration, but the reverse is not
always the case—it is possible to write a pure declaration in C and C++ that
is not a deﬁnition.
Functions are deﬁned by writing the body of the function immediately after
the signature, enclosed in curly braces:
foo.cpp
// definition of the max() function
int max(int a, int b)
{
return (a > b) ? a : b;
}
// definition of the min() function
int min(int a, int b)
{
return (a <= b) ? a : b;
}
A pure declaration can be provided for a function so that it can be used in
other translation units (or later in the same translation unit). This is done by

126
3. Fundamentals of Software Engineering for Games
writing a function signature followed by a semicolon, with an optional preﬁx
of extern:
foo.h
extern int max(int a, int b); // a function declaration
int min(int a, int b); // also a declaration (the extern
// is optional/assumed)
Variables and instances of classes and structs are deﬁned by writing the
data type followed by the name of the variable or instance and an optional
array speciﬁer in square brackets:
foo.cpp
// All of these are variable definitions:
U32 gGlobalInteger = 5;
F32 gGlobalFloatArray[16];
MyClass gGlobalInstance;
A global variable deﬁned in one translation unit can optionally be declared for
use in other translation units by using the extern keyword:
foo.h
// These are all pure declarations:
extern U32 gGlobalInteger;
extern F32 gGlobalFloatArray[16];
extern MyClass gGlobalInstance;
Multiplicity of Declarations and Deﬁnitions
Not surprisingly, any particular data object or function in a C/C++ program
can have multiple identical declarations, but each can have only one deﬁnition.
If two or more identical deﬁnitions exist in a single translation unit, the com-
piler will notice that multiple entities have the same name and ﬂag an er-
ror. If two or more identical deﬁnitions exist in different translation units, the
compiler will not be able to identify the problem, because it operates on one
translation unit at a time. But in this case, the linker will give us a “multiply
deﬁned symbol” error when it tries to resolve the cross-references.
Deﬁnitions in Header Files and Inlining
It is usually dangerous to place deﬁnitions in header ﬁles. The reason for this
should be pretty obvious: if a header ﬁle containing a deﬁnition is #included

3.2. Data, Code and Memory
127
into more than one .cpp ﬁle, it’s a sure-ﬁre way of generating a “multiply
deﬁned symbol” linker error.
Inline function deﬁnitions are an exception to this rule, because each invo-
cation of an inline function gives rise to a brand new copy of that function’s
machine code, embedded directly into the calling function. In fact, inline func-
tion deﬁnitions must be placed in header ﬁles if they are to be used in more
than one translation unit. Note that it is not sufﬁcient to tag a function decla-
ration with the inline keyword in a .h ﬁle and then place the body of that
function in a .cpp ﬁle. The compiler must be able to “see” the body of the
function in order to inline it. For example:
foo.h
// This function definition will be inlined properly.
inline int max(int a, int b)
{
return (a > b) ? a : b;
}
// This declaration cannot be inlined because the
// compiler cannot "see" the body of the function.
inline int min(int a, int b);
foo.cpp
// The body of min() is effectively "hidden" from the
// compiler, so it can ONLY be inlined within foo.cpp.
int min(int a, int b)
{
return (a <= b) ? a : b;
}
The inline keyword is really just a hint to the compiler. It does a cost/
beneﬁt analysis of each inline function, weighing the size of the function’s
code versus the potential performance beneﬁts of inling it, and the compiler
gets the ﬁnal say as to whether the function will really be inlined or not. Some
compilers provide syntax like __forceinline, allowing the programmer
to bypass the compiler’s cost/beneﬁt analysis and control function inlining
directly.
3.2.2.3
Linkage
Every deﬁnition in C and C++ has a property known as linkage. A deﬁnition
with external linkage is visible to and can be referenced by translation units

128
3. Fundamentals of Software Engineering for Games
other than the one in which it appears. A deﬁnition with internal linkage can
only be “seen” inside the translation unit in which it appears and thus cannot
be referenced by other translation units. We call this property linkage because
it dictates whether or not the linker is permitted to cross-reference the entity
in question. So, in a sense, linkage is the translation unit’s equivalent of the
public: and private: keywords in C++ class deﬁnitions.
By default, deﬁnitions have external linkage. The static keyword is used
to change a deﬁnition’s linkage to internal. Note that two or more identical
static deﬁnitions in two or more different .cpp ﬁles are considered to be
distinct entities by the linker (just as if they had been given different names),
so they will not generate a “multiply deﬁned symbol” error. Here are some
examples:
foo.cpp
// This variable can be used by other .cpp files
// (external linkage).
U32 gExternalVariable;
// This variable is only usable within foo.cpp (internal
// linkage).
static U32 gInternalVariable;
// This function can be called from other .cpp files
// (external linkage).
void externalFunction()
{
// ...
}
// This function can only be called from within foo.cpp
// (internal linkage).
static void internalFunction()
{
// ...
}
bar.cpp
// This declaration grants access to foo.cpp's variable.
extern U32 gExternalVariable;
// This 'gInternalVariable' is distinct from the one
// defined in foo.cpp -- no error. We could just as
// well have named it gInternalVariableForBarCpp -- the

3.2. Data, Code and Memory
129
// net effect is the same.
static U32 gInternalVariable;
// This function is distinct from foo.cpp's
// version -- no error. It acts as if we had named it
// internalFunctionForBarCpp().
static void internalFunction()
{
// ...
}
// ERROR -- multiply defined symbol!
void externalFunction()
{
// ...
}
Technically speaking, declarations don’t have a linkage property at all, be-
cause they do not allocate any storage in the executable image; therefore, there
is no question as to whether or not the linker should be permitted to cross-
reference that storage. A declaration is merely a reference to an entity deﬁned
elsewhere. However, it is sometimes convenient to speak about declarations
as having internal linkage, because a declaration only applies to the transla-
tion unit in which it appears. If we allow ourselves to loosen our terminology
in this manner, then declarations always have internal linkage—there is no
way to cross-reference a single declaration in multiple .cpp ﬁles. (If we put a
declaration in a header ﬁle, then multiple .cpp ﬁles can “see” that declaration,
but they are in effect each getting a distinct copy of the declaration, and each
copy has internal linkage within that translation unit.)
This leads us to the real reason why inline function deﬁnitions are per-
mitted in header ﬁles: it is because inline functions have internal linkage by de-
fault, just as if they had been declared static. If multiple .cpp ﬁles #include
a header containing an inline function deﬁnition, each translation unit gets a
private copy of that function’s body, and no “multiply deﬁned symbol” errors
are generated. The linker sees each copy as a distinct entity.
3.2.3
C/C++ Memory Layout
A program written in C or C++ stores its data in a number of different places
in memory. In order to understand how storage is allocated and how the
various types of C/C++ variables work, we need to understand the memory
layout of a C/C++ program.

130
3. Fundamentals of Software Engineering for Games
3.2.3.1
Executable Image
When a C/C++ program is built, the linker creates an executable ﬁle. Most
UNIX-like operating systems, including many game consoles, employ a pop-
ular executable ﬁle format called the executable and linking format (ELF). Exe-
cutable ﬁles on those systems therefore have a .elf extension. The Windows
executable format is similar to the ELF format; executables under Windows
have a .exe extension. Whatever its format, the executable ﬁle always con-
tains a partial image of the program as it will exist in memory when it runs.
I say a “partial” image because the program generally allocates memory at
runtime in addition to the memory laid out in its executable image.
The executable image is divided into contiguous blocks called segments
or sections. Every operating system lays things out a little differently, and
the layout may also differ slightly from executable to executable on the same
operating system. But the image is usually comprised of at least the following
four segments:
1.
Text segment. Sometimes called the code segment, this block contains exe-
cutable machine code for all functions deﬁned by the program.
2.
Data segment. This segment contains all initialized global and static vari-
ables. The memory needed for each global variable is laid out exactly
as it will appear when the program is run, and the proper initial values
are all ﬁlled in. So when the executable ﬁle is loaded into memory, the
initialized global and static variables are ready to go.
3.
BSS segment. “BSS” is an outdated name which stands for “block started
by symbol.” This segment contains all of the uninitialized global and
static variables deﬁned by the program. The C and C++ languages ex-
plicitly deﬁne the initial value of any uninitialized global or static vari-
able to be zero. But rather than storing a potentially very large block of
zeros in the BSS section, the linker simply stores a count of how many
zero bytes are required to account for all of the uninitialized globals and
statics in the segment. When the executable is loaded into memory, the
operating system reserves the requested number of bytes for the BSS
section and ﬁlls it with zeros prior to calling the program’s entry point
(e.g., main() or WinMain()).
4.
Read-only data segment. Sometimes called the rodata segment, this seg-
ment contains any read-only (constant) global data deﬁned by the pro-
gram.
For example, all ﬂoating-point constants (e.g., const float
kPi = 3.141592f;) and all global object instances that have been de-
clared with the const keyword (e.g., const Foo gReadOnlyFoo;)

3.2. Data, Code and Memory
131
reside in this segment. Note that integer constants (e.g., const int
kMaxMonsters = 255;) are often used as manifest constants by the
compiler, meaning that they are inserted directly into the machine code
wherever they are used. Such constants occupy storage in the text seg-
ment, but they are not present in the read-only data segment.
Global variables (variables deﬁned at ﬁle scope outside any function or
class declaration) are stored in either the data or BSS segments, depending
on whether or not they have been initialized. The following global will be
stored in the data segment, because it has been initialized:
foo.cpp
F32 gInitializedGlobal = -2.0f;
and the following global will be allocated and initialized to zero by the oper-
ating system, based on the speciﬁcations given in the BSS segment, because it
has not been initialized by the programmer:
foo.cpp
F32 gUninitializedGlobal;
We’ve seen that the static keyword can be used to give a global variable
or function deﬁnition internal linkage, meaning that it will be “hidden” from
other translation units. The static keyword can also be used to declare a
global variable within a function. A function-static variable is lexically scoped to
the function in which it is declared (i.e., the variable’s name can only be “seen”
inside the function). It is initialized the ﬁrst time the function is called (rather
than before main() is called, as with ﬁle-scope statics). But in terms of mem-
ory layout in the executable image, a function-static variable acts identically
to a ﬁle-static global variable—it is stored in either the data or BSS segment
based on whether or not it has been initialized.
void readHitchhikersGuide(U32 book)
{
static U32 sBooksInTheTrilogy = 5; // data segment
static U32 sBooksRead;
// BSS segment
// ...
}
3.2.3.2
Program Stack
When an executable program is loaded into memory and run, the operating
system reserves an area of memory for the program stack. Whenever a function

132
3. Fundamentals of Software Engineering for Games
is called, a contiguous area of stack memory is pushed onto the stack—we call
this block of memory a stack frame. If function a() calls another function b(),
a new stack frame for b() is pushed on top of a()’s frame. When b() returns,
its stack frame is popped, and execution continues wherever a() left off.
A stack frame stores three kinds of data:
1.
It stores the return address of the calling function so that execution may
continue in the calling function when the called function returns.
2.
The contents of all relevant CPU registers are saved in the stack frame.
This allows the new function to use the registers in any way it sees ﬁt,
without fear of overwriting data needed by the calling function. Upon
return to the calling function, the state of the registers is restored so that
execution of the calling function may resume. The return value of the
called function, if any, is usually left in a speciﬁc register so that the
calling function can retrieve it, but the other registers are restored to
their original values.
3.
The stack frame also contains all local variables declared by the function;
these are also known as automatic variables. This allows each distinct
function invocation to maintain its own private copy of every local vari-
able, even when a function calls itself recursively. (In practice, some
local variables are actually allocated to CPU registers rather than being
stored in the stack frame, but for the most part such variables operate as
if they were allocated within the function’s stack frame.)
Pushing and popping stack frames is usually implemented by adjusting
the value of a single register in the CPU, known as the stack pointer. Fig-
ure 3.10 illustrates what happens when the functions shown below are exe-
cuted.
void c()
{
U32 localC1;
// ...
}
F32 b()
{
F32 localB1;
I32 localB2;
// ...

3.2. Data, Code and Memory
133
c();
// ...
return localB1;
}
void a()
{
U32 aLocalsA1[5];
// ...
F32 localA2 = b();
// ...
}
When a function containing automatic variables returns, its stack frame is
abandoned and all automatic variables in the function should be treated as if
they no longer exist. Technically, the memory occupied by those variables is
a()’s
stack
frame
saved CPU registers
return address
aLocalsA1[5]
localA2
a()’s
stack
frame
saved CPU registers
return address
aLocalsA1[5]
localA2
a()’s
stack
frame
saved CPU registers
return address
aLocalsA1[5]
localA2
b()’s
stack
frame
saved CPU registers
return address
localB1
localB2
b()’s
stack
frame
saved CPU registers
return address
localB1
localB2
saved CPU registers
return address
localC1
c()’s
stack
frame
function a() is called
function b() is called
function c() is called
Figure 3.10. Stack frames.

134
3. Fundamentals of Software Engineering for Games
still there in the abandoned stack frame—but that memory will very likely be
overwritten as soon as another function is called. A common error involves
returning the address of a local variable, like this:
U32* getMeaningOfLife()
{
U32 anInteger = 42;
return &anInteger;
}
You might get away with this if you use the returned pointer immediately
and don’t call any other functions in the interim. But more often than not, this
kind of code will crash—sometimes in ways that can be difﬁcult to debug.
3.2.3.3
Dynamic Allocation Heap
Thus far, we’ve seen that a program’s data can be stored as global or static
variables or as local variables. The globals and statics are allocated within the
executable image, as deﬁned by the data and BSS segments of the executable
ﬁle. The locals are allocated on the program stack. Both of these kinds of
storage are statically deﬁned, meaning that the size and layout of the memory
is known when the program is compiled and linked. However, a program’s
memory requirements are often not fully known at compile time. A program
usually needs to allocate additional memory dynamically.
To allow for dynamic allocation, the operating system maintains a block
of memory for each running process from which memory can be allocated
by calling malloc() (or an OS-speciﬁc function like HeapAlloc() under
Windows) and later returned for reuse by the process at some future time by
calling free() (or an OS-speciﬁc function like HeapFree()). This memory
block is known as heap memory, or the free store. When we allocate memory
dynamically, we sometimes say that this memory resides on the heap.
In C++, the global new and delete operators are used to allocate and free
memory to and from the free store. Be wary, however—individual classes
may overload these operators to allocate memory in custom ways, and even
the global new and delete operators can be overloaded, so you cannot simply
assume that new is always allocating from the global heap.
We will discuss dynamic memory allocation in more depth in Chapter
6. For additional information, see http://en.wikipedia.org/wiki/Dynamic_
memory_allocation.

3.2. Data, Code and Memory
135
3.2.4
Member Variables
C structs and C++ classes allow variables to be grouped into logical units.
It’s important to remember that a class or struct declaration allocates no
memory. It is merely a description of the layout of the data—a cookie cutter
which can be used to stamp out instances of that struct or class later on.
For example:
struct Foo
// struct declaration
{
U32
mUnsignedValue;
F32
mFloatValue;
bool mBooleanValue;
};
Once a struct or class has been declared, it can be allocated (deﬁned) in
any of the ways that an atomic data type can be allocated; for example,
•
as an automatic variable, on the program stack;
void someFunction()
{
Foo localFoo;
// ...
}
•
as a global, ﬁle-static or function-static;
Foo gFoo;
static Foo sFoo;
void someFunction()
{
static Foo sLocalFoo;
// ...
}
•
dynamically allocated from the heap. In this case, the pointer or refer-
ence variable used to hold the address of the data can itself be allocated
as an automatic, global, static or even dynamically.
Foo* gpFoo = NULL; // global pointer to a Foo
void someFunction()
{
// allocate a Foo instance from the heap
gpFoo = new Foo;

136
3. Fundamentals of Software Engineering for Games
// ...
// allocate another Foo, assign to local pointer
Foo* pAnotherFoo = new Foo;
// ...
// allocate a POINTER to a Foo from the heap
Foo** ppFoo = new Foo*;
(*ppFoo) = pAnotherFoo;
}
3.2.4.1
Class-Static Members
As we’ve seen, the static keyword has many different meanings depending
on context:
•
When used at ﬁle scope, static means “restrict the visibility of this
variable or function so it can only be seen inside this .cpp ﬁle.”
•
When used at function scope, static means “this variable is a global,
not an automatic, but it can only be seen inside this function.”
•
When used inside a struct or class declaration, static means “this
variable is not a regular member variable, but instead acts just like a
global.”
Notice that when static is used inside a class declaration, it does not
control the visibility of the variable (as it does when used at ﬁle scope)—rather,
it differentiates between regular per-instance member variables and per-class
variables that act like globals. The visibility of a class-static variable is deter-
mined by the use of public:, protected: or private: keywords in the
class declaration. Class-static variables are automatically included within the
namespace of the class or struct in which they are declared. So the name
of the class or struct must be used to disambiguate the variable whenever
it is used outside that class or struct (e.g., Foo::sVarName).
Like an extern declaration for a regular global variable, the declaration
of a class-static variable within a class allocates no memory. The memory for
the class-static variable must be deﬁned in a .cpp ﬁle. For example:
foo.h
class Foo
{
public:

3.2. Data, Code and Memory
137
static F32 sClassStatic;
// allocates no
// memory!
};
foo.cpp
F32 Foo::sClassStatic = -1.0f; // define memory and
// initialize
3.2.5
Object Layout in Memory
Figure
3.11.
Mem-
ory layout of a simple
struct.
It’s useful to be able to visualize the memory layout of your classes and structs.
This is usually pretty straightforward—we can simply draw a box for the
struct or class, with horizontal lines separating data members. An example
of such a diagram for the struct Foo listed below is shown in Figure 3.11.
struct Foo
{
U32
mUnsignedValue;
F32
mFloatValue;
I32
mSignedValue;
};
Figure 3.12. A mem-
ory
layout
using
width
to
indicate
member sizes.
The sizes of the data members are important and should be represented in
your diagrams. This is easily done by using the width of each data member to
indicate its size in bits—i.e., a 32-bit integer should be roughly four times the
width of an eight-bit integer (see Figure 3.12).
struct Bar
{
U32
mUnsignedValue;
F32
mFloatValue;
bool
mBooleanValue; // diagram assumes this is 8 bits
};
3.2.5.1
Alignment and Packing
As we start to think more carefully about the layout of our structs and classes
in memory, we may start to wonder what happens when small data members
are interspersed with larger members. For example:
struct InefficientPacking
{
U32
mU1; // 32 bits
F32
mF2; // 32 bits
U8
mB3; // 8 bits

138
3. Fundamentals of Software Engineering for Games
I32
mI4; // 32 bits
bool
mB5; // 8 bits
char* mP6; // 32 bits
};
Figure 3.13.
Inefﬁ-
cient struct packing
due to mixed data
member sizes.
You might imagine that the compiler simply packs the data members into
memory as tightly as it can. However, this is not usually the case. Instead, the
compiler will typically leave “holes” in the layout, as depicted in Figure 3.13.
(Some compilers can be requested not to leave these holes by using a prepro-
cessor directive like #pragma pack, or via command-line options; but the
default behavior is to space out the members as shown in Figure 3.13.)
Why does the compiler leave these “holes”? The reason lies in the fact that
every data type has a natural alignment, which must be respected in order to
permit the CPU to read and write memory effectively. The alignment of a data
object refers to whether its address in memory is a multiple of its size (which is
generally a power of two):
•
An object with 1-byte alignment resides at any memory address.
•
An object with 2-byte alignment resides only at even addresses (i.e., ad-
dresses whose least signiﬁcant nibble is 0x0, 0x2, 0x4, 0x8, 0xA, 0xC or
0xE).
•
An object with 4-byte alignment resides only at addresses that are a mul-
tiple of four (i.e., addresses whose least signiﬁcant nibble is 0x0, 0x4, 0x8
or 0xC).
•
A 16-byte aligned object resides only at addresses that are a multiple of
16 (i.e., addresses whose least signiﬁcant nibble is 0x0).
Alignment is important because many modern processors can actually
only read and write properly aligned blocks of data. For example, if a pro-
gram requests that a 32-bit (4-byte) integer be read from address 0x6A341174,
the memory controller will load the data happily because the address is 4-byte
aligned (in this case, its least signiﬁcant nibble is 0x4). However, if a request
is made to load a 32-bit integer from address 0x6A341173, the memory con-
troller now has to read two 4-byte blocks: the one at 0x6A341170 and the one
at 0x6A341174. It must then mask and shift the two parts of the 32-bit integer
and logically OR them together into the destination register on the CPU. This
is shown in Figure 3.14.
Some microprocessors don’t even go this far. If you request a read or write
of unaligned data, you might just get garbage. Or your program might just
crash altogether! (The PlayStation 2 is a notable example of this kind of intol-
erance for unaligned data.)

3.2. Data, Code and Memory
139
CPU
alignedValue
0x6A341170
0x6A341174
0x6A341178
register
-alignedValue
0x6A341170
0x6A341174
0x6A341178
un-
-alignedValue
un-
shift
shift
-alignedValue
un-
Aligned read from
0x6A341174
Unaligned read from
0x6A341173
CPU
register
Figure 3.14. Aligned and unaligned reads of a 32-bit integer.
Different data types have different alignment requirements. A good rule
of thumb is that a data type should be aligned to a boundary equal to the
width of the data type in bytes. For example, 32-bit values generally have a 4-
byte alignment requirement, 16-bit values should be 2-byte aligned, and 8-bit
values can be stored at any address (1-byte aligned). On CPUs that support
SIMD vector math, the SIMD registers each contain four 32-bit ﬂoats, for a
total of 128 bits or 16 bytes. And as you would guess, a four-ﬂoat SIMD vector
typically has a 16-byte alignment requirement.
Figure 3.15.
More
efﬁcient packing by
grouping small mem-
bers together.
This brings us back to those “holes” in the layout of struct Inefficient
Packing shown in Figure 3.13. When smaller data types like 8-bit bools are
interspersed with larger types like 32-bit integers or floats in a structure or
class, the compiler introduces padding (holes) in order to ensure that every-
thing is properly aligned. It’s a good idea to think about alignment and pack-
ing when declaring your data structures. By simply rearranging the members
of struct InefficientPacking from the example above, we can elimi-
nate some of the wasted padding space, as shown below and in Figure 3.15:
struct MoreEfficientPacking
{
U32
mU1; // 32 bits (4-byte aligned)
F32
mF2; // 32 bits (4-byte aligned)
I32
mI4; // 32 bits (4-byte aligned)

140
3. Fundamentals of Software Engineering for Games
char* mP6; // 32 bits (4-byte aligned)
U8
mB3; // 8 bits
(1-byte aligned)
bool
mB5; // 8 bits
(1-byte aligned)
};
You’ll notice in Figure 3.15 that the size of the structure as a whole is now
20 bytes, not 18 bytes as we might expect, because it has been padded by two
bytes at the end. This padding is added by the compiler to ensure proper
alignment of the structure in an array context. That is, if an array of these
structs is deﬁned and the ﬁrst element of the array is aligned, then the padding
at the end guarantees that all subsequent elements will also be aligned properly.
The alignment of a structure as a whole is equal to the largest alignment
requirement among its members. In the example above, the largest mem-
ber alignment is 4-byte, so the structure as a whole should be 4-byte aligned.
I usually like to add explicit padding to the end of my structs to make the
wasted space visible and explicit, like this:
struct BestPacking
{
U32
mU1;
// 32 bits (4-byte aligned)
F32
mF2;
// 32 bits (4-byte aligned)
I32
mI4;
// 32 bits (4-byte aligned)
char* mP6;
// 32 bits (4-byte aligned)
U8
mB3;
// 8 bits (1-byte aligned)
bool
mB5;
// 8 bits (1-byte aligned)
U8
_pad[2]; // explicit padding
};
3.2.5.2
Memory Layout of C++ Classes
Two things make C++ classes a little different from C structures in terms of
memory layout: inheritance and virtual functions.
Figure 3.16. Effect of
inheritance on class
layout.
When class B inherits from class A, B’s data members simply appear im-
mediately after A’s in memory, as shown in Figure 3.16. Each new derived
class simply tacks its data members on at the end, although alignment require-
ments may introduce padding between the classes. (Multiple inheritance does
some whacky things, like including multiple copies of a single base class in the
memory layout of a derived class. We won’t cover the details here, because
game programmers usually prefer to avoid multiple inheritance altogether
anyway.)
If a class contains or inherits one or more virtual functions, then four ad-
ditional bytes (or eight bytes if the target hardware uses 64-bit addresses) are
added to the class layout, typically at the very beginning of the class’ lay-
out. These four or eight bytes are collectively called the virtual table pointer or

3.2. Data, Code and Memory
141
vpointer, because they contain a pointer to a data structure known as the vir-
tual function table or vtable. The vtable for a particular class contains pointers
to all the virtual functions that it declares or inherits. Each concrete class has
its own virtual table, and every instance of that class has a pointer to it, stored
in its vpointer.
The virtual function table is at the heart of polymorphism, because it al-
lows code to be written that is ignorant of the speciﬁc concrete classes it is deal-
ing with. Returning to the ubiquitous example of a Shape base class with de-
rived classes for Circle, Rectangle and Triangle, let’s imagine that Shape
deﬁnes a virtual function called Draw(). The derived classes all override
this function, providing distinct implementations named Circle::Draw(),
Rectangle::Draw() and Triangle::Draw(). The virtual table for any
class derived from Shape will contain an entry for the Draw() function, but
that entry will point to different function implementations, depending on the
concrete class. Circle’s vtable will contain a pointer to Circle::Draw(),
while Rectangle’s virtual table will point to Rectangle::Draw(), and
Triangle’s vtable will point to Triangle::Draw(). Given an arbitrary
pointer to a Shape (Shape* pShape), the code can simply dereference the
vtable pointer, look up the Draw() function’s entry in the vtable, and call
it. The result will be to call Circle::Draw() when pShape points to an
instance of Circle, Rectangle::Draw() when pShape points to a Rec-
tangle, and Triangle::Draw() when pShape points to a Triangle.
These ideas are illustrated by the following code excerpt. Notice that the
base class Shape deﬁnes two virtual functions, SetId() and Draw(), the
latter of which is declared to be pure virtual. (This means that Shape pro-
vides no default implementation of the Draw() function, and derived classes
must override it if they want to be instantiable.) Class Circle derives from
Shape, adds some data members and functions to manage its center and ra-
dius, and overrides the Draw()function; this is depicted in Figure 3.17. Class
Triangle also derives from Shape. It adds an array of Vector3 objects to
store its three vertices and adds some functions to get and set the individual
vertices. Class Triangle overrides Draw() as we’d expect, and for illustra-
tive purposes it also overrides SetId(). The memory image generated by
the Triangle class is shown in Figure 3.18.
class Shape
{
public:
virtual void
SetId(int id) { m_id = id; }
int
GetId() const { return m_id; }

142
3. Fundamentals of Software Engineering for Games
Shape::m_id
Circle::m_center
Circle::m_radius
vtable pointer
pointer to SetId()
pointer to Draw()
+0x00
+0x04
+0x08
+0x14
pShape1
Instance of Circle
Circle’s Virtual Table
Circle::Draw()
{
    // code to draw a Circle
}
Shape::SetId(int id)
{
    m_id = id;
}
Figure 3.17. pShape1 points to an instance of class Circle.
virtual void
Draw() = 0; // pure virtual -- no impl.
private:
int
m_id;
};
class Circle : public Shape
{
public:
void
SetCenter(const Vector3& c) { m_center=c; }
Vector3
GetCenter() const { return m_center; }
void
SetRadius(float r) { m_radius = r; }
float
GetRadius() const { return m_radius; }
virtual void Draw()
{
// code to draw a circle
}
private:
Vector3
m_center;
float
m_radius;
};
class Triangle : public Shape
{
public:
void
SetVertex(int i, const Vector3& v);
Vector3 GetVertex(int i) const { return m_vtx[i]; }

3.2. Data, Code and Memory
143
Shape::m_id
Triangle::m_vtx[0]
Triangle::m_vtx[1]
vtable pointer
pointer to SetId()
pointer to Draw()
+0x00
+0x04
+0x08
+0x14
pShape2
Instance of Triangle
Triangle’s Virtual Table
Triangle::Draw()
{
    // code to draw a Triangle
}
Triangle::SetId(int id)
{
    Shape::SetId(id);
    // do additional work
    // specific to Triangles
}
Triangle::m_vtx[2]
+0x20
Figure 3.18. pShape2 points to an instance of class Triangle.
virtual void Draw()
{
// code to draw a triangle
}
virtual void SetId(int id)
{
// call base class' implementation
Shape::SetId(id);
// do additional work specific to Triangles...
}
private:
Vector3
m_vtx[3];
};
// -----------------------------
void main(int, char**)
{
Shape* pShape1 = new Circle;
Shape* pShape2 = new Triangle;
pShape1->Draw();
pShape2->Draw();
// ...
}

144
3. Fundamentals of Software Engineering for Games
Metric (SI)
IEC
Value
Unit
Name
Value
Unit
Name
1000
kB
kilobyte
1024
KiB
kibibyte
10002
MB
megabyte
10242
MiB
mebibyte
10003
GB
gigabyte
10243
GiB
gibibyte
10004
TB
terabyte
10244
TiB
tebibyte
10005
PB
petabyte
10245
PiB
pebibyte
10006
EB
exabyte
10246
EiB
exbibyte
10007
ZB
zettabyte
10247
ZiB
zebibyte
10008
YB
yottabyte
10248
YiB
yobibyte
Table 3.1. Comparison of Metric (SI) units and IEC units for describing quantities of bytes.
3.2.6
Kilobytes and Kibibytes
If you are a computer programmer, you’ve probably used Metric (SI) units like
kilobytes (kB) and megabytes (MB) to decribe quantities of memory. What a
lot of people don’t realize is that the use of these units isn’t strictly correct.
When we speak of a “kilobyte,” we usually mean 1024 bytes. But SI units
deﬁne the preﬁx “kilo” to mean 103 or 1000, not 1024.
To resolve this ambiguity, the International Electrotechnical Commission
(IEC) in 1998 established a new set of SI-like preﬁxes for use in computer sci-
ence. These preﬁxes are deﬁned in terms of powers of two rather than pow-
ers of ten, so that computer engineers can precisely and conveniently specify
quantities that are powers of two. In this new system, instead of kilobyte
(1000 bytes), we say kibibyte (1024 bytes, abbreviated KiB). And instead of
megabyte (1,000,000 bytes), we say mebibyte (1024 × 1024 = 1,048,576 bytes,
abbreviated MiB). Table 3.1 summarizes the sizes, preﬁxes and names of the
most commonly used byte quantity units in both the SI and IEC systems. We’ll
use IEC units throughout this book.
3.3
Catching and Handling Errors
There are a number of ways to catch and handle error conditions in a game
engine. As a game programmer, it’s important to understand these different
mechanisms, their pros and cons and when to use each one.

3.3. Catching and Handling Errors
145
3.3.1
Types of Errors
In any software project there are two basic kinds of error conditions: user errors
and programmer errors. A user error occurs when the user of the program does
something incorrect, such as typing an invalid input, attempting to open a
ﬁle that does not exist, etc. A programmer error is the result of a bug in the
code itself. Although it may be triggered by something the user has done, the
essence of a programmer error is that the problem could have been avoided
if the programmer had not made a mistake, and the user has a reasonable
expectation that the program should have handled the situation gracefully.
Of course, the deﬁnition of “user” changes depending on context. In the
context of a game project, user errors can be roughly divided into two cate-
gories: errors caused by the person playing the game and errors caused by
the people who are making the game during development. It is important to
keep track of which type of user is affected by a particular error and handle
the error appropriately.
There’s actually a third kind of user—the other programmers on your
team. (And if you are writing a piece of game middleware software, like Ha-
vok or OpenGL, this third category extends to other programmers all over the
world who are using your library.) This is where the line between user errors
and programmer errors gets blurry. Let’s imagine that programmer A writes a
function f(), and programmer B tries to call it. If B calls f() with invalid ar-
guments (e.g., a NULL pointer, or an out-of-range array index), then this could
be seen as a user error by programmer A, but it would be a programmer er-
ror from B’s point of view. (Of course, one can also argue that programmer
A should have anticipated the passing of invalid arguments and should have
handled them gracefully, so the problem really is a programmer error, on A’s
part.) The key thing to remember here is that the line between user and pro-
grammer can shift depending on context—it is rarely a black-and-white dis-
tinction.
3.3.2
Handling Errors
When handling errors, the requirements differ signiﬁcantly between the two
types.
It is best to handle user errors as gracefully as possible, displaying
some helpful information to the user and then allowing him or her to con-
tinue working—or in the case of a game, to continue playing. Programmer
errors, on the other hand, should not be handled with a graceful “inform and
continue” policy. Instead, it is usually best to halt the program and provide
detailed low-level debugging information, so that a programmer can quickly
identify and ﬁx the problem. In an ideal world, all programmer errors would
be caught and ﬁxed before the software ships to the public.

146
3. Fundamentals of Software Engineering for Games
3.3.2.1
Handling Player Errors
When the “user” is the person playing your game, errors should obviously be
handled within the context of gameplay. For example, if the player attempts
to reload a weapon when no ammo is available, an audio cue and/or an ani-
mation can indicate this problem to the player without taking him or her “out
of the game.”
3.3.2.2
Handling Developer Errors
When the “user” is someone who is making the game, such as an artist, an-
imator or game designer, errors may be caused by an invalid asset of some
sort. For example, an animation might be associated with the wrong skeleton,
or a texture might be the wrong size, or an audio ﬁle might have been sam-
pled at an unsupported sample rate. For these kinds of developer errors, there
are two competing camps of thought.
On the one hand, it seems important to prevent bad game assets from per-
sisting for too long. A game typically contains literally thousands of assets,
and a problem asset might get “lost,” in which case one risks the possibility of
the bad asset surviving all the way into the ﬁnal shipping game. If one takes
this point of view to an extreme, then the best way to handle bad game assets
is to prevent the entire game from running whenever even a single problem-
atic asset is encountered. This is certainly a strong incentive for the developer
who created the invalid asset to remove or ﬁx it immediately.
On the other hand, game development is a messy and iterative process,
and generating “perfect” assets the ﬁrst time around is rare indeed. By this
line of thought, a game engine should be robust to almost any kind of prob-
lem imaginable, so that work can continue even in the face of totally invalid
game asset data. But this too is not ideal, because the game engine would
become bloated with error-catching and error-handling code that won’t be
needed once the development pace settles down and the game ships. And the
probability of shipping the product with “bad” assets becomes too high.
In my experience, the best approach is to ﬁnd a middle ground between
these two extremes. When a developer error occurs, I like to make the error
obvious and then allow the team to continue to work in the presence of the
problem. It is extremely costly to prevent all the other developers on the team
from working, just because one developer tried to add an invalid asset to the
game. A game studio pays its employees well, and when multiple team mem-
bers experience downtime, the costs are multiplied by the number of people
who are prevented from working. Of course, we should only handle errors in
this way when it is practical to do so, without spending inordinate amounts
of engineering time, or bloating the code.

3.3. Catching and Handling Errors
147
As an example, let’s suppose that a particular mesh cannot be loaded. In
my view, it’s best to draw a big red box in the game world at the places that
mesh would have been located, perhaps with a text string hovering over each
one that reads, “Mesh blah-dee-blah failed to load.” This is superior to printing
an easy-to-miss message to an error log. And it’s far better than just crash-
ing the game, because then no one will be able to work until that one mesh
reference has been repaired. Of course, for particularly egregious problems
it’s ﬁne to just spew an error message and crash. There’s no silver bullet for
all kinds of problems, and your judgment about what type of error handling
approach to apply to a given situation will improve with experience.
3.3.2.3
Handling Programmer Errors
The best way to detect and handle programmer errors (a.k.a. bugs) is often
to embed error-checking code into your source code and arrange for failed
error checks to halt the program. Such a mechanism is known as an assertion
system; we’ll investigate assertions in detail in Section 3.3.3.3. Of course, as
we said above, one programmer’s user error is another programmer’s bug;
hence, assertions are not always the right way to handle every programmer
error. Making a judicious choice between an assertion and a more graceful
error-handling technique is a skill that one develops over time.
3.3.3
Implementation of Error Detection and Handling
We’ve looked at some philosophical approaches to handling errors. Now let’s
turn our attention to the choices we have as programmers when it comes to
implementing error detection and handling code.
3.3.3.1
Error Return Codes
A common approach to handling errors is to return some kind of failure code
from the function in which the problem is ﬁrst detected. This could be a
Boolean value indicating success or failure, or it could be an “impossible”
value, one that is outside the range of normally returned results. For exam-
ple, a function that returns a positive integer or ﬂoating-point value could
return a negative value to indicate that an error occurred. Even better than
a Boolean or an “impossible” return value, the function could be designed to
return an enumerated value to indicate success or failure. This clearly sepa-
rates the error code from the output(s) of the function, and the exact nature
of the problem can be indicated on failure (e.g., enum Error { kSuccess,
kAssetNotFound, kInvalidRange, ... }).

148
3. Fundamentals of Software Engineering for Games
The calling function should intercept error return codes and act appropri-
ately. It might handle the error immediately. Or, it might work around the
problem, complete its own execution and then pass the error code on to what-
ever function called it.
3.3.3.2
Exceptions
Error return codes are a simple and reliable way to communicate and respond
to error conditions. However, error return codes have their drawbacks. Per-
haps the biggest problem with error return codes is that the function that de-
tects an error may be totally unrelated to the function that is capable of han-
dling the problem. In the worst-case scenario, a function that is 40 calls deep
in the call stack might detect a problem that can only be handled by the top-
level game loop, or by main(). In this scenario, every one of the 40 functions
on the call stack would need to be written so that it can pass an appropriate
error code all the way back up to the top-level error-handling function.
One way to solve this problem is to throw an exception. Exception han-
dling is a very powerful feature of C++. It allows the function that detects a
problem to communicate the error to the rest of the code without knowing
anything about which function might handle the error. When an exception is
thrown, relevant information about the error is placed into a data object of the
programmer’s choice known as an exception object. The call stack is then au-
tomatically unwound, in search of a calling function that has wrapped its call
in a try-catch block. If a try-catch block is found, the exception object is
matched against all possible catch clauses, and if a match is found, the cor-
responding catch’s code block is executed. The destructors of any automatic
variables are called as needed during the stack unwinding process.
The ability to separate error detection from error handling in such a clean
way is certainly attractive, and exception handling is an excellent choice for
some software projects. However, exception handling does add some over-
head to the program. The stack frame of any function that contains a try-
catch block must be augmented to contain additional information required
by the stack unwinding process. Also, if even one function in your program
(or a library that your program links with) uses exception handling, your en-
tire program must use exception handling—the compiler can’t know which
functions might be above you on the call stack when you throw an exception.
Arguably more important than the overhead issue is the fact that excep-
tions are really no better than goto statements. Joel Spolsky of Microsoft
and Fog Creek Software fame argues that exceptions are in fact worse than
gotos because they aren’t easily seen in the source code. A function that nei-
ther throws nor catches exceptions may nevertheless become involved in the

3.3. Catching and Handling Errors
149
stack-unwinding process, if it ﬁnds itself sandwiched between such functions
in the call stack. This can make writing robust software difﬁcult. When the
possibility for exception throwing exists, pretty much every function in your
codebase needs to be robust to the carpet being pulled out from under it and
all its local objects destroyed whenever it makes a function call.
Clearly there are some pretty strong arguments for turning off exception
handling in your game engine altogether. This is the approach employed at
Naughty Dog and also on most of the projects I’ve worked on at Electronic
Arts and Midway. That said, your mileage may vary! There is no perfect tool
and no one right way to do anything. When used judiciously, exceptions can
make your code easier to write and work with. Just be careful out there.
There are many interesting articles on this topic on the web. Here’s one
good thread that covers most of the key issues on both sides of the debate:
•
http://www.joelonsoftware.com/items/2003/10/13.html
•
http://www.nedbatchelder.com/text/exceptions-vs-status.html
•
http://www.joelonsoftware.com/items/2003/10/15.html
3.3.3.3
Assertions
An assertion is a line of code that checks an expression. If the expression evalu-
ates to true, nothing happens. But if the expression evaluates to false, the pro-
gram is stopped, a message is printed and the debugger is invoked if possible.
Steve Maguire provides an in-depth discussion of assertions in his must-read
book, Writing Solid Code [30].
Assertions check a programmer’s assumptions. They act like land mines
for bugs. They check the code when it is ﬁrst written to ensure that it is func-
tioning properly. They also ensure that the original assumptions continue
to hold for long periods of time, even when the code around them is con-
stantly changing and evolving. For example, if a programmer changes code
that used to work, but accidentally violates its original assumptions, they’ll
hit the land mine. This immediately informs the programmer of the problem
and permits him or her to rectify the situation with minimum fuss. Without
assertions, bugs have a tendency to “hide out” and manifest themselves later
in ways that are difﬁcult and time-consuming to track down. But with asser-
tions embedded in the code, bugs announce themselves the moment they are
introduced—which is usually the best moment to ﬁx the problem, while the
code changes that caused the problem are fresh in the programmer’s mind.
Assertions are implemented as a #define macro, which means that the
assertion checks can be stripped out of the code if desired by simply changing
the #define. The cost of the assertion checks can usually be tolerated during

150
3. Fundamentals of Software Engineering for Games
development, but stripping out the assertions prior to shipping the game can
buy back that little bit of crucial performance if necessary.
Assertion Implementation
Assertions are usually implemented via a combination of a #defined macro
that evaluates to an if/else clause, a function that is called when the asser-
tion fails (the expression evaluates to false), and a bit of assembly code that
halts the program and breaks into the debugger when one is attached. Here’s
a typical implementation:
#if ASSERTIONS_ENABLED
// define some inline assembly that causes a break
// into the debugger -- this will be different on each
// target CPU
#define debugBreak() asm { int 3 }
// check the expression and fail if it is false
#define ASSERT(expr) \
if (expr) { } \
else \
{ \
reportAssertionFailure(#expr, \
__FILE__, __LINE__); \
debugBreak(); \
}
#else
#define ASSERT(expr)
// evaluates to nothing
#endif
Let’s break down this deﬁnition so we can see how it works:
•
The outer #if/#else/#endif is used to strip assertions from the code
base. When ASSERTIONS_ENABLED is nonzero, the ASSERT() macro
is deﬁned in its full glory, and all assertion checks in the code will be in-
cluded in the program. But when assertions are turned off, ASSERT(expr)
evaluates to nothing, and all instances of it in the code are effectively re-
moved.
•
The debugBreak() macro evaluates to whatever assembly-language
instructions are required in order to cause the program to halt and the

3.3. Catching and Handling Errors
151
debugger to take charge (if one is connected). This differs from CPU to
CPU, but it is usually a single assembly instruction.
•
The ASSERT() macro itself is deﬁned using a full if/else statement
(as opposed to a lone if). This is done so that the macro can be used in
any context, even within other unbracketed if/else statements.
Here’s an example of what would happen if ASSERT() were deﬁned using
a solitary if:
#define ASSERT(expr)
if (!(expr)) debugBreak()
void f()
{
if (a < 5)
ASSERT(a >= 0);
else
doSomething(a);
}
This expands to the following incorrect code:
void f()
{
if (a < 5)
if (!(a >= 0))
debugBreak();
else // oops! bound to the wrong if()!
doSomething(a);
}
•
The else clause of an ASSERT() macro does two things. It displays
some kind of message to the programmer indicating what went wrong,
and then it breaks into the debugger. Notice the use of #expr as the ﬁrst
argument to the message display function. The pound (#) preprocessor
operator causes the expression expr to be turned into a string, thereby
allowing it to be printed out as part of the assertion failure message.
•
Notice also the use of __FILE__ and __LINE__. These compiler-deﬁn-
ed macros magically contain the .cpp ﬁle name and line number of the
line of code on which they appear. By passing them into our message
display function, we can print the exact location of the problem.
I highly recommend the use of assertions in your code. However, it’s im-
portant to be aware of their performance cost. You may want to consider

152
3. Fundamentals of Software Engineering for Games
deﬁning two kinds of assertion macros. The regular ASSERT() macro can be
left active in all builds, so that errors are easily caught even when not running
in debug mode. A second assertion macro, perhaps called SLOW_ASSERT(),
could be activated only in debug builds. This macro could then be used in
places where the cost of assertion checking is too high to permit inclusion
in release builds. Obviously SLOW_ASSERT() is of lower utility, because it is
stripped out of the version of the game that your testers play every day. But at
least these assertions become active when programmers are debugging their
code.
It’s also extremely important to use assertions properly. They should be
used to catch bugs in the program itself—never to catch user errors. Also, as-
sertions should always cause the entire game to halt when they fail. It’s usu-
ally a bad idea to allow assertions to be skipped by testers, artists, designers
and other non-engineers. (This is a bit like the boy who cried wolf: if asser-
tions can be skipped, then they cease to have any signiﬁcance, rendering them
ineffective.) In other words, assertions should only be used to catch fatal er-
rors. If it’s OK to continue past an assertion, then it’s probably better to notify
the user of the error in some other way, such as with an on-screen message, or
some ugly bright-orange 3D graphics.
3.4
Pipelines, Caches and Optimization
We said in Section 1.2.1 that games are soft real-time systems. The term “real-
time” means that game software must operate with deadlines—the most ob-
vious of which is the requirement that each frame must be completed within
16.6 ms (to achieve 60 FPS) or 33.3 ms (to achieve 30 FPS). The “soft” part
means that people won’t die if our frame rate dies. But nonetheless, there’s no
doubt about it: game software needs to perform as efﬁciently as possible.
The term optimization is a blanket term that covers anything the program-
mers, game designers and/or artists can do in order to improve the perfor-
mance—and ultimately the frame rate—of their game. It can also refer to other
kinds of improvements, like reducing the size of assets so that they will ﬁt
into memory. In this section, we’ll focus our attention on one speciﬁc aspect
of performance optimization: that of making our software perform its com-
putations as quickly as possible. Once you’ve read it, check out Alexander
Alexandrescu’s talk, “Three Optimization Tips for C++” for more inform-
ation—it’s available here: http://www.slideshare.net/andreialexandrescu1/
three-optimization-tips-for-c-15708507.

3.4. Pipelines, Caches and Optimization
153
3.4.1
The Parallelism Paradigm Shift
In order to optimize the performance of a piece of software, we need to under-
stand what kinds of things can slow it down. These things change over time,
as computer hardware evolves.
In the early days of computing, CPUs were relatively slow, so program-
mers would optimize their code by focusing on reducing the number of cy-
cles spent on any given task. CPUs did exactly one operation at a time, so
programmers could literally read the disassembly and count up the cycles
consumed by each instruction. And because memory accesses were relatively
cheap, programmers would often trade more memory for fewer cycles.
Today, the situation is almost entirely different. Computers and game con-
soles now contain multiple CPU cores running in parallel, and software has
to be written to take advantage of this parallelism. (See Section 7.6 for an in-
depth discussion of multicore computing in a games context.) This paradigm
shift towards parallel processing also extends down into the design of the
CPU core itself. Modern CPUs are pipelined, meaning that multiple instruc-
tions can be “in ﬂight” simultaneously. And today’s GPUs are essentially
massively parallel compute engines, capable of performing hundreds or even
thousands of computations in parallel.
In part because of this shift towards parallelism, CPU performance has
been improving at a much faster rate than the speed with which memory can
be accessed. Today’s CPUs employ complex memory cacheing schemes to
reduce memory access latency. Nowadays, the mantra is: “Memory is ex-
pensive, compute cycles are cheap.” As a result of all this, the rules of per-
formance optimization have been ﬂipped entirely upside-down from those of
the early days. Instead of reducing the number of instructions executed, it’s
now commonplace to do more work on the CPU in order to avoid having to
access memory!
3.4.2
Memory Caches
To understand why memory access patterns affect performance, we need ﬁrst
to understand how modern processors read and write memory. Accessing
main system RAM on a modern game console or personal computer is a slow
operation, often taking thousands of processor cycles to complete. Contrast
this with a register access on the CPU itself, which takes on the order of tens of
cycles or sometimes even a single cycle. To reduce the average cost of reading
and writing to main RAM, modern processors utilize one or more high-speed
memory caches.

154
3. Fundamentals of Software Engineering for Games
A cache is nothing more than a bank of memory that can be read from and
written to by the CPU much more quickly than main RAM. A cache achieves
minimum memory access latency in two ways: First, cache memory typically
utilizes the fastest (and most expensive) technology available. Second, cache
memory is located as physically close as possible to the CPU core, typically on
the same die. As a result of these two factors, cache memory is usually quite
a bit smaller in size than main RAM.
A memory cacheing system improves memory access performance by keep-
ing local copies in the cache of those chunks of data that are most frequently
accessed by the program. If the data requested by the CPU is already in the
cache, it can be provided to the CPU very quickly—on the order of tens of
cycles. This is called a cache hit. However, if the data is not already present in
the cache, it must be fetched into the cache from main RAM. This is called a
cache miss. Reading data from main RAM can take thousands of cycles, so the
cost of a cache miss is very high indeed.
3.4.2.1
Cache Lines
In order to reduce the impact of a cache miss, the cache controller tries to
make the most of the situation by loading more memory into the cache than
was actually requested. For example, let’s say the program tries to read the
contents of an int variable, which typically occupies a single machine word—
that’s 32 or 64 bits in size, depending on the architecture. Instead of spending
thousands of cycles to read just that one word, the cache controller reads a
larger contiguous block of memory containing that word. The idea is that if
the program is accessing memory sequentially, which is often the case, the
ﬁrst read might incur the cost of a cache miss, but subsequent reads will be
low-cost cache hits.
There is a simple one-to-many correspondance between memory addresses
in the cache and memory addresses in main RAM. We can think of the ad-
dress space of the cache as being “mapped” onto the main RAM address
space in a repeating pattern, starting at address 0 in main RAM and con-
tinuing on up until all main RAM addresses have been “covered” by the
cache.
As a concrete example, let’s say that our cache is 32 KiB1 in size,
and that cache lines are 128 bytes each. The cache can therefore hold 256
cache lines (256 × 128 = 32768 B = 32 KiB). Let’s further assume that main
RAM is 256 MiB in size. So main RAM is 8192 times as big as the cache
((256 × 1024)/32 = 8192). That means we need to overlay the address space
1Recall from Section 3.2.6 that KiB stands for “kibibyte.” 1 KiB = 1024 bytes.

3.4. Pipelines, Caches and Optimization
155
Lines 0 -255
Lines 0 -255
Lines 0 -255
Lines 0 - 255
0x18000
0x10000
0x08000
0x00000
Lines 0 -255
Lines 0 -255
0x1FFFF
0x17FFF
0x0FFFF
0x07FFF
0xFFF8000
0xFFF0000
0xFFFFFFF
0xFFF7FFF
0x00000
0x0007F
Line 0
Line 1
Line 2
Line 255
Line 254
0x00080
0x000FF
0x00100
0x0017F
...
...
Main RAM
Cache
0x07F00
0x07F7F
0x07F80
0x07FFF
Figure 3.19. Direct mapping between main memory addresses and cache lines.
of the cache onto the main RAM address space 8192 times in order to cover all
possible physical memory locations.
Given any address in main RAM, we can ﬁnd its address in the cache by
taking the main RAM address modulo the size of the cache. So for a 32 KiB
cache and 256 MiB of main RAM, the cache addresses 0x0000 through 0x7FFF
(that’s 32 KiB) map to main RAM addresses 0x0000 through 0x7FFF, but they
also map to the addresses 0x8000 through 0xFFFF, 0x10000 through 0x17FFF,
0x18000 through 0x1FFFF and so on, all the way up to the last block at ad-
dresses 0xFFF8000 through 0xFFFFFFF. Figure 3.19 illustrates the mapping
between main RAM and cache RAM.
The cache can only deal with memory addresses that are aligned to a multi-
ple of the cache line size (see Section 3.2.5.1 for a discussion of memory align-
ment). So, the cache can really only be addressed in units of lines, not bytes.
Consider a cache that is 2M bytes in total size, containing lines that are 2n in
size. We can convert any main RAM address to a cache line index as follows.
First we strip off the n least-signiﬁcant bits of the main RAM address to con-
vert from units of bytes to line indices (i.e., we divide the address by 2n). Then
we split the resulting address into two pieces: The (M −n) least-signiﬁcant
bits become the cache line index, and all the remaining bits tell us from which
cache-sized block in main RAM the cache line came from. The block index
is stored in a special data structure within the cache controller known as the
translation lookaside buffer, or TLB. Without the TLB, we would not be able to
keep track of the one-to-many relationship between cache line indices and
main RAM addresses.

156
3. Fundamentals of Software Engineering for Games
3.4.2.2
Instruction Cache and Data Cache
When writing high-performance code for a game engine or for any other
performance-critical system, it is important to realize that both data and code
are cached. The instruction cache (I-cache, sometimes denoted I$) is used to
preload executable machine code before it runs, while the data cache (D-cache,
or D$) is used to speed up reading and writing of data to main RAM. The
two caches are always physically distinct, because it is never desirable for an
instruction read to cause valid data to be bumped out of the cache, or vice
versa. So when optimizing our code, we must consider both D-cache and
I-cache performance (although as we’ll see, optimizing one tends to have a
positive effect on the other).
3.4.2.3
Set Associativity and Replacement Policy
The simple mapping between cache lines and main RAM addresses described
above is known as a direct-mapped cache. It means that each address in main
RAM maps to only one line in the cache. Using our 32 KiB cache with 128-
byte lines as an example, the main RAM address 0x203 maps to cache line 4
(because 0x203 is 515, and ⌊515/128⌋= 4). However, in our example there
are 8192 unique cache-line-sized blocks of main RAM that all map to cache
line 4. Speciﬁcally, cache line 4 corresponds to main RAM addresses 0x200
through 0x27F, but also to addresses 0x8200 through 0x827F, and 0x10200
through 0x1027f and so on.
When a cache miss occurs, the CPU must load the corresponding cache
line from main memory into the cache. If the line in the cache contains no valid
data, we simply copy the data into it and we’re done. But if the line already
contains data (from a different main memory block), we must overwrite it.
This is known as evicting the data, or “kicking” the data out of the cache.
The problem with a direct-mapped cache is that it can result in pathologi-
cal cases; for example, two unrelated main memory blocks might keep evict-
ing one another in a ping-pong fashion. We can obtain better average perfor-
mance if each main memory address can map to two or more distinct lines
in the cache. In a 2-way set associative cache, each main RAM address maps
to two cache lines. This is illustrated in Figure 3.20. Obviously a 4-way set
associative cache performs even better than a 2-way, and an 8-way or 16-way
cache can outperform a 4-way cache and so on.
Once we have more than one “cache way,” the cache controller is faced
with a dilemma: When a cache miss occurs, which of the “ways” should we
evict and which one should we allow to stay resident in the cache? The an-
swer to this question differs between CPU designs, and is known as the CPU’s

3.4. Pipelines, Caches and Optimization
157
Line 0, Way 0
Line 0, Way 1
Line 1, Way 0
...
Line 1, Way 1
0x00000
0x0007F
0x00080
0x000FF
0x00080
0x000FF
0x00000
0x0007F
Line 0
Line 1
Line 2
...
0x00000
0x0007F
0x00100
0x0017F
0x00080
0x000FF
Main RAM
Cache
(2-Way Set Associative)
Figure 3.20. A 2-way set associative cache.
replacement policy. One popular policy is to simply always evict the “oldest”
data.
3.4.2.4
Write Policy
We haven’t talked yet about what happens when the CPU writes data to RAM.
How the cache controller handles writes is known as its write policy. The sim-
plest kind of cache is called a write-through cache; in this relatively simple cache
design, all writes to the cache are mirrored to main RAM immediately. In a
write-back (or copy-back) cache design, data is ﬁrst written into the cache and
the cache line is only ﬂushed out to main RAM under certain circumstances,
such as when a dirty cache line needs to be evicted in order to read in a new
cache line from main RAM, or when the program explicitly requests a ﬂush to
occur.
3.4.2.5
Multilevel Caches
The hit rate is a measure of how often a program hits the cache, as opposed to
incurring the large cost of a cache miss. The higher the hit rate, the better the
program will perform (all other things being equal). There is a fundamental
trade-off between cache latency and hit rate. The larger the cache, the higher
the hit rate—but larger caches cannot be located as close to the CPU, so they
tend to be slower than smaller ones.
Most game consoles employ at least two levels of cache. The CPU ﬁrst tries
to ﬁnd the data it’s looking for in the level 1 (L1) cache. This cache is small
but has a very low access latency. If the data isn’t there, it tries the larger but
higher-latency level 2 (L2) cache. Only if the data cannot be found in the L2
cache do we incur the full cost of a main memory access. Because the latency

158
3. Fundamentals of Software Engineering for Games
CPU Die
L2
Cache
Main RAM
slowest
Core 1
L1
Cache
slower
fast
Core 0
L1
Cache
slower
fast
Figure 3.21. Level 1 and level 2 caches.
of main RAM can be so high relative to the CPU’s clock rate, some PCs even
include a level 3 (L3) cache.
3.4.2.6
Cache Consistency: MESI and MOESI
When multiple CPU cores share a single main memory store, things get more
complicated. It’s typical for each core to have its own L1 cache, but multiple
cores might share an L2 cache, as well as sharing main RAM. See Figure 3.21
for an illustration of a two-level cache architecture with two CPU cores shar-
ing one main memory store and an L2 cache.
In the presence of multiple cores, it’s important for the system to maintain
cache coherency. This amounts to making sure that the data in the caches match
one another and the contents of main RAM. Coherency doesn’t have to be
maintained at every moment—all that matters is that the running program
can never tell that the contents of the caches are out of sync.
The two most common cache coherency protocols are known as MESI
(modiﬁed, exclusive, shared, invalid) and MOESI (modiﬁed, owned, exclu-
sive, shared, invalid). A detailed discussion of these protocols is outside our
scope here,
but you can read more about them on Wikipedia here:
http://en.wikipedia.org/wiki/MOESI_protocol.
3.4.2.7
Avoiding Cache Misses
Obviously cache misses cannot be totally avoided, since data has to move
to and from main RAM eventually. However, the trick to high-performance

3.4. Pipelines, Caches and Optimization
159
computing is to arrange your data in RAM and code your algorithms in such
a way that the minimum number of cache misses occur.
The best way to avoid D-cache misses is to organize your data in contigu-
ous blocks that are as small as possible and then access them sequentially. This
yields the minimum number of cache misses. When the data is contiguous
(i.e., you don’t “jump around” in memory a lot), a single cache miss will load
the maximum amount of relevant data in one go. When the data is small, it
is more likely to ﬁt into a single cache line (or at least a minimum number
of cache lines). And when you access your data sequentially (i.e., you don’t
“jump around” within the contiguous memory block), you achieve the mini-
mum number of cache misses, since the CPU never has to reload a cache line
from the same region of RAM.
Avoiding I-cache misses follows the same basic principle as avoiding D-
cache misses. However, the implementation requires a different approach.
The easiest thing to do is to keep your high-performance loops as small as
possible in terms of code size, and avoid calling functions within your inner-
most loops. This helps to ensure that the entire body of the loop will remain
in the I-cache the entire time the loop is running.
If your loop does need to call functions, it’s best if the code for the function
being called is located in close proximity in memory to the code containing the
body of the loop. The compiler and linker dictate how your code is laid out in
memory, so you might think you have little control over I-cache misses. How-
ever, most C/C++ linkers follow some simple rules that you can leverage,
once you know what they are:
•
The machine code for a single function is almost always contiguous in
memory. That is, the linker almost never splits a function up in order
to intersperse another function in the middle. (Inline functions are the
exception to this rule—more on this topic below.)
•
Functions are laid out in memory in the order they appear in the trans-
lation unit’s source code (.cpp ﬁle).
•
Therefore, functions in a single translation unit are always contiguous
in memory. That is, the linker does not split up a complied translation
unit (.obj ﬁle) in order to intersperse code from some other translation
unit (unless function level linking is enabled2).
2Most modern compilers support function level linking, in which functions are linked into the
executable image individually. This allows uncalled functions to be stripped, for example. Some
compilers even allow the developer to specify the order in which the functions should appear in
the executable.

160
3. Fundamentals of Software Engineering for Games
So, following the same principles that we applied when avoiding D-cache
misses, we should follow the rules of thumb listed below in order to avoid
I-cache misses:
•
Keep high-performance code as small as possible, in terms of number of
machine language instructions. (The compiler and linker take care of
keeping our functions contiguous in memory.)
•
Avoid calling functions from within a performance-critical section of code.
•
If you do have to call a function, place it as close as possible to the calling
function—preferably immediately before or after the calling function
and never in a different translation unit (because then you completely
lose control over its proximity to the calling function).
•
Use inline functions judiciously. Inlining a small function can be a big
performance boost. However, too much inlining bloats the size of the
code, which can cause a performance-critical section of code to no longer
ﬁt within the cache. Let’s say we write a tight loop that processes a
large amount of data—if the entire body of that loop doesn’t ﬁt into
the cache, then we are signing up for two I-cache misses during every
iteration of the loop. In such a situation, it is probably best to rethink the
algorithm and/or implementation so that less code is required within
critical loops.
3.4.3
Instruction Pipelining and Superscalar CPUs
We mentioned in Section 3.4.1 that the recent shift toward parallel processing
applies not only to computers with more than one CPU core, but also to the
cores themselves. There are two closely related architectural devices that can
increase parallelism within the CPU itself: instruction pipelining and superscalar
architecture.
Pipelining can be understood by imagining yourself doing your laundry.
You have a washer and a dryer and too many loads of laundry to count! How
can you get the loads done as quickly as possible? If you take each load and
run it ﬁrst through the washer and then through the dryer, you’re not making
as efﬁcient use of your hardware as you can. While the washer is busy, the
dryer is sitting idle, and vice versa. To increase the efﬁciency, it’s best to start
a new load washing just as soon as the ﬁrst load goes into the dryer. That way,
both machines are operating all the time.
CPU instruction pipelining works in a similar fashion. In order to exe-
cute a single machine language instruction, the CPU must perform a number
of steps. These correspond to the washing and drying steps in our analogy

3.4. Pipelines, Caches and Optimization
161
fetch
decode
execute
memory
register
write-back
1
clock 1
clock 2
clock 3
clock 4
clock 5
1
2
2
3
1
1
3
4
1
2
4
5
2
3
Figure 3.22. The ﬂow of instructions through a pipelined CPU.
above. First, the instruction must be fetched from memory (or better yet, the
I-cache). Then it must be decoded. It can then be executed. If the instruction
requires a data access, a memory access cycle can be run. Finally, the contents
of registers are written to memory if necessary. Each of these steps is per-
formed by a separate circuit on the CPU, and these circuits are connected to
one another to form a pipeline. The CPU keeps all of these circuits busy all the
time by feeding a new instruction into the pipeline as soon as the ﬁrst stage
becomes free. This process is illustrated in Figure 3.22.
The latency of a pipeline is the amount of time required to completely pro-
cess a single instruction. This is just the sum of the latencies of all the stages
in the pipeline. The bandwidth or throughput of a pipeline is a measure of how
many instructions it can process per unit time. The bandwidth of a pipeline
is determined by the latency of its slowest stage—much as a chain is only as
strong as its weakest link.
A superscalar processor includes multiple redundant copies of the circuitry
for some or all of its pipeline stages. This allows it to process multiple in-
struction streams in parallel. For example, if the CPU has two integer arith-
metic/logic units (ALUs), then two integer instructions can be in ﬂight simul-
taneously.
We should pause to note here that different types of data often utilize dif-
ferent bits of circuitry on the CPU die. For example, integer arithmetic might
be performed by one circuit, while ﬂoating-point math is done by another cir-
cuit, and SIMD vector math is performed by yet a third circuit. This kind of
CPU architecture acts a bit like a superscalar architecture, in that an integer
multiply, a ﬂoating-point multiply and a SIMD vector multiply (for example)
can all be performed simultaneously. But to be truly superscalar, a CPU needs
to have multiple integer, ﬂoating-point and/or vector units.

162
3. Fundamentals of Software Engineering for Games
fetch
decode
execute
memory
register
write-back
mov
clock 1
clock 2
clock 3
clock 4
mov 5,r3    
mul r0,10,r1
add r1,7,r2 
mov
mul
mov
mul
mov
mul
clock 5
clock 6
clock 7
mov
mul
mul
add
Data Dependency
STALL
Figure 3.23. A data dependency between instructions causes a pipeline stall.
3.4.3.1
Data Dependencies and Stalls
A pipelined CPU tries to keep all of its stages busy by issuing a new instruc-
tion on every clock cycle. If the results of one instruction are required in order
to execute another instruction, then the later instruction must wait until the
earlier instruction has passed all the way through the pipeline. This can intro-
duce a “bubble” in the pipeline, which degrades throughput.
For example, consider the following sequence of instructions:
mov 5,r3
;; load the value 5 into register 3
mul r0,10,r1
;; multiply the contents of r0 by 10,
;; store in r1
add r1,7,r2
;; add 7 to r1, store in r2
Ideally, we’d like to issue the mov, mul and add instructions on three consec-
utive clock cycles, to keep the pipeline as busy as possible. But in this case, the
results of the mul instruction are used by the add instruction. This means that
the CPU must wait until the mul has made it all the way through the pipeline
before issuing the add. If the pipeline contains ﬁve stages, that means ﬁve
wasted cycles (see Figure 3.23). This is called a data dependency, and it results
in a stall in the pipeline.
Optimizing compilers try to automatically reorder machine instructions in
order to avoid stalls.
For example, if the instructions following the three
shown above are not dependent on any previous results, they might be moved
up so that they can be executed while the mul is doing its thing.

3.4. Pipelines, Caches and Optimization
163
3.4.3.2
Branch Prediction
Another way a stall can be introduced is by branching. A branch is what
happens when you use an if statement in your code. Whenever a branch
instruction is encountered, a pipelined CPU has no choice but to try to guess
at which branch is going to be taken. It continues to issue the instructions from
the selected branch, in the hopes that its guess is correct. But we often don’t
know whether or not the guess is wrong until some of the calculations that are
already in ﬂight pop out at the back end of the pipeline. If the guess ends up
being wrong, we’ve executed instructions that shouldn’t have been executed
at all. So the pipeline must be ﬂushed and restarted at the ﬁrst instruction of
the correct branch.
The simplest guess a CPU can make is to assume that backward branches
are always taken (because this is the kind of branch you ﬁnd at the end of a
while or for loop) and that forward branches are never taken. Most high-
quality CPUs include branch prediction hardware that can improve the quality
these guesses signiﬁcantly. A branch predictor can track the results of a branch
instruction over multiple iterations of a loop and discover patterns that help
it make better guesses on subsequent iterations.
On CPUs without good branch prediction hardware, it’s up to the pro-
grammer to improve the performance of the code. This can be done by rewrit-
ing performance-critical loops such that branches are either reduced or en-
tirely eliminated. One approach is to actually perform the calculations for both
cases of a branch and then select between the two results using a branchless
mechanism, such as logically ANDing with a bitmask. The fsel (ﬂoating-
point select) instruction is an example of such a mechanism.
PS3 game programmers had to deal with the poor performance of “branchy”
code all the time, because the branch predictors on the Cell processor were
frankly pretty terrible. But the AMD Jaguar CPU on the PS4 has highly ad-
vanced branch prediction hardware, so game programmers can breathe a little
easier when writing code for the PS4.
3.4.3.3
Load-Hit-Store
A load-hit-store is a particularly bad kind of pipeline stall, prevalent on the
PowerPC architectures found in the Xbox 360 and PlayStation 3. It can hap-
pen when, for example, your code tries to convert a ﬂoating-point value into
an integer value and then use that value in subsequent operations. The crux
of the problem in converting a float to an int is that the CPU has no way
to transfer data from its ﬂoating-point registers directly into its integer regis-
ters. As such, the value must be written from the ﬂoating-point register out to

164
3. Fundamentals of Software Engineering for Games
memory and then loaded back again into an integer register. For example:
stfs
fr3,0(r3)
; Store the float, using r3 as a pointer
lwz
r9,0(r3)
; Read it back, this time into an
; integer register
oris
r9,r9,0x8000
; Force it to negative
The problem is that the oris instruction has to wait until the data is avail-
able in register r9. It takes multiple cycles to store the data into the L1 cache
and read it back again. During this time, the entire pipeline is forced to stall.
See http://www.gamasutra.com/view/feature/132084/sponsored_feature_
common_.php and http://assemblyrequired.crashworks.org/2008/07/08/
load-hit-stores-and-the-__restrict-keyword for more details.

4
3D Math for Games
A
game is a mathematical model of a virtual world simulated in real time
on a computer of some kind. Therefore, mathematics pervades every-
thing we do in the game industry. Game programmers make use of virtu-
ally all branches of mathematics, from trigonometry to algebra to statistics to
calculus. However, by far the most prevalent kind of mathematics you’ll be
doing as a game programmer is 3D vector and matrix math (i.e., 3D linear
algebra).
Even this one branch of mathematics is very broad and very deep, so we
cannot hope to cover it in any great depth in a single chapter. Instead, I will
attempt to provide an overview of the mathematical tools needed by a typi-
cal game programmer. Along the way, I’ll offer some tips and tricks, which
should help you keep all of the rather confusing concepts and rules straight in
your head. For an excellent in-depth coverage of 3D math for games, I highly
recommend Eric Lengyel’s book on the topic [28].
4.1
Solving 3D Problems in 2D
Many of the mathematical operations we’re going to learn about in the follow-
ing chapter work equally well in 2D and 3D. This is very good news, because
it means you can sometimes solve a 3D vector problem by thinking and draw-
ing pictures in 2D (which is considerably easier to do!) Sadly, this equivalence
165

166
4. 3D Math for Games
between 2D and 3D does not hold all the time. Some operations, like the cross
product, are only deﬁned in 3D, and some problems only make sense when
all three dimensions are considered. Nonetheless, it almost never hurts to
start by thinking about a simpliﬁed two-dimensional version of the problem
at hand. Once you understand the solution in 2D, you can think about how
the problem extends into three dimensions. In some cases, you’ll happily dis-
cover that your 2D result works in 3D as well. In others, you’ll be able to
ﬁnd a coordinate system in which the problem really is two-dimensional. In
this book, we’ll employ two-dimensional diagrams wherever the distinction
between 2D and 3D is not relevant.
4.2
Points and Vectors
y
z
x
Figure 4.1.
A point
represented in Car-
tesian coordinates.
h
r
Figure 4.2.
A point
represented in cylin-
drical coordinates.
The majority of modern 3D games are made up of three-dimensional objects
in a virtual world. A game engine needs to keep track of the positions, orien-
tations and scales of all these objects, animate them in the game world, and
transform them into screen space so they can be rendered on screen. In games,
3D objects are almost always made up of triangles, the vertices of which are
represented by points. So, before we learn how to represent whole objects in
a game engine, let’s ﬁrst take a look at the point and its closely related cousin,
the vector.
4.2.1
Points and Cartesian Coordinates
Technically speaking, a point is a location in n-dimensional space. (In games,
n is usually equal to 2 or 3.) The Cartesian coordinate system is by far the
most common coordinate system employed by game programmers. It uses
two or three mutually perpendicular axes to specify a position in 2D or 3D
space. So, a point P is represented by a pair or triple of real numbers, (Px, Py)
or (Px, Py, Pz) (see Figure 4.1).
Of course, the Cartesian coordinate system is not our only choice. Some
other common systems include:
•
Cylindrical coordinates. This system employs a vertical “height” axis h,
a radial axis r emanating out from the vertical, and a yaw angle theta
(θ). In cylindrical coordinates, a point P is represented by the triple of
numbers (Ph, Pr, Pθ). This is illustrated in Figure 4.2.
•
Spherical coordinates. This system employs a pitch angle phi (φ), a yaw
angle theta (θ) and a radial measurement r. Points are therefore repre-
sented by the triple of numbers (Pr, Pφ, Pθ). This is illustrated in Fig-
ure 4.3.

4.2. Points and Vectors
167
Cartesian coordinates are by far the most widely used coordinate system
in game programming. However, always remember to select the coordinate
system that best maps to the problem at hand. For example, in the game
Crank the Weasel by Midway Home Entertainment, the main character Crank
runs around an art-deco city picking up loot. I wanted to make the items of
loot swirl around Crank’s body in a spiral, getting closer and closer to him
until they disappeared. I represented the position of the loot in cylindrical
coordinates relative to the Crank character’s current position. To implement
the spiral animation, I simply gave the loot a constant angular speed in θ, a
small constant linear speed inward along its radial axis r and a very slight
constant linear speed upward along the h-axis so the loot would gradually
rise up to the level of Crank’s pants pockets. This extremely simple animation
looked great, and it was much easier to model using cylindrical coordinates
than it would have been using a Cartesian system.
4.2.2
Left-Handed versus Right-Handed Coordinate Systems
r
Figure 4.3.
A point
represented in spher-
ical coordinates.
In three-dimensional Cartesian coordinates, we have two choices when ar-
ranging our three mutually perpendicular axes: right-handed (RH) and left-
handed (LH). In a right-handed coordinate system, when you curl the ﬁngers
of your right hand around the z-axis with the thumb pointing toward posi-
tive z coordinates, your ﬁngers point from the x-axis toward the y-axis. In a
left-handed coordinate system the same thing is true using your left hand.
The only difference between a left-handed coordinate system and a right-
handed coordinate system is the direction in which one of the three axes is
pointing. For example, if the y-axis points upward and x points to the right,
then z comes toward us (out of the page) in a right-handed system, and away
from us (into the page) in a left-handed system. Left- and right-handed Carte-
sian coordinate systems are depicted in Figure 4.4.
Right-Handed
x
z
y
Left-Handed
x
y
z
Figure 4.4. Left- and right-handed Cartesian coordinate systems.

168
4. 3D Math for Games
It is easy to convert from left-handed to right-handed coordinates and vice
versa. We simply ﬂip the direction of any one axis, leaving the other two
axes alone. It’s important to remember that the rules of mathematics do not
change between left-handed and right-handed coordinate systems. Only our
interpretation of the numbers—our mental image of how the numbers map
into 3D space—changes. Left-handed and right-handed conventions apply
to visualization only, not to the underlying mathematics. (Actually, handed-
ness does matter when dealing with cross products in physical simulations,
because a cross product is not actually a vector—it’s a special mathematical
object known as a pseudovector. We’ll discuss pseudovectors in a little more
depth in Section 4.2.4.9.)
The mapping between the numerical representation and the visual repre-
sentation is entirely up to us as mathematicians and programmers. We could
choose to have the y-axis pointing up, with z forward and x to the left (RH)
or right (LH). Or we could choose to have the z-axis point up. Or the x-axis
could point up instead—or down. All that matters is that we decide upon a
mapping, and then stick with it consistently.
That being said, some conventions do tend to work better than others for
certain applications. For example, 3D graphics programmers typically work
with a left-handed coordinate system, with the y-axis pointing up, x to the
right and positive z pointing away from the viewer (i.e., in the direction the
virtual camera is pointing). When 3D graphics are rendered onto a 2D screen
using this particular coordinate system, increasing z-coordinates correspond
to increasing depth into the scene (i.e., increasing distance away from the vir-
tual camera). As we will see in subsequent chapters, this is exactly what is
required when using a z-buffering scheme for depth occlusion.
4.2.3
Vectors
A vector is a quantity that has both a magnitude and a direction in n-dimensional
space. A vector can be visualized as a directed line segment extending from a
point called the tail to a point called the head. Contrast this to a scalar (i.e., an
ordinary real-valued number), which represents a magnitude but has no di-
rection. Usually scalars are written in italics (e.g., v) while vectors are written
in boldface (e.g., v).
A 3D vector can be represented by a triple of scalars (x, y, z), just as a point
can be. The distinction between points and vectors is actually quite subtle.
Technically, a vector is just an offset relative to some known point. A vector
can be moved anywhere in 3D space—as long as its magnitude and direction
don’t change, it is the same vector.
A vector can be used to represent a point, provided that we ﬁx the tail of
the vector to the origin of our coordinate system. Such a vector is sometimes

4.2. Points and Vectors
169
called a position vector or radius vector. For our purposes, we can interpret any
triple of scalars as either a point or a vector, provided that we remember that
a position vector is constrained such that its tail remains at the origin of the
chosen coordinate system. This implies that points and vectors are treated in
subtly different ways mathematically. One might say that points are absolute,
while vectors are relative.
The vast majority of game programmers use the term “vector” to refer
both to points (position vectors) and to vectors in the strict linear algebra
sense (purely directional vectors). Most 3D math libraries also use the term
“vector” in this way. In this book, we’ll use the term “direction vector” or
just “direction” when the distinction is important. Be careful to always keep
the difference between points and directions clear in your mind (even if your
math library doesn’t). As we’ll see in Section 4.3.6.1, directions need to be
treated differently from points when converting them into homogeneous co-
ordinates for manipulation with 4 × 4 matrices, so getting the two types of
vector mixed up can and will lead to bugs in your code.
4.2.3.1
Cartesian Basis Vectors
It is often useful to deﬁne three orthogonal unit vectors (i.e., vectors that are mu-
tually perpendicular and each with a length equal to one), corresponding to
the three principal Cartesian axes. The unit vector along the x-axis is typically
called i, the y-axis unit vector is called j, and the z-axis unit vector is called k.
The vectors i, j and k are sometimes called Cartesian basis vectors.
Any point or vector can be expressed as a sum of scalars (real numbers)
multiplied by these unit basis vectors. For example,
(5, 3, −2) = 5i + 3j −2k.
4.2.4
Vector Operations
Most of the mathematical operations that you can perform on scalars can be
applied to vectors as well. There are also some new operations that apply only
to vectors.
4.2.4.1
Multiplication by a Scalar
Multiplication of a vector a by a scalar s is accomplished by multiplying the
individual components of a by s:
sa = (sax, say, saz).

170
4. 3D Math for Games
v
2v
v
Figure 4.5. Multiplication of a vector by the scalar 2.
Multiplication by a scalar has the effect of scaling the magnitude of the
vector, while leaving its direction unchanged, as shown in Figure 4.5. Multi-
plication by −1 ﬂips the direction of the vector (the head becomes the tail and
vice versa).
The scale factor can be different along each axis. We call this nonuniform
scale, and it can be represented as the component-wise product of a scaling vector
s and the vector in question, which we’ll denote with the ⊗operator. Techni-
cally speaking, this special kind of product between two vectors is known as
the Hadamard product. It is rarely used in the game industry—in fact, nonuni-
form scaling is one of its only commonplace uses in games:
s ⊗a = (sxax, syay, szaz).
(4.1)
As we’ll see in Section 4.3.7.3, a scaling vector s is really just a compact
way to represent a 3 × 3 diagonal scaling matrix S. So another way to write
Equation (4.1) is as follows:
aS =
ax
ay
az



sx
0
0
0
sy
0
0
0
sz

=
sxax
syay
szaz

.
We’ll explore matrices in more depth in Section 4.3.
4.2.4.2
Addition and Subtraction
The addition of two vectors a and b is deﬁned as the vector whose compo-
nents are the sums of the components of a and b. This can be visualized by
placing the head of vector a onto the tail of vector b—the sum is then the
vector from the tail of a to the head of b (see also Figure 4.6):
a + b =
(ax + bx), (ay + by), (az + bz)
.
Vector subtraction a −b is nothing more than addition of a and −b (i.e., the
result of scaling b by −1, which ﬂips it around). This corresponds to the vector

4.2. Points and Vectors
171
a + b
–b
b
a
a – b
Figure 4.6. Vector addition and subtraction.
x
y
Figure 4.7. Magnitude of a vector (shown
in 2D for ease of illustration).
whose components are the difference between the components of a and the
components of b:
a −b =
(ax −bx), (ay −by), (az −bz)
.
Vector addition and subtraction are depicted in Figure 4.6.
Adding and Subtracting Points and Directions
You can add and subtract direction vectors freely. However, technically speak-
ing, points cannot be added to one another—you can only add a direction vec-
tor to a point, the result of which is another point. Likewise, you can take the
difference between two points, resulting in a direction vector. These opera-
tions are summarized below:
•
direction + direction = direction
•
direction – direction = direction
•
point + direction = point
•
point – point = direction
•
point + point = nonsense
4.2.4.3
Magnitude
The magnitude of a vector is a scalar representing the length of the vector as
it would be measured in 2D or 3D space. It is denoted by placing vertical bars
around the vector’s boldface symbol. We can use the Pythagorean theorem to
calculate a vector’s magnitude, as shown in Figure 4.7:
|a| =
q
a2x + a2y + a2z.

172
4. 3D Math for Games
4.2.4.4
Vector Operations in Action
Believe it or not, we can already solve all sorts of real-world game problems
given just the vector operations we’ve learned thus far. When trying to solve a
problem, we can use operations like addition, subtraction, scaling and magni-
tude to generate new data out of the things we already know. For example, if
we have the current position vector of an AI character P1, and a vector v rep-
resenting her current velocity, we can ﬁnd her position on the next frame P2
by scaling the velocity vector by the frame time interval ∆t, and then adding it
to the current position. As shown in Figure 4.8, the resulting vector equation
is P2 = P1 + v ∆t. (This is known as explicit Euler integration—it’s actually
only valid when the velocity is constant, but you get the idea.)
1
2
Figure 4.8.
Simple
vector addition can be
used to ﬁnd a char-
acter’s position in the
next frame, given her
position and velocity
in the current frame.
As another example, let’s say we have two spheres, and we want to know
whether they intersect.
Given that we know the center points of the two
spheres, C1 and C2, we can ﬁnd a direction vector between them by sim-
ply subtracting the points, d = C2 −C1. The magnitude of this vector d = |d|
determines how far apart the spheres’ centers are. If this distance is less than
the sum of the spheres’ radii, they are intersecting; otherwise they’re not. This
is shown in Figure 4.9.
Square roots are expensive to calculate on most computers, so game pro-
grammers should always use the squared magnitude whenever it is valid to do
so:
|a|2 = (a2
x + a2
y + a2
z).
Using the squared magnitude is valid when comparing the relative lengths
of two vectors (“is vector a longer than vector b?”), or when comparing a
1
2
y
x
1r
2r
d
2 –
1
Figure 4.9. A sphere-sphere intersection test involves only vector subtraction, vector magnitude
and ﬂoating-point comparison operations.

4.2. Points and Vectors
173
vector’s magnitude to some other (squared) scalar quantity. So in our sphere-
sphere intersection test, we should calculate d2 = |d|2 and compare this to
the squared sum of the radii, (r1 + r2)2 for maximum speed. When writing
high-performance software, never take a square root when you don’t have to!
4.2.4.5
Normalization and Unit Vectors
A unit vector is a vector with a magnitude (length) of one. Unit vectors are
very useful in 3D mathematics and game programming, for reasons we’ll see
below.
Given an arbitrary vector v of length v = |v|, we can convert it to a unit
vector u that points in the same direction as v, but has unit length. To do
this, we simply multiply v by the reciprocal of its magnitude. We call this
normalization:
u = v
|v| = 1
v v.
4.2.4.6
Normal Vectors
A vector is said to be normal to a surface if it is perpendicular to that surface.
Normal vectors are highly useful in games and computer graphics. For exam-
ple, a plane can be deﬁned by a point and a normal vector. And in 3D graphics,
lighting calculations make heavy use of normal vectors to deﬁne the direction
of surfaces relative to the direction of the light rays impinging upon them.
Normal vectors are usually of unit length, but they do not need to be.
Be careful not to confuse the term “normalization” with the term “normal
vector.” A normalized vector is any vector of unit length. A normal vector
is any vector that is perpendicular to a surface, whether or not it is of unit
length.
4.2.4.7
Dot Product and Projection
Vectors can be multiplied, but unlike scalars there are a number of different
kinds of vector multiplication. In game programming, we most often work
with the following two kinds of multiplication:
•
the dot product (a.k.a. scalar product or inner product), and
•
the cross product (a.k.a. vector product or outer product).
The dot product of two vectors yields a scalar; it is deﬁned by adding the
products of the individual components of the two vectors:
a · b = axbx + ayby + azbz = d
(a scalar).

174
4. 3D Math for Games
The dot product can also be written as the product of the magnitudes of the
two vectors and the cosine of the angle between them:
a · b = |a| |b| cos θ.
The dot product is commutative (i.e., the order of the two vectors can be
reversed) and distributive over addition:
a · b = b · a;
a · (b + c) = a · b + a · c.
And the dot product combines with scalar multiplication as follows:
sa · b = a · sb = s(a · b).
Vector Projection
If u is a unit vector (|u| = 1), then the dot product (a · u) represents the length
of the projection of vector a onto the inﬁnite line deﬁned by the direction of
u, as shown in Figure 4.10. This projection concept works equally well in 2D
or 3D and is highly useful for solving a wide variety of three-dimensional
problems.
a
u
a u
Figure 4.10. Vector projection using the dot product.
Magnitude as a Dot Product
The squared magnitude of a vector can be found by taking the dot product of
that vector with itself. Its magnitude is then easily found by taking the square
root:
|a|2 = a · a;
|a| = √a · a.
This works because the cosine of zero degrees is 1, so |a| |a| cos θ = |a| |a| =
|a|2.

4.2. Points and Vectors
175
(a · b) = ab
(a · b) = –ab
(a · b) = 0
(a · b) > 0
(a · b) < 0
a
b
a
b
a
b
a
b
b
a
Figure 4.11. Some common dot product tests.
Dot Product Tests
Dot products are great for testing if two vectors are collinear or perpendicular,
or whether they point in roughly the same or roughly opposite directions. For
any two arbitrary vectors a and b, game programmers often use the following
tests, as shown in Figure 4.11:
•
Collinear. (a · b) = |a| |b| = ab (i.e., the angle between them is exactly 0
degrees—this dot product equals +1 when a and b are unit vectors).
•
Collinear but opposite. (a · b) = −ab (i.e., the angle between them is 180
degrees—this dot product equals −1 when a and b are unit vectors).
•
Perpendicular. (a · b) = 0 (i.e., the angle between them is 90 degrees).
•
Same direction. (a · b) > 0 (i.e., the angle between them is less than 90
degrees).
•
Opposite directions. (a · b) < 0 (i.e., the angle between them is greater
than 90 degrees).
Some Other Applications of the Dot Product
Dot products can be used for all sorts of things in game programming. For
example, let’s say we want to ﬁnd out whether an enemy is in front of the
player character or behind him. We can ﬁnd a vector from the player’s posi-
tion P to the enemy’s position E by simple vector subtraction (v = E −P).
Let’s assume we have a vector f pointing in the direction that the player is
facing. (As we’ll see in Section 4.3.10.3, the vector f can be extracted directly
from the player’s model-to-world matrix.) The dot product d = v · f can be

176
4. 3D Math for Games
Q
n
P
h = (P – Q) n
Figure 4.12. The dot product can be used to ﬁnd the height of a point above or below a plane.
used to test whether the enemy is in front of or behind the player—it will be
positive when the enemy is in front and negative when the enemy is behind.
The dot product can also be used to ﬁnd the height of a point above or
below a plane (which might be useful when writing a moon-landing game
for example). We can deﬁne a plane with two vector quantities: a point Q
lying anywhere on the plane, and a unit vector n that is perpendicular (i.e.,
normal) to the plane. To ﬁnd the height h of a point P above the plane, we
ﬁrst calculate a vector from any point on the plane (Q will do nicely) to the
point in question P. So we have v = P −Q. The dot product of vector v with
the unit-length normal vector n is just the projection of v onto the line deﬁned
by n. But that is exactly the height we’re looking for. Therefore,
h = v · n = (P −Q) · n.
(4.2)
This is illustrated in Figure 4.12.
4.2.4.8
Cross Product
The cross product (also known as the outer product or vector product) of two vec-
tors yields another vector that is perpendicular to the two vectors being multi-
plied, as shown in Figure 4.13. The cross product operation is only deﬁned in
three dimensions:
a × b =
(aybz −azby), (azbx −axbz), (axby −aybx)
= (aybz −azby)i + (azbx −axbz)j + (axby −aybx)k.
Figure
4.13.
The
cross
product
of
vectors
a
and
b
(right-handed).
Magnitude of the Cross Product
The magnitude of the cross product vector is the product of the magnitudes
of the two vectors and the sine of the angle between them. (This is similar to
the deﬁnition of the dot product, but it replaces the cosine with the sine.)

4.2. Points and Vectors
177
2
1
3
2
1
3
1
Figure 4.14. Area of a parallelogram expressed as the magnitude of a cross product.
The magnitude of the cross product |a×b| is equal to the area of the paral-
lelogram whose sides are a and b, as shown in Figure 4.14. Since a triangle is
one half of a parallelogram, the area of a triangle whose vertices are speciﬁed
by the position vectors V1, V2 and V3 can be calculated as one half of the
magnitude of the cross product of any two of its sides:
Atriangle = 1
2
(V2 −V1) × (V3 −V1)
 .
Direction of the Cross Product
When using a right-handed coordinate system, you can use the right-hand rule
to determine the direction of the cross product. Simply cup your ﬁngers such
that they point in the direction you’d rotate vector a to move it on top of vector
b, and the cross product (a × b) will be in the direction of your thumb.
Note that the cross product is deﬁned by the left-hand rule when using
a left-handed coordinate system. This means that the direction of the cross
product changes depending on the choice of coordinate system. This might
seem odd at ﬁrst, but remember that the handedness of a coordinate system
does not affect the mathematical calculations we carry out—it only changes
our visualization of what the numbers look like in 3D space. When converting
from a right-handed system to a left-handed system or vice versa, the numer-
ical representations of all the points and vectors stay the same, but one axis
ﬂips. Our visualization of everything is therefore mirrored along that ﬂipped
axis. So if a cross product just happens to align with the axis we’re ﬂipping
(e.g., the z-axis), it needs to ﬂip when the axis ﬂips. If it didn’t, the mathe-
matical deﬁnition of the cross product itself would have to be changed so that
the z-coordinate of the cross product comes out negative in the new coordi-
nate system. I wouldn’t lose too much sleep over all of this. Just remember:
when visualizing a cross product, use the right-hand rule in a right-handed
coordinate system and the left-hand rule in a left-handed coordinate system.

178
4. 3D Math for Games
Properties of the Cross Product
The cross product is not commutative (i.e., order matters):
a × b ̸= b × a.
However, it is anti-commutative:
a × b = −(b × a).
The cross product is distributive over addition:
a × (b + c) = (a × b) + (a × c).
And it combines with scalar multiplication as follows:
(sa) × b = a × (sb) = s(a × b).
The Cartesian basis vectors are related by cross products as follows:
i × j = −(j × i) = k
j × k = −(k × j) = i
k × i = −(i × k) = j
These three cross products deﬁne the direction of positive rotations about the
Cartesian axes. The positive rotations go from x to y (about z), from y to z
(about x) and from z to x (about y). Notice how the rotation about the y-axis
“reversed” alphabetically, in that it goes from z to x (not from x to z). As we’ll
see below, this gives us a hint as to why the matrix for rotation about the y-
axis looks inverted when compared to the matrices for rotation about the x-
and z-axes.
The Cross Product in Action
The cross product has a number of applications in games. One of its most
common uses is for ﬁnding a vector that is perpendicular to two other vectors.
As we’ll see in Section 4.3.10.2, if we know an object’s local unit basis vectors,
(ilocal, jlocal and klocal), we can easily ﬁnd a matrix representing the object’s
orientation. Let’s assume that all we know is the object’s klocal vector—i.e., the
direction in which the object is facing. If we assume that the object has no roll
about klocal, then we can ﬁnd ilocal by taking the cross product between klocal
(which we already know) and the world-space up vector jworld (which equals

4.2. Points and Vectors
179
[ 0 1 0 ]). We do so as follows: ilocal = normalize(jworld × klocal). We can then
ﬁnd jlocal by simply crossing ilocal and klocal as follows: jlocal = klocal × ilocal.
A very similar technique can be used to ﬁnd a unit vector normal to the
surface of a triangle or some other plane. Given three points on the plane, P1,
P2 and P3, the normal vector is just n = normalize
 (P2 −P1) × (P3 −P1)

.
Cross products are also used in physics simulations. When a force is ap-
plied to an object, it will give rise to rotational motion if and only if it is ap-
plied off-center. This rotational force is known as a torque, and it is calculated
as follows. Given a force F, and a vector r from the center of mass to the point
at which the force is applied, the torque N = r × F.
4.2.4.9
Pseudovectors and Exterior Algebra
We mentioned in Section 4.2.2 that the cross product doesn’t actually produce
a vector—it produces a special kind of mathematical object known as a pseu-
dovector. The difference between a vector and a pseudovector is pretty sub-
tle. In fact, you can’t tell the difference between them at all when performing
the kinds of transformations we normally encounter in game programming—
translation, rotation and scaling. It’s only when you reﬂect the coordinate sys-
tem (as happens when you move from a left-handed coordinate system to a
right-handed system) that the special nature of pseudovectors becomes ap-
parent. Under reﬂection, a vector transforms into its mirror image, as you’d
probably expect. But when a pseudovector is reﬂected, it transforms into its
mirror image and also changes direction.
Positions and all of the derivatives thereof (linear velocity, acceleration,
jerk) are represented by true vectors (also known as polar vectors or contravari-
ant vectors). Angular velocities and magnetic ﬁelds are represented by pseu-
dovectors (also known as axial vectors, covariant vectors, bivectors or 2-blades).
The surface normal of a triangle (which is calculated using a cross product) is
also a pseudovector.
It’s pretty interesting to note that the cross product (A×B), the scalar triple
product (A·(B×C)) and the determinant of a matrix are all inter-related, and
pseudovectors lie at the heart of it all. Mathematicians have come up with
a set of algebraic rules, called an exterior algebra or Grassman algebra, which
describe how vectors and pseudovectors work and allow us to calculate areas
of parallelograms (in 2D), volumes of parallelepipeds (in 3D), and so on in
higher dimensions.
We won’t get into all the details here, but the basic idea of Grassman al-
gebra is to introduce a special kind of vector product known as the wedge
product, denoted A ∧B. A pairwise wedge product yields a pseudovector
and is equivalent to a cross product, which also represents the signed area of

180
4. 3D Math for Games
u
v
u
v
w
Figure 4.15.
In the exterior algebra (Grassman algebra), a single wedge product yields a pseu-
dovector or bivector, and two wedge products yields a pseudoscalar or trivector.
the parallelogram formed by the two vectors (where the sign tells us whether
we’re rotating from A to B or vice versa). Doing two wedge products in a row,
A ∧B ∧C, is equivalent to the scalar triple product A · (B × C) and produces
another strange mathematical object known as a pseudoscalar (also known as
a trivector or a 3-blade), which can be interpreted as the signed volume of the
parallelepiped formed by the three vectors (see Figure 4.15). This extends into
higher dimensions as well.
What does all this mean for us as game programmers? Not too much. All
we really need to keep in mind is that some vectors in our code are actually
pseudovectors, so that we can transform them properly when changing hand-
edness, for example. Of course if you really want to geek out, you can impress
your friends by talking about exterior algebras and wedge products and explain-
ing how cross products aren’t really vectors. Which might make you look cool
at your next social engagement ...or not.
For more information, see http://en.wikipedia.org/wiki/Pseudovector and
http://en.wikipedia.org/wiki/Exterior_algebra.

4.3. Matrices
181
Figure 4.16. Linear interpolation (LERP) between points A and B, with β = 0.4.
4.2.5
Linear Interpolation of Points and Vectors
In games, we often need to ﬁnd a vector that is midway between two known
vectors. For example, if we want to smoothly animate an object from point A
to point B over the course of two seconds at 30 frames per second, we would
need to ﬁnd 60 intermediate positions between A and B.
A linear interpolation is a simple mathematical operation that ﬁnds an in-
termediate point between two known points. The name of this operation is
often shortened to LERP. The operation is deﬁned as follows, where β ranges
from 0 to 1 inclusive:
L = LERP(A, B, β) = (1 −β)A + βB
=
(1 −β)Ax + βBx,
(1 −β)Ay + βBy,
(1 −β)Az + βBz

Geometrically, L = LERP(A, B, β) is the position vector of a point that lies
β percent of the way along the line segment from point A to point B, as shown
in Figure 4.16. Mathematically, the LERP function is just a weighted average of
the two input vectors, with weights (1 −β) and β, respectively. Notice that
the weights always add to 1, which is a general requirement for any weighted
average.
4.3
Matrices
A matrix is a rectangular array of m×n scalars. Matrices are a convenient way
of representing linear transformations such as translation, rotation and scale.
A matrix M is usually written as a grid of scalars Mrc enclosed in square
brackets, where the subscripts r and c represent the row and column indices of
the entry, respectively. For example, if M is a 3 × 3 matrix, it could be written
as follows:
M =


M11
M12
M13
M21
M22
M23
M31
M32
M33

.

182
4. 3D Math for Games
We can think of the rows and/or columns of a 3 × 3 matrix as 3D vectors.
When all of the row and column vectors of a 3×3 matrix are of unit magnitude,
we call it a special orthogonal matrix. This is also known as an isotropic matrix,
or an orthonormal matrix. Such matrices represent pure rotations.
Under certain constraints, a 4×4 matrix can represent arbitrary 3D transfor-
mations, including translations, rotations, and changes in scale. These are called
transformation matrices, and they are the kinds of matrices that will be most
useful to us as game engineers. The transformations represented by a matrix
are applied to a point or vector via matrix multiplication. We’ll investigate
how this works below.
An afﬁne matrix is a 4 × 4 transformation matrix that preserves parallelism
of lines and relative distance ratios, but not necessarily absolute lengths and
angles. An afﬁne matrix is any combination of the following operations: rota-
tion, translation, scale and/or shear.
4.3.1
Matrix Multiplication
The product P of two matrices A and B is written P = AB. If A and B are
transformation matrices, then the product P is another transformation ma-
trix that performs both of the original transformations. For example, if A is a
scale matrix and B is a rotation, the matrix P would both scale and rotate the
points or vectors to which it is applied. This is particularly useful in game pro-
gramming, because we can precalculate a single matrix that performs a whole
sequence of transformations and then apply all of those transformations to a
large number of vectors efﬁciently.
To calculate a matrix product, we simply take dot products between the
rows of the nA × mA matrix A and the columns of the nB × mB matrix B.
Each dot product becomes one component of the resulting matrix P. The
two matrices can be multiplied as long as the inner dimensions are equal (i.e.,
mA = nB). For example, if A and B are 3 × 3 matrices, then P = AB may be
expressed as follows:
P =


P11
P12
P13
P21
P22
P23
P31
P32
P33


=


Arow1 · Bcol1
Arow1 · Bcol2
Arow1 · Bcol3
Arow2 · Bcol1
Arow2 · Bcol2
Arow2 · Bcol3
Arow3 · Bcol1
Arow3 · Bcol2
Arow3 · Bcol3

.

4.3. Matrices
183
Matrix multiplication is not commutative. In other words, the order in
which matrix multiplication is done matters:
AB ̸= BA
We’ll see exactly why this matters in Section 4.3.2.
Matrix multiplication is often called concatenation, because the product of
n transformation matrices is a matrix that concatenates, or chains together,
the original sequence of transformations in the order the matrices were multi-
plied.
4.3.2
Representing Points and Vectors as Matrices
Points and vectors can be represented as row matrices (1×n) or column matrices
(n × 1), where n is the dimension of the space we’re working with (usually 2
or 3). For example, the vector v = (3, 4, −1) can be written either as
v1 =
3
4
−1
or as
v2 =


3
4
−1

= vT
1.
Here, the superscripted T represents matrix transposition (see Section 4.3.5).
The choice between column and row vectors is a completely arbitrary one,
but it does affect the order in which matrix multiplications are written. This
happens because when multiplying matrices, the inner dimensions of the two
matrices must be equal, so
•
to multiply a 1×n row vector by an n×n matrix, the vector must appear
to the left of the matrix (v′
1×n = v1×n Mn×n), whereas
•
to multiply an n × n matrix by an n × 1 column vector, the vector must
appear to the right of the matrix (v′
n×1 = Mn×n vn×1).
If multiple transformation matrices A, B and C are applied in order to a
vector v, the transformations “read” from left to right when using row vectors,
but from right to left when using column vectors. The easiest way to remember
this is to realize that the matrix closest to the vector is applied ﬁrst. This is
illustrated by the parentheses below:
v′ = (((vA)B)C)
Row vectors: read left-to-right;
v′T = (CT(BT(ATvT)))
Column vectors: read right-to-left.

184
4. 3D Math for Games
In this book we’ll adopt the row vector convention, because the left-to-right
order of transformations is most intuitive to read for English-speaking people.
That said, be very careful to check which convention is used by your game
engine, and by other books, papers or web pages you may read. You can
usually tell by seeing whether vector-matrix multiplications are written with
the vector on the left (for row vectors) or the right (for column vectors) of the
matrix. When using column vectors, you’ll need to transpose all the matrices
shown in this book.
4.3.3
The Identity Matrix
The identity matrix is a matrix that, when multiplied by any other matrix,
yields the very same matrix. It is usually represented by the symbol I. The
identity matrix is always a square matrix with 1’s along the diagonal and 0’s
everywhere else:
I3×3 =


1
0
0
0
1
0
0
0
1

;
AI = IA ≡A.
4.3.4
Matrix Inversion
The inverse of a matrix A is another matrix (denoted A−1) that undoes the ef-
fects of matrix A. So, for example, if A rotates objects by 37 degrees about
the z-axis, then A−1 will rotate by −37 degrees about the z-axis. Likewise,
if A scales objects to be twice their original size, then A−1 scales objects to
be half-sized. When a matrix is multiplied by its own inverse, the result is
always the identity matrix, so A(A−1) ≡(A−1)A ≡I. Not all matrices have
inverses. However, all afﬁne matrices (combinations of pure rotations, trans-
lations, scales and shears) do have inverses. Gaussian elimination or lower-
upper (LU) decomposition can be used to ﬁnd the inverse, if one exists.
Since we’ll be dealing with matrix multiplication a lot, it’s important to
note here that the inverse of a sequence of concatenated matrices can be writ-
ten as the reverse concatenation of the individual matrices’ inverses. For example,
(ABC)−1 = C−1B−1A−1.

4.3. Matrices
185
4.3.5
Transposition
The transpose of a matrix M is denoted MT. It is obtained by reﬂecting the
entries of the original matrix across its diagonal. In other words, the rows of
the original matrix become the columns of the transposed matrix, and vice
versa:


a
b
c
d
e
f
g
h
i


T
=


a
d
g
b
e
h
c
f
i

.
The transpose is useful for a number of reasons. For one thing, the inverse
of an orthonormal (pure rotation) matrix is exactly equal to its transpose—
which is good news, because it’s much cheaper to transpose a matrix than
it is to ﬁnd its inverse in general. Transposition can also be important when
moving data from one math library to another, because some libraries use
column vectors while others expect row vectors. The matrices used by a row-
vector–based library will be transposed relative to those used by a library that
employs the column vector convention.
As with the inverse, the transpose of a sequence of concatenated matri-
ces can be rewritten as the reverse concatenation of the individual matrices’
transposes. For example,
(ABC)T = CTBTAT.
This will prove useful when we consider how to apply transformation matri-
ces to points and vectors.
4.3.6
Homogeneous Coordinates
You may recall from high-school algebra that a 2 × 2 matrix can represent a
rotation in two dimensions. To rotate a vector r through an angle of φ degrees
(where positive rotations are counterclockwise), we can write
r′
x
r′
y

=
rx
ry
 
cos φ
sin φ
−sin φ
cos φ

.
It’s probably no surprise that rotations in three dimensions can be represented
by a 3 × 3 matrix. The two-dimensional example above is really just a three-
dimensional rotation about the z-axis, so we can write
r′
x
r′
y
r′
z

=
rx
ry
rz



cos φ
sin φ
0
−sin φ
cos φ
0
0
0
1

.

186
4. 3D Math for Games
The question naturally arises: Can a 3 × 3 matrix be used to represent
translations? Sadly, the answer is no. The result of translating a point r by
a translation t requires adding the components of t to the components of r
individually:
r + t =
(rx + tx)
(ry + ty)
(rz + tz)
.
Matrix multiplication involves multiplication and addition of matrix elements,
so the idea of using multiplication for translation seems promising. But, un-
fortunately, there is no way to arrange the components of t within a 3 × 3
matrix such that the result of multiplying it with the column vector r yields
sums like (rx + tx).
The good news is that we can obtain sums like this if we use a 4×4 matrix.
What would such a matrix look like? Well, we know that we don’t want any
rotational effects, so the upper 3 × 3 should contain an identity matrix. If
we arrange the components of t across the bottom-most row of the matrix
and set the fourth element of the r vector (usually called w) equal to 1, then
taking the dot product of the vector r with column 1 of the matrix will yield
(1·rx)+(0·ry)+(0·rz)+(tx ·1), which is exactly what we want. If the bottom
right-hand corner of the matrix contains a 1 and the rest of the fourth column
contains zeros, then the resulting vector will also have a 1 in its w component.
Here’s what the ﬁnal 4 × 4 translation matrix looks like:
r + t =
rx
ry
rz
1


1
0
0
0
0
1
0
0
0
0
1
0
tx
ty
tz
1


=
(rx + tx)
(ry + ty)
(rz + tz)
1
.
When a point or vector is extended from three dimensions to four in this
manner, we say that it has been written in homogeneous coordinates. A point
in homogeneous coordinates always has w = 1. Most of the 3D matrix math
done by game engines is performed using 4 × 4 matrices with four-element
points and vectors written in homogeneous coordinates.
4.3.6.1
Transforming Direction Vectors
Mathematically, points (position vectors) and direction vectors are treated in
subtly different ways. When transforming a point by a matrix, the translation,
rotation and scale of the matrix are all applied to the point. But when trans-
forming a direction by a matrix, the translational effects of the matrix are ig-
nored. This is because direction vectors have no translation per se—applying
a translation to a direction would alter its magnitude, which is usually not
what we want.

4.3. Matrices
187
In homogeneous coordinates, we achieve this by deﬁning points to have
their w components equal to one, while direction vectors have their w compo-
nents equal to zero. In the example below, notice how the w = 0 component
of the vector v multiplies with the t vector in the matrix, thereby eliminating
translation in the ﬁnal result:
v
0 
U
0
t
1

=
(vU + 0t)
0
=
vU
0
.
Technically, a point in homogeneous (four-dimensional) coordinates can
be converted into non-homogeneous (three-dimensional) coordinates by di-
viding the x, y and z components by the w component:
x
y
z
w
≡
h x
w
y
w
z
w
i
.
This sheds some light on why we set a point’s w component to one and a
vector’s w component to zero. Dividing by w = 1 has no effect on the coordi-
nates of a point, but dividing a pure direction vector’s components by w = 0
would yield inﬁnity. A point at inﬁnity in 4D can be rotated but not translated,
because no matter what translation we try to apply, the point will remain at
inﬁnity. So in effect, a pure direction vector in three-dimensional space acts
like a point at inﬁnity in four-dimensional homogeneous space.
4.3.7
Atomic Transformation Matrices
Any afﬁne transformation matrix can be created by simply concatenating a
sequence of 4×4 matrices representing pure translations, pure rotations, pure
scale operations and/or pure shears. These atomic transformation building
blocks are presented below. (We’ll omit shear from these discussions, as it
tends to be used only rarely in games.)
Notice that all afﬁne 4 × 4 transformation matrices can be partitioned into
four components:
Mafﬁne =
U3×3
03×1
t1×3
1

.
•
the upper 3 × 3 matrix U, which represents the rotation and/or scale,
•
a 1 × 3 translation vector t,
•
a 3 × 1 vector of zeros 0 =
0
0
0T, and
•
a scalar 1 in the bottom-right corner of the matrix.

188
4. 3D Math for Games
When a point is multiplied by a matrix that has been partitioned like this, the
result is as follows:
r′
1×3
1
=
r1×3
1 U3×3
03×1
t1×3
1

=
(rU + t)
1
.
4.3.7.1
Translation
The following matrix translates a point by the vector t:
r + t =
rx
ry
rz
1


1
0
0
0
0
1
0
0
0
0
1
0
tx
ty
tz
1


(4.3)
=
(rx + tx)
(ry + ty)
(rz + tz)
1
,
or in partitioned shorthand:
r
1 I
0
t
1

=
(r + t)
1
.
To invert a pure translation matrix, simply negate the vector t (i.e., negate tx,
ty and tz).
4.3.7.2
Rotation
All 4 × 4 pure rotation matrices have the form

r
1
 R
0
0
1

=

rR
1

.
The t vector is zero, and the upper 3 × 3 matrix R contains cosines and sines
of the rotation angle, measured in radians.
The following matrix represents rotation about the x-axis by an angle φ.
rotatex(r, φ) =
rx
ry
rz
1


1
0
0
0
0
cos φ
sin φ
0
0
−sin φ
cos φ
0
0
0
0
1

.
(4.4)
The matrix below represents rotation about the y-axis by an angle θ. (Notice
that this one is transposed relative to the other two—the positive and negative
sine terms have been reﬂected across the diagonal.)
rotatey(r, θ) =
rx
ry
rz
1


cos θ
0
−sin θ
0
0
1
0
0
sin θ
0
cos θ
0
0
0
0
1

.
(4.5)

4.3. Matrices
189
The following matrix represents rotation about the z-axis by an angle γ:
rotatez(r, γ) =
rx
ry
rz
1


cos γ
sin γ
0
0
−sin γ
cos γ
0
0
0
0
1
0
0
0
0
1

.
(4.6)
Here are a few observations about these matrices:
•
The 1 within the upper 3 × 3 always appears on the axis we’re rotating
about, while the sine and cosine terms are off-axis.
•
Positive rotations go from x to y (about z), from y to z (about x) and from
z to x (about y). The z to x rotation “wraps around,” which is why the
rotation matrix about the y-axis is transposed relative to the other two.
(Use the right-hand or left-hand rule to remember this.)
•
The inverse of a pure rotation is just its transpose. This works because
inverting a rotation is equivalent to rotating by the negative angle. You
may recall that cos(−θ) = cos(θ) while sin(−θ) = −sin(θ), so negating
the angle causes the two sine terms to effectively switch places, while
the cosine terms stay put.
4.3.7.3
Scale
The following matrix scales the point r by a factor of sx along the x-axis, sy
along the y-axis and sz along the z-axis:
rS =
rx
ry
rz
1


sx
0
0
0
0
sy
0
0
0
0
sz
0
0
0
0
1


(4.7)
=
sxrx
syry
szrz
1
,
or in partitioned shorthand:
r
1 S3×3
0
0
1

=
rS3×3
1
.
Here are some observations about this kind of matrix:
•
To invert a scaling matrix, simply substitute sx, sy and sz with their
reciprocals (i.e., 1/sx, 1/sy and 1/sz).
•
When the scale factor along all three axes is the same (sx = sy = sz),
we call this uniform scale. Spheres remain spheres under uniform scale,
whereas under nonuniform scale they become ellipsoids. To keep the

190
4. 3D Math for Games
mathematics of bounding sphere checks simple and fast, many game
engines impose the restriction that only uniform scale may be applied
to renderable geometry or collision primitives.
•
When a uniform scale matrix Su and a rotation matrix R are concate-
nated, the order of multiplication is unimportant (i.e., SuR = RSu).
This only works for uniform scale!
4.3.8
4 × 3 Matrices
The rightmost column of an afﬁne 4 × 4 matrix always contains the vector
0
0
0
1T. As such, game programmers often omit the fourth column to
save memory. You’ll encounter 4 × 3 afﬁne matrices frequently in game math
libraries.
4.3.9
Coordinate Spaces
We’ve seen how to apply transformations to points and direction vectors us-
ing 4 × 4 matrices. We can extend this idea to rigid objects by realizing that
such an object can be thought of as an inﬁnite collection of points. Applying
a transformation to a rigid object is like applying that same transformation to
every point within the object. For example, in computer graphics an object is
usually represented by a mesh of triangles, each of which has three vertices
represented by points. In this case, the object can be transformed by applying
a transformation matrix to all of its vertices in turn.
We said above that a point is a vector whose tail is ﬁxed to the origin of
some coordinate system. This is another way of saying that a point (position
vector) is always expressed relative to a set of coordinate axes. The triplet
of numbers representing a point changes numerically whenever we select a
new set of coordinate axes. In Figure 4.17, we see a point P represented by
two different position vectors—the vector PA gives the position of P relative
A
A
B
B
A
B
Figure 4.17. Position vectors for the point P relative to different coordinate axes.

4.3. Matrices
191
to the “A” axes, while the vector PB gives the position of that same point
relative to a different set of axes “B.”
In physics, a set of coordinate axes represents a frame of reference, so we
sometimes refer to a set of axes as a coordinate frame (or just a frame). People in
the game industry also use the term coordinate space (or simply space) to refer
to a set of coordinate axes. In the following sections, we’ll look at a few of the
most common coordinate spaces used in games and computer graphics.
4.3.9.1
Model Space
When a triangle mesh is created in a tool such as Maya or 3DStudioMAX, the
positions of the triangles’ vertices are speciﬁed relative to a Cartesian coor-
dinate system, which we call model space (also known as object space or local
space). The model-space origin is usually placed at a central location within
the object, such as at its center of mass, or on the ground between the feet of a
humanoid or animal character.
Most game objects have an inherent directionality. For example, an air-
plane has a nose, a tail ﬁn and wings that correspond to the front, up and
left/right directions. The model-space axes are usually aligned to these natu-
ral directions on the model, and they’re given intuitive names to indicate their
directionality as illustrated in Figure 4.18.
•
Front. This name is given to the axis that points in the direction that the
object naturally travels or faces. In this book, we’ll use the symbol F to
refer to a unit basis vector along the front axis.
•
Up. This name is given to the axis that points towards the top of the
object. The unit basis vector along this axis will be denoted U.
•
Left or right. The name “left” or “right” is given to the axis that points
toward the left or right side of the object. Which name is chosen de-
pends on whether your game engine uses left-handed or right-handed
left
front
up
Figure 4.18. One possible choice of the model-space front, left and up axis basis vectors for an
airplane.

192
4. 3D Math for Games
coordinates. The unit basis vector along this axis will be denoted L or
R, as appropriate.
The mapping between the (front, up, left) labels and the (x, y, z) axes
is completely arbitrary. A common choice when working with right-handed
axes is to assign the label front to the positive z-axis, the label left to the positive
x-axis and the label up to the positive y-axis (or in terms of unit basis vectors,
F = k, L = i and U = j). However, it’s equally common for +x to be front and
+z to be right (F = i, R = k, U = j). I’ve also worked with engines in which
the z-axis is oriented vertically. The only real requirement is that you stick to
one convention consistently throughout your engine.
As an example of how intuitive axis names can reduce confusion, consider
Euler angles (pitch, yaw, roll), which are often used to describe an aircraft’s
orientation. It’s not possible to deﬁne pitch, yaw, and roll angles in terms of
the (i, j, k) basis vectors because their orientation is arbitrary. However, we
can deﬁne pitch, yaw and roll in terms of the (L, U, F) basis vectors, because
their orientations are clearly deﬁned. Speciﬁcally,
•
pitch is rotation about L or R,
•
yaw is rotation about U, and
•
roll is rotation about F.
4.3.9.2
World Space
World space is a ﬁxed coordinate space, in which the positions, orientations
and scales of all objects in the game world are expressed. This coordinate
space ties all the individual objects together into a cohesive virtual world.
The location of the world-space origin is arbitrary, but it is often placed
near the center of the playable game space to minimize the reduction in ﬂoating-
point precision that can occur when (x, y, z) coordinates grow very large.
Likewise, the orientation of the x-, y- and z-axes is arbitrary, although most
of the engines I’ve encountered use either a y-up or a z-up convention. The
y-up convention was probably an extension of the two-dimensional conven-
tion found in most mathematics textbooks, where the y-axis is shown going
up and the x-axis going to the right. The z-up convention is also common,
because it allows a top-down orthographic view of the game world to look
like a traditional two-dimensional xy-plot.
As an example, let’s say that our aircraft’s left wingtip is at (5, 0, 0) in
model space. (In our game, front vectors correspond to the positive z-axis in
model space with y up, as shown in Figure 4.18.) Now imagine that the jet
is facing down the positive x-axis in world space, with its model-space origin

4.3. Matrices
193
zW
xW
xM
zM
(5,0,0)M
(–25,50,3)W
(–25,50,8)W
Aircraft:
Left
Wingtip:
Figure 4.19. A lear jet whose left wingtip is at (5, 0, 0) in model space. If the jet is rotated by
90 degrees about the world-space y-axis, and its model-space origin translated to (−25, 50, 8)
in world space, then its left wingtip would end up at (−25, 50, 3) when expressed in world-space
coordinates.
at some arbitrary location, such as (−25, 50, 8). Because the F vector of the
airplane, which corresponds to +z in model space, is facing down the +x-axis
in world space, we know that the jet has been rotated by 90 degrees about
the world y-axis. So, if the aircraft were sitting at the world-space origin, its
left wingtip would be at (0, 0, −5) in world space. But because the aircraft’s
origin has been translated to (−25, 50, 8), the ﬁnal position of the jet’s left
wingtip in world space is (−25, 50, [8 −5]) = (−25, 50, 3). This is illustrated
in Figure 4.19.
We could of course populate our friendly skies with more than one Lear
jet. In that case, all of their left wingtips would have coordinates of (5, 0, 0)
in model space. But in world space, the left wingtips would have all sorts of
interesting coordinates, depending on the orientation and translation of each
aircraft.
4.3.9.3
View Space
View space (also known as camera space) is a coordinate frame ﬁxed to the cam-
era. The view space origin is placed at the focal point of the camera. Again,
any axis orientation scheme is possible. However, a y-up convention with z
increasing in the direction the camera is facing (left-handed) is typical because
it allows z coordinates to represent depths into the screen. Other engines
and APIs, such as OpenGL, deﬁne view space to be right-handed, in which
case the camera faces towards negative z, and z coordinates represent negative
depths. Two possible deﬁnitions of view space are illustrated in Figure 4.20.
4.3.10
Change of Basis
In games and computer graphics, it is often quite useful to convert an object’s
position, orientation and scale from one coordinate system into another. We
call this operation a change of basis.

194
4. 3D Math for Games
Left-Handed
x
z
y
Right-Handed
z
x
y
Virtual
Screen
Virtual 
Screen
Figure 4.20. Left- and right-handed examples of view space, also known as camera space.
4.3.10.1
Coordinate Space Hierarchies
Coordinate frames are relative. That is, if you want to quantify the position,
orientation and scale of a set of axes in three-dimensional space, you must
specify these quantities relative to some other set of axes (otherwise the num-
bers would have no meaning). This implies that coordinate spaces form a hi-
erarchy—every coordinate space is a child of some other coordinate space, and
the other space acts as its parent. World space has no parent; it is at the root
of the coordinate-space tree, and all other coordinate systems are ultimately
speciﬁed relative to it, either as direct children or more-distant relatives.
4.3.10.2
Building a Change of Basis Matrix
The matrix that transforms points and directions from any child coordinate
system C to its parent coordinate system P can be written MC→P (pronounced
“C to P”). The subscript indicates that this matrix transforms points and di-
rections from child space to parent space. Any child-space position vector PC
can be transformed into a parent-space position vector PP as follows:
PP = PCMC→P ;
MC→P =


iC
0
jC
0
kC
0
tC
1


=


iCx
iCy
iCz
0
jCx
jCy
jCz
0
kCx
kCy
kCz
0
tCx
tCy
tCz
1

.

4.3. Matrices
195
In this equation,
•
iC is the unit basis vector along the child space x-axis, expressed in
parent-space coordinates;
•
jC is the unit basis vector along the child space y-axis, in parent space;
•
kC is the unit basis vector along the child space z-axis, in parent space;
and
•
tC is the translation of the child coordinate system relative to parent
space.
This result should not be too surprising. The tC vector is just the transla-
tion of the child-space axes relative to parent space, so if the rest of the matrix
were identity, the point (0, 0, 0) in child space would become tC in parent
space, just as we’d expect. The iC, jC and kC unit vectors form the upper 3×3
of the matrix, which is a pure rotation matrix because these vectors are of unit
length. We can see this more clearly by considering a simple example, such as
a situation in which child space is rotated by an angle γ about the z-axis, with
no translation. Recall from Equation (4.6) that the matrix for such a rotation is
given by
rotatez(r, γ) =
rx
ry
rz
1


cos γ
sin γ
0
0
−sin γ
cos γ
0
0
0
0
1
0
0
0
0
1

.
But in Figure 4.21, we can see that the coordinates of the iC and jC vectors, ex-
pressed in parent space, are iC =
cos γ
sin γ
0
and jC =
−sin γ
cos γ
0
.
When we plug these vectors into our formula for MC→P , with kC =
0
0
1
,
it exactly matches the matrix rotatez(r, γ) from Equation (4.6).
x
y
γ
γ
γ
γ
γ
γ
C
C
Figure 4.21. Change of basis when child axes are rotated by an angle γ relative to parent.

196
4. 3D Math for Games
Scaling the Child Axes
Scaling of the child coordinate system is accomplished by simply scaling the
unit basis vectors appropriately. For example, if child space is scaled up by a
factor of two, then the basis vectors iC, jC and kC will be of length 2 instead
of unit length.
4.3.10.3
Extracting Unit Basis Vectors from a Matrix
The fact that we can build a change of basis matrix out of a translation and
three Cartesian basis vectors gives us another powerful tool: Given any afﬁne
4 × 4 transformation matrix, we can go in the other direction and extract the
child-space basis vectors iC, jC and kC from it by simply isolating the ap-
propriate rows of the matrix (or columns if your math library uses column
vectors).
This can be incredibly useful. Let’s say we are given a vehicle’s model-to-
world transform as an afﬁne 4×4 matrix (a very common representation). This
is really just a change of basis matrix, transforming points in model space into
their equivalents in world space. Let’s further assume that in our game, the
positive z-axis always points in the direction that an object is facing. So, to ﬁnd
a unit vector representing the vehicle’s facing direction, we can simply extract
kC directly from the model-to-world matrix (by grabbing its third row). This
vector will already be normalized and ready to go.
4.3.10.4
Transforming Coordinate Systems versus Vectors
We’ve said that the matrix MC→P transforms points and directions from child
space into parent space. Recall that the fourth row of MC→P contains tC,
the translation of the child coordinate axes relative to the world-space axes.
Therefore, another way to visualize the matrix MC→P is to imagine it taking
the parent coordinate axes and transforming them into the child axes. This is
the reverse of what happens to points and direction vectors. In other words, if
a matrix transforms vectors from child space to parent space, then it also trans-
forms coordinate axes from parent space to child space. This makes sense when
you think about it—moving a point 20 units to the right with the coordinate
axes ﬁxed is the same as moving the coordinate axes 20 units to the left with
the point ﬁxed. This concept is illustrated in Figure 4.22.
Of course, this is just another point of potential confusion. If you’re think-
ing in terms of coordinate axes, then transformations go in one direction, but
if you’re thinking in terms of points and vectors, they go in the other direction!
As with many confusing things in life, your best bet is probably to choose a

4.3. Matrices
197
x
y
x'
y'
y
x
P'
P
P
Figure 4.22. Two ways to interpret a transformation matrix. On the left, the point moves against
a ﬁxed set of axes. On the right, the axes move in the opposite direction while the point remains
ﬁxed.
single “canonical” way of thinking about things and stick with it. For exam-
ple, in this book we’ve chosen the following conventions:
•
Transformations apply to vectors (not coordinate axes).
•
Vectors are written as rows (not columns).
Taken together, these two conventions allow us to read sequences of ma-
trix multiplications from left to right and have them make sense (e.g., in the
expression rD = rAMA→BMB→CMC→D, the B’s and C’s in effect “cancel
out,” leaving only rD = rAMA→D). Obviously if you start thinking about the
coordinate axes moving around rather than the points and vectors, you either
have to read the transforms from right to left, or ﬂip one of these two conven-
tions around. It doesn’t really matter what conventions you choose as long as
you ﬁnd them easy to remember and work with.
That said, it’s important to note that certain problems are easier to think
about in terms of vectors being transformed, while others are easier to work
with when you imagine the coordinate axes moving around. Once you get
good at thinking about 3D vector and matrix math, you’ll ﬁnd it pretty easy
to ﬂip back and forth between conventions as needed to suit the problem at
hand.
4.3.11
Transforming Normal Vectors
A normal vector is a special kind of vector, because in addition to (usually!)
being of unit length, it carries with it the additional requirement that it should
always remain perpendicular to whatever surface or plane it is associated with.
Special care must be taken when transforming a normal vector to ensure that
both its length and perpendicularity properties are maintained.
In general, if a point or (non-normal) vector can be rotated from space A to
space B via the 3×3 matrix MA→B, then a normal vector n will be transformed

198
4. 3D Math for Games
from space A to space B via the inverse transpose of that matrix, (M−1
A→B)T. We
will not prove or derive this result here (see [28, Section 3.5] for an excellent
derivation). However, we will observe that if the matrix MA→B contains only
uniform scale and no shear, then the angles between all surfaces and vectors
in space B will be the same as they were in space A. In this case, the ma-
trix MA→B will actually work just ﬁne for any vector, normal or non-normal.
However, if MA→B contains nonuniform scale or shear (i.e., is non-orthogonal),
then the angles between surfaces and vectors are not preserved when moving
from space A to space B. A vector that was normal to a surface in space A will
not necessarily be perpendicular to that surface in space B. The inverse trans-
pose operation accounts for this distortion, bringing normal vectors back into
perpendicularity with their surfaces even when the transformation involves
nonuniform scale or shear. Another way of looking at this is that the inverse
transpose is required because a surface normal is really a pseudovector rather
than a regular vector (see Section 4.2.4.9).
4.3.12
Storing Matrices in Memory
In the C and C++ languages, a two-dimensional array is often used to store a
matrix. Recall that in C/C++ two-dimensional array syntax, the ﬁrst subscript
is the row and the second is the column, and the column index varies fastest
as you move through memory sequentially.
float m[4][4]; // [row][col], col varies fastest
// "flatten" the array to demonstrate ordering
float* pm = &m[0][0];
ASSERT( &pm[0] == &m[0][0] );
ASSERT( &pm[1] == &m[0][1] );
ASSERT( &pm[2] == &m[0][2] );
// etc.
We have two choices when storing a matrix in a two-dimensional C/C++
array. We can either
1.
store the vectors (iC, jC, kC, tC) contiguously in memory (i.e., each row
contains a single vector), or
2.
store the vectors strided in memory (i.e., each column contains one vector).
The beneﬁt of approach (1) is that we can address any one of the four
vectors by simply indexing into the matrix and interpreting the four con-
tiguous values we ﬁnd there as a 4-element vector. This layout also has the

4.3. Matrices
199
beneﬁt of matching up exactly with row vector matrix equations (which is an-
other reason why I’ve selected row vector notation for this book). Approach
(2) is sometimes necessary when doing fast matrix-vector multiplies using a
vector-enabled (SIMD) microprocessor, as we’ll see later in this chapter. In
most game engines I’ve personally encountered, matrices are stored using ap-
proach (1), with the vectors in the rows of the two-dimensional C/C++ array.
This is shown below:
float M[4][4];
M[0][0]=ix;
M[0][1]=iy;
M[0][2]=iz;
M[0][3]=0.0f;
M[1][0]=jx;
M[1][1]=jy;
M[1][2]=jz;
M[1][3]=0.0f;
M[2][0]=kx;
M[2][1]=ky;
M[2][2]=kz;
M[2][3]=0.0f;
M[3][0]=tx;
M[3][1]=ty;
M[3][2]=tz;
M[3][3]=1.0f;
The matrix looks like this when viewed in a debugger:
M[][]
[0]
[0] ix
[1] iy
[2] iz
[3] 0.0000
[1]
[0] jx
[1] jy
[2] jz
[3] 0.0000
[2]
[0] kx
[1] ky
[2] kz
[3] 0.0000
[3]
[0] tx
[1] ty
[2] tz
[3] 1.0000
One easy way to determine which layout your engine uses is to ﬁnd a
function that builds a 4 × 4 translation matrix. (Every good 3D math library
provides such a function.) You can then inspect the source code to see where
the elements of the t vector are being stored. If you don’t have access to the
source code of your math library (which is pretty rare in the game industry),
you can always call the function with an easy-to-recognize translation like

200
4. 3D Math for Games
(4, 3, 2), and then inspect the resulting matrix. If row 3 contains the values
4.0f, 3.0f, 2.0f, 1.0f, then the vectors are in the rows, otherwise the vec-
tors are in the columns.
4.4
Quaternions
We’ve seen that a 3 × 3 matrix can be used to represent an arbitrary rotation
in three dimensions. However, a matrix is not always an ideal representation
of a rotation, for a number of reasons:
1.
We need nine ﬂoating-point values to represent a rotation, which seems
excessive considering that we only have three degrees of freedom—
pitch, yaw and roll.
2.
Rotating a vector requires a vector-matrix multiplication, which involves
three dot products, or a total of nine multiplications and six additions.
We would like to ﬁnd a rotational representation that is less expensive
to calculate, if possible.
3.
In games and computer graphics, it’s often important to be able to ﬁnd
rotations that are some percentage of the way between two known rota-
tions. For example, if we are to smoothly animate a camera from some
starting orientation A to some ﬁnal orientation B over the course of a
few seconds, we need to be able to ﬁnd lots of intermediate rotations
between A and B over the course of the animation. It turns out to be dif-
ﬁcult to do this when the A and B orientations are expressed as matrices.
Thankfully, there is a rotational representation that overcomes these three
problems. It is a mathematical object known as a quaternion. A quaternion
looks a lot like a four-dimensional vector, but it behaves quite differently. We
usually write quaternions using non-italic, non-boldface type, like this: q =
qx
qy
qz
qw

.
Quaternions were developed by Sir William Rowan Hamilton in 1843 as
an extension to the complex numbers.
(Speciﬁcally, a quaternion may be
interpreted as a four-dimensional complex number, with a single real axis
and three imaginary axes represented by the imaginary numbers i, j and
k. As such, a quaternion can be written in “complex form” as follows: q =
iqx + jqy + kqz + qw.) Quaternions were ﬁrst used to solve problems in the
area of mechanics. Technically speaking, a quaternion obeys a set of rules
known as a four-dimensional normed division algebra over the real numbers.
Thankfully, we won’t need to understand the details of these rather esoteric

4.4. Quaternions
201
algebraic rules. For our purposes, it will sufﬁce to know that the unit-length
quaternions (i.e., all quaternions obeying the constraint q2
x + q2
y + q2
z + q2
w = 1)
represent three-dimensional rotations.
There are a lot of great papers, web pages and presentations on quater-
nions available on the web for further reading. Here’s one of my favorites:
http://graphics.ucsd.edu/courses/cse169_w05/CSE169_04.ppt.
4.4.1
Unit Quaternions as 3D Rotations
A unit quaternion can be visualized as a three-dimensional vector plus a fourth
scalar coordinate. The vector part qV is the unit axis of rotation, scaled by the
sine of the half-angle of the rotation. The scalar part qS is the cosine of the
half-angle. So the unit quaternion q can be written as follows:
q =
qV
qS

=

a sin θ
2
cos θ
2

,
where a is a unit vector along the axis of rotation, and θ is the angle of rota-
tion. The direction of the rotation follows the right-hand rule, so if your thumb
points in the direction of a, positive rotations will be in the direction of your
curved ﬁngers.
Of course, we can also write q as a simple four-element vector:
q =
qx
qy
qz
qw

,where
qx = qVx = ax sin θ
2,
qy = qVy = ay sin θ
2,
qz = qVz = az sin θ
2,
qw = qS = cos θ
2.
A unit quaternion is very much like an axis+angle representation of a ro-
tation (i.e., a four-element vector of the form
a
θ
). However, quaternions
are more convenient mathematically than their axis+angle counterparts, as we
shall see below.
4.4.2
Quaternion Operations
Quaternions support some of the familiar operations from vector algebra,
such as magnitude and vector addition. However, we must remember that
the sum of two unit quaternions does not represent a 3D rotation, because
such a quaternion would not be of unit length. As a result, you won’t see
any quaternion sums in a game engine, unless they are scaled in some way to
preserve the unit length requirement.

202
4. 3D Math for Games
4.4.2.1
Quaternion Multiplication
One of the most important operations we will perform on quaternions is that
of multiplication. Given two quaternions p and q representing two rotations
P and Q, respectively, the product pq represents the composite rotation (i.e.,
rotation Q followed by rotation P). There are actually quite a few different
kinds of quaternion multiplication, but we’ll restrict this discussion to the va-
riety used in conjunction with 3D rotations, namely the Grassman product.
Using this deﬁnition, the product pq is deﬁned as follows:
pq =
(pSqV + qSpV + pV × qV )
(pSqS −pV · qV )
.
Notice how the Grassman product is deﬁned in terms of a vector part, which
ends up in the x, y and z components of the resultant quaternion, and a scalar
part, which ends up in the w component.
4.4.2.2
Conjugate and Inverse
The inverse of a quaternion q is denoted q−1 and is deﬁned as a quaternion
that, when multiplied by the original, yields the scalar 1 (i.e., qq−1 = 0i + 0j +
0k+1). The quaternion
0
0
0
1
represents a zero rotation (which makes
sense since sin(0) = 0 for the ﬁrst three components, and cos(0) = 1 for the
last component).
In order to calculate the inverse of a quaternion, we must ﬁrst deﬁne a
quantity known as the conjugate. This is usually denoted q∗and it is deﬁned
as follows:
q∗=

−qV
qS

.
In other words, we negate the vector part but leave the scalar part unchanged.
Given this deﬁnition of the quaternion conjugate, the inverse quaternion
q−1 is deﬁned as follows:
q−1 = q∗
|q|2 .
Our quaternions are always of unit length (i.e., |q| = 1), because they represent
3D rotations. So, for our purposes, the inverse and the conjugate are identical:
q−1 = q∗=
−qV
qS

when
|q| = 1.
This fact is incredibly useful, because it means we can always avoid doing
the (relatively expensive) division by the squared magnitude when inverting
a quaternion, as long as we know a priori that the quaternion is normalized.

4.4. Quaternions
203
This also means that inverting a quaternion is generally much faster than in-
verting a 3 × 3 matrix—a fact that you may be able to leverage in some situa-
tions when optimizing your engine.
Conjugate and Inverse of a Product
The conjugate of a quaternion product (pq) is equal to the reverse product of
the conjugates of the individual quaternions:
(pq)∗= q∗p∗.
Likewise, the inverse of a quaternion product is equal to the reverse product
of the inverses of the individual quaternions:
(pq)−1 = q−1p−1.
(4.8)
This is analogous to the reversal that occurs when transposing or inverting
matrix products.
4.4.3
Rotating Vectors with Quaternions
How can we apply a quaternion rotation to a vector? The ﬁrst step is to rewrite
the vector in quaternion form. A vector is a sum involving the unit basis vec-
tors i, j and k. A quaternion is a sum involving i, j and k, but with a fourth
scalar term as well. So it makes sense that a vector can be written as a quater-
nion with its scalar term qS equal to zero. Given the vector v, we can write a
corresponding quaternion v =
v
0
=
vx
vy
vz
0
.
In order to rotate a vector v by a quaternion q, we premultiply the vector
(written in its quaternion form v) by q and then post-multiply it by the inverse
quaternion q−1. Therefore, the rotated vector v′ can be found as follows:
v′ = rotate(q, v) = qvq−1.
This is equivalent to using the quaternion conjugate, because our quaternions
are always unit length:
v′ = rotate(q, v) = qvq∗.
(4.9)
The rotated vector v′ is obtained by simply extracting it from its quaternion
form v′.
Quaternion multiplication can be useful in all sorts of situations in real
games. For example, let’s say that we want to ﬁnd a unit vector describ-
ing the direction in which an aircraft is ﬂying. We’ll further assume that in

204
4. 3D Math for Games
our game, the positive z-axis always points toward the front of an object by
convention. So the forward unit vector of any object in model space is always
FM ≡
0
0
1
by deﬁnition. To transform this vector into world space, we
can simply take our aircraft’s orientation quaternion q and use it with Equa-
tion (4.9) to rotate our model-space vector FM into its world-space equivalent
FW (after converting these vectors into quaternion form, of course):
FW = qFMq−1 = q
0
0
1
0
q−1.
4.4.3.1
Quaternion Concatenation
Rotations can be concatenated in exactly the same way that matrix-based trans-
formations can, by multiplying the quaternions together. For example, con-
sider three distinct rotations, represented by the quaternions q1, q2 and q3,
with matrix equivalents R1, R2 and R3. We want to apply rotation 1 ﬁrst,
followed by rotation 2 and ﬁnally rotation 3. The composite rotation matrix
Rnet can be found and applied to a vector v as follows:
Rnet = R1R2R3;
v′ = vR1R2R3
= vRnet.
Likewise, the composite rotation quaternion qnet can be found and applied to
vector v (in its quaternion form, v) as follows:
qnet = q3q2q1;
v′ = q3q2q1 v q−1
1 q−1
2 q−1
3
= qnet v q−1
net.
Notice how the quaternion product must be performed in an order opposite
to that in which the rotations are applied (q3q2q1). This is because quater-
nion rotations always multiply on both sides of the vector, with the uninverted
quaternions on the left and the inverted quaternions on the right. As we saw
in Equation (4.8), the inverse of a quaternion product is the reverse product of
the individual inverses, so the uninverted quaternions read right-to-left while
the inverted quaternions read left-to-right.
4.4.4
Quaternion-Matrix Equivalence
We can convert any 3D rotation freely between a 3×3 matrix representation R
and a quaternion representation q. If we let q = [qV qS] = [qV x qV y qV z qS]

4.4. Quaternions
205
=
x
y
z
w
, then we can ﬁnd R as follows:
R =


1 −2y2 −2z2
2xy + 2zw
2xz −2yw
2xy −2zw
1 −2x2 −2z2
2yz + 2xw
2xz + 2yw
2yz −2xw
1 −2x2 −2y2

.
Likewise, given R, we can ﬁnd q as follows (where q[0] = qV x, q[1] =
qV y, q[2] = qV z and q[3] = qS). This code assumes that we are using row vec-
tors in C/C++ (i.e., that the rows of matrix correspond to the rows of the ma-
trix R shown above). The code was adapted from a Gamasutra article by Nick
Bobic, published on July 5, 1998, which is available here: http://www.gama
sutra.com/view/feature/3278/rotating_objects_using_quaternions.php. For
a discussion of some even faster methods for converting a matrix to a quater-
nion, leveraging various assumptions about the nature of the matrix, see
http://www.euclideanspace.com/maths/geometry/rotations/conversions/
matrixToQuaternion/index.htm.
void matrixToQuaternion(
const float
R[3][3],
float
q[/*4*/])
{
float trace = R[0][0] + R[1][1] + R[2][2];
// check the diagonal
if (trace > 0.0f)
{
float s = sqrt(trace + 1.0f);
q[3] = s * 0.5f;
float t = 0.5f / s;
q[0] = (R[2][1] - R[1][2]) * t;
q[1] = (R[0][2] - R[2][0]) * t;
q[2] = (R[1][0] - R[0][1]) * t;
}
else
{
// diagonal is negative
int i = 0;
if (R[1][1] > R[0][0]) i = 1;
if (R[2][2] > R[i][i]) i = 2;
static const int NEXT[3] = {1, 2, 0};
int j = NEXT[i];
int k = NEXT[j];
float s = sqrt((R[i][j]

206
4. 3D Math for Games
- (R[j][j] + R[k][k]))
+ 1.0f);
q[i] = s * 0.5f;
float t;
if (s != 0.0)
t = 0.5f / s;
else
t = s;
q[3] = (R[k][j] - R[j][k]) * t;
q[j] = (R[j][i] + R[i][j]) * t;
q[k] = (R[k][i] + R[i][k]) * t;
}
}
Let’s pause for a moment to consider notational conventions. In this book,
we write our quaternions like this: [ x y z w ]. This differs from the [ w x y z ]
convention found in many academic papers on quaternions as an extension of
the complex numbers. Our convention arises from an effort to be consistent
with the common practice of writing homogeneous vectors as [ x
y
z
1 ]
(with the w = 1 at the end). The academic convention arises from the par-
allels between quaternions and complex numbers. Regular two-dimensional
complex numbers are typically written in the form c = a + jb, and the cor-
responding quaternion notation is q = w + ix + jy + kz. So be careful out
there—make sure you know which convention is being used before you dive
into a paper head ﬁrst!
4.4.5
Rotational Linear Interpolation
Rotational interpolation has many applications in the animation, dynamics
and camera systems of a game engine. With the help of quaternions, rotations
can be easily interpolated just as vectors and points can.
The easiest and least computationally intensive approach is to perform a
four-dimensional vector LERP on the quaternions you wish to interpolate.
Given two quaternions qA and qB representing rotations A and B, we can ﬁnd
an intermediate rotation qLERP that is β percent of the way from A to B as
follows:
qLERP = LERP(qA, qB, β) = (1 −β)qA + βqB
|(1 −β)qA + βqB|
= normalize






(1 −β)qAx + βqBx
(1 −β)qAy + βqBy
(1 −β)qAz + βqBz
(1 −β)qAw + βqBw


T


.

4.4. Quaternions
207
qA
qLERP = LERP(qA, qB, 0.4)
qB
Figure 4.23. Linear interpolation (LERP) between two quaternions qA and qB.
Notice that the resultant interpolated quaternion had to be renormalized.
This is necessary because the LERP operation does not preserve a vector’s
length in general.
Geometrically, qLERP = LERP(qA, qB, β) is the quaternion whose orienta-
tion lies β percent of the way from orientation A to orientation B, as shown
(in two dimensions for clarity) in Figure 4.23. Mathematically, the LERP oper-
ation results in a weighed average of the two quaternions, with weights (1 −β)
and β (notice that these two weights sum to 1).
4.4.5.1
Spherical Linear Interpolation
The problem with the LERP operation is that it does not take account of the
fact that quaternions are really points on a four-dimensional hypersphere. A
LERP effectively interpolates along a chord of the hypersphere, rather than
along the surface of the hypersphere itself. This leads to rotation animations
that do not have a constant angular speed when the parameter β is changing
at a constant rate. The rotation will appear slower at the end points and faster
in the middle of the animation.
To solve this problem, we can use a variant of the LERP operation known
as spherical linear interpolation, or SLERP for short. The SLERP operation uses
sines and cosines to interpolate along a great circle of the 4D hypersphere,
rather than along a chord, as shown in Figure 4.24. This results in a constant
angular speed when β varies at a constant rate.
The formula for SLERP is similar to the LERP formula, but the weights
(1 −β) and β are replaced with weights wp and wq involving sines of the
angle between the two quaternions.
SLERP(p, q, β) = wpp + wqq,

208
4. 3D Math for Games
qA
qLERP = LERP(qA, qB, 0.4)
qB
qSLERP = SLERP(qA, qB, 0.4)
0.4 along chord
0.4 along arc
Figure 4.24. Spherical linear interpolation along a great circle arc of a 4D hypersphere.
where
wp = sin(1 −β)θ
sin θ
,
wq = sin βθ
sin θ .
The cosine of the angle between any two unit-length quaternions can be
found by taking their four-dimensional dot product. Once we know cos θ, we
can calculate the angle θ and the various sines we need quite easily:
cos θ = p · q = pxqx + pyqy + pzqz + pwqw;
θ = cos−1(p · q).
4.4.5.2
To SLERP or Not to SLERP (That’s Still the Question)
The jury is still out on whether or not to use SLERP in a game engine. Jonathan
Blow wrote a great article positing that SLERP is too expensive, and LERP’s
quality is not really that bad—therefore, he suggests, we should understand
SLERP but avoid it in our game engines (see http://number-none.com/pro
duct/Understanding%20Slerp,%20Then%20Not%20Using%20It/index.html).
On the other hand, some of my colleagues at Naughty Dog have found that
a good SLERP implementation performs nearly as well as LERP. (For exam-
ple, on the PS3’s SPUs, Naughty Dog’s Ice team’s implementation of SLERP
takes 20 cycles per joint, while its LERP implementation takes 16.25 cycles per
joint.) Therefore, I’d personally recommend that you proﬁle your SLERP and
LERP implementations before making any decisions. If the performance hit
for SLERP isn’t unacceptable, I say go for it, because it may result in slightly

4.5. Comparison of Rotational Representations
209
better-looking animations. But if your SLERP is slow (and you cannot speed
it up, or you just don’t have the time to do so), then LERP is usually good
enough for most purposes.
4.5
Comparison of Rotational Representations
We’ve seen that rotations can be represented in quite a few different ways.
This section summarizes the most common rotational representations and
outlines their pros and cons. No one representation is ideal in all situations.
Using the information in this section, you should be able to select the best
representation for a particular application.
4.5.1
Euler Angles
We brieﬂy explored Euler angles in Section 4.3.9.1. A rotation represented via
Euler angles consists of three scalar values: yaw, pitch and roll. These quanti-
ties are sometimes represented by a 3D vector
θY
θP
θR

.
The beneﬁts of this representation are its simplicity, its small size (three
ﬂoating-point numbers) and its intuitive nature—yaw, pitch and roll are easy
to visualize. You can also easily interpolate simple rotations about a single
axis. For example, it’s trivial to ﬁnd intermediate rotations between two dis-
tinct yaw angles by linearly interpolating the scalar θY . However, Euler angles
cannot be interpolated easily when the rotation is about an arbitrarily oriented
axis.
In addition, Euler angles are prone to a condition known as gimbal lock.
This occurs when a 90-degree rotation causes one of the three principal axes
to “collapse” onto another principal axis. For example, if you rotate by 90
degrees about the x-axis, the y-axis collapses onto the z-axis. This prevents
any further rotations about the original y-axis, because rotations about y and
z have effectively become equivalent.
Another problem with Euler angles is that the order in which the rotations
are performed around each axis matters. The order could be PYR, YPR, RYP
and so on, and each ordering may produce a different composite rotation.
No one standard rotation order exists for Euler angles across all disciplines
(although certain disciplines do follow speciﬁc conventions). So the rotation
angles
θY
θP
θR

do not uniquely deﬁne a particular rotation—you need
to know the rotation order to interpret these numbers properly.
A ﬁnal problem with Euler angles is that they depend upon the mapping
from the x-, y- and z-axes onto the natural front, left/right and up directions
for the object being rotated. For example, yaw is always deﬁned as rotation

210
4. 3D Math for Games
about the up axis, but without additional information we cannot tell whether
this corresponds to a rotation about x, y or z.
4.5.2
3 × 3 Matrices
A 3 × 3 matrix is a convenient and effective rotational representation for a
number of reasons. It does not suffer from gimbal lock, and it can represent
arbitrary rotations uniquely. Rotations can be applied to points and vectors
in a straightforward manner via matrix multiplication (i.e., a series of dot
products). Most CPUs and all GPUs now have built-in support for hardware-
accelerated dot products and matrix multiplication. Rotations can also be re-
versed by ﬁnding an inverse matrix, which for a pure rotation matrix is the
same thing as ﬁnding the transpose—a trivial operation. And 4 × 4 matrices
offer a way to represent arbitrary afﬁne transformations—rotations, transla-
tions and scaling—in a totally consistent way.
However, rotation matrices are not particularly intuitive. Looking at a big
table of numbers doesn’t help one picture the corresponding transformation
in three-dimensional space. Also, rotation matrices are not easily interpolated.
Finally, a rotation matrix takes up a lot of storage (nine ﬂoating-point num-
bers) relative to Euler angles (three ﬂoats).
4.5.3
Axis + Angle
We can represent rotations as a unit vector, deﬁning the axis of rotation plus
a scalar for the angle of rotation. This is known as an axis+angle represen-
tation, and it is sometimes denoted by the four-dimensional vector
a
θ
=
ax
ay
az
θ
, where a is the axis of rotation and θ the angle in radians. In a
right-handed coordinate system, the direction of a positive rotation is deﬁned
by the right-hand rule, while in a left-handed system, we use the left-hand
rule instead.
The beneﬁts of the axis+angle representation are that it is reasonably in-
tuitive and also compact. (It only requires four ﬂoating-point numbers, as
opposed to the nine required for a 3 × 3 matrix.)
One important limitation of the axis+angle representation is that rotations
cannot be easily interpolated. Also, rotations in this format cannot be ap-
plied to points and vectors in a straightforward way—one needs to convert
the axis+angle representation into a matrix or quaternion ﬁrst.
4.5.4
Quaternions
As we’ve seen, a unit-length quaternion can represent 3D rotations in a man-
ner analogous to the axis+angle representation. The primary difference be-

4.5. Comparison of Rotational Representations
211
tween the two representations is that a quaternion’s axis of rotation is scaled
by the sine of the half-angle of rotation, and instead of storing the angle in the
fourth component of the vector, we store the cosine of the half-angle.
The quaternion formulation provides two immense beneﬁts over the axis
+angle representation. First, it permits rotations to be concatenated and ap-
plied directly to points and vectors via quaternion multiplication. Second, it
permits rotations to be easily interpolated via simple LERP or SLERP oper-
ations. Its small size (four ﬂoating-point numbers) is also a beneﬁt over the
matrix formulation.
4.5.5
SQT Transformations
By itself, a quaternion can only represent a rotation, whereas a 4×4 matrix can
represent an arbitrary afﬁne transformation (rotation, translation and scale).
When a quaternion is combined with a translation vector and a scale factor (ei-
ther a scalar for uniform scaling or a vector for nonuniform scaling), then we
have a viable alternative to the 4 × 4 matrix representation of afﬁne transfor-
mations. We sometimes call this an SQT transform, because it contains a scale
factor, a quaternion for rotation and a translation vector.
SQT =
s
q
t
(uniform scale s),
or
SQT =
s
q
t
(nonuniform scale vector s).
SQT transforms are widely used in computer animation because of their
smaller size (eight ﬂoats for uniform scale, or ten ﬂoats for nonuniform scale,
as opposed to the 12 ﬂoating-point numbers needed for a 4 × 3 matrix) and
their ability to be easily interpolated. The translation vector and scale factor
are interpolated via LERP, and the quaternion can be interpolated with either
LERP or SLERP.
4.5.6
Dual Quaternions
A rigid transformation is a transformation involving a rotation and a transla-
tion—a “corkscrew” motion. Such transformations are prevalent in animation
and robotics. A rigid transformation can be represented using a mathematical
object known as a dual quaternion. The dual quaternion representation offers a
number of beneﬁts over the typical vector-quaternion representation. The key
beneﬁt is that linear interpolation blending can be performed in a constant-
speed, shortest-path, coordinate-invariant manner, similar to using LERP for
translation vectors and SLERP for rotational quatnerions (see Section 4.4.5.1),

212
4. 3D Math for Games
but in a way that is easily generalizable to blends involving three or more
transforms.
A dual quaternion is like an ordinary quaternion, except that its four com-
ponents are dual numbers instead of regular real-valued numbers. A dual num-
ber can be written as the sum of a non-dual part and a dual part as follows:
ˆa = a + εb. Here ε is a magical number called the dual unit, deﬁned in such
a way that ε2 = 0 (yet without ε itself being zero). This is analogous to the
imaginary number j = √−1 used when writing a complex number as the sum
of a real and an imaginary part: c = a + jb.
Because each dual number can be represented by two real numbers (the
non-dual and dual parts, a and b), a dual quaternion can be represented by
an eight-element vector. It can also be represented as the sum of two ordinary
quaternions, where the second one is multiplied by the dual unit, as follows:
ˆq = qa + εqb.
A full discussion of dual numbers and dual quaternions is beyond our
scope here. However, the following excellent paper outlines the theory and
practice of using dual quaternions to represent rigid transformations: https://
www.cs.tcd.ie/publications/tech-reports/reports.06/TCD-CS-2006-46.pdf.
Note that in this paper, a dual number is written in the form ˆa = a0 + εaε,
whereas I have used a + εb above to underscore the similarity between dual
numbers and complex numbers.1
4.5.7
Rotations and Degrees of Freedom
The term “degrees of freedom” (or DOF for short) refers to the number of mu-
tually independent ways in which an object’s physical state (position and ori-
entation) can change. You may have encountered the phrase “six degrees of
freedom” in ﬁelds such as mechanics, robotics and aeronautics. This refers
to the fact that a three-dimensional object (whose motion is not artiﬁcially
constrained) has three degrees of freedom in its translation (along the x-, y-
and z-axes) and three degrees of freedom in its rotation (about the x-, y- and
z-axes), for a total of six degrees of freedom.
The DOF concept will help us to understand how different rotational rep-
resentations can employ different numbers of ﬂoating-point parameters, yet
all specify rotations with only three degrees of freedom. For example, Euler
angles require three ﬂoats, but axis+angle and quaternion representations use
1Personally I would have prefered the symbol a1 over a0, so that a dual number would be
written ˆa = (1)a1 + (ε)aε. Just as when we plot a complex number in the complex plane, we can
think of the real unit as a “basis vector” along the real axis, and the dual unit ε as a “basis vector”
along the dual axis.

4.6. Other Useful Mathematical Objects
213
four ﬂoats, and a 3 × 3 matrix takes up nine ﬂoats. How can these representa-
tions all describe 3-DOF rotations?
The answer lies in constraints. All 3D rotational representations employ
three or more ﬂoating-point parameters, but some representations also have
one or more constraints on those parameters. The constraints indicate that the
parameters are not independent—a change to one parameter induces changes
to the other parameters in order to maintain the validity of the constraint(s).
If we subtract the number of constraints from the number of ﬂoating-point
parameters, we arrive at the number of degrees of freedom—and this number
should always be three for a 3D rotation:
NDOF = Nparameters −Nconstraints.
(4.10)
The following list shows Equation (4.10) in action for each of the rotational
representations we’ve encountered in this book.
•
Euler Angles. 3 parameters −0 constraints = 3 DOF.
•
Axis+Angle. 4 parameters −1 constraint = 3 DOF.
Constraint: Axis is constrained to be unit length.
•
Quaternion. 4 parameters −1 constraint = 3 DOF.
Constraint: Quaternion is constrained to be unit length.
•
3 × 3 Matrix. 9 parameters −6 constraints = 3 DOF.
Constraints: All three rows and all three columns must be of unit length
(when treated as three-element vectors).
4.6
Other Useful Mathematical Objects
As game engineers, we will encounter a host of other mathematical objects
in addition to points, vectors, matrices and quaternions. This section brieﬂy
outlines the most common of these.
4.6.1
Lines, Rays and Line Segments
An inﬁnite line can be represented by a point P0 plus a unit vector u in the
direction of the line. A parametric equation of a line traces out every possible
point P along the line by starting at the initial point P0 and moving an arbi-
trary distance t along the direction of the unit vector v. The inﬁnitely large set
of points P becomes a vector function of the scalar parameter t:
P(t) = P0 + t u,
where −∞< t < ∞.
(4.11)
This is depicted in Figure 4.25.

214
4. 3D Math for Games
t = 0
t = 1
t = 2
t = 3
t = –1
0
Figure 4.25. Parametric equation of a line.
0
Figure 4.26. Parametric equation of a ray.
A ray is a line that extends to inﬁnity in only one direction. This is easily
expressed as P(t) with the constraint t ≥0, as shown in Figure 4.26.
A line segment is bounded at both ends by P0 and P1. It too can be repre-
sented by P(t), in either one of the following two ways (where L = P1 −P0,
L = |L| is the length of the line segment, and u = (1/L)L is a unit vector in
the direction of L):
1.
P(t) = P0 + t u, where 0 ≤t ≤L, or
2.
P(t) = P0 + t L, where 0 ≤t ≤1.
The latter format, depicted in Figure 4.27, is particularly convenient be-
cause the parameter t is normalized; in other words, t always goes from zero
to one, no matter which particular line segment we are dealing with. This
means we do not have to store the constraint L in a separate ﬂoating-point
parameter; it is already encoded in the vector L = L u (which we have to
store anyway).
1
0
0
1
Figure 4.27. Parametric equation of a line segment, with normalized parameter t.
4.6.2
Spheres
Spheres are ubiquitous in game engine programming. A sphere is typically
deﬁned as a center point C plus a radius r, as shown in Figure 4.28. This packs
nicely into a four-element vector,
Cx
Cy
Cz
r
. As we’ll see below when
we discuss SIMD vector processing, there are distinct beneﬁts to being able to
pack data into a vector containing four 32-bit ﬂoats (i.e., a 128-bit package).

4.6. Other Useful Mathematical Objects
215
Figure 4.28. Point-radius representation of a sphere.
4.6.3
Planes
A plane is a 2D surface in 3D space. As you may recall from high-school alge-
bra, the equation of a plane is often written as follows:
Ax + By + Cz + D = 0.
This equation is satisﬁed only for the locus of points P =
x
y
z
that lie
on the plane.
Planes can be represented by a point P0 and a unit vector n that is nor-
mal to the plane. This is sometimes called point-normal form, as depicted in
Figure 4.29.
Figure 4.29.
A plane
in point-normal form.
It’s interesting to note that when the parameters A, B and C from the tra-
ditional plane equation are interpreted as a 3D vector, that vector lies in the
direction of the plane normal. If the vector
A
B
C
is normalized to unit
length, then the normalized vector
a
b
c
= n, and the normalized param-
eter d = D/
√
A2 + B2 + C2 is just the distance from the plane to the origin.
The sign of d is positive if the plane’s normal vector n is pointing toward the
origin (i.e., the origin is on the “front” side of the plane) and negative if the
normal is pointing away from the origin (i.e., the origin is “behind” the plane).
Another way of looking at this is that the plane equation and the point-
normal form are really just two ways of writing the same equation. Imagine
testing whether or not an arbitrary point P =
x
y
z
lies on the plane. To
do this, we ﬁnd the signed distance from point P to the origin along the nor-
mal n =
a
b
c
, and if this signed distance is equal to the signed distance
d = −n · P0 from the plane from the origin, then P must lie on the plane. So

216
4. 3D Math for Games
let’s set them equal and expand some terms:
(signed distance P to origin) = (signed distance plane to origin)
n · P = n · P0
n · P −n · P0 = 0
ax + by + cz −n · P0 = 0
ax + by + cz + d = 0.
(4.12)
Equation (4.12) only holds when the point P lies on the plane. But what
happens when the point P does not lie on the plane? In this case, the left-hand
side of the plane equation (ax + by + cz, which is equal to n · P) tells how
far “off” the point is from being on the plane. This expression calculates the
difference between the distance from P to the origin and the distance from the
plane to the origin. In other words, the left-hand side of Equation (4.12) gives
us the perpendicular distance h between the point and the plane! This is just
another way to write Equation (4.2) from Section 4.2.4.7.
h = (P −P0) · n;
h = ax + by + cz + d.
(4.13)
A plane can actually be packed into a four-element vector, much like a
sphere can. To do so, we observe that to describe a plane uniquely, we need
only the normal vector n =
a
b
c
and the distance from the origin d. The
four-element vector L =

n
d

=

a
b
c
d

is a compact and convenient
way to represent and store a plane in memory. Note that when P is written in
homogeneous coordinates with w = 1, the equation (L · P) = 0 is yet another
way of writing (n·P) = −d. These equations are satisﬁed for all points P that
lie on the plane L.
Planes deﬁned in four-element vector form can be easily transformed from
one coordinate space to another. Given a matrix MA→B that transforms points
and (non-normal) vectors from space A to space B, we already know that to
transform a normal vector such as the plane’s n vector, we need to use the in-
verse transpose of that matrix, (M−1
A→B)T. So it shouldn’t be a big surprise to
learn that applying the inverse transpose of a matrix to a four-element plane
vector L will, in fact, correctly transform that plane from space A to space B.
We won’t derive or prove this result any further here, but a thorough expla-
nation of why this little “trick” works is provided in Section 4.2.3 of [28].
4.6.4
Axis-Aligned Bounding Boxes (AABB)
An axis-aligned bounding box (AABB) is a 3D cuboid whose six rectangular
faces are aligned with a particular coordinate frame’s mutually orthogonal

4.6. Other Useful Mathematical Objects
217
axes. As such, an AABB can be represented by a six-element vector containing
the minimum and maximum coordinates along each of the 3 principal axes,
[ xmin, ymin, zmin, xmax, ymax, zmax ], or two points Pmin and Pmax.
This simple representation allows for a particularly convenient and inex-
pensive method of testing whether a point P is inside or outside any given
AABB. We simply test if all of the following conditions are true:
Px ≥xmin and Px ≤xmax and
Py ≥ymin and Py ≤ymax and
Pz ≥zmin and Pz ≤zmax.
Because intersection tests are so speedy, AABBs are often used as an “early
out” collision check; if the AABBs of two objects do not intersect, then there is
no need to do a more detailed (and more expensive) collision test.
4.6.5
Oriented Bounding Boxes (OBB)
An oriented bounding box (OBB) is a cuboid that has been oriented so as to
align in some logical way with the object it bounds. Usually an OBB aligns
with the local-space axes of the object. Hence, it acts like an AABB in local
space, although it may not necessarily align with the world-space axes.
Various techniques exist for testing whether or not a point lies within an
OBB, but one common approach is to transform the point into the OBB’s
“aligned” coordinate system and then use an AABB intersection test as pre-
sented above.
4.6.6
Frusta
Right
Bottom
Figure 4.30. A frustum.
As shown in Figure 4.30, a frustum is a group of six planes that deﬁne a trun-
cated pyramid shape. Frusta are commonplace in 3D rendering because they
conveniently deﬁne the viewable region of the 3D world when rendered via
a perspective projection from the point of view of a virtual camera. Four of
the planes bound the edges of the screen space, while the other two planes
represent the the near and far clipping planes (i.e., they deﬁne the minimum
and maximum z coordinates possible for any visible point).
One convenient representation of a frustum is as an array of six planes,
each of which is represented in point-normal form (i.e., one point and one
normal vector per plane).
Testing whether a point lies inside a frustum is a bit involved, but the basic
idea is to use dot products to determine whether the point lies on the front or
back side of each plane. If it lies inside all six planes, it is inside the frustum.

218
4. 3D Math for Games
A helpful trick is to transform the world-space point being tested by apply-
ing the camera’s perspective projection to it. This takes the point from world
space into a space known as homogeneous clip space. In this space, the frustum
is just an axis-aligned cuboid (AABB). This permits much simpler in/out tests
to be performed.
4.6.7
Convex Polyhedral Regions
A convex polyhedral region is deﬁned by an arbitrary set of planes, all with nor-
mals pointing inward (or outward). The test for whether a point lies inside
or outside the volume deﬁned by the planes is relatively straightforward; it
is similar to a frustum test, but with possibly more planes. Convex regions
are very useful for implementing arbitrarily shaped trigger regions in games.
Many engines employ this technique; for example, the Quake engine’s ubiq-
uitous brushes are just volumes bounded by planes in exactly this way.
4.7
Hardware-Accelerated SIMD Math
SIMD stands for “single instruction multiple data.” This refers to the ability of
most modern microprocessors to perform a single mathematical operation on
multiple data items in parallel, using a single machine instruction. For exam-
ple, the CPU might multiply four pairs of ﬂoating-point numbers in parallel
with a single instruction. SIMD is widely used in game engine math libraries,
because it permits common vector operations such as dot products and matrix
multiplication to be performed extremely rapidly.
Intel ﬁrst introduced MMX instructions with their Pentium line of CPUs
in 1994. These instructions permitted SIMD calculations to be performed on
eight 8-bit integers, four 16-bit integers, or two 32-bit integers packed into
special 64-bit MMX registers. Intel followed this up with various revisions
of an extended instruction set called Streaming SIMD Extensions, or SSE, the
ﬁrst version of which appeared in the Pentium III processor. The SSE instruc-
tion set utilizes 128-bit registers that can contain integer or IEEE ﬂoating-point
data.
The SSE mode most commonly used by game engines is called packed 32-
bit ﬂoating-point mode. In this mode, four 32-bit float values are packed into
a single 128-bit register; four operations such as additions or multiplications
are performed in parallel on four pairs of ﬂoats using a single instruction. This
is just what the doctor ordered when multiplying a four-element vector by a
4 × 4 matrix.

4.7. Hardware-Accelerated SIMD Math
219
x
y
z
w
32 bits
32 bits
32 bits
32 bits
Figure 4.31. The four components of an SSE register in 32-bit ﬂoating-point mode.
4.7.1
SSE Registers
In packed 32-bit ﬂoating-point mode, each 128-bit SSE register contains four
32-bit ﬂoats. The individual ﬂoats within an SSE register are conveniently
referred to as
x
y
z
w
, just as they would be when doing vector/matrix
math in homogeneous coordinates on paper (see Figure 4.31). To see how the
SSE registers work, here’s an example of a SIMD instruction:
addps xmm0, xmm1
The addps instruction adds the four ﬂoats in the 128-bit XMM0 register with
the four ﬂoats in the XMM1 register, and stores the four results back into
XMM0. Put another way,
xmm0.x = xmm0.x + xmm1.x;
xmm0.y = xmm0.y + xmm1.y;
xmm0.z = xmm0.z + xmm1.z;
xmm0.w = xmm0.w + xmm1.w.
The four ﬂoating-point values stored in an SSE register can be extracted to
or loaded from memory or registers individually, but such operations tend to
be comparatively slow. Moving data between the x87 FPU registers and the
SSE registers is particularly bad, because the CPU has to wait for either the x87
or the SSE unit to spit out its pending calculations. This stalls out the CPU’s
entire instruction execution pipeline and results in a lot of wasted cycles. In a
nutshell, code that mixes regular mathematics with SSE mathematics should
be avoided like the plague.
To minimize the costs of going back and forth between memory, x87 FPU
registers, and SSE registers, most SIMD math libraries do their best to leave
data in the SSE registers for as long as possible. This means that even scalar
values are left in SSE registers, rather than being transferred out to float
variables. For example, a dot product between two vectors produces a scalar
result, but if we leave that result in an SSE register, it can be used later in other
vector calculations without incurring a transfer cost. Scalars are represented
by duplicating the single ﬂoating-point value across all four “slots” in an SSE
register. So to store the scalar s in an SSE register, we’d set x = y = z = w = s.

220
4. 3D Math for Games
4.7.2
The __m128 Data Type
Using one of these magic SSE 128-bit values in C or C++ is quite easy. The Mi-
crosoft Visual Studio compiler provides a predeﬁned data type called __m128.
This data type can be used to declare global variables, automatic variables
and even class and structure members. In many cases, variables of this type
will be stored in RAM. But when used in calculations, __m128 values are
manipulated directly in the CPU’s SSE registers. In fact, declaring automatic
variables and function arguments to be of type __m128 often results in the
compiler storing those values directly in SSE registers, rather than keeping
them in RAM on the program stack.
4.7.2.1
Aside: gcc’s vector Types
The GNU C/C++ compiler gcc (used to compile code for the PS3, for example)
provides a whole family of 128-bit vector types that work similarly to __m128
in Visual Studio. These are declared like regular C/C++ types but they are
preceded by the keyword vector. For example, a SIMD variable containing
four floats would be declared vector float. gcc also provides a means
of writing literal SIMD values into your source code. For example, you can
initialize a vector float with a value like this:
vector float v = (vector float)(-1.0f, 2.0f, 0.5f, 1.0f);
The corresponding Visual Studio code is a tad more clunky:
// use compiler intrinsic to load "literal" value
__m128 v = _mm_set_ps(-1.0f, 2.0f, 0.5f, 1.0f);
4.7.2.2
Alignment of __m128 Variables
When an __m128 variable is stored in RAM, it is the programmer’s respon-
sibility to ensure that the variable is aligned to a 16-byte address boundary.
This means that the hexadecimal address of an __m128 variable must always
end in the nibble 0x0. The compiler will automatically pad structures and
classes so that if the entire struct or class is aligned to a 16-byte boundary, all
of the __m128 data members within it will be properly aligned as well. If you
declare an automatic or global struct/class containing one or more __m128s,
the compiler will align the object for you. However, it is still your responsi-
bility to align dynamically allocated data structures (i.e., data allocated with
new or malloc())—the compiler can’t help you there. See Section 5.2.1.3 for
information on aligned memory allocations.

4.7. Hardware-Accelerated SIMD Math
221
4.7.3
Coding with SSE Intrinsics
SSE mathematics can be done in raw assembly language, or via inline assem-
bly in C or C++. However, writing code like this is not only non-portable, it’s
also a big pain in the butt. To make life easier, modern compilers provide in-
trinsics—special commands that look and behave like regular C functions, but
are actually boiled down to inline assembly code by the compiler. Many in-
trinsics translate into a single assembly language instruction, although some
are macros that translate into a sequence of instructions.
In order to use the __m128 data type and SSE intrinsics, your .cpp ﬁle
must #include <xmmintrin.h>.
As an example, let’s take another look at the addps assembly language
instruction.
This instruction can be invoked in C/C++ using the intrinsic
_mm_add_ps(). Here’s a comparison of what the code would look like with
and without the use of the intrinsic.
__m128 addWithAssembly(const __m128 a, const __m128 b)
{
// NOTE: the function args a and b are already in
// xmm0 and xmm1 thanks to the calling convention
__asm addps xmm0, xmm1
// NOTE: a __m128 return value is expected to be
// in xmm0 thanks to the calling convention, so
// we don't need to do anything to return the
// result -- not even a return statement!
}
__m128 addWithIntrinsics(const __m128 a, const __m128 b)
{
return _mm_add_ps(a, b);
}
These two implementations look roughly equivalent at ﬁrst glance. However,
notice how in the assembly language version, we have to use the __asm key-
word to invoke inline assembly instruction, and we rely on some very speciﬁc
knowledge of the compiler’s calling conventions in order to gain access to the
function arguments and the return value. This makes the function a bit more
difﬁcult to write, and the resulting code is not at all portable.
On the other hand, the version using intrinsics involves no inline assembly,
and the SSE assembly language instruction looks just like a regular function
call. This version is much more intuitive and clear, and the source code is
more portable. Moreover, the use of intrinsics provides the compiler with

222
4. 3D Math for Games
additional “meta-knowledge” with which to optimize your code. Once you
bust out the __asm keyword, the compiler can’t make any assumptions and
hence its ability to apply optimizations is restricted.
If you’d like to experiment with these example functions, they can be in-
voked via the following test bed function. Notice the use of two new in-
trinsics: _mm_set_ps() which initializes an __m128 variable (i.e., an SSE
register) with four ﬂoating-point values, and _mm_load_ps(), which loads
values from an in-memory array of floats into an __m128 variable. Also
notice that we are forcing our four global float arrays to be 16-byte aligned
via the __declspec(align(16)) directive—if we omit these directives, the
program will either crash or its performance will be signiﬁcantly reduced, de-
pending on the target hardware.
#include <xmmintrin.h>
// ... function definitions from above ...
void testSSE()
{
__declspec(align(16)) float A[4];
__declspec(align(16)) float B[4]
= { 8.0f, 6.0f, 4.0f, 2.0f };
__declspec(align(16)) float C[4];
__declspec(align(16)) float D[4];
// set a = (1, 2, 3, 4) from literal values, and
// load b = (2, 4, 6, 8) from a floating-point array
// (just to illustrate the two ways of doing this)
// NOTE that B[] is written backwards because Intel
// is little-endian!
__m128 a = _mm_set_ps(2.0f,-1.0f, 3.0f, 4.0f);
__m128 b = _mm_load_ps(&B[0]);
// test the two functions
__m128 c = addWithAssembly(a, b);
__m128 d = addWithIntrinsics(a, b);
// store the original values back so we can print them
_mm_store_ps(&A[0], a);
_mm_store_ps(&B[0], b);
// store results into float arrays so we can print them
_mm_store_ps(&C[0], c);
_mm_store_ps(&D[0], d);

4.7. Hardware-Accelerated SIMD Math
223
// inspect the results (NOTE: looks backwards because
// Intel is little-endian!)
printf("a = %g %g %g %g\n", A[0], A[1], A[2], A[3]);
printf("b = %g %g %g %g\n", B[0], B[1], B[2], B[3]);
printf("c = %g %g %g %g\n", C[0], C[1], C[2], C[3]);
printf("d = %g %g %g %g\n", D[0], D[1], D[2], D[3]);
}
4.7.3.1
Terminology in the SSE Documentation
We should pause here to make an observation about terminology. Microsoft’s
documentation uses the convention
w
x
y
z
when referring to the names
of the individual 32-bit ﬂoats within an SSE register. In this book, we use the
convention
x
y
z
w
. This is just a naming issue—it doesn’t matter what
you call the elements of an SSE register, as long as you’re consistent about how
you interpret each element. Perhaps it is easiest to think of an SSE register r as
containing the elements
r0
r1
r2
r3

.
4.7.4
Vector-Matrix Multiplication with SSE
Let’s take a look at how vector-matrix multiplication might be implemented
using SSE instructions. We want to multiply the 1 × 4 vector v with the 4 × 4
matrix M to generate a result vector r.
r = vM
rx
ry
rz
rw

=
vx
vy
vz
vw



M11
M12
M13
M14
M21
M22
M23
M24
M31
M32
M33
M34
M41
M42
M43
M44


=


(vxM11 + vyM21 + vzM31 + vwM41)
(vxM12 + vyM22 + vzM32 + vwM42)
(vxM13 + vyM23 + vzM33 + vwM43)
(vxM14 + vyM24 + vzM34 + vwM44)


T
.
The multiplication involves taking the dot product of the row vector v with
the columns of matrix M. So, to do this calculation using SSE instructions,
we might ﬁrst try storing v in an SSE register (__m128) and storing each of
the columns of M in SSE registers as well. Then we could calculate all of the
products vkMij in parallel using only four mulps instructions, like this:
__m128 mulVectorMatrixAttempt1(
const __m128& v,
const __m128& Mcol0,
const __m128& Mcol1,

224
4. 3D Math for Games
const __m128& Mcol2,
const __m128& Mcol3)
{
const __m128 vMcol0 = _mm_mul_ps(v, Mcol0);
const __m128 vMcol1 = _mm_mul_ps(v, Mcol1);
const __m128 vMcol2 = _mm_mul_ps(v, Mcol2);
const __m128 vMcol3 = _mm_mul_ps(v, Mcol3);
// ... then what?
}
The above code would yield the following intermediate results:
vMcol1 =
vxM11
vyM21
vzM31
vwM41

;
vMcol2 =
vxM12
vyM22
vzM32
vwM42

;
vMcol3 =
vxM13
vyM23
vzM33
vwM43

;
vMcol4 =
vxM14
vyM24
vzM34
vwM44

.
But the problem with doing it this way is that we now have to add “across
the registers” in order to generate the results we need. For example, rx =
(vxM11 + vyM21 + vzM31 + vwM41), so we’d need to add the four components
of vMcol1 together. Adding across a register like this is inefﬁcient in SSE
(as it is in pretty much every SIMD architecture, including the PS3’s Altivec).
Moreover, it leaves the four components of the result in four separate SSE
registers, which would need to be combined into the single result vector r.
We can do better.
The “trick” here is to multiply with the rows of M, not its columns. That
way, we’ll have results that we can add in parallel, and the ﬁnal sums will
end up in the four components of a single SSE register representing the output
vector r. However, we don’t want to multiply v as-is with the rows of M—we
want to multiply vx with all of row 1, vy with all of row 2, vz with all of row 3
and vw with all of row 4. To do this, we need to replicate a single component of
v, such as vx, across a register to yield a vector like
vx
vx
vx
vx

. Then we
can multiply the replicated component vectors by the appropriate rows of M.
Thankfully there’s a powerful SSE instruction that can replicate values like
this. It is called shufps, and it’s wrapped by the intrinsic _mm_shuffle_
ps(). This beast is a bit complicated to understand, because it’s a general-
purpose instruction that can shufﬂe the components of an SSE register around
in semi-arbitrary ways. However, for our purposes we need only know that
the following macros replicate the x, y, z or w components of a vector across
an entire register:
#define SHUFFLE_PARAM(x, y, z, w) \
((x) | ((y) << 2) | ((z) << 4) | ((w) << 6))

4.7. Hardware-Accelerated SIMD Math
225
#define _mm_replicate_x_ps(v) \
_mm_shuffle_ps((v), (v), SHUFFLE_PARAM(0, 0, 0, 0))
#define _mm_replicate_y_ps(v) \
_mm_shuffle_ps((v), (v), SHUFFLE_PARAM(1, 1, 1, 1))
#define _mm_replicate_z_ps(v) \
_mm_shuffle_ps((v), (v), SHUFFLE_PARAM(2, 2, 2, 2))
#define _mm_replicate_w_ps(v) \
_mm_shuffle_ps((v), (v), SHUFFLE_PARAM(3, 3, 3, 3))
Given these convenient macros, we can write our vector-matrix multipli-
cation function as follows:
__m128 mulVectorMatrixAttempt2(
const __m128& v,
const __m128& Mrow0,
const __m128& Mrow1,
const __m128& Mrow2,
const __m128& Mrow3)
{
const __m128 xxxx = _mm_replicate_x_ps(v);
const __m128 yyyy = _mm_replicate_y_ps(v);
const __m128 zzzz = _mm_replicate_z_ps(v);
const __m128 wwww = _mm_replicate_w_ps(v);
const __m128 xMrow0 = _mm_mul_ps(xxxx, Mrow0);
const __m128 yMrow1 = _mm_mul_ps(yyyy, Mrow1);
const __m128 zMrow2 = _mm_mul_ps(zzzz, Mrow2);
const __m128 wMrow3 = _mm_mul_ps(wwww, Mrow3);
__m128 result = _mm_add_ps(xMrow0, yMrow1);
result
= _mm_add_ps(result, zMrow2);
result
= _mm_add_ps(result, wMrow3);
return result;
}
This code produces the following intermediate vectors:
xMrow1 =
vxM11
vxM12
vxM13
vxM14

;
yMrow2 =
vyM21
vyM22
vyM23
vyM24

;
zMrow3 =
vzM31
vzM32
vzM33
vzM34

;
wMrow4 =
vwM41
vwM42
vwM43
vwM44

.

226
4. 3D Math for Games
Adding these four vectors in parallel produces our result r:
r =


(vxM11 + vyM21 + vzM31 + vwM41)
(vxM12 + vyM22 + vzM32 + vwM42)
(vxM13 + vyM23 + vzM33 + vwM43)
(vxM14 + vyM24 + vzM34 + vwM44)


T
.
On some CPUs, the code shown above can be optimized even further by
using a rather handy multiply-and-add instruction, usually denoted madd. This
instruction multiplies its ﬁrst two arguments and then adds the result to its
third argument. Unfortunately, SSE doesn’t support a madd instruction, but
we can fake it reasonably well with a macro like this:
#define _mm_madd_ps(a, b, c) \
_mm_add_ps(_mm_mul_ps((a), (b)), (c))
__m128 mulVectorMatrixFinal(
const __m128 v,
const __m128 Mrow[4])
{
__m128 result;
result = _mm_mul_ps (_mm_replicate_x_ps(v), Mrow[0]);
result = _mm_madd_ps(_mm_replicate_y_ps(v), Mrow[1],
result);
result = _mm_madd_ps(_mm_replicate_z_ps(v), Mrow[2],
result);
result = _mm_madd_ps(_mm_replicate_w_ps(v), Mrow[3],
result);
return result;
}
We can of course perform 4×4 matrix-matrix multiplication using a similar
approach. When calculating the product P = AB, we treat each row of A as
a vector and multiply it with the rows of B as we did in mulVectorMatrix-
Final(), adding the results of each dot product to produce the correspond-
ing row in the product P.
Check out http://msdn.microsoft.com/en-us/
library/t467de55(v=vs.90).aspx for a full listing of the SSE intrinsics for the
Microsoft Visual Studio compiler.

4.8. Random Number Generation
227
4.8
Random Number Generation
Random numbers are ubiquitous in game engines, so it behooves us to have
a brief look at the two most common random number generators, the linear
congruential generator and the Mersenne Twister. It’s important to realize that
random number generators don’t actually generate random numbers—they
merely produce a complex, but totally deterministic, predeﬁned sequence of
values. For this reason, we call the sequences they produce pseudorandom.
What differentiates a good generator from a bad one is how long the sequence
of numbers is before it repeats (its period), and how well the sequences hold
up under various well-known randomness tests.
4.8.1
Linear Congruential Generators
Linear congruential generators are a very fast and simple way to generate a
sequence of pseudorandom numbers. Depending on the platform, this algo-
rithm is sometimes used in the standard C library’s rand() function. How-
ever, your mileage may vary, so don’t count on rand() being based on any
particular algorithm. If you want to be sure, you’ll be better off implementing
your own random number generator.
The linear congruential algorithm is explained in detail in the book Nu-
merical Recipes in C, so I won’t go into the details of it here.
What I will say is that this random number generator does not produce
particularly high-quality pseudorandom sequences. Given the same initial
seed value, the sequence is always exactly the same. The numbers produced
do not meet many of the criteria widely accepted as desirable, such as a long
period, low- and high-order bits that have similarly long periods, and absence
of sequential or spatial correlation between the generated values.
4.8.2
Mersenne Twister
The Mersenne Twister pseudorandom number generator algorithm was de-
signed speciﬁcally to improve upon the various problems of the linear congru-
ential algorithm. Wikipedia provides the following description of the beneﬁts
of the algorithm:
1.
It was designed to have a colossal period of 219937 −1 (the creators of
the algorithm proved this property). In practice, there is little reason
to use larger ones, as most applications do not require 219937 unique
combinations (219937 ≈4.3 × 106001).

228
4. 3D Math for Games
2.
It has a very high order of dimensional equidistribution. Note that this
means, by default, that there is negligible serial correlation between suc-
cessive values in the output sequence.
3.
It passes numerous tests for statistical randomness, including the strin-
gent Diehard tests.
4.
It is fast.
Various implementations of the Twister are available on the web, includ-
ing a particularly cool one that uses SIMD vector instructions for an extra
speed boost, called SFMT (SIMD-oriented fast Mersenne Twister). SFMT can
be downloaded from http://www.math.sci.hiroshima-u.ac.jp/~m-mat/MT/
SFMT/index.html.
4.8.3
Mother-of-All and Xorshift
In 1994, George Marsaglia, a computer scientist and mathematician best known
for developing the Diehard battery of tests of randomness (http://www.stat.
fsu.edu/pub/diehard), published a pseudorandom number generation algo-
rithm that is much simpler to implement and runs faster than the Mersenne
Twister algorithm. He claimed that it could produce a sequence of 32-bit pseu-
dorandom numbers with a period of non-repetition of 2250. It passed all of the
Diehard tests and still stands today as one of the best pseudorandom number
generators for high-speed applications. He called his algorithm the Mother of
All Pseudorandom Number Generators, because it seemed to him to be the only
random number generator one would ever need.
Later, Marsaglia published another generator called Xorshift, which is be-
tween Mersenne and Mother-of-All in terms of randomness, but runs slightly
faster than Mother.
You can read about George Marsaglia at http://en.wikipedia.org/wiki/
George_Marsaglia, and about the Mother-of-All generator at ftp://ftp.forth.
org/pub/C/mother.c and at http://www.agner.org/random. You can down-
load a PDF of George’s paper on Xorshift at http://www.jstatsoft.org/v08/
i14/paper.

Part II
Low-Level
Engine Systems

This page intentionally left blank
This page intentionally left blank

5
Engine Support Systems
E
very game engine requires some low-level support systems that manage
mundane but crucial tasks, such as starting up and shutting down the en-
gine, conﬁguring engine and game features, managing the engine’s memory
usage, handling access to ﬁle system(s), providing access to the wide range
of heterogeneous asset types used by the game (meshes, textures, animations,
audio, etc.), and providing debugging tools for use by the game development
team. This chapter will focus on the lowest-level support systems found in
most game engines. In the chapters that follow, we will explore some of the
larger core systems, including resource management, human interface devices
and in-game debugging tools.
5.1
Subsystem Start-Up and Shut-Down
A game engine is a complex piece of software consisting of many interacting
subsystems. When the engine ﬁrst starts up, each subsystem must be conﬁg-
ured and initialized in a speciﬁc order. Interdependencies between subsys-
tems implicitly deﬁne the order in which they must be started—i.e., if sub-
system B depends on subsystem A, then A will need to be started up before B
can be initialized. Shut-down typically occurs in the reverse order, so B would
shut down ﬁrst, followed by A.
231

232
5. Engine Support Systems
5.1.1
C++ Static Initialization Order (or Lack Thereof)
Since the programming language used in most modern game engines is C++,
we should brieﬂy consider whether C++’s native start-up and shut-down se-
mantics can be leveraged in order to start up and shut down our engine’s sub-
systems. In C++, global and static objects are constructed before the program’s
entry point (main(), or WinMain() under Windows) is called. However,
these constructors are called in a totally unpredictable order. The destructors
of global and static class instances are called after main() (or WinMain())
returns, and once again they are called in an unpredictable order. Clearly this
behavior is not desirable for initializing and shutting down the subsystems
of a game engine, or indeed any software system that has interdependencies
between its global objects.
This is somewhat unfortunate, because a common design pattern for im-
plementing major subsystems such as the ones that make up a game engine is
to deﬁne a singleton class (often called a manager) for each subsystem. If C++
gave us more control over the order in which global and static class instances
were constructed and destroyed, we could deﬁne our singleton instances as
globals, without the need for dynamic memory allocation. For example, we
could write:
class RenderManager
{
public:
RenderManager()
{
// start up the manager...
}
~RenderManager()
{
// shut down the manager...
}
// ...
};
// singleton instance
static RenderManager gRenderManager;
Alas, with no way to directly control construction and destruction order, this
approach won’t work.
5.1.1.1
Construct On Demand
There is one C++ “trick” we can leverage here. A static variable that is de-
clared within a function will not be constructed before main() is called, but

5.1. Subsystem Start-Up and Shut-Down
233
rather on the ﬁrst invocation of that function. So if our global singleton is
function-static, we can control the order of construction for our global single-
tons.
class RenderManager
{
public:
// Get the one and only instance.
static RenderManager& get()
{
// This function-static will be constructed on the
// first call to this function.
static RenderManager sSingleton;
return sSingleton;
}
RenderManager()
{
// Start up other managers we depend on, by
// calling their get() functions first...
VideoManager::get();
TextureManager::get();
// Now start up the render manager.
// ...
}
~RenderManager()
{
// Shut down the manager.
// ...
}
};
You’ll ﬁnd that many software engineering textbooks suggest this design
or a variant that involves dynamic allocation of the singleton as shown below.
static RenderManager& get()
{
static RenderManager* gpSingleton = NULL;
if (gpSingleton == NULL)
{
gpSingleton = new RenderManager;
}
ASSERT(gpSingleton);
return *gpSingleton;
}

234
5. Engine Support Systems
Unfortunately, this still gives us no way to control destruction order. It
is possible that C++ will destroy one of the managers upon which the
RenderManager depends for its shut-down procedure,
prior to the
RenderManager’s destructor being called. In addition, it’s difﬁcult to predict
exactly when the RenderManager singleton will be constructed, because the
construction will happen on the ﬁrst call to RenderManager::get()—and
who knows when that might be? Moreover, the programmers using the class
may not be expecting an innocuous-looking get() function to do something
expensive, like allocating and initializing a heavyweight singleton. This is an
unpredictable and dangerous design. Therefore, we are prompted to resort to
a more direct approach that gives us greater control.
5.1.2
A Simple Approach That Works
Let’s presume that we want to stick with the idea of singleton managers for
our subsystems. In this case, the simplest “brute-force” approach is to deﬁne
explicit start-up and shut-down functions for each singleton manager class.
These functions take the place of the constructor and destructor, and in fact
we should arrange for the constructor and destructor to do absolutely nothing.
That way, the start-up and shut-down functions can be explicitly called in the
required order from within main() (or from some overarching singleton object
that manages the engine as a whole). For example:
class RenderManager
{
public:
RenderManager()
{
// do nothing
}
~RenderManager()
{
// do nothing
}
void startUp()
{
// start up the manager...
}
void shutDown()
{
// shut down the manager...
}

5.1. Subsystem Start-Up and Shut-Down
235
// ...
};
class PhysicsManager
{ /* similar... */ };
class AnimationManager
{ /* similar... */ };
class MemoryManager
{ /* similar... */ };
class FileSystemManager { /* similar... */ };
// ...
RenderManager
gRenderManager;
PhysicsManager
gPhysicsManager;
AnimationManager
gAnimationManager;
TextureManager
gTextureManager;
VideoManager
gVideoManager;
MemoryManager
gMemoryManager;
FileSystemManager
gFileSystemManager;
// ...
int main(int argc, const char* argv)
{
// Start up engine systems in the correct order.
gMemoryManager.startUp();
gFileSystemManager.startUp();
gVideoManager.startUp();
gTextureManager.startUp();
gRenderManager.startUp();
gAnimationManager.startUp();
gPhysicsManager.startUp();
// ...
// Run the game.
gSimulationManager.run();
// Shut everything down, in reverse order.
// ...
gPhysicsManager.shutDown();
gAnimationManager.shutDown();
gRenderManager.shutDown();
gFileSystemManager.shutDown();
gMemoryManager.shutDown();
return 0;
}

236
5. Engine Support Systems
There are “more elegant” ways to accomplish this. For example, you could
have each manager register itself into a global priority queue and then walk
this queue to start up all the managers in the proper order. You could deﬁne
the manger-to-manager dependency graph by having each manager explicitly
list the other managers upon which it depends and then write some code to
calculate the optimal start-up order given their interdependencies. You could
use the construct-on-demand approach outlined above. In my experience, the
brute-force approach always wins out, because of the following:
•
It’s simple and easy to implement.
•
It’s explicit. You can see and understand the start-up order immediately
by just looking at the code.
•
It’s easy to debug and maintain. If something isn’t starting early enough,
or is starting too early, you can just move one line of code.
One minor disadvantage to the brute-force manual start-up and shut-down
method is that you might accidentally shut things down in an order that isn’t
strictly the reverse of the start-up order. But I wouldn’t lose any sleep over it.
As long as you can start up and shut down your engine’s subsystems success-
fully, you’re golden.
5.1.3
Some Examples from Real Engines
Let’s take a brief look at some examples of engine start-up and shut-down
taken from real game engines.
5.1.3.1
OGRE
OGRE is by its authors’ admission a rendering engine, not a game engine
per se. But by necessity it provides many of the low-level features found in
full-ﬂedged game engines, including a simple and elegant start-up and shut-
down mechanism. Everything in OGRE is controlled by the singleton object
Ogre::Root. It contains pointers to every other subsystem in OGRE and man-
ages their creation and destruction. This makes it very easy for a programmer
to start up OGRE—just new an instance of Ogre::Root and you’re done.
Here are a few excerpts from the OGRE source code so we can see what
it’s doing:
OgreRoot.h
class _OgreExport Root : public Singleton<Root>
{
// <some code omitted...>
// Singletons
LogManager* mLogManager;

5.1. Subsystem Start-Up and Shut-Down
237
ControllerManager* mControllerManager;
SceneManagerEnumerator* mSceneManagerEnum;
SceneManager* mCurrentSceneManager;
DynLibManager* mDynLibManager;
ArchiveManager* mArchiveManager;
MaterialManager* mMaterialManager;
MeshManager* mMeshManager;
ParticleSystemManager* mParticleManager;
SkeletonManager* mSkeletonManager;
OverlayElementFactory* mPanelFactory;
OverlayElementFactory* mBorderPanelFactory;
OverlayElementFactory* mTextAreaFactory;
OverlayManager* mOverlayManager;
FontManager* mFontManager;
ArchiveFactory *mZipArchiveFactory;
ArchiveFactory *mFileSystemArchiveFactory;
ResourceGroupManager* mResourceGroupManager;
ResourceBackgroundQueue* mResourceBackgroundQueue;
ShadowTextureManager* mShadowTextureManager;
// etc.
};
OgreRoot.cpp
Root::Root(const String& pluginFileName,
const String& configFileName,
const String& logFileName) :
mLogManager(0),
mCurrentFrame(0),
mFrameSmoothingTime(0.0f),
mNextMovableObjectTypeFlag(1),
mIsInitialised(false)
{
// superclass will do singleton checking
String msg;
// Init
mActiveRenderer = 0;
mVersion
= StringConverter::toString(OGRE_VERSION_MAJOR)
+ "."
+ StringConverter::toString(OGRE_VERSION_MINOR)
+ "."
+ StringConverter::toString(OGRE_VERSION_PATCH)
+ OGRE_VERSION_SUFFIX + " "
+ "(" + OGRE_VERSION_NAME + ")";
mConfigFileName = configFileName;
// create log manager and default log file if there
// is no log manager yet

238
5. Engine Support Systems
if(LogManager::getSingletonPtr() == 0)
{
mLogManager = new LogManager();
mLogManager->createLog(logFileName, true, true);
}
// dynamic library manager
mDynLibManager = new DynLibManager();
mArchiveManager = new ArchiveManager();
// ResourceGroupManager
mResourceGroupManager = new ResourceGroupManager();
// ResourceBackgroundQueue
mResourceBackgroundQueue
= new ResourceBackgroundQueue();
// and so on...
}
OGRE provides a templated Ogre::Singleton base class from which all
of its singleton (manager) classes derive. If you look at its implementation,
you’ll see that Ogre::Singleton does not use deferred construction but in-
stead relies on Ogre::Root to explicitly new each singleton. As we discussed
above, this is done to ensure that the singletons are created and destroyed in
a well-deﬁned order.
5.1.3.2
Naughty Dog’s Uncharted Series and The Last of Us
The Uncharted/The Last of Us engine created by Naughty Dog, Inc. uses a sim-
ilar explicit technique for starting up its subsystems. You’ll notice by looking
at the following code that engine start-up is not always a simple sequence
of allocating singleton instances. A wide range of operating system services,
third-party libraries and so on must all be started up during engine initial-
ization. Also, dynamic memory allocation is avoided wherever possible, so
many of the singletons are statically allocated objects (e.g., g_fileSystem,
g_languageMgr, etc.) It’s not always pretty, but it gets the job done.
Err BigInit()
{
init_exception_handler();
U8* pPhysicsHeap = new(kAllocGlobal, kAlign16)
U8[ALLOCATION_GLOBAL_PHYS_HEAP];
PhysicsAllocatorInit(pPhysicsHeap,
ALLOCATION_GLOBAL_PHYS_HEAP);
g_textDb.Init();

5.2. Memory Management
239
g_textSubDb.Init();
g_spuMgr.Init();
g_drawScript.InitPlatform();
PlatformUpdate();
thread_t init_thr;
thread_create(&init_thr, threadInit, 0, 30,
64*1024, 0, "Init");
char masterConfigFileName[256];
snprintf(masterConfigFileName,
sizeof(masterConfigFileName),
MASTER_CFG_PATH);
{
Err err = ReadConfigFromFile(
masterConfigFileName);
if (err.Failed())
{
MsgErr("Config file not found (%s).\n",
masterConfigFileName);
}
}
memset(&g_discInfo, 0, sizeof(BootDiscInfo));
int err1 = GetBootDiscInfo(&g_discInfo);
Msg("GetBootDiscInfo() : 0x%x\n", err1);
if(err1 == BOOTDISCINFO_RET_OK)
{
printf("titleId
: [%s]\n",
g_discInfo.titleId);
printf("parentalLevel : [%d]\n",
g_discInfo.parentalLevel);
}
g_fileSystem.Init(g_gameInfo.m_onDisc);
g_languageMgr.Init();
if (g_shouldQuit) return Err::kOK;
// and so on...
5.2
Memory Management
As game developers, we are always trying to make our code run more quickly.
The performance of any piece of software is dictated not only by the algo-
rithms it employs, or the efﬁciency with which those algorithms are coded,

240
5. Engine Support Systems
but also by how the program utilizes memory (RAM). Memory affects perfor-
mance in two ways:
1.
Dynamic memory allocation via malloc() or C++’s global operator
new is a very slow operation. We can improve the performance of our
code by either avoiding dynamic allocation altogether or by making use
of custom memory allocators that greatly reduce allocation costs.
2.
On modern CPUs, the performance of a piece of software is often dom-
inated by its memory access patterns. As we’ll see, data that is located in
small, contiguous blocks of memory can be operated on much more efﬁ-
ciently by the CPU than if that same data were to be spread out across
a wide range of memory addresses. Even the most efﬁcient algorithm,
coded with the utmost care, can be brought to its knees if the data upon
which it operates is not laid out efﬁciently in memory.
In this section, we’ll learn how to optimize our code’s memory utilization
along these two axes.
5.2.1
Optimizing Dynamic Memory Allocation
Dynamic memory allocation via malloc() and free() or C++’s global new
and delete operators—also known as heap allocation—is typically very slow.
The high cost can be attributed to two main factors. First, a heap allocator
is a general-purpose facility, so it must be written to handle any allocation
size, from one byte to one gigabyte. This requires a lot of management over-
head, making the malloc() and free() functions inherently slow. Sec-
ond, on most operating systems a call to malloc() or free() must ﬁrst
context-switch from user mode into kernel mode, process the request and then
context-switch back to the program. These context switches can be extraordi-
narily expensive. One rule of thumb often followed in game development is:
Keep heap allocations to a minimum, and never allocate from the
heap within a tight loop.
Of course, no game engine can entirely avoid dynamic memory allocation,
so most game engines implement one or more custom allocators. A custom
allocator can have better performance characteristics than the operating sys-
tem’s heap allocator for two reasons. First, a custom allocator can satisfy re-
quests from a preallocated memory block (itself allocated using malloc() or
new, or declared as a global variable). This allows it to run in user mode and

5.2. Memory Management
241
entirely avoid the cost of context-switching into the operating system. Second,
by making various assumptions about its usage patterns, a custom allocator
can be much more efﬁcient than a general-purpose heap allocator.
In the following sections, we’ll take a look at some common kinds of cus-
tom allocators. For additional information on this topic, see Christian Gyr-
ling’s excellent blog post, http://www.swedishcoding.com/2008/08/31/
are-we-out-of-memory.
5.2.1.1
Stack-Based Allocators
Many games allocate memory in a stack-like fashion. Whenever a new game
level is loaded, memory is allocated for it. Once the level has been loaded,
little or no dynamic memory allocation takes place.
At the conclusion of
the level, its data is unloaded and all of its memory can be freed. It makes
a lot of sense to use a stack-like data structure for these kinds of memory
allocations.
A stack allocator is very easy to implement. We simply allocate a large con-
tiguous block of memory using malloc() or global new, or by declaring a
global array of bytes (in which case the memory is effectively allocated out of
the executable’s BSS segment). A pointer to the top of the stack is maintained.
All memory addresses below this pointer are considered to be in use, and all
addresses above it are considered to be free. The top pointer is initialized to
the lowest memory address in the stack. Each allocation request simply moves
the pointer up by the requested number of bytes. The most recently allocated
block can be freed by simply moving the top pointer back down by the size of
the block.
It is important to realize that with a stack allocator, memory cannot be
freed in an arbitrary order. All frees must be performed in an order oppo-
site to that in which they were allocated. One simple way to enforce these
restrictions is to disallow individual blocks from being freed at all. Instead,
we can provide a function that rolls the stack top back to a previously marked
location, thereby freeing all blocks between the current top and the roll-back
point.
It’s important to always roll the top pointer back to a point that lies at the
boundary between two allocated blocks, because otherwise new allocations
would overwrite the tail end of the top-most block. To ensure that this is
done properly, a stack allocator often provides a function that returns a marker
representing the current top of the stack. The roll-back function then takes one
of these markers as its argument. This is depicted in Figure 5.1. The interface
of a stack allocator often looks something like this.

242
5. Engine Support Systems
Obtain marker after allocating blocks A and B.
A
B
Allocate additional blocks C, D and E.
A
B
C
D
E
Free back to marker.
A
B
Figure 5.1. Stack allocation and freeing back to a marker.
class StackAllocator
{
public:
// Stack marker: Represents the current top of the
// stack. You can only roll back to a marker, not to
// arbitrary locations within the stack.
typedef U32 Marker;
// Constructs a stack allocator with the given total
// size.
explicit StackAllocator(U32 stackSize_bytes);
// Allocates a new block of the given size from stack
// top.
void* alloc(U32 size_bytes);
// Returns a marker to the current stack top.
Marker getMarker();
// Rolls the stack back to a previous marker.
void freeToMarker(Marker marker);
// Clears the entire stack (rolls the stack back to
// zero).
void clear();

5.2. Memory Management
243
private:
// ...
};
Double-Ended Stack Allocators
A single memory block can actually contain two stack allocators—one that
allocates up from the bottom of the block and one that allocates down from
the top of the block. A double-ended stack allocator is useful because it uses
memory more efﬁciently by allowing a trade-off to occur between the memory
usage of the bottom stack and the memory usage of the top stack. In some sit-
uations, both stacks may use roughly the same amount of memory and meet
in the middle of the block. In other situations, one of the two stacks may eat
up a lot more memory than the other stack, but all allocation requests can still
be satisﬁed as long as the total amount of memory requested is not larger than
the block shared by the two stacks. This is depicted in Figure 5.2.
In Midway’s Hydro Thunder arcade game, all memory allocations are made
from a single large block of memory managed by a double-ended stack allo-
cator. The bottom stack is used for loading and unloading levels (race tracks),
while the top stack is used for temporary memory blocks that are allocated
and freed every frame. This allocation scheme worked extremely well and
ensured that Hydro Thunder never suffered from memory fragmentation prob-
lems (see Section 5.2.1.4). Steve Ranck, Hydro Thunder’s lead engineer, de-
scribes this allocation technique in depth in [6, Section 1.9].
Lower
Upper
Figure 5.2. A double-ended stack allocator.
5.2.1.2
Pool Allocators
It’s quite common in game engine programming (and software engineering in
general) to allocate lots of small blocks of memory, each of which are the same
size. For example, we might want to allocate and free matrices, or iterators, or
links in a linked list, or renderable mesh instances. For this type of memory
allocation pattern, a pool allocator is often the perfect choice.
A pool allocator works by preallocating a large block of memory whose
size is an exact multiple of the size of the elements that will be allocated. For
example, a pool of 4×4 matrices would be an exact multiple of 64 bytes—that’s
16 elements per matrix times four (for 32-bit floats) or eight bytes (for 64-bit

244
5. Engine Support Systems
doubles) per element. Each element within the pool is added to a linked list
of free elements; when the pool is ﬁrst initialized, the free list contains all of the
elements. Whenever an allocation request is made, we simply grab the next
free element off the free list and return it. When an element is freed, we simply
tack it back onto the free list. Both allocations and frees are O(1) operations,
since each involves only a couple of pointer manipulations, no matter how
many elements are currently free. (The notation O(1) is an example of “big
O” notation. In this case it means that the execution time of both allocations
and frees are roughly constant and do not depend on things like the number
of elements currently in the pool. See Section 5.3.3 for an explanation of “big
O” notation.)
The linked list of free elements can be a singly-linked list, meaning that we
need a single pointer (four bytes on 32-bit machines or eight bytes on 64-bit
machines) for each free element. Where should we obtain the memory for all
these pointers? Certainly they could be stored in a separate preallocated mem-
ory block, occupying (sizeof(void*) * numElementsInPool) bytes.
However, this is unduly wasteful.
The key is to realize that the memory
blocks residing on the free list are, by deﬁnition, free memory blocks. So why
not store each free list “next” pointer within the free block itself? This little
“trick” works as long as elementSize >= sizeof(void*). We don’t waste
any memory, because our free list pointers all reside inside the free memory
blocks—in memory that wasn’t being used for anything anyway!
If each element is smaller than a pointer, then we can use pool element in-
dices instead of pointers to implement our linked list. For example, if our pool
contains 16-bit integers, then we can use 16-bit indices as the “next pointers”
in our linked list. This works as long as the pool doesn’t contain more than
216 = 65,536 elements.
5.2.1.3
Aligned Allocations
As we saw in Section 3.2.5.1, every variable and data object has an alignment
requirement. An 8-bit integer variable can be aligned to any address, but a 32-
bit integer or ﬂoating-point variable must be 4-byte aligned, meaning its ad-
dress can only end in the nibbles 0x0, 0x4, 0x8 or 0xC. A 128-bit SIMD vector
value generally has a 16-byte alignment requirement, meaning that its mem-
ory address can end only in the nibble 0x0. On the PS3, memory blocks that
are to be transferred to an SPU via the direct memory access (DMA) controller
should be 128-byte aligned for maximum DMA throughput, meaning they can
only end in the bytes 0x00 or 0x80.
All memory allocators must be capable of returning aligned memory blocks.
This is relatively straightforward to implement. We simply allocate a little bit

5.2. Memory Management
245
more memory than was actually requested, adjust the address of the memory
block upward slightly so that it is aligned properly, and then return the ad-
justed address. Because we allocated a bit more memory than was requested,
the returned block will still be large enough, even with the slight upward ad-
justment.
In most implementations, the number of additional bytes allocated is equal
to the alignment. For example, if the request is for a 16-byte aligned memory
block, we would allocate 16 additional bytes. This allows for the worst-case
address adjustment of 15 bytes, plus one extra byte so that we can use the
same calculations even if the original block is already aligned. This simpliﬁes
and speeds up the code at the expense of one wasted byte per allocation. It’s
also important because, as we’ll see below, we’ll need those extra bytes to
store some additional information that will be used when the block is freed.
We determine the amount by which the block’s address must be adjusted
by masking off the least-signiﬁcant bits of the original block’s memory ad-
dress, subtracting this from the desired alignment, and using the result as the
adjustment offset. The alignment should always be a power of two (4-byte and
16-byte alignments are typical), so to generate the mask we simply subtract
one from the alignment. For example, if the request is for a 16-byte aligned
block, then the mask would be (16−1) = 15 = 0x0000000F. Taking the bitwise
AND of this mask and any misaligned address will yield the amount by which
the address is misaligned. For example, if the originally allocated block’s ad-
dress is 0x50341233, ANDing this address with the mask 0x0000000F yields
0x00000003, so the address is misaligned by three bytes. To align the address,
we add (alignment −misalignment) = (16 −3) = 13 = 0xD bytes to it. The
ﬁnal aligned address is therefore 0x50341233 + 0xD = 0x50341240.
Here’s one possible implementation of an aligned memory allocator:
// Aligned allocation function. IMPORTANT: 'alignment'
// must be a power of 2 (typically 4 or 16).
void* allocateAligned(size_t size_bytes, size_t alignment)
{
ASSERT((alignment & (alignment - 1)) == 0); // pwr of 2
// Determine total amount of memory to allocate.
size_t expandedSize_bytes = size_bytes + alignment;
// Allocate unaligned block & convert address to uintptr_t.
uintptr_t rawAddress = reinterpret_cast<uintptr_t>(
allocateUnaligned(expandedSize_bytes));
// Calculate the adjustment by masking off the lower bits
// of the address, to determine how "misaligned" it is.

246
5. Engine Support Systems
size_t mask = (alignment - 1);
uintptr_t misalignment = (rawAddress & mask);
ptrdiff_t adjustment = alignment - misalignment;
// Calculate the adjusted address, return as pointer.
uintptr_t alignedAddress = rawAddress + adjustment;
return static_cast<void*>(pAlignedMem);
}
When this block is later freed, the code will pass us the adjusted address,
not the original address we allocated. How, then, do we actually free the
memory? We need some way to convert an adjusted address back into the
original, possibly misaligned address.
To accomplish this, we simply store some meta-information in those extra
bytes we allocated in order to align the data in the ﬁrst place. The smallest ad-
justment we might make is one byte. That’s enough room to store the number
of bytes by which the address was adjusted (since it will never be more than
256). We always store this information in the byte immediately preceding
the adjusted address (no matter how many bytes of adjustment we actually
added), so that it is trivial to ﬁnd it again, given the adjusted address. Here’s
how the modiﬁed allocateAligned() function would look. The process
of allocating and freeing aligned blocks is illustrated in Figure 5.3.
// Aligned allocation function. IMPORTANT: 'alignment'
// must be a power of 2 (typically 4 or 16).
void* allocateAligned(size_t size_bytes, size_t alignment)
{
ASSERT(alignment >= 1);
ASSERT(alignment <= 128);
ASSERT((alignment & (alignment - 1)) == 0); // pwr of 2
// Determine total amount of memory to allocate.
size_t expandedSize_bytes = size_bytes + alignment;
// Allocate unaligned block & convert address to uintptr_t.
uintptr_t rawAddress = reinterpret_cast<uintptr_t>(
allocateUnaligned(expandedSize_bytes));
// Calculate the adjustment by masking off the lower bits
// of the address, to determine how "misaligned" it is.
size_t mask = (alignment - 1);
uintptr_t misalignment = (rawAddress & mask);
ptrdiff_t adjustment = alignment - misalignment;
// Calculate the adjusted address.
uintptr_t alignedAddress = rawAddress + adjustment;

5.2. Memory Management
247
// Store the adjustment in the byte immediately
// preceding the adjusted address.
ASSERT(adjustment < 256);
U8* pAlignedMem = reinterpret_cast<U8*>(alignedAddress);
pAlignedMem[-1] = static_cast<U8>(adjustment);
return static_cast<void*>(pAlignedMem);
}
And here’s how the corresponding freeAligned() function would be im-
plemented.
void freeAligned(void* pMem)
{
const U8* pAlignedMem
= reinterpret_cast<const U8*>(pMem);
uintptr_t alignedAddress
= reinterpret_cast<uintptr_t>(pMem);
ptrdiff_t adjustment
= static_cast<ptrdiff_t>(pAlignedMem[-1]);
uintptr_t rawAddress = alignedAddress - adjustment;
void* pRawMem = reinterpret_cast<void*>(rawAddress);
freeUnaligned(pRawMem);
}
Figure 5.3.
Aligned memory allocation with a 16-byte alignment requirement. The difference
between the allocated memory address and the adjusted (aligned) address is stored in the byte
immediately preceding the adjusted address, so that it may be retrieved during free.
5.2.1.4
Single-Frame and Double-Buffered Memory Allocators
Virtually all game engines allocate at least some temporary data during the
game loop. This data is either discarded at the end of each iteration of the loop

248
5. Engine Support Systems
or used on the next frame and then discarded. This allocation pattern is so
common that many engines support single-frame and double-buffered allocators.
Single-Frame Allocators
A single-frame allocator is implemented by reserving a block of memory and
managing it with a simple stack allocator as described above. At the begin-
ning of each frame, the stack’s “top” pointer is cleared to the bottom of the
memory block. Allocations made during the frame grow toward the top of
the block. Rinse and repeat.
StackAllocator g_singleFrameAllocator;
// Main Game Loop
while (true)
{
// Clear the single-frame allocator's buffer every
// frame.
g_singleFrameAllocator.clear();
// ...
// Allocate from the single-frame buffer. We never
// need to free this data! Just be sure to use it
// only this frame.
void* p = g_singleFrameAllocator.alloc(nBytes);
// ...
}
One of the primary beneﬁts of a single-frame allocator is that allocated
memory needn’t ever be freed—we can rely on the fact that the allocator will
be cleared at the start of every frame. Single-frame allocators are also blind-
ingly fast. The one big negative is that using a single-frame allocator requires
a reasonable level of discipline on the part of the programmer. You need to
realize that a memory block allocated out of the single-frame buffer will only
be valid during the current frame. Programmers must never cache a pointer to
a single-frame memory block across the frame boundary!
Double-Buffered Allocators
A double-buffered allocator allows a block of memory allocated on frame i to
be used on frame (i + 1). To accomplish this, we create two single-frame stack
allocators of equal size and then ping-pong between them every frame.

5.2. Memory Management
249
class DoubleBufferedAllocator
{
U32
m_curStack;
StackAllocator m_stack[2];
public:
void swapBuffers()
{
m_curStack = (U32)!m_curStack;
}
void clearCurrentBuffer()
{
m_stack[m_curStack].clear();
}
void* alloc(U32 nBytes)
{
return m_stack[m_curStack].alloc(nBytes);
}
// ...
};
// ...
DoubleBufferedAllocator g_doubleBufAllocator;
// Main Game Loop
while (true)
{
// Clear the single-frame allocator every frame as
// before.
g_singleFrameAllocator.clear();
// Swap the active and inactive buffers of the double-
// buffered allocator.
g_doubleBufAllocator.swapBuffers();
// Now clear the newly active buffer, leaving last
// frame's buffer intact.
g_doubleBufAllocator.clearCurrentBuffer();
// ...

250
5. Engine Support Systems
// Allocate out of the current buffer, without
// disturbing last frame's data. Only use this data
// this frame or next frame. Again, this memory never
// needs to be freed.
void* p = g_doubleBufAllocator.alloc(nBytes);
// ...
}
This kind of allocator is extremely useful for caching the results of asyn-
chronous processing on a multicore game console like the Xbox 360, Xbox One,
PlayStation 3 or PlayStation 4. On frame i, we can kick off an asynchronous
job on one of the PS3’s SPUs, for example, handing it the address of a desti-
nation buffer that has been allocated from our double-buffered allocator. The
job runs and produces its results some time before the end of frame i, storing
them into the buffer we provided. On frame (i + 1), the buffers are swapped.
The results of the job are now in the inactive buffer, so they will not be over-
written by any double-buffered allocations that might be made during this
frame. As long as we use the results of the job before frame (i + 2), our data
won’t be overwritten.
5.2.2
Memory Fragmentation
Another problem with dynamic heap allocations is that memory can become
fragmented over time. When a program ﬁrst runs, its heap memory is entirely
free. When a block is allocated, a contiguous region of heap memory of the
appropriate size is marked as “in use,” and the remainder of the heap remains
free. When a block is freed, it is marked as such, and adjacent free blocks are
merged into a single, larger free block. Over time, as allocations and deallo-
cations of various sizes occur in random order, the heap memory begins to
look like a patchwork of free and used blocks. We can think of the free regions
as “holes” in the fabric of used memory. When the number of holes becomes
large, and/or the holes are all relatively small, we say the memory has become
fragmented. This is illustrated in Figure 5.4.
The problem with memory fragmentation is that allocations may fail even
when there are enough free bytes to satisfy the request. The crux of the prob-
lem is that allocated memory blocks must always be contiguous. For example,
in order to satisfy a request of 128 KiB, there must exist a free “hole” that
is 128 KiB or larger. If there are two holes, each of which is 64 KiB in size,
then enough bytes are available but the allocation fails because they are not
contiguous bytes.

5.2. Memory Management
251
free
After one allocation...
After eight allocations...
After eight allocations and three frees...
After n allocations and m frees...
free
used
Figure 5.4. Memory fragmentation.
Memory fragmentation is not as much of a problem on operating sys-
tems that support virtual memory. A virtual memory system maps discontigu-
ous blocks of physical memory known as pages into a virtual address space, in
which the pages appear to the application to be contiguous. Stale pages can
be swapped to the hard disk when physical memory is in short supply and
reloaded from disk when they are needed. For a detailed discussion of how
virtual memory works, see http://en.wikipedia.org/wiki/Virtual_memory.
Most embedded systems cannot afford to implement a virtual memory sys-
tem. While some modern consoles do technically support it, most console
game engines still do not make use of virtual memory due to the inherent
performance overhead.
5.2.2.1
Avoiding Fragmentation with Stack and Pool Allocators
The detrimental effects of memory fragmentation can be avoided by using
stack and/or pool allocators.

252
5. Engine Support Systems
Single free block, always contiguous
Allocated blocks, always contiguous
deallocation
allocation
Figure 5.5. A stack allocator is free from fragmentation problems.
Allocated and free blocks all the same size
Figure 5.6. A pool allocator is not degraded by fragmentation.
•
A stack allocator is impervious to fragmentation because allocations are
always contiguous, and blocks must be freed in an order opposite to that
in which they were allocated. This is illustrated in Figure 5.5.
•
A pool allocator is also free from fragmentation problems. Pools do be-
come fragmented, but the fragmentation never causes premature out-of-
memory conditions as it does in a general-purpose heap. Pool allocation
requests can never fail due to a lack of a large enough contiguous free
block, because all of the blocks are exactly the same size. This is shown
in Figure 5.6.
5.2.2.2
Defragmentation and Relocation
When differently sized objects are being allocated and freed in a random or-
der, neither a stack-based allocator nor a pool-based allocator can be used. In
such cases, fragmentation can be avoided by periodically defragmenting the
heap. Defragmentation involves coalescing all of the free “holes” in the heap
by shifting allocated blocks from higher memory addresses down to lower
addresses (thereby shifting the holes up to higher addresses). One simple
algorithm is to search for the ﬁrst “hole” and then take the allocated block
immediately above the hole and shift it down to the start of the hole. This has
the effect of “bubbling up” the hole to a higher memory address. If this pro-
cess is repeated, eventually all the allocated blocks will occupy a contiguous
region of memory at the low end of the heap’s address space, and all the holes
will have bubbled up into one big hole at the high end of the heap. This is
illustrated in Figure 5.7.

5.2. Memory Management
253
A
B
C
D
E
A
B
C
D
E
A
B
C
D
E
A
B
C
D
E
A
B
C
D
E
Figure 5.7. Defragmentation by shifting allocated blocks to lower addresses.
The shifting of memory blocks described above is not particularly tricky
to implement. What is tricky is accounting for the fact that we’re moving
allocated blocks of memory around. If anyone has a pointer into one of these
allocated blocks, then moving the block will invalidate the pointer.
The solution to this problem is to patch any and all pointers into a shifted
memory block so that they point to the correct new address after the shift. This
procedure is known as pointer relocation. Unfortunately, there is no general-
purpose way to ﬁnd all the pointers that point into a particular region of mem-
ory. So if we are going to support memory defragmentation in our game en-
gine, programmers must either carefully keep track of all the pointers man-
ually so they can be relocated, or pointers must be abandoned in favor of
something inherently more amenable to relocation, such as smart pointers or
handles.
A smart pointer is a small class that contains a pointer and acts like a
pointer for most intents and purposes. But because a smart pointer is a class,
it can be coded to handle memory relocation properly. One approach is to
arrange for all smart pointers to add themselves to a global linked list. When-
ever a block of memory is shifted within the heap, the linked list of all smart
pointers can be scanned, and each pointer that points into the shifted block of
memory can be adjusted appropriately.
A handle is usually implemented as an index into a non-relocatable ta-
ble, which itself contains the pointers. When an allocated block is shifted in
memory, the handle table can be scanned and all relevant pointers found and
updated automatically. Because the handles are just indices into the pointer
table, their values never change no matter how the memory blocks are shifted,
so the objects that use the handles are never affected by memory relocation.
Another problem with relocation arises when certain memory blocks can-
not be relocated. For example, if you are using a third-party library that does

254
5. Engine Support Systems
not use smart pointers or handles, it’s possible that any pointers into its data
structures will not be relocatable. The best way around this problem is usu-
ally to arrange for the library in question to allocate its memory from a special
buffer outside of the relocatable memory area. The other option is to simply
accept that some blocks will not be relocatable. If the number and size of the
non-relocatable blocks are both small, a relocation system will still perform
quite well.
It is interesting to note that all of Naughty Dog’s engines have supported
defragmentation. Handles are used wherever possible to avoid the need to
relocate pointers. However, in some cases raw pointers cannot be avoided.
These pointers are carefully tracked and relocated manually whenever a mem-
ory block is shifted due to defragmentation. A few of Naughty Dog’s game
object classes are not relocatable for various reasons. However, as mentioned
above, this doesn’t pose any practical problems, because the number of such
objects is always very small, and their sizes are tiny when compared to the
overall size of the relocatable memory area.
Amortizing Defragmentation Costs
Defragmentation can be a slow operation because it involves copying memory
blocks. However, we needn’t fully defragment the heap all at once. Instead,
the cost can be amortized over many frames. We can allow up to N allocated
blocks to be shifted each frame, for some small value of N like 8 or 16. If
our game is running at 30 frames per second, then each frame lasts 1/30 of a
second (33 ms). So, the heap can usually be completely defragmented in less
than one second without having any noticeable effect on the game’s frame
rate. As long as allocations and deallocations aren’t happening at a faster rate
than the defragmentation shifts, the heap will remain mostly defragmented at
all times.
This approach is only valid when the size of each block is relatively small,
so that the time required to move a single block does not exceed the time
allotted to relocation each frame. If very large blocks need to be relocated,
we can often break them up into two or more subblocks, each of which can
be relocated independently. This hasn’t proved to be a problem in Naughty
Dog’s engine, because relocation is only used for dynamic game objects, and
they are never larger than a few kibibytes—and usually much smaller.
5.3
Containers
Game programmers employ a wide variety of collection-oriented data struc-
tures, also known as containers or collections. The job of a container is always

5.3. Containers
255
the same—to house and manage zero or more data elements; however, the de-
tails of how they do this vary greatly, and each type of container has its pros
and cons. Common container data types include, but are certainly not limited
to, the following.
•
Array. An ordered, contiguous collection of elements accessed by in-
dex. The length of the array is usually statically deﬁned at compile time.
It may be multidimensional. C and C++ support these natively (e.g.,
int a[5]).
•
Dynamic array. An array whose length can change dynamically at run-
time (e.g., STL’s std::vector).
•
Linked list. An ordered collection of elements not stored contiguously
in memory but rather linked to one another via pointers (e.g., STL’s
std::list).
•
Stack.
A container that supports the last-in-ﬁrst-out (LIFO) model
for adding and removing elements, also known as push/pop (e.g.,
std::stack).
•
Queue. A container that supports the ﬁrst-in-ﬁrst-out (FIFO) model for
adding and removing elements (e.g., std::queue).
•
Deque. A double-ended queue—supports efﬁcient insertion and removal
at both ends of the array (e.g., std::deque).
•
Tree. A container in which elements are grouped hierarchically. Each
element (node) has zero or one parent and zero or more children. A tree
is a special case of a DAG (see below).
•
Binary search tree (BST). A tree in which each node has at most two chil-
dren, with an order property to keep the nodes sorted by some well-
deﬁned criteria. There are various kinds of binary search trees, includ-
ing red-black trees, splay trees, AVL trees, etc.
•
Binary heap. A binary tree that maintains itself in sorted order, much like
a binary search tree, via two rules: the shape property, which speciﬁes
that the tree must be fully ﬁlled and that the last row of the tree is ﬁlled
from left to right; and the heap property, which states that every node
is, by some user-deﬁned criterion, “greater than” or “equal to” all of its
children.
•
Priority queue. A container that permits elements to be added in any
order and then removed in an order deﬁned by some property of the
elements themselves (i.e., their priority). A priority queue is typically
implemented as a heap (e.g., std::priority_queue), but other im-
plementations are possible. A priority queue is a bit like a list that stays

256
5. Engine Support Systems
sorted at all times, except that a priority queue only supports retrieval of
the highest-priority element, and it is rarely implemented as a list under
the hood.
•
Dictionary. A table of key-value pairs. A value can be “looked up” ef-
ﬁciently given the corresponding key. A dictionary is also known as a
map or hash table, although technically a hash table is just one possible
implementation of a dictionary (e.g., std::map, std::hash_map).
•
Set. A container that guarantees that all elements are unique accord-
ing to some criteria. A set acts like a dictionary with only keys, but no
values.
•
Graph. A collection of nodes connected to one another by unidirectional
or bidirectional pathways in an arbitrary pattern.
•
Directed acyclic graph (DAG). A collection of nodes with unidirectional
(i.e., directed) interconnections, with no cycles (i.e., there is no nonempty
path that starts and ends on the same node).
5.3.1
Container Operations
Game engines that make use of container classes inevitably make use of vari-
ous commonplace algorithms as well. Some examples include:
•
Insert. Add a new element to the container. The new element might be
placed at the beginning of the list, or the end, or in some other location;
or the container might not have a notion of ordering at all.
•
Remove. Remove an element from the container; this may require a ﬁnd
operation (see below). However, if an iterator is available that refers
to the desired element, it may be more efﬁcient to remove the element
using the iterator.
•
Sequential access (iteration). Accessing each element of the container in
some “natural” predeﬁned order.
•
Random access. Accessing elements in the container in an arbitrary order.
•
Find. Search a container for an element that meets a given criterion.
There are all sorts of variants on the ﬁnd operation, including ﬁnding
in reverse, ﬁnding multiple elements, etc. In addition, different types of
data structures and different situations call for different algorithms (see
http://en.wikipedia.org/wiki/Search_algorithm).
•
Sort. Sort the contents of a container according to some given criteria.
There are many different sorting algorithms, including bubble sort, se-
lection sort, insertion sort, quicksort and so on. (See http://en.wikipedia.
org/wiki/Sorting_algorithm for details.)

5.3. Containers
257
5.3.2
Iterators
An iterator is a little class that “knows” how to efﬁciently visit the elements
in a particular kind of container. It acts like an array index or pointer—it
refers to one element in the container at a time, it can be advanced to the next
element, and it provides some sort of mechanism for testing whether or not
all elements in the container have been visited. As an example, the ﬁrst of
the following two code snippets iterates over a C-style array using a pointer,
while the second iterates over an STL linked list using almost identical syntax.
void processArray(int container[], int numElements)
{
int* pBegin = &container[0];
int* pEnd = &container[numElements];
for (int* p = pBegin; p != pEnd; p++)
{
int element = *p;
// process element...
}
}
void processList(std::list<int>& container)
{
std::list<int>::iterator pBegin = container.begin();
std::list<int>::iterator pEnd = container.end();
std::list<inf>::iterator p;
for (p = pBegin; p != pEnd; p++)
{
int element = *p;
// process element...
}
}
The key beneﬁts to using an iterator over attempting to access the con-
tainer’s elements directly are as follows:
•
Direct access would break the container class’ encapsulation. An iter-
ator, on the other hand, is typically a friend of the container class, and
as such it can iterate efﬁciently without exposing any implementation
details to the outside world. (In fact, most good container classes hide
their internal details and cannot be iterated over without an iterator.)
•
An iterator can simplify the process of iterating. Most iterators act like
array indices or pointers, so a simple loop can be written in which the

258
5. Engine Support Systems
iterator is incremented and compared against a terminating condition—
even when the underlying data structure is arbitrarily complex. For ex-
ample, an iterator can make an in-order depth-ﬁrst tree traversal look
no more complex than a simple array iteration.
5.3.2.1
Preincrement versus Postincrement
Notice in the above example that we are using C++’s postincrement operator,
p++, rather than the preincrement operator, ++p. This is a subtle but some-
times important optimization. The preincrement operator increments the con-
tents of the variable before its (now modiﬁed) value is used in the expression.
The postincrement operator increments the contents of the variable after it has
been used. This means that writing ++p introduces a data dependency into your
code—the CPU must wait for the increment operation to be completed before
its value can be used in the expression. On a deeply pipelined CPU, this intro-
duces a stall. On the other hand, with p++ there is no data dependency. The
value of the variable can be used immediately, and the increment operation
can happen later or in parallel with its use. Either way, no stall is introduced
into the pipeline.
Of course, within the “update” expression of a for loop (for(init_expr;
test_expr; update_expr) { ... }), there should be no difference between
pre- and postincrement. This is because any good compiler will recognize that
the value of the variable isn’t used in update_expr. But in cases where the
value is used, postincrement is superior because it doesn’t introduce a stall
in the CPU’s pipeline. Therefore, it’s good to get in the habit of always using
postincrement, unless you absolutely need the semantics of preincrement.
5.3.3
Algorithmic Complexity
The choice of which container type to use for a given application depends
upon the performance and memory characteristics of the container being con-
sidered. For each container type, we can determine the theoretical perfor-
mance of common operations such as insertion, removal, ﬁnd and sort.
We usually indicate the amount of time T that an operation is expected to
take as a function of the number of elements n in the container:
T = f(n).
Rather than try to ﬁnd the exact function f, we concern ourselves only with
ﬁnding the overall order of the function. For example, if the actual theoretical
function were any of the following,
T = 5n2 + 17,

5.3. Containers
259
T = 102n2 + 50n + 12,
T = 1
2n2,
we would, in all cases, simplify the expression down to its most relevant
term—in this case n2. To indicate that we are only stating the order of the
function, not its exact equation, we use “big O” notation and write
T = O(n2).
The order of an algorithm can usually be determined via an inspection
of the pseudocode. If the algorithm’s execution time is not dependent upon
the number of elements in the container at all, we say it is O(1) (i.e., it com-
pletes in constant time). If the algorithm performs a loop over the elements in
the container and visits each element once, such as in a linear search of an
unsorted list, we say the algorithm is O(n). If two loops are nested, each of
which potentially visits each node once, then we say the algorithm is O(n2). If
a divide-and-conquer approach is used, as in a binary search (where half of the
list is eliminated at each step), then we would expect that only ⌊log2(n) + 1⌋
elements will actually be visited by the algorithm in the worst case, and hence
we refer to it as an O(log n) operation. If an algorithm executes a subalgorithm
n times, and the subalgorithm is O(log n), then the resulting algorithm would
be O(n log n).
To select an appropriate container class, we should look at the operations
that we expect to be most common, then select the container whose perfor-
mance characteristics for those operations are most favorable. The most com-
mon orders you’ll encounter are listed here from fastest to slowest: O(1),
O(log n), O(n), O(n log n), O(n2), O(nk) for k > 2.
We should also take the memory layout and usage characteristics of our
containers into account.
For example, an array (e.g., int a[5] or
std::vector) stores its elements contiguously in memory and requires no
overhead storage for anything other than the elements themselves. (Note that
a dynamic array does require a small ﬁxed overhead.) On the other hand, a
linked list (e.g., std::list) wraps each element in a “link” data structure
that contains a pointer to the next element and possibly also a pointer to the
previous element, for a total of up to 16 bytes of overhead per element on a
64-bit machine. Also, the elements in a linked list need not be contiguous in
memory and often aren’t. A contiguous block of memory is usually much
more cache-friendly than a set of disparate memory blocks. Hence, for high-
speed algorithms, arrays are usually better than linked lists in terms of cache
performance (unless the nodes of the linked list are themselves allocated from

260
5. Engine Support Systems
a small, contiguous block of memory). But a linked list is better for situations
in which speed of inserting and removing elements is of prime importance.
5.3.4
Building Custom Container Classes
Many game engines provide their own custom implementations of the com-
mon container data structures. This practice is especially prevalent in console
game engines and games targeted at mobile phone and PDA platforms. The
reasons for building these classes yourself include:
•
Total control. You control the data structure’s memory requirements, the
algorithms used, when and how memory is allocated, etc.
•
Opportunities for optimization. You can optimize your data structures and
algorithms to take advantage of hardware features speciﬁc to the con-
sole(s) you are targeting; or ﬁne-tune them for a particular application
within your engine.
•
Customizability. You can provide custom algorithms not prevalent in
third-party libraries like STL (for example, searching for the n most rel-
evant elements in a container, instead of just the single most relevant).
•
Elimination of external dependencies. Since you built the software yourself,
you are not beholden to any other company or team to maintain it. If
problems arise, they can be debugged and ﬁxed immediately rather than
waiting until the next release of the library (which might not be until
after you have shipped your game!)
•
Control over concurrent data structures. When you write your own con-
tainer classes, you have full control over the means by which they are
protected against concurrent access on a multithreaded or multicore sys-
tem. For example, on the PS4, Naughty Dog uses lightweight “spin
lock” mutexes for the majority of our concurrent data structures, be-
cause they work well with our ﬁber-based job scheduling system. A
third-party container library might not have given us this kind of ﬂexi-
bility.
We cannot cover all possible data structures here, but let’s look at a few
common ways in which game engine programmers tend to tackle containers.
5.3.4.1
To Build or Not to Build
We will not discuss the details of how to implement all of these data types
and algorithms here—a plethora of books and online resources are available
for that purpose. However, we will concern ourselves with the question of
where to obtain implementations of the types and algorithms that you need.
As game engine designers, we have a number of choices:

5.3. Containers
261
1.
Build the needed data structures manually.
2.
Rely on third-party implementations. Some common choices include
a.
the C++ standard template library (STL),
b.
a variant of STL, such as STLport,
c.
the powerful and robust Boost libraries (http://www.boost.org).
Both STL and Boost are attractive, because they provide a rich and power-
ful set of container classes covering pretty much every type of data structure
imaginable. In addition, both of these packages provide a powerful suite of
template-based generic algorithms—implementations of common algorithms,
such as ﬁnding an element in a container, which can be applied to virtually
any type of data object. However, third-party packages like these may not be
appropriate for some kinds of game engines. And even if we decide to use a
third-party package, we must select between Boost and the various ﬂavors of
STL, or another third-party library. So let’s take a moment to investigate some
of the pros and cons of each approach.
STL
The beneﬁts of the standard template library include:
•
STL offers a rich set of features.
•
Reasonably robust implementations are available on a wide variety of
platforms.
•
STL comes “standard” with virtually all C++ compilers.
However, the STL also has numerous drawbacks, including:
•
STL has a steep learning curve. The documentation is now quite good,
but the header ﬁles are cryptic and difﬁcult to understand on most plat-
forms.
•
STL is often slower than a data structure that has been crafted speciﬁ-
cally for a particular problem.
•
STL also almost always eats up more memory than a custom-designed
data structure.
•
STL does a lot of dynamic memory allocation, and it’s sometimes chal-
lenging to control its appetite for memory in a way that is suitable for
high-performance, memory-limited console games.
•
STL’s implementation and behavior varies slightly from compiler to com-
piler, making its use in multiplatform engines more difﬁcult.

262
5. Engine Support Systems
As long as the programmer is aware of the pitfalls of STL and uses it ju-
diciously, it can have a place in game engine programming. It is best suited
to a game engine that will run on a personal computer platform, because the
advanced virtual memory systems on modern PCs make memory allocation
cheaper, and the probability of running out of physical RAM is often negligi-
ble. On the other hand, STL is not generally well-suited for use on memory-
limited consoles that lack advanced CPUs and virtual memory. And code that
uses STL may not port easily to other platforms. Here are some rules of thumb
that I use:
•
First and foremost, be aware of the performance and memory character-
istics of the particular STL class you are using.
•
Try to avoid heavier-weight STL classes in code that you believe will be
a performance bottleneck.
•
Prefer STL in situations where memory is not at a premium. For ex-
ample, embedding a std::list inside a game object is OK, but em-
bedding a std::list inside every vertex of a 3D mesh is probably not
a good idea. Adding every vertex of your 3D mesh to a std::list
is probably also not OK—the std::list class dynamically allocates a
small “link” object for every element inserted into it, and that can result
in a lot of tiny, fragmented memory allocations.
•
If your engine is to be multiplatform, I highly recommend STLport
(http://www.stlport.org), an implementation of STL that was speciﬁ-
cally designed to be portable across a wide range of compilers and tar-
get platforms, more efﬁcient and more feature-rich than the original STL
implementations.
The Medal of Honor: Paciﬁc Assault engine for the PC made heavy use of
STL, and while MOHPA did have its share of frame rate problems, the team
was able to work around the performance problems caused by STL (primar-
ily by carefully limiting and controlling its use). OGRE, the popular object-
oriented rendering library that we use for some of the examples in this book,
also makes heavy use of STL. Your mileage may vary. Using STL on a game
engine project is certainly feasible, but it must be used with utmost care.
Boost
The Boost project was started by members of the C++ Standards Commit-
tee Library Working Group, but it is now an open source project with many
contributors from across the globe. The aim of the project is to produce li-
braries that extend and work together with STL, for both commercial and

5.3. Containers
263
non-commercial use. Many of the Boost libraries have already been included
in the C++ Standards Committee’s Library Technical Report (TR1), which is a
step toward becoming part of a future C++ standard. Here is a brief summary
of what Boost brings to the table:
•
Boost provides a lot of useful facilities not available in STL.
•
In some cases, Boost provides alternatives to work around certain prob-
lems with STL’s design or implementation.
•
Boost does a great job of handling some very complex problems, like
smart pointers. (Bear in mind that smart pointers are complex beasts,
and they can be performance hogs. Handles are usually preferable; see
Section 15.5 for details.)
•
The Boost libraries’ documentation is usually excellent. Not only does
the documentation explain what each library does and how to use it, but
in most cases it also provides an excellent in-depth discussion of the de-
sign decisions, constraints and requirements that went into constructing
the library. As such, reading the Boost documentation is a great way to
learn about the principles of software design.
If you are already using STL, then Boost can serve as an excellent exten-
sion and/or alterative to many of STL’s features. However, be aware of the
following caveats:
•
Most of the core Boost classes are templates, so all that one needs in
order to use them is the appropriate set of header ﬁles. However, some
of the Boost libraries build into rather large .lib ﬁles and may not be
feasible for use in very small-scale game projects.
•
While the worldwide Boost community is an excellent support network,
the Boost libraries come with no guarantees. If you encounter a bug, it
will ultimately be your team’s responsibility to work around it or ﬁx it.
•
The Boost libraries are distributed under the Boost Software License.
Read the license information (http://www.boost.org/more/license_info.
html) carefully to be sure it is right for your engine.
Loki
There is a rather esoteric branch of C++ programming known as template meta-
programming. The core idea is to use the compiler to do a lot of the work that
would otherwise have to be done at runtime by exploiting the template fea-
ture of C++ and in effect “tricking” the compiler into doing things it wasn’t

264
5. Engine Support Systems
originally designed to do. This can lead to some startlingly powerful and use-
ful programming tools.
By far the most well-known and probably most powerful template meta-
programming library for C++ is Loki, a library designed and written by An-
drei Alexandrescu (whose home page is at http://www.erdani.org). The li-
brary can be obtained from SourceForge at http://loki-lib.sourceforge.net.
Loki is extremely powerful; it is a fascinating body of code to study and
learn from. However, its two big weaknesses are of a practical nature: (a) its
code can be daunting to read and use, much less truly understand, and (b)
some of its components are dependent upon exploiting “side-effect” behav-
iors of the compiler that require careful customization in order to be made to
work on new compilers. So Loki can be somewhat tough to use, and it is not as
portable as some of its “less-extreme” counterparts. Loki is not for the faint of
heart. That said, some of Loki’s concepts such as policy-based programming can
be applied to any C++ project, even if you don’t use the Loki library per se. I
highly recommend that all software engineers read Andrei’s ground-breaking
book, Modern C++ Design [2], from which the Loki library was born.
5.3.4.2
Dynamic Arrays and Chunky Allocation
Fixed size C-style arrays are used quite a lot in game programming, because
they require no memory allocation, are contiguous and hence cache-friendly,
and support many common operations such as appending data and searching
very efﬁciently.
When the size of an array cannot be determined a priori, programmers
tend to turn either to linked lists or dynamic arrays. If we wish to maintain
the performance and memory characteristics of ﬁxed-length arrays, then the
dynamic array is often the data structure of choice.
The easiest way to implement a dynamic array is to allocate an n-element
buffer initially and then grow the list only if an attempt is made to add more
than n elements to it. This gives us the favorable characteristics of a ﬁxed
size array but with no upper bound. Growing is implemented by allocating
a new larger buffer, copying the data from the original buffer into the new
buffer, and then freeing the original buffer. The size of the buffer is increased
in some orderly manner, such as adding n to it on each grow, or doubling it
on each grow. Most of the implementations I’ve encountered never shrink the
array, only grow it (with the notable exception of clearing the array to zero
size, which might or might not free the buffer). Hence the size of the array
becomes a sort of “high water mark.” The STL std::vector class works in
this manner.
Of course, if you can establish a high water mark for your data, then you’re
probably better off just allocating a single buffer of that size when the engine

5.3. Containers
265
starts up. Growing a dynamic array can be incredibly costly due to realloca-
tion and data copying costs. The impact of these things depends on the sizes
of the buffers involved. Growing can also lead to fragmentation when dis-
carded buffers are freed. So, as with all data structures that allocate memory,
caution must be exercised when working with dynamic arrays. Dynamic ar-
rays are probably best used during development, when you are as yet unsure
of the buffer sizes you’ll require. They can always be converted into ﬁxed size
arrays once suitable memory budgets have been established.)
5.3.4.3
Linked Lists
If contiguous memory is not a primary concern, but the ability to insert and
remove elements at random is paramount, then a linked list is usually the data
structure of choice. Linked lists are quite easy to implement, but they’re also
quite easy to get wrong if you’re not careful. This section provides a few tips
and tricks for creating robust linked lists.
The Basics of Linked Lists
A linked list is a very simple data structure. Each element in the list has a
pointer to the next element, and, in a doubly-linked list, it also has a pointer
to the previous element. These two pointers are referred to as links. The list
as a whole is tracked using a special pair of pointers called the head and tail
pointers. The head pointer points to the ﬁrst element, while the tail pointer
points to the last element.
Inserting a new element into a doubly-linked list involves adjusting the
next pointer of the previous element and the previous pointer of the next ele-
ment to both point at the new element and then setting the new element’s next
and previous pointers appropriately as well. There are four cases to handle
when adding a node to a linked list:
•
adding the ﬁrst element to a previously empty list;
•
prepending an element before the current head element;
•
appending an element after the current tail element; and
•
inserting an interior element.
These cases are illustrated in Figure 5.8.
Removing an element involves the same kinds of operations in and around
the node being removed. Again there are four cases: removing the head ele-
ment, removing the tail element, removing an interior element, and removing
the last element (emptying the list).

266
5. Engine Support Systems
Head
Tail
Head
Tail
Head
Tail
Head
Tail
Head
Tail
Head
Tail
Head
Tail
Head
Tail
Add First
Prepend
(Push Front)
Insert
Append
(Push Back)
Figure 5.8. The four cases that must be handled when adding an element to a linked list: add ﬁrst,
prepend, append and insert.
The Link Data Structure
Linked list code isn’t particularly tough to write, but it can be error-prone.
As such, it’s usually a good idea to write a general-purpose linked list facility
that can be used to manage lists of any element type. To do this, we need
to separate the data structure that contains the links (i.e., the next and pre-
vious pointers) from the element data structure. The link data structure is
typically a simple struct or class, often called something like Link, Node
or LinkNode, and templated on the type of element to which it refers. It will
usually look something like this.
template< typename ELEMENT >
struct Link
{
Link<ELEMENT>* m_pPrev;
Link<ELEMENT>* m_pNext;
ELEMENT*
m_pElem;
};
Extrusive Lists
An extrusive list is a linked list in which the Link data structures are entirely
separate from the element data structures. Each Link contains a pointer to the
element, as shown in the example. Whenever an element is to be inserted into
a linked list, a link is allocated for it, and the pointers to the element and the

5.3. Containers
267
next and previous links are set up appropriately. When an element is removed
from a linked list, its link can be freed.
The beneﬁt of the extrusive design is that an element can reside in multiple
linked lists simultaneously—all we need is one link per list. The downside is
that the Link objects must be dynamically allocated. Often a pool allocator
(see Section 5.2.1.2) is used to allocate links, because they are always exactly
the same size (viz., 12 bytes on a machine with 32-bit pointers). A pool alloca-
tor is an excellent choice due to its speed and its freedom from fragmentation
problems.
Intrusive Lists
An intrusive list is a linked list in which the Link data structure is embed-
ded in the target element itself. The big beneﬁt of this approach is that we no
longer need to dynamically allocate the links—we get a link “for free” when-
ever we allocate an element. For example, we might have:
class SomeElement
{
Link<SomeElement> m_link;
// other members...
};
We can also derive our element class from class Link. Using inheritance
like this is virtually identical to embedding a Link as the ﬁrst member of the
class, but it has the additional beneﬁt of allowing a pointer to a link
(Link<SomeElement>*) to be down-cast into a pointer to the element it-
self (SomeElement*). This means we can eliminate the back-pointer to the
element that would otherwise have to be embedded within the Link. Here’s
how such a design might be implemented in C++.
template< typename ELEMENT >
struct Link
{
Link<ELEMENT>*
m_pPrev;
Link<ELEMENT>*
m_pNext;
// No ELEMENT* pointer required, thanks to
// inheritance.
};
class SomeElement : public Link<SomeElement>
{
// other members...
};

268
5. Engine Support Systems
The big pitfall of the intrusive linked list design is that it prevents an ele-
ment from residing in more than one linked list at a time (because each ele-
ment has one and only one link). We can allow an element to be a member of
N concurrent lists by providing it with N embedded link instances (in which
case we cannot use the inheritance method). However, the number N must
be ﬁxed a priori, so this approach is still not quite as ﬂexible as the extrusive
design.
The choice between intrusive and extrusive linked lists depends on the
application and the constraints under which you are operating. If dynamic
memory allocation must be avoided at all costs, then an intrusive list is prob-
ably best. If you can afford the overhead of pool allocation, then an extrusive
design may be preferable. Sometimes only one of the two approaches will
be feasible. For example, if we wish to store instances of a class deﬁned by a
third-party library in a linked list and are unable or unwilling to modify that
library’s source code, then an extrusive list is the only option.
Head and Tail Pointers: Circular Lists
To fully implement a linked list, we need to provide a head and a tail pointer.
The simplest approach is to embed these pointers in their own data structure,
perhaps called LinkedList, as follows.
template< typename ELEMENT >
class LinkedList
{
Link<ELEMENT>* m_pTail;
Link<ELEMENT>* m_pHead;
// member functions for manipulating the list...
};
You may have noticed that there isn’t much difference between a
LinkedList and a Link—they both contain a pair of pointers to Link. As
it turns out, there are some distinct beneﬁts to using an instance of class Link
to manage the head and tail of the list, like this:
template< typename ELEMENT >
class LinkedList
{
Link<ELEMENT> m_root;
// contains head and tail
// member functions for manipulating the list...
};

5.3. Containers
269
m_root
Figure 5.9. When the head and tail pointers are stored in a link, the linked list can be made circular,
which simpliﬁes the implementation and has some additional beneﬁts.
The embedded m_root member is a Link, no different from any other Link
in the list (except that its m_pElement member will always be NULL). This al-
lows us to make the linked list circular as shown in Figure 5.9. In other words,
the m_pNext pointer of the last “real” node in the list points to m_root, as
does the m_pPrev pointer of the ﬁrst “real” node in the list.
This design is preferable to the one involving two “loose” pointers for the
head and tail, because it simpliﬁes the logic for inserting and removing ele-
ments. To see why this is the case, consider the code that would be required
to remove an element from a linked list when “loose” head and tail pointers
are being used.
void LinkedList::remove(Link<ELEMENT>& link)
{
if (link.m_pNext)
link.m_pNext->m_pPrev = link.m_pPrev;
else
// removing last element in the list
m_pTail = link.m_pPrev;
if (link.m_pPrev)
link.m_pPrev->m_pNext = link.m_pNext;
else
// removing first element in the list
m_pHead = link.m_pNext;
link.m_pPrev = link.m_pNext = NULL;
}
The code is a bit simpler when we use the m_root design:
void LinkedList::remove(Link<ELEMENT>& link)
{
// the link must currently be a member of the list
ASSERT(link.m_pNext != NULL);
ASSERT(link.m_pPrev != NULL);
link.m_pNext->m_pPrev = link.m_pPrev;

270
5. Engine Support Systems
link.m_pPrev->m_pNext = link.m_pNext;
// indicate the link is no longer in any list
link.m_pPrev = link.m_pNext = NULL;
}
The example code shown above highlights an additional beneﬁt of the cir-
cularly linked list approach: A link’s m_pPrev and m_pNext pointers are
never null, unless the link is not a member of any list (i.e., the link is un-
used/inactive). This gives us a simple test for list membership.
Contrast this with the “loose” head/tail pointer design. In that case, the
m_pPrev pointer of the ﬁrst element in the list is always null, as is the m_pNext
pointer of the last element. And if there is only one element in the list, that
link’s next and previous pointers will both be null. This makes it impossible to
know whether or not a given Link is a member of a list or not.
Singly-Linked Lists
A singly-linked list is one in which the elements have a next pointer but no pre-
vious pointer. (The list as a whole might have both a head and a tail pointer,
or it might have only a head pointer.) Such a design is obviously a memory
saver, but the cost of this approach becomes evident when inserting or re-
moving an element from the list. We have no m_pPrev pointer, so we need
to traverse the list from the head in order to ﬁnd the previous element, so
that its m_pNext pointer can be updated appropriately. Therefore, removal
is an O(1) operation for a doubly-linked list, but it’s an O(n) operation for a
singly-linked list.
This inherent insertion and removal cost is often prohibitive, so most linked
lists are doubly-linked. However, if you know for certain that you will only
ever add and remove elements from the head of the list (as when implement-
ing a stack), or if you always add to the head and remove from the tail (as
with a queue—and your list has both a head and a tail pointer), then you can
get away with a singly-linked list and save yourself some memory.
5.3.4.4
Dictionaries and Hash Tables
A dictionary is a table of key-value pairs. A value in the dictionary can be
looked up quickly, given its key. The keys and values can be of any data type.
This kind of data structure is usually implemented either as a binary search
tree or as a hash table.
In a binary tree implementation, the key-value pairs are stored in the nodes
of the binary tree, and the tree is maintained in key-sorted order. Looking up
a value by key involves performing an O(log n) binary search.

5.3. Containers
271
In a hash table implementation, the values are stored in a ﬁxed size table,
where each slot in the table represents one or more keys. To insert a key-
value pair into a hash table, the key is ﬁrst converted into integer form via a
process known as hashing (if it is not already an integer). Then an index into
the hash table is calculated by taking the hashed key modulo the size of the
table. Finally, the key-value pair is stored in the slot corresponding to that
index. Recall that the modulo operator (% in C/C++) ﬁnds the remainder of
dividing the integer key by the table size. So if the hash table has ﬁve slots,
then a key of 3 would be stored at index 3 (3 % 5 == 3), while a key of 6
would be stored at index 1 (6 % 5 == 1). Finding a key-value pair is an
O(1) operation in the absence of collisions.
Collisions: Open and Closed Hash Tables
Sometimes two or more keys end up occupying the same slot in the hash table.
This is known as a collision. There are two basic ways to resolve a collision,
giving rise to two different kinds of hash tables:
•
Open. In an open hash table (see Figure 5.10), collisions are resolved
by simply storing more than one key-value pair at each index, usually
in the form of a linked list. This approach is easy to implement and
imposes no upper bound on the number of key-value pairs that can be
stored. However, it does require memory to be allocated dynamically
whenever a new key-value pair is added to the table.
•
Closed. In a closed hash table (see Figure 5.11), collisions are resolved via
a process of probing until a vacant slot is found. (“Probing” means ap-
plying a well-deﬁned algorithm to search for a free slot.) This approach
is a bit more difﬁcult to implement, and it imposes an upper limit on
the number of key-value pairs that can reside in the table (because each
Slot 0
Slot 1
Slot 2
Slot 3
Slot 4
(55, apple)
(0, orange)
(26, grape)
(33, plum)
Figure 5.10. An open hash table.

272
5. Engine Support Systems
(55, apple)
(0, orange)
collision!
(33, plum)
(55, apple)
(26, grape)
(33, plum)
(0, orange)
(26, grape)
probe to
find new 
slot
0
1
2
3
4
0
1
2
3
4
Figure 5.11. A closed hash table.
slot can hold only one key-value pair). But the main beneﬁt of this kind
of hash table is that it uses up a ﬁxed amount of memory and requires
no dynamic memory allocation. Therefore, it is often a good choice in a
console engine.
Hashing
Hashing is the process of turning a key of some arbitrary data type into an
integer, which can be used modulo the table size as an index into the table.
Mathematically, given a key k, we want to generate an integer hash value h
using the hash function H and then ﬁnd the index i into the table as follows:
h = H(k),
i = h mod N,
where N is the number of slots in the table, and the symbol mod represents
the modulo operation, i.e., ﬁnding the remainder of the quotient h/N.
If the keys are unique integers, the hash function can be the identity func-
tion, H(k) = k. If the keys are unique 32-bit ﬂoating-point numbers, a hash
function might simply reinterpret the bit pattern of the 32-bit ﬂoat as if it were
a 32-bit integer.
U32 hashFloat(float f)
{
union
{
float m_asFloat;
U32
m_asU32;
} u;
u.m_asFloat = f;
return u.m_asU32;
}

5.3. Containers
273
If the key is a string, we can employ a string hashing function, which combines
the ASCII or UTF codes of all the characters in the string into a single 32-bit
integer value.
The quality of the hashing function H(k) is crucial to the efﬁciency of the
hash table. A “good” hashing function is one that distributes the set of all
valid keys evenly across the table, thereby minimizing the likelihood of colli-
sions. A hash function must also be reasonably quick to calculate, and deter-
ministic in the sense that it must produce the exact same output every time it
is called with an indentical input.
Strings are probably the most prevalent type of key you’ll encounter, so
it’s particularly helpful to know a “good” string hashing function. Here are a
few reasonably good ones:
•
LOOKUP3 by Bob Jenkins (http://burtleburtle.net/bob/c/lookup3.c).
•
Cyclic redundancy check functions, such as CRC-32 (http://en.wikipedia.
org/wiki/Cyclic_redundancy_check).
•
Message-digest algorithm 5 (MD5), a cryptographic hash that yields ex-
cellent results but is quite expensive to calculate (http://en.wikipedia.
org/wiki/MD5).
•
A number of other excellent alternatives can be found in an article by
Paul Hsieh available at http://www.azillionmonkeys.com/qed/hash
.html.
Implementing a Closed Hash Table
In a closed hash table, the key-value pairs are stored directly in the table,
rather than in a linked list at each table entry. This approach allows the pro-
grammer to deﬁne a priori the exact amount of memory that will be used by
the hash table. A problem arises when we encounter a collision—two keys that
end up wanting to be stored in the same slot in the table. To address this, we
use a process known as probing.
The simplest approach is linear probing. Imagine that our hashing function
has yielded a table index of i, but that slot is already occupied; we simply try
slots (i + 1), (i + 2) and so on until an empty slot is found (wrapping around
to the start of the table when i = N). Another variation on linear probing is
to alternate searching forwards and backwards, (i + 1), (i −1), (i + 2), (i −2)
and so on, making sure to modulo the resulting indices into the valid range of
the table.
Linear probing tends to cause key-value pairs to “clump up.” To avoid
these clusters, we can use an algorithm known as quadratic probing. We start
at the occupied table index i and use the sequence of probes ij = (i ± j2) for

274
5. Engine Support Systems
j = 1, 2, 3, . . . . In other words, we try (i + 12), (i −12), (i + 22), (i −22) and so
on, remembering to always modulo the resulting index into the valid range of
the table.
When using closed hashing, it is a good idea to make your table size a
prime number. Using a prime table size in conjunction with quadratic probing
tends to yield the best coverage of the available table slots with minimal clus-
tering. See http://stackoverﬂow.com/questions/1145217/why-should-hash
-functions-use-a-prime-number-modulus for a good discussion of why prime
hash table sizes are preferable.
5.4
Strings
Strings are ubiquitous in almost every software project, and game engines are
no exception. On the surface, the string may seem like a simple, fundamental
data type. But, when you start using strings in your projects, you will quickly
discover a wide range of design issues and constraints, all of which must be
carefully accounted for.
5.4.1
The Problem with Strings
The most fundamental question about strings is how they should be stored
and managed in your program. In C and C++, strings aren’t even an atomic
type—they are implemented as arrays of characters. The variable length of
strings means we either have to hard-code limitations on the sizes of our
strings, or we need to dynamically allocate our string buffers. C++ program-
mers often prefer to use a string class, rather than deal directly with character
arrays. But then, which string class should we use? STL provides a reason-
ably good string class, but if you’ve decided not to use STL, you might be
stuck writing your own.
Another big string-related problem is that of localization—the process of
adapting your software for release in other languages. This is also known as
internationalization, or I18N for short. Any string that you display to the user
in English must be translated into whatever languages you plan to support.
(Strings that are used internally to the program but are never displayed to the
user are exempt from localization, of course.) This not only involves mak-
ing sure that you can represent all the character glyphs of all the languages
you plan to support (via an appropriate set of fonts), but it also means ensur-
ing that your game can handle different text orientations. For example, tra-
ditional Chinese text is oriented vertically instead of horizontally (although
modern Chinese and Japanese is commonly written horizontally and left-to-

5.4. Strings
275
right), and some languages like Hebrew read right-to-left. Your game also
needs to gracefully deal with the possibility that a translated string will be
either much longer or much shorter than its English counterpart.
Finally, it’s important to realize that strings are used internally within a
game engine for things like resource ﬁle names and object ids. For example,
when a game designer lays out a level, it’s highly convenient to permit him
or her to identify the objects in the level using meaningful names, like “Play-
erCamera,” “enemy-tank-01” or “explosionTrigger.”
How our engine deals with these internal strings often has pervasive ram-
iﬁcations on the performance of the game. This is because strings are in-
herently expensive to work with at runtime. Comparing or copying ints
or floats can be accomplished via simple machine language instructions.
On the other hand, comparing strings requires an O(n) scan of the character
arrays using a function like strcmp() (where n is the length of the string).
Copying a string requires an O(n) memory copy, not to mention the possi-
bility of having to dynamically allocate the memory for the copy. During one
project I worked on, we proﬁled our game’s performance only to discover that
strcmp() and strcpy() were the top two most expensive functions! By
eliminating unnecessary string operations and using some of the techniques
outlined in this section, we were able to all but eliminate these functions from
our proﬁle and increase the game’s frame rate signiﬁcantly. (I’ve heard similar
stories from developers at a number of different studios.)
5.4.2
String Classes
String classes can make working with strings much more convenient for the
programmer. However, a string class can have hidden costs that are difﬁcult
to see until the game is proﬁled. For example, passing a string to a function
using a C-style character array is fast because the address of the ﬁrst character
is typically passed in a hardware register. On the other hand, passing a string
object might incur the overhead of one or more copy constructors, if the func-
tion is not declared or used properly. Copying strings might involve dynamic
memory allocation, causing what looks like an innocuous function call to end
up costing literally thousands of machine cycles.
For this reason, in game programming I generally like to avoid string
classes. However, if you feel a strong urge to use a string class, make sure you
pick or implement one that has acceptable runtime performance character-
istics—and be sure all programmers that use it are aware of its costs. Know
your string class: Does it treat all string buffers as read-only? Does it utilize
the copy on write optimization? (See http://en.wikipedia.org/wiki/Copy-on
-write.) In C++11, does it provide a move constructor? As a rule of thumb,

276
5. Engine Support Systems
always pass string objects by reference, never by value (as the latter often in-
curs string-copying costs). Proﬁle your code early and often to ensure that
your string class isn’t becoming a major source of lost frame rate!
One situation in which a specialized string class does seem justiﬁable to me
is when storing and managing ﬁle system paths. Here, a hypothetical Path
class could add signiﬁcant value over a raw C-style character array. For exam-
ple, it might provide functions for extracting the ﬁlename, ﬁle extension or di-
rectory from the path. It might hide operating system differences by automat-
ically converting Windows-style backslashes to UNIX-style forward slashes
or some other operating system’s path separator. Writing a Path class that
provides this kind of functionality in a cross-platform way could be highly
valuable within a game engine context. (See Section 6.1.1.4 for more details
on this topic.)
5.4.3
Unique Identiﬁers
The objects in any virtual game world need to be uniquely identiﬁed in some
way.
For example, in Pac Man we might encounter game objects named
“pac_man,” “blinky,” “pinky,” “inky” and “clyde.” Unique object identiﬁers
allow game designers to keep track of the myriad objects that make up their
game worlds and also permit those objects to be found and operated on at
runtime by the engine. In addition, the assets from which our game objects
are constructed—meshes, materials, textures, audio clips, animations and so
on—all need unique identiﬁers as well.
Strings seem like a natural choice for such identiﬁers. Assets are often
stored in individual ﬁles on disk, so they can usually be identiﬁed uniquely by
their ﬁle paths, which of course are strings. And game objects are created by
game designers, so it is natural for them to assign their objects understandable
string names, rather than have to remember integer object indices, or 64- or
128-bit globally unique identiﬁers (GUIDs). However, the speed with which
comparisons between unique identiﬁers can be made is of paramount impor-
tance in a game, so strcmp() simply doesn’t cut it. We need a way to have
our cake and eat it too—a way to get all the descriptiveness and ﬂexibility of
a string, but with the speed of an integer.
5.4.3.1
Hashed String Ids
One good solution is to hash our strings. As we’ve seen, a hash function maps
a string onto a semi-unique integer. String hash codes can be compared just
like any other integers, so comparisons are fast. If we store the actual strings
in a hash table, then the original string can always be recovered from the hash

5.4. Strings
277
code. This is useful for debugging purposes and to permit hashed strings to
be displayed on-screen or in log ﬁles. Game programmers sometimes use the
term string id to refer to such a hashed string. The Unreal engine uses the term
name instead (implemented by class FName).
As with any hashing system, collisions are a possibility (i.e., two different
strings might end up with the same hash code). However, with a suitable
hash function, we can all but guarantee that collisions will not occur for all
reasonable input strings we might use in our game. After all, a 32-bit hash
code represents more than four billion possible values. So, if our hash func-
tion does a good job of distributing strings evenly throughout this very large
range, we are unlikely to collide. At Naughty Dog, we used a variant of the
CRC-32 algorithm to hash our strings, and we have encountered only a hand-
ful of collisions in over seven years of development on Uncharted and The Last
of Us. And when a collision does occur, ﬁxing it is a simple matter of slightly
altering one of the strings (e.g., append a “2” or a “b” to one of the strings, or
use a totally different but synonymous string).
5.4.3.2
Some Implementation Ideas
Conceptually, it’s easy enough to run a hash function on your strings in or-
der to generate string ids. Practically speaking, however, it’s important to
consider when the hash will be calculated. Most game engines that use string
ids do the hashing at runtime. At Naughty Dog, we permit runtime hashing
of strings, but we also preprocess our source code using a simple utility that
searches for macros of the form SID('any-string') and translates each one
directly into the appropriate hashed integer value. This permits string ids to
be used anywhere that an integer manifest constant can be used, including
the constant case labels of a switch statement. (The result of a function call
that generates a string id at runtime is not a constant, so it cannot be used as
a case label.)
The process of generating a string id from a string is sometimes called in-
terning the string, because in addition to hashing it, the string is typically also
added to a global string table. This allows the original string to be recov-
ered from the hash code later. You may also want your tools to be capable
of hashing strings into string ids. That way, when the tool generates data for
consumption by your engine, the strings will already have been hashed.
The main problem with interning a string is that it is a slow operation. The
hashing function must be run on the string, which can be an expensive propo-
sition, especially when a large number of strings are being interned. In addi-
tion, memory must be allocated for the string, and it must be copied into the
lookup table. As a result (if you are not generating string ids at compile-time),

278
5. Engine Support Systems
it is usually best to intern each string only once and save off the result for later
use. For example, it would be preferable to write code like this because the
latter implementation causes the strings to be unnecessarily re-interned every
time the function f() is called.
static StringId
sid_foo = internString("foo");
static StringId
sid_bar = internString("bar");
// ...
void f(StringId id)
{
if (id == sid_foo)
{
// handle case of id == "foo"
}
else if (id == sid_bar)
{
// handle case of id == "bar"
}
}
The following approach is less efﬁcient:
void f(StringId id)
{
if (id == internString("foo"))
{
// handle case of id == "foo"
}
else if (id == internString("bar"))
{
// handle case of id == "bar"
}
}
Here’s one possible implementation of internString().
stringid.h
typedef U32 StringId;
extern StringId internString(const char* str);
stringid.cpp
static HashTable<StringId, const char*> gStringIdTable;

5.4. Strings
279
StringId internString(const char* str)
{
StringId sid = hashCrc32(str);
HashTable<StringId, const char*>::iterator it
= gStringIdTable.find(sid);
if (it == gStringTable.end())
{
// This string has not yet been added to the
// table. Add it, being sure to copy it in case
// the original was dynamically allocated and
// might later be freed.
gStringTable[sid] = strdup(str);
}
return sid;
}
Another idea employed by the Unreal Engine is to wrap the string id and
a pointer to the corresponding C-style character array in a tiny class. In the
Unreal Engine, this class is called FName.
Using Debug Memory for Strings
When using string ids, the strings themselves are only kept around for human
consumption. When you ship your game, you almost certainly won’t need the
strings—the game itself should only ever use the ids. As such, it’s a good idea
to store your string table in a region of memory that won’t exist in the retail
game. For example, a PS3 development kit has 256 MiB of retail memory, plus
an additional 256 MiB of “debug” memory that is not available on a retail
unit. If we store our strings in debug memory, we needn’t worry about their
impact on the memory footprint of the ﬁnal shipping game. (We just need to
be careful never to write production code that depends on the strings being
available!)
5.4.4
Localization
Localization of a game (or any software project) is a big undertaking. It is
a task best handled by planning for it from day one and accounting for it at
every step of development. However, this is not done as often as we all would
like. Here are some tips that should help you plan your game engine project
for localization. For an in-depth treatment of software localization, see [29].

280
5. Engine Support Systems
5.4.4.1
Unicode
The problem for most English-speaking software developers is that they are
trained from birth (or thereabouts!) to think of strings as arrays of eight-bit
ASCII character codes (i.e., characters following the ANSI standard). ANSI
strings work great for a language with a simple alphabet, like English. But,
they just don’t cut it for languages with complex alphabets containing a great
many more characters, sometimes totally different glyphs than English’s 26
letters. To address the limitations of the ANSI standard, the Unicode character
set system was devised.
The basic idea behind Unicode is to assign every character or glyph from
every language in common use around the globe to a unique hexadecimal
code known as a code point. When storing a string of characters in memory,
we select a particular encoding—a speciﬁc means of representing the Unicode
code points for each character—and following those rules, we lay down a
sequence of bits in memory that represent the string. UTF-8 and UTF-16 are
two common encodings. You should select the speciﬁc encoding standard that
best suits your needs.
Please set down this book right now and read the article entitled, “The
Absolute Minimum Every Software Developer Absolutely, Positively Must
Know About Unicode and Character Sets (No Excuses!)” by Joel Spolsky.
You can ﬁnd it here: http://www.joelonsoftware.com/articles/Unicode.html.
(Once you’ve done that, please pick up the book again!)
UTF-32
The simplest Unicode encoding is UTF-32. In this encoding, each Unicode
code point is encoded into a 32-bit (4-byte) value. This encoding wastes a lot
of space, for two reasons: First, most strings in Western European languages
do not use any of the highest-valued code points, so an average of at least
16 bits (2 bytes) is usually wasted per character. Second, the highest Unicode
code point is 0x10FFFF, so even if we wanted to create a string that uses every
possible Unicode glyph, we’d still only need 21 bits per character, not 32.
That said, UTF-32 does have simplicity in its favor. It is a ﬁxed-length en-
coding, meaning that every character occupies the same number of bits in
memory (32 bits to be precise). As such, we can determine the length of any
UTF-32 string by taking its length in bytes and dividing by four.
UTF-8
In the UTF-8 encoding scheme, the code points for each character in a string
are stored using eight-bit (one-byte) granularity, but some code points occupy

5.4. Strings
281
more than one byte. Hence the number of bytes occupied by a UTF-8 character
string is not necessarily the length of the string in characters. This is known
as a variable-length encoding, or a multibyte character set (MBCS), because each
character in a string may take one or more bytes of storage.
One of the big beneﬁts of the UTF-8 encoding is that it is backwards-
compatible with the ANSI encoding. This works because the ﬁrst 127 Uni-
code code points correspond numerically to the old ANSI character codes.
This means that every ANSI character will be represented by exactly one byte
in UTF-8, and a string of ANSI characters can be interpreted as a UTF-8 string
without modiﬁcation.
To represent higher-valued code points, the UTF-8 standard uses multi-
byte characters.
Each multibyte character starts with a byte whose most-
signiﬁcant bit is 1 (i.e., its value lies in the range 128–255, inclusive). Such
high-valued bytes will never appear in an ANSI string, so there is no am-
biguity when distinguishing between single-byte characters and multibyte
characters.
UTF-16
The UTF-16 encoding employs a somewhat simpler, albeit more expensive
approach. Each character in a UTF-16 string is represented by either one or
two 16-bit values. The UTF-16 encoding is known as a wide character set
(WCS) because each character is at least 16 bits wide, instead of the eight bits
used by “regular” ANSI chars and their UTF-8 counterparts.
In UTF-16, the set of all possible Unicode code points is divided into 17
planes containing 216 code points each. The ﬁrst plane is known as the ba-
sic multilingual plane (BMP). It contains the most commonly used code points
across a wide range of languages. As such, many UTF-16 strings can be repre-
sented entirely by code points within the ﬁrst plane, meaning that each char-
acter in such a string is represented by only one 16-bit value. However, if a
character from one of the other planes (known as supplementary planes) is re-
quired in a string, it is represented by two consecutive 16-bit values.
The UCS-2 (2-byte universal character set) encoding is a limited subset
of the UTF-16 encoding, utilizing only the basic multilingual page. As such,
it cannot represent characters whose Unicode code points are numerically
higher than 0xFFFF. This simpliﬁes the format, because every character is
guaranteed to occupy exactly 16 bits (two bytes).
In other words, UCS-2
is a ﬁxed-length character encoding, while in general UTF-8 and UTF-16 are
variable-length encodings.
If we know a priori that a UTF-16 string only utilizes code points from the
BMP (or if we are dealing with a UCS-2 encoded string), we can determine

282
5. Engine Support Systems
the number of characters in the string by simply dividing the number of bytes
by two. Of course, if supplemental planes are used in a UTF-16 string, this
simple “trick” no longer works.
Note that a UTF-16 encoding can be little-endian or big-endian (see Section
3.2.1.6), depending on the native endianness of your target CPU. When stor-
ing UTF-16 text on-disc, it’s common to precede the text data with a byte order
mark (BOM) indicating whether the individual 16-bit characters are stored in
little- or big-endian format. (This is true of UTF-32 encoded string data as
well, of course.)
5.4.4.2
char versus wchar_t
The standard C/C++ library deﬁnes two data types for dealing with character
strings—char and wchar_t. The char type is intended for use with legacy
ANSI strings and with multibyte character sets (MBCS), including (but not
limited to) UTF-8. The wchar_t type is a “wide” character type, intended to
be capable of representing any valid code point in a single integer. As such,
its size is compiler- and system-speciﬁc. It could be eight bits on a system
that does not support Unicode at all. It could be 16 bits if the UCS-2 encoding
is assumed for all wide characters, or if a multi-word encoding like UTF-16
is being employed. Or it could be 32 bits if UTF-32 is the “wide” character
encoding of choice.
Because of this inherent ambiguity in the deﬁnition of wchar_t, if you
need to write truly portable string-handling code, you’ll need to deﬁne your
own character data type(s) and provide a library of functions for dealing with
whatever Unicode encoding(s) you need to support. However, if you are tar-
geting a speciﬁc platform and compiler, you can write your code within the
limits of that particular implementation, at the loss of some portability.
The following article does a good job of outlining the pros and cons of
using the wchar_t data type: http://icu-project.org/docs/papers/unicode_
wchar_t.html.
5.4.4.3
Unicode Under Windows
Under Windows, the wchar_t data type is used exclusively for UTF-16 en-
coded Unicode strings, and the char type is used for ANSI strings and legacy
Windows code page string encodings. When reading the Windows API docs,
the term “Unicode” is therefore always synonymous with “wide character
set” (WCS) and UTF-16 encoding. This is a bit confusing, because of course
Unicode strings can in general be encoded in the “non-wide” multibyte UTF-8
format.

5.4. Strings
283
ANSI
WCS
MBCS
strcmp()
wcscmp()
_mbscmp()
strcpy()
wcscpy()
_mbscpy()
strlen()
wcslen()
_mbstrlen()
Table 5.1. Variants of some common standard C library string functions for use with ANSI, wide
and multibyte character sets.
The Windows API deﬁnes three sets of character/string manipulation func-
tions: one set for single-byte character set ANSI strings (SBCS), one set for
multibyte character set (MBCS) strings, and one set for wide character set
strings. The ANSI functions are essentially the old-school “C-style” string
functions we all grew up with. The MBCS string functions handle a variety
of multibyte encodings and are primarily designed for dealing with legacy
Windows code pages encodings. The WCS functions handle Unicode UTF-16
strings.
Throughout the Windows API, a preﬁx or sufﬁx of “w,” “wcs” or “W” in-
dicates a wide character set (UTF-16) encoding; a preﬁx or sufﬁx of “mb” indi-
cates a multibyte encoding; and a preﬁx or sufﬁx of “a” or “A,” or the lack of
any preﬁx or sufﬁx, indicates an ANSI or Windows code pages encoding. STL
uses a similar convention—for example, std::string is STL’s ANSI string
class, while std::wstring is its wide character equivalent. Unfortunately,
the names of the functions aren’t always 100% consistent. This all leads to
some confusion among programmers who aren’t in the know. (But you aren’t
one of those programmers!) Table 5.1 lists some examples.
Windows also provides functions for translating between ANSI character
strings, multibyte strings and wide UTF-16 strings. For example, wcstombs()
converts a wide UTF-16 string into a multibyte string according to the cur-
rently active locale setting.
The Windows API uses a little preprocessor trick to allow you to write
code that is at least superﬁcially portable between wide (Unicode) and non-
wide (ANSI/MBCS) string encodings. The generic character data type TCHAR
is deﬁned to be a typedef to char when building your application in “ANSI
mode,” and it’s deﬁned to be a typedef to wchar_t when building your ap-
plication in “Unicode mode.” The macro _T() is used to convert an eight-bit
string literal (e.g., char* s = "this is a string";) into a wide string
literal (e.g., wchar_t* s = L"this is a string";) when compiling in
“Unicode mode.” Likewise, a suite of “fake” API functions are provided that
“automagically” morph into their appropriate 8-bit or 16-bit variant, depend-
ing on whether you are building in “Unicode mode” or not. These magic

284
5. Engine Support Systems
character-set-independent functions are either named with no preﬁx or sufﬁx,
or with a “t,” “tcs” or “T” preﬁx or sufﬁx.
Complete documentation for all of these functions can be found on Mi-
crosoft’s MSDN web site. Here’s a link to the documentation for strcmp()
and its ilk, from which you can quite easily navigate to the other related
string-manipulation functions using the tree view on the left-hand side of
the page, or via the search bar: http://msdn2.microsoft.com/en-us/library/
kk6xf663(VS.80).aspx.
5.4.4.4
Unicode on Consoles
The Xbox 360 software development kit (XDK) uses WCS strings pretty much
exclusively, for all strings—even for internal strings like ﬁle paths. This is cer-
tainly one valid approach to the localization problem, and it makes for very
consistent string handling throughout the XDK. However, the UTF-16 encod-
ing is a bit wasteful on memory, so different game engines may employ dif-
ferent conventions. At Naughty Dog, we use eight-bit char strings through-
out our engine, and we handle foreign languages via a UTF-8 encoding. The
choice of encoding is not particularly important, as long as you select one as
early in the project as possible and stick with it consistently.
5.4.4.5
Other Localization Concerns
Even once you have adapted your software to use Unicode characters, there
is still a host of other localization problems to contend with. For one thing,
strings aren’t the only place where localization issues arise. Audio clips in-
cluding recorded voices must be translated. Textures may have English words
painted into them that require translation. Many symbols have different mean-
ings in different cultures. Even something as innocuous as a no-smoking sign
might be misinterpreted in another culture. In addition, some markets draw
the boundaries between the various game-rating levels differently. For exam-
ple, in Japan a Teen-rated game is not permitted to show blood of any kind,
whereas in North America small red blood spatters are considered acceptable.
For strings, there are other details to worry about as well. You will need
to manage a database of all human-readable strings in your game, so that
they can all be reliably translated.
The software must display the proper
language given the user’s installation settings. The formatting of the strings
may be totally different in different languages—for example, Chinese is some-
times written vertically, and Hebrew reads right-to-left. The lengths of the
strings will vary greatly from language to language. You’ll also need to de-
cide whether to ship a single DVD or Blu-ray disc that contains all languages
or ship different discs for particular territories.

5.4. Strings
285
Id
English
French
p1score
“Player 1 Score”
“Grade Joueur 1”
p2score
“Player 2 Score”
“Grade Joueur 2”
p1wins
“Player one wins!”
“Joueur un gagne!”
p2wins
“Player two wins!”
“Joueur deux gagne!”
Table 5.2. Example of a string database used for localization.
The most crucial components in your localization system will be the cen-
tral database of human-readable strings and an in-game system for looking
up those strings by id. For example, let’s say you want a heads-up display
that lists the score of each player with “Player 1 Score:” and “Player 2 Score:”
labels and that also displays the text “Player 1 Wins” or “Player 2 Wins” at
the end of a round. These four strings would be stored in the localization
database under unique ids that are understandable to you, the developer of
the game. So our database might use the ids “p1score,” “p2score,” “p1wins”
and “p2wins,” respectively. Once our game’s strings have been translated into
French, our database would look something like the simple example shown
in Table 5.2. Additional columns can be added for each new language your
game supports.
The exact format of this database is up to you. It might be as simple as
a Microsoft Excel worksheet that can be saved as a comma-separated values
(CSV) ﬁle and parsed by the game engine or as complex as a full-ﬂedged Or-
acle database. The speciﬁcs of the string database are largely unimportant to
the game engine, as long as it can read in the string ids and the corresponding
Unicode strings for whatever language(s) your game supports. (However, the
speciﬁcs of the database may be very important from a practical point of view,
depending upon the organizational structure of your game studio. A small
studio with in-house translators can probably get away with an Excel spread-
sheet located on a network drive. But a large studio with branch ofﬁces in
Britain, Europe, South America and Japan would probably ﬁnd some kind of
distributed database a great deal more amenable.)
At runtime, you’ll need to provide a simple function that returns the Uni-
code string in the “current” language, given the unique id of that string. The
function might be declared like this:
wchar_t getLocalizedString(const char* id);
and it might be used like this:
void drawScoreHud(const Vector3& score1Pos,
const Vector3& score2Pos)

286
5. Engine Support Systems
{
renderer.displayTextOrtho(getLocalizedString("p1score"),
score1Pos);
renderer.displayTextOrtho(getLocalizedString("p2score"),
score2Pos);
// ...
}
Of course, you’ll need some way to set the “current” language globally. This
might be done via a conﬁguration setting, which is ﬁxed during the installa-
tion of the game. Or you might allow users to change the current language on
the ﬂy via an in-game menu. Either way, the setting is not difﬁcult to imple-
ment; it can be as simple as a global integer variable specifying the index of
the column in the string table from which to read (e.g., column one might be
English, column two French, column three Spanish and so on).
Once you have this infrastructure in place, your programmers must re-
member to never display a raw string to the user. They must always use the id of
a string in the database and call the look-up function in order to retrieve the
string in question.
5.4.4.6
Case Study: Naughty Dog’s Localization Tool
At Naughty Dog, we use a localization database that we developed in-house.
The localization tool’s back end consists of a MySQL database located on a
server that is accessible both to the developers within Naughty Dog and also
to the various external companies with which we work to translate our text
and speech audio clips into the various languages our games support. The
front end is a web interface that “speaks” to the database, allowing users to
view all of the text and audio assets, edit their contents, provide translations
for each asset, search for assets by id or by content and so on.
In Naughty Dog’s localization tool, each asset is either a string (for use in
the menus or HUD) or a speech audio clip with optional subtitle text (for use
as in-game dialog or within cinematics). Each asset has a unique identiﬁer,
which is represented as a hashed string id (see Section 5.4.3.1). If a string is
required for use in the menus or HUD, we look it up by its id and get back a
Unicode (UTF-8) string suitable for display on-screen. If a line of dialog must
be played, we likewise look up the audio clip by its id and use the data in-
engine to look up its corresponding subtitle (if any). The subtitle is treated
just like a menu or HUD string, in that it is returned by the localization tool’s
API as a UTF-8 string suitable for display.

5.4. Strings
287
Figure 5.12. Naughty Dog’s localization tool’s main window, showing a list of pure text assets used
in the menus and HUD. The user has just performed a search for an asset called MENU_NEWGAME.
Figure 5.13. Detailed asset view, showing the MENU_NEWGAME string.

288
5. Engine Support Systems
Figure 5.14. Text translations of the string “NEW GAME” into all languages supported by Naughty
Dog’s The Last of Us.
Figure 5.12 shows the main interface of the localization tool, in this case
displayed in the Chrome web browser. In this image, you can see that the
user has typed in the id MENU_NEWGAME in order to look up the string “NEW
GAME” (used on the game’s main menu for launching a new game). Fig-
ure 5.13 shows the detailed view of the MENU_NEWGAME asset. If the user
hits the “Text Translations” button in the upper-left corner of the asset de-
tails window, the screen shown in Figure 5.14 comes up, allowing the user
to enter or edit the various translations of the string. Figure 5.15 shows an-
other tab on the localization tool’s main page, this time listing audio speech
assets. Finally, Figure 5.16 depicts the detailed asset view for the speech as-
set BADA_GAM_MIL_ESCAPE_OVERPASS_001 (“We missed all the action”),
showing translations of this line of dialog into some of the supported lan-
guages.

5.4. Strings
289
Figure 5.15.
Naughty Dog’s localization tool’s main window again, this time showing a list of
speech audio assets with accompanying subtitle text.
Figure
5.16.
Detailed asset view showing recorded translations for the speech asset
BADA_GAM_MIL_ESCAPE_OVERPASS_001 (“We missed all the action”).

290
5. Engine Support Systems
5.5
Engine Conﬁguration
Game engines are complex beasts, and they invariably end up having a large
number of conﬁgurable options. Some of these options are exposed to the
player via one or more options menus in-game. For example, a game might
expose options related to graphics quality, the volume of music and sound
effects, or controller conﬁguration. Other options are created for the beneﬁt
of the game development team only and are either hidden or stripped out
of the game completely before it ships. For example, the player character’s
maximum walk speed might be exposed as an option so that it can be ﬁne-
tuned during development, but it might be changed to a hard-coded value
prior to ship.
5.5.1
Loading and Saving Options
A conﬁgurable option can be implemented trivially as a global variable or a
member variable of a singleton class. However, conﬁgurable options are not
particularly useful unless their values can be conﬁgured, stored on a hard
disk, memory card or other storage medium, and later retrieved by the game.
There are a number of simple ways to load and save conﬁguration options:
•
Text conﬁguration ﬁles. By far the most common method of saving and
loading conﬁguration options is by placing them into one or more text
ﬁles. The format of these ﬁles varies widely from engine to engine, but it
is usually very simple. For example, Windows INI ﬁles (which are used
by the OGRE renderer) consist of ﬂat lists of key-value pairs grouped
into logical sections.
[SomeSection]
Key1=Value1
Key2=Value2
[AnotherSection]
Key3=Value3
Key4=Value4
Key5=Value5
The XML format is another common choice for conﬁgurable game op-
tions ﬁles.
•
Compressed binary ﬁles. Most modern consoles have hard disk drives in
them, but older consoles could not afford this luxury. As a result, all
game consoles since the Super Nintendo Entertainment System (SNES)
have come equipped with proprietary removable memory cards that

5.5. Engine Conﬁguration
291
permit both reading and writing of data. Game options are sometimes
stored on these cards, along with saved games. Compressed binary ﬁles
are the format of choice on a memory card, because the storage space
available on these cards is often very limited.
•
The Windows registry. The Microsoft Windows operating system pro-
vides a global options database known as the registry. It is stored as a
tree, where the interior nodes (known as registry keys) act like ﬁle folders,
and the leaf nodes store the individual options as key-value pairs. Any
application, game or otherwise, can reserve an entire subtree (i.e., a reg-
istry key) for its exclusive use, and then store any set of options within
it. The Windows registry acts like a carefully organized collection of INI
ﬁles, and in fact it was introduced into Windows as a replacement for
the ever-growing network of INI ﬁles used by both the operating sys-
tem and Windows applications.
•
Command line options. The command line can be scanned for option set-
tings. The engine might provide a mechanism for controlling any option
in the game via the command line, or it might expose only a small subset
of the game’s options here.
•
Environment variables. On personal computers running Windows, Linux
or MacOS, environment variables are sometimes used to store conﬁgu-
ration options as well.
•
Online user proﬁles. With the advent of online gaming communities like
Xbox Live, each user can create a proﬁle and use it to save achievements,
purchased and unlockable game features, game options and other infor-
mation. The data are stored on a central server and can be accessed by
the player wherever an Internet connection is available.
5.5.2
Per-User Options
Most game engines differentiate between global options and per-user options.
This is necessary because most games allow each player to conﬁgure the game
to his or her liking. It is also a useful concept during development of the game,
because it allows each programmer, artist and designer to customize his or her
work environment without affecting other team members.
Obviously care must be taken to store per-user options in such a way that
each player “sees” only his or her options and not the options of other play-
ers on the same computer or console. In a console game, the user is typically
allowed to save his or her progress, along with per-user options such as con-
troller preferences, in “slots” on a memory card or hard disk. These slots are
usually implemented as ﬁles on the media in question.

292
5. Engine Support Systems
On a Windows machine, each user has a folder under C:\Users contain-
ing information such as the user’s desktop, his or her My Documents folder,
his or her Internet browsing history and temporary ﬁles and so on. A hid-
den subfolder named AppData is used to store per-user information on a per-
application basis; each application creates a folder under AppData and can use
it to store whatever per-user information it requires.
Windows games sometimes store per-user conﬁguration data in the reg-
istry. The registry is arranged as a tree, and one of the top-level children of the
root node, called HKEY_CURRENT_USER, stores settings for whichever user
happens to be logged on. Every user has his or her own subtree in the registry
(stored under the top-level subtree HKEY_USERS), and HKEY_CURRENT_USER
is really just an alias to the current user’s subtree. So games and other ap-
plications can manage per-user conﬁguration options by simply reading and
writing them to keys under the HKEY_CURRENT_USER subtree.
5.5.3
Conﬁguration Management in Some Real Engines
In this section, we’ll take a brief look at how some real game engines manage
their conﬁguration options.
5.5.3.1
Example: Quake’s Cvars
The Quake family of engines uses a conﬁguration management system known
as console variables, or cvars for short. A cvar is just a ﬂoating-point or string
global variable whose value can be inspected and modiﬁed from within
Quake’s in-game console. The values of some cvars can be saved to disk and
later reloaded by the engine.
At runtime, cvars are stored in a global linked list. Each cvar is a dynam-
ically allocated instance of struct cvar_t, which contains the variable’s
name, its value as a string or ﬂoat, a set of ﬂag bits, and a pointer to the next
cvar in the linked list of all cvars. Cvars are accessed by calling Cvar_Get(),
which creates the variable if it doesn’t already exist and modiﬁed by calling
Cvar_Set(). One of the bit ﬂags, CVAR_ARCHIVE, controls whether or not
the cvar will be saved into a conﬁguration ﬁle called conﬁg.cfg. If this ﬂag is
set, the value of the cvar will persist across multiple runs of the game.
5.5.3.2
Example: OGRE
The OGRE rendering engine uses a collection of text ﬁles in Windows INI
format for its conﬁguration options. By default, the options are stored in three
ﬁles, each of which is located in the same folder as the executable program:
•
plugins.cfg contains options specifying which optional engine plug-ins
are enabled and where to ﬁnd them on disk.

5.5. Engine Conﬁguration
293
•
resources.cfg contains a search path specifying where game assets (a.k.a.
media, a.k.a. resources) can be found.
•
ogre.cfg contains a rich set of options specifying which renderer (DirectX
or OpenGL) to use and the preferred video mode, screen size, etc.
Out of the box, OGRE provides no mechanism for storing per-user con-
ﬁguration options. However, the OGRE source code is freely available, so it
would be quite easy to change it to search for its conﬁguration ﬁles in the
user’s home directory instead of in the folder containing the executable. The
Ogre::ConfigFile class makes it easy to write code that reads and writes
brand new conﬁguration ﬁles as well.
5.5.3.3
Example: Uncharted and The Last of Us
Naughty Dog’s Uncharted/The Last of Us engine makes use of a number of
conﬁguration mechanisms.
In-Game Menu Settings
The Naughty Dog engine supports a powerful in-game menu system, allow-
ing developers to control global conﬁguration options and invoke commands.
The data types of the conﬁgurable options must be relatively simple (primar-
ily Boolean, integer and ﬂoating-point variables), but this limitation has not
prevented the developers at Naughty Dog from creating literally hundreds of
useful menu-driven options.
Each conﬁguration option is implemented as a global variable, or a mem-
ber of a singleton struct or class. When the menu option that controls an op-
tion is created, the address of the variable is provided, and the menu item
directly controls its value. As an example, the following function creates a
submenu item containing some options for Naughty Dog’s rail vehicles (sim-
ple vehicles that ride on splines, used in the “Out of the Frying Pan” jeep chase
level in Uncharted: Drake’s Fortune, the convoy level in Uncharted 2, the horse
chase in Uncharted 3 and some of the roaming Hunter vehicles in The Last of
Us). It deﬁnes menu items controlling three global variables: two Booleans
and one ﬂoating-point value. The items are collected onto a menu, and a
special item is returned that will bring up the menu when selected. Presum-
ably the code calling this function adds this item to the parent menu that it is
building.
DMENU::ItemSubmenu * CreateRailVehicleMenu()
{
extern bool g_railVehicleDebugDraw2D;
extern bool g_railVehicleDebugDrawCameraGoals;

294
5. Engine Support Systems
extern float g_railVehicleFlameProbability;
DMENU::Menu * pMenu
= new DMENU::Menu("RailVehicle");
pMenu->PushBackItem(
new DMENU::ItemBool("Draw 2D Spring Graphs",
DMENU::ToggleBool,
&g_railVehicleDebugDraw2D));
pMenu->PushBackItem(
new DMENU::ItemBool("Draw Goals (Untracked)",
DMENU::ToggleBool,
&g_railVehicleDebugDrawCameraGoals));
DMENU::ItemFloat * pItemFloat;
pItemFloat = new DMENU::ItemFloat(
"FlameProbability",
DMENU::EditFloat, 5, "%5.2f",
&g_railVehicleFlameProbability);
pItemFloat->SetRangeAndStep(0.0f, 1.0f, 0.1f, 0.01f);
pMenu->PushBackItem(pItemFloat);
DMENU::ItemSubmenu * pSubmenuItem;
pSubmenuItem = new DMENU::ItemSubmenu(
"RailVehicle...", pMenu);
return pSubmenuItem;
}
The value of any option can be saved by simply marking it with the cir-
cle button on the Dualshock joypad when the corresponding menu item is se-
lected. The menu settings are saved in an INI-style text ﬁle, allowing the saved
global variables to retain the values across multiple runs of the game. The abil-
ity to control which options are saved on a per-menu-item basis is highly useful,
because any option that is not saved will take on its programmer-speciﬁed de-
fault value. If a programmer changes a default, all users will “see” the new
value, unless of course a user has saved a custom value for that particular
option.
Command Line Arguments
The Naughty Dog engine scans the command line for a predeﬁned set of spe-
cial options. The name of the level to load can be speciﬁed, along with a
number of other commonly used arguments.

5.5. Engine Conﬁguration
295
Scheme Data Deﬁnitions
The vast majority of engine and game conﬁguration information in Uncharted
and The Last of Us is speciﬁed using a Lisp-like language called Scheme. Using
a proprietary data compiler, data structures deﬁned in the Scheme language
are transformed into binary ﬁles that can be loaded by the engine. The data
compiler also spits out header ﬁles containing C struct declarations for ev-
ery data type deﬁned in Scheme. These header ﬁles allow the engine to prop-
erly interpret the data contained in the loaded binary ﬁles. The binary ﬁles
can even be recompiled and reloaded on the ﬂy, allowing developers to alter
the data in Scheme and see the effects of their changes immediately (as long
as data members are not added or removed, as that would require a recompile
of the engine).
The following example illustrates the creation of a data structure specify-
ing the properties of an animation. It then exports three unique animations to
the game. You may have never read Scheme code before, but for this relatively
simple example it should be pretty self-explanatory. One oddity you’ll notice
is that hyphens are permitted within Scheme symbols, so simple-animation
is a single symbol (unlike in C/C++ where simple-animation would be
the subtraction of two variables, simple and animation).
simple-animation.scm
;; Define a new data type called simple-animation.
(deftype simple-animation ()
(
(name
string)
(speed
float
:default 1.0)
(fade-in-seconds
float
:default 0.25)
(fade-out-seconds float
:default 0.25)
)
)
;; Now define three instances of this data structure...
(define-export anim-walk
(new simple-animation
:name "walk"
:speed 1.0
)
)
(define-export anim-walk-fast
(new simple-animation
:name "walk"
:speed 2.0
)
)

296
5. Engine Support Systems
(define-export anim-jump
(new simple-animation
:name "jump"
:fade-in-seconds 0.1
:fade-out-seconds 0.1
)
)
This Scheme code would generate the following C/C++ header ﬁle:
simple-animation.h
// WARNING: This file was automatically generated from
// Scheme. Do not hand-edit.
struct SimpleAnimation
{
const char* m_name;
float
m_speed;
float
m_fadeInSeconds;
float
m_fadeOutSeconds;
};
In-game, the data can be read by calling the LookupSymbol() function, which
is templated on the data type returned, as follows:
#include "simple-animation.h"
void someFunction()
{
SimpleAnimation* pWalkAnim
= LookupSymbol<SimpleAnimation*>("anim-walk");
SimpleAnimation* pFastWalkAnim
= LookupSymbol<SimpleAnimation*>(
"anim-walk-fast");
SimpleAnimation* pJumpAnim
= LookupSymbol<SimpleAnimation*>("anim-jump");
// use the data here...
}
This system gives the programmers a great deal of ﬂexibility in deﬁning
all sorts of conﬁguration data—from simple Boolean, ﬂoating-point and string
options all the way to complex, nested, interconnected data structures. It is
used to specify detailed animation trees, physics parameters, player mechan-
ics and so on.

6
Resources and
the File System
G
ames are by nature multimedia experiences. A game engine therefore
needs to be capable of loading and managing a wide variety of different
kinds of media—texture bitmaps, 3D mesh data, animations, audio clips, col-
lision and physics data, game world layouts, and the list goes on. Moreover,
because memory is usually scarce, a game engine needs to ensure that only
one copy of each media ﬁle is loaded into memory at any given time. For ex-
ample, if ﬁve meshes share the same texture, then we would like to have only
one copy of that texture in memory, not ﬁve. Most game engines employ some
kind of resource manager (a.k.a. asset manager, a.k.a. media manager) to load and
manage the myriad resources that make up a modern 3D game.
Every resource manager makes heavy use of the ﬁle system. On a per-
sonal computer, the ﬁle system is exposed to the programmer via a library of
operating system calls. However, game engines often “wrap” the native ﬁle
system API in an engine-speciﬁc API, for two primary reasons. First, the en-
gine might be cross-platform, in which case the game engine’s ﬁle system API
can shield the rest of the software from differences between different target
hardware platforms. Second, the operating system’s ﬁle system API might
not provide all the tools needed by a game engine. For example, many en-
gines support ﬁle streaming (i.e., the ability to load data “on the ﬂy” while the
game is running), yet most operating systems don’t provide a streaming ﬁle
297

298
6. Resources and the File System
system API out of the box. Console game engines also need to provide ac-
cess to a variety of removable and non-removable media, from memory sticks
to optional hard drives to a DVD-ROM or Blu-ray ﬁxed disk to network ﬁle
systems (e.g., Xbox Live or the PlayStation Network, PSN). The differences
between various kinds of media can likewise be “hidden” behind a game en-
gine’s ﬁle system API.
In this chapter, we’ll ﬁrst explore the kinds of ﬁle system APIs found in
modern 3D game engines. Then we’ll see how a typical resource manager
works.
6.1
File System
A game engine’s ﬁle system API typically addresses the following areas of
functionality:
•
manipulating ﬁle names and paths,
•
opening, closing, reading and writing individual ﬁles,
•
scanning the contents of a directory, and
•
handling asynchronous ﬁle I/O requests (for streaming).
We’ll take a brief look at each of these in the following sections.
6.1.1
File Names and Paths
A path is a string describing the location of a ﬁle or directory within a ﬁle
system hierarchy. Each operating system uses a slightly different path format,
but paths have essentially the same structure on every operating system. A
path generally takes the following form:
volume/directory1/directory2/.../directoryN/file-name
or
volume/directory1/directory2/.../directory(N −1)/directoryN
In other words, a path generally consists of an optional volume speciﬁer fol-
lowed by a sequence of path components separated by a reserved path separa-
tor character such as the forward or backward slash (/ or \). Each component
names a directory along the route from the root directory to the ﬁle or direc-
tory in question. If the path speciﬁes the location of a ﬁle, the last compo-
nent in the path is the ﬁle name; otherwise it names the target directory. The
root directory is usually indicated by a path consisting of the optional volume
speciﬁer followed by a single path separator character (e.g., / on UNIX, or
C:\ on Windows).

6.1. File System
299
6.1.1.1
Differences Across Operating Systems
Each operating system introduces slight variations on this general path struc-
ture. Here are some of the key differences between Microsoft DOS, Microsoft
Windows, the UNIX family of operating systems and Apple Macintosh OS:
•
UNIX uses a forward slash (/) as its path component separator, while
DOS and older versions of Windows used a backslash (\) as the path
separator. Recent versions of Windows allow either forward or back-
ward slashes to be used to separate path components, although some
applications still fail to accept forward slashes.
•
Mac OS 8 and 9 use the colon (:) as the path separator character. Mac
OS X is based on UNIX, so it supports UNIX’s forward slash notation.
•
UNIX and its variants don’t support volumes as separate directory hi-
erarchies. The entire ﬁle system is contained within a single monolithic
hierarchy, and local disk drives, network drives and other resources are
mounted so that they appear to be subtrees within the main hierarchy. As
a result, UNIX paths never have a volume speciﬁer.
•
On Microsoft Windows, volumes can be speciﬁed in two ways. A local
disk drive is speciﬁed using a single letter followed by a colon (e.g., the
ubiquitous C:). A remote network share can either be mounted so that
it looks like a local disk, or it can be referenced via a volume speciﬁer
consisting of two backslashes followed by the remote computer name
and the name of a shared directory or resource on that machine (e.g.,
\\some-computer\some-share). This double backslash notation is
an example of the Universal Naming Convention (UNC).
•
Under DOS and early versions of Windows, a ﬁle name could be up to
eight characters in length, with a three-character extension which was
separated from the main ﬁle name by a dot. The extension described
the ﬁle’s type, for example .txt for a text ﬁle or .exe for an executable
ﬁle. In recent Windows implementations, ﬁle names can contain any
number of dots (as they can under UNIX), but the characters after the
ﬁnal dot are still interpreted as the ﬁle’s extension by many applications
including the Windows Explorer.
•
Each operating system disallows certain characters in the names of ﬁles
and directories. For example, a colon cannot appear anywhere in a Win-
dows or DOS path except as part of a drive letter volume speciﬁer. Some
operating systems permit a subset of these reserved characters to ap-
pear in a path as long as the path is quoted in its entirety or the offend-
ing character is escaped by preceding it with a backslash or some other

300
6. Resources and the File System
reserved escape character. For example, ﬁle and directory names may
contain spaces under Windows, but such a path must be surrounded by
double quotes in certain contexts.
•
Both UNIX and Windows have the concept of a current working directory
or CWD (also known as the present working directory or PWD). The CWD
can be set from a command shell via the cd (change directory) command
on both operating systems, and it can be queried by typing cd with no
arguments under Windows or by executing the pwd command on UNIX.
Under UNIX there is only one CWD. Under Windows, each volume has
its own private CWD.
•
Operating systems that support multiple volumes, like Windows, also
have the concept of a current working volume. From a Windows command
shell, the current volume can be set by entering its drive letter and a
colon followed by the Enter key (e.g., C:<Enter>).
•
Consoles often also employ a set of predeﬁned path preﬁxes to rep-
resent multiple volumes.
For example, PlayStation 3 uses the preﬁx
/dev_bdvd/ to refer to the Blu-ray disk drive, while /dev_hddx/ refers
to one or more hard disks (where x is the index of the device). On a PS3
development kit, /app_home/ maps to a user-deﬁned path on what-
ever host machine is being used for development. During development,
the game usually reads its assets from /app_home/ rather than from
the Blu-ray or the hard disk.
6.1.1.2
Absolute and Relative Paths
All paths are speciﬁed relative to some location within the ﬁle system. When a
path is speciﬁed relative to the root directory, we call it an absolute path. When
it is relative to some other directory in the ﬁle system hierarchy, we call it a
relative path.
Under both UNIX and Windows, absolute paths start with a path sepa-
rator character (/ or \), while relative paths have no leading path separator.
On Windows, both absolute and relative paths may have an optional volume
speciﬁer—if the volume is omitted, then the path is assumed to refer to the
current working volume.
The following paths are all absolute:
Windows
•
C:\Windows\System32
•
D:\ (root directory on the D: volume)
•
\ (root directory on the current working volume)

6.1. File System
301
•
\game\assets\animation\walk.anim (current working volume)
•
\\joe-dell\Shared_Files\Images\foo.jpg (network path)
UNIX
•
/usr/local/bin/grep
•
/game/src/audio/effects.cpp
•
/ (root directory)
The following paths are all relative:
Windows
•
System32 (relative to CWD \Windows on the current volume)
•
X:animation\walk.anim (relative to CWD \game\assets on the
X: volume)
UNIX
•
bin/grep (relative to CWD /usr/local)
•
src/audio/effects.cpp (relative to CWD /game)
6.1.1.3
Search Paths
The term path should not be confused with the term search path. A path is
a string representing the location of a single ﬁle or directory within the ﬁle
system hierarchy. A search path is a string containing a list of paths, each sep-
arated by a special character such as a colon or semicolon, which is searched
when looking for a ﬁle. For example, when you run any program from a com-
mand prompt, the operating system ﬁnds the executable ﬁle by searching each
directory on the search path contained in the shell’s environment variable.
Some game engines also use search paths to locate resource ﬁles. For ex-
ample, the OGRE rendering engine uses a resource search path contained in
a text ﬁle named resources.cfg. The ﬁle provides a simple list of directo-
ries and ZIP archives that should be searched in order when trying to ﬁnd an
asset. That said, searching for assets at runtime is a time-consuming propo-
sition. Usually there’s no reason our assets’ paths cannot be known a priori.
Presuming this is the case, we can avoid having to search for assets at all—
which is clearly a superior approach.
6.1.1.4
Path APIs
Clearly, paths are much more complex than simple strings. There are many
things a programmer may need to do when dealing with paths, such as iso-
lating the directory, ﬁlename and extension, canonicalizing a path, converting

302
6. Resources and the File System
back and forth between absolute and relative paths and so on. It can be ex-
tremely helpful to have a feature-rich API to help with these tasks.
Microsoft Windows provides an API for this purpose. It is implemented
by the dynamic link library shlwapi.dll, and it is exposed via the header
ﬁle shlwapi.h. Complete documentation for this API is provided on the
Microsoft Developer’s Network (MSDN) at the following URL: http://msdn2.
microsoft.com/en-us/library/bb773559(VS.85).aspx.
Of course, the shlwapi API is only available on Win32 platforms. Sony
provides similar APIs for use on the PlayStation 3 and PlayStation 4. But when
writing a cross-platform game engine, we cannot use platform-speciﬁc APIs
directly. A game engine may not need all of the functions provided by an
API like shlwapi anyway. For these reasons, game engines often implement
a stripped-down path-handling API that meets the engine’s particular needs
and works on every operating system targeted by the engine. Such an API can
be implemented as a thin wrapper around the native API on each platform or
it can be written from scratch.
6.1.2
Basic File I/O
The standard C library provides two APIs for opening, reading and writing
the contents of ﬁles—one buffered and the other unbuffered. Every ﬁle I/O
API requires data blocks known as buffers to serve as the source or destina-
tion of the bytes passing between the program and the ﬁle on disk. We say a
ﬁle I/O API is buffered when the API manages the necessary input and output
data buffers for you. With an unbuffered API, it is the responsibility of the
programmer using the API to allocate and manage the data buffers. The stan-
dard C library’s buffered ﬁle I/O routines are sometimes referred to as the
stream I/O API, because they provide an abstraction which makes disk ﬁles
look like streams of bytes.
The standard C library functions for buffered and unbuffered ﬁle I/O are
listed in Table 6.1.
The standard C library I/O functions are well-documented, so we will not
repeat detailed documentation for them here. For more information, please
refer to http://msdn.microsoft.com/en-us/library/c565h7xx.aspx for Micro-
soft’s implementation of the buffered (stream I/O) API, and to http://msdn.
microsoft.com/en-us/library/40bbyw78.aspx for Microsoft’s implementation
of the unbuffered (low-level I/O) API.
On UNIX and its variants, the standard C library’s unbuffered I/O routes
are native operating system calls. However, on Microsoft Windows these rou-
tines are merely wrappers around an even lower-level API. The Win32 func-
tion CreateFile() creates or opens a ﬁle for writing or reading, ReadFile()

6.1. File System
303
Operation
Buffered API
Unbuffered API
Open a ﬁle
fopen()
open()
Close a ﬁle
fclose()
close()
Read from a ﬁle
fread()
read()
Write to a ﬁle
fwrite()
write()
Seek to an offset
fseek()
seek()
Return current offset
ftell()
tell()
Read a single line
fgets()
n/a
Write a single line
fputs()
n/a
Read formatted string
fscanf()
n/a
Write formatted string
fprintf()
n/a
Query ﬁle status
fstat()
stat()
Table 6.1. Buffered and unbuffered ﬁle operations in the standard C library.
and WriteFile() read and write data, respectively, and CloseFile()
closes an open ﬁle handle. The advantage to using low-level system calls as
opposed to standard C library functions is that they expose all of the details of
the native ﬁle system. For example, you can query and control the security at-
tributes of ﬁles when using the Windows native API—something you cannot
do with the standard C library.
Some game teams ﬁnd it useful to manage their own buffers. For example,
the Red Alert 3 team at Electronic Arts observed that writing data into log ﬁles
was causing signiﬁcant performance degradation. They changed the logging
system so that it accumulated its output into a memory buffer, writing the
buffer out to disk only when it was ﬁlled. Then they moved the buffer dump
routine out into a separate thread to avoid stalling the main game loop.
6.1.2.1
To Wrap or Not to Wrap
A game engine can be written to use the standard C library’s ﬁle I/O func-
tions or the operating system’s native API. However, many game engines
wrap the ﬁle I/O API in a library of custom I/O functions.
There are at
least three advantages to wrapping the operating system’s I/O API. First, the
engine programmers can guarantee identical behavior across all target plat-
forms, even when native libraries are inconsistent or buggy on a particular
platform. Second, the API can be simpliﬁed down to only those functions ac-
tually required by the engine, which keeps maintenance efforts to a minimum.
Third, extended functionality can be provided. For example, the engine’s cus-
tom wrapper API might be capable of dealing with ﬁles on a hard disk, a
DVD-ROM or Blu-ray disk on a console, ﬁles on a network (e.g., remote ﬁles

304
6. Resources and the File System
managed by Xbox Live or PSN), and also with ﬁles on memory sticks or other
kinds of removable media.
6.1.2.2
Synchronous File I/O
Both of the standard C library’s ﬁle I/O libraries are synchronous, meaning
that the program making the I/O request must wait until the data has been
completely transferred to or from the media device before continuing. The
following code snippet demonstrates how the entire contents of a ﬁle might be
read into an in-memory buffer using the synchronous I/O function fread().
Notice how the function syncReadFile() does not return until all the data
has been read into the buffer provided.
bool syncReadFile(const char* filePath,
U8* buffer,
size_t bufferSize,
size_t& rBytesRead)
{
FILE* handle = fopen(filePath, "rb");
if (handle)
{
// BLOCK here until all data has been read.
size_t bytesRead = fread(buffer, 1,
bufferSize, handle);
int err = ferror(handle); // get error if any
fclose(handle);
if (0 == err)
{
rBytesRead = bytesRead;
return true;
}
}
rBytesRead = 0;
return false;
}
void main(int argc, const char* argv[])
{
U8 testBuffer[512];
size_t bytesRead = 0;
if (syncReadFile("C:\\testfile.bin",
testBuffer, sizeof(testBuffer),
bytesRead))

6.1. File System
305
{
printf("success: read %u bytes\n", bytesRead);
// contents of buffer can be used here...
}
}
6.1.3
Asynchronous File I/O
Streaming refers to the act of loading data in the background while the main
program continues to run.
Many games provide the player with a seam-
less, load-screen-free playing experience by streaming data for upcoming lev-
els from the DVD-ROM, Blu-ray disk or hard drive while the game is being
played. Audio and texture data are probably the most commonly streamed
types of data, but any type of data can be streamed, including geometry, level
layouts and animation clips.
In order to support streaming, we must utilize an asynchronous ﬁle I/O li-
brary, i.e., one which permits the program to continue to run while its I/O re-
quests are being satisﬁed. Some operating systems provide an asynchronous
ﬁle I/O library out of the box. For example, the Windows Common Lan-
guage Runtime (CLR, the virtual machine upon which languages like Visual
BASIC, C#, managed C++ and J# are implemented) provides functions like
System.IO.BeginRead() and System.IO.BeginWrite(). An asynchro-
nous API known as fios is available for the PlayStation 3 and PlayStation 4.
If an asynchronous ﬁle I/O library is not available for your target platform, it
is possible to write one yourself. And even if you don’t have to write it from
scratch, it’s probably a good idea to wrap the system API for portability.
The following code snippet demonstrates how the entire contents of a ﬁle
might be read into an in-memory buffer using an asynchronous read oper-
ation. Notice that the asyncReadFile() function returns immediately—
the data is not present in the buffer until our callback function asyncRead-
Complete() has been called by the I/O library.
AsyncRequestHandle g_hRequest; // async I/O request handle
U8 g_asyncBuffer[512];
// input buffer
static void asyncReadComplete(AsyncRequestHandle hRequest);
void main(int argc, const char* argv[])
{
// NOTE: This call to asyncOpen() might itself be an
// asynchronous call, but we'll ignore that detail
// here and just assume it's a blocking function.

306
6. Resources and the File System
AsyncFileHandle hFile = asyncOpen(
"C:\\testfile.bin");
if (hFile)
{
// This function requests an I/O read, then
// returns immediately (non-blocking).
g_hRequest = asyncReadFile(
hFile,
// file handle
g_asyncBuffer,
// input buffer
sizeof(g_asyncBuffer),
// size of buffer
asyncReadComplete);
// callback function
}
// Now go on our merry way...
// (This loop simulates doing real work while we wait
// for the I/O read to complete.)
for (;;)
{
OutputDebugString("zzz...\n");
Sleep(50);
}
}
// This function will be called when the data has been read.
static void asyncReadComplete(AsyncRequestHandle hRequest)
{
if (hRequest == g_hRequest
&& asyncWasSuccessful(hRequest))
{
// The data is now present in g_asyncBuffer[] and
// can be used. Query for the number of bytes
// actually read:
size_t bytes = asyncGetBytesReadOrWritten(
hRequest);
char msg[256];
snprintf(msg, sizeof(msg),
"async success, read %u bytes\n",
bytes);
OutputDebugString(msg);
}
}
Most asynchronous I/O libraries permit the main program to wait for an
I/O operation to complete some time after the request was made. This can be

6.1. File System
307
useful in situations where only a limited amount of work can be done before
the results of a pending I/O request are needed. This is illustrated in the
following code snippet.
U8 g_asyncBuffer[512];
// input buffer
void main(int argc, const char* argv[])
{
AsyncRequestHandle hRequest = ASYNC_INVALID_HANDLE;
AsyncFileHandle hFile = asyncOpen(
"C:\\testfile.bin");
if (hFile)
{
// This function requests an I/O read, then
// returns immediately (non-blocking).
hRequest = asyncReadFile(
hFile,
// file handle
g_asyncBuffer,
// input buffer
sizeof(g_asyncBuffer),
// size of buffer
NULL);
// no callback
}
// Now do some limited amount of work...
for (int i = 0; i < 10; i++)
{
OutputDebugString("zzz...\n");
Sleep(50);
}
// We can't do anything further until we have that
// data, so wait for it here.
asyncWait(hRequest);
if (asyncWasSuccessful(hRequest))
{
// The data is now present in g_asyncBuffer[] and
// can be used. Query for the number of bytes
// actually read:
size_t bytes = asyncGetBytesReadOrWritten(
hRequest);
char msg[256];
snprintf(msg, sizeof(msg),
"async success, read %u bytes\n",
bytes);
OutputDebugString(msg);
}
}

308
6. Resources and the File System
Some asynchronous I/O libraries allow the programmer to ask for an esti-
mate of how long a particular asynchronous operation will take to complete.
Some APIs also allow you to set deadlines on a request (which effectively pri-
oritizes the request relative to other pending requests), and to specify what
happens when a request misses its deadline (e.g., cancel the request, notify
the program and keep trying, etc.)
6.1.3.1
Priorities
It’s important to remember that ﬁle I/O is a real-time system, subject to dead-
lines just like the rest of the game. Therefore, asynchronous I/O operations
often have varying priorities. For example, if we are streaming audio from
the hard disk or Blu-ray and playing it on the ﬂy, loading the next buffer-full
of audio data is clearly higher priority than, say, loading a texture or a chunk
of a game level. Asynchronous I/O systems must be capable of suspending
lower-priority requests, so that higher-priority I/O requests have a chance to
complete within their deadlines.
6.1.3.2
How Asynchronous File I/O Works
Asynchronous ﬁle I/O works by handling I/O requests in a separate thread.
The main thread calls functions that simply place requests on a queue and
then return immediately. Meanwhile, the I/O thread picks up requests from
the queue and handles them sequentially using blocking I/O routines like
read() or fread(). When a request is completed, a callback provided by
the main thread is called, thereby notifying it that the operation is done. If the
main thread chooses to wait for an I/O request to complete, this is handled via
a semaphore. (Each request has an associated semaphore, and the main thread
can put itself to sleep waiting for that semaphore to be signaled by the I/O
thread upon completion of the request.)
Virtually any synchronous operation you can imagine can be transformed
into an asynchronous operation by moving the code into a separate thread—
or by running it on a physically separate processor, such as on one of the CPU
cores on the PlayStation 4. See Section 7.6 for more details.
6.2
The Resource Manager
Every game is constructed from a wide variety of resources (sometimes called
assets or media). Examples include meshes, materials, textures, shader pro-
grams, animations, audio clips, level layouts, collision primitives, physics pa-
rameters, and the list goes on. A game’s resources must be managed, both in

6.2. The Resource Manager
309
terms of the ofﬂine tools used to create them, and in terms of loading, unload-
ing and manipulating them at runtime. Therefore, every game engine has a
resource manager of some kind.
Every resource manager is comprised of two distinct but integrated com-
ponents. One component manages the chain of ofﬂine tools used to create the
assets and transform them into their engine-ready form. The other component
manages the resources at runtime, ensuring that they are loaded into memory
in advance of being needed by the game and making sure they are unloaded
from memory when no longer needed.
In some engines, the resource manager is a cleanly designed, uniﬁed, cen-
tralized subsystem that manages all types of resources used by the game. In
other engines, the resource manager doesn’t exist as a single subsystem per se,
but rather is spread across a disparate collection of subsystems, perhaps writ-
ten by different individuals at various times over the engine’s long and some-
times colorful history. But no matter how it is implemented, a resource man-
ager invariably takes on certain responsibilities and solves a well-understood
set of problems. In this section, we’ll explore the functionality and some of
the implementation details of a typical game engine resource manager.
6.2.1
Ofﬂine Resource Management and the Tool Chain
6.2.1.1
Revision Control for Assets
On a small game project, the game’s assets can be managed by keeping loose
ﬁles sitting around on a shared network drive with an ad hoc directory struc-
ture. This approach is not feasible for a modern commercial 3D game, com-
prised of a massive number and variety of assets. For such a project, the team
requires a more formalized way to track and manage its assets.
Some game teams use a source code revision control system to manage
their resources. Art source ﬁles (Maya scenes, Photoshop PSD ﬁles, Illustrator
ﬁles, etc.) are checked in to Perforce or a similar package by the artists. This
approach works reasonably well, although some game teams build custom
asset management tools to help ﬂatten the learning curve for their artists. Such
tools may be simple wrappers around a commercial revision control system,
or they might be entirely custom.
Dealing with Data Size
One of the biggest problems in the revision control of art assets is the sheer
amount of data. Whereas C++ and script source code ﬁles are small, relative
to their impact on the project, art ﬁles tend to be much, much larger. Because

310
6. Resources and the File System
many source control systems work by copying ﬁles from the central repository
down to the user’s local machine, the sheer size of the asset ﬁles can render
these packages almost entirely useless.
I’ve seen a number of different solutions to this problem employed at vari-
ous studios. Some studios turn to commercial revision control systems like
Alienbrain that have been speciﬁcally designed to handle very large data
sizes. Some teams simply “take their lumps” and allow their revision con-
trol tool to copy assets locally. This can work, as long as your disks are big
enough and your network bandwidth sufﬁcient, but it can also be inefﬁcient
and slow the team down. Some teams build elaborate systems on top of their
revision control tool to ensure that a particular end user only gets local copies
of the ﬁles he or she actually needs. In this model, the user either has no access
to the rest of the repository or can access it on a shared network drive when
needed.
At Naughty Dog we use a proprietary tool that makes use of UNIX sym-
bolic links to virtually eliminate data copying, while permitting each user to
have a complete local view of the asset repository. As long as a ﬁle is not
checked out for editing, it is a symlink to a master ﬁle on a shared network
drive. A symbolic link occupies very little space on the local disk, because it
is nothing more than a directory entry. When the user checks out a ﬁle for
editing, the symlink is removed, and a local copy of the ﬁle replaces it. When
the user is done editing and checks the ﬁle in, the local copy becomes the new
master copy, its revision history is updated in a master database, and the local
ﬁle turns back into a symlink. This system works very well, but it requires
the team to build their own revision control system from scratch; I am un-
aware of any commercial tool that works like this. Also, symbolic links are a
UNIX feature—such a tool could probably be built with Windows junctions
(the Windows equivalent of a symbolic link), but I haven’t seen anyone try it
as yet.
6.2.1.2
The Resource Database
As we’ll explore in depth in the next section, most assets are not used in their
original format by the game engine. They need to pass through some kind of
asset conditioning pipeline, whose job it is to convert the assets into the bi-
nary format needed by the engine. For every resource that passes through
the asset conditioning pipeline, there is some amount of metadata that de-
scribes how that resource should be processed. When compressing a texture
bitmap, we need to know what type of compression best suits that particu-
lar image. When exporting an animation, we need to know what range of
frames in Maya should be exported. When exporting character meshes out of

6.2. The Resource Manager
311
a Maya scene containing multiple characters, we need to know which mesh
corresponds to which character in the game.
To manage all of this metadata, we need some kind of database. If we are
making a very small game, this database might be housed in the brains of
the developers themselves. I can hear them now: “Remember: the player’s
animations need to have the ‘ﬂip X’ ﬂag set, but the other characters must not
have it set...or...rats...is it the other way around?”
Clearly for any game of respectable size, we simply cannot rely on the
memories of our developers in this manner. For one thing, the sheer volume of
assets becomes overwhelming quite quickly. Processing individual resource
ﬁles by hand is also far too time-consuming to be practical on a full-ﬂedged
commercial game production. Therefore, every professional game team has
some kind of semiautomated resource pipeline, and the data that drive the
pipeline is stored in some kind of resource database.
The resource database takes on vastly different forms in different game
engines. In one engine, the metadata describing how a resource should be
built might be embedded into the source assets themselves (e.g., it might be
stored as so-called blind data within a Maya ﬁle). In another engine, each
source resource ﬁle might be accompanied by a small text ﬁle that describes
how it should be processed. Still other engines encode their resource build-
ing metadata in a set of XML ﬁles, perhaps wrapped in some kind of custom
graphical user interface. Some engines employ a true relational database, such
as Microsoft Access, MySQL or conceivably even a heavyweight database like
Oracle.
Whatever its form, a resource database must provide the following basic
functionality:
•
The ability to deal with multiple types of resources, ideally (but certainly
not necessarily) in a somewhat consistent manner.
•
The ability to create new resources.
•
The ability to delete resources.
•
The ability to inspect and modify existing resources.
•
The ability to move a resource’s source ﬁle(s) from one location to an-
other on-disk. (This is very helpful because artists and game designers
often need to rearrange assets to reﬂect changing project goals, rethink-
ing of game designs, feature additions and cuts, etc.)
•
The ability of a resource to cross-reference other resources (e.g., the ma-
terial used by a mesh, or the collection of animations needed by level
17). These cross-references typically drive both the resource building
process and the loading process at runtime.

312
6. Resources and the File System
•
The ability to maintain referential integrity of all cross-references within
the database and to do so in the face of all common operations such as
deleting or moving resources around.
•
The ability to maintain a revision history, complete with a log of who
made each change and why.
•
It is also very helpful if the resource database supports searching or
querying in various ways.
For example, a developer might want to
know in which levels a particular animation is used or which textures
are referenced by a set of materials. Or they might simply be trying to
ﬁnd a resource whose name momentarily escapes them.
It should be pretty obvious from looking at the above list that creating a re-
liable and robust resource database is no small task. When designed well and
implemented properly, the resource database can quite literally make the dif-
ference between a team that ships a hit game and a team that spins its wheels
for 18 months before being forced by management to abandon the project (or
worse). I know this to be true, because I’ve personally experienced both.
6.2.1.3
Some Successful Resource Database Designs
Every game team will have different requirements and make different deci-
sions when designing their resource database. However, for what it’s worth,
here are some designs that have worked well in my own experience.
Unreal Engine 4
Unreal’s resource database is managed by their über-tool, UnrealEd.
Un-
realEd is responsible for literally everything, from resource metadata manage-
ment to asset creation to level layout and more. UnrealEd has its drawbacks,
but its single biggest beneﬁt is that UnrealEd is a part of the game engine it-
self. This permits assets to be created and then immediately viewed in their
full glory, exactly as they will appear in-game. The game can even be run from
within UnrealEd in order to visualize the assets in their natural surroundings
and see if and how they work in-game.
Another big beneﬁt of UnrealEd is what I would call one-stop shopping. Un-
realEd’s Generic Browser (depicted in Figure 6.1) allows a developer to access
literally every resource that is consumed by the engine. Having a single, uni-
ﬁed and reasonably consistent interface for creating and managing all types
of resources is a big win. This is especially true considering that the resource
data in most other game engines is fragmented across countless inconsistent
and often cryptic tools. Just being able to ﬁnd any resource easily in UnrealEd
is a big plus.

6.2. The Resource Manager
313
Figure 6.1. UnrealEd’s Generic Browser.
Unreal can be less error-prone than many other engines, because assets
must be explicitly imported into Unreal’s resource database. This allows re-
sources to be checked for validity very early in the production process. In
most game engines, any old data can be thrown into the resource database,
and you only know whether or not that data is valid when it is eventually
built—or sometimes not until it is actually loaded into the game at runtime.
But with Unreal, assets can be validated as soon as they are imported into
UnrealEd. This means that the person who created the asset gets immediate
feedback as to whether his or her asset is conﬁgured properly.
Of course, Unreal’s approach has some serious drawbacks. For one thing,
all resource data is stored in a small number of large package ﬁles. These ﬁles
are binary, so they are not easily merged by a revision control package like
CVS, Subversion or Perforce. This presents some major problems when more
than one user wants to modify resources that reside in a single package. Even
if the users are trying to modify different resources, only one user can lock the
package at a time, so the other has to wait. The severity of this problem can be
reduced by dividing resources into relatively small, granular packages, but it
cannot practically be eliminated.

314
6. Resources and the File System
Referential integrity is quite good in UnrealEd, but there are still some
problems. When a resource is renamed or moved around, all references to it
are maintained automatically using a dummy object that remaps the old re-
source to its new name/location. The problem with these dummy remapping
objects is that they hang around and accumulate and sometimes cause prob-
lems, especially if a resource is deleted. Overall, Unreal’s referential integrity
is quite good, but it is not perfect.
Despite its problems, UnrealEd is by far the most user-friendly, well-in-
tegrated and streamlined asset creation toolkit, resource database and asset
conditioning pipeline that I have ever worked with.
Naughty Dog’s Uncharted / The Last of Us Engine
For Uncharted: Drake’s Fortune, Naughty Dog stored its resource metadata in
a MySQL database. A custom graphical user interface was written to man-
age the contents of the database. This tool allowed artists, game designers
and programmers alike to create new resources, delete existing resources and
inspect and modify resources as well. This GUI was a crucial component of
the system, because it allowed users to avoid having to learn the intricacies of
interacting with a relational database via SQL.
The original MySQL database used on Uncharted did not provide a use-
ful history of the changes made to the database, nor did it provide a good
way to roll back “bad” changes. It also did not support multiple users edit-
ing the same resource, and it was difﬁcult to administer. Naughty Dog has
since moved away from MySQL in favor of an XML ﬁle-based asset database,
managed under Perforce.
Builder, Naughty Dog’s resource database GUI, is depicted in Figure 6.2.
The window is broken into two main sections: a tree view showing all re-
sources in the game on the left and a properties window on the right, allowing
the resource(s) that are selected in the tree view to be viewed and edited. The
resource tree contains folders for organizational purposes, so that the artists
and game designers can organize their resources in any way they see ﬁt. Vari-
ous types of resources can be created and managed within any folder, includ-
ing actors and levels, and the various subresources that comprise them (pri-
marily meshes, skeletons and animations). Animations can also be grouped
into pseudo-folders known as bundles. This allows large groups of anima-
tions to be created and then managed as a unit, and prevents a lot of wasted
time dragging individual animations around in the tree view.
The asset conditioning pipeline employed on Uncharted and The Last of Us
consists of a set of resource exporters, compilers and linkers that are run from
the command line. The engine is capable of dealing with a wide variety of

6.2. The Resource Manager
315
Figure 6.2. The front-end GUI for Naughty Dog’s ofﬂine resource database, Builder.
different kinds of data objects, but these are packaged into one of two types
of resource ﬁles: actors and levels. An actor can contain skeletons, meshes,
materials, textures and/or animations. A level contains static background
meshes, materials and textures, and also level-layout information. To build an
actor, one simply types ba name-of-actor on the command line; to build a level,
one types bl name-of-level. These command-line tools query the database to
determine exactly how to build the actor or level in question. This includes
information on how to export the assets from DCC tools like Maya and Pho-
toshop, how to process the data, and how to package it into binary .pak ﬁles
that can be loaded by the game engine. This is much simpler than in many
engines, where resources have to be exported manually by the artists—a time-
consuming, tedious and error-prone task.

316
6. Resources and the File System
The beneﬁts of the resource pipeline design used by Naughty Dog include:
•
Granular resources. Resources can be manipulated in terms of logical en-
tities in the game—meshes, materials, skeletons and animations. These
resource types are granular enough that the team almost never has con-
ﬂicts in which two users want to edit the same resource simultaneously.
•
The necessary features (and no more). The Builder tool provides a powerful
set of features that meet the needs of the team, but Naughty Dog didn’t
waste any resources creating features they didn’t need.
•
Obvious mapping to source ﬁles. A user can very quickly determine which
source assets (native DCC ﬁles, like Maya .ma ﬁles or photoshop .psd
ﬁles) make up a particular resource.
•
Easy to change how DCC data is exported and processed. Just click on the
resource in question and twiddle its processing properties within the
resource database GUI.
•
Easy to build assets. Just type ba or bl followed by the resource name on
the command line. The dependency system takes care of the rest.
Of course, Naughty Dog’s tool chain does have some drawbacks as well, in-
cluding:
•
Lack of visualization tools. The only way to preview an asset is to load
it into the game or the model/animation viewer (which is really just a
special mode of the game itself).
•
The tools aren’t fully integrated. Naughty Dog uses one tool to lay out lev-
els, another to manage the majority of resources in the resource database,
and a third to set up materials and shaders (this is not part of the re-
source database front end). Building the assets is done on the command
line. It might be a bit more convenient if all of these functions were to
be integrated into a single tool. However, Naughty Dog has no plans
to do this, because the beneﬁt would probably not outweigh the costs
involved.
OGRE’s Resource Manager System
OGRE is a rendering engine, not a full-ﬂedged game engine. That said, OGRE
does boast a reasonably complete and very well-designed runtime resource
manager. A simple, consistent interface is used to load virtually any kind of
resource. And the system has been designed with extensibility in mind. Any
programmer can quite easily implement a resource manager for a brand new
kind of asset and integrate it easily into OGRE’s resource framework.

6.2. The Resource Manager
317
One of the drawbacks of OGRE’s resource manager is that it is a runtime-
only solution. OGRE lacks any kind of ofﬂine resource database. OGRE does
provide some exporters that are capable of converting a Maya ﬁle into a mesh
that can be used by OGRE (complete with materials, shaders, a skeleton and
optional animations).
However, the exporter must be run manually from
within Maya itself. Worse, all of the metadata describing how a particular
Maya ﬁle should be exported and processed must be entered by the user do-
ing the export.
In summary, OGRE’s runtime resource manager is powerful and well-
designed. But, OGRE would beneﬁt a great deal from an equally powerful and
modern resource database and asset conditioning pipeline on the tools side.
Microsoft’s XNA
XNA is a game development toolkit by Microsoft, targeted at the PC and Xbox
360 platforms. XNA’s resource management system is unique, in that it lever-
ages the project management and build systems of the Visual Studio IDE to
manage and build the assets in the game as well. XNA’s game development
tool, Game Studio Express, is just a plug-in to Visual Studio Express. You can
read more about Game Studio Express at http://msdn.microsoft.com/en-us/
library/bb203894.aspx.
6.2.1.4
The Asset Conditioning Pipeline
In Section 1.7, we learned that resource data is typically created using ad-
vanced digital content creation (DCC) tools like Maya, ZBrush, Photoshop or
Houdini. However, the data formats used by these tools are usually not suit-
able for direct consumption by a game engine. So the majority of resource
data is passed through an asset conditioning pipeline (ACP) on its way to the
game engine. The ACP is sometimes referred to as the resource conditioning
pipeline (RCP), or simply the tool chain.
Every resource pipeline starts with a collection of source assets in native
DCC formats (e.g., Maya .ma or .mb ﬁles, Photoshop .psd ﬁles, etc.). These
assets are typically passed through three processing stages on their way to the
game engine:
1.
Exporters. We need some way of getting the data out of the DCC’s na-
tive format and into a format that we can manipulate. This is usually
accomplished by writing a custom plug-in for the DCC in question. It
is the plug-in’s job to export the data into some kind of intermediate ﬁle
format that can be passed to later stages in the pipeline. Most DCC ap-
plications provide a reasonably convenient mechanism for doing this.

318
6. Resources and the File System
Maya actually provides three: a C++ SDK, a scripting language called
MEL and most recently a Python interface as well.
In cases where a DCC application provides no customization hooks, we
can always save the data in one of the DCC tool’s native formats. With
any luck, one of these will be an open format, a reasonably intuitive
text format, or some other format that we can reverse engineer. Presum-
ing this is the case, we can pass the ﬁle directly to the next stage of the
pipeline.
2.
Resource compilers. We often have to “massage” the raw data exported
from a DCC application in various ways in order to make them game-
ready. For example, we might need to rearrange a mesh’s triangles into
strips, or compress a texture bitmap, or calculate the arc lengths of the
segments of a Catmull-Rom spline. Not all types of resources need to
be compiled—some might be game-ready immediately upon being ex-
ported.
3.
Resource linkers. Multiple resource ﬁles sometimes need to be combined
into a single useful package prior to being loaded by the game engine.
This mimics the process of linking together the object ﬁles of a compiled
C++ program into an executable ﬁle, and so this process is sometimes
called resource linking. For example, when building a complex compos-
ite resource like a 3D model, we might need to combine the data from
multiple exported mesh ﬁles, multiple material ﬁles, a skeleton ﬁle and
multiple animation ﬁles into a single resource. Not all types of resources
need to be linked—some assets are game-ready after the export or com-
pile steps.
Resource Dependencies and Build Rules
Much like compiling the source ﬁles in a C or C++ project and then linking
them into an executable, the asset conditioning pipeline processes source as-
sets (in the form of Maya geometry and animation ﬁles, Photoshop PSD ﬁles,
raw audio clips, text ﬁles, etc.), converts them into game-ready form and then
links them together into a cohesive whole for use by the engine. And just like
the source ﬁles in a computer program, game assets often have interdepen-
dencies. (For example, a mesh refers to one or more materials, which in turn
refer to various textures.) These interdependencies typically have an impact
on the order in which assets must be processed by the pipeline. (For exam-
ple, we might need to build a character’s skeleton before we can process any
of that character’s animations.) In addition, the dependencies between assets
tell us which assets need to be rebuilt when a particular source asset changes.

6.2. The Resource Manager
319
Build dependencies revolve not only around changes to the assets them-
selves, but also around changes to data formats. If the format of the ﬁles used
to store triangle meshes changes, for instance, all meshes in the entire game
may need to be reexported and/or rebuilt. Some game engines employ data
formats that are robust to version changes. For example, an asset may contain
a version number, and the game engine may include code that “knows” how
to load and make use of legacy assets. The downside of such a policy is that
asset ﬁles and engine code tend to become bulky. When data format changes
are relatively rare, it may be better to just bite the bullet and reprocess all the
ﬁles when format changes do occur.
Every asset conditioning pipeline requires a set of rules that describe the
interdependencies between the assets, and some kind of build tool that can
use this information to ensure that the proper assets are built, in the proper
order, when a source asset is modiﬁed. Some game teams roll their own build
system. Others use an established tool, such as make. Whatever solution is
selected, teams should treat their build dependency system with utmost care.
If you don’t, changes to sources assets may not trigger the proper assets to
be rebuilt. The result can be inconsistent game assets, which may lead to
visual anomalies or even engine crashes. In my personal experience, I’ve wit-
nessed countness hours wasted in tracking down problems that could have
been avoided had the asset interdependencies been properly speciﬁed and
the build system implemented to use them reliably.
6.2.2
Runtime Resource Management
Let us turn our attention now to how the assets in our resource database are
loaded, managed and unloaded within the engine at runtime.
6.2.2.1
Responsibilities of the Runtime Resource Manager
A game engine’s runtime resource manager takes on a wide range of respon-
sibilities, all related to its primary mandate of loading resources into memory:
•
Ensures that only one copy of each unique resource exists in memory at
any given time.
•
Manages the lifetime of each resource.
•
Loads needed resources and unloads resources that are no longer needed.
•
Handles loading of composite resources. A composite resource is a re-
source comprised of other resources. For example, a 3D model is a com-
posite resource that consists of a mesh, one or more materials, one or
more textures and optionally a skeleton and multiple skeletal anima-
tions.

320
6. Resources and the File System
•
Maintains referential integrity. This includes internal referential integrity
(cross-references within a single resource) and external referential in-
tegrity (cross-references between resources). For example, a model refers
to its mesh and skeleton; a mesh refers to its materials, which in turn re-
fer to texture resources; animations refer to a skeleton, which ultimately
ties them to one or more models. When loading a composite resource,
the resource manager must ensure that all necessary subresources are
loaded, and it must patch in all of the cross-references properly.
•
Manages the memory usage of loaded resources and ensures that resources
are stored in the appropriate place(s) in memory.
•
Permits custom processing to be performed on a resource after it has been
loaded, on a per-resource-type basis. This process is sometimes known
as logging in or load-initializing the resource.
•
Usually (but not always) provides a single uniﬁed interface through which
a wide variety of resource types can be managed. Ideally a resource
manager is also easily extensible, so that it can handle new types of re-
sources as they are needed by the game development team.
•
Handles streaming (i.e., asynchronous resource loading), if the engine
supports this feature.
6.2.2.2
Resource File and Directory Organization
In some game engines (typically PC engines), each individual resource is man-
aged in a separate “loose” ﬁle on-disk. These ﬁles are typically contained
within a tree of directories whose internal organization is designed primar-
ily for the convenience of the people creating the assets; the engine typically
doesn’t care where resource ﬁles are located within the resource tree. Here’s a
typical resource directory tree for a hypothetical game called Space Evaders:
SpaceEvaders
Root directory for entire game.
Resources
Root of all resources.
NPC
Non-player character models and animations.
Pirate
Models and animations for pirates.
Marine
Models and animations for marines.
...
Player
Player character models and animations.

6.2. The Resource Manager
321
Weapons
Models and animations for weapons.
Pistol
Models and animations for the pistol.
Rifle
Models and animations for the riﬂe.
BFG
Models and animations for the big...uh...gun.
...
Levels
Background geometry and level layouts.
Level1
First level’s resources.
Level2
Second level’s resources.
...
Objects
Miscellaneous 3D objects.
Crate
The ubiquitous breakable crate.
Barrel
The ubiquitous exploding barrel.
Other engines package multiple resources together in a single ﬁle, such as
a ZIP archive, or some other composite ﬁle (perhaps of a proprietary format).
The primary beneﬁt of this approach is improved load times. When loading
data from ﬁles, the three biggest costs are seek times (i.e., moving the read head
to the correct place on the physical media), the time required to open each
individual ﬁle, and the time to read the data from the ﬁle into memory. Of
these, the seek times and ﬁle-open times can be nontrivial on many operating
systems. When a single large ﬁle is used, all of these costs are minimized. A
single ﬁle can be organized sequentially on the disk, reducing seek times to
a minimum. And with only one ﬁle to open, the cost of opening individual
resource ﬁles is eliminated.
Solid-state drives (SSD) do not suffer from the seek time problems that
plague spinning media like DVDs, Blu-ray discs and hard disc drives (HDD).
However, no game console to date includes a solid-state drive as the primary
ﬁxed storage device (not even the PS4 and Xbox One). So designing your
game’s I/O patterns in order to minimize seek times is likely to be a necessity
for some time to come.
The OGRE rendering engine’s resource manager permits resources to exist
as loose ﬁles on disk, or as virtual ﬁles within a large ZIP archive. The primary
beneﬁts of the ZIP format are the following:
1.
It is an open format. The zlib and zziplib libraries used to read and
write ZIP archives are freely available. The zlib SDK is totally free (see
http://www.zlib.net), while the zziplib SDK falls under the Lesser Gnu
Public License (LGPL) (see http://zziplib.sourceforge.net).
2.
The virtual ﬁles within a ZIP archive “remember” their relative paths. This
means that a ZIP archive “looks like” a raw ﬁle system for most in-

322
6. Resources and the File System
tents and purposes. The OGRE resource manager identiﬁes all resources
uniquely via strings that appear to be ﬁle system paths. However, these
paths sometimes identify virtual ﬁles within a ZIP archive instead of
loose ﬁles on disk, and a game programmer needn’t be aware of the
difference in most situations.
3.
ZIP archives may be compressed. This reduces the amount of disk space
occupied by resources. But, more importantly, it again speeds up load
times, as less data need be loaded into memory from the ﬁxed disk. This
is especially helpful when reading data from a DVD-ROM or Blu-ray
disk, as the data transfer rates of these devices are much slower than a
hard disk drive. Hence the cost of decompressing the data after it has
been loaded into memory is often more than offset by the time saved in
loading less data from the device.
4.
ZIP archives are modular. Resources can be grouped together into a ZIP
ﬁle and managed as a unit. One particularly elegant application of this
idea is in product localization. All of the assets that need to be localized
(such as audio clips containing dialogue and textures that contain words
or region-speciﬁc symbols) can be placed in a single ZIP ﬁle, and then
different versions of this ZIP ﬁle can be generated, one for each language
or region. To run the game for a particular region, the engine simply
loads the corresponding version of the ZIP archive.
Unreal Engine 3 takes a similar approach, with a few important differ-
ences. In Unreal, all resources must be contained within large composite ﬁles
known as packages (a.k.a. “pak ﬁles”). No loose disk ﬁles are permitted. The
format of a package ﬁle is proprietary. The Unreal Engine’s game editor, Un-
realEd, allows developers to create and manage packages and the resources
they contain.
6.2.2.3
Resource File Formats
Each type of resource ﬁle potentially has a different format. For example, a
mesh ﬁle is always stored in a different format than that of a texture bitmap.
Some kinds of assets are stored in standardized, open formats. For example,
textures are typically stored as Targa ﬁles (TGA), Portable Network Graphics
ﬁles (PNG), Tagged Image File Format ﬁles (TIFF), Joint Photographic Experts
Group ﬁles (JPEG) or Windows Bitmap ﬁles (BMP)—or in a standardized
compressed format such as DirectX’s S3 Texture Compression family of for-
mats (S3TC, also known as DXTn or DXTC). Likewise, 3D mesh data is often
exported out of a modeling tool like Maya or Lightwave into a standardized
format such as OBJ or COLLADA for consumption by the game engine.

6.2. The Resource Manager
323
Sometimes a single ﬁle format can be used to house many different types
of assets. For example, the Granny SDK by Rad Game Tools (http://www.
radgametools.com) implements a ﬂexible open ﬁle format that can be used to
store 3D mesh data, skeletal hierarchies and skeletal animation data. (In fact
the Granny ﬁle format can be easily repurposed to store virtually any kind of
data imaginable.)
Many game engine programmers roll their own ﬁle formats for various
reasons. This might be necessary if no standardized format provides all of
the information needed by the engine. Also, many game engines endeavor to
do as much ofﬂine processing as possible in order to minimize the amount of
time needed to load and process resource data at runtime. If the data needs to
conform to a particular layout in memory, for example, a raw binary format
might be chosen so that the data can be laid out by an ofﬂine tool (rather than
attempting to format it at runtime after the resource has been loaded).
6.2.2.4
Resource GUIDs
Every resource in a game must have some kind of globally unique identiﬁer
(GUID). The most common choice of GUID is the resource’s ﬁle system path
(stored either as a string or a 32-bit hash). This kind of GUID is intuitive,
because it clearly maps each resource to a physical ﬁle on-disk. And it’s guar-
anteed to be unique across the entire game, because the operating system al-
ready guarantees that no two ﬁles will have the same path.
However, a ﬁle system path is by no means the only choice for a resource
GUID. Some engines use a less-intuitive type of GUID, such as a 128-bit hash
code, perhaps assigned by a tool that guarantees uniqueness. In other engines,
using a ﬁle system path as a resource identiﬁer is infeasible. For example, Un-
real Engine 3 stores many resources in a single large ﬁle known as a package,
so the path to the package ﬁle does not uniquely identify any one resource.
To overcome this problem, an Unreal package ﬁle is organized into a folder
hierarchy containing individual resources. Unreal gives each individual re-
source within a package a unique name, which looks much like a ﬁle system
path. So in Unreal, a resource GUID is formed by concatenating the (unique)
name of the package ﬁle with the in-package path of the resource in question.
For example, the Gears of War resource GUID Locust_Boomer.Physical-
Materials.LocustBoomerLeather identiﬁes a material called Locust-
BoomerLeather within the PhysicalMaterials folder of the Locust_-
Boomer package ﬁle.
6.2.2.5
The Resource Registry
In order to ensure that only one copy of each unique resource is loaded into
memory at any given time, most resource managers maintain some kind of

324
6. Resources and the File System
registry of loaded resources. The simplest implementation is a dictionary—i.e.,
a collection of key-value pairs. The keys contain the unique ids of the resources,
while the values are typically pointers to the resources in memory.
Whenever a resource is loaded into memory, an entry for it is added to the
resource registry dictionary, using its GUID as the key. Whenever a resource
is unloaded, its registry entry is removed. When a resource is requested by
the game, the resource manager looks up the resource by its GUID within
the resource registry. If the resource can be found, a pointer to it is simply
returned. If the resource cannot be found, it can either be loaded automatically
or a failure code can be returned.
At ﬁrst blush, it might seem most intuitive to automatically load a re-
quested resource if it cannot be found in the resource registry. And in fact,
some game engines do this. However, there are some serious problems with
this approach. Loading a resource is a slow operation, because it involves lo-
cating and opening a ﬁle on disk, reading a potentially large amount of data
into memory (from a potentially slow device like a DVD-ROM drive), and
also possibly performing post-load initialization of the resource data once it
has been loaded. If the request comes during active gameplay, the time it takes
to load the resource might cause a very noticeable hitch in the game’s frame
rate, or even a multi-second freeze. For this reason, engines tend to take one
of two alternative approaches:
1.
Resource loading might be disallowed completely during active game-
play. In this model, all of the resources for a game level are loaded en
masse just prior to gameplay, usually while the player watches a loading
screen or progress bar of some kind.
2.
Resource loading might be done asynchronously (i.e., the data might be
streamed). In this model, while the player is engaged in level X, the re-
sources for level Y are being loaded in the background. This approach
is preferable because it provides the player with a load-screen-free play
experience. However, it is considerably more difﬁcult to implement.
6.2.2.6
Resource Lifetime
The lifetime of a resource is deﬁned as the time period between when it is ﬁrst
loaded into memory and when its memory is reclaimed for other purposes.
One of the resource manager’s jobs is to manage resource lifetimes—either
automatically or by providing the necessary API functions to the game, so it
can manage resource lifetimes manually.

6.2. The Resource Manager
325
Each resource has its own lifetime requirements:
•
Some resources must be loaded when the game ﬁrst starts up and must
stay resident in memory for the entire duration of the game. That is,
their lifetimes are effectively inﬁnite. These are sometimes called global
resources or global assets. Typical examples include the player character’s
mesh, materials, textures and core animations, textures and fonts used
on the heads-up display, and the resources for all of the standard-issue
weapons used throughout the game. Any resource that is visible or au-
dible to the player throughout the entire game (and cannot be loaded on
the ﬂy when needed) should be treated as a global resource.
•
Other resources have a lifetime that matches that of a particular game
level. These resources must be in memory by the time the level is ﬁrst
seen by the player and can be dumped once the player has permanently
left the level.
•
Some resources might have a lifetime that is shorter than the duration of
the level in which they are found. For example, the animations and au-
dio clips that make up an in-game cinematic (a mini-movie that advances
the story or provides the player with important information) might be
loaded in advance of the player seeing the cinematic and then dumped
once the cinematic has played.
•
Some resources like background music, ambient sound effects or full-
screen movies are streamed “live” as they play. The lifetime of this kind
of resource is difﬁcult to deﬁne, because each byte only persists in mem-
ory for a tiny fraction of a second, but the entire piece of music sounds
like it lasts for a long period of time. Such assets are typically loaded in
chunks of a size that matches the underlying hardware’s requirements.
For example, a music track might be read in 4 KiB chunks, because that
might be the buffer size used by the low-level sound system. Only two
chunks are ever present in memory at any given moment—the chunk
that is currently playing and the chunk immediately following it that is
being loaded into memory.
The question of when to load a resource is usually answered quite easily,
based on knowledge of when the asset is ﬁrst seen by the player. However, the
question of when to unload a resource and reclaim its memory is not so easily
answered. The problem is that many resources are shared across multiple
levels. We don’t want to unload a resource when level X is done, only to
immediately reload it because level Y needs the same resource.
One solution to this problem is to reference-count the resources. Whenever
a new game level needs to be loaded, the list of all resources used by that

326
6. Resources and the File System
Event
A
B
C
D
E
Initial state
0
0
0
0
0
Level X counts incremented
1
1
1
0
0
Level X loads
(1)
(1)
(1)
0
0
Level X plays
1
1
1
0
0
Level Y counts incremented
1
2
2
1
1
Level X counts decremented
0
1
1
1
1
Level X unloads, level Y loads
(0)
1
1
(1)
(1)
Level Y plays
0
1
1
1
1
Table 6.2. Resource usage as two levels load and unload.
level is traversed, and the reference count for each resource is incremented
by one (but they are not loaded yet). Next, we traverse the resources of any
unneeded levels and decrement their reference counts by one; any resource
whose reference count drops to zero is unloaded. Finally, we run through the
list of all resources whose reference count just went from zero to one and load
those assets into memory.
For example, imagine that level X uses resources A, B and C, and that level
Y uses resources B, C, D and E. (B and C are shared between both levels.) Ta-
ble 6.2 shows the reference counts of these ﬁve resources as the player plays
through levels X and Y. In this table, reference counts are shown in boldface
type to indicate that the corresponding resource actually exists in memory,
while a grey background indicates that the resource is not in memory. A ref-
erence count in parentheses indicates that the corresponding resource data is
being loaded or unloaded.
6.2.2.7
Memory Management for Resources
Resource management is closely related to memory management, because we
must inevitably decide where the resources should end up in memory once
they have been loaded. The destination of every resource is not always the
same. For one thing, certain types of resources must reside in video RAM
(or, on the PlayStation 4, in a memory block that has been mapped for access
via the high-speed “garlic” bus). Typical examples include textures, vertex
buffers, index buffers and shader code. Most other resources can reside in
main RAM, but different kinds of resources might need to reside within dif-
ferent address ranges. For example, a resource that is loaded and stays resi-
dent for the entire game (global resources) might be loaded into one region of
memory, while resources that are loaded and unloaded frequently might go
somewhere else.

6.2. The Resource Manager
327
The design of a game engine’s memory allocation subsystem is usually
closely tied to that of its resource manager. Sometimes we will design the
resource manager to take best advantage of the types of memory allocators
we have available, or vice versa—we may design our memory allocators to
suit the needs of the resource manager.
As we saw in Section 5.2.1.4, one of the primary problems facing any re-
source management system is the need to avoid fragmenting memory as re-
sources are loaded and unloaded. We’ll discuss a few of the more-common
solutions to this problem below.
Heap-Based Resource Allocation
One approach is to simply ignore memory fragmentation issues and use a
general-purpose heap allocator to allocate your resources (like the one imple-
mented by malloc() in C, or the global new operator in C++). This works
best if your game is only intended to run on personal computers, on operat-
ing systems that support virtual memory allocation. On such a system, phys-
ical memory will become fragmented, but the operating system’s ability to
map noncontiguous pages of physical RAM into a contiguous virtual mem-
ory space helps to mitigate some of the effects of fragmentation.
If your game is running on a console with limited physical RAM and only
a rudimentary virtual memory manager (or none whatsoever), then fragmen-
tation will become a problem. In this case, one alternative is to defragment
your memory periodically. We saw how to do this in Section 5.2.2.2.
Stack-Based Resource Allocation
A stack allocator does not suffer from fragmentation problems, because mem-
ory is allocated contiguously and freed in an order opposite to that in which it
was allocated. A stack allocator can be used to load resources if the following
two conditions are met:
•
The game is linear and level-centric (i.e., the player watches a loading
screen, then plays a level, then watches another loading screen, then
plays another level).
•
Each level ﬁts into memory in its entirety.
Presuming that these requirements are satisﬁed, we can use a stack allocator to
load resources as follows: When the game ﬁrst starts up, the global resources
are allocated ﬁrst. The top of the stack is then marked, so that we can free back
to this position later. To load a level, we simply allocate its resources on the

328
6. Resources and the File System
Load LSR data, then obtain marker.
Load-and-
stay-resident
(LSR) data
Load level A.
LSR data
Level A’s
resources
Unload level A, free back to marker.
LSR data
Load level B.
LSR data
Level B’s
resources
Figure 6.3. Loading resources using a stack allocator.
top of the stack. When the level is complete, we can simply set the stack top
back to the marker we took earlier, thereby freeing all of the level’s resources
in one fell swoop without disturbing the global resources. This process can
be repeated for any number of levels, without ever fragmenting memory. Fig-
ure 6.3 illustrates how this is accomplished.
A double-ended stack allocator can be used to augment this approach.
Two stacks are deﬁned within a single large memory block. One grows up
from the bottom of the memory area, while the other grows down from the
top. As long as the two stacks never overlap, the stacks can trade memory
resources back and forth naturally—something that wouldn’t be possible if
each stack resided in its own ﬁxed size block.
On Hydro Thunder, Midway used a double-ended stack allocator.
The
lower stack was used for persistent data loads, while the upper was used for
temporary allocations that were freed every frame. Another way a double-

6.2. The Resource Manager
329
ended stack allocator can be used is to ping-pong level loads. Such an ap-
proach was used at Bionic Games, Inc. for one of their projects. The basic idea
is to load a compressed version of level B into the upper stack, while the cur-
rently active level A resides (in uncompressed form) in the lower stack. To
switch from level A to level B, we simply free level A’s resources (by clear-
ing the lower stack) and then decompress level B from the upper stack into
the lower stack. Decompression is generally much faster than loading data
from disk, so this approach effectively eliminates the load time that would
otherwise be experienced by the player between levels.
Pool-Based Resource Allocation
Another resource allocation technique that is common in game engines that
support streaming is to load resource data in equally sized chunks. Because
the chunks are all the same size, they can be allocated using a pool allocator (see
Section 5.2.1.2). When resources are later unloaded, the chunks can be freed
without causing fragmentation.
Of course, a chunk-based allocation approach requires that all resource
data be laid out in a manner that permits division into equally sized chunks.
We cannot simply load an arbitrary resource ﬁle in chunks, because the ﬁle
might contain a contiguous data structure like an array or a very large struct
that is larger than a single chunk. For example, if the chunks that contain
an array are not arranged sequentially in RAM, the continuity of the array
will be lost, and array indexing will cease to function properly. This means
that all resource data must be designed with “chunkiness” in mind. Large
contiguous data structures must be avoided in favor of data structures that are
either small enough to ﬁt within a single chunk or do not require contiguous
RAM to function properly (e.g., linked lists).
Each chunk in the pool is typically associated with a particular game level.
(One simple way to do this is to give each level a linked list of its chunks.) This
allows the engine to manage the lifetimes of each chunk appropriately, even
when multiple levels with different life spans are in memory concurrently.
For example, when level X is loaded, it might allocate and make use of N
chunks. Later, level Y might allocate an additional M chunks. When level X
is eventually unloaded, its N chunks are returned to the free pool. If level Y
is still active, its M chunks need to remain in memory. By associating each
chunk with a speciﬁc level, the lifetimes of the chunks can be managed easily
and efﬁciently. This is illustrated in Figure 6.4.
One big trade-off inherent in a “chunky” resource allocation scheme is
wasted space. Unless a resource ﬁle’s size is an exact multiple of the chunk

330
6. Resources and the File System
File A
Chunk 1
File A
Chunk 2
File A
Chunk 3
File B
Chunk 1
File B
Chunk 2
File C
Chunk 1
File C
Chunk 2
File C
Chunk 3
File C
Chunk 4
File D
Chunk 1
File D
Chunk 2
File D
Chunk 3
File E
Chunk 1
File E
Chunk 2
File E
Chunk 3
File E
Chunk 4
File E
Chunk 5
File E
Chunk 6
Level X
(files A, D)
Level Y
(files B, C, E)
Figure 6.4. Chunky allocation of resources for levels X and Y.
size, the last chunk in a ﬁle will not be fully utilized (see Figure 6.5). Choos-
ing a smaller chunk size can help to mitigate this problem, but the smaller the
chunks, the more onerous the restrictions on the layout of the resource data.
(As an extreme example, if a chunk size of one byte were selected, then no
data structure could be larger than a single byte—clearly an untenable situ-
ation.) A typical chunk size is on the order of a few kibibytes. For example
at Naughty Dog, we use a chunky resource allocator as part of our resource
streaming system, and our chunks are 512 KiB in size on the PS3 and 1 MiB on
the PS4. You may also want to consider selecting a chunk size that is a mul-
tiple of the operating system’s I/O buffer size to maximize efﬁciency when
loading individual chunks.
Figure 6.5. The last chunk of a resource ﬁle is often not fully utilized.
Resource Chunk Allocators
One way to limit the effects of wasted chunk memory is to set up a special
memory allocator that can utilize the unused portions of chunks. As far as
I’m aware, there is no standardized name for this kind of allocator, but we
will call it a resource chunk allocator for lack of a better name.
A resource chunk allocator is not particularly difﬁcult to implement. We
need only maintain a linked list of all chunks that contain unused memory,

6.2. The Resource Manager
331
along with the locations and sizes of each free block. We can then allocate
from these free blocks in any way we see ﬁt. For example, we might manage
the linked list of free blocks using a general-purpose heap allocator. Or we
might map a small stack allocator onto each free block; whenever a request
for memory comes in, we could then scan the free blocks for one whose stack
has enough free RAM and then use that stack to satisfy the request.
Unfortunately, there’s a rather grotesque-looking ﬂy in our ointment here.
If we allocate memory in the unused regions of our resource chunks, what
happens when those chunks are freed? We cannot free part of a chunk—it’s
an all or nothing proposition. So any memory we allocate within an unused
portion of a resource chunk will magically disappear when that resource is
unloaded.
A simple solution to this problem is to only use our free-chunk allocator for
memory requests whose lifetimes match the lifetime of the level with which a
particular chunk is associated. In other words, we should only allocate mem-
ory out of level A’s chunks for data that is associated exclusively with level
A and only allocate from B’s chunks memory that is used exclusively by level
B. This requires our resource chunk allocator to manage each level’s chunks
separately. And it requires the users of the chunk allocator to specify which
level they are allocating for, so that the correct linked list of free blocks can be
used to satisfy the request.
Thankfully, most game engines need to allocate memory dynamically when
loading resources, over and above the memory required for the resource ﬁles
themselves. So a resource chunk allocator can be a fruitful way to reclaim
chunk memory that would otherwise have been wasted.
Sectioned Resource Files
Another useful idea that is related to “chunky” resource ﬁles is the concept
of ﬁle sections. A typical resource ﬁle might contain between one and four
sections, each of which is divided into one or more chunks for the purposes
of pool allocation as described above. One section might contain data that
is destined for main RAM, while another section might contain video RAM
data. Another section could contain temporary data that is needed during
the loading process but is discarded once the resource has been completely
loaded. Yet another section might contain debugging information. This debug
data could be loaded when running the game in debug mode, but not loaded
at all in the ﬁnal production build of the game. The Granny SDK’s ﬁle system
(http://www.radgametools.com) is an excellent example of how to implement
ﬁle sectioning in a simple and ﬂexible manner.

332
6. Resources and the File System
Mesh 1
Material 1
Mesh 2
Material 2
Skeleton 1
Anim 1
Anim 2
Anim 3
Texture 1
File 1
Texture 2
File 2
Texture 3
File 3
Anim 4
Anim 5
Anim 6
File 4
File 5
File 6
= internal cross-reference
= external cross-reference
Legend
= file boundary
Figure 6.6. Example of a resource database dependency graph.
6.2.2.8
Composite Resources and Referential Integrity
Usually a game’s resource database consists of multiple resource ﬁles, each ﬁle
containing one or more data objects. These data objects can refer to and depend
upon one another in arbitrary ways. For example, a mesh data structure might
contain a reference to its material, which in turn contains a list of references to
textures. Usually cross-references imply dependency (i.e., if resource A refers
to resource B, then both A and B must be in memory in order for the resources
to be functional in the game.) In general, a game’s resource database can be
represented by a directed graph of interdependent data objects.
Cross-references between data objects can be internal (a reference between
two objects within a single ﬁle) or external (a reference to an object in a dif-
ferent ﬁle). This distinction is important because internal and external cross-
references are often implemented differently. When visualizing a game’s re-
source database, we can draw dotted lines surrounding individual resource
ﬁles to make the internal/external distinction clear—any edge of the graph
that crosses a dotted line ﬁle boundary is an external reference, while edges
that do not cross ﬁle boundaries are internal. This is illustrated in Figure 6.6.
We sometimes use the term composite resource to describe a self-sufﬁcient
cluster of interdependent resources. For example, a model is a composite re-
source consisting of one or more triangle meshes, an optional skeleton and an

6.2. The Resource Manager
333
optional collection of animations. Each mesh is mapped with a material, and
each material refers to one or more textures. To fully load a composite re-
source like a 3D model into memory, all of its dependent resources must be
loaded as well.
6.2.2.9
Handling Cross-References between Resources
One of the more-challenging aspects of implementing a resource manager
is managing the cross-references between resource objects and guaranteeing
that referential integrity is maintained. To understand how a resource man-
ager accomplishes this, let’s look at how cross-references are represented in
memory, and how they are represented on-disk.
In C++, a cross-reference between two data objects is usually implemented
via a pointer or a reference. For example, a mesh might contain the data mem-
ber Material* m_pMaterial (a pointer) or Material& m_material (a
reference) in order to refer to its material. However, pointers are just memory
addresses—they lose their meaning when taken out of the context of the run-
ning application. In fact, memory addresses can and do change even between
subsequent runs of the same application. Clearly when storing data to a disk
ﬁle, we cannot use pointers to describe inter-object dependencies.
GUIDs as Cross-References
One good approach is to store each cross-reference as a string or hash code
containing the unique id of the referenced object. This implies that every re-
source object that might be cross-referenced must have a globally unique iden-
tiﬁer or GUID.
To make this kind of cross-reference work, the runtime resource man-
ager maintains a global resource look-up table. Whenever a resource object
is loaded into memory, a pointer to that object is stored in the table with its
GUID as the look-up key. After all resource objects have been loaded into
memory and their entries added to the table, we can make a pass over all of
the objects and convert all of their cross-references into pointers, by looking
up the address of each cross-referenced object in the global resource look-up
table via that object’s GUID.
Pointer Fix-Up Tables
Another approach that is often used when storing data objects into a binary
ﬁle is to convert the pointers into ﬁle offsets. Consider a group of C structs or
C++ objects that cross-reference each other via pointers. To store this group
of objects into a binary ﬁle, we need to visit each object once (and only once)

334
6. Resources and the File System
Addresses:
Offsets:
0x0
0x240
0x4A0
0x7F0
0x2A080
0x2D750
0x2F110
0x32EE0
Figure 6.7. In-memory object images become contiguous when saved into a binary ﬁle.
in an arbitrary order and write each object’s memory image into the ﬁle se-
quentially. This has the effect of serializing the objects into a contiguous image
within the ﬁle, even when their memory images are not contiguous in RAM.
This is shown in Figure 6.7.
Because the objects’ memory images are now contiguous within the ﬁle,
we can determine the offset of each object’s image relative to the beginning of
the ﬁle. During the process of writing the binary ﬁle image, we locate every
pointer within every data object, convert each pointer into an offset and store
those offsets into the ﬁle in place of the pointers. We can simply overwrite the
pointers with their offsets, because the offsets never require more bits to store
than the original pointers. In effect, an offset is the binary ﬁle equivalent of a
pointer in memory. (Do be aware of the differences between your development
platform and your target platform. If you write out a memory image on a 64-
bit Windows machine, its pointers will all be 64 bits wide and the resulting
ﬁle won’t be compatible with a 32-bit console.)
Of course, we’ll need to convert the offsets back into pointers when the
ﬁle is loaded into memory some time later. Such conversions are known as
pointer ﬁx-ups. When the ﬁle’s binary image is loaded, the objects contained in
the image retain their contiguous layout, so it is trivial to convert an offset into
a pointer. We merely add the offset to the address of the ﬁle image as a whole.
This is demonstrated by the code snippet below and illustrated in Figure 6.8.
U8* ConvertOffsetToPointer(U32 objectOffset,
U8* pAddressOfFileImage)

6.2. The Resource Manager
335
{
U8* pObject = pAddressOfFileImage + objectOffset;
return pObject;
}
The problem we encounter when trying to convert pointers into offsets,
and vice versa, is how to ﬁnd all of the pointers that require conversion. This
problem is usually solved at the time the binary ﬁle is written. The code that
writes out the images of the data objects has knowledge of the data types and
classes being written, so it has knowledge of the locations of all the pointers
within each object. The locations of the pointers are stored into a simple ta-
ble known as a pointer ﬁx-up table. This table is written into the binary ﬁle
along with the binary images of all the objects. Later, when the ﬁle is loaded
into RAM again, the table can be consulted in order to ﬁnd and ﬁx up every
pointer. The table itself is just a list of offsets within the ﬁle—each offset rep-
resents a single pointer that requires ﬁxing up. This is illustrated in Figure 6.9.
Storing C++ Objects as Binary Images: Constructors
One important step that is easy to overlook when loading C++ objects from a
binary ﬁle is to ensure that the objects’ constructors are called. For example,
Addresses:
Offsets:
0x0
0x240
0x4A0
0x7F0
0x30100
0x30340
0x305A0
0x308F0
Figure 6.8. Contiguous resource ﬁle image, after it has
been loaded into RAM.
Addresses:
Offsets:
Object 1
Object 2
Object 3
Object 4
0x0
0x240
0x4A0
0x7F0
Object 1
Object 4
Object 2
Object 3
0x2A080
0x2D750
0x2F110
0x32EE0
0x32EE0
0x2F110
0x2A080
0x4A0
0x240
0x0
Pointers converted 
to offsets; locations 
of pointers stored in 
fix-up table.
Fix-Up Table
0x200
0x340
0x810
Pointers to various 
objects are present.
3 pointers
Figure 6.9. A pointer ﬁx-up table.

336
6. Resources and the File System
if we load a binary image containing three objects—an instance of class A, an
instance of class B, and an instance of class C—then we must make sure that
the correct constructor is called on each of these three objects.
There are two common solutions to this problem. First, you can simply
decide not to support C++ objects in your binary ﬁles at all. In other words,
restrict yourself to plain old data structures (abbreviated PODS or POD)—
i.e., C structs and C++ structs and classes that contain no virtual functions and
trivial do-nothing constructors. (See http://en.wikipedia.org/wiki/Plain_Old_
Data_Structures for a more complete discussion of PODS.)
Second, you can save off a table containing the offsets of all non-PODS
objects in your binary image along with some indication of which class each
object is an instance of. Then, once the binary image has been loaded, you can
iterate through this table, visit each object and call the appropriate construc-
tor using placement new syntax (i.e., calling the constructor on a preallocated
block of memory). For example, given the offset to an object within the binary
image, we might write:
void* pObject = ConvertOffsetToPointer(objectOffset,
pAddressOfFileImage);
::new(pObject) ClassName; // placement new syntax
where ClassName is the class of which the object is an instance.
Handling External References
The two approaches described above work very well when applied to re-
sources in which all of the cross-references are internal—i.e., they only refer-
ence objects within a single resource ﬁle. In this simple case, you can load the
binary image into memory and then apply the pointer ﬁx-ups to resolve all
the cross-references. But when cross-references reach out into other resource
ﬁles, a slightly augmented approach is required.
To successfully represent an external cross-reference, we must specify not
only the offset or GUID of the data object in question, but also the path to the
resource ﬁle in which the referenced object resides.
The key to loading a multi-ﬁle composite resource is to load all of the in-
terdependent ﬁles ﬁrst. This can be done by loading one resource ﬁle and
then scanning through its table of cross-references and loading any externally
referenced ﬁles that have not already been loaded. As we load each data ob-
ject into RAM, we can add the object’s address to the master look-up table.
Once all of the interdependent ﬁles have been loaded and all of the objects are
present in RAM, we can make a ﬁnal pass to ﬁx up all of the pointers using
the master look-up table to convert GUIDs or ﬁle offsets into real addresses.

6.2. The Resource Manager
337
6.2.2.10
Post-Load Initialization
Ideally, each and every resource would be completely prepared by our ofﬂine
tools, so that it is ready for use the moment it has been loaded into memory.
Practically speaking, this is not always possible. Many types of resources re-
quire at least some “massaging” after having been loaded in order to prepare
them for use by the engine. In this book, I will use the term post-load initializa-
tion to refer to any processing of resource data after it has been loaded. Other
engines may use different terminology. (For example, at Naughty Dog we call
this logging in a resource.) Most resource managers also support some kind of
tear-down step prior to a resource’s memory being freed. (At Naughty Dog,
we call this logging out a resource.)
Post-load initialization generally comes in one of two varieties:
•
In some cases, post-load initialization is an unavoidable step. For ex-
ample, on a PC, the vertices and indices that describe a 3D mesh are
loaded into main RAM, but they must be transferred into video RAM
before they can be rendered. This can only be accomplished at runtime,
by creating a Direct X vertex buffer or index buffer, locking it, copying
or reading the data into the buffer and then unlocking it.
•
In other cases, the processing done during post-load initialization is
avoidable (i.e., could be moved into the tools), but is done for conve-
nience or expedience. For example, a programmer might want to add
the calculation of accurate arc lengths to our engine’s spline library.
Rather than spend the time to modify the tools to generate the arc length
data, the programmer might simply calculate it at runtime during post-
load initialization. Later, when the calculations are perfected, this code
can be moved into the tools, thereby avoiding the cost of doing the cal-
culations at runtime.
Clearly, each type of resource has its own unique requirements for post-
load initialization and tear-down.
So, resource managers typically permit
these two steps to be conﬁgurable on a per-resource-type basis. In a non-
object-oriented language like C, we can envision a look-up table that maps
each type of resource to a pair of function pointers, one for post-load initial-
ization and one for tear-down. In an object-oriented language like C++, life is
even easier—we can make use of polymorphism to permit each class to han-
dle post-load initialization and tear-down in a unique way.
In C++, post-load initialization could be implemented as a special con-
structor, and tear-down could be done in the class’ destructor. However, there
are some problems with using constructors and destructors for this purpose.

338
6. Resources and the File System
For example, one typically needs to construct all loaded objects ﬁrst, then ap-
ply pointer ﬁx-ups, and ﬁnally perform post-load initialization as a separate
step. As such, most developers defer post-load initialization and tear-down
to plain old virtual functions. For example, we might choose to use a pair of
virtual functions named something sensible like Init() and Destroy().
Post-load initialization is closely related to a resource’s memory alloca-
tion strategy, because new data is often generated by the initialization rou-
tine. In some cases, the data generated by the post-load initialization step
augments the data loaded from the ﬁle. (For example, if we are calculating
the arc lengths of the segments of a Catmull-Rom spline curve after it has
been loaded, we would probably want to allocate some additional memory in
which to store the results.) In other cases, the data generated during post-
load initialization replaces the loaded data. (For example, we might allow
mesh data in an older out-of-date format to be loaded and then automati-
cally converted into the latest format for backwards compatibility reasons.) In
this case, the loaded data may need to be discarded, either partially or in its
entirety, after the post-load step has generated the new data.
The Hydro Thunder engine had a simple but powerful way of handling this.
It would permit resources to be loaded in one of two ways: (a) directly into
its ﬁnal resting place in memory or (b) into a temporary area of memory. In
the latter case, the post-load initialization routine was responsible for copy-
ing the ﬁnalized data into its ultimate destination; the temporary copy of the
resource would be discarded after post-load initialization was complete. This
was very useful for loading resource ﬁles that contained both relevant and
irrelevant data. The relevant data would be copied into its ﬁnal destination
in memory, while the irrelevant data would be discarded. For example, mesh
data in an out-of-date format could be loaded into temporary memory and
then converted into the latest format by the post-load initialization routine,
without having to waste any memory keeping the old-format data kicking
around.

7
The Game Loop and
Real-Time Simulation
G
ames are real-time, dynamic, interactive computer simulations. As such,
time plays an incredibly important role in any electronic game. There are
many different kinds of time to deal with in a game engine—real time, game
time, the local timeline of an animation, the actual CPU cycles spent within
a particular function, and the list goes on. Every engine system might deﬁne
and manipulate time differently. We must have a solid understanding of all
the ways time can be used in a game. In this chapter, we’ll take a look at how
real-time, dynamic simulation software works and explore the common ways
in which time plays a role in such a simulation.
7.1
The Rendering Loop
In a graphical user interface (GUI), of the sort found on a Windows PC or a
Macintosh, the majority of the screen’s contents are static. Only a small part
of any one window is actively changing appearance at any given moment.
Because of this, graphical user interfaces have traditionally been drawn on-
screen via a technique known as rectangle invalidation, in which only the small
portions of the screen whose contents have actually changed are redrawn.
Older 2D video games used similar techniques to minimize the number of
pixels that needed to be drawn.
339

340
7. The Game Loop and Real-Time Simulation
Real-time 3D computer graphics are implemented in an entirely different
way. As the camera moves about in a 3D scene, the entire contents of the screen
or window change continually, so the concept of invalid rectangles no longer
applies. Instead, an illusion of motion and interactivity is produced in much
the same way that a movie produces it—by presenting the viewer with a series
of still images in rapid succession.
Obviously, producing a rapid succession of still images on-screen requires
a loop. In a real-time rendering application, this is sometimes known as the
render loop. At its simplest, a rendering loop is structured as follows:
while (!quit)
{
// Update the camera transform based on interactive
// inputs or by following a predefined path.
updateCamera();
// Update positions, orientations and any other
// relevant visual state of any dynamic elements
// in the scene.
updateSceneElements();
// Render a still frame into an off-screen frame
// buffer known as the "back buffer".
renderScene();
// Swap the back buffer with the front buffer, making
// the most recently rendered image visible
// on-screen. (Or, in windowed mode, copy (blit) the
// back buffer's contents to the front buffer.
swapBuffers();
}
7.2
The Game Loop
A game is composed of many interacting subsystems, including device I/O,
rendering, animation, collision detection and resolution, optional rigid body
dynamics simulation, multiplayer networking, audio, and the list goes on.
Most game engine subsystems require periodic servicing while the game is
running. However, the rate at which these subsystems need to be serviced
varies from subsystem to subsystem. Animation typically needs to be up-
dated at a rate of 30 or 60 Hz, in synchronization with the rendering subsys-
tem. However, a dynamics (physics) simulation may actually require more

7.2. The Game Loop
341
frequent updates (e.g., 120 Hz). Higher-level systems, like AI, might only
need to be serviced once or twice per second, and they needn’t necessarily be
synchronized with the rendering loop at all.
There are a number of ways to implement the periodic updating of our
game engine subsystems. We’ll explore some of the possible architectures in
a moment. But for the time being, let’s stick with the simplest way to update
our engine’s subsystems—using a single loop to update everything. Such a
loop is often called the game loop, because it is the master loop that services
every subsystem in the engine.
7.2.1
A Simple Example: Pong
Pong is a well-known genre of table tennis video games that got its start in
1958, in the form of an analog computer game called Tennis for Two, created
by William A. Higinbotham at the Brookhaven National Laboratory and dis-
played on an oscilloscope. The genre is best known by its later incarnations on
digital computers—the Magnavox Oddysey game Table Tennis and the Atari
arcade game Pong.
In pong, a ball bounces back and forth between two movable vertical pad-
dles and two ﬁxed horizontal walls. The human players control the positions
of the paddles via control wheels. (Modern re-implementations allow control
via a joystick, the keyboard or some other human interface device.) If the ball
passes by a paddle without striking it, the other team wins the point and the
ball is reset for a new round of play.
The following pseudocode demonstrates what the game loop of a pong
game might look like at its core:
void main() // Pong
{
initGame();
while (true) // game loop
{
readHumanInterfaceDevices();
if (quitButtonPressed())
{
break; // exit the game loop
}
movePaddles();
moveBall();

342
7. The Game Loop and Real-Time Simulation
collideAndBounceBall();
if (ballImpactedSide(LEFT_PLAYER))
{
incremenentScore(RIGHT_PLAYER);
resetBall();
}
else if (ballImpactedSide(RIGHT_PLAYER))
{
incrementScore(LEFT_PLAYER);
resetBall();
}
renderPlayfield();
}
}
Clearly this example is somewhat contrived. The original pong games
were certainly not implemented by redrawing the entire screen at a rate of
30 frames per second. Back then, CPUs were so slow that they could barely
muster the power to draw two lines for the paddles and a box for the ball
in real time. Specialized 2D sprite hardware was often used to draw moving
objects on-screen. However, we’re only interested in the concepts here, not
the implementation details of the original Pong.
As you can see, when the game ﬁrst runs, it calls initGame() to do
whatever set-up might be required by the graphics system, human I/O de-
vices, audio system, etc. Then the main game loop is entered. The state-
ment while (true) tells us that the loop will continue forever, unless inter-
rupted internally. The ﬁrst thing we do inside the loop is to read the human
interface device(s). We check to see whether either human player pressed
the “quit” button—if so, we exit the game via a break statement.
Next,
the positions of the paddles are adjusted slightly upward or downward in
movePaddles(), based on the current deﬂection of the control wheels, joy-
sticks or other I/O devices. The function moveBall() adds the ball’s current
velocity vector to its position in order to ﬁnd its new position next frame.
In collideAndBounceBall(), this position is then checked for collisions
against both the ﬁxed horizontal walls and the paddles. If collisions are de-
tected, the ball’s position is recalculated to account for any bounce. We also
note whether the ball impacted either the left or right edge of the screen. This
means that it missed one of the paddles, in which case we increment the other
player’s score and reset the ball for the next round. Finally, renderPlay-
field() draws the entire contents of the screen.

7.3. Game Loop Architectural Styles
343
7.3
Game Loop Architectural Styles
Game loops can be implemented in a number of different ways—but at their
core, they usually boil down to one or more simple loops, with various em-
bellishments. We’ll explore a few of the more common architectures below.
7.3.1
Windows Message Pumps
On a Windows platform, games need to service messages from the Windows
operating system in addition to servicing the various subsystems in the game
engine itself. Windows games therefore contain a chunk of code known as a
message pump. The basic idea is to service Windows messages whenever they
arrive and to service the game engine only when no Windows messages are
pending. A message pump typically looks something like this:
while (true)
{
// Service any and all pending Windows messages.
MSG msg;
while (PeekMessage(&msg, NULL, 0, 0) > 0)
{
TranslateMessage(&msg);
DispatchMessage(&msg);
}
// No more Windows messages to process -- run one
// iteration of our "real" game loop.
RunOneIterationOfGameLoop();
}
One of the side-effects of implementing the game loop like this is that Win-
dows messages take precedence over rendering and simulating the game. As
a result, the game will temporarily freeze whenever you resize or drag the
game’s window around on the desktop.
7.3.2
Callback-Driven Frameworks
Most game engine subsystems and third-party game middleware packages
are structured as libraries. A library is a suite of functions and/or classes that
can be called in any way the application programmer sees ﬁt. Libraries pro-
vide maximum ﬂexibility to the programmer. But, libraries are sometimes
difﬁcult to use, because the programmer must understand how to properly
use the functions and classes they provide.

344
7. The Game Loop and Real-Time Simulation
In contrast, some game engines and game middleware packages are struc-
tured as frameworks. A framework is a partially constructed application—the
programmer completes the application by providing custom implementations
of missing functionality within the framework (or overriding its default be-
havior). But he or she has little or no control over the overall ﬂow of control
within the application, because it is controlled by the framework.
In a framework-based rendering engine or game engine, the main game
loop has been written for us, but it is largely empty. The game program-
mer can write callback functions in order to “ﬁll in” the missing details. The
OGRE rendering engine is an example of a library that has been wrapped
in a framework. At the lowest level, OGRE provides functions that can be
called directly by a game engine programmer. However, OGRE also pro-
vides a framework that encapsulates knowledge of how to use the low-level
OGRE library effectively. If the programmer chooses to use the OGRE frame-
work, he or she derives a class from Ogre::FrameListener and overrides
two virtual functions: frameStart() and frameEnded(). As you might
guess, these functions are called before and after the main 3D scene has been
rendered by OGRE, respectively. The OGRE framework’s implementation of
its internal game loop looks something like the following pseudocode. (See
Ogre::Root::renderOneFrame() in OgreRoot.cpp for the actual source
code.)
while (true)
{
for (each frameListener)
{
frameListener.frameStarted();
}
renderCurrentScene();
for (each frameListener)
{
frameListener.frameEnded();
}
finalizeSceneAndSwapBuffers();
}
A particular game’s frame listener implementation might look something like
this.

7.3. Game Loop Architectural Styles
345
class GameFrameListener : public Ogre::FrameListener
{
public:
virtual void frameStarted(const FrameEvent& event)
{
// Do things that must happen before the 3D scene
// is rendered (i.e., service all game engine
// subsystems).
pollJoypad(event);
updatePlayerControls(event);
updateDynamicsSimulation(event);
resolveCollisions(event);
updateCamera(event);
// etc.
}
virtual void frameEnded(const FrameEvent& event)
{
// Do things that must happen after the 3D scene
// has been rendered.
drawHud(event);
// etc.
}
};
7.3.3
Event-Based Updating
In games, an event is any interesting change in the state of the game or its en-
vironment. Some examples include: the human player pressing a button on
the joypad, an explosion going off, an enemy character spotting the player,
and the list goes on. Most game engines have an event system, which permits
various engine subsystems to register interest in particular kinds of events
and to respond to those events when they occur (see Section 15.7 for details).
A game’s event system is usually very similar to the event/messaging sys-
tem underlying virtually all graphical user interfaces (for example, Microsoft
Windows’ window messages, the event handling system in Java’s AWT or the
services provided by C#’s delegate and event keywords).
Some game engines leverage their event system in order to implement the
periodic servicing of some or all of their subsystems. For this to work, the
event system must permit events to be posted into the future—that is, to be
queued for later delivery. A game engine can then implement periodic updat-
ing by simply posting an event. In the event handler, the code can perform

346
7. The Game Loop and Real-Time Simulation
whatever periodic servicing is required. It can then post a new event 1/30 or
1/60 of a second into the future, thus continuing the periodic servicing for as
long as it is required.
7.4
Abstract Timelines
In game programming, it can be extremely useful to think in terms of abstract
timelines. A timeline is a continuous, one-dimensional axis whose origin (t =
0) can lie at any arbitrary location relative to other timelines in the system. A
timeline can be implemented via a simple clock variable that stores absolute
time values in either integer or ﬂoating-point format.
7.4.1
Real Time
We can think of times measured directly via the CPU’s high-resolution timer
register (see Section 7.5.3) as lying on what we’ll call the real timeline. The ori-
gin of this timeline is deﬁned to coincide with the moment the CPU was last
powered on or reset. It measures times in units of CPU cycles (or some mul-
tiple thereof), although these time values can be easily converted into units of
seconds by multiplying them by the frequency of the high-resolution timer on
the current CPU.
7.4.2
Game Time
We needn’t limit ourselves to working with the real timeline exclusively. We
can deﬁne as many other timeline(s) as we need in order to solve the prob-
lems at hand. For example, we can deﬁne a game timeline that is technically
independent of real time. Under normal circumstances, game time coincides
with real time. If we wish to pause the game, we can simply stop updating
the game timeline temporarily. If we want our game to go into slow motion,
we can update the game clock more slowly than the real-time clock. All sorts
of effects can be achieved by scaling and warping one timeline relative to an-
other.
Pausing or slowing down the game clock is also a highly useful debug-
ging tool. To track down a visual anomaly, a developer can pause game time
in order to freeze the action. Meanwhile, the rendering engine and debug
ﬂy-through camera can continue to run, as long as they are governed by a dif-
ferent clock (either the real-time clock or a separate camera clock). This allows
the developer to ﬂy the camera around the game world to inspect it from any
angle desired. We can even support single-stepping the game clock, by ad-
vancing the game clock by one target frame interval (e.g., 1/30 of a second)

7.4. Abstract Timelines
347
each time a “single-step” button is pressed on the joypad or keyboard while
the game is in a paused state.
When using the approach described above, it’s important to realize that
the game loop is still running when the game is paused—only the game clock
has stopped. Single-stepping the game by adding 1/30 of a second to a paused
game clock is not the same thing as setting a breakpoint in your main loop,
and then hitting the F5 key repeatedly to run one iteration of the loop at a time.
Both kinds of single-stepping can be useful for tracking down different kinds
of problems. We just need to keep the differences between these approaches
in mind.
7.4.3
Local and Global Timelines
We can envision all sorts of other timelines. For example, an animation clip
or audio clip might have a local timeline, with its origin (t = 0) deﬁned to
coincide with the start of the clip. The local timeline measures how time pro-
gressed when the clip was originally authored or recorded. When the clip is
played back in-game, we needn’t play it at the original rate. We might want
to speed up an animation, or slow down an audio sample. We can even play
an animation backwards by running its local clock in reverse.
Any one of these effects can be visualized as a mapping between the local
timeline and a global timeline, such as real time or game time. To play an
animation clip back at its originally authored speed, we simply map the start
of the animation’s local timeline (t = 0) onto the desired start time (τ = τstart)
along the global timeline. This is shown in Figure 7.1.
To play an animation clip back at half speed, we can imagine scaling the
local timeline to twice its original size prior to mapping it onto the global
timeline. To accomplish this, we simply keep track of a time scale factor or
playback rate R, in addition to the clip’s global start time τstart. This is illus-
trated in Figure 7.2. A clip can even be played in reverse, by using a negative
time scale (R < 0) as shown in Figure 7.3.
Clip A
t = 0 sec
5 sec
start
102 sec
105 sec
110 sec
Figure 7.1. Playing an animation clip can be visualized as mapping its local timeline onto the global
game timeline.

348
7. The Game Loop and Real-Time Simulation
start
R
(scale t by 1/R = 0.5)
t
t
t
Figure 7.2. Animation playback speed can be controlled by simply scaling the local timeline prior
to mapping it onto the global timeline.
t = 5 sec
0 sec
start
102 sec
105 sec
110 sec
 Clip A
Clip A
t = 0 sec
5 sec
R = –1
(ﬂip t)
Figure 7.3. Playing an animation in reverse is like mapping the clip to the global timeline with a
time scale of R = −1.
7.5
Measuring and Dealing with Time
In this section, we’ll investigate some of the subtle and not-so-subtle distinc-
tions between different kinds of timelines and clocks and see how they are
implemented in real game engines.
7.5.1
Frame Rate and Time Deltas
The frame rate of a real-time game describes how rapidly the sequence of still
3D frames is presented to the viewer. The unit of Hertz (Hz), deﬁned as the
number of cycles per second, can be used to describe the rate of any periodic
process. In games and ﬁlm, frame rate is typically measured in frames per sec-
ond (FPS), which is the same thing as Hertz for all intents and purposes. Films
traditionally run at 24 FPS. Games in North America and Japan are typically
rendered at 30 or 60 FPS, because this is the natural refresh rate of the NTSC
color television standard used in these regions. In Europe and most of the rest

7.5. Measuring and Dealing with Time
349
of the world, games update at 50 FPS, because this is the natural refresh rate
of a PAL or SECAM color television signal.
The amount of time that elapses between frames is known as the frame
time, time delta or delta time. This last term is commonplace because the du-
ration between frames is often represented mathematically by the symbol ∆t.
(Technically speaking, ∆t should really be called the frame period, since it is the
inverse of the frame frequency: T = 1/f. But, game programmers hardly ever
use the term “period” in this context.) If a game is being rendered at exactly
30 FPS, then its delta time is 1/30 of a second, or 33.3 ms (milliseconds). At
60 FPS, the delta time is half as big, 1/60 of a second or 16.6 ms. To really
know how much time has elapsed during one iteration of the game loop, we
need to measure it. We’ll see how this is done below.
We should note here that milliseconds are a common unit of time mea-
surement in games. For example, we might say that the animation system
is taking 4 ms to run, which implies that it occupies about 12% of the entire
frame (4/33.3 ≈0.12). Other common units include seconds and machine
cycles. We’ll discuss time units and clock variables in more depth below.
7.5.2
From Frame Rate to Speed
Let’s imagine that we want to make a spaceship ﬂy through our game world
at a constant speed of 40 meters per second (or in a 2D game, we might specify
this as 40 pixels per second). One simple way to accomplish this is to multiply
the ship’s speed v (measured in meters per second) by the duration of one
frame ∆t (measured in seconds), yielding a change in position ∆x = v∆t
(which is measured in meters per frame). This position delta can then be added
to the ship’s current position x1, in order to ﬁnd its position next frame: x2 =
x1 + ∆x = x1 + v∆t.
This is actually a simple form of numerical integration known as the explicit
Euler method (see Section 12.4.4). It works well as long as the speeds of our
objects are roughly constant. To handle variable speeds, we need to resort to
somewhat more-complex integration methods. But all numerical integration
techniques make use of the elapsed frame time ∆t in one way or another. So
it is safe to say that the perceived speeds of the objects in a game are dependent
upon the frame duration, ∆t. Hence a central problem in game programming
is to determine a suitable value for ∆t. In the sections that follow, we’ll discuss
various ways of doing this.
7.5.2.1
Old-School CPU-Dependent Games
In many early video games, no attempt was made to measure how much real
time had elapsed during the game loop. The programmers would essentially

350
7. The Game Loop and Real-Time Simulation
ignore ∆t altogether and instead specify the speeds of objects directly in terms
of meters (or pixels, or some other distance unit) per frame. In other words,
they were, perhaps unwittingly, specifying object speeds in terms of ∆x =
v∆t, instead of in terms of v.
The net effect of this simplistic approach was that the perceived speeds of
the objects in these games were entirely dependent upon the frame rate that
the game was actually achieving on a particular piece of hardware. If this kind
of game were to be run on a computer with a faster CPU than the machine for
which it was originally written, the game would appear to be running in fast
forward. For this reason, I’ll call these games CPU-dependent games.
Some older PCs provided a “Turbo” button to support these kinds of games.
When the Turbo button was pressed, the PC would run at its fastest speed, but
CPU-dependent games would run in fast forward. When the Turbo button
was not pressed, the PC would mimic the processor speed of an older gen-
eration of PCs, allowing CPU-dependent games written for those PCs to run
properly.
7.5.2.2
Updating Based on Elapsed Time
To make our games CPU-independent, we must measure ∆t in some way,
rather than simply ignoring it. Doing this is quite straightforward. We simply
read the value of the CPU’s high-resolution timer twice—once at the begin-
ning of the frame and once at the end. Then we subtract, producing an accu-
rate measure of ∆t for the frame that has just passed. This delta is then made
available to all engine subsystems that need it, either by passing it to every
function that we call from within the game loop or by storing it in a global
variable or encapsulating it within a singleton class of some kind. (We’ll de-
scribe the CPU’s high-resolution timer in more detail Section 7.5.3.)
The approach outlined above is used by many game engines. In fact, I am
tempted to go out on a limb and say that most game engines use it. However,
there is one big problem with this technique: We are using the measured value
of ∆t taken during frame k as an estimate of the duration of the upcoming
frame (k + 1). This isn’t necessarily very accurate. (As they say in investing,
“past performance is not a guarantee of future results.”) Something might
happen next frame that causes it to take much more time (or much less) than
the current frame. We call such an event a frame-rate spike.
Using last frame’s delta as an estimate of the upcoming frame can have
some very real detrimental effects. For example, if we’re not careful it can put
the game into a “viscious cycle” of poor frame times. Let’s assume that our
physics simulation is most stable when updated once every 33.3 ms (i.e., at
30 Hz). If we get one bad frame, taking say 57 ms, then we might make the

7.5. Measuring and Dealing with Time
351
mistake of stepping the physics system twice on the next frame, presumably to
“cover” the 57 ms that has passed. Those two steps take roughly twice as long
to complete as a regular step, causing the next frame to be at least as bad as
this one was, and possibly worse. This only serves to exacerbate and prolong
the problem.
7.5.2.3
Using a Running Average
It is true that game loops tend to have at least some frame-to-frame coherency.
If the camera is pointed down a hallway containing lots of expensive-to-draw
objects on one frame, there’s a good chance it will still be pointed down that
hallway on the next. Therefore, one reasonable approach is to average the
frame-time measurements over a small number of frames and use that as the
next frame’s estimate of ∆t. This allows the game to adapt to a varying frame
rate, while softening the effects of momentary performance spikes. The longer
the averaging interval, the less responsive the game will be to a varying frame
rate, but spikes will have less of an impact as well.
7.5.2.4
Governing the Frame Rate
We can avoid the inaccuracy of using last frame’s ∆t as an estimate of this
frame’s duration altogether, by ﬂipping the problem on its head. Rather than
trying to guess at what next frame’s duration will be, we can instead attempt
to guarantee that every frame’s duration will be exactly 33.3 ms (or 16.6 ms if
we’re running at 60 FPS). To do this, we measure the duration of the current
frame as before. If the measured duration is less than the ideal frame time, we
simply put the main thread to sleep until the target frame time has elapsed.
If the measured duration is more than the ideal frame time, we must “take
our lumps” and wait for one more whole frame time to elapse. This is called
frame-rate governing.
Clearly this approach only works when your game’s frame rate is reason-
ably close to your target frame rate on average. If your game is ping-ponging
between 30 FPS and 15 FPS due to frequent “slow” frames, then the game’s
quality can degrade signiﬁcantly. As such, it’s still a good idea to design all
engine systems so that they are capable of dealing with arbitrary frame du-
rations. During development, you can leave the engine in “variable frame
rate” mode, and everything will work as expected. Later on, when the game
is getting closer to achieving its target frame rate consistently, we can switch
on frame-rate governing and start to reap its beneﬁts.
Keeping the frame rate consistent can be important for a number of rea-
sons. Some engine systems, such as the numerical integrators used in a physics
simulation, operate best when updated at a constant rate. A consistent frame

352
7. The Game Loop and Real-Time Simulation
rate also looks better, and as we’ll see in the next section, it can be used to
avoid the tearing that can occur when the video buffer is updated at a rate that
doesn’t match the refresh rate of the monitor (see Section 7.5.2.5).
In addition, when elapsed frame times are consistent, features like record
and playback become a lot more reliable. As its name implies, the record-and-
playback feature allows a player’s gameplay experience to be recorded and
later played back in exactly the same way. This can be a fun game feature,
and it’s also a valuable testing and debugging tool. For example, difﬁcult-to-
ﬁnd bugs can be reproduced by simply playing back a recorded game that
demonstrates the bug.
To implement record and playback, we make note of every relevant event
that occurs during gameplay, saving each one in a list along with an accurate
time stamp. The list of events can then be replayed with exactly the same tim-
ing, using the same initial conditions and an identical initial random seed.
In theory, doing this should produce a gameplay experience that is indis-
tinguishable from the original playthrough. However, if the frame rate isn’t
consistent, things may not happen in exactly the same order. This can cause
“drift,” and pretty soon your AI characters are ﬂanking when they should
have fallen back.
7.5.2.5
Screen Tearing and V-Sync
A visual anomaly known as screen tearing occurs when the back buffer is
swapped with the front buffer while the screen has only been partially “drawn”
by the video hardware. When tearing occurs, a portion of the screen shows the
new image, while the remainder shows the old one. To avoid tearing, many
rendering engines wait for the vertical blanking interval of the monitor before
swapping buffers.
Older CRT monitors and TVs “draw” the contents of the in-memory frame
buffer by exciting phosphors on the screen via a beam of electrons that scans
from left-to-right and top-to-bottom. On such displays, the v-blank interval
is the time during which the electron gun is “blanked” (turned off) while it
is being reset to the top-left corner of the screen. Modern LCD, plasma and
LED displays no longer use an electron beam, and they don’t require any time
between ﬁnishing the draw of the last scan line of one frame and the ﬁrst scan
line of the next. But the v-blank interval still exists, in part because video
standards were established when CRTs were the norm, and in part because of
the need to support older displays.
Waiting for the v-blank interval is called v-sync. It is really just another
form of frame-rate governing, because it effectively clamps the frame rate of
the main game loop to a multiple of the screen’s refresh rate. For example,
on an NTSC monitor that refreshes at a rate of 60 Hz, the game’s real update

7.5. Measuring and Dealing with Time
353
rate is effectively quantized to a multiple of 1/60 of a second. If more than
1/60 of a second elapses between frames, we must wait until the next v-blank
interval, which means waiting 2/60 of a second (30 FPS). If we miss two v-
blanks, then we must wait a total of 3/60 of a second (20 FPS) and so on. Also,
be careful not to make assumptions about the frame rate of your game, even
when it is synchronized to the v-blank interval; if your game supports them,
you must keep in mind that the PAL and SECAM standards are based around
an update rate of 50 Hz, not 60 Hz.
7.5.3
Measuring Real Time with a High-Resolution Timer
We’ve talked a lot about measuring the amount of real “wall clock” time that
elapses during each frame. In this section, we’ll investigate how such timing
measurements are made in detail.
Most operating systems provide a function for querying the system time,
such as the standard C library function time(). However, such functions
are not suitable for measuring elapsed times in a real-time game because they
do not provide sufﬁcient resolution. For example, time() returns an integer
representing the number of seconds since midnight, January 1, 1970, so its res-
olution is one second—far too coarse, considering that a frame takes only tens
of milliseconds to execute.
All modern CPUs contain a high-resolution timer, which is usually imple-
mented as a hardware register that counts the number of CPU cycles (or some
multiple thereof) that have elapsed since the last time the processor was pow-
ered on or reset. This is the timer that we should use when measuring elapsed
time in a game, because its resolution is usually on the order of the dura-
tion of a few CPU cycles. For example, on a 3 GHz Pentium processor, the
high-resolution timer increments once per CPU cycle, or 3 billion times per
second. Hence the resolution of the high-res timer is 1/3 billion = 3.33 ×
10−10 seconds = 0.333 ns (one-third of a nanosecond).
This is more than
enough resolution for all of our time-measurement needs in a game.
Different microprocessors and different operating systems provide differ-
ent ways to query the high-resolution timer. On a Pentium, a special instruc-
tion called rdtsc (read time-stamp counter) can be used, although the Win32
API wraps this facility in a pair of functions: QueryPerformanceCounter()
reads the 64-bit counter register and QueryPerformanceFrequency() re-
turns the number of counter increments per second for the current CPU. On
a PowerPC architecture, such as the chips found in the Xbox 360 and PlaySta-
tion 3, the instruction mftb (move from time base register) can be used to read
the two 32-bit time base registers, while on other PowerPC architectures, the
instruction mfspr (move from special-purpose register) is used instead.

354
7. The Game Loop and Real-Time Simulation
A CPU’s high-resolution timer register is 64 bits wide on most processors,
to ensure that it won’t wrap too often. The largest possible value of a 64-
bit unsigned integer is 0xFFFFFFFFFFFFFFFF ≈1.8 × 1019 clock ticks. So,
on a 3 GHz Pentium processor that updates its high-res timer once per CPU
cycle, the register’s value will wrap back to zero once every 195 years or so—
deﬁnitely not a situation we need to lose too much sleep over. In contrast, a
32-bit integer clock will wrap after only about 1.4 seconds at 3 GHz.
7.5.3.1
High-Resolution Clock Drift
Be aware that even timing measurements taken via a high-resolution timer
can be inaccurate in certain circumstances. For example, on some multicore
processors, the high-resolution timers are independent on each core, and they
can (and do) drift apart. If you try to compare absolute timer readings taken
on different cores to one another, you might end up with some strange
results—even negative time deltas. Be sure to keep an eye out for these kinds
of problems.
7.5.4
Time Units and Clock Variables
Whenever we measure or specify time durations in a game, we have two
choices to make:
1.
What time units should be used? Do we want to store our times in sec-
onds, or milliseconds, or machine cycles...or in some other unit?
2.
What data type should be used to store time measurements? Should
we employ a 64-bit integer, or a 32-bit integer, or a 32-bit ﬂoating point
variable?
The answers to these questions depend on the intended purpose of a given
measurement. This gives rise to two more questions: How much precision do
we need? And what range of magnitudes do we expect to be able to represent?
7.5.4.1
64-Bit Integer Clocks
We’ve already seen that a 64-bit unsigned integer clock, measured in machine
cycles, supports both an extremely high precision (a single cycle is 0.333 ns in
duration on a 3 GHz CPU) and a broad range of magnitudes (a 64-bit clock
wraps once roughly every 195 years at 3 GHz). So this is the most ﬂexible time
representation, presuming you can afford 64 bits worth of storage.

7.5. Measuring and Dealing with Time
355
7.5.4.2
32-Bit Integer Clocks
When measuring relatively short durations with high precision, we can turn
to a 32-bit integer clock, measured in machine cycles. For eample, to proﬁle
the performance of a block of code, we might do something like this:
// Grab a time snapshot.
U64 begin_ticks = readHiResTimer();
// This is the block of code whose performance we wish
// to measure.
doSomething();
doSomethingElse();
nowReallyDoSomething();
// Measure the duration.
U64 end_ticks = readHiResTimer();
U32 dt_ticks = static_cast<U32>(end_ticks - begin_ticks);
// Now use or cache the value of dt_ticks...
Notice that we still store the raw time measurements in 64-bit integer vari-
ables. Only the time delta dt is stored in a 32-bit variable. This circumvents po-
tential problems with wrapping at the 32-bit boundary. For example, if begin
_ticks = 0x12345678FFFFFFB7 and
end_ticks = 0x1234567900000039,
then we would measure a negative time delta if we were to truncate the indi-
vidual time measurements to 32 bits each prior to subtracting them.
7.5.4.3
32-Bit Floating-Point Clocks
Another common approach is to store relatively small time deltas in ﬂoating-
point format, measured in units of seconds. To do this, we simply multiply a
duration measured in CPU cycles by the CPU’s clock frequency, which is in
cycles per second. For example:
// Start off assuming an ideal frame time (30 FPS).
F32 dt_seconds = 1.0f / 30.0f;
// Prime the pump by reading the current time.
U64 begin_ticks = readHiResTimer();
while (true) // main game loop
{
runOneIterationOfGameLoop(dt_seconds);
// Read the current time again, and calculate the delta.

356
7. The Game Loop and Real-Time Simulation
U64 end_ticks = readHiResTimer();
// Check our units: seconds = ticks / (ticks/second)
dt_seconds = (F32)(end_ticks - begin_ticks)
/ (F32)getHiResTimerFrequency();
// Use end_ticks as the new begin_ticks for next frame.
begin_ticks = end_ticks;
}
Notice once again that we must be careful to subtract the two 64-bit time
measurements before converting them into ﬂoating-point format. This ensures
that we don’t store too large a magnitude into a 32-bit ﬂoating-point variable.
7.5.4.4
Limitations of Floating-Point Clocks
Recall that in a 32-bit IEEE ﬂoat, the 23 bits of the mantissa are dynamically
distributed between the whole and fractional parts of the value, by way of
the exponent (see Section 3.2.1.4). Small magnitudes require only a few bits,
leaving plenty of bits of precision for the fraction. But once the magnitude of
our clock grows too large, its whole part eats up more bits, leaving fewer bits
for the fraction. Eventually, even the least-signiﬁcant bits of the whole part
become implicit zeros. This means that we must be cautious when storing
long durations in a ﬂoating-point clock variable. If we keep track of the amount
of time that has elapsed since the game was started, a ﬂoating-point clock will
eventually become inaccurate to the point of being unusable.
Floating-point clocks are usually only used to store relatively short time
deltas, measuring at most a few minutes and more often just a single frame
or less. If an absolute-valued ﬂoating-point clock is used in a game, you will
need to reset the clock to zero periodically to avoid accumulation of large
magnitudes.
7.5.4.5
Other Time Units
Some game engines allow timing values to be speciﬁed in a game-deﬁned unit
that is ﬁne-grained enough to permit an integer format to be used (as opposed
to requiring a ﬂoating-point format), precise enough to be useful for a wide
range of applications within the engine, and yet large enough that a 32-bit
clock won’t wrap too often. One common choice is a 1/300 second time unit.
This works well because (a) it is ﬁne-grained enough for many purposes, (b) it
only wraps once every 165.7 days and (c) it is an even multiple of both NTSC
and PAL refresh rates. A 60 FPS frame would be 5 such units in duration,
while a 50 FPS frame would be 6 units in duration.

7.5. Measuring and Dealing with Time
357
Obviously a 1/300 second time unit is not precise enough to handle subtle
effects, like time-scaling an animation. (If we tried to slow a 30 FPS animation
down to less than 1/10 of its regular speed, we’d be out of precision!) So
for many purposes, it’s still best to use ﬂoating-point time units or machine
cycles. But a 1/300 second time unit can be used effectively for things like
specifying how much time should elapse between the shots of an automatic
weapon, or how long an AI-controlled character should wait before starting
his patrol, or the amount of time the player can survive when standing in a
pool of acid.
7.5.5
Dealing with Breakpoints
When your game hits a breakpoint, its loop stops running and the debugger
takes over. However, if your game is running on the same computer that
your debugger is running on, then the CPU continues to run, and the real-
time clock continues to accrue cycles. A large amount of wall clock time can
pass while you are inspecting your code at a breakpoint. When you allow the
program to continue, this can lead to a measured frame time many seconds,
or even minutes or hours in duration!
Clearly if we allow such a huge delta-time to be passed to the subsystems
in our engine, bad things will happen. If we are lucky, the game might con-
tinue to function properly after lurching forward many seconds in a single
frame. Worse, the game might just crash.
A simple approach can be used to get around this problem. In the main
game loop, if we ever measure a frame time in excess of some predeﬁned up-
per limit (e.g., 1 second), we can assume that we have just resumed execution
after a breakpoint, and we set the delta time artiﬁcially to 1/30 or 1/60 of
a second (or whatever the target frame rate is). In effect, the game becomes
frame-locked for one frame, in order to avoid a massive spike in the measured
frame duration.
// Start off assuming the ideal dt (30 FPS).
F32 dt = 1.0f / 30.0f;
// Prime the pump by reading the current time.
U64 begin_ticks = readHiResTimer();
while (true) // main game loop
{
updateSubsystemA(dt);
updateSubsystemB(dt);
// ...
renderScene();
swapBuffers();

358
7. The Game Loop and Real-Time Simulation
// Read the current time again, and calculate an
// estimate of next frame's delta time.
U64 end_ticks = readHiResTimer();
dt = (F32)(end_ticks - begin_ticks)
/ (F32)getHiResTimerFrequency();
// If dt is too large, we must have resumed from a
// breakpoint -- frame-lock to the target rate this
// frame.
if (dt > 1.0f)
{
dt = 1.0f/30.0f;
}
// Use end_ticks as the new begin_ticks for next frame.
begin_ticks = end_ticks;
}
7.5.6
A Simple Clock Class
Some game engines encapsulate their clock variables in a class. An engine
might have a few instances of this class—one to represent real “wall clock”
time, another to represent “game time” (which can be paused, slowed down
or sped up relative to real time), another to track time for full-motion videos
and so on. A clock class is reasonably straightforward to implement. I’ll
present a simple implementation below, making note of a few common tips,
tricks and pitfalls in the process.
A clock class typically contains a variable that tracks the absolute time that
has elapsed since the clock was created. As described above, it’s important to
select a suitable data type and time unit for this variable. In the following
example, we’ll store absolute times in the same way the CPU does—with a
64-bit unsigned integer, measured in machine cycles. There are other possible
implementations, of course, but this is probably the simplest.
A clock class can support some nifty features, like time scaling. This can be
implemented by simply multiplying the measured time delta by an arbitrary
scale factor prior to adding it to the clock’s running total. We can also pause
time by simply skipping its update while the clock is paused. Single-stepping
a clock can be implemented by adding a ﬁxed time interval to a paused clock
in response to a button press on the joypad or keyboard. All of this is demon-
strated by the example class shown below.
class Clock
{
U64
m_timeCycles;

7.5. Measuring and Dealing with Time
359
F32
m_timeScale;
bool
m_isPaused;
static F32 s_cyclesPerSecond;
static inline U64 secondsToCycles(F32 timeSeconds)
{
return (U64)(timeSeconds * s_cyclesPerSecond);
}
// WARNING: Dangerous -- only use to convert small
// durations into seconds.
static inline F32 cyclesToSeconds(U64 timeCycles)
{
return (F32)timeCycles / s_cyclesPerSecond;
}
public:
// Call this when the game first starts up.
static void init()
{
s_cyclesPerSecond
= (F32)readHiResTimerFrequency();
}
// Construct a clock. (Notice the use of 'explicit' to
// prevent automatic conversion from F32 to Clock.)
explicit Clock(F32 startTimeSeconds = 0.0f) :
m_timeCycles(secondsToCycles(startTimeSeconds)),
m_timeScale(1.0f), // default to unscaled
m_isPaused(false)
// default to running
{
}
// Return the current time in cycles. NOTE that we do
// not return absolute time measurements in floating-
// point seconds, because a 32-bit float doesn't have
// enough precision. See calcDeltaSeconds().
U64 getTimeCycles() const
{
return m_timeCycles;
}
// Determine the difference between this clock's
// absolute time and that of another clock, in
// seconds. We only return time deltas as floating-
// point seconds, due to the precision limitations of

360
7. The Game Loop and Real-Time Simulation
// a 32-bit float.
F32 calcDeltaSeconds(const Clock& other)
{
U64 dt = m_timeCycles - other.m_timeCycles;
return cyclesToSeconds(dt);
}
// This function should be called once per frame,
// with the real measured frame time delta in seconds.
void update(F32 dtRealSeconds)
{
if (!m_isPaused)
{
U64 dtScaledCycles
= secondsToCycles(dtRealSeconds
* m_timeScale);
m_timeCycles += dtScaledCycles;
}
}
void setPaused(bool wantPaused)
{
m_isPaused = wantPaused;
}
bool isPaused() const
{
return m_isPaused;
}
void setTimeScale(F32 scale)
{
m_timeScale = scale;
}
F32 getTimeScale() const
{
return m_timeScale;
}
void singleStep()
{
if (m_isPaused)
{
// Add one ideal frame interval; don't forget
// to scale it by our current time scale!

7.6. Multiprocessor Game Loops
361
U64 dtScaledCycles = secondsToCycles(
(1.0f/30.0f) * m_timeScale);
m_timeCycles += dtScaledCycles;
}
}
};
7.6
Multiprocessor Game Loops
Now that we’ve investigated basic single-threaded game loops and learned
some of the ways in which time is commonly measured and manipulated in
a game engine, let’s turn our attention to some more complex kinds of game
loops. In this section, we’ll explore how game loops have evolved to take
advantage of modern multiprocessor hardware. In the following section, we’ll
see how networked multiplayer games typically structure their game loops.
In 2004, microprocessor manufacturers industry-wide encountered a prob-
lem with heat dissipation that prevented them from producing faster CPUs.
Moore’s Law, which predicts an approximate doubling in transistor counts ev-
ery 18 to 24 months, still holds true. But in 2004, its assumed correlation with
doubling processor speeds was shown to be no longer valid. As a result, mi-
croprocessor manufacturers shifted their focus toward multicore CPUs. (For
more information on this trend, see Microsoft’s “The Manycore Shift White-
paper,” available at http://www.microsoftpost.com/microsoft-download/
the-manycore-shift-white-paper, and “Multicore Eroding Moore’s Law” by
Dean Dauger, available at http://www.macresearch.org/multicore_eroding_
moores_law.) The net effect on the software industry was a major shift to-
ward parallel processing techniques. As a result, modern game engines run-
ning on multicore systems like the Xbox 360, Xbox One, PlayStation 3 and
PlayStation 4 can no longer rely on a single main game loop to service their
subsystems.
The shift from single core to multicore has been painful. Multithreaded
program design is a lot harder than single-threaded programming.
Most
game companies took on the transformation gradually, by selecting a handful
of engine subsystems for parallelization and leaving the rest under the control
of the old, single-threaded main loop. By 2008, most game studios had com-
pleted the transformation for the most part and have embraced parallelism
to varying degrees within their engines. Five years later, game studios don’t
have a choice. Consoles like the Xbox One and PS4 and virtually all PCs con-
tain multicore CPUs. You’d have to be crazy not to take advantage of all this
parallel processing power!

362
7. The Game Loop and Real-Time Simulation
We don’t have room here for a full treatise on parallel programming ar-
chitectures and techniques. (Refer to [20] for an in-depth discussion of this
topic.) However, we will take a brief look at some of the most common ways
in which game engines leverage multicore hardware. There are many differ-
ent software architectures possible—but the goal of all of these architectures
is to maximize hardware utilization (i.e., to attempt to minimize the amount
of time during which any particular hardware thread, core or CPU is idle).
7.6.1
Multiprocessor Game Console Architectures
The Xbox 360, Xbox One, PlayStation 3 and PlayStation 4 are all multiproces-
sor consoles. In order to have a meaningful discussion of parallel software
architectures, let’s take a brief look at how these consoles are structured inter-
nally.
7.6.1.1
Xbox 360
The Xbox 360 consists of three identical PowerPC processor cores. Each core
has a dedicated L1 instruction cache and L1 data cache, and the three cores
share a single L2 cache.
(Refer to Section 3.4.2 for a description of mem-
ory caches.) The three cores and the GPU share a uniﬁed 512 MiB pool of
RAM, which can be used for executable code, application data, textures, video
RAM—you name it. The Xbox 360 architecture is described in a great deal
more depth in the PowerPoint presentation entited “Xbox 360 System Archi-
tecture” by Jeff Andrews and Nick Baker of the Xbox Semiconductor Technol-
ogy Group, available at http://www.cis.upenn.edu/~milom/cis501-Fall08/
papers/xbox-system.pdf. However, the preceding extremely brief overview
should sufﬁce for our purposes. Figure 7.4 shows the Xbox 360’s architecture
in highly simpliﬁed form.
7.6.1.2
PlayStation 3
The PlayStation 3 hardware makes use of the Cell Broadband Engine (CBE)
architecture (see Figure 7.5), developed jointly by Sony, Toshiba and IBM. The
PS3 takes a radically different approach to the one employed by the Xbox 360.
Instead of three identical processors, it contains a number of different types of
processors, each designed for speciﬁc tasks. And instead of a uniﬁed memory
architecture, the PS3 divides its RAM into a number of blocks, each of which
is designed for efﬁcient use by certain processing units in the system. The
architecture is described in detail at http://www.blachford.info/computer/
Cell/Cell1_v2.html, but the following overview and the diagram shown in
Figure 7.5 should sufﬁce for our purposes.

7.6. Multiprocessor Game Loops
363
Main RAM
512 MiB GDDR3
Shared L2 Cache
1 MiB / 8-way
ATI R500 
GPU
@ 500 MHz
L1 D$
32 KiB
4-way
L1 I$
32 KiB
2-way
PowerPC
Core 0
@ 3.2 GHz
L1 D$
32 KiB
4-way
L1 I$
32 KiB
2-way
PowerPC
Core 0
@ 3.2 GHz
L1 D$
32 KiB
4-way
L1 I$
32 KiB
2-way
PowerPC
Core 0
@ 3.2 GHz
Figure 7.4. A simpliﬁed view of the Xbox 360 hardware architecture.
The PS3’s main CPU is called the Power Processing Unit (PPU). It is a
PowerPC processor, much like the ones found in the Xbox 360. In addition
to this central processor, the PS3 has six coprocessors known as Synergistic
Processing Units (SPUs). These coprocessors are based around the PowerPC
instruction set, but they have been streamlined for maximum hardware sim-
plicity and maximum performance.
System RAM
256 MiB XDR
DMA Ring Bus
L1 D$
32 KiB
4-way
L1 I$
32 KiB
2-way
Local
Store
256 KiB
Local
Store
256 KiB
Local
Store
256 KiB
SPU 0
@ 3.2 GHz
SPU 1
@ 3.2 GHz
SPU 6
@ 3.2 GHz
DMA
Controller
L2 Cache
512 KiB
PPU
@ 3.2 GHz
Video RAM
256 MiB GDDR3
Nvidia RSX 
GPU
@ 550 MHz
Figure 7.5. Simpliﬁed view of the PS3’s cell broadband architecture.

364
7. The Game Loop and Real-Time Simulation
The GPU on the PS3, known as the RSX, has a dedicated 256 MiB of video
RAM. The PPU has access to 256 MiB of system RAM. In addition, each SPU
has a dedicated high-speed 256 KiB RAM area called its local store (LS). Local
store memory performs about as efﬁciently as an L1 cache, making the SPUs
blindingly fast.
The SPUs never read directly from main RAM. Instead, a direct memory
access (DMA) controller allows blocks of data to be copied back and forth
between system RAM and the SPUs’ local stores. These data transfers happen
in parallel, so both the PPU and SPUs can be doing useful calculations while
they wait for data to arrive.
7.6.1.3
PlayStation 4
The PlayStation 4 hardware represents a radical departure from the PS3’s Cell
architecture. Instead of utilizing a single central processor (the PPU) and six
coprocessors (the SPUs), the PS4 employs an eight-core Jaguar AMD CPU.
Instead of the PowerPC instruction set common to the PPU and SPUs of the
PS3, the PS4’s Jaguar processor utilizes an Intel instruction set. And instead of
the PS3’s numerous dedicated memory stores, the PS4 employs a truly uniﬁed
memory architecture, in which all eight CPU cores and the GPU share access
to a single 8 GiB block of RAM.
The PS4 should prove to be an extremely powerful piece of gaming hard-
ware. Its memory is high-performance GDDR5 (graphics double data rate,
version 5) RAM. The CPU accesses this RAM via a zippy 20 GiB/second bus.
The GPU can access RAM via two different buses: The “onion” bus accesses
memory via the CPU’s caches to ensure cache-coherent shared memory ac-
cess between the CPU and the GPU. This bus supports data transfers at a rate
of 10 GiB/second in each direction (to and from the GPU). The “garlic” bus,
on the other hand, provides the GPU with direct access to RAM at a stag-
geringly high data rate of up to 176 GiB/second. It achieves these high data
rates by foregoing cache coherency with the CPU entirely. Memory must be
speciﬁcally allocated to the “garlic” bus for exclusive use by the GPU, but the
amount of RAM that is allocated to the GPU is up to the programmer.
The two-bus, uniﬁed memory architecture of the PS4 (known as a heteroge-
neous uniﬁed memory architecture or hUMA) provides game programmers with
a good balance between ﬂexibility and raw performance. And it’s no accident
that this architecture maps very well to the memory access patterns that are
typical of most games. Rendering data usually comes in two basic ﬂavors:
1.
data that is shared between the CPU and the GPU (e.g., matrices that
represent object transforms and skeletal animation, lighting parameters
and other kinds of “shader constants”), and

7.6. Multiprocessor Game Loops
365
2.
data that is almost exclusively produced and managed by the GPU (e.g.,
geometry buffers, frame buffers).
The shared data tends to be quite small in size, while the GPU-exclusive data
tends to be much larger. The “onion” bus is designed to handle the shared
data, and the “garlic” bus is designed to handle the GPU-exclusive data.
The PS4’s eight-core Jaguar CPU is a high-performance computing device.
Yes, it runs at a slower clock rate than the PS3’s PPU and SPUs (1.6 GHz as
opposed to 3.2 GHz). But it makes up for this by including hardware that
makes less-than-optimal code (in other words, pretty much all game code that
hasn’t been manually downcoded to assembly language!) run much faster
than it would on the PS3. For example, the Jaguar supports advanced branch
prediction circuitry, which can correctly recognize all sorts of common branch-
ing patterns in your code. This means a lot fewer pipeline stalls in “branchy”
code. At the end of the day, what this means to game programmers is that
they can focus their efforts more on making great games and less on trying to
rearrange their source code to eliminate branches in an effort to buy back lost
performance.
Another aspect of the PS4 architecture that should allow it to surpass the
performance of the PS3 is its powerful GPU. A modern GPU is essentially
a massively parallel high-performance microprocessor. By “massively paral-
lel,” we’re talking about hundreds or even thousands of operations happening
in parallel. And the GPU on the PS4 has been anointed with even more pro-
cessing power than it probably needs to render breathtaking scenes at 1080p
resoultion. The architects of the PS4 knew that this extra bandwidth could
be put to good use by industrious game programmers. In effect, the GPU on
the PS4 acts a bit like a very large bank of SPUs that can be shared between
rendering tasks and other high-performance processing.
Programming a GPU to perform non-graphics-related tasks is known as
general-purpose GPU computing, or GPGPU for short. To program a GPU in this
way, one typically employs a C-like language that has been custom tailored to
the task. Examples of such languages include OpenCL and Nvidia’s propri-
etary CUDA programming model. Full coverage of GPGPU computing is be-
yond our scope here, but see http://en.wikipedia.org/wiki/General-purpose_
computing_on_graphics_processing_units for more information on this fasci-
nating topic.
We should mention here that the PS4 utilizes what is known as a heteroge-
neous system architecture (HSA). This relatively new architectural trend aims to
eliminate the bottlenecks between various processing centers within a com-
puter system. Previously, the CPU and GPU were totally separate devices

366
7. The Game Loop and Real-Time Simulation
AMD Jaguar CPU @ 1.6 GHz
CPC 0
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 0
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 1
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 3
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 2
CPC 0
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 4
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 5
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 7
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 6
AMD Radeon GPU
(comparable to 7870)
@ 800 MHz
1152 stream processors
snoop
snoop
“Onion” Bus
(10 GiB/s each way)
“Garlic” Bus
(176 GiB/s)
(non cache-coherent)
L2 Cache
2 MiB / 16-way
L2 Cache
2 MiB / 16-way
CPU Bus (20 GiB/s)
Main RAM
8 GiB GDDR5
Cache Coherent
Memory Controller
Figure 7.6. Simpliﬁed view of the PS4’s architecture.
with their own custom memories (and even living on separate circuit boards).
Transferring data between them required cumbersome and high-latency com-
munication over a specialized bus like AGP or PCIe. With HSA, the CPU and
GPU share a single uniﬁed memory store called a heterogeneous uniﬁed mem-
ory architecture (hUMA) and can “send” data to one another by simply tossing
around a pointer to the data.
Figure 7.6 shows a block diagram of the PS4 hardware architecture.
7.6.1.4
Xbox One
The Xbox One hardware architecture is uncannily similar to that of the PlaySta-
tion 4. This is due in large part to the fact that both consoles are based around
AMD’s Jaguar line of multicore CPUs. It’s also due in part to the fact that
the architects of both consoles are a highly competitive bunch, so they went
to great lengths to make sure that the “other guys” didn’t one-up them too
badly. As a result, they ended up arriving at very similar architectures.

7.6. Multiprocessor Game Loops
367
That being said, there are some important differences between the design
of the Xbox One and the PlayStation 4. Some of the key differences are out-
lined below.
•
Memory type. The Xbox One utilizes GDDR3 RAM, while the PS4 uses
GDDR5 RAM. This gives the PS4 higher theoretical memory bandwidth.
The Xbox One counteracts this to some degree by providing its GPU
with a dedicated 32 MiB memory store, implemented as very high-speed
eSRAM (embedded static RAM, meaning it lives on the same physical
die as the GPU itself) with a higher theoretical data rate than that of the
PS4.
•
Bus speeds. The buses in the Xbox One support higher bandwidth data
transfers than those of the PS4. For example, while the PS4’s main CPU
bus can transfer data at 20 GiB/second, the Xbox One’s CPU bus op-
erates at a theoretical maximum of 30 GiB/second. Of course, theory
and practice are very different things, and average performance metrics
depend on the speciﬁc ways in which memory is read and written by
the software. Performance data gathered from real game software indi-
cates that typical data rates on both systems are quite a bit lower than
the theoretical maxima most of the time.
•
GPU. The GPU on the Xbox One is not quite as powerful as the GPU on
the PS4. While the PS4’s GPU is roughly equivalent to an AMD Radeon
7870, with 1152 parallel stream processors, the Xbox One’s GPU is closer
to an AMD Radeon 7790, supporting only 768 stream processors. This
leaves less compute power available for performing GPGPU tasks, over
and above what’s needed to render the scene. That said, however, the
Xbox One’s GPU does run at a faster clock rate (853 MHz as opposed to
the PS4’s 800 MHz). And it does take a very industrious programmer to
take advantage of a GPU’s extra horsepower for GPGPU purposes.
•
Operating system and gaming ecosystem. Of course a lot of the value of a
console comes not from its raw power, but from the “ecosystem” around
it. Sony provides its gamers with the PlayStation Network (PSN), while
Microsoft offers its users Xbox Live. The set of games available on each
platform will be different—some games will be exclusive to one or the
other platform. And of course the operating system and overall user
interface of the two consoles will differ. Deciding which one is “better”
is purely a matter of individual taste.
The differences between the Xbox One and PS4 are subtle to be sure, and
it remains to be seen which console will “win” this next generation of console

368
7. The Game Loop and Real-Time Simulation
AMD Jaguar CPU @ 1.75 GHz
CPC 0
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 0
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 1
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 3
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 2
CPC 0
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 4
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 5
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 7
L1 D$
32 KiB
8-way
L1 I$
32 KiB
2-way
Core 6
AMD Radeon GPU
(comparable to 7790)
@ 853 MHz
768 stream processors
30 GiB/s
up to
204 GiB/s
eSRAM
32 MiB
Main RAM
8 GiB GDDR3
Cache Coherent
Memory Access
L2 Cache
2 MiB / 16-way
L2 Cache
2 MiB / 16-way
68 GiB/s
(non cache-coherent)
30 GiB/s
(cache-coherent)
Figure 7.7. Simpliﬁed view of the Xbox One’s architecture.
“wars” (if there can really be a winner). But for our purposes in this book, we
don’t care about any of that. We just want to learn how to program the darn
things. And for that purpose, we can consider the PS4 and Xbox One to be
roughly equivalent architecturally.
See Figure 7.7 for a block diagram of the Xbox One hardware architecture.
7.6.2
SIMD
As we saw in Section 4.7, most modern CPUs (including the Xbox 360’s three
PowerPC processors, and the PS3’s PPU and SPUs) provide a class of instruc-
tions known as single instruction, multiple data (SIMD). Such instructions can
perform a particular operation on more than one piece of data simultaneously,
and as such they represent a ﬁne-grained form of hardware parallelism. CPUs
provide a number of different SIMD instruction variants, but by far the most

7.6. Multiprocessor Game Loops
369
commonly used in games are instructions that operate on four 32-bit ﬂoating-
point values in parallel, because they allow 3D vector and matrix math to be
performed four times more quickly than with their single instruction, single
data (SISD) counterparts.
7.6.3
Fork and Join
Another way to utilize multicore or multiprocessor hardware is to adapt di-
vide-and-conquer algorithms for parallelism. This is often called the fork/join
approach. The basic idea is to divide a unit of work into smaller subunits, dis-
tribute these workloads onto multiple processing cores or hardware threads
(fork), and then merge the results once all workloads have been completed
(join).1 When applied to the game loop, the fork/join architecture results in a
main loop that looks very similar to its single-threaded counterpart, but with
some of the major phases of the update loop being parallelized. This architec-
ture is illustrated in Figure 7.8.
Let’s take a look at a concrete example. Blending animations using linear
interpolation (LERP) is an operation that can be done on each joint indepen-
dently of all other joints within a skeleton (see Section 11.5.2.2). We’ll assume
that we want to blend pairs of skeletal poses for ﬁve characters, each of which
has 100 joints, meaning that we need to process 500 pairs of joint poses.
To parallelize this task, we can divide the work into N batches, each con-
taining roughly 500/N joint-pose pairs, where N is selected based on the
available processing resources. (On the Xbox 360, N should probably be 3
or 6, because the console has three cores with two hardware threads each. On
a PS3, N might range anywhere from 1 to 6, depending on how many SPUs
are available.) We then “fork” (i.e., create) N threads, requesting each one to
work on a different group of pose pairs. The main thread can either continue
doing some useful work that is independent of the animation blending task,
or it can go to sleep, waiting on a semaphore that will tell it when all of the
worker threads have completed their tasks. Finally, we “join” the individ-
ual resultant joint poses into a cohesive whole—in this case by calculating the
ﬁnal global pose of each of our ﬁve skeletons. (The global pose calculation
needs access to the local poses of all the joints in each skeleton, so it doesn’t
parallelize well within a single skeleton. However, we could imagine forking
again to calculate the global pose, this time with each thread working on one
or more whole skeletons.)
1Here we are using the terms “fork” and “join” to describe the general idea of divide-and-
conquer. Do not confuse this with the fork() and wait() UNIX system calls, which represent
one very speciﬁc implementation of the general fork/join concept.

370
7. The Game Loop and Real-Time Simulation
Figure 7.8. Fork and join used to parallelize selected CPU-intensive parts of the game loop.
You can ﬁnd sample code illustrating how to fork and join worker threads
using Win32 system calls at http://msdn.microsoft.com/en-us/library/
ms682516(VS.85).aspx.
7.6.4
One Thread per Subsystem
Yet another approach to multitasking is to assign particular engine subsys-
tems to run in separate threads. A master thread controls and synchronizes
the operations of these secondary subsystem threads and also continues to
handle the lion’s share of the game’s high-level logic (the main game loop).
On a hardware platform with multiple physical CPUs or hardware threads,
this design allows these threaded engine subsystems to execute in parallel.
This design is well suited to any engine subsystem that performs a relatively
isolated function repeatedly, such as a rendering engine, physics simulation,
animation pipeline or audio engine. The architecture is depicted in Figure 7.9.
Threaded architectures are usually supported by some kind of thread li-
brary on the target hardware system. On a personal computer running Win-
dows, the Win32 thread API is usually used. On a UNIX-based system, a
library like pthreads might be the best choice. On the PlayStation 3, a library
known as SPURS permits workloads to be run on the six synergistic process-
ing units (SPUs).
SPURS provides two primary ways to run code on the
SPUs—the task model and the job model. The task model can be used to seg-
regate engine subsystems into coarse-grained independent units of execution

7.6. Multiprocessor Game Loops
371
Skin Matrix 
Palette
Calc
Finalize Animation
Main
Thread
HID
Update Game 
Objects
Kick Off Animation
Kick Dynamics Sim
Finalize Collision
Ragdoll Physics
Kick Rendering
(for next frame)
Pose
Blending
Ragdoll
Skinning
Global 
Pose Calc
Post Animation 
Game Object Update
Sleep
Sleep
Simulate
and
Integrate
Sleep
Sleep
Visibility
Determination
Submit
Primitives
Full-Screen 
Effects
Wait for V-
Blank
Swap Buffers
Sort
Other Processing
(AI Planning, Audio 
Work, etc.)
Broad
Phase Coll.
Narrow
Phase Coll.
Resolve 
Collisions,
Constraints
Wait for GPU
Sleep
Animation
Thread
Dynamics
Thread
Rendering
Thread
Sleep
Figure 7.9. One thread per major engine subsystem.
that act very much like threads. We’ll discuss the SPURS job model in the next
section.
7.6.5
Jobs
One problem with the multithreaded approach is that each thread represents
a relatively coarse-grained chunk of work (e.g., all animation tasks are in one
thread, all collision and physics tasks in another). This can place restrictions
on how the various processors in the system can be utilized. If one of the
subsystem threads has not completed its work, the progress of other threads,
including that of the main game loop, may be blocked.
Another way to take advantage of parallel hardware architecture is to di-
vide up the work that is done by the game engine into multiple small, rela-
tively independent jobs. A job is best thought of as a pairing between a chunk
of data and a bit of code that operates on that data. When a job is ready to
be run, it is placed on a queue to be picked up and worked on by the next
available processing unit. This approach is supported on the PlayStation 3 via

372
7. The Game Loop and Real-Time Simulation
Matrix Palette
Finalize Animation
HID
Update Game 
Objects
Kick Animation Jobs
Kick Dynamics Jobs
Finalize Collision
Ragdoll Physics
Kick Rendering
(for next frame)
Ragdoll
Skinning
Post Animation 
Game Object Update
Other Processing
(AI Planning, Audio 
Work, etc.)
Visibility
Visibility
Sort
Pose Blend
Sort
Visibility
Sort
Pose Blend
Physics Sim
Submit Prims
Gobal Pose
Submit Prims
Gobal Pose
Collisions/
Constraints
Matrix Palette
Ragdoll
Skinning
Visibility
Sort
Visibility
Pose Blend
Visibility
Visibility
Pose Blend
Pose Blend
Sort
Physics Sim
Gobal Pose
Broad Phase
Narrow
Phase
Narrow
Phase
Figure 7.10. In a job model, work is broken down into ﬁne-grained chunks that can be picked up
by any available processor. This can help maximize processor utilization while providing the main
game loop with improved ﬂexibility.
the SPURS job model. The main game loop runs on the PPU, and the six SPUs
are used as job processors. Each job’s code and data are sent to an SPU’s local
store via a DMA transfer. The SPU processes the job, and then it DMAs its
results back to main RAM.
As shown in Figure 7.10, the fact that jobs are relatively ﬁne-grained and
independent of one another helps to maximize processor utilization. It can
also reduce or eliminate some of the restrictions placed on the main thread
in the one-thread-per-subsystem design. This architecture also scales up or
down naturally to hardware with any number of processing units (something
the one-thread-per-subsystem architecture does not do particularly well).
7.6.6
Asynchronous Program Design
When writing or retroﬁtting a game engine to take advantage of multitasking
hardware, programmers must be careful to design their code in an asynchro-
nous manner. This means that the results of an operation will usually not
be available immediately after requesting them, as they would be in a syn-
chronous design. For example, a game might request that a ray be cast into

7.6. Multiprocessor Game Loops
373
the world in order to determine whether the player has line-of-sight to an
enemy character. In a synchronous design, the ray cast would be done imme-
diately in response to the request, and when the ray casting function returned,
the results would be available, as shown below.
while (true) // main game loop
{
// ...
// Cast a ray to see if the player has line of sight
// to the enemy.
RayCastResult r = castRay(playerPos, enemyPos);
// Now process the results...
if (r.hitSomething() && isEnemy(r.getHitObject()))
{
// Player can see the enemy.
// ...
}
// ...
}
In an asynchronous design, a ray cast request would be made by calling
a function that simply sets up and enqueues a ray cast job, and then returns
immediately. The main thread can continue doing other unrelated work while
the job is being processed by another CPU or core. Later, once the job has been
completed, the main thread can pick up the results of the ray cast query and
process them:
while (true) // main game loop
{
// ...
// Cast a ray to see if the player has line of sight
// to the enemy.
RayCastResult r;
requestRayCast(playerPos, enemyPos, &r);
// Do other unrelated work while we wait for the
// other CPU to perform the ray cast for us.
// ...
// OK, we can't do any more useful work. Wait for the
// results of our ray cast job. If the job is

374
7. The Game Loop and Real-Time Simulation
// complete, this function will return immediately.
// Otherwise, the main thread will idle until the
// results are ready...
waitForRayCastResults(&r);
// Process results...
if (r.hitSomething() && isEnemy(r.getHitObject()))
{
// Player can see the enemy.
// ...
}
// ...
}
In many instances, asynchronous code can kick off a request on one frame,
and pick up the results on the next. In this case, you may see code that looks
like this:
RayCastResult r;
bool rayJobPending = false;
while (true) // main game loop
{
// ...
// Wait for the results of last frame's ray cast job.
if (rayJobPending)
{
waitForRayCastResults(&r);
// Process results...
if (r.hitSomething() && isEnemy(r.getHitObject()))
{
// Player can see the enemy.
// ...
}
}
// Cast a new ray for next frame.
rayJobPending = true;
requestRayCast(playerPos, enemyPos, &r);
// Do other work...
// ...
}

7.7. Networked Multiplayer Game Loops
375
7.7
Networked Multiplayer Game Loops
The game loop of a networked multiplayer game is particularly interesting, so
we’ll have a brief look at how such loops are structured. We don’t have room
here to go into all of the details of how multiplayer games work. (Refer to [3]
for an excellent in-depth discussion of the topic.) However, we’ll provide a
brief overview of the two most common multiplayer architectures here, and
then we’ll look at how these architectures affect the structure of the game loop.
7.7.1
Client-Server
In the client-server model, the vast majority of the game’s logic runs on a sin-
gle server machine. Hence the server’s code closely resembles that of a non-
networked single-player game. Multiple client machines can connect to the
server in order to take part in the online game. The client is basically a “dumb”
rendering engine that also reads human interface devices and controls the lo-
cal player character, but otherwise simply renders whatever the server tells
it to render. Great pains are taken in the client code to ensure that the in-
puts of the local human player are immediately translated into the actions of
the player’s character on-screen. This avoids what would otherwise be an
extremely annoying sense of delayed reaction on the part of the player char-
acter. But other than this so-called player prediction code, the client is usually
not much more than a rendering and audio engine, combined with some net-
working code.
The server may be running on a dedicated machine, in which case we say
it is running in dedicated server mode. However, the client and server needn’t
be on separate machines, and in fact it is quite typical for one of the client ma-
chines to also be running the server. In fact, in many client-server multiplayer
games, the single-player game mode is really just a degenerate multiplayer
game, in which there is only one client, and both the client and server are
running on the same machine. This is known as client-on-top-of-server mode.
The game loop of a client-server multiplayer game can be implemented
in a number of different ways. Since the client and server are conceptually
separate entities, they could be implemented as entirely separate processes
(i.e., separate applications). They could also be implemented as two sepa-
rate threads of execution within a single process. However, both of these ap-
proaches require quite a lot of overhead to permit the client and server to
communicate locally, when being run in client-on-top-of-server mode. As a
result, a lot of multiplayer games run both client and server in a single thread,
serviced by a single game loop.

376
7. The Game Loop and Real-Time Simulation
It’s important to realize that the client and server code can be updated at
different rates. For example, in Quake, the server runs at 20 FPS (50 ms per
frame), while the client typically runs at 60 FPS (16.6 ms per frame). This is
implemented by running the main game loop at the faster of the two rates
(60 FPS) and then servicing the server code once roughly every three frames.
In reality, the amount of time that has elapsed since the last server update is
tracked, and when it reaches or exceeds 50 ms, a server frame is run and the
timer is reset. Such a game loop might look something like this:
F32 dtReal = 1.0f/30.0f; // the real frame delta time
F32 dtServer = 0.0f;
// the server's delta time
U64 begin_ticks = readHiResTimer();
while (true) // main game loop
{
// Run the server at 50 ms intervals.
dtServer += dtReal;
if (dtServer >= 0.05f)
// 50 ms
{
runServerFrame(0.05f);
dtServer -= 0.05f;
// reset for next update
}
// Run the client at maximum frame rate.
runClientFrame(dtReal);
// Read the current time, and calculate an estimate
// of next frame's real delta time.
U64 end_ticks = readHiResTimer();
dtReal = (F32)(end_ticks - begin_ticks)
/ (F32)getHiResTimerFrequency();
// Use end_ticks as the new begin_ticks for next frame.
begin_ticks = end_ticks;
}
7.7.2
Peer-to-Peer
In the peer-to-peer multiplayer architecture, every machine in the online game
acts somewhat like a server and somewhat like a client. One and only one ma-
chine has authority over each dynamic object in the game. So, each machine
acts like a server for those objects over which it has authority. For all other

7.7. Networked Multiplayer Game Loops
377
objects in the game world, the machine acts like a client, rendering the objects
in whatever state is provided to it by that object’s remote authority.
The structure of a peer-to-peer multiplayer game loop is much simpler
than a client-server game loop, in that at the top-most level it looks very much
like a single-player game loop. However, the internal details of the code can
be a bit more confusing. In a client-server model, it is usually quite clear
which code is running on the server and which code is client-side. But in a
peer-to-peer architecture, much of the code needs to be set up to handle two
possible cases: one in which the local machine has authority over the state of
an object in the game, and one in which the object is just a dumb proxy for
a remote authoritative representation. These two modes of operation are of-
ten implemented by having two kinds of game objects—a full-ﬂedged “real”
game object, over which the local machine has authority and a “proxy” ver-
sion that contains a minimal subset of the state of the remote object.
Peer-to-peer architectures are made even more complex because authority
over an object sometimes needs to migrate from machine to machine. For ex-
ample, if one computer drops out of the game, all of the objects over which it
had authority must be picked up by the other machines in the game. Likewise,
when a new machine joins the game, it should ideally take over authority of
some game objects from other machines in order to balance the load. The
details are beyond the scope of this book. The key point here is that multi-
player architectures can have profound effects on the structure of a game’s
main loop.
7.7.3
Case Study: Quake II
The following is an excerpt from the Quake II game loop. The source code for
Quake, Quake II and Quake 3 Arena is available on Id Software’s website, http:
//www.idsoftware.com. As you can see, all of the elements we’ve discussed
are present, including the Windows message pump (in the Win32 version of
the game), calculation of the real frame delta time, ﬁxed-time and time-scaled
modes of operation, and servicing of both server-side and client-side engine
systems.
int WINAPI WinMain (HINSTANCE hInstance,
HINSTANCE hPrevInstance,
LPSTR lpCmdLine, int nCmdShow)
{
MSG
msg;
int
time, oldtime, newtime;
char
*cddir;

378
7. The Game Loop and Real-Time Simulation
ParseCommandLine (lpCmdLine);
Qcommon_Init (argc, argv);
oldtime = Sys_Milliseconds ();
/* main window message loop */
while (1)
{
// Windows message pump.
while (PeekMessage (&msg, NULL, 0, 0,
PM_NOREMOVE))
{
if (!GetMessage (&msg, NULL, 0, 0))
Com_Quit ();
sys_msg_time = msg.time;
TranslateMessage (&msg);
DispatchMessage (&msg);
}
// Measure real delta time in milliseconds.
do
{
newtime = Sys_Milliseconds ();
time = newtime - oldtime;
} while (time < 1);
// Run a frame of the game.
Qcommon_Frame (time);
oldtime = newtime;
}
// never gets here
return TRUE;
}
void Qcommon_Frame (int msec)
{
char *s;
int
time_before, time_between, time_after;
// [some details omitted...]

7.7. Networked Multiplayer Game Loops
379
// Handle fixed-time mode and time scaling.
if (fixedtime->value)
msec = fixedtime->value;
else if (timescale->value)
{
msec *= timescale->value;
if (msec < 1)
msec = 1;
}
// Service the in-game console.
do
{
s = Sys_ConsoleInput ();
if (s)
Cbuf_AddText (va("%s\n",s));
} while (s);
Cbuf_Execute ();
// Run a server frame.
SV_Frame (msec);
// Run a client frame.
CL_Frame (msec);
// [some details omitted...]
}

This page intentionally left blank
This page intentionally left blank

8
Human Interface
Devices (HID)
G
ames are interactive computer simulations, so the human player(s) need
some way of providing inputs to the game. All sorts of human interface
devices (HID) exist for gaming, including joysticks, joypads, keyboards and
mice, track balls, the Wii remote and specialized input devices like steering
wheels, ﬁshing rods, dance pads and even electric guitars. In this chapter,
we’ll investigate how game engines typically read, process and utilize the in-
puts from human interface devices. We’ll also have a look at how outputs
from these devices provide feedback to the human player.
8.1
Types of Human Interface Devices
A wide range of human interface devices are available for gaming purposes.
Consoles like the Xbox 360 and PS3 come equipped with joypad controllers,
as shown in Figures 8.1 and 8.2. Nintendo’s Wii console is well known for
its unique and innovative Wii Remote controller (commonly referred to as the
“Wiimote”), shown in Figure 8.3. And with the Wii U, Nintendo has created
an innovative mix between a controller and a semi-mobile gaming device (Fig-
ure 8.4). PC games are generally either controlled via a keyboard and mouse
or via a joypad. (Microsoft designed the Xbox 360 joypad so that it can be used
both on the Xbox 360 and on Windows/DirectX PC platforms.) As shown in
381

382
8. Human Interface Devices (HID)
Figure 8.1. Standard joypads for the Xbox 360 and PlaySta-
tion 3 consoles.
Figure 8.2. The DualShock 4 joypad for the PlayStation 4.
Figure 8.3. The innovative Wii Remote for the Nintendo Wii.
Figure 8.4. The Wii U controller by Nintendo.
Figure 8.5, arcade machines have one or more built-in controllers, such as a
joystick and various buttons, or a track ball, a steering wheel, etc. An ar-
cade machine’s input device is usually somewhat customized to the game in
question, although input hardware is often reused among arcade machines
produced by the same manufacturer.
On console platforms, specialized input devices and adapters are usually
available, in addition to the “standard” input device such as the joypad. For
Figure 8.5. Buttons and joysticks for the arcade game Mortal Kombat II by Midway.

8.2. Interfacing with a HID
383
Figure 8.6. Many specialized input devices are available for use with consoles.
Figure 8.7. Steering wheel adapter for the Nintendo Wii.
example, guitar and drum devices are available for the Guitar Hero series of
games, steering wheels can be purchased for driving games, and games like
Dance Dance Revolution use a special dance pad device. Some of these devices
are shown in Figure 8.6.
The Nintendo Wiimote is one of the most ﬂexible input devices on the
market today. As such, it is often adapted to new purposes, rather than re-
placed with an entirely new device. For example, Mario Kart Wii comes with
a plastic steering wheel adapter into which the Wiimote can be inserted (see
Figure 8.7).
8.2
Interfacing with a HID
All human interface devices provide input to the game software, and some
also allow the software to provide feedback to the human player via various
kinds of outputs as well. Game software reads and writes HID inputs and
outputs in various ways, depending on the speciﬁc design of the device in
question.

384
8. Human Interface Devices (HID)
8.2.1
Polling
Some simple devices, like game pads and old-school joysticks, are read by
polling the hardware periodically (usually once per iteration of the main game
loop). This means explicitly querying the state of the device, either by read-
ing hardware registers directly, reading a memory-mapped I/O port, or via
a higher-level software interface (which, in turn, reads the appropriate regis-
ters or memory-mapped I/O ports). Likewise, outputs might be sent to the
HID by writing to special registers or memory-mapped I/O addresses, or via
a higher-level API that does our dirty work for us.
Microsoft’s XInput API, for use with Xbox 360 game pads on both the Xbox
360 and Windows PC platforms, is a good example of a simple polling mech-
anism. Every frame, the game calls the function XInputGetState(). This
function communicates with the hardware and/or drivers, reads the data in
the appropriate way and packages it all up for convenient use by the software.
It returns a pointer to an XINPUT_STATE struct, which in turn contains an em-
bedded instance of a struct called XINPUT_GAMEPAD. This struct contains the
current states of all of the controls (buttons, thumb sticks and triggers) on the
device.
8.2.2
Interrupts
Some HIDs only send data to the game engine when the state of the controller
changes in some way. For example, a mouse spends a lot of its time just sitting
still on the mouse pad. There’s no reason to send a continuous stream of
data between the mouse and the computer when the mouse isn’t moving—
we need only transmit information when it moves, or a button is pressed or
released.
This kind of device usually communicates with the host computer via hard-
ware interrupts. An interrupt is an electronic signal generated by the hardware,
which causes the CPU to temporarily suspend execution of the main program
and run a small chunk of code called an interrupt service routine (ISR). Inter-
rupts are used for all sorts of things, but in the case of a HID, the ISR code will
probably read the state of the device, store it off for later processing, and then
relinquish the CPU back to the main program. The game engine can pick up
the data the next time it is convenient to do so.
8.2.3
Wireless Devices
The inputs and outputs of a Bluetooth device, like the Wiimote, the Dual-
Shock 3 and the Xbox 360 wireless controller, cannot be read and written by

8.3. Types of Inputs
385
simply accessing registers or memory-mapped I/O ports. Instead, the soft-
ware must “talk” to the device via the Bluetooth protocol. The software can
request the HID to send input data (such as the states of its buttons) back to
the host, or it can send output data (such as rumble settings or a stream of
audio data) to the device. This communication is often handled by a thread
separate from the game engine’s main loop, or at least encapsulated behind a
relatively simple interface that can be called from the main loop. So from the
point of view of the game programmer, the state of a Bluetooth device can be
made to look pretty much indistinguishable from a traditional polled device.
8.3
Types of Inputs
Although human interface devices for games vary widely in terms of form
factor and layout, most of the inputs they provide fall into one of a small
number of categories. We’ll investigate each category in depth below.
8.3.1
Digital Buttons
Almost every HID has at least a few digital buttons. These are buttons that can
only be in one of two states: pressed and not pressed. Game programmers often
refer to a pressed button as being down and a non-pressed button as being up.
Electrical engineers speak of a circuit containing a switch as being closed
(meaning electricity is ﬂowing through the circuit) or open (no electricity is
ﬂowing—the circuit has inﬁnite resistance).
Whether closed corresponds to
pressed or not pressed depends on the hardware. If the switch is normally open,
then when it is not pressed (up), the circuit is open, and when it is pressed
(down), the circuit is closed. If the switch is normally closed, the reverse is true—
the act of pressing the button opens the circuit.
In software, the state of a digital button (pressed or not pressed) is usually
represented by a single bit. It’s common for 0 to represent not pressed (up)
and 1 to represent pressed (down). But again, depending on the nature of the
circuitry and the decisions made by the programmers who wrote the device
driver, the sense of these values might be reversed.
It is quite common for the states of all of the buttons on a device to be
packed into a single unsigned integer value. For example, in Microsoft’s XIn-
put API, the state of the Xbox 360 joypad is returned in a struct called XINPUT
_GAMEPAD, shown below.
typedef struct _XINPUT_GAMEPAD
{
WORD
wButtons;

386
8. Human Interface Devices (HID)
BYTE
bLeftTrigger;
BYTE
bRightTrigger;
SHORT sThumbLX;
SHORT sThumbLY;
SHORT sThumbRX;
SHORT sThumbRY;
} XINPUT_GAMEPAD;
This struct contains a 16-bit unsigned integer (WORD) variable named
wButtons that holds the state of all buttons. The following masks deﬁne
which physical button corresponds to each bit in the word. (Note that bits 10
and 11 are unused.)
#define XINPUT_GAMEPAD_DPAD_UP
0x0001 // bit 0
#define XINPUT_GAMEPAD_DPAD_DOWN
0x0002 // bit 1
#define XINPUT_GAMEPAD_DPAD_LEFT
0x0004 // bit 2
#define XINPUT_GAMEPAD_DPAD_RIGHT
0x0008 // bit 3
#define XINPUT_GAMEPAD_START
0x0010 // bit 4
#define XINPUT_GAMEPAD_BACK
0x0020 // bit 5
#define XINPUT_GAMEPAD_LEFT_THUMB
0x0040 // bit 6
#define XINPUT_GAMEPAD_RIGHT_THUMB
0x0080 // bit 7
#define XINPUT_GAMEPAD_LEFT_SHOULDER
0x0100 // bit 8
#define XINPUT_GAMEPAD_RIGHT_SHOULDER
0x0200 // bit 9
#define XINPUT_GAMEPAD_A
0x1000 // bit 12
#define XINPUT_GAMEPAD_B
0x2000 // bit 13
#define XINPUT_GAMEPAD_X
0x4000 // bit 14
#define XINPUT_GAMEPAD_Y
0x8000 // bit 15
An individual button’s state can be read by masking the wButtons word
with the appropriate bitmask via C/C++’s bitwise AND operator (&) and then
checking if the result is nonzero. For example, to determine if the A button is
pressed (down), we would write:
bool IsButtonADown(const XINPUT_GAMEPAD& pad)
{
// Mask off all bits but bit 12 (the A button).
return ((pad.wButtons & XINPUT_GAMEPAD_A) != 0);
}
8.3.2
Analog Axes and Buttons
An analog input is one that can take on a range of values (rather than just 0
or 1). These kinds of inputs are often used to represent the degree to which
a trigger is pressed, or the two-dimensional position of a joystick (which is
represented using two analog inputs, one for the x-axis and one for the y-axis,

8.3. Types of Inputs
387
Figure 8.8. Two analog inputs can be used to represent the x and y deﬂection of a joystick.
as shown in Figure 8.8). Because of this common usage, analog inputs are
sometimes called analog axes, or just axes.
On some devices, certain buttons are analog as well, meaning that the
game can actually detect how hard the player is pressing on them. However,
the signals produced by analog buttons are usually too noisy to be particu-
larly usable. Games that use analog button inputs effectively are rare. One
good example is Metal Gear Solid 2 on the PS2. It uses pressure-sensitive (ana-
log) button data in aim mode to tell the difference between releasing the X
button quickly (which ﬁres the weapon) and releasing it slowly (which aborts
the shot)—a useful feature in a stealth game, where you don’t want to alert
the enemies unless you have to!
Strictly speaking, analog inputs are not really analog by the time they
make it to the game engine. An analog input signal is usually digitized, mean-
ing it is quantized and represented using an integer in software. For example,
an analog input might range from −32,768 to 32,767 if represented by a 16-bit
signed integer. Sometimes analog inputs are converted to ﬂoating point—the
values might range from −1 to 1, for instance. But as we know from Section
3.2.1.3, ﬂoating-point numbers are really just quantized digital values as well.
Reviewing the deﬁnition of XINPUT_GAMEPAD (repeated below), we can
see that Microsoft chose to represent the deﬂections of the left and right thumb
sticks on the Xbox 360 gamepad using 16-bit signed integers (sThumbLX and
sThumbLY for the left stick and sThumbRX and sThumbRY for the right).
Hence, these values range from −32,768 (left or down) to 32,767 (right or
up). However, to represent the positions of the left and right shoulder trig-
gers, Microsoft chose to use eight-bit unsigned integers (bLeftTrigger and
bRightTrigger respectively). These input values range from 0 (not pressed)
to 255 (fully pressed). Different game machines use different digital represen-
tions for their analog axes.

388
8. Human Interface Devices (HID)
typedef struct _XINPUT_GAMEPAD
{
WORD
wButtons;
// 8-bit unsigned
BYTE
bLeftTrigger;
BYTE
bRightTrigger;
// 16-bit signed
SHORT sThumbLX;
SHORT sThumbLY;
SHORT sThumbRX;
SHORT sThumbRY;
} XINPUT_GAMEPAD;
8.3.3
Relative Axes
The position of an analog button, trigger, joystick or thumb stick is absolute,
meaning that there is a clear understanding of where zero lies. However, the
inputs of some devices are relative. For these devices, there is no clear location
at which the input value should be zero. Instead, a zero input indicates that
the position of the device has not changed, while nonzero values represent
a delta from the last time the input value was read. Examples include mice,
mouse wheels and track balls.
8.3.4
Accelerometers
The PlayStation’s DualShock joypads and the Nintendo Wiimote all contain
acceleration sensors (accelerometers). These devices can detect acceleration
along the three principle axes (x, y and z), as shown in Figure 8.9. These are
relative analog inputs, much like a mouse’s two-dimensional axes. When the
controller is not accelerating these inputs are zero, but when the controller is
accelerating, they measure the acceleration up to ±3 g along each axis, quan-
tized into three signed eight-bit integers, one for each of x, y and z.
Figure 8.9. Accelerometer axes for the Wiimote.

8.3. Types of Inputs
389
8.3.5
3D Orientation with the Wiimote or DualShock
Some Wii and PS3 games make use of the three accelerometers in the Wi-
imote or DualShock joypad to estimate the orientation of the controller in the
player’s hand. For example, in Super Mario Galaxy, Mario hops onto a large
ball and rolls it around with his feet. To control Mario in this mode, the Wi-
imote is held with the IR sensor facing the ceiling. Tilting the Wiimote left,
right, forward or back causes the ball to accelerate in the corresponding direc-
tion.
A trio of accelerometers can be used to detect the orientation of Wiimote
or DualShock joypad, because of the fact that we are playing these games on
the surface of the Earth where there is a constant downward acceleration due
to gravity of 1 g (≈9.8 m/s2). If the controller is held perfectly level, with the
IR sensor pointing toward your TV set, the vertical (z) acceleration should be
approximately −1 g.
If the controller is held upright, with the IR sensor pointing toward the
ceiling, we would expect to see a 0 g acceleration on the z sensor, and +1 g
on the y sensor (because it is now experiencing the full gravitational effect).
Holding the Wiimote at a 45-degree angle should produce roughly sin(45◦) =
cos(45◦) = 0.707 g on both the y and z inputs. Once we’ve calibrated the
accelerometer inputs to ﬁnd the zero points along each axis, we can calculate
pitch, yaw and roll easily, using inverse sine and cosine operations.
Two caveats here: First, if the person holding the Wiimote is not holding
it still, the accelerometer inputs will include this acceleration in their values,
invalidating our math. Second, the z-axis of the accelerometer has been cal-
ibrated to account for gravity, but the other two axes have not. This means
that the z-axis has less precision available for detecting orientation. Many
Wii games request that the user hold the Wiimote in a nonstandard orienta-
tion, such as with the buttons facing the player’s chest, or with the IR sen-
sor pointing toward the ceiling. This maximizes the precision of the orien-
tation reading by placing the x- or y-accelerometer axis in line with gravity,
instead of the gravity-calibrated z-axis. For more information on this topic,
see http://druid.caughq.org/presentations/turbo/Wiimote-Hacking.pdf.
8.3.6
Cameras
The Wiimote has a unique feature not found on any other standard console
HID—an infrared (IR) sensor. This sensor is essentially a low-resolution cam-
era that records a two-dimension infrared image of whatever the Wiimote is
pointed at. The Wii comes with a “sensor bar” that sits on top of your televi-
sion set and contains two infrared light emitting diodes (LEDs). In the image

390
8. Human Interface Devices (HID)
Image Recorded by 
Infrared Camera
Sensor Bar
Figure 8.10. The Wii sensor bar houses two infrared LEDs, which produce two bright spots on
the image recorded by the Wiimote’s IR camera.
recorded by the IR camera, these LEDs appear as two bright dots on an oth-
erwise dark background. Image processing software in the Wiimote analyzes
the image and isolates the location and size of the two dots. (Actually, it can
detect and transmit the locations and sizes of up to four dots.) This position
and size information can be read by the console via a Bluetooth wireless con-
nection.
The position and orientation of the line segment formed by the two dots
can be used to determine the pitch, yaw and roll of the Wiimote (as long as it
is being pointed toward the sensor bar). By looking at the separation between
the dots, software can also determine how close or far away the Wiimote is
from the TV. Some software also makes use of the sizes of the dots. This is
illustrated in Figure 8.10.
Figure 8.11.
Sony’s
PlayStation Eye for
the PS3.
Another popular camera device is Sony’s PlayStation Eye for the PS3,
shown in Figure 8.11. This device is basically a high-quality color camera,
which can be used for a wide range of applications. It can be used for sim-
ple video conferencing, like any web cam. It could also conceivably be used
much like the Wiimote’s IR camera, for position, orientation and depth sens-
ing. The gamut of possibilities for these kinds of advanced input devices has
only begun to be tapped by the gaming community.
With the PlayStation 4, Sony has improved the Eye and re-dubbed it the
PlayStation Camera. When combined with the PlayStation Move controller
(see Figure 8.12) or the DualShock 4 controller, the PlayStation can detect ges-
tures in basically the same way that Microsoft’s innovative Kinect system (Fig-
ure 8.13).

8.4. Types of Outputs
391
Figure 8.12. Sony’s PlayStation Camera, PlayStation Move controller and DualShock 4 joypad for the PS4.
Figure 8.13. The Microsoft Kinect for Xbox 360 (top) and Xbox One (bottom).
8.4
Types of Outputs
Human interface devices are primarily used to transmit inputs from the player
to the game software. However, some HIDs can also provide feedback to the
human player via various kinds of outputs.
8.4.1
Rumble
Game pads like the PlayStation’s DualShock line of controllers and the Xbox
and Xbox 360 controllers have a rumble feature. This allows the controller to
vibrate in the player’s hands, simulating the turbulence or impacts that the
character in the game world might be experiencing. Vibrations are usually

392
8. Human Interface Devices (HID)
produced by one or more motors, each of which rotates a slightly unbalanced
weight at various speeds. The game can turn these motors on and off, and
control their speeds to produce different tactile effects in the player’s hands.
8.4.2
Force-Feedback
Force-feedback is a technique in which an actuator on the HID is driven by
a motor in order to slightly resist the motion the human operator is trying to
impart to it. It is common in arcade driving games, where the steering wheel
resists the player’s attempt to turn it, simulating difﬁcult driving conditions or
tight turns. As with rumble, the game software can typically turn the motor(s)
on and off, and can also control the strength and direction of the forces applied
to the actuator.
8.4.3
Audio
Audio is usually a stand-alone engine system. However, some HIDs provide
outputs that can be utilized by the audio system. For example, the Wiimote
contains a small, low-quality speaker. The Xbox 360, Xbox One and Dual-
Shock 4 controllers have a headphone jack and can be used just like any USB
audio device for both output (speakers) and input (microphone). One com-
mon use of USB headsets is for multiplayer games, in which human players
can communicate with one another via a voice over IP (VoIP) connection.
8.4.4
Other Inputs and Outputs
Human interface devices may of course support many other kinds of inputs
and outputs. On some older consoles like the Sega Dreamcast, the memory
card slots were located on the game pad. The Xbox 360 game pad, the Sixaxis
and DualShock 3, and the Wiimote all have four LEDs which can be illumi-
nated by game software if desired. The color of the light bar on the front of
the DualShock 4 controller can be controlled by game software. And of course
specialized devices like musical instruments, dance pads, etc. have their own
particular kinds of inputs and outputs.
Innovation is actively taking place in the ﬁeld of human interfaces. Some
of the most interesting areas today are gestural interfaces and thought-con-
trolled devices. We can certainly expect more innovation from console and
HID manufacturers in years to come.
8.5
Game Engine HID Systems
Most game engines don’t use “raw” HID inputs directly. The data is usu-
ally massaged in various ways to ensure that the inputs coming from the HID

8.5. Game Engine HID Systems
393
translate into smooth, pleasing, intuitive behaviors in-game. In addition, most
engines introduce at least one additional level of indirection between the HID
and the game in order to abstract HID inputs in various ways. For example, a
button-mapping table might be used to translate raw button inputs into log-
ical game actions, so that human players can reassign the buttons’ functions
as they see ﬁt. In this section, we’ll outline the typical requirements of a game
engine HID system and then explore each one in some depth.
8.5.1
Typical Requirements
A game engine’s HID system usually provides some or all of the following
features:
•
dead zones,
•
analog signal ﬁltering,
•
event detection (e.g., button up, button down),
•
detection of button sequences and multibutton combinations (known as
chords),
•
gesture detection,
•
management of multiple HIDs for multiple players,
•
multiplatform HID support,
•
controller input remapping,
•
context-sensitive inputs, and
•
the ability to temporarily disable certain inputs.
8.5.2
Dead Zone
A joystick, thumb stick, shoulder trigger, or any other analog axis produces
input values that range between a predeﬁned minimum and maximum value,
which we’ll call Imin and Imax. When the control is not being touched, we
would expect it to produce a steady and clear “undisturbed” value, which
we’ll call I0. The undisturbed value is usually numerically equal to zero, and
it either lies halfway between Imin and Imax for a centered, two-way control
like a joystick axis, or it coincides with Imin for a one-way control like a trigger.
Unfortunately, because HIDs are analog devices by nature, the voltage pro-
duced by the device is noisy, and the actual inputs we observe may ﬂuctuate
slightly around I0. The most common solution to this problem is to introduce
a small dead zone around I0. The dead zone might be deﬁned as [ I0 −δ, I0 +δ ]
for a joy stick, or [ I0, I0 +δ ] for a trigger. Any input values that are within the
dead zone are simply clamped to I0. The dead zone must be wide enough to

394
8. Human Interface Devices (HID)
account for the noisiest inputs generated by an undisturbed control, but small
enough not to interfere with the player’s sense of the HID’s responsiveness.
8.5.3
Analog Signal Filtering
Signal noise is a problem even when the controls are not within their dead
zones. This noise can sometimes cause the in-game behaviors controlled by
the HID to appear jerky or unnatural. For this reason, many games ﬁlter the
raw inputs coming from the HID. A noise signal is usually of a high frequency
relative to the signal produced by the human player. Therefore, one solution
is to pass the raw input data through a simple low-pass ﬁlter, prior to it being
used by the game.
A discrete ﬁrst-order low-pass ﬁlter can be implemented by combining the
current unﬁltered input value with last frame’s ﬁltered input. If we denote
the sequence of unﬁltered inputs by the time-varying function u(t) and the
ﬁltered inputs by f(t), where t denotes time, then we can write
f(t) = (1 −a)f(t −∆t) + au(t),
(8.1)
where the parameter a is determined by the frame duration ∆t and a ﬁltering
constant RC (which is just the product of the resistance and the capacitance
in a traditional analog RC low-pass ﬁlter circuit):
a =
∆t
RC + ∆t.
(8.2)
This can be implemented trivially in C or C++ as follows, where it is as-
sumed the calling code will keep track of last frame’s ﬁltered input for use
on the subsequent frame. For more information, see http://en.wikipedia.org/
wiki/Low-pass_ﬁlter.
F32 lowPassFilter(F32 unfilteredInput,
F32 lastFramesFilteredInput,
F32 rc, F32 dt)
{
F32 a = dt / (rc + dt);
return (1 - a) * lastFramesFilteredInput
+ a * unfilteredInput;
}
Another way to ﬁlter HID input data is to calculate a simple moving av-
erage. For example, if we wish to average the input data over a 3/30 second
(3 frame) interval, we simply store the raw input values in a 3-element circu-
lar buffer. The ﬁltered input value is then the sum of the values in this array

8.5. Game Engine HID Systems
395
at any moment, divided by 3. There are a few minor details to account for
when implementing such a ﬁlter. For example, we need to properly handle
the ﬁrst two frames of input, during which the 3-element array has not yet
been ﬁlled with valid data. However, the implementation is not particularly
complicated. The code below shows one way to properly implement an N-
element moving average.
template< typename TYPE, int SIZE >
class MovingAverage
{
TYPE
m_samples[SIZE];
TYPE
m_sum;
U32
m_curSample;
U32
m_sampleCount;
public:
MovingAverage() :
m_sum(static_cast<TYPE>(0)),
m_curSample(0),
m_sampleCount(0)
{
}
void addSample(TYPE data)
{
if (m_sampleCount == SIZE)
{
m_sum -= m_samples[m_curSample];
}
else
{
m_sampleCount++;
}
m_samples[m_curSample] = data;
m_sum += data;
m_curSample++;
if (m_curSample >= SIZE)
{
m_curSample = 0;
}
}
F32 getCurrentAverage() const
{

396
8. Human Interface Devices (HID)
if (m_sampleCount != 0)
{
return static_cast<F32>(m_sum)
/ static_cast<F32>(m_sampleCount);
}
return 0.0f;
}
};
8.5.4
Detecting Input Events
The low-level HID interface typically provides the game with the current
states of the device’s various inputs. However, games are often interested
in detecting events, such as changes in state, rather than just inspecting the
current state each frame. The most common HID events are probably button-
down (pressed) and button-up (released), but of course we can detect other
kinds of events as well.
8.5.4.1
Button Up and Button Down
Let’s assume for the moment that our buttons’ input bits are 0 when not
pressed and 1 when pressed. The easiest way to detect a change in button
state is to keep track of the buttons’ state bits as observed last frame and com-
pare them to the state bits observed this frame. If they differ, we know an
event occurred. The current state of each button tells us whether the event is
a button-up or a button-down.
We can use simple bit-wise operators to detect button-down and button-
up events.
Given a 32-bit word buttonStates containing the current
state bits of up to 32 buttons, we want to generate two new 32-bit words:
one for button-down events, which we’ll call buttonDowns, and one for
button-up events, which we’ll call buttonUps. In both cases, the bit corre-
sponding to each button will be 0 if the event has not occurred this frame
and 1 if it has. To implement this, we also need last frame’s button states,
prevButtonStates.
The exclusive OR (XOR) operator produces a 0 if its two inputs are identi-
cal and a 1 if they differ. So if we apply the XOR operator to the previous and
current button state words, we’ll get 1’s only for buttons whose states have
changed between last frame and this frame. To determine whether the event
is a button-up or a button-down, we need to look at the current state of each
button. Any button whose state has changed that is currently down generates
a button-down event, and vice versa for button-up events. The following code
applies these ideas in order to generate our two button event words:

8.5. Game Engine HID Systems
397
class ButtonState
{
U32 m_buttonStates;
// current frame's button
// states
U32 m_prevButtonStates;
// previous frame's states
U32 m_buttonDowns;
// 1 = button pressed this
// frame
U32 m_buttonUps;
// 1 = button released this
// frame
void DetectButtonUpDownEvents()
{
// Assuming that m_buttonStates and
// m_prevButtonStates are valid, generate
// m_buttonDowns and m_buttonUps.
// First determine which bits have changed via
// XOR.
U32 buttonChanges = m_buttonStates
^ m_prevButtonStates;
// Now use AND to mask off only the bits that
// are DOWN.
m_buttonDowns = buttonChanges & m_buttonStates;
// Use AND-NOT to mask off only the bits that
// are UP.
m_buttonUps = buttonChanges & (~m_buttonStates);
}
// ...
};
8.5.4.2
Chords
A chord is a group of buttons that, when pressed at the same time, produce a
unique behavior in the game. Here are a few examples:
•
Super Mario Galaxy’s start-up screen requires you to press the A and B
buttons on the Wiimote together in order to start a new game.
•
Pressing the 1 and 2 buttons on the Wiimote at the same time put it into
Bluetooth discovery mode (no matter what game you’re playing).
•
The “grapple” move in many ﬁghting games is triggered by a two-button
combination.

398
8. Human Interface Devices (HID)
•
For development purposes, holding down both the left and right trig-
gers on the DualShock 3 in Uncharted allows the player character to ﬂy
anywhere in the game world, with collisions turned off. (Sorry, this
doesn’t work in the shipping game!) Many games have a cheat like
this to make development easier. (It may or may not be triggered by
a chord, of course.) It is called no-clip mode in the Quake engine, because
the character’s collision volume is not clipped to the valid playable area
of the world. Other engines use different terminology.
Detecting chords is quite simple in principle: We merely watch the states
of two or more buttons and only perform the requested operation when all of
them are down.
There are some subtleties to account for, however. For one thing, if the
chord includes a button or buttons that have other purposes in the game, we
must take care not to perform both the actions of the individual buttons and
the action of chord when it is pressed. This is usually done by including a
check that the other buttons in the chord are not down when detecting the
individual button presses.
Another ﬂy in the ointment is that humans aren’t perfect, and they often
press one or more of the buttons in the chord slightly earlier than the rest. So
our chord-detection code must be robust to the possibility that we’ll observe
one or more individual buttons on frame i and the rest of the chord on frame
i + 1 (or even multiple frames later). There are a number of ways to handle
this:
•
You can design your button inputs such that a chord always does the
actions of the individual buttons plus some additional action. For exam-
ple, if pressing L1 ﬁres the primary weapon and L2 lobs a grenade, per-
haps the L1 + L2 chord could ﬁre the primary weapon, lob a grenade,
and send out an energy wave that doubles the damage done by these
weapons. That way, whether or not the individual buttons are detected
before the chord or not, the behavior will be identical from the point of
view of the player.
•
You can introduce a delay between when an individual button-down
event is seen and when it “counts” as a valid game event. During the
delay period (say 2 or 3 frames), if a chord is detected, then it takes
precedence over the individual button-down events. This gives the hu-
man player some leeway in performing the chord.
•
You can detect the chord when the buttons are pressed, but wait to trig-
ger the effect until the buttons are released again.
•
You can begin the single-button move immediately and allow it to be
preempted by the chord move.

8.5. Game Engine HID Systems
399
8.5.4.3
Sequences and Gesture Detection
The idea of introducing a delay between when a button actually goes down
and when it really “counts” as down is a special case of gesture detection. A
gesture is a sequence of actions performed via a HID by the human player
over a period of time. For example, in a ﬁghting game or brawler, we might
want to detect a sequence of button presses, such as A-B-A. We can extend this
idea to non-button inputs as well. For example, A-B-A-Left-Right-Left, where
the latter three actions are side-to-side motions of one of the thumb sticks on
the game pad. Usually a sequence or gesture is only considered to be valid if
it is performed within some maximum time frame. So a rapid A-B-A within a
quarter of a second might “count,” but a slow A-B-A performed over a second
or two might not.
Gesture detection is generally implemented by keeping a brief history of
the HID actions performed by the player. When the ﬁrst component of the
gesture is detected, it is stored in the history buffer, along with a time stamp
indicating when it occurred. As each subsequent component is detected, the
time between it and the previous component is checked. If it is within the
allowable time window, it too is added to the history buffer. If the entire
sequence is completed within the allotted time (i.e., the history buffer is ﬁlled),
an event is generated telling the rest of the game engine that the gesture has
occurred. However, if any non-valid intervening inputs are detected, or if any
component of the gesture occurs outside of its valid time window, the entire
history buffer is reset and the player must start the gesture over again.
Let’s look at three concrete examples, so we can really understand how
this works.
Rapid Button Tapping
Many games require the user to tap a button rapidly in order to perform an
action. The frequency of the button presses may or may not translate into
some quantity in the game, such as the speed with which the player character
runs or performs some other action. The frequency is usually also used to de-
ﬁne the validity of the gesture—if the frequency drops below some minimum
value, the gesture is no longer considered valid.
We can detect the frequency of a button press by simply keeping track of
the last time we saw a button-down event for the button in question. We’ll call
this Tlast. The frequency f is then just the inverse of the time interval between
presses ∆T = Tcur −Tlast and f = 1/∆T. Every time we detect a new button-
down event, we calculate a new frequency f. To implement a minimum valid
frequency, we simply check f against the minimum frequency fmin (or we can

400
8. Human Interface Devices (HID)
just check ∆T against the maximum period ∆Tmax = 1/fmin directly). If this
threshold is satisﬁed, we update the value of Tlast, and the gesture is consid-
ered to be on-going. If the threshold is not satisﬁed, we simply don’t update
Tlast. The gesture will be considered invalid until a new pair of rapid-enough
button-down events occurs. This is illustrated by the following pseudocode:
class ButtonTapDetector
{
U32
m_buttonMask; // which button to observe (bit
// mask)
F32
m_dtMax;
// max allowed time between
// presses
F32
m_tLast;
// last button-down event, in
// seconds
public:
// Construct an object that detects rapid tapping of
// the given button (identified by an index).
ButtonTapDetector(U32 buttonId, F32 dtMax) :
m_buttonMask(1U << buttonId),
m_dtMax(dtMax),
m_tLast(CurrentTime() - dtMax) // start out
// invalid
{
}
// Call this at any time to query whether or not
// the gesture is currently being performed.
void IsGestureValid() const
{
F32 t = CurrentTime();
F32 dt = t - m_tLast;
return (dt < m_dtMax);
}
// Call this once per frame.
void Update()
{
if (ButtonsJustWentDown(m_buttonMask))
{
m_tLast = CurrentTime();
}
}
};
In the above code excerpt, we assume that each button is identiﬁed by a
unique id. The id is really just an index, ranging from 0 to N−1 (where N is the

8.5. Game Engine HID Systems
401
number of buttons on the HID in question). We convert the button id to a bit-
mask by shifting an unsigned 1 bit to the left by an amount equaling the but-
ton’s index (1U << buttonId). The function ButtonsJustWentDown()
returns a nonzero value if any one of the buttons speciﬁed by the given bitmask
just went down this frame. Here, we’re only checking for a single button-
down event, but we can and will use this same function later to check for
multiple simultaneous button-down events.
Multibutton Sequence
Let’s say we want to detect the sequence A-B-A, performed within at most
one second. We can detect this button sequence as follows: We maintain a
variable that tracks which button in the sequence we’re currently looking for.
If we deﬁne the sequence with an array of button ids (e.g., aButtons[3]
= {A, B, A}), then our variable is just an index i into this array. It starts
out initialized to the ﬁrst button in the sequence, i = 0. We also maintain a
start time for the entire sequence, Tstart, much as we did in the rapid button-
pressing example.
The logic goes like this: Whenever we see a button-down event that match-
es the button we’re currently looking for, we check its time stamp against the
start time of the entire sequence, Tstart. If it occurred within the valid time
window, we advance the current button to the next button in the sequence;
for the ﬁrst button in the sequence only (i = 0), we also update Tstart. If we see
a button-down event that doesn’t match the next button in the sequence, or
if the time delta has grown too large, we reset the button index i back to the
beginning of the sequence and set Tstart to some invalid value (such as 0). This
is illustrated by the code below.
class ButtonSequenceDetector
{
U32* m_aButtonIds;
// sequence of buttons to watch for
U32
m_buttonCount; // number of buttons in sequence
F32
m_dtMax;
// max time for entire sequence
U32
m_iButton;
// next button to watch for in seq.
F32
m_tStart;
// start time of sequence, in
// seconds
public:
// Construct an object that detects the given button
// sequence. When the sequence is successfully
// detected, the given event is broadcast so that the
// rest of the game can respond in an appropriate way.

402
8. Human Interface Devices (HID)
ButtonSequenceDetector(U32* aButtonIds,
U32 buttonCount,
F32 dtMax,
EventId eventIdToSend) :
m_aButtonIds(aButtonIds),
m_buttonCount(buttonCount),
m_dtMax(dtMax),
m_eventId(eventIdToSend), // event to send when
// complete
m_iButton(0),
// start of sequence
m_tStart(0)
// initial value
// irrelevant
{
}
// Call this once per frame.
void Update()
{
ASSERT(m_iButton < m_buttonCount);
// Determine which button we're expecting next, as
// a bitmask (shift a 1 up to the correct bit
// index).
U32 buttonMask = (1U << m_aButtonId[m_iButton]);
// If any button OTHER than the expected button
// just went down, invalidate the sequence. (Use
// the bitwise NOT operator to check for all other
// buttons.)
if (ButtonsJustWentDown(~buttonMask))
{
m_iButton = 0; // reset
}
// Otherwise, if the expected button just went
// down, check dt and update our state appropriately.
else if (ButtonsJustWentDown(buttonMask))
{
if (m_iButton == 0)
{
// This is the first button in the
// sequence.
m_tStart = CurrentTime();
m_iButton++; // advance to next button
}

8.5. Game Engine HID Systems
403
else
{
F32 dt = CurrentTime() - m_tStart;
if (dt < m_dtMax)
{
// Sequence is still valid.
m_iButton++; // advance to next button
// Is the sequence complete?
if (m_iButton == m_buttonCount)
{
BroadcastEvent(m_eventId);
m_iButton = 0; // reset
}
}
else
{
// Sorry, not fast enough.
m_iButton = 0; // reset
}
}
}
}
};
Thumb Stick Rotation
As an example of a more-complex gesture, let’s see how we might detect when
the player is rotating the left thumb stick in a clockwise circle. We can detect
this quite easily by dividing the two-dimensional range of possible stick po-
sitions into quadrants, as shown in Figure 8.14. In a clockwise rotation, the
stick passes through the upper-left quadrant, then the upper-right, then the
Figure 8.14. Detecting circular rotations of the stick by dividing the 2D range of stick inputs into
quadrants.

404
8. Human Interface Devices (HID)
lower-right and ﬁnally the lower-left. We can treat each of these cases like a
button press and detect a full rotation with a slightly modiﬁed version of the
sequence detection code shown above. We’ll leave this one as an exercise for
the reader. Try it!
8.5.5
Managing Multiple HIDs for Multiple Players
Most game machines allow two or more HIDs to be attached for multiplayer
games. The engine must keep track of which devices are currently attached
and route each one’s inputs to the appropriate player in the game. This im-
plies that we need some way of mapping controllers to players. This might be
as simple as a one-to-one mapping between controller index and player index,
or it might be something more sophisticated, such as assigning controllers to
players at the time the user hits the Start button.
Even in a single-player game with only one HID, the engine needs to be
robust to various exceptional conditions, such as the controller being acciden-
tally unplugged or running out of batteries. When a controller’s connection
is lost, most games pause gameplay, display a message and wait for the con-
troller to be reconnected. Some multiplayer games suspend or temporarily
remove the avatar corresponding to a removed controller, but allow the other
players to continue playing the game; the removed/suspended avatar might
reactivate when the controller is reconnected.
On systems with battery-operated HIDs, the game or the operating system
is responsible for detecting low-battery conditions. In response, the player
is usually warned in some way, for example via an unobtrusive on-screen
message and/or a sound effect.
8.5.6
Cross-Platform HID Systems
Many game engines are cross-platform. One way to handle HID inputs and
outputs in such an engine would be to sprinkle conditional compilation di-
rectives all over the code, wherever interactions with the HID take place, as
shown below. This is clearly not an ideal solution, but it does work.
#if TARGET_XBOX360
if (ButtonsJustWentDown(XB360_BUTTONMASK_A))
#elif TARGET_PS3
if (ButtonsJustWentDown(PS3_BUTTONMASK_TRIANGLE))
#elif TARGET_WII
if (ButtonsJustWentDown(WII_BUTTONMASK_A))
#endif
{
// do something...
}

8.5. Game Engine HID Systems
405
A better solution is to provide some kind of hardware abstraction layer,
thereby insulating the game code from hardware-speciﬁc details.
If we’re lucky, we can abstract most of the differences beween the HIDs
on the different platforms by a judicious choice of abstract button and axis
ids. For example, if our game is to ship on Xbox 360 and PS3, the layout
of the controls (buttons, axes and triggers) on these two joypads are almost
identical. The controls have different ids on each platform, but we can come
up with generic control ids that cover both types of joypad quite easily. For
example:
enum AbstractControlIndex
{
// Start and back buttons
AINDEX_START,
// Xbox 360 Start, PS3 Start
AINDEX_BACK_SELECT,
// Xbox 360 Back, PS3 Select
// Left D-pad
AINDEX_LPAD_DOWN,
AINDEX_LPAD_UP,
AINDEX_LPAD_LEFT,
AINDEX_LPAD_RIGHT,
// Right "pad" of four buttons
AINDEX_RPAD_DOWN,
// Xbox 360 A, PS3 X
AINDEX_RPAD_UP,
// Xbox 360 Y, PS3 Triangle
AINDEX_RPAD_LEFT,
// Xbox 360 X, PS3 Square
AINDEX_RPAD_RIGHT,
// Xbox 360 B, PS3 Circle
// Left and right thumb stick buttons
AINDEX_LSTICK_BUTTON,
// Xbox 360 LThumb, PS3 L3,
// Xbox white
AINDEX_RSTICK_BUTTON,
// Xbox 360 RThumb, PS3 R3,
// Xbox black
// Left and right shoulder buttons
AINDEX_LSHOULDER,
// Xbox 360 L shoulder, PS3 L1
AINDEX_RSHOULDER,
// Xbox 360 R shoulder, PS3 R1
// Left thumb stick axes
AINDEX_LSTICK_X,
AINDEX_LSTICK_Y,
// Right thumb stick axes
AINDEX_RSTICK_X,
AINDEX_RSTICK_Y,

406
8. Human Interface Devices (HID)
// Left and right trigger axes
AINDEX_LTRIGGER,
// Xbox 360 -Z, PS3 L2
AINDEX_RTRIGGER,
// Xbox 360 +Z, PS3 R2
};
Our abstraction layer can translate between the raw control ids on the cur-
rent target hardware into our abstract control indices. For example, when-
ever we read the state of the buttons into a 32-bit word, we can perform a
bit-swizzling operation that rearranges the bits into the proper order to corre-
spond to our abstract indices. Analog inputs can likewise be shufﬂed around
into the proper order.
In performing the mapping between physical and abstract controls, we’ll
sometimes need to get a bit clever. For example, on the Xbox, the left and right
triggers act as a single axis, producing negative values when the left trigger is
pressed, zero when neither is trigger is pressed, and positive values when the
right trigger is pressed. To match the behavior of the PlayStation’s DualShock
controller, we might want to separate this axis into two distinct axes on the
Xbox, scaling the values appropriately so the range of valid values is the same
on all platforms.
This is certainly not the only way to handle HID I/O in a multiplatform
engine. We might want to take a more functional approach, for example, by
naming our abstract controls according to their function in the game, rather
than their physical locations on the joypad. We might introduce higher-level
functions that detect abstract gestures, with custom detection code on each
platform, or we might just bite the bullet and write platform-speciﬁc versions
of all of the game code that requires HID I/O. The possibilities are numerous,
but virtually all cross-platform game engines insulate the game from hard-
ware details in some manner.
8.5.7
Input Remapping
Many games allow the player some degree of choice with regard to the func-
tionality of the various controls on the physical HID. A common option is the
sense of the vertical axis of the right thumb stick for camera control in a con-
sole game. Some folks like to push forward on the stick to angle the camera
up, while others like an inverted control scheme, where pulling back on the
stick angles the camera up (much like an airplane control stick). Other games
allow the player to select between two or more predeﬁned button mappings.
Some PC games allow the user full control over the functions of individual
keys on the keyboard, the mouse buttons and the mouse wheel, plus a choice
between various control schemes for the two mouse axes.

8.5. Game Engine HID Systems
407
To implement this, we turn to a favorite saying of an old professor of mine,
Professor Jay Black of the University of Waterloo, “Every problem in computer
science can be solved with a level of indirection.” We assign each function in
the game a unique id and then provide a simple table, which maps each phys-
ical or abstract control index to a logical function in the game. Whenever the
game wishes to determine whether a particular logical game function should
be activated, it looks up the corresponding abstract or physical control id in
the table and then reads the state of that control. To change the mapping, we
can either swap out the entire table wholesale, or we can allow the user to edit
individual entries in the table.
We’re glossing over a few details here. For one thing, different controls
produce different kinds of inputs. Analog axes may produce values ranging
from −32,768 to 32,767, or from 0 to 255, or some other range. The states of all
the digital buttons on a HID are usually packed into a single machine word.
Therefore, we must be careful to only permit control mappings that make
sense. We cannot use a button as the control for a logical game function that
requires an axis, for example. One way around this problem is to normalize
all of the inputs. For example, we could re-scale the inputs from all analog
axes and buttons into the range [0, 1]. This isn’t quite as helpful as you might
at ﬁrst think, because some axes are inherently bidirectional (like a joy stick)
while others are unidirectional (like a trigger). But, if we group our controls
into a few classes, we can normalize the inputs within those classes and permit
remapping only within compatible classes. A reasonable set of classes for a
standard console joypad and their normalized input values might be:
•
Digital buttons. States are packed into a 32-bit word, one bit per button.
•
Unidirectional absolute axes (e.g., triggers, analog buttons). Produce ﬂoat-
ing-point input values in the range [0, 1].
•
Bidirectional absolute axes (e.g., joy sticks). Produce ﬂoating-point input
values in the range [−1, 1].
•
Relative axes (e.g., mouse axes, wheels, track balls). Produce ﬂoating-point
input values in the range [−1, 1], where ±1 represents the maximum
relative offset possible within a single game frame (i.e., during a period
of 1/30 or 1/60 of a second).
8.5.8
Context-Sensitive Controls
In many games, a single physical control can have different functions, de-
pending on context. A simple example is the ubiquitous “use” button. If
pressed while standing in front of a door, the “use” button might cause the

408
8. Human Interface Devices (HID)
character to open the door. If it is pressed while standing near an object, it
might cause the player character to pick up the object and so on. Another
common example is a modal control scheme. When the player is walking
around, the controls are used to navigate and control the camera. When the
player is riding a vehicle, the controls are used to steer the vehicle, and the
camera controls might be different as well.
Context-sensitive controls are reasonably straightforward to implement
via a state machine.
Depending on what state we’re in, a particular HID
control may have a different purpose. The tricky part is deciding what state
to be in. For example, when the context-sensitive “use” button is pressed,
the player might be standing at a point equidistant between a weapon and a
health pack, facing the center point between them. Which object do we use in
this case? Some games implement a priority system to break ties like this. Per-
haps the weapon has a higher weight than the health pack, so it would “win”
in this example. Implementing context-sensitive controls isn’t rocket science,
but it invariably requires lots of trial and error to get it feeling and behaving
just right. Plan on lots of iteration and focus testing!
Another related concept is that of control ownership. Certain controls on
the HID might be “owned” by different parts of the game. For example, some
inputs are for player control, some for camera control and still others are for
use by the game’s wrapper and menu system (pausing the game, and so on).
Some game engines introduce the concept of a logical device, which is com-
posed of only a subset of the inputs on the physical device. One logical device
might be used for player control, while another is used by the camera system,
and another by the menu system.
8.5.9
Disabling Inputs
In most games, it is sometimes necessary to disallow the player from control-
ling his or her character. For example, when the player character is involved
in an in-game cinematic, we might want to disable all player controls tem-
porarily; or when the player is walking through a narrow doorway, we might
want to temporarily disable free camera rotation.
One rather heavy-handed approach is to use a bitmask to disable indi-
vidual controls on the input device itself. Whenever the control is read, the
disable mask is checked, and if the corresponding bit is set, a neutral or zero
value is returned instead of the actual value read from the device. We must be
particularly cautious when disabling controls, however. If we forget to reset
the disable mask, the game can get itself into a state where the player looses
all control forever and must restart the game. It’s important to check our logic

8.6. Human Interface Devices in Practice
409
carefully, and it’s also a good idea to put in some fail-safe mechanisms to en-
sure that the disable mask is cleared at certain key times, such as whenever
the player dies and re-spawns.
Disabling a HID input masks it for all possible clients, which can be overly
limiting. A better approach is probably to put the logic for disabling speciﬁc
player actions or camera behaviors directly into the player or camera code
itself. That way, if the camera decides to ignore the deﬂection of the right
thumb stick, for example, other game engine systems still have the freedom
to read the state of that stick for other purposes.
8.6
Human Interface Devices in Practice
Correct and smooth handling of human interface devices is an important part
of any good game. Conceptually speaking, HIDs may seem quite straightfor-
ward. However, there can be quite a few “gotchas” to deal with, including
variations between different physical input devices, proper implementation
of low-pass ﬁltering, bug-free handling of control scheme mappings, achiev-
ing just the right “feel” in your joypad rumble, limitations imposed by console
manufacturers via their technical requirements checklists (TRCs), and the list
goes on. A game team should expect to devote a nontrivial amount of time
and engineering bandwidth to a careful and complete implementation of the
human interface device system. This is extremely important because the HID
system forms the underpinnings of your game’s most precious resource—its
player mechanics.

This page intentionally left blank
This page intentionally left blank

9
Tools for Debugging
and Development
D
eveloping game software is a complex, intricate, math-intensive and
error-prone business. So it should be no surprise that virtually every
professional game team builds a suite of tools for themselves, in order to make
the game development process easier and less error-prone. In this chapter,
we’ll take a look at the development and debugging tools most often found in
professional-grade game engines.
9.1
Logging and Tracing
Remember when you wrote your ﬁrst program in BASIC or Pascal? (OK, may-
be you don’t. If you’re signiﬁcantly younger than me—and there’s a pretty
good chance of that—you probably wrote your ﬁrst program in Java, or maybe
Python or Lua.) In any case, you probably remember how you debugged your
programs back then. You know, back when you thought a debugger was one of
those glowing blue insect zapper things? You probably used print statements
to dump out the internal state of your program. C/C++ programmers call this
printf debugging (after the standard C library function, printf()).
It turns out that printf debugging is still a perfectly valid thing to do—
even if you know that a debugger isn’t a device for frying hapless insects
411

412
9. Tools for Debugging and Development
at night. Especially in real-time programming, it can be difﬁcult to trace cer-
tain kinds of bugs using breakpoints and watch windows. Some bugs are
timing-dependent: they only happen when the program is running at full
speed. Other bugs are caused by a complex sequence of events too long and
intricate to trace manually one-by-one. In these situations, the most powerful
debugging tool is often a sequence of print statements.
Every game platform has some kind of console or teletype (TTY) output
device. Here are some examples:
•
In a console application written in C/C++, running under Linux or
Win32, you can produce output in the console by printing to stdout
or stderr via printf(), fprintf() or STL’s iostream interface.
•
Unfortunately, printf() and iostream don’t work if your game is
built as a windowed application under Win32, because there’s no con-
sole in which to display the output. However, if you’re running under
the Visual Studio debugger, it provides a debug console to which you
can print via the Win32 function OutputDebugString().
•
On the PlayStation 3 and PlayStation 4, an application known as the Tar-
get Manager (or PlayStation Neighborhood on the PS4) runs on your PC
and allows you to launch programs on the console. The Target Manager
includes a set of TTY output windows to which messages can be printed
by the game engine.
So printing out information for debugging purposes is almost always as
easy as adding calls to printf() throughout your code. However, most
game engines go a bit farther than this. In the following sections, we’ll in-
vestigate the kinds of printing facilities most game engines provide.
9.1.1
Formatted Output with OutputDebugString()
The Windows SDK function OutputDebugString() is great for printing de-
bugging information to Visual Studio’s Debug Output window. However, un-
like printf(), OutputDebugString() does not support formatted output—
it can only print raw strings in the form of arrays. For this reason, most Win-
dows game engines wrap it in a custom function, like this:
#include <stdio.h>
// for va_list et al
#ifndef WIN32_LEAN_AND_MEAN
#define WIN32_LEAN_AND_MEAN 1
#endif
#include <windows.h>
// for OutputDebugString()

9.1. Logging and Tracing
413
int VDebugPrintF(const char* format, va_list argList)
{
const U32 MAX_CHARS = 1024;
static char s_buffer[MAX_CHARS];
int charsWritten
= vsnprintf(s_buffer, MAX_CHARS, format, argList);
// Now that we have a formatted string, call the
// Win32 API.
OutputDebugString(s_buffer);
return charsWritten;
}
int DebugPrintF(const char* format, ...)
{
va_list argList;
va_start(argList, format);
int charsWritten = VDebugPrintF(format, argList);
va_end(argList);
return charsWritten;
}
Notice that two functions are implemented: DebugPrintF() takes a
variable-length argument list (speciﬁed via the ellipsis, ...), while VDebug-
PrintF() takes a va_list argument. This is done so that programmers can
build additional printing functions in terms of VDebugPrintF(). (It’s im-
possible to pass ellipses from one function to another, but it is possible to pass
va_lists around.)
9.1.2
Verbosity
Once you’ve gone to the trouble of adding a bunch of print statements to your
code in strategically chosen locations, it’s nice to be able to leave them there,
in case they’re needed again later. To permit this, most engines provide some
kind of mechanism for controlling the level of verbosity via the command line,
or dynamically at runtime. When the verbosity level is at its minimum value
(usually zero), only critical error messages are printed. When the verbosity is
higher, more of the print statements embedded in the code start to contribute
to the output.
The simplest way to implement this is to store the current verbosity level
in a global integer variable, perhaps called g_verbosity. We then provide

414
9. Tools for Debugging and Development
a VerboseDebugPrintF() function whose ﬁrst argument is the verbosity
level at or above which the message will be printed. This function could be
implemented as follows:
int g_verbosity = 0;
void VerboseDebugPrintF(int verbosity,
const char* format, ...)
{
// Only print when the global verbosity level is
// high enough.
if (g_verbosity >= verbosity)
{
va_list argList;
va_start(argList, format);
VDebugPrintF(format, argList);
va_end(argList);
}
}
9.1.3
Channels
It’s also extremely useful to be able to categorize your debug output into
channels. One channel might contain messages from the animation system,
while another might be used to print messages from the physics system, for
example.
On some platforms, like the PlayStation 3, debug output can be directed to
one of 14 distinct TTY windows. In addition, messages are mirrored to a spe-
cial TTY window that contains the output from all of the other 14 windows.
This makes it very easy for a developer to focus in on only the messages he
or she wants to see. When working on an animation problem, one can simply
ﬂip to the animation TTY and ignore all the other output. When working on a
general problem of unknown origin, the “all” TTY can be consulted for clues.
Other platforms like Windows provide only a single debug output console.
However, even on these systems it can be helpful to divide your output into
channels. The output from each channel might be assigned a different color.
You might also implement ﬁlters, which can be turned on and off at runtime,
and restrict output to only a speciﬁed channel or set of channels. In this model,
if a developer is debugging an animation-related problem, for example, he or
she can simply ﬁlter out all of the channels except the animation channel.
A channel-based debug output system can be implemented quite easily by
adding an additional channel argument to our debug printing function. Chan-

9.1. Logging and Tracing
415
nels might be numbered, or better, assigned symbolic values via a C/C++
enum declaration. Or channels might be named using a string or hashed string
id. The printing function can simply consult the list of active channels and
only print the message if the speciﬁed channel is among them.
If you don’t have more than 32 or 64 channels, it can be helpful to identify
the channels via a 32- or 64-bit mask. This makes implementing a channel
ﬁlter as easy as specifying a single integer. When a bit in the mask is 1, the
corresponding channel is active; when the bit is 0, the channel is muted.
9.1.3.1
Using Redis to Manage TTY Channels
The developers at Naughty Dog use a web-based interface called Connector
as their window into the various streams of debugging information that are
emitted by the game engine at runtime. The game spits out its debug text
into various named channels, each associated with a different engine system
(animation, rendering, AI, sound, etc.) These data streams are collected by a
lightweight Redis key-value store (see http://redis.io for more information on
Redis). The Connector interface allows users to view and ﬁlter this Redis data
easily from any web browser.
9.1.4
Mirroring Output to a File
It’s a good idea to mirror all debug output to one or more log ﬁles (e.g., one ﬁle
per channel). This permits problems to be diagnosed after the fact. Ideally the
log ﬁle(s) should contain all of the debug output, independent of the current
verbosity level and active channels mask. This allows unexpected problems to
be caught and tracked down by simply inspecting the most-recent log ﬁles.
You may want to consider ﬂushing your log ﬁle(s) after every call to your
debug output function to ensure that if the game crashes, the log ﬁle(s) won’t
be missing the last buffer-full of output. The last data printed are usually the
most useful for determining the cause of a crash, so we want to be sure that
the log ﬁle always contains the most up-to-date output. Of course, ﬂushing
the output buffer can be expensive. So you should only ﬂush buffers after
every debug output call if either (a) you are not doing a lot of logging, or (b)
you discover that it is truly necessary on your particular platform. If ﬂushing
is deemed to be necessary, you can always provide an engine conﬁguration
option to turn it on and off.
9.1.5
Crash Reports
Some game engines produce special text output and/or log ﬁles when the
game crashes. In most operating systems, a top-level exception handler can be

416
9. Tools for Debugging and Development
installed that will catch most crashes. In this function, you could print out all
sorts of useful information. You could even consider emailing the crash report
to the entire programming team. This can be incredibly enlightening for the
programmers: When they see just how often the art and design teams are
crashing, they may discover a renewed sense of urgency in their debugging
tasks!
Here are just a few examples of the kinds of information you can include
in a crash report:
•
Current level(s) being played at the time of the crash.
•
World-space location of the player character when the crash occurred.
•
Animation/action state of the player when the game crashed.
•
Gameplay script(s) that were running at the time of the crash. (This can
be especially helpful if the script is the cause of the crash!)
•
Stack trace. Most operating systems provide a mechanism for walking
the call stack (although they are nonstandard and highly platform spe-
ciﬁc). With such a facility, you can print out the symbolic names of all
non-inline functions on the stack at the time the crash occurred.
•
State of all memory allocators in the engine (amount of memory free,
degree of fragmentation, etc.). This kind of data can be helpful when
bugs are caused by low-memory conditions, for example.
•
Any other information you think might be relevant when tracking down
the cause of a crash.
•
A screenshot of the game at the moment it crashed.
9.2
Debug Drawing Facilities
Modern interactive games are driven almost entirely by math. We use math
to position and orient objects in the game world, move them around, test for
collisions, and cast rays to determine lines of sight, and of course we use
matrix multiplication to transform objects from object space to world space
and eventually into screen space for rendering. Almost all modern games are
three-dimensional, but even in a two-dimensional game it can be very difﬁ-
cult to mentally visualize the results of all these mathematical calculations.
For this reason, most good game engines provide an API for drawing colored
lines, simple shapes and 3D text. We call this a debug drawing facility, because
the lines, shapes and text that are drawn with it are intended for visualization
during development and debugging and are removed prior to shipping the
game.

9.2. Debug Drawing Facilities
417
A debug drawing API can save you huge amounts of time. For example,
if you are trying to ﬁgure out why your projectiles are not hitting the enemy
characters, which is easier? Deciphering a bunch of numbers in the debugger?
Or drawing a line showing the trajectory of the projectile in three dimensions
within your game? With a debug drawing API, logical and mathematical er-
rors become immediately obvious. One might say that a picture is worth a
thousand minutes of debugging.
Here are some examples of debug drawing in action within Naughty Dog’s
engine. The following screenshots were all taken within our play-test level,
one of many special levels we use for testing out new features and debugging
problems in the game.
•
Figure 9.1 shows a visualization of an enemy NPC’s perception of the
player. The little “stick man” ﬁgure represents the location of the player
as perceived by the NPC. When the player has broken the line of sight
between himself and the NPC, the “stick man” will remain at the player’s
last known location, even after the player sneaks away.
•
Figure 9.2 shows how a wireframe sphere can be used to visualize the
dynamically expanding blast radius of an explosion.
•
Figure 9.3 shows how circles can be used to visualize the radii used by
Drake when searching for ledges to hang from in the game. A line shows
the ledge he is currently hanging from.
Figure 9.1. Visualizing the line of sight from an NPC to the player in The Last of Us (© 2013/™
SCEA. Created and developed by Naughty Dog, PlayStation 3).

418
9. Tools for Debugging and Development
Figure 9.2. Visualizing the expanding blast sphere of an explosion in the Uncharted/The Last of
Us engine (©˜2014/™SCEA. Created and developed by Naughty Dog, PlayStation 3).
•
Figure 9.4 shows an AI character that has been placed in a special debug-
ging mode. In this mode, the character’s brain is effectively turned off,
and the developer is given full control over the character’s movements
and actions via a simple heads-up menu. The developer can paint tar-
get points in the game world by simply aiming the camera and can then
instruct the character to walk, run or sprint to the speciﬁed points. The
Figure 9.3. Spheres and vectors used in Drake’s ledge hang and shimmy system in the Uncharted
series (© 2014/™SCEA. Created and developed by Naughty Dog, PlayStation 3).

9.2. Debug Drawing Facilities
419
Figure 9.4.
Manually controlling an NPC’s actions for debugging purposes in The Last of Us
(© 2013/™SCEA. Created and developed by Naughty Dog, PlayStation 3).
user can also tell the character to enter or leave nearby cover, ﬁre its
weapon and so on.
9.2.1
Debug Drawing API
A debug drawing API generally needs to satisfy the following requirements:
•
The API should be simple and easy to use.
•
It should support a useful set of primitives, including (but not limited
to):
◦
lines,
◦
spheres,
◦
points (usually represented as small crosses or spheres, because a
single pixel is very difﬁcult to see),
◦
coordinate axes (typically, the x-axis is drawn in red, y in green and
z in blue),
◦
bounding boxes, and
◦
formatted text.
•
It should provide a good deal of ﬂexibility in controlling how primitives
are drawn, including:
◦
color,
◦
line width,

420
9. Tools for Debugging and Development
◦
sphere radii,
◦
the size of points, lengths of coordinate axes, and dimensions of
other “canned” primitives.
•
It should be possible to draw primitives in world space (full 3D, us-
ing the game camera’s perspective projection matrix) or in screen space
(either using an orthographic projection, or possibly a perspective pro-
jection). World-space primitives are useful for annotating objects in the
3D scene. Screen-space primitives are helpful for displaying debugging
information in the form of a heads-up display that is independent of
camera position or orientation.
•
It should be possible to draw primitives with or without depth testing
enabled.
◦
When depth testing is enabled, the primitives will be occluded by
real objects in your scene. This makes their depth easy to visualize,
but it also means that the primitives may sometimes be difﬁcult to
see or totally hidden by the geometry of your scene.
◦
With depth testing disabled, the primitives will “hover” over the
real objects in the scene. This makes it harder to gauge their real
depth, but it also ensures that no primitive is ever hidden from
view.
•
It should be possible to make calls to the drawing API from anywhere in
your code. Most rendering engines require that geometry be submitted
for rendering during a speciﬁc phase of the game loop, usually at the
end of each frame. This requirement implies that the system must queue
up all incoming debug drawing requests, so that they may be submitted
at the proper time later on.
•
Ideally, every debug primitive should have a lifetime associated with it.
The lifetime controls how long the primitive will remain on-screen after
having been requested. If the code that is drawing the primitive is called
every frame, the lifetime can be one frame—the primitive will remain
on-screen because it will be refreshed every frame. However, if the code
that draws the primitive is called rarely or intermittently (e.g., a function
that calculates the initial velocity of a projectile), then you do not want
the primitive to ﬂicker on-screen for just one frame and then disappear.
In such situations, the programmer should be able to give his or her
debug primitives a longer lifetime, on the order of a few seconds.
•
It’s also important that the debug drawing system be capable of han-
dling a large number of debug primitives efﬁciently. When you’re draw-
ing debug information for 1,000 game objects, the number of primitives

9.2. Debug Drawing Facilities
421
can really add up, and you don’t want your game to be unusable when
debug drawing is turned on.
The debug drawing API in Naughty Dog’s engine looks something like
this:
class DebugDrawManager
{
public:
// Adds a line segment to the debug drawing queue.
void AddLine(const Point& fromPosition,
const Point& toPosition,
Color color,
float lineWidth = 1.0f,
float duration = 0.0f,
bool depthEnabled = true);
// Adds an axis-aligned cross (3 lines converging at
// a point) to the debug drawing queue.
void AddCross(const Point& position,
Color color,
float size,
float duration = 0.0f,
bool depthEnabled = true);
// Adds a wireframe sphere to the debug drawing queue.
void AddSphere(const Point& centerPosition,
float radius,
Color color,
float duration = 0.0f,
bool depthEnabled = true);
// Adds a circle to the debug drawing queue.
void AddCircle(const Point& centerPosition,
const Vector& planeNormal,
float radius,
Color color,
float duration = 0.0f,
bool depthEnabled = true);
// Adds a set of coordinate axes depicting the
// position and orientation of the given
// transformation to the debug drawing queue.
void AddAxes(const Transform& xfm,
Color color,
float size,
float duration = 0.0f,
bool depthEnabled = true);

422
9. Tools for Debugging and Development
// Adds a wireframe triangle to the debug drawing
// queue.
void AddTriangle(const Point& vertex0,
const Point& vertex1,
const Point& vertex2,
Color color,
float lineWidth = 1.0f,
float duration = 0.0f,
bool depthEnabled = true);
// Adds an axis-aligned bounding box to the debug
// queue.
void AddAABB(const Point& minCoords,
const Point& maxCoords,
Color color,
float lineWidth = 1.0f,
float duration = 0.0f,
bool depthEnabled = true);
// Adds an oriented bounding box to the debug queue.
void AddOBB(const Mat44& centerTransform,
const Vector& scaleXYZ,
Color color,
float lineWidth = 1.0f,
float duration = 0.0f,
bool depthEnabled = true);
// Adds a text string to the debug drawing queue.
void AddString(const Point& pos,
const char* text,
Color color,
float duration = 0.0f,
bool depthEnabled = true);
};
// This global debug drawing manager is configured for
// drawing in full 3D with a perspective projection.
extern DebugDrawManager g_debugDrawMgr;
// This global debug drawing manager draws its
// primitives in 2D screen space. The (x,y) coordinates
// of a point specify a 2D location on-screen, and the
// z coordinate contains a special code that indicates
// whether the (x,y) coordidates are measured in absolute
// pixels or in normalized coordinates that range from
// 0.0 to 1.0. (The latter mode allows drawing to be
// independent of the actual resolution of
the screen.)
extern DebugDrawManager g_debugDrawMgr2D;

9.3. In-Game Menus
423
Here’s an example of this API being used within game code:
void Vehicle::Update()
{
// Do some calculations...
// Debug-draw my velocity vector.
const Point& start = GetWorldSpacePosition();
Point end = start + GetVelocity();
g_debugDrawMgr.AddLine(start, end, kColorRed);
// Do some other calculations...
// Debug-draw my name and number of passengers.
{
char buffer[128];
sprintf(buffer, "Vehicle %s: %d passengers",
GetName(), GetNumPassengers());
const Point& pos = GetWorldSpacePosition();
g_debugDrawMgr.AddString(pos,
buffer, kColorWhite, 0.0f, false);
}
}
You’ll notice that the names of the drawing functions use the verb “add”
rather than “draw.” This is because the debug primitives are typically not
drawn immediately when the drawing function is called. Instead, they are
added to a list of visual elements that will be drawn at a later time. Most high-
speed 3D rendering engines require that all visual elements be maintained in
a scene data structure so that they can be drawn efﬁciently, usually at the end
of the game loop. We’ll learn a lot more about how rendering engines work in
Chapter 10.
9.3
In-Game Menus
Every game engine has a large number of conﬁguration options and features.
In fact, each major subsystem, including rendering, animation, collision,
physics, audio, networking, player mechanics, AI and so on, exposes its own
specialized conﬁguration options. It is highly useful to programmers, artists
and game designers alike to be able to conﬁgure these options while the game
is running, without having to change the source code, recompile and relink
the game executable, and then rerun the game. This can greatly reduce the
amount of time the game development team spends on debugging problems
and setting up new levels or game mechanics.

424
9. Tools for Debugging and Development
One simple and convenient way to permit this kind of thing is to provide
a system of in-game menus. Items on an in-game menu can do any number of
things, including (but certainly not limited to):
•
toggling global Boolean settings,
•
adjusting global integer and ﬂoating-point values,
•
calling arbitrary functions, which can perform literally any task within
the engine, and
•
bringing up submenus, allowing the menu system to be organized hier-
archically for easy navigation.
An in-game menu should be easy and convenient to bring up, perhaps
via a simple button press on the joypad. (Of course, you’ll want to choose
a button combination that doesn’t occur during normal gameplay.) Bringing
up the menus usually pauses the game. This allows the developer to play the
game until the moment just before a problem occurs, then pause the game
by bringing up the menus, adjust engine settings in order to visualize the
problem more clearly, and then un-pause the game to inspect the problem in
depth.
Let’s take a brief look at how the menu system works in the Uncharted /
The Last of Us engine, by Naughty Dog. Figure 9.5 shows the top-level menu.
It contains submenus for each major subsystem in the engine. In Figure 9.6,
we’ve drilled down one level into the Rendering... submenu. Since the ren-
Figure 9.5. Main development menu in The Last of Us (© 2013/™SCEA. Created and developed by
Naughty Dog, PlayStation 3).

9.3. In-Game Menus
425
Figure 9.6.
Rendering submenu in The Last of Us (© 2013/™SCEA. Created and developed by
Naughty Dog, PlayStation 3).
dering engine is a highly complex system, its menu contains many submenus
controlling various aspects of rendering. To control the way in which 3D
meshes are rendered, we drill down further into the Mesh Options...
sub-
menu, shown in Figure 9.7. On this menu, we can turn off rendering of all
Figure 9.7. Mesh options subsubmenu in The Last of Us (© 2013/™SCEA. Created and developed
by Naughty Dog, PlayStation 3).

426
9. Tools for Debugging and Development
Figure 9.8. Background meshes turned off (The Last of Us © 2013/™SCEA. Created and developed
by Naughty Dog, PlayStation 3).
static background meshes, leaving only the dynamic foreground meshes visi-
ble. This is shown in Figure 9.8. (Ah ha, there’s that pesky deer!)
9.4
In-Game Console
Some engines provide an in-game console, either in lieu of or in addition to an
in-game menu system. An in-game console provides a command-line inter-
face to the game engine’s features, much as a DOS command prompt provides
users with access to various features of the Windows operating system, or a
csh, tcsh, ksh or bash shell prompt provides users with access to the features
of UNIX-like operating systems. Much like a menu system, the game engine
console can provide commands allowing a developer to view and manipulate
global engine settings, as well as running arbitrary commands.
A console is somewhat less convenient than a menu system, especially for
those who aren’t very fast typists. However, a console can be much more
powerful than a menu. Some in-game consoles provide only a rudimentary
set of hard-coded commands, making them about as ﬂexible as a menu sys-
tem. But others provide a rich interface to virtually every feature of the engine.
A screenshot of the in-game console in Minecraft is shown in Figure 9.9.
Some game engines provide a powerful scripting language that can be
used by programmers and game designers to extend the functionality of the
engine, or even build entirely new games. If the in-game console “speaks”

9.5. Debug Cameras and Pausing the Game
427
Figure 9.9.
The in-game console in Minecraft, overlaid on top of the main game screen and
displaying a list of valid commands.
this same scripting language, then anything you can do in script can also be
done interactively via the console. We’ll explore scripting languages in depth
in Section 15.8.
9.5
Debug Cameras and Pausing the Game
An in-game menu or console system is best accompanied by two other crucial
features: (a) the ability to detach the camera from the player character and ﬂy
it around the game world in order to scrutinize any aspect of the scene, and
(b) the ability to pause, un-pause and single-step the game (see Section 7.5.6).
When the game is paused, it is still important to be able to control the camera.
To support this, we can simply keep the rendering engine and camera controls
running, even when the game’s logical clock is paused.
Slow motion mode is another incredibly useful feature for scrutinizing an-
imations, particle effects, physics and collision behaviors, AI behaviors, and
the list goes on. This feature is easy to implement. Presuming we’ve taken care
to update all gameplay elements using a clock that is distinct from the real-
time clock, we can put the game into slo-mo by simply updating the gameplay
clock at a rate that is slower than usual. This approach can also be used to im-
plement a fast-motion mode, which can be useful for moving rapidly through
time-consuming portions of gameplay in order to get to an area of interest
(not to mention being a great source of laughs, especially when accompanied
by a bad vocal rendition of Benny Hill music...).
9.6
Cheats
When developing or debugging a game, it’s important to allow the user to
break the rules of the game in the name of expediency. Such features are aptly

428
9. Tools for Debugging and Development
named cheats. For example, many engines allow you to “pick up” the player
character and ﬂy him or her around in the game world, with collisions dis-
abled so he or she can pass through all obstacles. This can be incredibly help-
ful for testing out gameplay. Rather than taking the time to actually play the
game in an attempt to get the player character into some desirable location,
you can simply pick him up, ﬂy him over to where you want him to be, and
then drop him back into his regular gameplay mode.
Other useful cheats include, but are certainly not limited to:
•
Invincible player. As a developer, you often don’t want to be bothered
having to defend yourself from enemy characters, or worrying about
falling from too high a height, as you test out a feature or track down a
bug.
•
Give player weapon. It’s often useful to be able to give the player any
weapon in the game for testing purposes.
•
Inﬁnite ammo. When you’re trying to kill bad guys to test out the weapon
system or AI hit reactions, you don’t want to be scrounging for clips!
•
Select player mesh. If the player character has more than one “costume,”
it can be useful to be able to select any of them for testing purposes.
Obviously this list could go on for pages. The sky’s the limit—you can
add whatever cheats you need in order to develop or debug the game. You
might even want to expose some of your favorite cheats to the players of the
ﬁnal shipping game. Players can usually activate cheats by entering unpub-
lished cheat codes on the joypad or keyboard and/or by accomplishing certain
objectives in the game.
9.7
Screenshots and Movie Capture
Another extremely useful facility is the ability to capture screenshots and
write them to disk in a suitable image format such as Windows Bitmap ﬁles
(.bmp), JPEG (.jpg) or Targa (.tga). The details of how to capture a screenshot
vary from platform to platform, but they typically involve making a call to
the graphics API that allows the contents of the frame buffer to be transferred
from video RAM to main RAM, where it can be scanned and converted into
the image ﬁle format of your choice. The image ﬁles are typically written to a
predeﬁned folder on disk and named using a date and time stamp to guaran-
tee unique ﬁle names.
You may want to provide your users with various options controlling how
screenshots are to be captured. Some common examples include:

9.8. In-Game Proﬁling
429
•
Whether or not to include debug primitives and text in the screenshot.
•
Whether or not to include heads-up display (HUD) elements in the screen-
shot.
•
The resolution at which to capture. Some engines allow high-resolution
screenshots to be captured, perhaps by modifying the projection matrix
so that separate screenshots can be taken of the four quadrants of the
screen at normal resolution and then combined into the ﬁnal high-res
image.
•
Simple camera animations. For example, you could allow the user to
mark the starting and ending positions and orientations of the camera.
A sequence of screenshots could then be taken while gradually interpo-
lating the camera from the starting location to the ending location.
Some engines also provide a full-ﬂedged movie capture mode. Such a sys-
tem captures a sequence of screenshots at the target frame rate of the game,
which are processed either ofﬂine or at runtime to generate a movie ﬁle in
a suitable format such as MPEG-2 (H.262) or MPEG-4 Part 10 (H.264). The
PlayStation 4 has built-in support for movie capture and playback via the
Share button on the controller. On the PC and other consoles, you will have to
implement your own movie capture system. But even if your engine doesn’t
support real-time video capture, external hardware like Roxio Game Capture
HD Pro can always be used to capture the output from your game console or
PC. And for PC and Mac games, a great many software video capture tools are
available, including Fraps by Beepa, Camtasia by Camtasia Software, Dxtory by
ExKode, Debut by NCH Software and Action! by Mirillis.
9.8
In-Game Proﬁling
Games are real-time systems, so achieving and maintaining a high frame rate
(usually 30 FPS or 60 FPS) is important. Therefore, part of any game program-
mer’s job is ensuring that his or her code runs efﬁciently and within budget.
As we saw when we discussed the 80-20 rule in Chapter 2, a large percentage
of your code probably doesn’t need to be optimized. The only way to know
which bits require optimization is to measure your game’s performance. We dis-
cussed various third-party proﬁling tools in Chapter 2. However, these tools
have various limitations and may not be available at all on a console. For
this reason, and/or for convenience, many game engines provide an in-game
proﬁling tool of some sort.
Typically an in-game proﬁler permits the programmer to annotate blocks
of code that should be timed and give them human-readable names. The pro-

430
9. Tools for Debugging and Development
Figure 9.10. The Naughty Dog engine provides a proﬁle hierarchy display that allows the user to
drill down into particular function calls to inspect their costs.
ﬁler measures the execution time of each annotated block via the CPU’s hi-res
timer and stores the results in memory. A heads-up display is provided, which
shows up-to-date execution times for each code block (examples are shown in
Figures 9.10 and 9.11). The display often provides the data in various forms,
including raw numbers of cycles, execution times in microseconds, and per-
centages relative to the execution time of the entire frame.
Figure 9.11.
The timeline mode in Uncharted and The Last of Us (© 2014/™SCEA. Created and
developed by Naughty Dog, PlayStation 3) shows exactly when various operations are performed
across a single frame on the PS3’s SPUs, GPU and PPU.

9.8. In-Game Proﬁling
431
9.8.1
Hierarchical Proﬁling
Computer programs written in an imperative language are inherently hierar-
chical—a function calls other functions, which in turn call still more functions.
For example, let’s imagine that function a() calls functions b() and c(), and
function b() in turn calls functions d(), e() and f(). The pseudocode for
this is shown below.
void a()
{
b();
c();
}
void b()
{
d();
e();
f();
}
void c() { ... }
void d() { ... }
void e() { ... }
void f() { ... }
Figure 9.12. A hypo-
thetical function call
hierarchy.
Assuming function a() is called directly from main(), this function call
hierarchy is shown in Figure 9.12.
When debugging a program, the call stack shows only a snapshot of this
tree. Speciﬁcally, it shows us the path from whichever function in the hierar-
chy is currently executing all the way to the root function in the tree. In C/C++,
the root function is usually main() or WinMain(), although technically this
function is called by a start-up function that is part of the standard C runtime
library (CRT), so that function is the true root of the hierarchy. If we set a
breakpoint in function e(), for example, the call stack would look something
like this:
e()
←The currently executing function.
b()
a()
main()
_crt_startup()
←Root of the call hierarchy.

432
9. Tools for Debugging and Development
Figure 9.13. Call stack resulting from setting a breakpoint in function e().
This call stack is depicted in Figure 9.13 as a pathway from function e() to
the root of the function call tree.
9.8.1.1
Measuring Execution Times Hierarchically
If we measure the execution time of a single function, the time we measure
includes the execution time of any the child functions called and all of their
grandchildren, great-grandchildren and so on as well. To properly interpret
any proﬁling data we might collect, we must be sure to take the function call
hierarchy into account.
Many commercial proﬁlers can automatically instrument every single func-
tion in your program. This permits them to measure both the inclusive and
exclusive execution times of every function that is called during a proﬁling
session. As the name implies, inclusive times measure the execution time of
the function including all of its children, while exclusive times measure only
the time spent in the function itself. (The exclusive time of a function can
be calculated by subtracting the inclusive times of all its immediate children
from the inclusive time of the function in question.) In addition, some proﬁl-
ers record how many times each function is called. This is an important piece
of information to have when optimizing a program, because it allows you to
differentiate between functions that eat up a lot of time internally and func-
tions that eat up time because they are called a very large number of times.
In contrast, in-game proﬁling tools are not so sophisticated and usually
rely on manual instrumentation of the code. If our game engine’s main loop
is structured simply enough, we may be able to obtain valid data at a coarse
level without thinking much about the function call hierarchy. For example, a
typical game loop might look roughly like this:

9.8. In-Game Proﬁling
433
while (!quitGame)
{
PollJoypad();
UpdateGameObjects();
UpdateAllAnimations();
PostProcessJoints();
DetectCollisions();
RunPhysics();
GenerateFinalAnimationPoses();
UpdateCameras();
RenderScene();
UpdateAudio();
}
We could proﬁle this game at a very coarse level by measuring the execu-
tion times of each major phase of the game loop:
while (!quitGame)
{
{
PROFILE(SID('Poll Joypad'));
PollJoypad();
}
{
PROFILE(SID('Game Object Update'));
UpdateGameObjects();
}
{
PROFILE(SID('Animation'));
UpdateAllAnimations();
}
{
PROFILE(SID('Joint Post-Processing'));
PostProcessJoints();
}
{
PROFILE(SID('Collision'));
DetectCollisions();
}
{
PROFILE(SID('Physics'));
RunPhysics();
}
{
PROFILE(SID('Animation Finaling'));
GenerateFinalAnimationPoses();
}

434
9. Tools for Debugging and Development
{
PROFILE(SID('Cameras'));
UpdateCameras();
}
{
PROFILE(SID('Rendering'));
RenderScene();
}
{
PROFILE(SID('Audio'));
UpdateAudio();
}
}
The PROFILE() macro shown above would probably be implemented as a
class whose constructor starts the timer and whose destructor stops the timer
and records the execution time under the given name. Thus, it only times
the code within its containing block, by nature of the way C++ automatically
constructs and destroys objects as they go in and out of scope.
struct AutoProfile
{
AutoProfile(const char* name)
{
m_name = name;
m_startTime = QueryPerformanceCounter();
}
~AutoProfile()
{
__int64 endTime = QueryPerformanceCounter();
__int64 elapsedTime = endTime - m_startTime;
g_profileManager.storeSample(m_name, elapsedTime);
}
const char* m_name;
__int64
m_startTime;
};
#define PROFILE(name) AutoProfile p(name)
The problem with this simplistic approach is that it breaks down when
used within deeper levels of function call nesting. For example, if we embed
additional PROFILE() annotations within the RenderScene() function, we
need to understand the function call hierarchy in order to properly interpret
those measurements.

9.8. In-Game Proﬁling
435
One solution to this problem is to allow the programmer who is anno-
tating the code to indicate the hierarchical interrelationships between
proﬁling samples. For example, any PROFILE(...) samples taken within
the RenderScene() function could be declared to be children of the
PROFILE(SID('Rendering')) sample. These relationships are usually set
up separately from the annotations themselves, by predeclaring all of the sam-
ple bins. For example, we might set up the in-game proﬁler during engine
initialization as follows:
// This code declares various profile sample "bins",
// listing the name of the bin and the name of its
// parent bin, if any.
ProfilerDeclareSampleBin(SID('Rendering'), NULL);
ProfilerDeclareSampleBin(SID('Visibility'), SID('Rendering'));
ProfilerDeclareSampleBin(SID('Shaders'), SID('Rendering'));
ProfilerDeclareSampleBin(SID('Materials'), SID('Shaders'));
ProfilerDeclareSampleBin(SID('SubmitGeo'), SID('Rendering'));
ProfilerDeclareSampleBin(SID('Audio'), NULL);
// ...
This approach still has its problems. Speciﬁcally, it works well when every
function in the call hierarchy has only one parent, but it breaks down when
we try to proﬁle a function that is called by more than one parent function.
The reason for this should be pretty obvious. We’re statically declaring our
sample bins as if every function can only appear once in the function call hi-
erarchy, but actually the same function can reappear many times in the tree,
each time with a different parent. The result can be misleading data, because
a function’s time will be included in one of the parent bins, but really should
be distributed across all of its parents’ bins. Most game engines don’t make
an attempt to remedy this problem, since they are primarily interested in pro-
ﬁling coarse-grained functions that are only called from one speciﬁc location
in the function call hierarchy. But this limitation is something to be aware of
when proﬁling your code with a simple in-engine proﬁle of the sort found in
most game engines.
Of course, it is also possible to write a much more sophisticated proﬁling
system that handles nested instances of AutoProﬁle properly. This is an ex-
ample of the many trade-offs one makes when designing a game engine. Do
we invest the engineering time to create a fully hierarchical proﬁler? Or, do
we make do with something simpler and invest those programming resources
elsewhere? Ultimately, it’s up to you.

436
9. Tools for Debugging and Development
We would also like to account for how many times a given function is
called. In the example above, we know that each of the functions we proﬁled
are called exactly once per frame. But other functions, deeper in the function
call hierarchy, may be called more than once per frame. If we measure func-
tion x() to take 2 ms to execute, it’s important to know whether it takes 2 ms
to execute on its own, or whether its execution time is 2 ms but it was called
1,000 times during the frame. Keeping track of the number of times a function
is called per frame is quite simple—the proﬁling system can simply increment
a counter each time a sample is received and reset the counters at the start of
each frame.
9.8.2
Exporting to Excel
Some game engines permit the data captured by the in-game proﬁler to be
dumped to a text ﬁle for subsequent analysis. I ﬁnd that a comma-separated
values (CSV) format is best, because such ﬁles can be loaded easily into a
Microsoft Excel spreadsheet, where the data can be manipulated and analyzed
in myriad ways. I wrote such an exporter for the Medal of Honor: Paciﬁc Assault
engine. The columns corresponded to the various annotated blocks, and each
row represented the proﬁling sample taken during one frame of the game’s
execution. The ﬁrst column contained frame numbers and the second actual
game time measured in seconds. This allowed the team to graph how the
performance statistics varied over time and to determine how long each frame
actually took to execute. By adding some simple formulae to the exported
spreadsheet, we could calculate frame rates, execution time percentages and
so on.
9.9
In-Game Memory Stats and Leak Detection
In addition to runtime performance (i.e., frame rate), most game engines are
also constrained by the amount of memory available on the target hardware.
PC games are least affected by such constraints, because modern PCs have
sophisticated virtual memory managers. But even PC games are constrained
by the memory limitations of their so-called “min spec” machine—the least-
powerful machine on which the game is guaranteed to run, as promised by
the publisher and stated on the game’s packaging.
For this reason, most game engines implement custom memory-tracking
tools. These tools allow the developers to see how much memory is being
used by each engine subsystem and whether or not any memory is leaking
(i.e., memory is allocated but never freed). It’s important to have this infor-

9.9. In-Game Memory Stats and Leak Detection
437
mation so that you can make informed decisions when trying to cut back the
memory usage of your game so that it will ﬁt onto the console or type of PC
you are targeting.
Keeping track of how much memory a game actually uses can be a sur-
prisingly tricky job. You’d think you could simply wrap malloc()/free()
or new/delete in a pair of functions or macros that keep track of the
amount of memory that is allocated and freed. However, it’s never that simple
for a few reasons:
1.
You often can’t control the allocation behavior of other people’s code. Unless
you write the operating system, drivers and the game engine entirely
from scratch, there’s a good chance you’re going to end up linking your
game with at least some third-party libraries. Most good libraries pro-
vide memory allocation hooks so that you can replace their allocators with
your own. But some do not. It’s often difﬁcult to keep track of the mem-
ory allocated by each and every third-party library you use in your game
engine—but it usually can be done if you’re thorough and selective in
your choice of third-party libraries.
2.
Memory comes in different ﬂavors. For example, a PC has two kinds of
RAM: main RAM and video RAM (the memory residing on your graph-
ics card, which is used primarily for geometry and texture data). Even
if you manage to track all of the memory allocations and deallocations
occurring within main RAM, it can be well neigh impossible to track
video RAM usage. This is because graphics APIs like DirectX actually
hide the details of how video RAM is being allocated and used from the
developer. On a console, life is a bit easier only because you often end
up having to write a video RAM manager yourself. This is more difﬁcult
than using DirectX, but at least you have complete knowledge of what’s
going on.
3.
Allocators come in different ﬂavors. Many games make use of specialized
allocators for various purposes. For example, the Naughty Dog engine
has a global heap for general-purpose allocations, a special heap for man-
aging the memory created by game objects as they spawn into the game
world and are destroyed, a level-loading heap for data that is streamed into
memory during gameplay, a stack allocator for single-frame allocations
(the stack is cleared automatically every frame), an allocator for video
RAM, and a debug memory heap used only for allocations that will not be
needed in the ﬁnal shipping game. Each of these allocators grabs a large
hunk of memory when the game starts up and then manages that mem-

438
9. Tools for Debugging and Development
Figure 9.14. Tabular memory statistics from Naughty Dog’s The Last of Us engine.
ory block itself. If we were to track all the calls to new and delete, we’d
see one new for each of these six allocators and that’s all. To get any
useful information, we really need to track all of the allocations within
each of these allocators’ memory blocks.
Most professional game teams expend a signiﬁcant amount of effort on
creating in-engine memory-tracking tools that provide accurate and detailed
information. The resulting tools usually provide their output in a variety of
forms. For example, the engine might produce a detailed dump of all memory
allocations made by the game during a speciﬁc period of time. The data might
include high water marks for each memory allocator or each game system,
indicating the maximum amount of physical RAM required by each. Some
engines also provide heads-up displays of memory usage while the game is
running. This data might be tabular, as shown in Figure 9.14, or graphical as
shown in Figure 9.15.
In addition, when low-memory or out-of-memory conditions arise, a good
engine will provide this information in as helpful a way as possible. When
PC games are developed, the game team usually works on high-powered PCs
with more RAM than the min-spec machine being targeted. Likewise, console
games are developed on special development kits that have more memory than
a retail console. So in both cases, the game can continue to run even when it
technically has run out of memory (i.e., would no longer ﬁt on a retail console
or min-spec PC). When this kind of out-of-memory condition arises, the game

9.9. In-Game Memory Stats and Leak Detection
439
Figure 9.15. A graphical memory usage display, also from The Last of Us (© 2013/™SCEA. Created
and developed by Naughty Dog, PlayStation 3).
engine can display a message saying something like, “Out of memory—this
level will not run on a retail system.”
There are lots of other ways in which a game engine’s memory tracking
system can aid developers in pinpointing problems as early and as conve-
niently as possible. Here are just a few examples:
•
If a model fails to load, a bright red text string could be displayed in 3D
hovering in the game world where that object would have been.
•
If a texture fails to load, the object could be drawn with an ugly pink
texture that is very obviously not part of the ﬁnal game.
•
If an animation fails to load, the character could assume a special (pos-
sibly humorous) pose that indicates a missing animation, and the name
of the missing asset could hover over the character’s head.
The key to providing good memory analysis tools is (a) to provide accurate
information, (b) to present the data in a way that is convenient and that makes
problems obvious and (c) to provide contextual information to aid the team
in tracking down the root cause of problems when they occur.

This page intentionally left blank
This page intentionally left blank

Part III
Graphics, Motion
and Sound

This page intentionally left blank
This page intentionally left blank

10
The Rendering Engine
W
hen most people think about computer and video games, the ﬁrst thing
that comes to mind is the stunning three-dimensional graphics. Real-
time 3D rendering is an exceptionally broad and profound topic, so there’s
simply no way to cover all of the details in a single chapter. Thankfully there
are a great many excellent books and other resources available on this topic.
In fact, real-time 3D graphics is perhaps one of the best covered of all the
technologies that make up a game engine. The goal of this chapter, then, is
to provide you with a broad understanding of real-time rendering technology
and to serve as a jumping-off point for further learning. After you’ve read
through these pages, you should ﬁnd that reading other books on 3D graphics
seems like a journey through familiar territory. You might even be able to
impress your friends at parties (...or alienate them...)
We’ll begin by laying a solid foundation in the concepts, theory and math-
ematics that underlie any real-time 3D rendering engine. Next, we’ll have
a look at the software and hardware pipelines used to turn this theoretical
framework into reality. We’ll discuss some of the most common optimization
techniques and see how they drive the structure of the tools pipeline and the
runtime rendering API in most engines. We’ll end with a survey of some of
the advanced rendering techniques and lighting models in use by game en-
gines today. Throughout this chapter, I’ll point you to some of my favorite
443

444
10. The Rendering Engine
books and other resources that should help you to gain an even deeper un-
derstanding of the topics we’ll cover here.
10.1
Foundations of Depth-Buffered
Triangle Rasterization
When you boil it down to its essence, rendering a three-dimensional scene
involves the following basic steps:
•
A virtual scene is described, usually in terms of 3D surfaces represented
in some mathematical form.
•
A virtual camera is positioned and oriented to produce the desired view
of the scene. Typically the camera is modeled as an idealized focal point,
with an imaging surface hovering some small distance in front of it,
composed of virtual light sensors corresponding to the picture elements
(pixels) of the target display device.
•
Various light sources are deﬁned. These sources provide all the light rays
that will interact with and reﬂect off the objects in the environment and
eventually ﬁnd their way onto the image-sensing surface of the virtual
camera.
•
The visual properties of the surfaces in the scene are described. This de-
ﬁnes how light should interact with each surface.
•
For each pixel within the imaging rectangle, the rendering engine calcu-
lates the color and intensity of the light ray(s) converging on the virtual
camera’s focal point through that pixel. This is known as solving the ren-
dering equation (also called the shading equation).
This high-level rendering process is depicted in Figure 10.1.
Many different technologies can be used to perform the basic rendering
steps described above. The primary goal is usually photorealism, although
some games aim for a more stylized look (e.g., cartoon, charcoal sketch, wa-
tercolor and so on). As such, rendering engineers and artists usually attempt
to describe the properties of their scenes as realistically as possible and to
use light transport models that match physical reality as closely as possible.
Within this context, the gamut of rendering technologies ranges from tech-
niques designed for real-time performance at the expense of visual ﬁdelity, to
those designed for photorealism but which are not intended to operate in real
time.
Real-time rendering engines perform the steps listed above repeatedly, dis-
playing rendered images at a rate of 30, 50 or 60 frames per second to provide

10.1. Foundations of Depth-Buffered Triangle Rasterization
445
Virtual 
Screen
(Near Plane)
xC
zC
yC
Rendered
Image
Camera
Frustum
Camera
Figure 10.1. The high-level rendering approach used by virtually all 3D computer graphics technologies.
the illusion of motion. This means a real-time rendering engine has at most
33.3 ms to generate each image (to achieve a frame rate of 30 FPS). Usually
much less time is available, because bandwidth is also consumed by other en-
gine systems like animation, AI, collision detection, physics simulation, audio,
player mechanics and other gameplay logic. Considering that ﬁlm rendering
engines often take anywhere from many minutes to many hours to render a
single frame, the quality of real-time computer graphics these days is truly
astounding.
10.1.1
Describing a Scene
A real-world scene is composed of objects. Some objects are solid, like a brick,
and some are amorphous, like a cloud of smoke, but every object occupies a
volume of 3D space. An object might be opaque (in which case light cannot
pass through its volume), transparent (in which case light passes through it
without being scattered, so that we can see a reasonably clear image of what-
ever is behind the object), or translucent (meaning that light can pass through
the object but is scattered in all directions in the process, yielding only a blur
of colors that hint at the objects behind it).
Opaque objects can be rendered by considering only their surfaces. We
don’t need to know what’s inside an opaque object in order to render it, be-
cause light cannot penetrate its surface. When rendering a transparent or
translucent object, we really should model how light is reﬂected, refracted,
scattered and absorbed as it passes through the object’s volume. This requires
knowledge of the interior structure and properties of the object. However,

446
10. The Rendering Engine
most game engines don’t go to all that trouble. They just render the surfaces
of transparent and translucent objects in almost the same way opaque objects
are rendered. A simple numeric opacity measure known as alpha is used to
describe how opaque or transparent a surface is. This approach can lead to
various visual anomalies (for example, surface features on the far side of the
object may be rendered incorrectly), but the approximation can be made to
look reasonably realistic in many cases. Even amorphous objects like clouds
of smoke are often represented using particle effects, which are typically com-
posed of large numbers of semitransparent rectangular cards. Therefore, it’s
safe to say that most game rendering engines are primarily concerned with
rendering surfaces.
10.1.1.1
Representations Used by High-End Rendering Packages
Theoretically, a surface is a two-dimensional sheet comprised of an inﬁnite
number of points in three-dimensional space. However, such a description is
clearly not practical. In order for a computer to process and render arbitrary
surfaces, we need a compact way to represent them numerically.
Some surfaces can be described exactly in analytical form using a paramet-
ric surface equation. For example, a sphere centered at the origin can be rep-
resented by the equation x2 + y2 + z2 = r2. However, parametric equations
aren’t particularly useful for modeling arbitrary shapes.
In the ﬁlm industry, surfaces are often represented by a collection of rect-
angular patches each formed from a two-dimensional spline deﬁned by a small
number of control points. Various kinds of splines are used, including Bézier
surfaces (e.g., bicubic patches, which are third-order Béziers—see http://en.
wikipedia.org/wiki/Bezier_surface for more information), nonuniform ratio-
nal B-splines (NURBS—see http://en.wikipedia.org/wiki/Nurbs), Bézier tri-
angles and N-patches (also known as normal patches—see http://ubm.io/
1iGnvJ5 for more details). Modeling with patches is a bit like covering a statue
with little rectangles of cloth or paper maché.
High-end ﬁlm rendering engines like Pixar’s RenderMan use subdivision
surfaces to deﬁne geometric shapes. Each surface is represented by a mesh
of control polygons (much like a spline), but the polygons can be subdivided
into smaller and smaller polygons using the Catmull-Clark algorithm. This
subdivision typically proceeds until the individual polygons are smaller than
a single pixel in size. The biggest beneﬁt of this approach is that no matter how
close the camera gets to the surface, it can always be subdivided further so
that its silhouette edges won’t look faceted. To learn more about subdivision
surfaces, check out the following great article: http://ubm.io/1lx6th5.

10.1. Foundations of Depth-Buffered Triangle Rasterization
447
10.1.1.2
Triangle Meshes
Game developers have traditionally modeled their surfaces using triangle
meshes.
Triangles serve as a piecewise linear approximation to a surface,
much as a chain of connected line segments acts as a piecewise approxima-
tion to a function or curve (see Figure 10.2).
Figure 10.2.
A mesh
of triangles is a linear
approximation to a sur-
face, just as a series
of connected line seg-
ments can serve as a
linear approximation to
a function or curve.
Triangles are the polygon of choice for real-time rendering because they
have the following desirable properties:
•
The triangle is the simplest type of polygon. Any fewer than three vertices,
and we wouldn’t have a surface at all.
•
A triangle is always planar. Any polygon with four or more vertices need
not have this property because, while the ﬁrst three vertices deﬁne a
plane, the fourth vertex might lie above or below that plane.
•
Triangles remain triangles under most kinds of transformations, including afﬁne
transforms and perspective projections. At worst, a triangle viewed edge-
on will degenerate into a line segment. At every other orientation, it
remains triangular.
•
Virtually all commercial graphics-acceleration hardware is designed around tri-
angle rasterization. Starting with the earliest 3D graphics accelerators
for the PC, rendering hardware has been designed almost exclusively
around triangle rasterization. This decision can be traced all the way
back to the ﬁrst software rasterizers used in the earliest 3D games like
Castle Wolfenstein 3D and Doom. Like it or not, triangle-based technolo-
gies are entrenched in our industry and probably will be for years to
come.
Tessellation
The term tessellation describes a process of dividing a surface up into a collec-
tion of discrete polygons (which are usually either quadrilaterals, also known
as quads, or triangles). Triangulation is tessellation of a surface into triangles.
One problem with the kind of triangle mesh used in games is that its level
of tessellation is ﬁxed by the artist when he or she creates it. Fixed tessellation
can cause an object’s silhouette edges to look blocky, as shown in Figure 10.3;
this is especially noticeable when the object is close to the camera.
Ideally, we’d like a solution that can arbitrarily increase tessellation as an
object gets closer to the virtual camera. In other words, we’d like to have a
uniform triangle-to-pixel density, no matter how close or far away the object
is. Subdivision surfaces can achieve this ideal—surfaces can be tessellated
based on distance from the camera, so that every triangle is less than one pixel
in size.

448
10. The Rendering Engine
Figure 10.3. Fixed tessellation can cause an object’s silhouette edges to look blocky, especially
when the object is close to the camera.
Game developers often attempt to approximate this ideal of uniform tri-
angle-to-pixel density by creating a chain of alternate versions of each triangle
mesh, each known as a level of detail (LOD). The ﬁrst LOD, often called LOD 0,
represents the highest level of tessellation; it is used when the object is very
close to the camera. Subsequent LODs are tessellated at lower and lower res-
olutions (see Figure 10.4). As the object moves farther away from the camera,
the engine switches from LOD 0 to LOD 1 to LOD 2 and so on. This allows the
rendering engine to spend the majority of its time transforming and lighting
the vertices of the objects that are closest to the camera (and therefore occupy
the largest number of pixels on-screen).
Some game engines apply dynamic tessellation techniques to expansive
meshes like water or terrain. In this technique, the mesh is usually repre-
sented by a height ﬁeld deﬁned on some kind of regular grid pattern. The
region of the mesh that is closest to the camera is tessellated to the full resolu-
tion of the grid. Regions that are farther away from the camera are tessellated
using fewer and fewer grid points.
Progressive meshes are another technique for dynamic tessellation and LOD-
ing. With this technique, a single high-resolution mesh is created for display
when the object is very close to the camera. (This is essentially the LOD 0
Figure 10.4. A chain of LOD meshes, each with a ﬁxed level of tessellation, can be used to approx-
imate uniform triangle-to-pixel density. The leftmost torus is constructed from 5000 triangles,
the center torus from 450 triangles and the rightmost torus from 200 triangles.

10.1. Foundations of Depth-Buffered Triangle Rasterization
449
mesh.)
This mesh is automatically detessellated as the object gets farther
away by collapsing certain edges. In effect, this process automatically gen-
erates a semi-continuous chain of LODs. See http://research.microsoft.com/
en-us/um/people/hoppe/pm.pdf for a detailed discussion of progressive
mesh technology.
10.1.1.3
Constructing a Triangle Mesh
Now that we understand what triangle meshes are and why they’re used, let’s
take a brief look at how they’re constructed.
Winding Order
A triangle is deﬁned by the position vectors of its three vertices, which we
can denote p1, p2 and p3. The edges of a triangle can be found by simply
subtracting the position vectors of adjacent vertices. For example,
e12 = p2 −p1,
e13 = p3 −p1,
e23 = p3 −p2.
The normalized cross product of any two edges deﬁnes a unit face normal N:
N = e12 × e13
|e12 × e13|.
These derivations are illustrated in Figure 10.5. To know the direction of the
face normal (i.e., the sense of the edge cross product), we need to deﬁne which
side of the triangle should be considered the front (i.e., the outside surface of
an object) and which should be the back (i.e., its inside surface). This can be
deﬁned easily by specifying a winding order—clockwise (CW) or counterclock-
wise (CCW).
Most low-level graphics APIs give us a way to cull back-facing triangles
based on winding order. For example, if we set the cull mode parameter in
p1
p2
p3
e12
N
e13
e23
Figure 10.5. Deriving the edges and plane of a triangle from its vertices.

450
10. The Rendering Engine
Direct3D (D3DRS_CULL) to D3DCULLMODE_CW, then any triangle whose ver-
tices wind in a clockwise fashion in screen space will be treated as a back-
facing triangle and will not be drawn.
Back-face culling is important because we generally don’t want to waste
time drawing triangles that aren’t going to be visible anyway. Also, rendering
the back faces of transparent objects can actually cause visual anomalies. The
choice of winding order is an arbitrary one, but of course it must be consistent
across all assets in the entire game. Inconsistent winding order is a common
error among junior 3D modelers.
Triangle Lists
The easiest way to deﬁne a mesh is simply to list the vertices in groups of
three, each triple corresponding to a single triangle. This data structure is
known as a triangle list; it is illustrated in Figure 10.6.
V0
V1
V2
V3
V4
V5
V6
V7
...
V5
V7
V6
V0
V5
V1
V1
V2
V3
V0
V1
V3
Figure 10.6. A triangle list.
Indexed Triangle Lists
You probably noticed that many of the vertices in the triangle list shown in
Figure 10.6 were duplicated, often multiple times. As we’ll see in Section
10.1.2.1, we often store quite a lot of metadata with each vertex, so repeating
this data in a triangle list wastes memory. It also wastes GPU bandwidth,
because a duplicated vertex will be transformed and lit multiple times.
For these reasons, most rendering engines make use of a more efﬁcient
data structure known as an indexed triangle list. The basic idea is to list the
vertices once with no duplication and then to use lightweight vertex indices
(usually occupying only 16 bits each) to deﬁne the triples of vertices that con-

10.1. Foundations of Depth-Buffered Triangle Rasterization
451
V0
V1
V2
V3
V4
V5
V6
V7
Indices
0
1
3
1
2
3
0
5
1
...
5
7
6
Vertices
V0
V1
V2
V3
V4
V5
V6
V7
Figure 10.7. An indexed triangle list.
stitute the triangles. The vertices are stored in an array known as a vertex buffer
(DirectX) or vertex array (OpenGL). The indices are stored in a separate buffer
known as an index buffer or index array. This technique is shown in Figure 10.7.
Strips and Fans
Specialized mesh data structures known as triangle strips and triangle fans are
sometimes used for game rendering. Both of these data structures eliminate
the need for an index buffer, while still reducing vertex duplication to some
degree. They accomplish this by predeﬁning the order in which vertices must
appear and how they are combined to form triangles.
In a strip, the ﬁrst three vertices deﬁne the ﬁrst triangle. Each subsequent
vertex forms an entirely new triangle, along with its previous two neigh-
bors. To keep the winding order of a triangle strip consistent, the previous
two neighbor vertices swap places after each new triangle. A triangle strip is
shown in Figure 10.8.
In a fan, the ﬁrst three vertices deﬁne the ﬁrst triangle and each subsequent
vertex deﬁnes a new triangle with the previous vertex and the ﬁrst vertex in
the fan. This is illustrated in Figure 10.9.
Vertex Cache Optimization
When a GPU processes an indexed triangle list, each triangle can refer to any
vertex within the vertex buffer. The vertices must be processed in the order
they appear within the triangles, because the integrity of each triangle must be
maintained for the rasterization stage. As vertices are processed by the vertex
shader, they are cached for reuse. If a subsequent primitive refers to a vertex

452
10. The Rendering Engine
Interpreted
as triangles:
0
1
2
1 3
2
2
3
4
3
5
4
V0
V1
V2
V3
V4
V5
Vertices
V0
V1
V2
V3
V4
V5
Figure 10.8. A triangle strip.
0
1
2
0 2
3
0
3
4
V0
V1
V2
V3
V4
V0
V4
V3
V2
V1
Figure 10.9. A triangle fan.
that already resides in the cache, its processed attributes are used instead of
reprocessing the vertex.
Strips and fans are used in part because they can potentially save mem-
ory (no index buffer required) and in part because they tend to improve the
cache coherency of the memory accesses made by the GPU to video RAM.
Even better, we can use an indexed strip or indexed fan to virtually eliminate
vertex duplication (which can often save more memory than eliminating the
index buffer), while still reaping the cache coherency beneﬁts of the strip or
fan vertex ordering.
Indexed triangle lists can also be cache-optimized without restricting our-
selves to strip or fan vertex ordering. A vertex cache optimizer is an ofﬂine
geometry processing tool that attempts to list the triangles in an order that
optimizes vertex reuse within the cache. It generally takes into account fac-
tors such as the size of the vertex cache(s) present on a particular type of GPU
and the algorithms used by the GPU to decide when to cache vertices and
when to discard them. For example, the vertex cache optimizer included in
Sony’s Edge geometry processing library can achieve rendering throughput
that is up to 4% better than what is possible with triangle stripping.
10.1.1.4
Model Space
The position vectors of a triangle mesh’s vertices are usually speciﬁed relative
to a convenient local coordinate system called model space, local space, or object
space. The origin of model space is usually either in the center of the object

10.1. Foundations of Depth-Buffered Triangle Rasterization
453
L = i
F = k
U = j
Figure 10.10. One possible mapping of the model-space axes.
or at some other convenient location, like on the ﬂoor between the feet of a
character or on the ground at the horizontal centroid of the wheels of a vehicle.
As we learned in Section 4.3.9.1, the sense of the model-space axes is arbi-
trary, but the axes typically align with the natural “front,” “left,” “right” and
“up” directions on the model. For a little mathematical rigor, we can deﬁne
three unit vectors F, L (or R) and U and map them as desired onto the unit
basis vectors i, j and k (and hence to the x-, y- and z-axes, respectively) in
model space. For example, a common mapping is L = i, U = j and F = k.
The mapping is completely arbitrary, but it’s important to be consistent for all
models across the entire engine. Figure 10.10 shows one possible mapping of
the model-space axes for an aircraft model.
10.1.1.5
World Space and Mesh Instancing
Many individual meshes are composed into a complete scene by position-
ing and orienting them within a common coordinate system known as world
space. Any one mesh might appear many times in a scene—examples include
a street lined with identical lamp posts, a faceless mob of soldiers or a swarm
of spiders attacking the player. We call each such object a mesh instance.
A mesh instance contains a reference to its shared mesh data and also in-
cludes a transformation matrix that converts the mesh’s vertices from model
space to world space, within the context of that particular instance. This ma-
trix is called the model-to-world matrix, or sometimes just the world matrix. Us-
ing the notation from Section 4.3.10.2, this matrix can be written as follows:
MM→W =

(RS)M→W
0
tM
1

,

454
10. The Rendering Engine
where the upper 3 × 3 matrix (RS)M→W rotates and scales model-space ver-
tices into world space, and tM is the translation of the model-space axes ex-
pressed in world space. If we have the unit model-space basis vectors iM, jM
and kM, expressed in world-space coordinates, this matrix can also be written
as follows:
MM→W =


iM
0
jM
0
kM
0
tM
1

.
Given a vertex expressed in model-space coordinates, the rendering en-
gine calculates its world-space equivalent as follows:
vW = vMMM→W .
We can think of the matrix MM→W as a description of the position and orien-
tation of the model-space axes themselves, expressed in world-space coordi-
nates. Or we can think of it as a matrix that transforms vertices from model
space to world space.
When rendering a mesh, the model-to-world matrix is also applied to the
surface normals of the mesh (see Section 10.1.2.1). Recall from Section 4.3.11,
that in order to transform normal vectors properly, we must multiply them
by the inverse transpose of the model-to-world matrix. If our matrix does
not contain any scale or shear, we can transform our normal vectors correctly
by simply setting their w components to zero prior to multiplication by the
model-to-world matrix, as described in Section 4.3.6.1.
Some meshes like buildings, terrain and other background elements are
entirely static and unique. The vertices of these meshes are often expressed in
world space, so their model-to-world matrices are identity and can be ignored.
10.1.2
Describing the Visual Properties of a Surface
In order to properly render and light a surface, we need a description of its vi-
sual properties. Surface properties include geometric information, such as the
direction of the surface normal at various points on the surface. They also
encompass a description of how light should interact with the surface. This
includes diffuse color, shininess/reﬂectivity, roughness or texture, degree of
opacity or transparency, index of refraction and other optical properties. Sur-
face properties might also include a speciﬁcation of how the surface should
change over time (e.g., how an animated character’s skin should track the
joints of its skeleton or how the surface of a body of water should move).

10.1. Foundations of Depth-Buffered Triangle Rasterization
455
The key to rendering photorealistic images is properly accounting for light’s
behavior as it interacts with the objects in the scene. Hence rendering engi-
neers need to have a good understanding of how light works, how it is trans-
ported through an environment and how the virtual camera “senses” it and
translates it into the colors stored in the pixels on-screen.
10.1.2.1
Introduction to Light and Color
Light is electromagnetic radiation; it acts like both a wave and a particle in
different situations. The color of light is determined by its intensity I and its
wavelength λ (or its frequency f, where f = 1/λ). The visible gamut ranges
from a wavelength of 740 nm (or a frequency of 430 THz) to a wavelength of
380 nm (750 THz). A beam of light may contain a single pure wavelength (i.e.,
the colors of the rainbow, also known as the spectral colors), or it may contain
a mixture of various wavelengths. We can draw a graph showing how much
of each frequency a given beam of light contains, called a spectral plot. White
light contains a little bit of all wavelengths, so its spectral plot would look
roughly like a box extending across the entire visible band. Pure green light
contains only one wavelength, so its spectral plot would look like a single
inﬁnitesimally narrow spike at about 570 THz.
Light-Object Interactions
Light can have many complex interactions with matter. Its behavior is gov-
erned in part by the medium through which it is traveling and in part by the
shape and properties of the interfaces between different types of media (air-
solid, air-water, water-glass, etc.). Technically speaking, a surface is really just
an interface between two different types of media.
Despite all of its complexity, light can really only do four things:
•
It can be absorbed.
•
It can be reﬂected.
•
It can be transmitted through an object, usually being refracted (bent) in
the process.
•
It can be diffracted when passing through very narrow openings.
Most photorealistic rendering engines account for the ﬁrst three of these be-
haviors; diffraction is not usually taken into account because its effects are
rarely noticeable in most scenes.
Only certain wavelengths may be absorbed by a surface, while others are
reﬂected. This is what gives rise to our perception of the color of an object.
For example, when white light falls on a red object, all wavelengths except

456
10. The Rendering Engine
red are absorbed, hence the object appears red. The same perceptual effect is
achieved when red light is cast onto a white object—our eyes don’t know the
difference.
Reﬂections can be diffuse, meaning that an incoming ray is scattered equally
in all directions. Reﬂections can also be specular, meaning that an incident light
ray will reﬂect directly or be spread only into a narrow cone. Reﬂections can
also be anisotropic, meaning that the way in which light reﬂects from a surface
changes depending on the angle at which the surface is viewed.
When light is transmitted through a volume, it can be scattered (as is the
case for translucent objects), partially absorbed (as with colored glass), or re-
fracted (as happens when light travels through a prism). The refraction an-
gles can be different for different wavelengths, leading to spectral spreading.
This is why we see rainbows when light passes through raindrops and glass
prisms. Light can also enter a semi-solid surface, bounce around and then exit
the surface at a different point from the one at which it entered the surface. We
call this subsurface scattering, and it is one of the effects that gives skin, wax and
marble their characteristic warm appearance.
Color Spaces and Color Models
A color model is a three-dimensional coordinate system that measures colors.
A color space is a speciﬁc standard for how numerical colors in a particular
color model should be mapped onto the colors perceived by human beings in
the real world. Color models are typically three-dimensional because of the
three types of color sensors (cones) in our eyes, which are sensitive to different
wavelengths of light.
The most commonly used color model in computer graphics is the RGB
model. In this model, color space is represented by a unit cube, with the rela-
tive intensities of red, green and blue light measured along its axes. The red,
green and blue components are called color channels. In the canonical RGB
color model, each channel ranges from zero to one. So the color (0, 0, 0) rep-
resents black, while (1, 1, 1) represents white.
When colors are stored in a bitmapped image, various color formats can
be employed. A color format is deﬁned in part by the number of bits per pixel
it occupies and, more speciﬁcally, the number of bits used to represent each
color channel. The RGB888 format uses eight bits per channel, for a total of
24 bits per pixel. In this format, each channel ranges from 0 to 255 rather than
from zero to one. RGB565 uses ﬁve bits for red and blue and six for green, for
a total of 16 bits per pixel. A paletted format might use eight bits per pixel
to store indices into a 256-element color palette, each entry of which might be
stored in RGB888 or some other suitable format.

10.1. Foundations of Depth-Buffered Triangle Rasterization
457
A number of other color models are also used in 3D rendering. We’ll see
how the log-LUV color model is used for high dynamic range (HDR) lighting in
Section 10.3.1.5.
Opacity and the Alpha Channel
A fourth channel called alpha is often tacked on to RGB color vectors. As
mentioned in Section 10.1.1, alpha measures the opacity of an object. When
stored in an image pixel, alpha represents the opacity of the pixel.
RGB color formats can be extended to include an alpha channel, in which
case they are referred to as RGBA or ARGB color formats.
For example,
RGBA8888 is a 32 bit-per-pixel format with eight bits each for red, green, blue
and alpha. RGBA5551 is a 16 bit-per-pixel format with one-bit alpha; in this
format, colors can either be fully opaque or fully transparent.
10.1.2.2
Vertex Attributes
The simplest way to describe the visual properties of a surface is to specify
them at discrete points on the surface. The vertices of a mesh are a convenient
place to store surface properties, in which case they are called vertex attributes.
A typical triangle mesh includes some or all of the following attributes
at each vertex. As rendering engineers, we are of course free to deﬁne any
additional attributes that may be required in order to achieve a desired visual
effect on-screen.
•
Position vector (pi =
pix
piy
piz

). This is the 3D position of the ith
vertex in the mesh. It is usually speciﬁed in a coordinate space local to
the object, known as model space.
•
Vertex normal (ni =
nix
niy
niz

). This vector deﬁnes the unit sur-
face normal at the position of vertex i. It is used in per-vertex dynamic
lighting calculations.
•
Vertex tangent (ti =
tix
tiy
tiz

) and bitangent (bi =
bix
biy
biz

).
These two unit vectors lie perpendicular to one another and to the ver-
tex normal ni. Together, the three vectors ni, ti and bi deﬁne a set of
coordinate axes known as tangent space. This space is used for various
per-pixel lighting calculations, such as normal mapping and environ-
ment mapping. (The bitangent bi is sometimes confusingly called the
binormal, even though it is not normal to the surface.)
•
Diffuse color (di =
diR
diG
diB
diA

). This four-element vector de-
scribes the diffuse color of the surface, expressed in the RGB color space.
It typically also includes a speciﬁcation of the opacity or alpha (A) of the

458
10. The Rendering Engine
surface at the position of the vertex. This color may be calculated ofﬂine
(static lighting) or at runtime (dynamic lighting).
•
Specular color (si =
siR
siG
siB
siA

). This quantity describes the
color of the specular highlight that should appear when light reﬂects
directly from a shiny surface onto the virtual camera’s imaging plane.
•
Texture coordinates (uij =
uij
vij

). Texture coordinates allow a two-
(or sometimes three-) dimensional bitmap to be “shrink wrapped” onto
the surface of a mesh—a process known as texture mapping. A texture
coordinate (u, v) describes the location of a particular vertex within the
two-dimensional normalized coordinate space of the texture. A triangle
can be mapped with more than one texture; hence it can have more than
one set of texture coordinates. We’ve denoted the distinct sets of texture
coordinates via the subscript j above.
•
Skinning weights (kij =

kij
wij

). In skeletal animation, the vertices of
a mesh are attached to individual joints in an articulated skeleton. In this
case, each vertex must specify to which joint it is attached via an index,
k. A vertex can be inﬂuenced by multiple joints, in which case the ﬁnal
vertex position becomes a weighted average of these inﬂuences. Thus, the
weight of each joint’s inﬂuence is denoted by a weighting factor w. In
general, a vertex i can have multiple joint inﬂuences j, each denoted by
the pair of numbers (kij, wij).
10.1.2.3
Vertex Formats
Vertex attributes are typically stored within a data structure such as a C
struct or a C++ class. The layout of such a data structure is known as a
vertex format. Different meshes require different combinations of attributes
and hence need different vertex formats. The following are some examples of
common vertex formats:
// Simplest possible vertex -- position only (useful for
// shadow volume extrusion, silhouette edge detection
// for cartoon rendering, z-prepass, etc.)
struct Vertex1P
{
Vector3
m_p;
// position
};
// A typical vertex format with position, vertex normal
// and one set of texture coordinates.
struct Vertex1P1N1UV
{
Vector3 m_p;
// position

10.1. Foundations of Depth-Buffered Triangle Rasterization
459
Vector3 m_n;
// vertex normal
F32
m_uv[2]; // (u, v) texture coordinate
};
// A skinned vertex with position, diffuse and specular
// colors and four weighted joint influences.
struct Vertex1P1D1S2UV4J
{
Vector3 m_p;
// position
Color4
m_d;
// diffuse color and translucency
Color4
m_S;
// specular color
F32
m_uv0[2]; // first set of tex coords
F32
m_uv1[2]; // second set of tex coords
U8
m_k[4];
// four joint indices, and...
F32
m_w[3];
// three joint weights, for
// skinning (fourth is calc'd
// from the first three)
};
Clearly the number of possible permutations of vertex attributes—and
hence the number of distinct vertex formats—can grow to be extremely large.
(In fact the number of formats is theoretically unbounded, if one were to per-
mit any number of texture coordinates and/or joint weights.) Management
of all these vertex formats is a common source of headaches for any graphics
programmer.
Some steps can be taken to reduce the number of vertex formats that an
engine has to support. In practical graphics applications, many of the theoret-
ically possible vertex formats are simply not useful, or they cannot be handled
by the graphics hardware or the game’s shaders. Some game teams also limit
themselves to a subset of the useful/feasible vertex formats in order to keep
things more manageable. For example, they might only allow zero, two or
four joint weights per vertex, or they might decide to support no more than
two sets of texture coordinates per vertex. Some GPUs are capable of extract-
ing a subset of attributes from a vertex data structure, so game teams can also
choose to use a single “überformat” for all meshes and let the hardware select
the relevant attributes based on the requirements of the shader.
10.1.2.4
Attribute Interpolation
The attributes at a triangle’s vertices are just a coarse, discretized approxi-
mation to the visual properties of the surface as a whole. When rendering a
triangle, what really matters are the visual properties at the interior points of
the triangle as “seen” through each pixel on-screen. In other words, we need
to know the values of the attributes on a per-pixel basis, not a per-vertex basis.

460
10. The Rendering Engine
Figure 10.11. A Gouraud-shaded triangle with different shades of gray at the vertices.
Figure 10.12. Gouraud shading can make faceted objects appear to be smooth.
One simple way to determine the per-pixel values of a mesh’s surface at-
tributes is to linearly interpolate the per-vertex attribute data. When applied to
vertex colors, attribute interpolation is known as Gouraud shading. An exam-
ple of Gouraud shading applied to a triangle is shown in Figure 10.11, and its
effects on a simple triangle mesh are illustrated in Figure 10.12. Interpolation
is routinely applied to other kinds of vertex attribute information as well, such
as vertex normals, texture coordinates and depth.
Vertex Normals and Smoothing
As we’ll see in Section 10.1.3, lighting is the process of calculating the color
of an object at various points on its surface, based on the visual properties of
the surface and the properties of the light impinging upon it. The simplest
way to light a mesh is to calculate the color of the surface on a per-vertex basis.
In other words, we use the properties of the surface and the incoming light
to calculate the diffuse color of each vertex (di). These vertex colors are then
interpolated across the triangles of the mesh via Gouraud shading.
In order to determine how a ray of light will reﬂect from a point on a sur-
face, most lighting models make use of a vector that is normal to the surface
at the point of the light ray’s impact. Since we’re performing lighting calcula-
tions on a per-vertex basis, we can use the vertex normal ni for this purpose.
Therefore, the directions of a mesh’s vertex normals can have a signiﬁcant
impact on the ﬁnal appearance of a mesh.

10.1. Foundations of Depth-Buffered Triangle Rasterization
461
Figure 10.13. The directions of a mesh’s vertex normals can have a profound effect on the colors
calculated during per-vertex lighting calculations.
As an example, consider a tall, thin, four-sided box. If we want the box to
appear to be sharp-edged, we can specify the vertex normals to be perpendic-
ular to the faces of the box. As we light each triangle, we will encounter the
same normal vector at all three vertices, so the resulting lighting will appear
ﬂat, and it will abruptly change at the corners of the box just as the vertex
normals do.
We can also make the same box mesh look a bit like a smooth cylinder by
specifying vertex normals that point radially outward from the box’s center
line. In this case, the vertices of each triangle will have different vertex nor-
mals, causing us to calculate different colors at each vertex. Gouraud shading
will smoothly interpolate these vertex colors, resulting in lighting that appears
to vary smoothly across the surface. This effect is illustrated in Figure 10.13.
10.1.2.5
Textures
When triangles are relatively large, specifying surface properties on a per-
vertex basis can be too coarse-grained. Linear attribute interpolation isn’t al-
ways what we want, and it can lead to undesirable visual anomalies.
As an example, consider the problem of rendering the bright specular high-
light that can occur when light shines on a glossy object. If the mesh is highly
tessellated, per-vertex lighting combined with Gouraud shading can yield rea-
sonably good results. However, when the triangles are too large, the errors
that arise from linearly interpolating the specular highlight can become jar-
ringly obvious, as shown in Figure 10.14.

462
10. The Rendering Engine
Figure 10.14. Linear interpolation of vertex attributes does not always yield an adequate descrip-
tion of the visual properties of a surface, especially when tessellation is low.
To overcome the limitations of per-vertex surface attributes, rendering en-
gineers use bitmapped images known as texture maps. A texture often contains
color information and is usually projected onto the triangles of a mesh. In this
case, it acts a bit like those silly fake tattoos we used to apply to our arms when
we were kids. But a texture can contain other kinds of visual surface proper-
ties as well as colors. And a texture needn’t be projected onto a mesh—for
example, a texture might be used as a stand-alone data table. The individual
picture elements of a texture are called texels to differentiate them from the
pixels on the screen.
The dimensions of a texture bitmap are constrained to be powers of two
on some graphics hardware. Typical texture dimensions include 256 × 256,
512 × 512, 1024 × 1024 and 2048 × 2048, although textures can be any size
on most hardware, provided the texture ﬁts into video memory. Some graph-
ics hardware imposes additional restrictions, such as requiring textures to be
square, or lifts some restrictions, such as not constraining texture dimensions
to be powers of two.
Types of Textures
The most common type of texture is known as a diffuse map, or albedo map. It
describes the diffuse surface color at each texel on a surface and acts like a
decal or paint job on the surface.
Other types of textures are used in computer graphics as well, including
normal maps (which store unit normal vectors at each texel, encoded as RGB
values), gloss maps (which encode how shiny a surface should be at each texel),
environment maps (which contain a picture of the surrounding environment
for rendering reﬂections) and many others. See Section 10.3.1 for a discussion
of how various types of textures can be used for image-based lighting and
other effects.

10.1. Foundations of Depth-Buffered Triangle Rasterization
463
We can actually use texture maps to store any information that we happen
to need in our lighting calculations. For example, a one-dimensional texture
could be used to store sampled values of a complex math function, a color-to-
color mapping table, or any other kind of look-up table (LUT).
Texture Coordinates
Let’s consider how to project a two-dimensional texture onto a mesh. To do
this, we deﬁne a two-dimensional coordinate system known as texture space.
A texture coordinate is usually represented by a normalized pair of numbers
denoted (u, v). These coordinates always range from (0, 0) at the bottom left
corner of the texture to (1, 1) at the top right. Using normalized coordinates
like this allows the same coordinate system to be used regardless of the di-
mensions of the texture.
To map a triangle onto a 2D texture, we simply specify a pair of texture
coordinates (ui, vi) at each vertex i. This effectively maps the triangle onto the
image plane in texture space. An example of texture mapping is depicted in
Figure 10.15.
Figure 10.15. An example of texture mapping. The triangles are shown both in three-dimensional
space and in texture space.
Texture Addressing Modes
Texture coordinates are permitted to extend beyond the [0, 1] range.
The
graphics hardware can handle out-of-range texture coordinates in any one of
the following ways. These are known as texture addressing modes; which mode
is used is under the control of the user.
•
Wrap. In this mode, the texture is repeated over and over in every direc-
tion. All texture coordinates of the form (ju, kv) are equivalent to the
coordinate (u, v), where j and k are arbitrary integers.

464
10. The Rendering Engine
Figure 10.16. Texture addressing modes.
•
Mirror. This mode acts like wrap mode, except that the texture is mir-
rored about the v-axis for odd integer multiples of u, and about the u-
axis for odd integer multiples of v.
•
Clamp. In this mode, the colors of the texels around the outer edge of the
texture are simply extended when texture coordinates fall outside the
normal range.
•
Border color. In this mode, an arbitrary user-speciﬁed color is used for
the region outside the [0, 1] texture coordinate range.
These texture addressing modes are depicted in Figure 10.16.
Texture Formats
Texture bitmaps can be stored on disk in virtually any image format, pro-
vided your game engine includes the code necessary to read it into mem-
ory. Common formats include Targa (.tga), Portable Network Graphics (.png),
Windows Bitmap (.bmp) and Tagged Image File Format (.tif). In memory, tex-
tures are usually represented as two-dimensional (strided) arrays of pixels us-
ing various color formats, including RGB888, RGBA8888, RGB565, RGBA5551
and so on.
Most modern graphics cards and graphics APIs support compressed tex-
tures. DirectX supports a family of compressed formats known as DXT or
S3 Texture Compression (S3TC). We won’t cover the details here, but the ba-
sic idea is to break the texture into 4 × 4 blocks of pixels and use a small

10.1. Foundations of Depth-Buffered Triangle Rasterization
465
color palette to store the colors for each block.
You can read more about
S3 compressed texture formats at http://en.wikipedia.org/wiki/S3_Texture_
Compression.
Compressed textures have the obvious beneﬁt of using less memory than
their uncompressed counterparts. An additional unexpected plus is that they
are faster to render with as well. S3 compressed textures achieve this speed-
up because of more cache-friendly memory access patterns—4 × 4 blocks of
adjacent pixels are stored in a single 64- or 128-bit machine word—and be-
cause more of the texture can ﬁt into the cache at once. Compressed textures
do suffer from compression artifacts. While the anomalies are usually not no-
ticeable, there are situations in which uncompressed textures must be used.
Texel Density and Mipmapping
Imagine rendering a full-screen quad (a rectangle composed of two triangles)
that has been mapped with a texture whose resolution exactly matches that
of the screen. In this case, each texel maps exactly to a single pixel on-screen,
and we say that the texel density (ratio of texels to pixels) is one. When this
same quad is viewed at a distance, its on-screen area becomes smaller. The
resolution of the texture hasn’t changed, so the quad’s texel density is now
greater than one (meaning that more than one texel is contributing to each
pixel).
Clearly texel density is not a ﬁxed quantity—it changes as a texture-map-
ped object moves relative to the camera. Texel density affects the memory
consumption and the visual quality of a three-dimensional scene. When the
texel density is much less than one, the texels become signiﬁcantly larger than
a pixel on-screen, and you can start to see the edges of the texels. This de-
stroys the illusion. When texel density is much greater than one, many texels
contribute to a single pixel on-screen. This can cause a moiré banding pattern, as
shown in Figure 10.17. Worse, a pixel’s color can appear to swim and ﬂicker as
different texels within the boundaries of the pixel dominate its color depend-
ing on subtle changes in camera angle or position. Rendering a distant object
with a very high texel density can also be a waste of memory if the player can
never get close to it. After all, why keep such a high-res texture in memory if
no one will ever see all that detail?
Ideally we’d like to maintain a texel density that is close to one at all times,
for both nearby and distant objects. This is impossible to achieve exactly, but it
can be approximated via a technique called mipmapping. For each texture, we
create a sequence of lower-resolution bitmaps, each of which is one-half the
width and one-half the height of its predecessor. We call each of these images
a mipmap, or mip level. For example, a 64×64 texture would have the following

466
10. The Rendering Engine
Figure 10.17. A texel density greater than one can lead to a moiré pattern.
mip levels: 64 × 64, 32 × 32, 16 × 16, 8 × 8, 4 × 4, 2 × 2 and 1 × 1, as shown in
Figure 10.18. Once we have mipmapped our textures, the graphics hardware
selects the appropriate mip level based on a triangle’s distance away from the
camera, in an attempt to maintain a texel density that is close to one. For
example, if a texture takes up an area of 40 × 40 on-screen, the 64 × 64 mip
level might be selected; if that same texture takes up only a 10 × 10 area, the
16 × 16 mip level might be used. As we’ll see below, trilinear ﬁltering allows
the hardware to sample two adjacent mip levels and blend the results. In this
case, a 10 × 10 area might be mapped by blending the 16 × 16 and 8 × 8 mip
levels together.
Figure 10.18. Mip levels for a 64 × 64 texture.
World-Space Texel Density
The term “texel density” can also be used to describe the ratio of texels to
world-space area on a textured surface. For example, a 2 m cube mapped
with a 256×256 texture would have a texel density of 2562/22 = 16,384. I will
call this world-space texel density to differentiate it from the screen-space texel
density we’ve been discussing thus far.

10.1. Foundations of Depth-Buffered Triangle Rasterization
467
World-space texel density need not be close to one, and in fact the speciﬁc
value will usually be much greater than one and depends entirely upon your
choice of world units. Nonetheless, it is important for objects to be texture
mapped with a reasonably consistent world-space texel density. For exam-
ple, we would expect all six sides of a cube to occupy the same texture area.
If this were not the case, the texture on one side of the cube would have a
lower-resolution appearance than another side, which can be noticeable to
the player. Many game studios provide their art teams with guidelines and
in-engine texel density visualization tools in an effort to ensure that all objects
in the game have a reasonably consistent world-space texel density.
Texture Filtering
When rendering a pixel of a textured triangle, the graphics hardware samples
the texture map by considering where the pixel center falls in texture space.
There is usually not a clean one-to-one mapping between texels and pixels,
and pixel centers can fall at any place in texture space, including directly on
the boundary between two or more texels. Therefore, the graphics hardware
usually has to sample more than one texel and blend the resulting colors to
arrive at the actual sampled texel color. We call this texture ﬁltering.
Most graphics cards support the following kinds of texture ﬁltering:
•
Nearest neighbor. In this crude approach, the texel whose center is closest
to the pixel center is selected. When mipmapping is enabled, the mip
level is selected whose resolution is nearest to but greater than the ideal
theoretical resolution needed to achieve a screen-space texel density of
one.
•
Bilinear. In this approach, the four texels surrounding the pixel center
are sampled, and the resulting color is a weighted average of their colors
(where the weights are based on the distances of the texel centers from
the pixel center). When mipmapping is enabled, the nearest mip level is
selected.
•
Trilinear. In this approach, bilinear ﬁltering is used on each of the two
nearest mip levels (one higher-res than the ideal and the other lower-
res), and these results are then linearly interpolated. This eliminates
abrupt visual boundaries between mip levels on-screen.
•
Anisotropic.
Both bilinear and trilinear ﬁltering sample 2 × 2 square
blocks of texels. This is the right thing to do when the textured sur-
face is being viewed head-on, but it’s incorrect when the surface is at an
oblique angle relative to the virtual screen plane. Anisotropic ﬁltering
samples texels within a trapezoidal region corresponding to the view
angle, thereby increasing the quality of textured surfaces when viewed
at an angle.

468
10. The Rendering Engine
10.1.2.6
Materials
A material is a complete description of the visual properties of a mesh. This
includes a speciﬁcation of the textures that are mapped to its surface and also
various higher-level properties, such as which shader programs to use when
rendering the mesh, the input parameters to those shaders and other parame-
ters that control the functionality of the graphics acceleration hardware itself.
While technically part of the surface properties description, vertex attri-
butes are not considered to be part of the material. However, they come along
for the ride with the mesh, so a mesh-material pair contains all the informa-
tion we need to render the object. Mesh-material pairs are sometimes called
render packets, and the term “geometric primitive” is sometimes extended to
encompass mesh-material pairs as well.
A 3D model typically uses more than one material. For example, a model
of a human would have separate materials for the hair, skin, eyes, teeth and
various kinds of clothing. For this reason, a mesh is usually divided into sub-
meshes, each mapped to a single material. The OGRE rendering engine imple-
ments this design via its Ogre::SubMesh class.
10.1.3
Lighting Basics
Lighting is at the heart of all CG rendering. Without good lighting, an other-
wise beautifully modeled scene will look ﬂat and artiﬁcial. Likewise, even the
Figure 10.19. A variation on the classic “Cornell box” scene illustrating how realistic lighting can
make even the simplest scene appear photorealistic.

10.1. Foundations of Depth-Buffered Triangle Rasterization
469
Figure 10.20. A scene from The Last of Us (© 2013/™SCEA. Created and developed by Naughty
Dog, PlayStation 3) rendered without textures. (See Color Plate XIV.)
simplest of scenes can be made to look extremely realistic when it is lit accu-
rately. The classic “Cornell box” scene, shown in Figure 10.19, is an excellent
example of this.
The sequence of screenshots from Naughty Dog’s The Last of Us is another
good illustration of the importance of lighting. In Figure 10.20, the scene is
rendered without textures. Figure 10.21 shows the same scene with diffuse
textures applied. The fully lit scene is shown in Figure 10.22. Notice the
marked jump in realism when lighting is applied to the scene.
Figure 10.21. The same scene from The Last of Us (© 2013/™SCEA. Created and developed by
Naughty Dog, PlayStation 3) with only diffuse textures applied. (See Color Plate XV.)

470
10. The Rendering Engine
Figure 10.22. Scene from The Last of Us (© 2013/™SCEA. Created and developed by Naughty Dog,
PlayStation 3) with full lighting. (See Color Plate XVI.)
The term shading is often used as a loose generalization of lighting plus
other visual effects. As such, “shading” encompasses procedural deformation
of vertices to simulate the motion of a water surface, generation of hair curves
or fur shells, tessellation of high-order surfaces, and pretty much any other
calculation that’s required to render a scene.
In the following sections, we’ll lay the foundations of lighting that we’ll
need in order to understand graphics hardware and the rendering pipeline.
We’ll return to the topic of lighting in Section 10.3, where we’ll survey some
advanced lighting and shading techniques.
10.1.3.1
Local and Global Illumination Models
Rendering engines use various mathematical models of light-surface and light-
volume interactions called light transport models. The simplest models only ac-
count for direct lighting in which light is emitted, bounces off a single object in
the scene, and then proceeds directly to the imaging plane of the virtual cam-
era. Such simple models are called local illumination models, because only the
local effects of light on a single object are considered; objects do not affect one
another’s appearance in a local lighting model. Not surprisingly, local mod-
els were the ﬁrst to be used in games, and they are still in use today—local
lighting can produce surprisingly realistic results in some circumstances.
True photorealism can only be achieved by accounting for indirect light-
ing, where light bounces multiple times off many surfaces before reaching the
virtual camera. Lighting models that account for indirect lighting are called
global illumination models. Some global illumination models are targeted at

10.1. Foundations of Depth-Buffered Triangle Rasterization
471
simulating one speciﬁc visual phenomenon, such as producing realistic shad-
ows, modeling reﬂective surfaces, accounting for interreﬂection between ob-
jects (where the color of one object affects the colors of surrounding objects),
and modeling caustic effects (the intense reﬂections from water or a shiny
metal surface). Other global illumination models attempt to provide a holis-
tic account of a wide range of optical phenomena. Ray tracing and radiosity
methods are examples of such technologies.
Global illumination is described completely by a mathematical formula-
tion known as the rendering equation or shading equation. It was introduced in
1986 by J. T. Kajiya as part of a seminal SIGGRAPH paper. In a sense, every
rendering technique can be thought of as a full or partial solution to the ren-
dering equation, although they differ in their fundamental approach to solv-
ing it and in the assumptions, simpliﬁcations and approximations they make.
See http://en.wikipedia.org/wiki/Rendering_equation, [8], [1] and virtually
any other text on advanced rendering and lighting for more details on the
rendering equation.
10.1.3.2
The Phong Lighting Model
The most common local lighting model employed by game rendering engines
is the Phong reﬂection model. It models the light reﬂected from a surface as a
sum of three distinct terms:
•
The ambient term models the overall lighting level of the scene. It is a
gross approximation of the amount of indirect bounced light present in
the scene. Indirect bounces are what cause regions in shadow not to
appear totally black.
•
The diffuse term accounts for light that is reﬂected uniformly in all direc-
tions from each direct light source. This is a good approximation to the
way in which real light bounces off a matte surface, such as a block of
wood or a piece of cloth.
•
The specular term models the bright highlights we sometimes see when
viewing a glossy surface.
Specular highlights occur when the view-
ing angle is closely aligned with a path of direct reﬂection from a light
source.
Figure 10.23 shows how the ambient, diffuse and specular terms add to-
gether to produce the ﬁnal intensity and color of a surface.
To calculate Phong reﬂection at a speciﬁc point on a surface, we require
a number of input parameters. The Phong model is normally applied to all
three color channels (R, G and B) independently, so all of the color parameters

472
10. The Rendering Engine
Figure 10.23. Ambient, diffuse and specular terms are summed to calculate Phong reﬂection.
in the following discussion are three-element vectors. The inputs to the Phong
model are:
•
the viewing direction vector V =

Vx
Vy
Vz

, which extends from the
reﬂection point to the virtual camera’s focal point (i.e., the negation of
the camera’s world-space “front” vector);
•
the ambient light intensity for the three color channels,
A
=
AR
AG
AB

;
•
the surface normal N =
Nx
Ny
Nz

at the point the light ray im-
pinges on the surface;
•
the surface reﬂectance properties, which are
◦
the ambient reﬂectivity kA =
kAR
kAG
kAB

,
◦
the diffuse reﬂectivity kD =
kDR
kDG
kDB

,
◦
the specular reﬂectivity kS =
kSR
kSG
kSB

,
◦
a specular “glossiness” exponent α;
•
and, for each light source i,
◦
the light’s color and intensity Ci =
CiR
CiG
CiB

,
◦
the direction vector Li from the reﬂection point to the light source.
In the Phong model, the intensity I of light reﬂected from a point can be ex-
pressed with the following vector equation:
I = (kA ⊗A) +
X
i
kD(N · Li) + kS(Ri · V)α
⊗Ci,
where the sum is taken over all lights i affecting the point in question. Recall
that the operator ⊗represents the component-wise multiplication of two vectors
(the so-called Hadamard product). This expression can be broken into three

10.1. Foundations of Depth-Buffered Triangle Rasterization
473
N
T
T
Figure 10.24. Calculation of the reﬂected lighting vector R from the original lighting vector L
and the surface normal N.
scalar equations, one for each color channel, as follows:
IR = kARAR +
X
i
kDR(N · Li) + kSR(Ri · V)α
CiR,
IG = kAGAG +
X
i
kDG(N · Li) + kSG(Ri · V)α
CiG,
IB = kABAB +
X
i
kDB(N · Li) + kSB(Ri · V)α
CiB.
In these equations, the vector Ri =
Rix
Riy
Riz

is the reﬂection of the
light ray’s direction vector Li about the surface normal N.
The vector Ri can be easily calculated via a bit of vector math (see Fig-
ure 10.24). Any vector can be expressed as a sum of its normal and tangential
components. For example, we can break up the light direction vector L as
follows:
L = LN + LT .
We know that the dot product (N · L) represents the projection of L normal to
the surface (a scalar quantity). So the normal component LN is just the unit
normal vector N scaled by this dot product:
LN = (N · L)N.
The reﬂected vector R has the same normal component as L but the opposite
tangential component (−LT ). So we can ﬁnd R as follows:
R = LN −LT
= LN −(L −LN)
= 2LN −L;
R = 2(N · L)N −LT

474
10. The Rendering Engine
This equation can be used to ﬁnd all of the Ri values corresponding to the
light directions Li.
Blinn-Phong
The Blinn-Phong lighting model is a variation on Phong shading that calcu-
lates specular reﬂection in a slightly different way. We deﬁne the vector H to
be the vector that lies halfway between the view vector V and the light di-
rection vector L. The Blinn-Phong specular component is then (N · H)a, as
opposed to Phong’s (R · V)α. The exponent a is slightly different than the
Phong exponent α, but its value is chosen in order to closely match the equiv-
alent Phong specular term.
The Blinn-Phong model offers increased runtime efﬁciency at the cost of
some accuracy, although it actually matches empirical results more closely
than Phong for some kinds of surfaces. The Blinn-Phong model was used al-
most exclusively in early computer games and was hard-wired into the ﬁxed-
function pipelines of early GPUs. See http://en.wikipedia.org/wiki/Blinn%
E2%80%93Phong_shading_model for more details.
BRDF Plots
The three terms in the Phong lighting model are special cases of a general local
reﬂection model known as a bidirectional reﬂection distribution function (BRDF).
A BRDF calculates the ratio of the outgoing (reﬂected) radiance along a given
viewing direction V to the incoming irradiance along the incident ray L.
A BRDF can be visualized as a hemispherical plot, where the radial dis-
tance from the origin represents the intensity of the light that would be seen if
the reﬂection point were viewed from that direction. The diffuse Phong reﬂec-
tion term is kD(N · L). This term only accounts for the incoming illumination
ray L, not the viewing angle V. Hence the value of this term is the same for
all viewing angles. If we were to plot this term as a function of the viewing
angle in three dimensions, it would look like a hemisphere centered on the
point at which we are calculating the Phong reﬂection. This is shown in two
dimensions in Figure 10.25.
The specular term of the Phong model is kD(R · V)α. This term is de-
pendent on both the illumination direction L and the viewing direction V. It
produces a specular “hot spot” when the viewing angle aligns closely with the
reﬂection R of the illumination direction L about the surface normal. How-
ever, its contribution falls off very quickly as the viewing angle diverges from
the reﬂected illumination direction. This is shown in two dimensions in Fig-
ure 10.26.

10.1. Foundations of Depth-Buffered Triangle Rasterization
475
1
2
Figure 10.25.
The diffuse term of the Phong reﬂection model is dependent upon N · L but is
independent of the viewing angle V.
Figure 10.26. The specular term of the Phong reﬂection model is at its maximum when the view-
ing angle V coincides with the reﬂected light direction R and drops off quickly as V diverges
from R.
10.1.3.3
Modeling Light Sources
In addition to modeling the light’s interactions with surfaces, we need to de-
scribe the sources of light in the scene. As with all things in real-time render-
ing, we approximate real-world light sources using various simpliﬁed models.
Static Lighting
The fastest lighting calculation is the one you don’t do at all. Lighting is there-
fore performed ofﬂine whenever possible. We can precalculate Phong reﬂec-
tion at the vertices of a mesh and store the results as diffuse vertex color at-
tributes. We can also precalculate lighting on a per-pixel basis and store the
results in a kind of texture map known as a light map. At runtime, the light
map texture is projected onto the objects in the scene in order to determine the
light’s effects on them.
You might wonder why we don’t just bake lighting information directly
into the diffuse textures in the scene. There are a few reasons for this. For
one thing, diffuse texture maps are often tiled and/or repeated throughout a

476
10. The Rendering Engine
scene, so baking lighting into them wouldn’t be practical. Instead, a single
light map is usually generated per light source and applied to any objects that
fall within that light’s area of inﬂuence. This approach permits dynamic ob-
jects to move past a light source and be properly illuminated by it. It also
means that our light maps can be of a different (often lower) resolution than
our diffuse texture maps. Finally, a “pure” light map usually compresses bet-
ter than one that includes diffuse color information.
Ambient Lights
An ambient light corresponds to the ambient term in the Phong lighting model.
This term is independent of the viewing angle and has no speciﬁc direction.
An ambient light is therefore represented by a single color, corresponding to
the A color term in the Phong equation (which is scaled by the surface’s am-
bient reﬂectivity kA at runtime). The intensity and color of ambient light may
vary from region to region within the game world.
Figure 10.27. Model
of a directional light
source.
Figure 10.28. Model
of
a
point
light
source.
Directional Lights
A directional light models a light source that is effectively an inﬁnite distance
away from the surface being illuminated—like the sun. The rays emanating
from a directional light are parallel, and the light itself does not have any
particular location in the game world. A directional light is therefore modeled
as a light color C and a direction vector L. A directional light is depicted in
Figure 10.27.
Point (Omnidirectional) Lights
A point light (omnidirectional light) has a distinct position in the game world
and radiates uniformly in all directions. The intensity of the light is usually
considered to fall off with the square of the distance from the light source,
and beyond a predeﬁned maximum radius its effects are simply clamped to
zero. A point light is modeled as a light position P, a source color/intensity C
and a maximum radius rmax. The rendering engine only applies the effects of a
point light to those surfaces that fall within is sphere of inﬂuence (a signiﬁcant
optimization). Figure 10.28 illustrates a point light.
Spot Lights
A spot light acts like a point light whose rays are restricted to a cone-shaped
region, like a ﬂashlight. Usually two cones are speciﬁed with an inner and
an outer angle. Within the inner cone, the light is considered to be at full in-

10.1. Foundations of Depth-Buffered Triangle Rasterization
477
tensity. The light intensity falls off as the angle increases from the inner to
the outer angle, and beyond the outer cone it is considered to be zero. Within
both cones, the light intensity also falls off with radial distance. A spot light is
modeled as a position P, a source color C, a central direction vector L, a max-
imum radius rmax and inner and outer cone angles θmin and θmax. Figure 10.29
illustrates a spot light source.
Figure 10.29. Model
of
a
spot
light
source.
Area Lights
All of the light sources we’ve discussed thus far radiate from an idealized
point, either at inﬁnity or locally. A real light source almost always has a
nonzero area—this is what gives rise to the umbra and penumbra in the shad-
ows it casts.
Rather than trying to model area lights explicitly, CG engineers often use
various “tricks” to account for their behavior.
For example to simulate a
penumbra, we might cast multiple shadows and blend the results, or we
might blur the edges of a sharp shadow in some manner.
Emissive Objects
Some surfaces in a scene are themselves light sources. Examples include ﬂash-
lights, glowing crystal balls, ﬂames from a rocket engine and so on. Glowing
surfaces can be modeled using an emissive texture map—a texture whose colors
are always at full intensity, independent of the surrounding lighting environ-
ment. Such a texture could be used to deﬁne a neon sign, a car’s headlights
and so on.
Some kinds of emissive objects are rendered by combining multiple tech-
niques. For example, a ﬂashlight might be rendered using an emissive texture
for when you’re looking head-on into the beam, a colocated spot light that
casts light into the scene, a yellow translucent mesh to simulate the light cone,
some camera-facing transparent cards to simulate lens ﬂare (or a bloom effect
if high dynamic range lighting is supported by the engine), and a projected
texture to produce the caustic effect that a ﬂashlight has on the surfaces it il-
luminates. The ﬂashlight in Luigi’s Mansion is a great example of this kind of
effect combination, as shown in Figure 10.30.
10.1.4
The Virtual Camera
In computer graphics, the virtual camera is much simpler than a real cam-
era or the human eye. We treat the camera as an ideal focal point with a
rectangular virtual sensing surface called the imaging rectangle ﬂoating some

478
10. The Rendering Engine
Figure 10.30. The ﬂashlight in Luigi’s Mansion by Nintendo (Wii) is composed of numerous visual
effects, including a cone of translucent geometry for the beam, a dynamic spot light to cast light
into the scene, an emissive texture on the lens and camera-facing cards for the lens ﬂare. (See
Color Plate XVII.)
small distance in front of it. The imaging rectangle consists of a grid of square
or rectangular virtual light sensors, each corresponding to a single pixel on-
screen. Rendering can be thought of as the process of determining what color
and intensity of light would be recorded by each of these virtual sensors.
10.1.4.1
View Space
The focal point of the virtual camera is the origin of a 3D coordinate system
known as view space or camera space. The camera usually “looks” down the
positive or negative z-axis in view space, with y up and x to the left or right.
Typical left- and right-handed view-space axes are illustrated in Figure 10.31.
The camera’s position and orientation can be speciﬁed using a view-to-
world matrix, just as a mesh instance is located in the scene with its model-to-
world matrix. If we know the position vector and three unit basis vectors of
camera space, expressed in world-space coordinates, the view-to-world ma-
trix can be written as follows, in a manner analogous to that used to construct
a model-to-world matrix:
MV →W =


iV
0
jV
0
kV
0
tV
1

.
When rendering a triangle mesh, its vertices are transformed ﬁrst from
model space to world space, and then from world space to view space. To
perform this latter transformation, we need the world-to-view matrix, which
is the inverse of the view-to-world matrix. This matrix is sometimes called the
view matrix:
MW →V = M−1
V →W = Mview.

10.1. Foundations of Depth-Buffered Triangle Rasterization
479
Left-Handed
Right-Handed
Virtual
Screen
Virtual
Screen
Frustum
Frustum
xC
zC
yC
xC
zC
yC
Figure 10.31. Left- and right-handed camera-space axes.
Be careful here. The fact that the camera’s matrix is inverted relative to the
matrices of the objects in the scene is a common point of confusion and bugs
among new game developers.
The world-to-view matrix is often concatenated to the model-to-world ma-
trix prior to rendering a particular mesh instance. This combined matrix is
called the model-view matrix in OpenGL. We precalculate this matrix so that
the rendering engine only needs to do a single matrix multiply when trans-
forming vertices from model space into view space:
MM→V = MM→W MW →V = Mmodelview.
10.1.4.2
Projections
In order to render a 3D scene onto a 2D image plane, we use a special kind
of transformation known as a projection. The perspective projection is the most
common projection in computer graphics, because it mimics the kinds of im-
ages produced by a typical camera. With this projection, objects appear smaller
the farther away they are from the camera—an effect known as perspective fore-
shortening.
The length-preserving orthographic projection is also used by some games,
primarily for rendering plan views (e.g., front, side and top) of 3D models or
game levels for editing purposes, and for overlaying 2D graphics onto the
screen for heads-up displays and the like. Figure 10.32 illustrates how a cube
would look when rendered with these two types of projections.

480
10. The Rendering Engine
Figure 10.32. A cube rendered using a perspective projection (on the left) and an orthographic
projection (on the right).
10.1.4.3
The View Volume and the Frustum
The region of space that the camera can “see” is known as the view volume.
A view volume is deﬁned by six planes. The near plane corresponds to the
virtual image-sensing surface. The four side planes correspond to the edges of
the virtual screen. The far plane is used as a rendering optimization to ensure
that extremely distant objects are not drawn. It also provides an upper limit
for the depths that will be stored in the depth buffer (see Section 10.1.4.8).
When rendering the scene with a perspective projection, the shape of the
view volume is a truncated pyramid known as a frustum. When using an
orthographic projection, the view volume is a rectangular prism. Perspec-
tive and orthographic view volumes are illustrated in Figure 10.33 and Fig-
ure 10.34, respectively.
The six planes of the view volume can be represented compactly using
six four-element vectors (nix, niy, niz, di), where n = (nx, ny, nz) is the plane
normal and d is its perpendicular distance from the origin. If we prefer the
Far
Plane
yV
Near 
Plane
xV
zV
(r, b, n)
(r, b, f)
(r, t, f)
(l, t, f)
(l, b, n)
(l, t, n)
(l, b, f)
(r, t, n)
Figure 10.33. A perspective view volume (frustum).

10.1. Foundations of Depth-Buffered Triangle Rasterization
481
Far
Plane
yV
Near
Plane
xV
zV
(r, b, n)
(r, b, f)
(r, t, f)
(l, t, f)
(l, b, n)
(l, t, n)
(l, b, f)
(r, t, n)
Figure 10.34. An orthographic view volume.
point-normal plane representation, we can also describe the planes with six
pairs of vectors (Qi, ni), where Q is the arbitrary point on the plane and n is
the plane normal. (In both cases, i is an index representing the six planes.)
10.1.4.4
Projection and Homogeneous Clip Space
Both perspective and orthographic projections transform points in view space
into a coordinate space called homogeneous clip space. This three-dimensional
space is really just a warped version of view space. The purpose of clip space
is to convert the camera-space view volume into a canonical view volume that
is independent both of the kind of projection used to convert the 3D scene into
2D screen space, and of the resolution and aspect ratio of the screen onto which
the scene is going to be rendered.
In clip space, the canonical view volume is a rectangular prism extending
from −1 to +1 along the x- and y-axes. Along the z-axis, the view volume ex-
tends either from −1 to +1 (OpenGL) or from 0 to 1 (DirectX). We call this co-
ordinate system “clip space” because the view volume planes are axis-aligned,
making it convenient to clip triangles to the view volume in this space (even
when a perspective projection is being used). The canonical clip-space view
volume for OpenGL is depicted in Figure 10.35. Notice that the z-axis of clip
space goes into the screen, with y up and x to the right. In other words, ho-
mogeneous clip space is usually left-handed. A left-handed convention is used
here because it causes increasing z values to correspond to increasing depth
into the screen, with y increasing up and x increasing to the right as usual.

482
10. The Rendering Engine
Far
Plane
yH
Near
Plane
xH
zH
(1, –1, –1)
(1, –1, 1)
(1, 1, 1)
(–1, 1, 1)
(–1, –1, –1)
(–1, 1, –1)
Figure 10.35. The canonical view volume in homogeneous clip space.
Perspective Projection
An excellent explanation of perspective projection is given in Section 4.5.1 of
[28], so we won’t repeat it here. Instead, we’ll simply present the perspective
projection matrix MV →H below. (The subscript V →H indicates that this
matrix transforms vertices from view space into homogeneous clip space.) If
we take view space to be right-handed, then the near plane intersects the z-
axis at z = −n, and the far plane intersects it at z = −f. The virtual screen’s
left, right, bottom, and top edges lie at x = l, x = r, y = b and y = t on
the near plane, respectively. (Typically the virtual screen is centered on the
camera-space z-axis, in which case l = −r and b = −t, but this isn’t always the
case.) Using these deﬁnitions, the perspective projection matrix for OpenGL
is as follows:
MV →H =


 2n
r −l

0
0
0
0
 2n
t −b

0
0
r + l
r −l

t + b
t −b


−f + n
f −n

−1
0
0

−2nf
f −n

0


.

10.1. Foundations of Depth-Buffered Triangle Rasterization
483
DirectX deﬁnes the z-axis extents of the clip-space view volume to lie in
the range [0, 1] rather than in the range [−1, 1] as OpenGL does. We can easily
adjust the perspective projection matrix to account for DirectX’s conventions
as follows:
(MV →H)DirectX =


 2n
r −l

0
0
0
0
 2n
t −b

0
0
r + l
r −l

t + b
t −b


−
f
f −n

−1
0
0

−nf
f −n

0


.
Division by z
Perspective projection results in each vertex’s x- and y-coordinates being di-
vided by its z-coordinate. This is what produces perspective foreshortening.
To understand why this happens, consider multiplying a view-space point pV
expressed in four-element homogeneous coordinates by the OpenGL perspec-
tive projection matrix:
pH = pV MV →H
=
pV x
pV y
pV z
1


 2n
r −l

0
0
0
0
 2n
t −b

0
0
r + l
r −l

t + b
t −b


−f + n
f −n

−1
0
0

−2nf
f −n

0


.
The result of this multiplication takes the form
pH =
a
b
c
−pV z

.
(10.1)
When we convert any homogeneous vector into three-dimensional coor-
dinates, the x-, y- and z-components are divided by the w-component:
x
y
z
w
≡
h x
w
y
w
z
w
i
.
So, after dividing Equation (10.1) by the homogeneous w-component, which
is really just the negative view-space z-coordinate −pV z, we have:
pH =

a
−pV z
b
−pV z
c
−pV z

=

pHx
pHy
pHz

.

484
10. The Rendering Engine
Thus, the homogeneous clip-space coordinates have been divided by the view-
space z-coordinate, which is what causes perspective foreshortening.
Perspective-Correct Vertex Attribute Interpolation
In Section 10.1.2.4, we learned that vertex attributes are interpolated in order
to determine appropriate values for them within the interior of a triangle. At-
tribute interpolation is performed in screen space. We iterate over each pixel
of the screen and attempt to determine the value of each attribute at the cor-
responding location on the surface of the triangle. When rendering a scene with
a perspective projection, we must do this very carefully so as to account for
perspective foreshortening. This is known as perspective-correct attribute inter-
polation.
A derivation of perspective-correct interpolation is beyond our scope, but
sufﬁce it to say that we must divide our interpolated attribute values by the
corresponding z-coordinates (depths) at each vertex. For any pair of vertex
attributes A1 and A2, we can write the interpolated attribute at a percentage t
of the distance between them as follows:
A
pz
= (1 −t)
 A1
p1z

+ t
 A2
p2z

= LERP
 A1
p1z
, A2
p2z
, t

.
Refer to [28] for an excellent derivation of the math behind perspective-correct
attribute interpolation.
Orthographic Projection
An orthographic projection is performed by the following matrix:
(MV →H)ortho =



2
r −l

0
0
0
0

2
t −b

0
0
0
0

−
2
f −n

0

−r + l
r −l


−t + b
t −b


−f + n
f −n

1


.
This is just an everyday scale-and-translate matrix. (The upper-left 3 × 3
contains a diagonal nonuniform scaling matrix, and the lower row contains
the translation.) Since the view volume is a rectangular prism in both view
space and clip space, we need only scale and translate our vertices to convert
from one space to the other.

10.1. Foundations of Depth-Buffered Triangle Rasterization
485
xS
4:3
yS
xS
16:9
yS
Figure 10.36. The two most prevalent screen-space aspect ratios are 4:3 and 16:9.
10.1.4.5
Screen Space and Aspect Ratios
Screen space is a two-dimensional coordinate system whose axes are mea-
sured in terms of screen pixels. The x-axis typically points to the right, with
the origin at the top-left corner of the screen and y pointing down. (The rea-
son for the inverted y-axis is that CRT monitors scan the screen from top to
bottom.) The ratio of screen width to screen height is known as the aspect ratio.
The most common aspect ratios are 4:3 (the aspect ratio of a traditional tele-
vision screen) and 16:9 (the aspect ratio of a movie screen or HDTV). These
aspect ratios are illustrated in Figure 10.36.
We can render triangles expressed in homogeneous clip space by simply
drawing their (x, y) coordinates and ignoring z. But before we do, we scale
and shift the clip-space coordinates so that they lie in screen space rather than
within the normalized unit square. This scale-and-shift operation is known as
screen mapping.
10.1.4.6
The Frame Buffer
The ﬁnal rendered image is stored in a bitmapped color buffer known as the
frame buffer. Pixel colors are usually stored in RGBA8888 format, although
other frame buffer formats are supported by most graphics cards as well.
Some common formats include RGB565, RGB5551, and one or more paletted
modes.
The display hardware (CRT, ﬂat-screen monitor, HDTV, etc.) reads the con-
tents of the frame buffer at a periodic rate of 60 Hz for NTSC televisions used
in North America and Japan, or 50 Hz for PAL/SECAM televisions used in
Europe and many other places in the world. Rendering engines typically
maintain at least two frame buffers. While one is being scanned by the dis-
play hardware, the other one can be updated by the rendering engine. This is
known as double buffering. By swapping or “ﬂipping” the two buffers during
the vertical blanking interval (the period during which the CRT’s electron gun is
being reset to the top-left corner of the screen), double buffering ensures that
the display hardware always scans the complete frame buffer. This avoids a

486
10. The Rendering Engine
jarring effect known as tearing, in which the upper portion of the screen dis-
plays the newly rendered image while the bottom shows the remnants of the
previous frame’s image.
Some engines make use of three frame buffers—a technique aptly known
as triple buffering. This is done so that the rendering engine can start work on
the next frame, even while the previous frame is still being scanned by the
display hardware. For example, the hardware might still be scanning buffer
A when the engine ﬁnishes drawing buffer B. With triple buffering, it can
proceed to render a new frame into buffer C, rather than idling while it waits
for the display hardware to ﬁnish scanning buffer A.
Render Targets
Any buffer into which the rendering engine draws graphics is known as a
render target. As we’ll see later in this chapter, rendering engines make use
of all sorts of other off-screen render targets in addition to the frame buffers.
These include the depth buffer, the stencil buffer and various other buffers
used for storing intermediate rendering results.
10.1.4.7
Triangle Rasterization and Fragments
To produce an image of a triangle on-screen, we need to ﬁll in the pixels it
overlaps. This process is known as rasterization. During rasterization, the
triangle’s surface is broken into pieces called fragments, each one representing
a small region of the triangle’s surface that corresponds to a single pixel on
the screen. (In the case of multisample antialiasing, a fragment corresponds
to a portion of a pixel—see below.)
A fragment is like a pixel in training. Before it is written into the frame
buffer, it must pass a number of tests (described in more depth below). If
it fails any of these tests, it will be discarded. Fragments that pass the tests
are shaded (i.e., their colors are determined), and the fragment color is either
written into the frame buffer or blended with the pixel color that’s already
there. Figure 10.37 illustrates how a fragment becomes a pixel.
10.1.4.8
Occlusion and the Depth Buffer
When rendering two triangles that overlap each other in screen space, we
need some way of ensuring that the triangle that is closer to the camera will
appear on top. We could accomplish this by always rendering our triangles in
back-to-front order (the so-called painter’s algorithm). However, as shown in
Figure 10.38, this doesn’t work if the triangles are intersecting one another.

10.1. Foundations of Depth-Buffered Triangle Rasterization
487
Fragment
Pixel
Figure 10.37. A fragment is a small region of a triangle corresponding to a pixel on the screen. It
passes through the rendering pipeline and is either discarded or its color is written into the frame
buffer.
To implement triangle occlusion properly, independent of the order in
which the triangles are rendered, rendering engines use a technique known
as depth buffering or z-buffering. The depth buffer is a full-screen buffer that
typically contains 24-bit integer or (more rarely) ﬂoating-point depth informa-
tion for each pixel in the frame buffer. (The depth buffer is usually stored in
a 32-bits-per-pixel format, with a 24-bit depth value and an 8-bit stencil value
packed into each pixel’s 32-bit quadword.) Every fragment has a z-coordinate
that measures its depth “into” the screen. (The depth of a fragment is found
by interpolating the depths of the triangle’s vertices.) When a fragment’s color
is written into the frame buffer, its depth is stored into the corresponding pixel
of the depth buffer. When another fragment (from another triangle) is drawn
into the same pixel, the engine compares the new fragment’s depth to the
Figure 10.38.
The painter’s algorithm renders triangles in a back-to-front order to produce
proper triangle occlusion. However, the algorithm breaks down when triangles intersect one
another.

488
10. The Rendering Engine
depth already present in the depth buffer. If the fragment is closer to the cam-
era (i.e., if it has a smaller depth), it overwrites the pixel in the frame buffer.
Otherwise the fragment is discarded.
z-Fighting and the w-Buffer
When rendering parallel surfaces that are very close to one another, it’s im-
portant that the rendering engine can distinguish between the depths of the
two planes. If our depth buffer had inﬁnite precision, this would never be
a problem. Unfortunately, a real depth buffer only has limited precision, so
the depth values of two planes can collapse into a single discrete value when
the planes are close enough together. When this happens, the more-distant
plane’s pixels start to “poke through” the nearer plane, resulting in a noisy
effect known as z-ﬁghting.
To reduce z-ﬁghting to a minimum across the entire scene, we would like
to have equal precision whether we’re rendering surfaces that are close to
the camera or far away. However, with z-buffering this is not the case. The
precision of clip-space z-depths (pHz) are not evenly distributed across the
entire range from the near plane to the far plane, because of the division by
the view-space z-coordinate. Because of the shape of the 1/z curve, most of
the depth buffer’s precision is concentrated near the camera.
The plot of the function pHz = 1/pVz shown in Figure 10.39 demonstrates
this effect. Near the camera, the distance between two planes in view space
∆pVz gets transformed into a reasonably large delta in clip space ∆pHz. But
far from the camera, this same separation gets transformed into a tiny delta in
clip space. The result is z-ﬁghting, and it becomes rapidly more prevalent as
objects get farther away from the camera.
To circumvent this problem, we would like to store view-space z-coordinates
(pVz) in the depth buffer instead of clip-space z-coordinates (pHz).
View-
space z-coordinates vary linearly with the distance from the camera, so us-
ΔpHz
ΔpVz
ΔpVz
ΔpHz
pHz = 1/pVz
pHz = 1/pVz
Figure 10.39. A plot of the function 1/pVz, showing how most of the precision lies close to the
camera.

10.2. The Rendering Pipeline
489
ing them as our depth measure achieves uniform precision across the entire
depth range. This technique is called w-buffering, because the view-space z-
coordinate conveniently appears in the w-component of our homogeneous
clip-space coordinates. (Recall from Equation (10.1) that pHw = −pVz.)
The terminology can be a very confusing here. The z- and w-buffers store
coordinates that are expressed in clip space. But in terms of view-space coordi-
nates, the z-buffer stores 1/z (i.e., 1/pVz) while the w-buffer stores z (i.e., pVz)!
We should note here that the w-buffering approach is a bit more expen-
sive than its z-based counterpart. This is because with w-buffering, we cannot
linearly interpolate depths directly. Depths must be inverted prior to interpo-
lation and then re-inverted prior to being stored in the w-buffer.
10.2
The Rendering Pipeline
Now that we’ve completed our whirlwind tour of the major theoretical and
practical underpinnings of triangle rasterization, let’s turn our attention to
how it is typically implemented. In real-time game rendering engines, the
high-level rendering steps described in Section 10.1 are implemented using a
software/hardware architecture known as a pipeline. A pipeline is just an or-
dered chain of computational stages, each with a speciﬁc purpose, operating
on a stream of input data items and producing a stream of output data.
Each stage of a pipeline can typically operate independently of the other
stages. Hence, one of the biggest advantages of a pipelined architecture is that
it lends itself extremely well to parallelization. While the ﬁrst stage is chewing
on one data element, the second stage can be processing the results previously
produced by the ﬁrst stage, and so on down the chain.
Parallelization can also be achieved within an individual stage of the
pipeline. For example, if the computing hardware for a particular stage is
duplicated N times on the die, N data elements can be processed in parallel
by that stage. A parallelized pipeline is shown in Figure 10.40. Ideally the
stages operate in parallel (most of the time), and certain stages are capable of
operating on multiple data items simultaneously as well.
The throughput of a pipeline measures how many data items are processed
per second overall. The pipeline’s latency measures the amount of time it
takes for a single data element to make it through the entire pipeline. The
latency of an individual stage measures how long that stage takes to process
a single item. The slowest stage of a pipeline dictates the throughput of the
entire pipeline. It also has an impact on the average latency of the pipeline
as a whole. Therefore, when designing a rendering pipeline, we attempt to

490
10. The Rendering Engine
Figure 10.40. A parallelized pipeline. The stages all operate in parallel, and some stages are capable
of operating on multiple data items simultaneously as well.
minimize and balance latency across the entire pipeline and eliminate bottle-
necks. In a well-designed pipeline, all the stages operate simultaneously, and
no stage is ever idle for very long waiting for another stage to become free.
10.2.1
Overview of the Rendering Pipeline
Some graphics texts divide the rendering pipeline into three coarse-grained
stages. In this book, we’ll extend this pipeline back even further, to encompass
the ofﬂine tools used to create the scenes that are ultimately rendered by the
game engine. The high-level stages in our pipeline are:
•
Tools stage (ofﬂine). Geometry and surface properties (materials) are de-
ﬁned.
•
Asset conditioning stage (ofﬂine). The geometry and material data are pro-
cessed by the asset conditioning pipeline (ACP) into an engine-ready
format.
•
Application stage (CPU). Potentially visible mesh instances are identiﬁed
and submitted to the graphics hardware along with their materials for
rendering.
•
Geometry processing stage (GPU). Vertices are transformed and lit and pro-
jected into homogeneous clip space. Triangles are processed by the op-
tional geometry shader and then clipped to the frustum.
•
Rasterization stage (GPU). Triangles are converted into fragments that are
shaded, passed through various tests (z-test, alpha test, stencil test, etc.)
and ﬁnally blended into the frame buffer.

10.2. The Rendering Pipeline
491
Tools
ACP
Application
Geometry 
Processing
Vertice
Vertices
Mesh
Instance
Submeshes
Textures
Materials
Textures
Mesh
Materials
Materials
Textures
Rasterization
Vertice
Fragments
Vertice
Pixels
Vertice
Triangles
Figure 10.41. The format of geometric data changes radically as it passes through the various
stages of the rendering pipeline.
10.2.1.1
How the Rendering Pipeline Transforms Data
It’s interesting to note how the format of geometry data changes as it passes
through the rendering pipeline. The tools and asset conditioning stages deal
with meshes and materials. The application stage deals in terms of mesh in-
stances and submeshes, each of which is associated with a single material.
During the geometry stage, each submesh is broken down into individual
vertices, which are processed largely in parallel. At the conclusion of this
stage, the triangles are reconstructed from the fully transformed and shaded
vertices. In the rasterization stage, each triangle is broken into fragments, and
these fragments are either discarded, or they are eventually written into the
frame buffer as colors. This process is illustrated in Figure 10.41.
10.2.1.2
Implementation of the Pipeline
The ﬁrst two stages of the rendering pipeline are implemented ofﬂine, usually
executed by a Windows or Linux machine. The application stage is run either
by the main CPU of the game console or PC, or by parallel processing units
like the PS3’s SPUs. The geometry and rasterization stages are usually imple-
mented on the graphics processing unit (GPU). In the following sections, we’ll
explore some of the details of how each of these stages is implemented.
10.2.2
The Tools Stage
In the tools stage, meshes are authored by 3D modelers in a digital content
creation (DCC) application like Maya, 3ds Max, Lightwave, Softimage/XSI,

492
10. The Rendering Engine
SketchUp, etc.
The models may be deﬁned using any convenient surface
description—NURBS, quads, triangles, etc. However, they are invariably tes-
sellated into triangles prior to rendering by the runtime portion of the pipeline.
The vertices of a mesh may also be skinned. This involves associating each
vertex with one or more joints in an articulated skeletal structure, along with
weights describing each joint’s relative inﬂuence over the vertex. Skinning
information and the skeleton are used by the animation system to drive the
movements of a model—see Chapter 11 for more details.
Materials are also deﬁned by the artists during the tools stage. This in-
volves selecting a shader for each material, selecting textures as required by
the shader, and specifying the conﬁguration parameters and options of each
shader. Textures are mapped onto the surfaces, and other vertex attributes are
also deﬁned, often by “painting” them with some kind of intuitive tool within
the DCC application.
Materials are usually authored using a commercial or custom in-house ma-
terial editor. The material editor is sometimes integrated directly into the DCC
application as a plug-in, or it may be a stand-alone program. Some material
editors are live-linked to the game, so that material authors can see what the
materials will look like in the real game. Other editors provide an ofﬂine 3D
visualization view. Some editors even allow shader programs to be written
and debugged by the artist or a shader engineer. NVIDIA’s Fx Composer is
an example of such a tool; it is depicted in Figure 10.42.
The Unreal Engine provides a powerful graphical shading language. Such
tools allow rapid prototyping of visual effects by connecting various kinds of
nodes together with a mouse. These tools generally provide a WYSIWYG dis-
play of the resulting material. The shaders created by a graphical language
usually need to be hand-optimized by a rendering engineer, because a graph-
ical language invariably trades some runtime performance for its incredible
ﬂexibility, generality and ease of use. The Unreal graphical shader editor is
shown in Figure 10.43.
Materials may be stored and managed with the individual meshes. How-
ever, this can lead to duplication of data—and effort. In many games, a rela-
tively small number of materials can be used to deﬁne a wide range of objects
in the game. For example, we might deﬁne some standard, reusable materials
like wood, rock, metal, plastic, cloth, skin and so on. There’s no reason to du-
plicate these materials inside every mesh. Instead, many game teams build up
a library of materials from which to choose, and the individual meshes refer
to the materials in a loosely coupled manner.

10.2. The Rendering Pipeline
493
Figure 10.42.
Nvidia’s Fx Composer allows shader programs to be written, previsualized and
debugged easily.
Figure 10.43. The Unreal Engine 4 graphical shader language.
10.2.3
The Asset Conditioning Stage
The asset conditioning stage is itself a pipeline, sometimes called the asset
conditioning pipeline (ACP) or the tools pipeline. As we saw in Section 6.2.1.4,

494
10. The Rendering Engine
its job is to export, process and link together multiple types of assets into a
cohesive whole. For example, a 3D model is comprised of geometry (vertex
and index buffers), materials, textures and an optional skeleton. The ACP
ensures that all of the individual assets referenced by a 3D model are available
and ready to be loaded by the engine.
Geometric and material data is extracted from the DCC application and
is usually stored in a platform-independent intermediate format. The data is
then further processed into one or more platform-speciﬁc formats, depend-
ing on how many target platforms the engine supports. Ideally the platform-
speciﬁc assets produced by this stage are ready to load into memory and use
with little or no postprocessing at runtime. For example, mesh data targeted
for the Xbox 360 might be output as index and vertex buffers that are ready
to be uploaded to video RAM; on the PS3, geometry might be produced in
compressed data streams that are ready to be DMA’d to the SPUs for decom-
pression. The ACP often takes the needs of the material/shader into account
when building assets. For example, a particular shader might require tangent
and bitangent vectors as well as a vertex normal; the ACP could generate
these vectors automatically.
High-level scene graph data structures may also be computed during the as-
set conditioning stage. For example, static-level geometry may be processed
in order to build a BSP tree. (As we’ll investigate in Section 10.2.8.4, scene
graph data structures help the rendering engine to very quickly determine
which objects should be rendered, given a particular camera position and ori-
entation.)
Expensive lighting calculations are often done ofﬂine as part of the asset
conditioning stage. This is called static lighting; it may include calculation
of light colors at the vertices of a mesh (this is called “baked” vertex light-
ing), construction of texture maps that encode per-pixel lighting information
known as light maps, calculation of precomputed radiance transfer (PRT) coefﬁ-
cients (usually represented by spherical harmonic functions) and so on.
10.2.4
A Brief History of the GPU
In the early days of game development, all rendering was done on the CPU.
Games like Castle Wolfenstein 3D and Doom pushed the limits of what early
PCs could do, rendering interactive 3D scenes without any help from special-
ized graphics hardware (other than a standard VGA card).
As the popularity of these and other PC games took off, graphics hardware
was developed to ofﬂoad work from the CPU. The earliest graphics accelera-

10.2. The Rendering Pipeline
495
tors, like 3Dfx’s Voodoo line of cards, handled only the most expensive stage
in the pipeline—the rasterization stage. Subsequent graphics accelerators pro-
vided support for the geometry processing stage as well.
At ﬁrst, graphics hardware provided only a hard-wired but conﬁgurable
implementation known as the ﬁxed-function pipeline.
This technology was
known as hardware transformation and lighting, or hardware T&L for short. Later,
certain substages of the pipeline were made programmable. Engineers could
now write programs called shaders to control exactly how the pipeline pro-
cessed vertices (vertex shaders) and fragments (fragment shaders, more com-
monly known as pixel shaders). With the introduction of DirectX 10, a third
type of shader known as a geometry shader was added. It permits rendering en-
gineers to modify, cull, or create entire primitives (triangles, lines and points).
Graphics hardware has evolved around a specialized type of micropro-
cessor known as the graphics processing unit or GPU. A GPU is designed to
maximize throughput of the pipeline, which it achieves through massive par-
allelization. For example, a modern GPU like the AMD Radeon 7870 can per-
form a staggering 1,152 calculations simultaneously.
Even in its fully programmable form, a GPU is not quite a general-purpose
microprocessor—nor should it be. A GPU achieves its high processing speeds
(on the order of teraﬂops on today’s GPUs) by utilizing hundreds or even
thousands of parallel arithmetic units to process streams of data.
Certain
pipeline stages are either entirely ﬁxed in their function, or they are conﬁg-
urable but not programmable. Memory can only be accessed in controlled
ways, and specialized data caches are used to minimize unnecessary duplica-
tion of computations.
Although GPUs are not architected in the same way as a CPU, they are
deﬁnitely evolving into general-purpose high-performance compute engines.
Programming a GPU to perform non-graphics-related tasks is known as
general-purpose GPU computing, or GPGPU for short. In GPGPU program-
ming, non-graphics computations are performed by special shaders known
as compute shaders (CS). Effective utilization of GPU bandwidth via compute
shaders is one of the keys to achieving high performance on both the PS4
and the Xbox One.
See http://en.wikipedia.org/wiki/General-purpose_
computing_on_graphics_processing_units for more information on this fas-
cinating topic.
In the following sections, we’ll brieﬂy explore the architecture of a modern
GPU and see how the runtime portion of the rendering pipeline is typically
implemented. We’ll speak primarily about current GPU architectures, which
are used on personal computers with the latest graphics cards and on console
platforms like the Xbox One and the PS4. However, not all platforms sup-

496
10. The Rendering Engine
Configurable
Fixed-Function
Programmable
Primitive
Assembly
Geometry
Shader
Clipping
Screen
Mapping
Triangle
Setup
Triangle 
Traversal
Early 
Z Test
Pixel
Shader
Merge
/ ROP
Stream
Output
Vertex 
Shader
Frame
Buffer
Figure 10.44. The geometry processing and rasterization stages of the rendering pipeline, as implemented by a typical GPU.
The white stages are programmable, the light grey stages are conﬁgurable, and the dark grey boxes are ﬁxed-function.
port all of the features we’ll be discussing here. For example, the Wii does
not support programmable shaders, and most PC games need to support fall-
back rendering solutions to support older graphics cards with only limited
programmable shader support.
10.2.5
The GPU Pipeline
Virtually all GPUs break the pipeline into the substages described below and
depicted in Figure 10.44. Each stage is shaded to indicate whether its function-
ality is programmable, ﬁxed but conﬁgurable, or ﬁxed and non-conﬁgurable.
10.2.5.1
Vertex Shader
This stage is fully programmable. It is responsible for transformation and
shading/lighting of individual vertices. The input to this stage is a single ver-
tex (although in practice many vertices are processed in parallel). Its position
and normal are typically expressed in model space or world space. The vertex
shader handles transformation from model space to view space via the model-
view transform. Perspective projection is also applied, as well as per-vertex
lighting and texturing calculations, and skinning for animated characters. The
vertex shader can also perform procedural animation by modifying the posi-
tion of the vertex. Examples of this include foliage that sways in the breeze or
an undulating water surface. The output of this stage is a fully transformed
and lit vertex, whose position and normal are expressed in homogeneous clip
space (see Section 10.1.4.4).
On modern GPUs, the vertex shader has full access to texture data—a ca-
pability that used to be available only to the pixel shader. This is particularly
useful when textures are used as stand-alone data structures like heightmaps
or look-up tables.

10.2. The Rendering Pipeline
497
10.2.5.2
Geometry Shader
This optional stage is also fully programmable. The geometry shader operates
on entire primitives (triangles, lines and points) in homogeneous clip space.
It is capable of culling or modifying input primitives, and it can also generate
new primitives. Typical uses include shadow volume extrusion (see Section
10.3.3.1), rendering the six faces of a cube map (see Section 10.3.1.4), fur ﬁn
extrusion around silhouette edges of meshes, creation of particle quads from
point data (see Section 10.4.1), dynamic tessellation, fractal subdivision of line
segments for lightning effects, cloth simulations, and the list goes on.
10.2.5.3
Stream Output
Some GPUs permit the data that has been processed up to this point in the
pipeline to be written back to memory. From there, it can then be looped back
to the top of the pipeline for further processing. This feature is called stream
output.
Stream output permits a number of intriguing visual effects to be achieved
without the aid of the CPU. An excellent example is hair rendering. Hair is
often represented as a collection of cubic spline curves. It used to be that hair
physics simulation would be done on the CPU. The CPU would also tessellate
the splines into line segments. Finally the GPU would render the segments.
With stream output, the GPU can do the physics simulation on the con-
trol points of the hair splines within the vertex shader. The geometry shader
tessellates the splines, and the stream output feature is used to write the tes-
sellated vertex data to memory. The line segments are then piped back into
the top of the pipeline so they can be rendered.
10.2.5.4
Clipping
The clipping stage chops off those portions of the triangles that straddle the
frustum. Clipping is done by identifying vertices that lie outside the frustum
and then ﬁnding the intersection of the triangle’s edges with the planes of the
frustum. These intersection points become new vertices that deﬁne one or
more clipped triangles.
This stage is ﬁxed in function, but it is somewhat conﬁgurable. For ex-
ample, user-deﬁned clipping planes can be added in addition to the frustum
planes. This stage can also be conﬁgured to cull triangles that lie entirely out-
side the frustum.
10.2.5.5
Screen Mapping
Screen mapping simply scales and shifts the vertices from homogeneous clip
space into screen space. This stage is entirely ﬁxed and non-conﬁgurable.

498
10. The Rendering Engine
10.2.5.6
Triangle Set-up
During triangle set-up, the rasterization hardware is initialized for efﬁcient
conversion of the triangle into fragments. This stage is not conﬁgurable.
10.2.5.7
Triangle Traversal
Each triangle is broken into fragments (i.e., rasterized) by the triangle traversal
stage. Usually one fragment is generated for each pixel, although with certain
antialiasing techniques, multiple fragments may be created per pixel (see Sec-
tion 10.1.4.7). The triangle traversal stage also interpolates vertex attributes in
order to generate per-fragment attributes for processing by the pixel shader.
Perspective-correct interpolation is used where appropriate. This stage’s func-
tionality is ﬁxed and not conﬁgurable.
10.2.5.8
Early z-Test
Many graphics cards are capable of checking the depth of the fragment at this
point in the pipeline, discarding it if it is being occluded by the pixel already
in the frame buffer. This allows the (potentially very expensive) pixel shader
stage to be skipped entirely for occluded fragments.
Surprisingly, not all graphics hardware supports depth testing at this stage
of the pipeline. In older GPU designs, the z-test was done along with alpha
testing, after the pixel shader had run. For this reason, this stage is called the
early z-test or early depth test stage.
10.2.5.9
Pixel Shader
This stage is fully programmable. Its job is to shade (i.e., light and otherwise
process) each fragment. The pixel shader can also discard fragments, for ex-
ample because they are deemed to be entirely transparent. The pixel shader
can address one or more texture maps, run per-pixel lighting calculations, and
do whatever else is necessary to determine the fragment’s color.
The input to this stage is a collection of per-fragment attributes (which
have been interpolated from the vertex attributes by the triangle traversal
stage). The output is a single color vector describing the desired color of the
fragment.
10.2.5.10
Merging / Raster Operations Stage
The ﬁnal stage of the pipeline is known as the merging stage or blending stage,
also known as the raster operations stage or ROP in NVIDIA parlance. This
stage is not programmable, but it is highly conﬁgurable. It is responsible for
running various fragment tests including the depth test (see Section 10.1.4.8),

10.2. The Rendering Pipeline
499
alpha test (in which the values of the fragment’s and pixel’s alpha channels
can be used to reject certain fragments) and stencil test (see Section 10.3.3.1).
If the fragment passes all of the tests, its color is blended (merged) with
the color that is already present in the frame buffer. The way in which blend-
ing occurs is controlled by the alpha blending function—a function whose basic
structure is hard-wired, but whose operators and parameters can be conﬁg-
ured in order to produce a wide variety of blending operations.
Alpha blending is most commonly used to render semitransparent geom-
etry. In this case, the following blending function is used:
C′
D = ASCS + (1 −AS)CD.
The subscripts S and D stand for “source” (the incoming fragment) and “des-
tination” (the pixel in the frame buffer), respectively. Therefore, the color that
is written into the frame buffer (C′
D) is a weighted average of the existing frame
buffer contents (CD) and the color of the fragment being drawn (CS). The
blend weight (AS) is just the source alpha of the incoming fragment.
For alpha blending to look right, the semitransparent and translucent sur-
faces in the scene must be sorted and rendered in back-to-front order, af-
ter the opaque geometry has been rendered to the frame buffer. This is be-
cause after alpha blending has been performed, the depth of the new frag-
ment overwrites the depth of the pixel with which it was blended. In other
words, the depth buffer ignores transparency (unless depth writes have been
turned off, of course). If we are rendering a stack of translucent objects on
top of an opaque backdrop, the resulting pixel color should ideally be a blend
between the opaque surface’s color and the colors of all of the translucent
surfaces in the stack. If we try to render the stack in any order other than
back-to-front, depth-test failures will cause some of the translucent fragments
to be discarded, resulting in an incomplete blend (and a rather odd-looking
image).
Other alpha blending functions can be deﬁned as well, for purposes other
than transparency blending. The general blending equation takes the form
C′
D = (wS ⊗CS) + (wD ⊗CD), where the weighting factors wS and wD can
be selected by the programmer from a predeﬁned set of values including zero,
one, source or destination color, source or destination alpha and one minus
the source or destination color or alpha. The operator ⊗is either a regular
scalar-vector multiplication or a component-wise vector-vector multiplication
(a Hadamard product—see Section 4.2.4.1) depending on the data types of wS
and wD.

500
10. The Rendering Engine
10.2.6
Programmable Shaders
Now that we have an end-to-end picture of the GPU pipeline in mind, let’s
take a deeper look at the most interesting part of the pipeline—the program-
mable shaders. Shader architectures have evolved signiﬁcantly since their
introduction with DirectX 8. Early shader models supported only low-level
assembly language programming, and the instruction set and register set of
the pixel shader differed signiﬁcantly from those of the vertex shader. DirectX
9 brought with it support for high-level C-like shader languages such as Cg
(C for graphics), HLSL (High-Level Shading Language—Microsoft’s imple-
mentation of the Cg language) and GLSL (OpenGL shading language). With
DirectX 10, the geometry shader was introduced, and with it came a uniﬁed
shader architecture called shader model 4.0 in DirectX parlance. In the uniﬁed
shader model, all three types of shaders support roughly the same instruction
set and have roughly the same set of capabilities, including the ability to read
texture memory.
A shader takes a single element of input data and transforms it into zero
or more elements of output data.
•
In the case of the vertex shader, the input is a vertex whose position
and normal are expressed in model space or world space. The output
of the vertex shader is a fully transformed and lit vertex, expressed in
homogeneous clip space.
•
The input to the geometry shader is a single n-vertex primitive—a point
(n = 1), line segment (n = 2) or triangle (n = 3)—with up to n additional
vertices that act as control points. The output is zero or more primitives,
possibly of a different type than the input. For example, the geometry
shader could convert points into two-triangle quads, or it could trans-
form triangles into triangles but optionally discard some triangles and
so on.
•
The pixel shader’s input is a fragment whose attributes have been in-
terpolated from the three vertices of the triangle from which it came.
The output of the pixel shader is the color that will be written into the
frame buffer (presuming the fragment passes the depth test and other
optional tests). The pixel shader is also capable of discarding fragments
explicitly, in which case it produces no output.
10.2.6.1
Accessing Memory
Because the GPU implements a data processing pipeline, access to RAM is
very carefully controlled. A shader program cannot read from or write to

10.2. The Rendering Pipeline
501
memory directly. Instead, its memory accesses are limited to two methods:
registers and texture maps.1
Shader Registers
A shader can access RAM indirectly via registers. All GPU registers are in 128-
bit SIMD format. Each register is capable of holding four 32-bit ﬂoating-point
or integer values (represented by the float4 data type in the Cg language).
Such a register can contain a four-element vector in homogeneous coordinates
or a color in RGBA format, with each component in 32-bit ﬂoating-point for-
mat. Matrices can be represented by groups of three or four registers (rep-
resented by built-in matrix types like float4x4 in Cg). A GPU register can
also be used to hold a single 32-bit scalar, in which case the value is usually
replicated across all four 32-bit ﬁelds. Some GPUs can operate on 16-bit ﬁelds,
known as halfs. (Cg provides various built-in types like half4 and half4x4
for this purpose.)
Registers come in four ﬂavors, as follows:
•
Input registers. These registers are the shader’s primary source of in-
put data. In a vertex shader, the input registers contain attribute data
obtained directly from the vertices. In a pixel shader, the input regis-
ters contain interpolated vertex attribute data corresponding to a single
fragment. The values of all input registers are set automatically by the
GPU prior to invoking the shader.
•
Constant registers. The values of constant registers are set by the applica-
tion and can change from primitive to primitive. Their values are con-
stant only from the point of view of the shader program. They pro-
vide a secondary form of input to the shader. Typical contents include
the model-view matrix, the projection matrix, light parameters and any
other parameters required by the shader that are not available as vertex
attributes.
•
Temporary registers. These registers are for use by the shader program
internally and are typically used to store intermediate results of calcula-
tions.
•
Output registers. The contents of these registers are ﬁlled in by the shader
and serve as its only form of output. In a vertex shader, the output regis-
ters contain vertex attributes such as the transformed position and nor-
mal vectors in homogeneous clip space, optional vertex colors, texture
1The GPU on the PlayStation 4 provides a novel feature known as shader resource tables, or
SRTs. This allows the GPU to access main RAM directly via the high-speed “garlic” bus, which
saves a lot of CPU time because less time is spent “setting up” the scene prior to rendering.

502
10. The Rendering Engine
coordinates and so on. In a pixel shader, the output register contains the
ﬁnal color of the fragment being shaded.
The application provides the values of the constant registers when it sub-
mits primitives for rendering. The GPU automatically copies vertex or frag-
ment attribute data from video RAM into the appropriate input registers prior
to calling the shader program, and it also writes the contents of the output reg-
isters back into RAM at the conclusion of the program’s execution so that the
data can be passed to the next stage of the pipeline.
GPUs typically cache output data so that it can be reused without be-
ing recalculated. For example, the post-transform vertex cache stores the most-
recently processed vertices emitted by the vertex shader. If a triangle is en-
countered that refers to a previously processed vertex, it will be read from the
post-transform vertex cache if possible—the vertex shader need only be called
again if the vertex in question has since been ejected from the cache to make
room for newly processed vertices.
Textures
A shader also has direct read-only access to texture maps. Texture data is ad-
dressed via texture coordinates, rather than via absolute memory addresses.
The GPU’s texture samplers automatically ﬁlter the texture data, blending val-
ues between adjacent texels or adjacent mipmap levels as appropriate. Texture
ﬁltering can be disabled in order to gain direct access to the values of partic-
ular texels. This can be useful when a texture map is used as a data table, for
example.
Shaders can only write to texture maps in an indirect manner—by render-
ing the scene to an off-screen frame buffer that is interpreted as a texture map
by subsequent rendering passes. This feature is known as render to texture.
10.2.6.2
Introduction to High-Level Shader Language Syntax
High-level shader languages like Cg and GLSL are modeled after the C pro-
gramming language. The programmer can declare functions, deﬁne a simple
struct, and perform arithmetic. However, as we said above, a shader pro-
gram only has access to registers and textures. As such, the struct and vari-
able we declare in Cg or GLSL is mapped directly onto registers by the shader
compiler. We deﬁne these mappings in the following ways:
•
Semantics. Variables and struct members can be sufﬁxed with a colon
followed by a keyword known as a semantic. The semantic tells the

10.2. The Rendering Pipeline
503
shader compiler to bind the variable or data member to a particular ver-
tex or fragment attribute. For example, in a vertex shader we might
declare an input struct whose members map to the position and color
attributes of a vertex as follows:
struct VtxOut
{
float4 pos : POSITION; // map to position attribute
float4 color : COLOR;
// map to color attribute
};
•
Input versus output. The compiler determines whether a particular vari-
able or struct should map to input or output registers from the context
in which it is used. If a variable is passed as an argument to the shader
program’s main function, it is assumed to be an input; if it is the return
value of the main function, it is taken to be an output.
VtxOut vshaderMain(VtxIn in) // maps to input registers
{
VtxOut out;
// ...
return out;
// maps to output registers
}
•
Uniform declaration. To gain access to the data supplied by the applica-
tion via the constant registers, we can declare a variable with the key-
word uniform. For example, the model-view matrix could be passed
to a vertex shader as follows:
VtxOut vshaderMain(
VtxIn in,
uniform float4x4 modelViewMatrix)
{
VtxOut out;
// ...
return out;
}
Arithmetic operations can be performed by invoking C-style operators, or
by calling intrinsic functions as appropriate. For example, to multiply the
input vertex position by the model-view matrix, we could write:
VtxOut vshaderMain(VtxIn in,
uniform float4x4 modelViewMatrix)

504
10. The Rendering Engine
{
VtxOut out;
out.pos = mul(modelViewMatrix, in.pos);
out.color = float4(0, 1, 0, 1); // RGBA green
return out;
}
Data is obtained from textures by calling special intrinsic functions that
read the value of the texels at a speciﬁed texture coordinate. A number of
variants are available for reading one-, two- and three-dimensional textures in
various formats, with and without ﬁltering. Special texture addressing modes
are also available for accessing cube maps and shadow maps. References to
the texture maps themselves are declared using a special data type known as a
texture sampler declaration. For example, the data type sampler2D represents
a reference to a typical two-dimensional texture. The following simple Cg
pixel shader applies a diffuse texture to a triangle:
struct FragmentOut
{
float4 color : COLOR;
};
FragmentOut pshaderMain(float2 uv : TEXCOORD0,
uniform sampler2D texture)
{
FragmentOut out;
// look up texel at (u,v)
out.color = tex2D(texture, uv);
return out;
}
10.2.6.3
Effect Files
By itself, a shader program isn’t particularly useful. Additional information is
required by the GPU pipeline in order to call the shader program with mean-
ingful inputs. For example, we need to specify how the application-speciﬁed
parameters, like the model-view matrix, light parameters and so on, map to
the uniform variables declared in the shader program. In addition, some vi-
sual effects require two or more rendering passes, but a shader program only
describes the operations to be applied during a single rendering pass. If we

10.2. The Rendering Pipeline
505
are writing a game for the PC platform, we will need to deﬁne “fallback” ver-
sions of some of our more-advanced rendering effects, so that they will work
even on older graphics cards. To tie our shader program(s) together into a
complete visual effect, we turn to a ﬁle format known as an effect ﬁle.
Different rendering engines implement effects in slightly different ways.
In Cg, the effect ﬁle format is known as CgFX. OGRE uses a ﬁle format very
similar to CgFX known as a material ﬁle. GLSL effects can be described using
the COLLADA format, which is based on XML. Despite the differences, effects
generally take on the following hierarchical format:
•
At global scope, structs, shader programs (implemented as various
“main” functions) and global variables (which map to application-
speciﬁed constant parameters) are deﬁned.
•
One or more techniques are deﬁned. A technique represents one way to
render a particular visual effect. An effect typically provides a primary
technique for its highest-quality implementation and possibly a number
of fallback techniques for use on lower-powered graphics hardware.
•
Within each technique, one or more passes are deﬁned. A pass describes
how a single full-frame image should be rendered. It typically includes
a reference to a vertex, geometry and/or pixel shader program’s “main”
function, various parameter bindings and optional render state settings.
10.2.6.4
Further Reading
In this section, we’ve only had a small taste of what high-level shader pro-
gramming is like—a complete tutorial is beyond our scope here. For a much
more detailed introduction to Cg shader programming, refer to the Cg tuto-
rial available on NVIDIA’s website at https://developer.nvidia.com/content/
hello-cg-introductory-tutorial.
10.2.7
Antialiasing
When a triangle is rasterized, its edges can look jagged—the familiar “stair
step” effect we have all come to know and love (or hate). Technically speak-
ing, aliasing arises because we are using a discrete set of pixels to sample an
image that is really a smooth, continuous two-dimensional signal. (See Sec-
tion 13.3.2.1 for a detailed discussion of sampling and aliasing.)
The term antialiasing describes any technique that reduces the visual ar-
tifacts caused by aliasing. There are many different ways to antialias a ren-
dered scene.
The net effect of pretty much all of them is to “soften” the
edges of rendered triangles by blending them with surrounding pixels. Each

506
10. The Rendering Engine
Figure 10.45. No antialiasing (left), 4× MSAA (center) and Nvidia’s FXAA, preset 3 (right). Image
from Nvidia’s FXAA white paper by Timothy Lottes (http://bit.ly/1mIzCTv). (See Color Plate XVIII.)
technique has unique performance, memory-usage and quality characteris-
tics. Figure 10.45 shows a scene rendered ﬁrst without antialiasing, then with
4× MLAA and ﬁnally with Nvidia’s FXAA technique.
10.2.7.1
Full-Screen Antialiasing (FSAA)
In this technique, also known as super-sampled antialiasing (SSAA), the scene is
rendered into a frame buffer that is larger than the actual screen. Once render-
ing of the frame is complete, the resulting oversized image is downsampled to
the desired resolution. In 4× supersampling, the rendered image is twice as
wide and twice as tall as the screen, resulting in a frame buffer that occupies
four times the memory. It also requires four times the GPU processing power
because the pixel shader must be run four times for each screen pixel. As you
can see, FSAA is an incredibly expensive technique both in terms of memory
consumption and GPU cycles. As such, it is rarely used in practice.
10.2.7.2
Multisampled Antialiasing (MSAA)
Multisampled antialiasing is a technique that provides visual quality com-
parable to that of FSAA, while consuming a great deal less GPU bandwidth
(and the same amount of video RAM). The MSAA approach is based on the
observation that, thanks to the natural antialiasing effect of texture mipmap-

10.2. The Rendering Pipeline
507
ping, aliasing tends to be a problem primarily at the edges of triangles, not in
their interiors.
To understand how MSAA works, recall that the process of rasterizing a
triangle really boils down to three distinct operations: (1) Determining which
pixels the triangle overlaps (coverage), (2) determining whether or not each
pixel is occluded by some other triangle (depth testing) and (3) determining
the color of each pixel, presuming that the coverage and depth tests tell us
that the pixel should in fact be drawn (pixel shading).
When rasterizing a triangle without antialiasing, the coverage test, depth
test and pixel shading operations are all run at a single idealized point within
each screen pixel, usually located at its center. In MSAA, the coverage and
depth tests are run for N points known as subsamples within each screen pixel.
N is typically chosen to be 2, 4, 5, 8 or 16. However, the pixel shader is only
run once per screen pixel, no matter how many subsamples we use. This gives
MSAA a big advantage over FSAA in terms of GPU bandwidth, because shad-
ing is typically a great deal more expensive than coverage and depth testing.
In N× MSAA, the depth, stencil and color buffers are each allocated to
be N times as large as they would otherwise be. For each screen pixel, these
buffers contain N “slots,” one slot for each subsample. When rasterizing a
triangle, the coverage and depth tests are run N times for the N subsamples
within each fragment of the triangle. If at least one of the N tests indicates
that the fragment should be drawn, the pixel shader is run once. The color
obtained from the pixel shader is then stored only into those slots that cor-
respond to the subsamples that fell inside the triangle. Once the entire scene
has been rendered, the oversized color buffer is downsampled to yield the
ﬁnal screen-resolution image. This process involves averaging the color val-
ues found in the N subsample slots for each screen pixel. The net result is an
antialiased image with a shading cost equal to that of a non-antialiased image.
In Figure 10.46 we see a triangle that has been rasterized without antialias-
ing.
Figure 10.47 illustrates the 4× MSAA technique.
For more informa-
tion on MSAA, see http://mynameismjp.wordpress.com/2012/10/24/msaa
-overview.
10.2.7.3
Coverage Sample Antialiasing (CSAA)
This technique is an optimization of the MSAA technique pioneered by Nvidia.
For 4× CSAA, the pixel shader is run once, the depth test and color storage
is done for four subsample points per fragment, but the pixel coverage test is
performed for 16 “coverage subsamples” per fragment. This produces ﬁner-
grained color blending at the edges of triangles, similar to what you’d see
with 8× or 16× MSAA, but at the memory and GPU cost of 4× MSAA.

508
10. The Rendering Engine
Figure 10.46. Rasterizing a triangle without antialiasing.
Pixel shader runs at 
pixel center
2 of the 4 subsamples 
are inside the triangle
4 of the 4 subsamples 
are inside the triangle
50%
100%
Figure 10.47. Multisampled antialiasing (MSAA).

10.2. The Rendering Pipeline
509
10.2.7.4
Morphological Antialiasing (MLAA)
Morphological antialiasing focuses its efforts on correcting only those regions
of a scene that suffer the most from the effects of aliasing. In MLAA, the scene
is rendered at normal size, and then scanned in order to identify stair-stepped
patterns. When these patterns are found, they are blurred to reduce the effects
of aliasing. Fast approximate antialiasing (FXAA) is an optimized technique
developed by Nvidia that is similar to MLAA in its approach.
For a detailed discussion of MLAA, see http://visual-computing.intel
-research.net/publications/papers/2009/mlaa/mlaa.pdf. FXAA is described
in detail here: http://developer.download.nvidia.com/assets/gamedev/ﬁles/
sdk/11/FXAA_WhitePaper.pdf.
10.2.8
The Application Stage
Now that we understand how the GPU works, we can discuss the pipeline
stage that is responsible for driving it—the application stage. This stage has
three roles:
1.
Visibility determination. Only objects that are visible (or at least poten-
tially visible) should be submitted to the GPU, lest we waste valuable
resources processing triangles that will never be seen.
2.
Submitting geometry to the GPU for rendering. Submesh-material pairs are
sent to the GPU via a rendering call like DrawIndexedPrimitive()
(DirectX) or glDrawArrays() (OpenGL), or via direct construction of
the GPU command list. The geometry may be sorted for optimal ren-
dering performance. Geometry might be submitted more than once if
the scene needs to be rendered in multiple passes.
3.
Controlling shader parameters and render state. The uniform parameters
passed to the shader via constant registers are conﬁgured by the appli-
cation stage on a per-primitive basis. In addition, the application stage
must set all of the conﬁgurable parameters of the non-programmable
pipeline stages to ensure that each primitive is rendered appropriately.
In the following sections, we’ll brieﬂy explore how the application stage per-
forms these tasks.
10.2.8.1
Visibility Determination
The cheapest triangles are the ones you never draw. So it’s incredibly impor-
tant to cull objects from the scene that do not contribute to the ﬁnal rendered

510
10. The Rendering Engine
image prior to submitting them to the GPU. The process of constructing the
list of visible mesh instances is known as visibility determination.
Frustum Culling
In frustum culling, all objects that lie entirely outside the frustum are excluded
from our render list. Given a candidate mesh instance, we can determine
whether or not it lies inside the frustum by performing some simple tests be-
tween the object’s bounding volume and the six frustum planes. The bounding
volume is usually a sphere, because spheres are particularly easy to cull. For
each frustum plane, we move the plane outward a distance equal to the ra-
dius of the sphere, then we determine on which side of each modiﬁed plane
the center point of the sphere lies. If the sphere is found to be on the front side
of all six modiﬁed planes, the sphere is inside the frustum.
In practice, we don’t need to actually move the frustum planes. Recall
from Equation (4.13) that the perpendicular distance h from a point to a plane
can be calculated by plugging the point directly into the plane equation as
follows: h = ax+by +cz +d = n·P−n·P0 (see Section 4.6.3). So all we need
to do is plug the center point of our bounding sphere into the plane equations
for each frustum plane, giving us a value of hi for each plane i, and then we
can compare the hi values to the radius of the bounding sphere to determine
whether or not it lies inside each plane.
A scene graph data structure, described in Section 10.2.8.4, can help opti-
mize frustum culling by allowing us to ignore objects whose bounding spheres
are nowhere close to being inside the frustum.
Occlusion and Potentially Visible Sets
Even when objects lie entirely within the frustum, they may occlude one an-
other. Removing objects from the visible list that are entirely occluded by
other objects is called occlusion culling. In crowded environments viewed from
ground level, there can be a great deal of inter-object occlusion, making occlu-
sion culling extremely important. In less crowded scenes, or when scenes are
viewed from above, much less occlusion may be present, and the cost of oc-
clusion culling may outweigh its beneﬁts.
Gross occlusion culling of a large-scale environment can be done by pre-
calculating a potentially visible set (PVS). For any given camera vantage point,
a PVS lists those scene objects that might be visible. A PVS errs on the side
of including objects that aren’t actually visible, rather than excluding objects
that actually would have contributed to the rendered scene.
One way to implement a PVS system is to chop the level up into regions
of some kind. Each region can be provided with a list of the other regions

10.2. The Rendering Pipeline
511
that can be seen when the camera is inside it. These PVSs might be manually
speciﬁed by the artists or game designers. More commonly, an automated
ofﬂine tool generates the PVS based on user-speciﬁed regions. Such a tool
usually operates by rendering the scene from various randomly distributed
vantage points within a region. Every region’s geometry is color coded, so the
list of visible regions can be found by scanning the resulting frame buffer and
tabulating the region colors that are found. Because automated PVS tools are
imperfect, they typically provide the user with a mechanism for tweaking the
results, either by manually placing vantage points for testing, or by manually
specifying a list of regions that should be explicitly included or excluded from
a particular region’s PVS.
Portals
Another way to determine what portions of a scene are visible is to use portals.
In portal rendering, the game world is divided up into semiclosed regions
that are connected to one another via holes, such as windows and doorways.
These holes are called portals. They are usually represented by polygons that
describe their boundaries.
To render a scene with portals, we start by rendering the region that con-
tains the camera. Then, for each portal in the region, we extend a frustum-like
volume consisting of planes extending from the camera’s focal point through
each edge of the portal’s bounding polygon. The contents of the neighboring
region can be culled to this portal volume in exactly the same way geometry is
culled against the camera frustum. This ensures that only the visible geometry
in the adjacent regions will be rendered. Figure 10.48 provides an illustration
of this technique.
Occlusion Volumes (Antiportals)
If we ﬂip the portal concept on its head, pyramidal volumes can also be used
to describe regions of the scene that cannot be seen because they are being oc-
cluded by an object. These volumes are known as occlusion volumes or antipor-
tals. To construct an occlusion volume, we ﬁnd the silhouette edges of each
occluding object and extend planes outward from the camera’s focal point
through each of these edges. We test more-distant objects against these oc-
clusion volumes and cull them if they lie entirely within the occlusion region.
This is illustrated in Figure 10.49.
Portals are best used when rendering enclosed indoor environments with
a relatively small number of windows and doorways between “rooms.” In
this kind of scene, the portals occupy a relatively small percentage of the total

512
10. The Rendering Engine
A
B
E
C
D
F
G
Figure 10.48. Portals are used to deﬁne frustum-like volumes, which are used to cull the contents
of neighboring regions. In this example, objects A, B and D will be culled because they lie outside
one of the portals; the other objects will be visible.
volume of the camera frustum, resulting in a large number of objects outside
the portals that can be culled. Antiportals are best applied to large outdoor en-
vironments, in which nearby objects often occlude large swaths of the camera
frustum. In this case, the antiportals occupy a relatively large percentage of
the total camera frustum volume, resulting in large numbers of culled objects.
A
H
E
D
F
G
B
C
Figure 10.49. As a result of the antiportals corresponding to objects A, B and C, objects D, E, F and
G are culled. Therefore, only A, B, C and H are visible.

10.2. The Rendering Pipeline
513
10.2.8.2
Primitive Submission
Once a list of visible geometric primitives has been generated, the individual
primitives must be submitted to the GPU pipeline for rendering. This can be
accomplished by making calls to DrawIndexedPrimitive() in DirectX or
glDrawArrays() in OpenGL.
Render State
As we learned in Section 10.2.5, the functionality of many of the GPU pipeline’s
stages is ﬁxed but conﬁgurable. And even programmable stages are driven in
part by conﬁgurable parameters. Some examples of these conﬁgurable pa-
rameters are listed below (although this is by no means a complete list!)
•
world-view matrix;
•
light direction vectors;
•
texture bindings (i.e., which textures to use for a given material/shader);
•
texture addressing and ﬁltering modes;
•
time base for scrolling textures and other animated effects;
•
z-test (enabled or disabled); and
•
alpha blending options.
The set of all conﬁgurable parameters within the GPU pipeline is known
as the hardware state or render state. It is the application stage’s responsibility to
ensure that the hardware state is conﬁgured properly and completely for each
submitted primitive. Ideally these state settings are described completely by
the material associated with each submesh. So the application stage’s job boils
down to iterating through the list of visible mesh instances, iterating over
each submesh-material pair, setting the render state based on the material’s
speciﬁcations and then calling the low-level primitive submission functions
(DrawIndexedPrimitive(), glDrawArrays(), or similar).
State Leaks
If we forget to set some aspect of the render state between submitted prim-
itives, the settings used on the previous primitive will “leak” over onto the
new primitive. A render state leak might manifest itself as an object with the
wrong texture or an incorrect lighting effect, for example. Clearly it’s impor-
tant that the application stage never allow state leaks to occur.

514
10. The Rendering Engine
The GPU Command List
The application stage actually communicates with the GPU via a command
list. These commands interleave render state settings with references to the
geometry that should be drawn. For example, to render objects A and B with
material 1, followed by objects C, D and E using material 2, the command list
might look like this:
•
Set render state for material 1 (multiple commands, one per render state
setting).
•
Submit primitive A.
•
Submit primitive B.
•
Set render state for material 2 (multiple commands).
•
Submit primitive C.
•
Submit primitive D.
•
Submit primitive E.
Under the hood, API functions like DrawIndexedPrimitive() actually
just construct and submit GPU command lists. The cost of these API calls
can themselves be too high for some applications. To maximize performance,
some game engines build GPU command lists manually or by calling a low-
level rendering API like the PS3’s libgcm library.
10.2.8.3
Geometry Sorting
Render state settings are global—they apply to the entire GPU as a whole.
So in order to change render state settings, the entire GPU pipeline must be
ﬂushed before the new settings can be applied. This can cause massive per-
formance degradation if not managed carefully.
Clearly we’d like to change render settings as infrequently as possible. The
best way to accomplish this is to sort our geometry by material. That way, we
can install material A’s settings, render all geometry associated with material
A and then move on to material B.
Unfortunately, sorting geometry by material can have a detrimental effect
on rendering performance because it increases overdraw—a situation in which
the same pixel is ﬁlled multiple times by multiple overlapping triangles. Cer-
tainly some overdraw is necessary and desirable, as it is the only way to prop-
erly alpha-blend transparent and translucent surfaces into a scene. However,
overdraw of opaque pixels is always a waste of GPU bandwidth.
The early z-test is designed to discard occluded fragments before the ex-
pensive pixel shader has a chance to execute. But to take maximum advantage

10.2. The Rendering Pipeline
515
of early z, we need to draw the triangles in front-to-back order. That way, the
closest triangles will ﬁll the z-buffer right off the bat, and all of the fragments
coming from more-distant triangles behind them can be quickly discarded,
with little or no overdraw.
z-Prepass to the Rescue
How can we reconcile the need to sort geometry by material with the conﬂict-
ing need to render opaque geometry in a front-to-back order? The answer lies
in a GPU feature known as z-prepass.
The idea behind z-prepass is to render the scene twice: the ﬁrst time to
generate the contents of the z-buffer as efﬁciently as possible and the second
time to populate the frame buffer with full color information (but this time
with no overdraw, thanks to the contents of the z-buffer). The GPU provides a
special double-speed rendering mode in which the pixel shaders are disabled,
and only the z-buffer is updated. Opaque geometry can be rendered in front-
to-back order during this phase, to minimize the time required to generate
the z-buffer contents. Then the geometry can be resorted into material order
and rendered in full color with minimal state changes for maximum pipeline
throughput.
Once the opaque geometry has been rendered, transparent surfaces can be
drawn in back-to-front order. This brute-force method allows us to achieve the
proper alpha-blended result. Order-independent transparency (OIT) is a tech-
nique that permits transparent geometry to be rendered in an arbitrary order.
It works by storing multiple fragments per pixel, sorting each pixel’s frag-
ments and blending them only after the entire scene has been rendered. This
technique produces correct results without the need for pre-sorting the geom-
etry, but it comes at a high memory cost because the frame buffer must be
large enough to store all of the translucent fragments for each pixel.
10.2.8.4
Scene Graphs
Modern game worlds can be very large. The majority of the geometry in most
scenes does not lie within the camera frustum, so frustum culling all of these
objects explicitly is usually incredibly wasteful. Instead, we would like to de-
vise a data structure that manages all of the geometry in the scene and allows
us to quickly discard large swaths of the world that are nowhere near the cam-
era frustum prior to performing detailed frustum culling. Ideally, this data
structure should also help us to sort the geometry in the scene, either in front-
to-back order for the z-prepass or in material order for full-color rendering.
Such a data structure is often called a scene graph, in reference to the graph-
like data structures often used by ﬁlm rendering engines and DCC tools like

516
10. The Rendering Engine
Maya. However, a game’s scene graph needn’t be a graph, and in fact the
data structure of choice is usually some kind of tree. The basic idea behind
most of these data structures is to partition three-dimensional space in a way
that makes it easy to discard regions that do not intersect the frustum, with-
out having to frustum cull all of the individual objects within them. Examples
include quadtrees and octrees, BSP trees, kd-trees and spatial hashing tech-
niques.
Quadtrees and Octrees
A quadtree divides space into quadrants recursively. Each level of recursion
is represented by a node in the quadtree with four children, one for each
quadrant. The quadrants are typically separated by vertically oriented, axis-
aligned planes, so that the quadrants are square or rectangular. However,
some quadtrees subdivide space using arbitrarily shaped regions.
Quadtrees can be used to store and organize virtually any kind of spa-
tially distributed data. In the context of rendering engines, quadtrees are of-
ten used to store renderable primitives such as mesh instances, subregions of
terrain geometry or individual triangles of a large static mesh, for the pur-
poses of efﬁcient frustum culling. The renderable primitives are stored at the
leaves of the tree, and we usually aim to achieve a roughly uniform number of
primitives within each leaf region. This can be achieved by deciding whether
to continue or terminate the subdivision based on the number of primitives
within a region.
To determine which primitives are visible within the camera frustum, we
walk the tree from the root to the leaves, checking each region for intersection
with the frustum. If a given quadrant does not intersect the frustum, then
we know that none of its child regions will do so either, and we can stop
traversing that branch of the tree. This allows us to search for potentially
visible primitives much more quickly than would be possible with a linear
search (usually in O(log n) time). An example of a quadtree subdivision of
space is shown in Figure 10.50.
An octree is the three-dimensional equivalent of a quadtree, dividing space
into eight subregions at each level of the recursive subdivision. The regions of
an octree are often cubes or rectangular prisms but can be arbitrarily shaped
three-dimensional regions in general.
Bounding Sphere Trees
In the same way that a quadtree or octree subdivides space into (usually) rect-
angular regions, a bounding sphere tree divides space into spherical regions hi-

10.2. The Rendering Pipeline
517
Figure 10.50.
A top-down view of a space divided recursively into quadrants for storage in a
quadtree, based on the criterion of one point per region.
erarchically. The leaves of the tree contain the bounding spheres of the ren-
derable primitives in the scene. We collect these primitives into small logical
groups and calculate the net bounding sphere of each group. The groups are
themselves collected into larger groups, and this process continues until we
have a single group with a bounding sphere that encompasses the entire vir-
tual world. To generate a list of potentially visible primitives, we walk the tree
from the root to the leaves, testing each bounding sphere against the frustum,
and only recursing down branches that intersect it.
BSP Trees
A binary space partitioning (BSP) tree divides space in half recursively un-
til the objects within each half-space meet some predeﬁned criteria (much as
a quadtree divides space into quadrants). BSP trees have numerous uses, in-
cluding collision detection and constructive solid geometry, as well as its most
well-known application as a method for increasing the performance of frus-
tum culling and geometry sorting for 3D graphics. A kd-tree is a generaliza-
tion of the BSP tree concept to k dimensions.
In the context of rendering, a BSP tree divides space with a single plane
at each level of the recursion. The dividing planes can be axis-aligned, but
more commonly each subdivision corresponds to the plane of a single triangle
in the scene. All of the other triangles are then categorized as being either
on the front side or the back side of the plane. Any triangles that intersect
the dividing plane are themselves divided into three new triangles, so that
every triangle lies either entirely in front of or entirely behind the plane, or is

518
10. The Rendering Engine
coplanar with it. The result is a binary tree with a dividing plane and one or
more triangles at each interior node and triangles at the leaves.
A BSP tree can be used for frustum culling in much the same way a quad-
tree, octree or bounding sphere tree can. However, when generated with indi-
vidual triangles as described above, a BSP tree can also be used to sort trian-
gles into a strictly back-to-front or front-to-back order. This was particularly
important for early 3D games like Doom, which did not have the beneﬁt of a
z-buffer and so were forced to use the painter’s algorithm (i.e., to render the
scene from back to front) to ensure proper inter-triangle occlusion.
Given a camera view point in 3D space, a back-to-front sorting algorithm
walks the tree from the root. At each node, we check whether the view point
is in front of or behind that node’s dividing plane. If the camera is in front
of a node’s plane, we visit the node’s back children ﬁrst, then draw any tri-
angles that are coplanar with its dividing plane, and ﬁnally we visit its front
children. Likewise, when the camera’s view point is found to be behind a
node’s dividing plane, we visit the node’s front children ﬁrst, then draw the
triangles coplanar with the node’s plane and ﬁnally we visit its back children.
This traversal scheme ensures that the triangles farthest from the camera will
be visited before those that are closer to it, and hence it yields a back-to-front
ordering. Because this algorithm traverses all of the triangles in the scene, the
order of the traversal is independent of the direction the camera is looking.
A secondary frustum culling step would be required in order to traverse only
visible triangles. A simple BSP tree is shown in Figure 10.51, along with the
tree traversal that would be done for the camera position shown.
A
B
C
D2
D1
Camera
Visit A
Cam is in front
    Visit B
        Leaf node
Draw B
Draw A
    Visit C
        Cam is in front
            Visit D1
                Leaf node
Draw D1
Draw C
            Visit D2
                Leaf node
Draw D2
A
D2
D1
C
B
Figure 10.51. An example of back-to-front traversal of the triangles in a BSP tree. The triangles are
shown edge-on in two dimensions for simplicity, but in a real BSP tree the triangles and dividing
planes would be arbitrarily oriented in space.

10.3. Advanced Lighting and Global Illumination
519
Full coverage of BSP tree generation and usage algorithms is beyond our
scope here.
See http://www.gamedev.net/reference/articles/article657.asp
for more details on BSP trees.
10.2.8.5
Choosing a Scene Graph
Clearly there are many different kinds of scene graphs. Which data structure
to select for your game will depend upon the nature of the scenes you expect
to be rendering. To make the choice wisely, you must have a clear understand-
ing of what is required—and more importantly what is not required—when
rendering scenes for your particular game.
For example, if you’re implementing a ﬁghting game, in which two char-
acters battle it out in a ring surrounded by a mostly static environment, you
may not need much of a scene graph at all. If your game takes place primarily
in enclosed indoor environments, a BSP tree or portal system may serve you
well. If the action takes place outdoors on relatively ﬂat terrain, and the scene
is viewed primarily from above (as might be the case in a real-time strategy
game or god game), a simple quadtree might be all that’s required to achieve
high rendering speeds. On the other hand, if an outdoor scene is viewed
primarily from the point of view of someone on the ground, we may need
additional culling mechanisms. Densely populated scenes can beneﬁt from
an occlusion volume (antiportal) system, because there will be plenty of oc-
cluders. On the other hand, if your outdoor scene is very sparse, adding an
antiportal system probably won’t pay dividends (and might even hurt your
frame rate).
Ultimately, your choice of scene graph should be based on hard data ob-
tained by actually measuring the performance of your rendering engine. You
may be surprised to learn where all your cycles are actually going! But once
you know, you can select scene graph data structures and/or other optimiza-
tions to target the speciﬁc problems at hand.
10.3
Advanced Lighting and Global Illumination
In order to render photorealistic scenes, we need physically accurate global
illumination algorithms. A complete coverage of these techniques is beyond
our scope. In the following sections, we will brieﬂy outline the most prevalent
techniques in use within the game industry today. Our goal here is to provide
you with an awareness of these techniques and a jumping-off point for further
investigation. For an excellent in-depth coverage of this topic, see [8].

520
10. The Rendering Engine
Figure 10.52. An example of a normal-mapped surface.
10.3.1
Image-Based Lighting
A number of advanced lighting and shading techniques make heavy use of
image data, usually in the form of two-dimensional texture maps. These are
called image-based lighting algorithms.
10.3.1.1
Normal Mapping
A normal map speciﬁes a surface normal direction vector at each texel. This
allows a 3D modeler to provide the rendering engine with a highly detailed
description of a surface’s shape, without having to tessellate the model to a
high degree (as would be required if this same information were to be pro-
vided via vertex normals). Using a normal map, a single ﬂat triangle can be
made to look as though it were constructed from millions of tiny triangles. An
example of normal mapping is shown in Figure 10.52.
The normal vectors are typically encoded in the RGB color channels of the
texture, with a suitable bias to overcome the fact that RGB channels are strictly
positive while normal vector components can be negative. Sometimes only
two coordinates are stored in the texture; the third can be easily calculated at
runtime, given the assumption that the surface normals are unit vectors.
10.3.1.2
Heightmaps: Bump, Parallax and Displacement Mapping
As its name implies, a heightmap encodes the height of the ideal surface above
or below the surface of the triangle. Heightmaps are typically encoded as
grayscale images, since we only need a single height value per texel. Height-
maps can be used for bump mapping, parallax occlusion mapping and displace-
ment mapping—three techniques that can make a planar surface appear to have
height variation.

10.3. Advanced Lighting and Global Illumination
521
Figure 10.53. Comparison of bump mapping (left), parallax occlusion mapping (center) and displacement mapping (right).
In bump mapping, a heightmap is used as a cheap way to generate sur-
face normals.
This technique was primarily used in the early days of 3D
graphics—nowadays, most game engines store surface normal information
explicitly in a normal map, rather than calculating the normals from a height-
map.
Parallax occlusion mapping uses the information in a heightmap to arti-
ﬁcially adjust the texture coordinates used when rendering a ﬂat surface, in
such a way as to make the surface appear to contain surface details that move
semi-correctly as the camera moves. (This technique was used to produce the
bullet impact decals in the Uncharted series of games by Naughty Dog.)
Displacement mapping (also known as relief mapping) produces real sur-
face details by actually tessellating and then extruding surface polygons, again
using a heightmap to determine how much to displace each vertex. This pro-
duces the most convincing effect—one that properly self-occludes and self-
shadows—because real geometry is being generated. Figure 10.53 compares
bump mapping, parallax mapping and displacement mapping. Figure 10.54
shows an example of displacement mapping implemented in DirectX 9.
10.3.1.3
Specular/Gloss Maps
When light reﬂects directly off a shiny surface, we call this specular reﬂection.
The intensity of a specular reﬂection depends on the relative angles of the
viewer, the light source and the surface normal. As we saw in Section 10.1.3.2,
the specular intensity takes the form kS(R · V)α, where R is the reﬂection of
the light’s direction vector about the surface normal, V is the direction to the
viewer, kS is the overall specular reﬂectivity of the surface and α is called the
specular power.
Many surfaces aren’t uniformly glossy. For example, when a person’s face
is sweaty and dirty, wet regions appear shiny, while dry or dirty areas appear
dull. We can encode high-detail specularity information in a special texture
map known as a specular map.
If we store the value of kS in the texels of a specular map, we can control
how much specular reﬂection should be applied at each texel. This kind of

522
10. The Rendering Engine
Figure 10.54. DirectX 9 displacement mapping. Simple source geometry is tessellated at runtime
to produce the surface details.
specular map is sometimes called a gloss map. It is also called a specular mask,
because zero-valued texels can be used to “mask off” regions of the surface
where we do not want specular reﬂection applied. If we store the value of
α in our specular map, we can control the amount of “focus” our specular
highlights will have at each texel. This kind of texture is called a specular
power map. An example of a gloss map is shown in Figure 10.55.
10.3.1.4
Environment Mapping
An environment map looks like a panoramic photograph of the environment
taken from the point of view of an object in the scene, covering a full 360
degrees horizontally and either 180 degrees or 360 degrees vertically. An en-
vironment map acts like a description of the general lighting environment
surrounding an object. It is generally used to inexpensively render reﬂections.
The two most common formats are spherical environment maps and cubic
environment maps. A spherical map looks like a photograph taken through a
ﬁsheye lens, and it is treated as though it were mapped onto the inside of a
sphere whose radius is inﬁnite, centered about the object being rendered. The
problem with sphere maps is that they are addressed using spherical coordi-

10.3. Advanced Lighting and Global Illumination
523
Figure 10.55. This screenshot from EA’s Fight Night Round 3 shows how a gloss map can be used
to control the degree of specular reﬂection that should be applied to each texel of a surface. (See
Color Plate XIX.)
nates. Around the equator, there is plenty of resolution both horizontally and
vertically. However, as the vertical (azimuthal) angle approaches vertical, the
resolution of the texture along the horizontal (zenith) axis decreases to a single
texel. Cube maps were devised to avoid this problem.
A cube map looks like a composite photograph pieced together from pho-
tos taken in the six primary directions (up, down, left, right, front and back).
During rendering, a cube map is treated as though it were mapped onto the
six inner surfaces of a box at inﬁnity, centered on the object being rendered.
To read the environment map texel corresponding to a point P on the sur-
face of an object, we take the ray from the camera to the point P and reﬂect
it about the surface normal at P. The reﬂected ray is followed until it inter-
sects the sphere or cube of the environment map. The value of the texel at this
intersection point is used when shading the point P.
10.3.1.5
Three-Dimensional Textures
Modern graphics harware also includes support for three-dimensional tex-
tures. A 3D texture can be thought of as a stack of 2D textures. The GPU
knows how to address and ﬁlter a 3D texture, given a three-dimensional tex-
ture coordinate (u, v, w).
Three-dimensional textures can be useful for describing the appearance or
volumetric properties of an object. For example, we could render a marble
sphere and allow it to be cut by an arbitrary plane. The texture would look
continuous and correct across the cut no matter where it was made, because
the texture is well-deﬁned and continuous throughout the entire volume of
the sphere.

524
10. The Rendering Engine
10.3.2
High Dynamic Range Lighting
A display device like a television set or CRT monitor can only produce a lim-
ited range of intensities. This is why the color channels in the frame buffer are
limited to a zero to one range. But in the real world, light intensities can grow
arbitrarily large. High dynamic range (HDR) lighting attempts to capture this
wide range of light intensities.
HDR lighting performs lighting calculations without clamping the result-
ing intensities arbitrarily. The resulting image is stored in a format that per-
mits intensities to grow beyond one. The net effect is an image in which ex-
treme dark and light regions can be represented without loss of detail within
either type of region.
Prior to display on-screen, a process called tone mapping is used to shift and
scale the image’s intensity range into the range supported by the display de-
vice. Doing this permits the rendering engine to reproduce many real-world
visual effects, like the temporary blindness that occurs when you walk from
a dark room into a brightly lit area, or the way light seems to bleed out from
behind a brightly back-lit object (an effect known as bloom).
One way to represent an HDR image is to store the R, G and B channels
using 32-bit ﬂoating-point numbers, instead of 8-bit integers. Another alter-
native is to employ an entirely different color model altogether. The log-LUV
color model is a popular choice for HDR lighting. In this model, color is rep-
resented as an intensity channel (L) and two chromaticity channels (U and V ).
Because the human eye is more sensitive to changes in intensity than it is to
changes in chromaticity, the L channel is stored in 16 bits while U and V are
given only eight bits each. In addition, L is represented using a logarithmic
scale (base two) in order to capture a very wide range of light intensities.
10.3.3
Global Illumination
As we noted in Section 10.1.3.1, global illumination refers to a class of light-
ing algorithms that account for light’s interactions with multiple objects in the
scene, on its way from the light source to the virtual camera. Global illumina-
tion accounts for effects like the shadows that arise when one surface occludes
another, reﬂections, caustics and the way the color of one object can “bleed”
onto the objects around it. In the following sections, we’ll take a brief look
at some of the most common global illumination techniques. Some of these
methods aim to reproduce a single isolated effect, like shadows or reﬂections.
Others like radiosity and ray tracing methods aim to provide a holistic model
of global light transport.

10.3. Advanced Lighting and Global Illumination
525
10.3.3.1
Shadow Rendering
Shadows are created when a surface blocks light’s path. The shadows caused
by an ideal point light source would be sharp, but in the real world shadows
have blurry edges; this is called the penumbra. A penumbra arises because
real-world light sources cover some area and so produce light rays that graze
the edges of an object at different angles.
The two most prevalent shadow rendering techniques are shadow volumes
and shadow maps. We’ll brieﬂy describe each in the sections below. In both
techniques, objects in the scene are generally divided into three categories:
objects that cast shadows, objects that are to receive shadows and objects that
are entirely excluded from consideration when rendering shadows. Likewise,
the lights are tagged to indicate whether or not they should generate shadows.
This important optimization limits the number of light-object combinations
that need to be processed in order to produce the shadows in a scene.
Shadow Volumes
In the shadow volume technique, each shadow caster is viewed from the van-
tage point of a shadow-generating light source, and the shadow caster’s sil-
houette edges are identiﬁed. These edges are extruded in the direction of the
light rays emanating from the light source. The result is a new piece of geom-
etry that describes the volume of space in which the light is occluded by the
shadow caster in question. This is shown in Figure 10.56.
Figure 10.56. A shadow volume generated by extruding the silhouette edges of a shadow casting
object as seen from the point of view of the light source.

526
10. The Rendering Engine
A shadow volume is used to generate a shadow by making use of a special
full-screen buffer known as the stencil buffer. This buffer stores a single inte-
ger value corresponding to each pixel of the screen. Rendering can be masked
by the values in the stencil buffer—for example, we could conﬁgure the GPU
to only render fragments whose corresponding stencil values are nonzero. In
addition, the GPU can be conﬁgured so that rendered geometry updates the
values in the stencil buffer in various useful ways.
To render shadows, the scene is ﬁrst drawn to generate an unshadowed
image in the frame buffer, along with an accurate z-buffer. The stencil buffer
is cleared so that it contains zeros at every pixel. Each shadow volume is
then rendered from the point of view of the camera in such a way that front-
facing triangles increase the values in the stencil buffer by one, while back-
facing triangles decrease them by one. In areas of the screen where the shadow
volume does not appear at all, of course the stencil buffer’s pixels will be left
containing zero. The stencil buffer will also contain zeros where both the front
and back faces of the shadow volume are visible, because the front face will
increase the stencil value but the back face will decrease it again. In areas
where the back face of the shadow volume has been occluded by “real” scene
geometry, the stencil value will be one. This tells us which pixels of the screen
are in shadow. So we can render shadows in a third pass, by simply darkening
those regions of the screen that contain a nonzero stencil buffer value.
Shadow Maps
The shadow mapping technique is effectively a per-fragment depth test per-
formed from the point of view of the light instead of from the point of view
of the camera. The scene is rendered in two steps: First, a shadow map texture
is generated by rendering the scene from the point of view of the light source
and saving off the contents of the depth buffer. Second, the scene is rendered
as usual, and the shadow map is used to determine whether or not each frag-
ment is in shadow. At each fragment in the scene, the shadow map tells us
whether or not the light is being occluded by some geometry that is closer
to the light source, in just the same way that the z-buffer tells us whether a
fragment is being occluded by a triangle that is closer to the camera.
A shadow map contains only depth information—each texel records how
far away it is from the light source. Shadow maps are therefore typically ren-
dered using the hardware’s double-speed z-only mode (since all we care about
is the depth information). For a point light source, a perspective projection is
used when rendering the shadow map; for a directional light source, an or-
thographic projection is used instead.

10.3. Advanced Lighting and Global Illumination
527
Figure 10.57. The far left image is a shadow map—the contents of the z-buffer as rendered from
the point of view of a particular light source. The pixels of the center image are black where the
light-space depth test failed (fragment in shadow) and white where it succeeded (fragment not in
shadow). The far right image shows the ﬁnal scene rendered with shadows.
To render a scene using a shadow map, we draw the scene as usual from
the point of view of the camera. For each vertex of every triangle, we calculate
its position in light space—i.e., in the same “view space” that was used when
generating the shadow map in the ﬁrst place. These light-space coordinates
can be interpolated across the triangle, just like any other vertex attribute. This
gives us the position of each fragment in light space. To determine whether a
given fragment is in shadow or not, we convert the fragment’s light-space
(x, y)-coordinates into texture coordinates (u, v) within the shadow map. We
then compare the fragment’s light-space z-coordinate with the depth stored
at the corresponding texel in the shadow depth map. If the fragment’s light-
space z is farther away from the light than the texel in the shadow map, then
it must be occluded by some other piece of geometry that is closer to the light
source—hence it is in shadow. Likewise, if the fragment’s light-space z is
closer to the light source than the texel in the shadow map, then it is not oc-
cluded and is not in shadow. Based on this information, the fragment’s color
can be adjusted accordingly. The shadow mapping process is illustrated in
Figure 10.57.
10.3.3.2
Ambient Occlusion
Ambient occlusion is a technique for modeling contact shadows—the soft shad-
ows that arise when a scene is illuminated by only ambient light. In effect,
ambient occlusion describes how “accessible” each point on a surface is to
light in general. For example, the interior of a section of pipe is less accessi-
ble to ambient light than its exterior. If the pipe were placed outside on an
overcast day, its interior would generally appear darker than its exterior.
Figure 10.58 shows how ambient occlusion produces shadows underneath
a car and in its wheel wells, as well as within the seams between body pan-
els. Ambient occlusion is measured at a point on a surface by constructing

528
10. The Rendering Engine
Figure 10.58. A car rendered with ambient occlusion. Notice the darkened areas underneath the
vehicle and in the wheel wells.
a hemisphere with a very large radius centered on that point and determing
what percentage of that hemisphere’s area is visible from the point in ques-
tion. It can be precomputed ofﬂine for static objects, because ambient occlu-
sion is independent of view direction and the direction of incident light. It is
typically stored in a texture map that records the level of ambient occlusion at
each texel across the surface.
10.3.3.3
Reﬂections
Reﬂections occur when light bounces off a highly specular (shiny) surface pro-
ducing an image of another portion of the scene in the surface. Reﬂections can
be implemented in a number of ways. Environment maps are used to pro-
duce general reﬂections of the surrounding environment on the surfaces of
shiny objects. Direct reﬂections in ﬂat surfaces like mirrors can be produced
by reﬂecting the camera’s position about the plane of the reﬂective surface
and then rendering the scene from that reﬂected point of view into a texture.
The texture is then applied to the reﬂective surface in a second pass (see Fig-
ure 10.59).
10.3.3.4
Caustics
Caustics are the bright specular highlights arising from intense reﬂections or
refractions from very shiny surfaces like water or polished metal. When the
reﬂective surface moves, as is the case for water, the caustic effects glimmer
and “swim” across the surfaces on which they fall. Caustic effects can be pro-
duced by projecting a (possibly animated) texture containing semi-random
bright highlights onto the affected surfaces. An example of this technique is
shown in Figure 10.60.

10.3. Advanced Lighting and Global Illumination
529
Figure 10.59.
Mirror reﬂections in The Last of Us (© 2013/™SCEA. Created and developed by
Naughty Dog, PlayStation 3) implemented by rendering the scene to a texture that is subsequently
applied to the mirror’s surface. (See Color Plate XX.)
Figure 10.60.
Water caustics produced by projecting an animated texture onto the affected
surfaces.
10.3.3.5
Subsurface Scattering
When light enters a surface at one point, is scattered beneath the surface, and
then reemerges at a different point on the surface, we call this subsurface scat-
tering. This phenomenon is responsible for the “warm glow” of human skin,
wax and marble statues (e.g., Figure 10.61). Subsurface scattering is described
by a more-advanced variant of the BRDF (see Section 10.1.3.2) known as the
BSSRDF (bidirectional surface scattering reﬂectance distribution function).
Subsurface scattering can be simulated in a number of ways. Depth-map–
based subsurface scattering renders a shadow map (see Section 10.3.3.1), but
instead of using it to determine which pixels are in shadow, it is used to mea-
sure how far a beam of light would have to travel in order to pass all the

530
10. The Rendering Engine
Figure 10.61.
On the left, a dragon rendered without subsurface scattering (i.e., using a BRDF
lighting model). On the right, the same dragon rendered with subsurface scattering (i.e., using a
BSSRDF model). Images rendered by Rui Wang at the University of Virginia.
way through the occluding object. The shadowed side of the object is then
given an artiﬁcial diffuse lighting term whose intensity is inversely propor-
tional to the distance the light had to travel in order to emerge on the oppo-
site side of the object. This causes objects to appear to be glowing slightly
on the side opposite to the light source but only where the object is rela-
tively thin. For more information on subsurface scattering techniques, see
http://http.developer.nvidia.com/GPUGems/gpugems_ch16.html.
10.3.3.6
Precomputed Radiance Transfer (PRT)
Precomputed radiance transfer (PRT) is a popular technique that attempts to sim-
ulate the effects of radiosity-based rendering methods in real time. It does so
by precomputing and storing a complete description of how an incident light
ray would interact with a surface (reﬂect, refract, scatter, etc.) when approach-
ing from every possible direction. At runtime, the response to a particular
incident light ray can be looked up and quickly converted into very accurate
lighting results.
In general the light’s response at a point on the surface is a complex func-
tion deﬁned on a hemisphere centered about the point. A compact repre-
sentation of this function is required to make the PRT technique practical. A
common approach is to approximate the function as a linear combination of
spherical harmonic basis functions. This is essentially the three-dimensional
equivalent of encoding a simple scalar function f(x) as a linear combination
of shifted and scaled sine waves.
The details of PRT are far beyond our scope. For more information, see
http://web4.cs.ucl.ac.uk/staff/j.kautz/publications/prtSIG02.pdf. PRT light-
ing techniques are demonstrated in a DirectX sample program available in the
DirectX SDK—see http://msdn.microsoft.com/en-us/library/bb147287.aspx
for more details.

10.3. Advanced Lighting and Global Illumination
531
10.3.4
Deferred Rendering
In traditional triangle-rasterization–based rendering, all lighting and shading
calculations are performed on the triangle fragments in world space, view
space or tangent space. The problem with this technique is that it is inher-
ently inefﬁcient. For one thing, we potentially do work that we don’t need
to do. We shade the vertices of triangles, only to discover during the rasteri-
zation stage that the entire triangle is being depth-culled by the z-test. Early
z-tests help eliminate unnecessary pixel shader evaluations, but even this isn’t
perfect. What’s more, in order to handle a complex scene with lots of lights,
we end up with a proliferation of different versions of our vertex and pixel
shaders—versions that handle different numbers of lights, different types of
lights, different numbers of skinning weights, etc.
Deferred rendering is an alternative way to shade a scene that addresses
many of these problems. In deferred rendering, the majority of the lighting
calculations are done in screen space, not view space. We efﬁciently render
the scene without worrying about lighting. During this phase, we store all the
information we’re going to need to light the pixels in a “deep” frame buffer
known as the G-buffer. Once the scene has been fully rendered, we use the
information in the G-buffer to perform our lighting and shading calculations.
This is usually much more efﬁcient than view-space lighting, avoids the pro-
liferation of shader variants and permits some very pleasing effects to be ren-
dered relatively easily.
The G-buffer may be physically implemented as a collection of buffers,
but conceptually it is a single frame buffer containing a rich set of informa-
tion about the lighting and surface properties of the objects in the scene at
every pixel on the screen. A typical G-buffer might contain the following per-
pixel attributes: depth, surface normal in view space or world space, diffuse
color, specular power, even precomputed radiance transfer (PRT) coefﬁcients.
The following sequence of screenshots from Guerrilla Games’ Killzone 2 (Fig-
ure 10.62) shows some of the typical components of the G-buffer.
An in-depth discussion of deferred rendering is beyond our scope, but the
folks at Guerrilla Games have prepared an excellent presentation on the topic,
which is available at http://www.slideshare.net/guerrillagames/deferred
-rendering-in-killzone-2-9691589.
10.3.5
Physically Based Shading
Traditional game lighting engines have required artists and lighters to tweak
a wide variety of sometimes non-intuitive parameters, across numerous dis-
parate rendering engine systems, in order to achieve a desired “look” in-game.

532
10. The Rendering Engine
Figure 10.62. Screenshots from Killzone 2 by Guerrilla Games, showing some of the typical com-
ponents of the G-buffer used in deferred rendering. The upper image shows the ﬁnal rendered
image. Below it, clockwise from the upper left, are the albedo (diffuse) color, depth, view-space
normal, screen-space 2D motion vector (for motion blurring), specular power and specular inten-
sity. (See Color Plate XXI.)
This can be an arduous and time-consuming process. What’s worse, param-
eter settings that work well under one set of lighting conditions might not
work well under other lighting scenarios. To address these problems, render-
ing programmers are turning toward physically based shading models.
A physically based shading model attempts to approximate the ways in
which light travels and interacts with materials in the real world, allowing
artists and lighters to tweak shader parameters using intuitive, real-world
quantities measured in real-world units. A complete discussion of physically
based shading is beyond the scope of this book, but you can start to learn
more about it here: https://www.marmoset.co/toolbag/learn/pbr-theory.
10.4
Visual Effects and Overlays
The rendering pipeline we’ve discussed to this point is responsible primarily
for rendering three-dimensional solid objects. A number of specialized ren-
dering systems are typically layered on top of this pipeline, responsible for
rendering visual elements like particle effects, decals (small geometry over-
lays that represent bullet holes, cracks, scratches and other surface details),
hair and fur, rain or falling snow, water and other specialized visual effects.
Full-screen post effects may be applied, including vignette (a reduction of
brightness and saturation around the edges of the screen), motion blur, depth

10.4. Visual Effects and Overlays
533
of ﬁeld blurring, artiﬁcial/enhanced colorization, and the list goes on. Fi-
nally, the game’s menu system and heads-up display (HUD) are typically
realized by rendering text and other two- or three-dimensional graphics in
screen space overlaid on top of the three-dimensional scene.
An in-depth coverage of these engine systems is beyond our scope. In the
following sections, we’ll provide a brief overview of these rendering systems,
and point you in the direction of additional information.
10.4.1
Particle Effects
A particle rendering system is concerned with rendering amorphous objects
like clouds of smoke, sparks, ﬂame and so on. These are called particle effects.
The key features that differentiate a particle effect from other kinds of render-
able geometry are as follows:
•
It is composed of a very large number of relatively simple pieces of geom-
etry—most often simple cards called quads, composed of two triangles
each.
•
The geometry is often camera-facing (i.e., billboarded), meaning that the
engine must take steps to ensure that the face normals of each quad
always point directly at the camera’s focal point.
•
Its materials are almost always semitransparent or translucent. As such,
particle effects have some stringent rendering order constraints that do
not apply to the majority of opaque objects in a scene.
•
Particles animate in a rich variety of ways. Their positions, orientations,
sizes (scales), texture coordinates and many of their shader parameters
vary from frame to frame. These changes are deﬁned either by hand-
authored animation curves or via procedural methods.
•
Particles are typically spawned and killed continually. A particle emitter is
a logical entity in the world that creates particles at some user-speciﬁed
rate; particles are killed when they hit a predeﬁned death plane, or when
they have lived for a user-deﬁned length of time, or as decided by some
other user-speciﬁed criteria.
Particle effects could be rendered using regular triangle mesh geometry
with appropriate shaders. However, because of the unique characteristics
listed above, a specialized particle effect animation and rendering system is
always used to implement them in a real production game engine. A few
example particle effects are shown in Figure 10.63.

534
10. The Rendering Engine
Figure 10.63. Flame, smoke and bullet tracer particle effects in Uncharted 3: Drake’s Deception
(© 2011/™SCEA. Created and developed by Naughty Dog, PlayStation 3). (See Color Plate XXII.)
Particle system design and implementation is a rich topic that could oc-
cupy many chapters all on its own. For more information on particle systems,
see [1, Section 10.7], [14, Section 20.5], [9, Section 13.7] and [10, Section 4.1.2].
10.4.2
Decals
A decal is a relatively small piece of geometry that is overlaid on top of the reg-
ular geometry in the scene, allowing the visual appearance of the surface to be
modiﬁed dynamically. Examples include bullet holes, foot prints, scratches,
cracks, etc.
The approach most often used by modern engines is to model a decal as a
rectangular area that is to be projected along a ray into the scene. This gives
rise to a rectangular prism in 3D space. Whatever surface the prism intersects
ﬁrst becomes the surface of the decal. The triangles of the intersected geome-
try are extracted and clipped against the four bounding planes of the decal’s
projected prism. The resulting triangles are texture-mapped with a desired
decal texture by generating appropriate texture coordinates for each vertex.
These texture-mapped triangles are then rendered over the top of the regular
scene, often using parallax mapping to give them the illusion of depth and
with a slight z-bias (usually implemented by shifting the near plane slightly)
so they don’t experience z-ﬁghting with the geometry on which they are over-
laid. The result is the appearance of a bullet hole, scratch or other kind of
surface modiﬁcation. Some bullet-hole decals are depicted in Figure 10.64.
For more information on creating and rendering decals, see [7, Section 4.8]
and [28, Section 9.2].

10.4. Visual Effects and Overlays
535
Figure 10.64. Parallax-mapped decals from Uncharted 3: Drake’s Deception (© 2011/™SCEA. Cre-
ated and developed by Naughty Dog, PlayStation 3). (See Color Plate XXIII.)
10.4.3
Environmental Effects
Any game that takes place in a somewhat natural or realistic environment
requires some kind of environmental rendering effects. These effects are usu-
ally implemented via specialized rendering systems. We’ll take a brief look at
a few of the more common of these systems in the following sections.
10.4.3.1
Skies
The sky in a game world needs to contain vivid detail, yet technically speak-
ing it lies an extremely long distance away from the camera. Therefore, we
cannot model it as it really is and must turn instead to various specialized
rendering techniques.
One simple approach is to ﬁll the frame buffer with the sky texture prior
to rendering any 3D geometry. The sky texture should be rendered at an ap-
proximate 1:1 texel-to-pixel ratio, so that the texture is roughly or exactly the
resolution of the screen. The sky texture can be rotated and scrolled to corre-
spond to the motions of the camera in-game. During rendering of the sky, we
make sure to set the depth of all pixels in the frame buffer to the maximum
possible depth value. This ensures that the 3D scene elements will always sort
on top of the sky. The arcade hit Hydro Thunder rendered its skies in exactly
this manner.
On modern game platforms, where pixel shading costs can be high, sky
rendering is often done after the rest of the scene has been rendered. First
the z-buffer is cleared to the maximum z-value. Then the scene is rendered.

536
10. The Rendering Engine
Finally the sky is rendered, with z-testing enabled, z writing turned off, and
using a z-test value that is one less than the maximum. This causes the sky to
be drawn only where it is not occluded by closer objects like terrain, buildings
and trees. Drawing the sky last ensures that its pixel shader is run for the
minimum possible number of screen pixels.
For games in which the player can look in any direction, we can use a sky
dome or sky box. The dome or box is rendered with its center always at the
camera’s current location, so that it appears to lie at inﬁnity, no matter where
the camera moves in the game world. As with the sky texture approach, the
sky box or dome is rendered before any other 3D geometry, and all of the
pixels in the frame buffer are set to the maximum z-value when the sky is
rendered. This means that the dome or box can actually be tiny, relative to
other objects in the scene. Its size is irrelevant, as long as it ﬁlls the entire
frame buffer when it is drawn. For more information on sky rendering, see [1,
Section 10.3] and [39, page 253].
Clouds are often implemented with a specialized rendering and anima-
tion system as well. In early games like Doom and Quake, the clouds were just
planes with scrolling semitransparent cloud textures on them. More-recent
cloud techniques include camera-facing cards (billboards), particle-effect based
clouds and volumetric cloud effects.
10.4.3.2
Terrain
The goal of a terrain system is to model the surface of the earth and provide a
canvas of sorts upon which other static and dynamic elements can be laid out.
Terrain is sometimes modeled explicitly in a package like Maya. But if the
player can see far into the distance, we usually want some kind of dynamic
tessellation or other level of detail (LOD) system. We may also need to limit
the amount of data required to represent very large outdoor areas.
Height ﬁeld terrain is one popular choice for modeling large terrain areas.
The data size can be kept relatively small because a height ﬁeld is typically
stored in a grayscale texture map. In most height-ﬁeld– based terrain sys-
tems, the horizontal (y = 0) plane is tessellated in a regular grid pattern, and
the heights of the terrain vertices are determined by sampling the height ﬁeld
texture. The number of triangles per unit area can be varied based on distance
from the camera, thereby allowing large-scale features to be seen in the dis-
tance, while still permitting a good deal of detail to be represented for nearby
terrain. An example of a terrain deﬁned via a height ﬁeld bitmap is shown in
Figure 10.65.
Terrain systems usually provide specialized tools for “painting” the height
ﬁeld itself, carving out terrain features like roads, rivers and so on. Texture

10.4. Visual Effects and Overlays
537
Figure 10.65. A grayscale height ﬁeld bitmap (left) can be used to control the vertical positions
of the vertices in a terrain grid mesh (right). In this example, a water plane intersects the terrain
mesh to create islands.
mapping in a terrain system is often a blend between four or more textures.
This allows artists to “paint” in grass, dirt, gravel and other terrain features
by simply exposing one of the texture layers. The layers can be cross-blended
from one to another to provide smooth textural transitions. Some terrain tools
also permit sections of the terrain to be cut out to permit buildings, trenches
and other specialized terrain features to be inserted in the form of regular
mesh geometry. Terrain authoring tools are sometimes integrated directly into
the game world editor, while in other engines they may be stand-alone tools.
Of course, height ﬁeld terrain is just one of many options for modeling the
surface of the Earth in a game. For more information on terrain rendering,
see [6, Sections 4.16 through 4.19] and [7, Section 4.2].
10.4.3.3
Water
Water renderers are commonplace in games nowadays. There are lots of dif-
ferent kinds of water, including oceans, pools, rivers, waterfalls, fountains,
jets, puddles and damp solid surfaces. Each type of water generally requires
some specialized rendering technology. Some also require dynamic motion
simulations. Large bodies of water may require dynamic tessellation or other
LOD methodologies similar to those employed in a terrain system.
Water systems sometimes interact with a game’s rigid body dynamics sys-
tem (ﬂotation, force from water jets, etc.) and with gameplay (slippery sur-
faces, swimming mechanics, diving mechanics, riding vertical jets of water
and so on). Water effects are often created by combining disparate render-
ing technologies and subsystems. For example, a waterfall might make use

538
10. The Rendering Engine
of specialized water shaders, scrolling textures, particle effects for mist at the
base, a decal-like overlay for foam, and the list goes on. Today’s games offer
some pretty amazing water effects, and active research into technologies like
real-time ﬂuid dynamics promises to make water simulations even richer and
more realistic in the years ahead. For more information on water rendering
and simulation techniques, see [1, Sections 9.3, 9.5 and 9.6], [13] and [6, Sec-
tions 2.6 and 5.11].
10.4.4
Overlays
Most games have heads-up displays, in-game graphical user interfaces and
menu systems.
These overlays are typically comprised of two- and three-
dimensional graphics rendered directly in view space or screen space.
Overlays are generally rendered after the primary scene, with z-testing
disabled to ensure that they appear on top of the three-dimensional scene.
Two-dimensional overlays are typically implemented by rendering quads (tri-
angle pairs) in screen space using an orthographic projection. Three-dimen-
sional overlays may be rendered using an orthographic projection or via the
regular perspective projection with the geometry positioned in view space so
that it follows the camera around.
10.4.4.1
Normalized Screen Coordinates
The coordinates of two-dimensional overlays can be measured in terms of
screen pixels. However, if your game is going to be expected to support mul-
tiple screen resolutions (which is very common in PC games), it’s a far better
idea to use normalized screen coordinates. Normalized coordinates range from
zero to one along one of the two axes (but not both—see Section 10.4.4.2), and
they can easily be scaled into pixel-based measurements corresponding to an
arbitrary screen resolution. This allows us to lay out our overlay elements
without worrying about screen resolution at all (and only having to worry a
little bit about aspect ratio).
It’s easiest to deﬁne normalized coordinates so that they range from 0.0 to
1.0 along the y-axis. At a 4:3 aspect ratio, this means that the x-axis would
range from 0.0 to 1.333 (= 4/3), while at 16:9 the x-axis’ range would be from
0.0 to 1.777 (= 16/9). It’s important not to deﬁne our coordinates so that they
range from zero to one along both axes. Doing this would cause square visual
elements to have unequal x and y dimensions—or put another way, a visual
element with seemingly square dimensions would not look like a square on-
screen! Moreover, our “square” elements would stretch differently at different
aspect ratios—deﬁnitely not an acceptable state of affairs.

10.4. Visual Effects and Overlays
539
10.4.4.2
Relative Screen Coordinates
To really make normalized coordinates work well, it should be possible to
specify coordinates in absolute or relative terms. For example, positive co-
ordinates might be interpreted as being relative to the top-left corner of the
screen, while negative coordinates might be relative to the bottom-right cor-
ner. That way, if I want a HUD element to be a certain distance from the right
or bottom edges of the screen, I won’t have to change its normalized coordi-
nates when the aspect ratio changes. We might want to allow an even richer
set of possible alignment choices, such as aligning to the center of the screen
or aligning to another visual element.
That said, you’ll probably have some overlay elements that simply cannot
be laid out using normalized coordinates in such a way that they look right
at both the 4:3 and 16:9 aspect ratios. You may want to consider having two
distinct layouts, one for each aspect ratio, so you can ﬁne-tune them indepen-
dently.
10.4.4.3
Text and Fonts
A game engine’s text/font system is typically implemented as a special kind
of two-dimensional (or sometimes three-dimensional) overlay. At its core, a
text rendering system needs to be capable of displaying a sequence of charac-
ter glyphs corresponding to a text string, arranged in various orientations on
the screen. A font is often implemented via a texture map containing the var-
ious required glyphs. A font description ﬁle provides information such as the
bounding boxes of each glyph within the texture, and font layout information
such as kerning, baseline offsets and so on.
A good text/font system must account for the differences in character sets
and reading directions inherent in various languages. Some text systems also
provide various fun features like the ability to animate characters across the
screen in various ways, the ability to animate individual characters and so
on. Some game engines even go so far as to implement a subset of the Adobe
Flash standard in order to support a rich set of two-dimensional effects in their
overlays and text. However, it’s important to remember when implementing a
game font system that only those features that are actually required by the game
should be implemented. There’s no point in furnishing your engine with an
advanced text animation if your game never needs to display animated text!
10.4.5
Gamma Correction
CRT monitors tend to have a nonlinear response to luminance values. That
is, if a linearly increasing ramp of R, G or B values were to be sent to a CRT,

540
10. The Rendering Engine
Figure 10.66.
The effect of a CRT’s gamma response on
image quality and how the effect can be corrected for. Image
courtesy of www.wikipedia.org.
Figure 10.67. Gamma encoding and decoding curves. Image
courtesy of www.wikipedia.org.
the image that would result on-screen would be perceptually nonlinear to the
human eye. Visually, the dark regions of the image would look darker than
they should. This is illustrated in Figure 10.66.
The gamma response curve of a typical CRT display can be modeled quite
simply by the formula
Vout = V γ
in
where γCRT > 1. To correct for this effect, the colors sent to the CRT display are
usually passed through an inverse transformation (i.e., using a gamma value
γcorr < 1). The value of γCRT for a typical CRT monitor is 2.2, so the correction
value is usually γcorr ≈1/2.2 = 0.455. These gamma encoding and decoding
curves are shown in Figure 10.67.
Gamma encoding can be performed by the 3D rendering engine to ensure
that the values in the ﬁnal image are properly gamma-corrected. One problem
that is encountered, however, is that the bitmap images used to represent tex-
ture maps are often gamma-corrected themselves. A high-quality rendering
engine takes this fact into account, by gamma-decoding the textures prior to
rendering and then re-encoding the gamma of the ﬁnal rendered scene so that
its colors can be reproduced properly on-screen.
10.4.6
Full-Screen Post Effects
Full-screen post effects are effects applied to a rendered three-dimensional scene
that provide additional realism or a stylized look. These effects are often im-

10.5. Further Reading
541
plemented by passing the entire contents of the screen through a pixel shader
that applies the desired effect(s). This can be accomplished by rendering a
full-screen quad that has been mapped with a texture containing the unﬁl-
tered scene. A few examples of full-screen post effects are given below:
•
Motion blur.
This is typically implemented by rendering a buffer of
screen-space velocity vectors and using this vector ﬁeld to selectively
blur the rendered image. Blurring is accomplished by passing a con-
volution kernel over the image (see “Image Smoothing and Sharpening
by Discrete Convolution” by Dale A. Schumacher, published in [4], for
details).
•
Depth of ﬁeld blur. This blur effect can be produced by using the contents
of the depth buffer to adjust the degree of blur applied at each pixel.
•
Vignette. In this ﬁlmic effect, the brightness or saturation of the image
is reduced at the corners of the screen for dramatic effect. It is some-
times implemented by literally rendering a texture overlay on top of the
screen. A variation on this effect is used to produce the classic circular
effect used to indicate that the player is looking through a pair of binoc-
ulars or a weapon scope.
•
Colorization. The colors of screen pixels can be altered in arbitrary ways
as a post-processing effect. For example, all colors except red could be
desaturated to grey to produce a striking effect similar to the famous
scene of the little girl in the red coat from Schindler’s List.
10.5
Further Reading
We’ve covered a lot of material in a very short space in this chapter, but we’ve
only just scratched the surface. No doubt you’ll want to explore many of these
topics in much greater detail. For an excellent overview of the entire process of
creating three-dimensional computer graphics and animation for games and
ﬁlm, I highly recommend [23]. The technology that underlies modern real-
time rendering is covered in excellent depth in [1], while [14] is well known
as the deﬁnitive reference guide to all things related to computer graphics.
Other great books on 3D rendering include [44], [9] and [10]. The mathemat-
ics of 3D rendering is covered very well in [28]. No graphics programmer’s
library would be complete without one or more books from the Graphics Gems
series ([18], [4], [24], [19] and [37]) and/or the GPU Gems series ([13], [39]
and [35]). Of course, this short reference list is only the beginning—you will
undoubtedly encounter a great many more excellent books on rendering and
shaders over the course of your career as a game programmer.

This page intentionally left blank
This page intentionally left blank

11
Animation Systems
T
he majority of modern 3D games revolve around characters—often human
or humanoid, sometimes animal or alien. Characters are unique because
they need to move in a ﬂuid, organic way. This poses a host of new tech-
nical challenges, over and above what is required to simulate and animate
rigid objects like vehicles, projectiles, soccer balls and Tetris pieces. The task
of imbuing characters with natural-looking motion is handled by an engine
component known as the character animation system.
As we’ll see, an animation system gives game designers a powerful suite
of tools that can be applied to non-characters as well as characters. Any game
object that is not 100% rigid can take advantage of the animation system. So
whenever you see a vehicle with moving parts, a piece of articulated machin-
ery, trees waving gently in the breeze or even an exploding building in a game,
chances are good that the object makes at least partial use of the game engine’s
animation system.
11.1
Types of Character Animation
Character animation technology has come a long way since Donkey Kong. At
ﬁrst, games employed very simple techniques to provide the illusion of life-
like movement. As game hardware improved, more-advanced techniques be-
543

544
11. Animation Systems
came feasible in real time. Today, game designers have a host of powerful
animation methods at their disposal. In this section, we’ll take a brief look
at the evolution of character animation and outline the three most-common
techniques used in modern game engines.
11.1.1
Cel Animation
The precursor to all game animation techniques is known as traditional anima-
tion, or hand-drawn animation. This is the technique used in the earliest ani-
mated cartoons. The illusion of motion is produced by displaying a sequence
of still pictures known as frames in rapid succession. Real-time 3D rendering
can be thought of as an electronic form of traditional animation, in that a se-
quence of still full-screen images is presented to the viewer over and over to
produce the illusion of motion.
Cel animation is a speciﬁc type of traditional animation. A cel is a trans-
parent sheet of plastic on which images can be painted or drawn. An ani-
mated sequence of cels can be placed on top of a ﬁxed background painting
or drawing to produce the illusion of motion without having to redraw the
static background over and over.
The electronic equivalent to cel animation is a technology known as sprite
animation. A sprite is a small bitmap that can be overlaid on top of a full-
screen background image without disrupting it, often drawn with the aid of
specialized graphics hardware. Hence, a sprite is to 2D game animation what
a cel was to traditional animation. This technique was a staple during the 2D
game era. Figure 11.1 shows the famous sequence of sprite bitmaps that were
used to produce the illusion of a running humanoid character in almost every
Mattel Intellivision game ever made. The sequence of frames was designed so
that it animates smoothly even when it is repeated indeﬁnitely—this is known
as a looping animation. This particular animation would be called a run cycle in
modern parlance, because it makes the character appear to be running. Char-
acters typically have a number of looping animation cycles, including various
idle cycles, a walk cycle and a run cycle.
Figure 11.1. The sequence of sprite bitmaps used in most Intellivision games.
11.1.2
Rigid Hierarchical Animation
With the advent of 3D graphics, sprite techniques began to lose their appeal.
Doom made use of a sprite-like animation system: Its monsters were noth-

11.1. Types of Character Animation
545
ing more than camera-facing quads, each of which displayed a sequence of
texture bitmaps (known as an animated texture) to produce the illusion of mo-
tion. And this technique is still used today for low-resolution and/or distant
objects—for example crowds in a stadium, or hordes of soldiers ﬁghting a dis-
tant battle in the background. But for high-quality foreground characters, 3D
graphics brought with it the need for improved character animation methods.
The earliest approach to 3D character animation is a technique known as
rigid hierarchical animation. In this approach, a character is modeled as a col-
lection of rigid pieces. A typical breakdown for a humanoid character might
be pelvis, torso, upper arms, lower arms, upper legs, lower legs, hands, feet
and head. The rigid pieces are constrained to one another in a hierarchical
fashion, analogous to the manner in which a mammal’s bones are connected
at the joints. This allows the character to move naturally. For example, when
the upper arm is moved, the lower arm and hand will automatically follow it.
A typical hierarchy has the pelvis at the root, with the torso and upper legs as
its immediate children and so on as shown below:
Pelvis
Torso
UpperRightArm
LowerRightArm
RightHand
UpperLeftArm
UpperLeftArm
LeftHand
Head
UpperRightLeg
LowerRightLeg
RightFoot
UpperLeftLeg
UpperLeftLeg
LeftFoot
The big problem with the rigid hierarchy technique is that the behavior of
the character’s body is often not very pleasing due to “cracking” at the joints.
This is illustrated in Figure 11.2. Rigid hierarchical animation works well for
Figure 11.2. Cracking at the joints is a big problem in rigid hierarchical animation.

546
11. Animation Systems
robots and machinery that really are constructed of rigid parts, but it breaks
down under scrutiny when applied to “ﬂeshy” characters.
11.1.3
Per-Vertex Animation and Morph Targets
Rigid hierarchical animation tends to look unnatural because it is rigid. What
we really want is a way to move individual vertices so that triangles can
stretch to produce more natural-looking motion.
One way to achieve this is to apply a brute-force technique known as per-
vertex animation. In this approach, the vertices of the mesh are animated by an
artist, and motion data is exported, which tells the game engine how to move
each vertex at runtime. This technique can produce any mesh deformation
imaginable (limited only by the tessellation of the surface). However, it is
a data-intensive technique, since time-varying motion information must be
stored for each vertex of the mesh. For this reason, it has little application to
real-time games.
A variation on this technique known as morph target animation is used in
some real-time games. In this approach, the vertices of a mesh are moved by
an animator to create a relatively small set of ﬁxed, extreme poses. Animations
are produced by blending between two or more of these ﬁxed poses at runtime.
The position of each vertex is calculated using a simple linear interpolation
(LERP) between the vertex’s positions in each of the extreme poses.
The morph target technique is often used for facial animation, because
the human face is an extremely complex piece of anatomy, driven by roughly
50 muscles. Morph target animation gives an animator full control over ev-
ery vertex of a facial mesh, allowing him or her to produce both subtle and
extreme movements that approximate the musculature of the face well. Fig-
ure 11.3 shows a set of facial morph targets.
As computing power continues to increase, some studios are using jointed
facial rigs containing hundreds of joints as an alternative to morph targets.
Other studios combine the two techniques, using jointed rigs to achieve the
primary pose of the face and then applying small tweaks via morph targets.
Figure 11.3. A set of facial morph targets for the Ellie character in The Last of Us (© 2013/™SCEA.
Created and developed by Naughty Dog, PlayStation 3).

11.1. Types of Character Animation
547
11.1.4
Skinned Animation
As the capabilities of game hardware improved further, an animation tech-
nology known as skinned animation was developed. This technique has many
of the beneﬁts of per-vertex and morph target animation—permitting the tri-
angles of an animated mesh to deform. But it also enjoys the much more
efﬁcient performance and memory usage characteristics of rigid hierarchical
animation. It is capable of producing reasonably realistic approximations to
the movement of skin and clothing.
Skinned animation was ﬁrst used by games like Super Mario 64, and it is
still the most prevalent technique in use today, both by the game industry and
the feature ﬁlm industry. A host of famous modern game and movie char-
acters, including the dinosaurs from Jurrassic Park, Solid Snake (Metal Gear
Solid 4), Gollum (Lord of the Rings), Nathan Drake (Uncharted), Buzz Lightyear
(Toy Story), Marcus Fenix (Gears of War) and Joel (The Last of Us) were all ani-
mated, in whole or in part, using skinned animation techniques. The remain-
der of this chapter will be devoted primarily to the study of skinned/skeletal
animation.
In skinned animation, a skeleton is constructed from rigid “bones,” just as
in rigid hierarchical animation. However, instead of rendering the rigid pieces
on-screen, they remain hidden. A smooth continuous triangle mesh called a
skin is bound to the joints of the skeleton; its vertices track the movements of
the joints. Each vertex of the skin mesh can be weighted to multiple joints, so
the skin can stretch in a natural way as the joints move.
Figure 11.4. Eric Browning’s Crank the Weasel character, with internal skeletal structure.

548
11. Animation Systems
In Figure 11.4, we see Crank the Weasel, a game character designed by
Eric Browning for Midway Home Entertainment in 2001. Crank’s outer skin
is composed of a mesh of triangles, just like any other 3D model. However,
inside him we can see the rigid bones and joints that make his skin move.
11.1.5
Animation Methods as Data Compression Techniques
The most ﬂexible animation system conceivable would give the animator con-
trol over literally every inﬁnitesimal point on an object’s surface. Of course,
animating like this would result in an animation that contains a potentially
inﬁnite amount of data! Animating the vertices of a triangle mesh is a simpli-
ﬁcation of this ideal—in effect, we are compressing the amount of information
needed to describe an animation by restricting ourselves to moving only the
vertices. (Animating a set of control points is the analog of vertex animation
for models constructed out of higher-order patches.) Morph targets can be
thought of as an additional level of compression, achieved by imposing addi-
tional constraints on the system—vertices are constrained to move only along
linear paths between a ﬁxed number of predeﬁned vertex positions. Skeletal
animation is just another way to compress vertex animation data by imposing
constraints. In this case, the motions of a relatively large number of vertices
are constrained to follow the motions of a relatively small number of skeletal
joints.
When considering the trade-offs between various animation techniques, it
can be helpful to think of them as compression methods, analogous in many
respects to video compression techniques. We should generally aim to select
the animation method that provides the best compression without producing
unacceptable visual artifacts. Skeletal animation provides the best compres-
sion when the motion of a single joint is magniﬁed into the motions of many
vertices. A character’s limbs act like rigid bodies for the most part, so they can
be moved very efﬁciently with a skeleton. However, the motion of a face tends
to be much more complex, with the motions of individual vertices being more
independent. To convincingly animate a face using the skeletal approach, the
required number of joints approaches the number of vertices in the mesh, thus
diminishing its effectiveness as a compression technique. This is one reason
why morph target techniques are often favored over the skeletal approach for
facial animation. (Another common reason is that morph targets tend to be a
more natural way for animators to work.)
11.2
Skeletons
A skeleton is comprised of a hierarchy of rigid pieces known as joints. In the
game industry, we often use the terms “joint” and “bone” interchangeably,

11.2. Skeletons
549
Figure 11.5. The pelvis joint of this character connects to four other joints (tail, spine and two
legs), and so it produces four bones.
but the term bone is actually a misnomer. Technically speaking, the joints are
the objects that are directly manipulated by the animator, while the bones are
simply the empty spaces between the joints. As an example, consider the
pelvis joint in the Crank the Weasel character model. It is a single joint, but
because it connects to four other joints (the tail, the spine and the left and right
hip joints), this one joint appears to have four bones sticking out of it. This is
shown in more detail in Figure 11.5. Game engines don’t care a whip about
bones—only the joints matter. So whenever you hear the term “bone” being
used in the industry, remember that 99% of the time we are actually speaking
about joints.
11.2.1
The Skeleal Hierarchy
As we’ve mentioned, the joints in a skeleton form a hierarchy or tree structure.
One joint is selected as the root, and all other joints are its children, grandchil-
dren and so on. A typical joint hierarchy for skinned animation looks almost
identical to a typical rigid hierarchy. For example, a humanoid character’s
joint hierarchy might look something like this:
Pelvis
LowerSpine
MiddleSpine
UpperSpine
RightShoulder
RightElbow
RightHand
RightThumb

550
11. Animation Systems
RightIndexFinger
RightMiddleFinger
RightRingFinger
RightPinkyFinger
LeftShoulder
LeftElbow
LeftHand
LeftThumb
LeftIndexFinger
LeftMiddleFinger
LeftRingFinger
LeftPinkyFinger
Neck
Head
LeftEye
RightEye
various face joints
RightThigh
RightKnee
RightAnkle
LeftThigh
LeftKnee
LeftAnkle
We usually assign each joint an index from 0 to N −1. Because each joint
has one and only one parent, the hierarchical structure of a skeleton can be
fully described by storing the index of its parent with each joint. The root
joint has no parent, so its parent index is usually set to an invalid value such
as −1.
11.2.2
Representing a Skeleton in Memory
A skeleton is usually represented by a small top-level data structure that con-
tains an array of data structures for the individual joints. The joints are usually
listed in an order that ensures a child joint will always appear after its parent
in the array. This implies that joint zero is always the root of the skeleton.
Joint indices are usually used to refer to joints within animation data struc-
tures. For example, a child joint typically refers to its parent joint by specifying
its index. Likewise, in a skinned triangle mesh, a vertex refers to the joint or
joints to which it is bound by index. This is much more efﬁcient than referring
to joints by name, both in terms of the amount of storage required (a joint in-
dex can be 8 bits wide, as long as we are willing to accept a maximum of 256
joints per skeleton) and in terms of the amount of time it takes to look up a
referenced joint (we can use the joint index to jump immediately to a desired
joint in the array).

11.3. Poses
551
Each joint data structure typically contains the following information:
•
The name of the joint, either as a string or a hashed 32-bit string id.
•
The index of the joint’s parent within the skeleton.
•
The inverse bind pose transform of the joint. The bind pose of a joint is the
position, orientation and scale of that joint at the time it was bound to
the vertices of the skin mesh. We usually store the inverse of this transfor-
mation for reasons we’ll explore in more depth in the following sections.
A typical skeleton data structure might look something like this:
struct Joint
{
Matrix4x3
m_invBindPose; // inverse bind pose
// transform
const char* m_name;
// human-readable joint
// name
U8
m_iParent;
// parent index or 0xFF
// if root
};
struct Skeleton
{
U32
m_jointCount;
// number of joints
Joint*
m_aJoint;
// array of joints
};
11.3
Poses
No matter what technique is used to produce an animation, be it cel-based,
rigid hierarchical or skinned/skeletal, every animation takes place over time.
A character is imbued with the illusion of motion by arranging the character’s
body into a sequence of discrete, still poses and then displaying those poses in
rapid succession, usually at a rate of 30 or 60 poses per second. (Actually, as
we’ll see in Section 11.4.1.1, we often interpolate between adjacent poses rather
than displaying a single pose verbatim.) In skeletal animation, the pose of the
skeleton directly controls the vertices of the mesh, and posing is the anima-
tor’s primary tool for breathing life into her characters. So clearly, before we
can animate a skeleton, we must ﬁrst understand how to pose it.
A skeleton is posed by rotating, translating and possibly scaling its joints
in arbitrary ways. The pose of a joint is deﬁned as the joint’s position, orien-
tation and scale, relative to some frame of reference. A joint pose is usually
represented by a 4 × 4 or 4 × 3 matrix, or by an SQT data structure (scale,

552
11. Animation Systems
quaternion rotation and vector translation). The pose of a skeleton is just the
set of all of its joints’ poses and is normally represented as a simple array of
matrices or SQTs.
11.3.1
Bind Pose
Two different poses of the same skeleton are shown in Figure 11.6. The pose
on the left is a special pose known as the bind pose, also sometimes called the
reference pose or the rest pose. This is the pose of the 3D mesh prior to being
bound to the skeleton (hence the name). In other words, it is the pose that the
mesh would assume if it were rendered as a regular, unskinned triangle mesh,
without any skeleton at all. The bind pose is also called the T-pose because the
character is usually standing with his feet slightly apart and his arms out-
stretched in the shape of the letter T. This particular stance is chosen because
it keeps the limbs away from the body and each other, making the process of
binding the vertices to the joints easier.
Figure 11.6. Two different poses of the same skeleton. The pose on the left is the special pose
known as bind pose.
11.3.2
Local Poses
A joint’s pose is most often speciﬁed relative to its parent joint. A parent-
relative pose allows a joint to move naturally. For example, if we rotate the
shoulder joint, but leave the parent-relative poses of the elbow, wrist and ﬁn-
gers unchanged, the entire arm will rotate about the shoulder in a rigid man-

11.3. Poses
553
Figure 11.7. Every joint in a skeletal hierarchy deﬁnes a set of local coordinate space axes, known
as joint space.
ner, as we’d expect. We sometimes use the term local pose to describe a parent-
relative pose. Local poses are almost always stored in SQT format, for reasons
we’ll explore when we discuss animation blending.
Graphically, many 3D authoring packages like Maya represent joints as
small spheres. However, a joint has a rotation and a scale, not just a transla-
tion, so this visualization can be a bit misleading. In fact, a joint actually de-
ﬁnes a coordinate space no different in principle from the other spaces we’ve
encountered (like model space, world space or view space). So it is best to pic-
ture a joint as a set of Cartesian coordinate axes. Maya gives the user the op-
tion of displaying a joint’s local coordinate axes—this is shown in Figure 11.7.
Mathematically, a joint pose is nothing more than an afﬁne transformation.
The pose of joint j can be written as the 4 × 4 afﬁne transformation matrix Pj,
which is comprised of a translation vector Tj, a 3 × 3 diagonal scale matrix
Sj and a 3 × 3 rotation matrix Rj. The pose of an entire skeleton Pskel can be
written as the set of all poses Pj, where j ranges from 0 to N −1:
Pj =
SjRj
0
Tj
1

,
Pskel =
n
Pj
o
N−1
j=0 .
11.3.2.1
Joint Scale
Some game engines assume that joints will never be scaled, in which case Sj
is simply omitted and assumed to be the identity matrix. Other engines make
the assumption that scale will be uniform if present, meaning it is the same in

554
11. Animation Systems
all three dimensions. In this case, scale can be represented using a single scalar
value sj. Some engines even permit nonuniform scale, in which case scale can
be compactly represented by the three-element vector sj =
sjx
sjy
sjz

.
The elements of the vector sj correspond to the three diagonal elements of
the 3 × 3 scaling matrix Sj, so it is not really a vector per se. Game engines
almost never permit shear, so Sj is almost never represented by a full 3 × 3
scale/shear matrix, although it certainly could be.
There are a number of beneﬁts to omitting or constraining scale in a pose or
animation. Clearly using a lower-dimensional scale representation can save
memory. (Uniform scale requires a single ﬂoating-point scalar per joint per
animation frame, while nonuniform scale requires three ﬂoats, and a full 3×3
scale-shear matrix requires nine.) Restricting our engine to uniform scale has
the added beneﬁt of ensuring that the bounding sphere of a joint will never
be transformed into an ellipsoid, as it could be when scaled in a nonuniform
manner. This greatly simpliﬁes the mathematics of frustum and collision tests
in engines that perform such tests on a per-joint basis.
11.3.2.2
Representing a Joint Pose in Memory
As we mentioned above, joint poses are usually stored in SQT format. In C++,
such a data structure might look like this, where Q is ﬁrst to ensure proper
alignment and optimal structure packing. (Can you see why?)
struct JointPose
{
Quaternion
m_rot;
// Q
Vector3
m_trans; // T
F32
m_scale; // S (uniform scale only)
};
If nonuniform scale is permitted, we might deﬁne a joint pose like this instead:
struct JointPose
{
Quaternion
m_rot;
// Q
Vector4
m_trans; // T
Vector4
m_scale; // S
};
The local pose of an entire skeleton can be represented as follows, where
it is understood that the array m_aLocalPose is dynamically allocated to
contain just enough occurrences of JointPose to match the number of joints
in the skeleton.

11.3. Poses
555
struct SkeletonPose
{
Skeleton*
m_pSkeleton;
// skeleton + num joints
JointPose* m_aLocalPose; // local joint poses
};
11.3.2.3
The Joint Pose as a Change of Basis
It’s important to remember that a local joint pose is speciﬁed relative to the
joint’s immediate parent.
Any afﬁne transformation can be thought of as
transforming points and vectors from one coordinate space to another. So
when the joint pose transform Pj is applied to a point or vector that is ex-
pressed in the coordinate system of the joint j, the result is that same point or
vector expressed in the space of the parent joint.
As we’ve done in earlier chapters, we’ll adopt the convention of using
subscripts to denote the direction of a transformation. Since a joint pose takes
points and vectors from the child joint’s space (C) to that of its parent joint (P),
we can write it (PC→P)j. Alternatively, we can introduce the function p(j),
which returns the parent index of joint j, and write the local pose of joint j as
Pj→p(j).
On occasion we will need to transform points and vectors in the opposite
direction—from parent space into the space of the child joint. This transfor-
mation is just the inverse of the local joint pose. Mathematically, Pp(j)→j =
 Pj→p(j)
−1.
11.3.3
Global Poses
Sometimes it is convenient to express a joint’s pose in model space or world
space. This is called a global pose. Some engines express global poses in matrix
form, while others use the SQT format.
Mathematically, the model-space pose of a joint (j →M) can be found by
walking the skeletal hierarchy from the joint in question all the way to the
root, multiplying the local poses (j →p(j)) as we go. Consider the hierarchy
shown in Figure 11.8. The parent space of the root joint is deﬁned to be model
space, so p(0) ≡M. The model-space pose of joint J2 can therefore be written
as follows:
P2→M = P2→1P1→0P0→M.
Likewise, the model-space pose of joint J5 is just
P5→M = P5→4P4→3P3→0P0→M.

556
11. Animation Systems
0
1
2
3
4
5
xM
yM
Figure 11.8. A global pose can be calculated by walking the hierarchy from the joint in question
towards the root and model-space origin, concatenating the child-to-parent (local) transforms of
each joint as we go.
In general, the global pose (joint-to-model transform) of any joint j can be
written as follows:
Pj→M =
0
Y
i=j
Pi→p(i),
(11.1)
where it is understood that i becomes p(i) (the parent of joint i) after each
iteration in the product, and p(0) ≡M.
11.3.3.1
Representing a Global Pose in Memory
We can extend our SkeletonPose data structure to include the global pose
as follows, where again we dynamically allocate the m_aGlobalPose array
based on the number of joints in the skeleton:
struct SkeletonPose
{
Skeleton*
m_pSkeleton;
// skeleton + num joints
JointPose* m_aLocalPose;
// local joint poses
Matrix44*
m_aGlobalPose; // global joint poses
};
11.4
Clips
In a ﬁlm, every aspect of each scene is carefully planned out before any anima-
tions are created. This includes the movements of every character and prop
in the scene, and even the movements of the camera. This means that an en-
tire scene can be animated as one long, contiguous sequence of frames. And
characters need not be animated at all whenever they are off-camera.

11.4. Clips
557
Game animation is different. A game is an interactive experience, so one
cannot predict beforehand how the characters are going to move and behave.
The player has full control over his or her character and usually has partial
control over the camera as well. Even the decisions of the computer-driven
non-player characters are strongly inﬂuenced by the unpredictable actions of
the human player. As such, game animations are almost never created as long,
contiguous sequences of frames. Instead, a game character’s movement must
be broken down into a large number of ﬁne-grained motions. We call these
individual motions animation clips, or sometimes just animations.
Each clip causes the character to perform a single well-deﬁned action.
Some clips are designed to be looped—for example, a walk cycle or run cy-
cle. Others are designed to be played once—for example, throwing an object
or tripping and falling to the ground. Some clips affect the entire body of
the character—the character jumping into the air for instance. Other clips af-
fect only a part of the body—perhaps the character waving his right arm. The
movements of any one game character are typically broken down into literally
thousands of clips.
The only exception to this rule is when game characters are involved in a
noninteractive portion of the game, known as an in-game cinematic (IGC), non-
interactive sequence (NIS) or full-motion video (FMV). Noninteractive sequences
are typically used to communicate story elements that do not lend themselves
well to interactive gameplay, and they are created in much the same way
computer-generated ﬁlms are made (although they often make use of in-game
assets like character meshes, skeletons and textures). The terms IGC and NIS
typically refer to noninteractive sequences that are rendered in real time by
the game engine itself. The term FMV applies to sequences that have been
prerendered to an MP4, WMV or other type of movie ﬁle and are played back
at runtime by the engine’s full-screen movie player.
A variation on this style of animation is a semi-interactive sequence known
as a quick time event (QTE). In a QTE, the player must hit a button at the right
moment during an otherwise noninteractive sequence in order to see the suc-
cess animation and proceed; otherwise, a failure animation is played, and the
player must try again, possibly losing a life or suffering some other conse-
quence as a result.
11.4.1
The Local Timeline
We can think of every animation clip as having a local timeline, usually de-
noted by the independent variable t. At the start of a clip, t = 0, and at the
end, t = T, where T is the duration of the clip. Each unique value of the
variable t is called a time index. An example of this is shown in Figure 11.9.

558
11. Animation Systems
t = 0
t = (0.4)T
t = T
t = (0.8)T
Figure 11.9. The local timeline of an animation showing poses at selected time indices. Images
courtesy of Naughty Dog, Inc., © 2014/™SCEA.
11.4.1.1
Pose Interpolation and Continuous Time
It’s important to realize that the rate at which frames are displayed to the
viewer is not necessarily the same as the rate at which poses are created by
the animator. In both ﬁlm and game animation, the animator almost never
poses the character every 1/30 or 1/60 of a second. Instead, the animator
generates important poses known as key poses or key frames at speciﬁc times
within the clip, and the computer calculates the poses in between via linear or
curve-based interpolation. This is illustrated in Figure 11.10.
Because of the animation engine’s ability to interpolate poses (which we’ll
explore in depth later in this chapter), we can actually sample the pose of the
character at any time during the clip—not just on integer frame indices. In
other words, an animation clip’s timeline is continuous. In computer anima-
tion, the time variable t is a real (ﬂoating-point) number, not an integer.
Film animation doesn’t take full advantage of the continuous nature of
the animation timeline, because its frame rate is locked at exactly 24, 30 or 60
frames per second. In ﬁlm, the viewer sees the characters’ poses at frames 1,
2, 3 and so on—there’s never any need to ﬁnd a character’s pose on frame 3.7,
Figure 11.10. An animator creates a relatively small number of key poses, and the engine ﬁlls in the
rest of the poses via interpolation.

11.4. Clips
559
for example. So in ﬁlm animation, the animator doesn’t pay much (if any)
attention to how the character looks in between the integral frame indices.
In contrast, a real-time game’s frame rate always varies a little, depending
on how much load is currently being placed on the CPU and GPU. Also, game
animations are sometimes time-scaled in order to make the character appear to
move faster or slower than originally animated. So in a real-time game, an
animation clip is almost never sampled on integer frame numbers. In theory,
with a time scale of 1.0, a clip should be sampled at frames 1, 2, 3 and so
on. But in practice, the player might actually see frames 1.1, 1.9, 3.2 and so
on. And if the time scale is 0.5, then the player might actually see frames 1.1,
1.4, 1.9, 2.6, 3.2 and so on. A negative time scale can even be used to play
an animation in reverse. So in game animation, time is both continuous and
scalable.
11.4.1.2
Time Units
Because an animation’s timeline is continuous, time is best measured in units
of seconds. Time can also be measured in units of frames, presuming we de-
ﬁne the duration of a frame beforehand. Typical frame durations are 1/30 or
1/60 of a second for game animation. However, it’s important not to make
the mistake of deﬁning your time variable t as an integer that counts whole
frames. No matter which time units are selected, t should be a real (ﬂoating-
point) quantity, a ﬁxed-point number or an integer that measures very small
subframe time intervals. The goal is to have sufﬁcient resolution in your time
measurements for doing things like “tweening” between frames or scaling an
animation’s playback speed.
11.4.1.3
Frame versus Sample
Unfortunately, the term frame has more than one common meaning in the
game industry. This can lead to a great deal of confusion. Sometimes a frame
is taken to be a period of time that is 1/30 or 1/60 of a second in duration. But
in other contexts, the term frame is applied to a single point in time (e.g., we
might speak of the pose of the character “at frame 42”).
I personally prefer to use the term sample to refer to a single point in time,
and I reserve the word frame to describe a time period that is 1/30 or 1/60
of a second in duration. So for example, a one-second animation created at
a rate of 30 frames per second would consist of 31 samples and would be 30
frames in duration, as shown in Figure 11.11. The term “sample” comes from
the ﬁeld of signal processing. A continuous-time signal (i.e., a function f(t))
can be converted into a set of discrete data points by sampling that signal at
uniformly spaced time intervals. See Section 13.3.2.1 for more information on
sampling.

560
11. Animation Systems
26
27
28
29
30
1
2
3
4
5
...
31
Samples:
Frames:
30
29
28
27
26
6
5
4
3
2
1
Figure 11.11. A one-second animation sampled at 30 frames per second is 30 frames in duration
and consists of 31 samples.
11.4.1.4
Frames, Samples and Looping Clips
When a clip is designed to be played over and over repeatedly, we say it is
looped. If we imagine two copies of a 1 s (30-frame/31-sample) clip laid back-
to-front, then sample 31 of the ﬁrst clip will coincide exactly in time with sam-
ple 1 of the second clip, as shown in Figure 11.12. For a clip to loop properly,
then, we can see that the pose of the character at the end of the clip must
exactly match the pose at the beginning. This, in turn, implies that the last
sample of a looping clip (in our example, sample 31) is redundant. Many
game engines therefore omit the last sample of a looping clip.
This leads us to the following rules governing the number of samples and
frames in any animation clip:
•
If a clip is non-looping, an N-frame animation will have N + 1 unique
samples.
•
If a clip is looping, then the last sample is redundant, so an N-frame
animation will have N unique samples.
30
29
28
27
26
6
5
4
3
2
30
29
28
27
26
5
4
3
2
1
31
1
...
...
...
...
Figure 11.12. The last sample of a looping clip coincides in time with its ﬁrst sample and is, therefore,
redundant.
11.4.1.5
Normalized Time (Phase)
It is sometimes convenient to employ a normalized time unit u, such that u = 0
at the start of the animation, and u = 1 at the end, no matter what its duration
T may be. We sometimes refer to normalized time as the phase of the anima-
tion clip, because u acts like the phase of a sine wave when the animation is
looped. This is illustrated in Figure 11.13.

11.4. Clips
561
u = 0
u = 0.4
u = 1
u = 0.8
Figure 11.13. An animation clip, showing normalized time units. Images courtesy of Naughty Dog,
Inc., © 2014/™SCEA.
Normalized time is useful when synchronizing two or more animation
clips that are not necessarily of the same absolute duration. For example, we
might want to smoothly cross-fade from a 2-second (60-frame) run cycle into
a 3-second (90-frame) walk cycle. To make the cross-fade look good, we want
to ensure that the two animations remain synchronized at all times, so that the
feet line up properly in both clips. We can accomplish this by simply setting
the normalized start time of the walk clip, uwalk, to match the normalized time
index of the run clip, urun. We then advance both clips at the same normalized
rate so that they remain in sync. This is quite a bit easier and less error-prone
than doing the synchronization using the absolute time indices twalk and trun.
11.4.2
The Global Timeline
Just as every animation clip has a local timeline (whose clock starts at 0 at
the beginning of the clip), every character in a game has a global timeline
(whose clock starts when the character is ﬁrst spawned into the game world,
or perhaps at the start of the level or the entire game). In this book, we’ll use
the time variable τ to measure global time, so as not to confuse it with the
local time variable t.
We can think of playing an animation as simply mapping that clip’s local
timeline onto the character’s global timeline. For example, Figure 11.14 illus-
trates playing animation clip A starting at a global time of τstart = 102 seconds.
Clip A
t = 0 sec
5 sec
start
102 sec
105 sec
110 sec
Figure 11.14. Playing animation clip A starting at a global time of 102 seconds.

562
11. Animation Systems
Clip A
110 sec
start
102 sec
Clip A
...
105 sec
Figure 11.15. Playing a looping animation corresponds to laying down multiple back-to-back copies
of the clip.
As we saw above, playing a looping animation is like laying down an in-
ﬁnite number of back-to-front copies of the clip onto the global timeline. We
can also imagine looping an animation a ﬁnite number of times, which corre-
sponds to laying down a ﬁnite number of copies of the clip. This is illustrated
in Figure 11.15.
Time-scaling a clip makes it appear to play back more quickly or more
slowly than originally animated. To accomplish this, we simply scale the im-
age of the clip when it is laid down onto the global timeline. Time-scaling is
most naturally expressed as a playback rate, which we’ll denote R. For exam-
ple, if an animation is to play back at twice the speed (R = 2), then we would
scale the clip’s local timeline to one-half (1/R = 0.5) of its normal length when
mapping it onto the global timeline. This is shown in Figure 11.16.
Playing a clip in reverse corresponds to using a time scale of −1, as shown
in Figure 11.17.
start
R
(scale t by 1/R = 0.5)
t
t
t
Figure 11.16. Playing an animation at twice the speed corresponds to scaling its local timeline by a
factor of 1/2.
t = 5 sec
0 sec
start
102 sec
105 sec
110 sec
 Clip A
Clip A
t = 0 sec
5 sec
R = –1
(ﬂip t)
Figure 11.17. Playing a clip in reverse corresponds to a time scale of −1.

11.4. Clips
563
In order to map an animation clip onto a global timeline, we need the
following pieces of information about the clip:
•
its global start time τstart,
•
its playback rate R,
•
its duration T, and
•
the number of times it should loop, which we’ll denote N.
Given this information, we can map from any global time τ to the correspond-
ing local time t, and vice versa, using the following two relations:
t = (τ −τstart)R,
(11.2)
τ = τstart + 1
Rt.
If the animation doesn’t loop (N = 1), then we should clamp t into the
valid range [0, T] before using it to sample a pose from the clip:
t = clamp
h
(τ −τstart)R
i
T
0 .
If the animation loops forever (N = ∞), then we bring t into the valid
range by taking the remainder of the result after dividing by the duration T.
This is accomplished via the modulo operator (mod, or % in C/C++), as shown
below:
t =
 (τ −τstart)R

mod T.
If the clip loops a ﬁnite number of times (1 < N < ∞), we must ﬁrst clamp
t into the range [0, NT] and then modulo that result by T in order to bring t
into a valid range for sampling the clip:
t =

clamp
h
(τ −τstart)R
i
NT
0

mod T.
Most game engines work directly with local animation timelines and don’t
use the global timeline directly. However, working directly in terms of global
times can have some incredibly useful beneﬁts. For one thing, it makes syn-
chronizing animations trivial.
11.4.3
Comparison of Local and Global Clocks
The animation system must keep track of the time indices of every animation
that is currently playing. To do so, we have two choices:

564
11. Animation Systems
•
Local clock. In this approach, each clip has its own local clock, usually
represented by a ﬂoating-point time index stored in units of seconds
or frames, or in normalized time units (in which case it is often called
the phase of the animation). At the moment the clip begins to play, the
local time index t is usually taken to be zero. To advance the animations
forward in time, we advance the local clocks of each clip individually.
If a clip has a non-unit playback rate R, the amount by which its local
clock advances must be scaled by R.
•
Global clock. In this approach, the character has a global clock, usually
measured in seconds, and each clip simply records the global time at
which it started playing, τstart. The clips’ local clocks are calculated from
this information using Equation (11.3).
The local clock approach has the beneﬁt of being simple, and it is the most
obvious choice when designing an animation system. However, the global
clock approach has some distinct advantages, especially when it comes to syn-
chronizing animations, either within the context of a single character or across
multiple characters in a scene.
11.4.3.1
Synchronizing Animations with a Local Clock
With a local clock approach, we said that the origin of a clip’s local timeline
(t = 0) is usually deﬁned to coincide with the moment at which the clip starts
playing. Thus, to synchronize two or more clips, they must be played at ex-
actly the same moment in game time. This seems simple enough, but it can
become quite tricky when the commands used to play the animations are com-
ing from disparate engine subsystems.
For example, let’s say we want to synchronize the player character’s punch
animation with a non-player character’s corresponding hit reaction anima-
tion. The problem is that the player’s punch is initiated by the player subsys-
tem in response to detecting that a button was hit on the joy pad. Meanwhile,
the non-player character’s (NPC) hit reaction animation is played by the ar-
tiﬁcial intelligence (AI) subsystem. If the AI code runs before the player code
in the game loop, there will be a one-frame delay between the start of the
player’s punch and the start of the NPC’s reaction. And if the player code
runs before the AI code, then the opposite problem occurs when an NPC tries
to punch the player. If a message-passing (event) system is used to commu-
nicate between the two subsystems, additional delays might be incurred (see
Section 15.7 for more details). This problem is illustrated in Figure 11.18.
void GameLoop()
{
while (!quit)

11.4. Clips
565
{
// preliminary updates...
UpdateAllNpcs(); // react to punch event
// from last frame
// more updates...
UpdatePlayer(); // punch button hit - start punch
// anim, and send event to NPC to
// react
// still more updates...
}
}
Frame N+1
Frame N
NPC
Player
Update
Update
Update
send: Punch
Player 
Anim
NPC
Anim
play anim
play anim
Queue Event
Player Punch
(local t = 0)
request start (frame N)
Hit Reaction
(local t = 0)
start (frame N+1)
Figure 11.18.
The order of execution of disparate gameplay systems can introduce animation
synchronization problems when local clocks are used.
11.4.3.2
Synchronizing Animations with a Global Clock
A global clock approach helps to alleviate many of these synchronization
problems, because the origin of the timeline (τ = 0) is common across all
clips by deﬁnition. If two or more animations’ global start times are numer-
ically equal, the clips will start in perfect synchronization. If their playback
rates are also equal, then they will remain in sync with no drift. It no longer
matters when the code that plays each animation executes. Even if the AI code
that plays the hit reaction ends up running a frame later than the player’s
punch code, it is still trivial to keep the two clips in sync by simply noting the
global start time of the punch and setting the global start time of the reaction
animation to match it. This is shown in Figure 11.19.
Of course, we do need to ensure that the two character’s global clocks
match, but this is trivial to do. We can either adjust the global start times

566
11. Animation Systems
Frame N+1
Frame N
NPC
Player
Update
Update
Update
send: Punch
Player
Anim
NPC
Anim
play anim
play anim
Queue Event
Player Punch
(global start time: )
start at global 
(frame N)
NPC Hit Reaction
(global start time: )
start at global 
(frame N)
Figure 11.19. A global clock approach can alleviate animation synchronization problems.
to take account of any differences in the characters’ clocks, or we can simply
have all characters in the game share a single master clock.
11.4.4
A Simple Animation Data Format
Typically, animation data is extracted from a Maya scene ﬁle by sampling the
pose of the skeleton discretely at a rate of 30 or 60 samples per second. A sam-
ple comprises a full pose for each joint in the skeleton. The poses are usually
stored in SQT format: For each joint j, the scale component is either a single
ﬂoating-point scalar Sj or a three-element vector Sj =

Sjx
Sjy
Sjz

. The
rotational component is of course a four-element quaternion Qj = [Qjx Qjy
Qjz
Qjw]. And the translational component is a three-element vector Tj =
Tjx
Tjy
Tjz

. We sometimes say that an animation consists of up to 10
channels per joint, in reference to the 10 components of Sj, Qj, and Tj. This is
illustrated in Figure 11.20.
0
1
2
3
4
5
6
7
8
9
T0
Q0
S0
T1
Q1
S1
...
y
x
z
y
x
z
w
y
x
z
Figure 11.20.
An uncompressed animation clip contains 10 channels of ﬂoating-point data per
sample, per joint.

11.4. Clips
567
In C++, an animation clip can be represented in many different ways. Here
is one possibility:
struct JointPose { ... }; // SQT, defined as above
struct AnimationSample
{
JointPose*
m_aJointPose; // array of joint
// poses
};
struct AnimationClip
{
Skeleton*
m_pSkeleton;
F32
m_framesPerSecond;
U32
m_frameCount;
AnimationSample* m_aSamples; // array of samples
bool
m_isLooping;
};
An animation clip is authored for a speciﬁc skeleton and generally won’t
work on any other skeleton. As such, our example AnimationClip data
structure contains a reference to its skeleton, m_pSkeleton. (In a real en-
gine, this might be a unique skeleton id rather than a Skeleton* pointer. In
this case, the engine would presumably provide a way to quickly and conve-
niently look up a skeleton by its unique id.)
The number of JointPoses in the m_aJointPose array within each sam-
ple is presumed to match the number of joints in the skeleton. The number
of samples in the m_aSamples array is dictated by the frame count and by
whether or not the clip is intended to loop. For a non-looping animation,
the number of samples is (m_frameCount + 1). However, if the anima-
tion loops, then the last sample is identical to the ﬁrst sample and is usually
omitted. In this case, the sample count is equal to m_frameCount.
It’s important to realize that in a real game engine, animation data isn’t
actually stored in this simplistic format. As we’ll see in Section 11.8, the data
is usually compressed in various ways to save memory.
11.4.4.1
Animation Retargeting
We said above that an animation is typically only compatible with a single
skeleton. An exception to this rule can be made for skeletons that are closely
related. For example, if a group of skeletons are identical except for a number
of optional leaf joints that do not affect the fundamental hierarchy, then an an-
imation authored for one of these skeletons should work on any of them. The

568
11. Animation Systems
only requirement is that the engine be capable of ignoring animation channels
for joints that cannot be found in the skeleton being animated.
Other more-advanced techniques exist for retargeting animations authored
for one skeleton so that they work on a different skeleton. This is an active area
of research, and a full discussion of the topic is beyond the scope of this book.
For more information, see for example http://portal.acm.org/citation.cfm?
id=1450621 and http://chrishecker.com/Real-time_Motion_Retargeting_to_
Highly_Varied_User-Created_Morphologies.
11.4.5
Continuous Channel Functions
The samples of an animation clip are really just deﬁnitions of continuous func-
tions over time. You can think of these as 10 scalar-valued functions of time
per joint, or as two vector-valued functions and one quaternion-valued func-
tion per joint. Theoretically, these channel functions are smooth and continu-
ous across the entire clip’s local timeline, as shown in Figure 11.21 (with the
exception of explicitly authored discontinuities like camera cuts). In prac-
tice, however, many game engines interpolate linearly between the samples,
in which case the functions actually used are piecewise linear approximations to
the underlying continuous functions. This is depicted in Figure 11.22.
Figure 11.21. The animation samples in a clip deﬁne continuous functions over time.
t
Qy3
Figure 11.22. Many game engines use a piecewise linear approximation when interpolating channel
functions.

11.4. Clips
569
0
1
2
3
4
5
6
7
8
9
T0
Q0
S0
T1
Q1
S1
Footstep
Left
Footstep
Right
Reload
Weapon
Events
...
Figure 11.23.
A special event trigger channel can be added to an animation clip in order to
synchronize sound effects, particle effects and other game events with an animation.
11.4.6
Metachannels
Many games permit additional “metachannels” of data to be deﬁned for an
animation. These channels can encode game-speciﬁc information that doesn’t
have to do directly with posing the skeleton but which needs to be synchro-
nized with the animation.
It is quite common to deﬁne a special channel that contains event triggers at
various time indices, as shown in Figure 11.23. Whenever the animation’s lo-
cal time index passes one of these triggers, an event is sent to the game engine,
which can respond as it sees ﬁt. (We’ll discuss events in detail in Chapter 15.)
One common use of event triggers is to denote at which points during the
animation certain sound or particle effects should be played. For example,
when the left or right foot touches the ground, a footstep sound and a “cloud
of dust” particle effect could be initiated.
Another common practice is to permit special joints, known in Maya as
locators, to be animated along with the joints of the skeleton itself. Because a
joint or locator is just an afﬁne transform, these special joints can be used to
encode the position and orientation of virtually any object in the game.
A typical application of animated locators is to specify how the game’s
camera should be positioned and oriented during an animation. In Maya, a
locator is constrained to a camera, and the camera is then animated along with
the joints of the character(s) in the scene. The camera’s locator is exported and
used in-game to move the game’s camera around during the animation. The

570
11. Animation Systems
ﬁeld of view (focal length) of the camera, and possibly other camera attributes,
can also be animated by placing the relevant data into one or more additional
ﬂoating-point channels.
Other examples of non-joint animation channels include:
•
texture coordinate scrolling,
•
texture animation (a special case of texture coordinate scrolling in which
frames are arranged linearly within a texture, and the texture is scrolled
by one complete frame at each iteration),
•
animated material parameters (color, specularity, transparency, etc.),
•
animated lighting parameters (radius, cone angle, intensity, color, etc.),
and
•
any other parameters that need to change over time and are in some way
synchronized with an animation.
11.5
Skinning and Matrix Palette Generation
We’ve seen how to pose a skeleton by rotating, translating and possibly scal-
ing its joints. And we know that any skeletal pose can be represented math-
ematically as a set of local
 Pj→p(j)

or global (Pj→M) joint pose transforma-
tions, one for each joint j. Next, we will explore the process of attaching the
vertices of a 3D mesh to a posed skeleton. This process is known as skinning.
11.5.1
Per-Vertex Skinning Information
A skinned mesh is attached to a skeleton by means of its vertices. Each vertex
can be bound to one or more joints. If bound to a single joint, the vertex tracks
that joint’s movement exactly. If bound to two or more joints, the vertex’s
position becomes a weighted average of the positions it would have assumed
had it been bound to each joint independently.
To skin a mesh to a skeleton, a 3D artist must supply the following addi-
tional information at each vertex:
•
the index or indices of the joint(s) to which it is bound, and
•
for each joint, a weighting factor describing how much inﬂuence that joint
should have on the ﬁnal vertex position.
The weighting factors are assumed to add to one, as is customary when cal-
culating any weighted average.
Usually a game engine imposes an upper limit on the number of joints to
which a single vertex can be bound. A four-joint limit is typical for a number

11.5. Skinning and Matrix Palette Generation
571
of reasons. First, four 8-bit joint indices can be packed into a 32-bit word,
which is convenient. Also, while it’s pretty easy to see a difference in quality
between a two-, three- and even a four-joint-per-vertex model, most people
cannot see a quality difference as the number of joints per vertex is increased
beyond four.
Because the joint weights must sum to one, the last weight can be omitted
and often is. (It can be calculated at runtime as w3 = 1 −(w0 + w1 + w2).) As
such, a typical skinned vertex data structure might look as follows:
struct SkinnedVertex
{
float
m_position[3];
// (Px, Py, Pz)
float
m_normal[3];
// (Nx, Ny, Nz)
float
m_u, m_v;
// texture coords (u,v)
U8
m_jointIndex[4];
// joint indices
float
m_jointWeight[3]; // joint weights (last
// weight omitted)
};
11.5.2
The Mathematics of Skinning
The vertices of a skinned mesh track the movements of the joint(s) to which
they are bound. To make this happen mathematically, we would like to ﬁnd a
matrix that can transform the vertices of the mesh from their original positions
(in bind pose) into new positions that correspond to the current pose of the
skeleton. We shall call such a matrix a skinning matrix.
Like all mesh vertices, the position of a skinned vertex is speciﬁed in model
space. This is true whether its skeleton is in bind pose or in any other pose.
So the matrix we seek will transform vertices from model space (bind pose)
to model space (current pose). Unlike the other transforms we’ve seen thus
far, such as the model-to-world transform or the world-to-view transform, a
skinning matrix is not a change of basis transform. It morphs vertices into
new positions, but the vertices are in model space both before and after the
transformation.
11.5.2.1
Simple Example: One-Jointed Skeleton
Let us derive the basic equation for a skinning matrix. To keep things simple
at ﬁrst, we’ll work with a skeleton consisting of a single joint. We therefore
have two coordinate spaces to work with: model space, which we’ll denote
with the subscript M, and the joint space of our one and only joint, which
will be indicated by the subscript J. The joint’s coordinate axes start out in
bind pose, which we’ll denote with the superscript B. At any given moment

572
11. Animation Systems
xM
yM
xB
yB
xC
yC
Model Space Axes
Bind pose 
vertex position, 
in model space
Bind Pose 
Joint Space 
Axes
Current
Pose Joint 
Space Axes
Current pose 
vertex position, 
in model space
vM
B
vM
C
Figure 11.24. Bind pose and current pose of a simple, one-joint skeleton and a single vertex bound
to that joint.
during an animation, the joint’s axes move to a new position and orientation
in model space—we’ll indicate this current pose with the superscript C.
Now consider a single vertex that is skinned to our joint. In bind pose,
its model-space position is vB
M. The skinning process calculates the vertex’s
new model-space position in the current pose, vC
M. This is illustrated in Fig-
ure 11.24.
The “trick” to ﬁnding the skinning matrix for a given joint is to realize
that the position of a vertex bound to a joint is constant when expressed in
that joint’s coordinate space. So we take the bind-pose position of the vertex in
model space, convert it into joint space, move the joint into its current pose,
and ﬁnally convert the vertex back into model space. The net effect of this
round trip from model space to joint space and back again is to “morph” the
vertex from bind pose into the current pose.
Referring to the illustration in Figure 11.25, let’s assume that the coordi-
nates of the vertex vB
M are (4, 6) in model space (when the skeleton is in bind
pose). We convert this vertex into its equivalent joint-space coordinates vj,
which are roughly (1, 3) as shown in the diagram. Because the vertex is bound
to the joint, its joint-space coordinates will always be (1, 3) no matter how the
joint may move. Once we have the joint in the desired current pose, we con-
vert the vertex’s coordinates back into model space, which we’ll denote with
the symbol vC
M. In our diagram, these coordinates are roughly (18, 2). So the
skinning transformation has morphed our vertex from (4, 6) to (18, 2) in model
space, due entirely to the motion of the joint from its bind pose to the current
pose shown in the diagram.
Looking at the problem mathematically, we can denote the bind pose of the
joint j in model space by the matrix Bj→M. This matrix transforms a point or

11.5. Skinning and Matrix Palette Generation
573
xM
yM
xB
yB
xC
yC
1. Transform into
    joint space
vM
B
vM
C
v j
v j
3. Transform back
    into model space
2. Move joint into
    current pose
Figure 11.25. By transforming a vertex’s position into joint space, it can be made to “track” the
joint’s movements.
vector whose coordinates are expressed in joint j’s space into an equivalent
set of model-space coordinates. Now, consider a vertex whose coordinates
are expressed in model space with the skeleton in bind pose. To convert these
vertex coordinates into the space of joint j, we simply multiply it by the inverse
bind pose matrix, BM→j = (Bj→M)−1:
vj = vB
MBM→j = vB
M (Bj→M)−1 .
(11.3)
Likewise, we can denote the joint’s current pose (i.e., any pose that is not
bind pose) by the matrix Cj→M. To convert vj from joint space back into
model space, we simply multiply it by the current pose matrix as follows:
vC
M = vjCj→M.
If we expand vj using Equation (11.3), we obtain an equation that takes our
vertex directly from its position in bind pose to its position in the current pose:
vC
M = vjCj→M
= vB
M (Bj→M)−1 Cj→M
(11.4)
= vB
MKj.
The combined matrix Kj = (Bj→M)−1 Cj→M is known as a skinning matrix.
11.5.2.2
Extension to Multijointed Skeletons
In the example above, we considered only a single joint. However, the math
we derived above actually applies to any joint in any skeleton imaginable,
because we formulated everything in terms of global poses (i.e., joint space
to model space transforms).
To extend the above formulation to a skele-
ton containing multiple joints, we therefore need to make only two minor
adjustments:

574
11. Animation Systems
1.
We must make sure that our Bj→M and Cj→M matrices are calculated
properly for the joint in question, using Equation (11.1).
Bj→M and
Cj→M are just the bind pose and current pose equivalents, respectively,
of the matrix Pj→M used in that equation.
2.
We must calculate an array of skinning matrices Kj, one for each joint j.
This array is known as a matrix palette. The matrix palette is passed to the
rendering engine when rendering a skinned mesh. For each vertex, the
renderer looks up the appropriate joint’s skinning matrix in the palette
and uses it to transform the vertex from bind pose into current pose.
We should note here that the current pose matrix Cj→M changes every
frame as the character assumes different poses over time. However, the in-
verse bind-pose matrix is constant throughout the entire game, because the
bind pose of the skeleton is ﬁxed when the model is created. Therefore, the
matrix (Bj→M)−1 is generally cached with the skeleton, and needn’t be calcu-
lated at runtime. Animation engines generally calculate local poses for each
joint
 Cj→p(j)

, then use Equation (11.1) to convert these into global poses
(Cj→M), and ﬁnally multiply each global pose by the corresponding cached
inverse bind pose matrix (Bj→M)−1 in order to generate a skinning matrix
(Kj) for each joint.
11.5.2.3
Incorporating the Model-to-World Transform
Every vertex must eventually be transformed from model space into world
space. Some engines therefore premultiply the palette of skinning matrices by
the object’s model-to-world transform. This can be a useful optimization, as
it saves the rendering engine one matrix multiply per vertex when rendering
skinned geometry. (With hundreds of thousands of vertices to process, these
savings can really add up!)
To incorporate the model-to-world transform into our skinning matrices,
we simply concatenate it to the regular skinning matrix equation, as follows:
(Kj)W = (Bj→M)−1 Cj→MMM→W.
Some engines bake the model-to-world transform into the skinning matri-
ces like this, while others don’t. The choice is entirely up to the engineering
team and is driven by all sorts of factors. For example, one situation in which
we would deﬁnitely not want to do this is when a single animation is being ap-
plied to multiple characters simultaneously—a technique known as animation
instancing that is sometimes used for animating large crowds of characters. In
this case we need to keep the model-to-world transforms separate so that we
can share a single matrix palette across all characters in the crowd.

11.6. Animation Blending
575
11.5.2.4
Skinning a Vertex to Multiple Joints
When a vertex is skinned to more than one joint, we calculate its ﬁnal position
by assuming it is skinned to each joint individually, calculating a model-space
position for each joint and then taking a weighted average of the resulting posi-
tions. The weights are provided by the character rigging artist, and they must
always sum to one. (If they do not sum to one, they should be renormalized
by the tools pipeline.)
The general formula for a weighted average of N quantities a0 through
aN−1, with weights w0 through wN−1 and with P wi = 1 is:
a =
N−1
X
i=0
wiai.
This works equally well for vector quantities ai. So, for a vertex skinned to
N joints with indices j0 through jN−1 and weights w0 through wN−1, we can
extend Equation (11.4) as follows:
vC
M =
N−1
X
i=0
wivB
MKji,
where Kji is the skinning matrix for the joint ji.
11.6
Animation Blending
The term animation blending refers to any technique that allows more than one
animation clip to contribute to the ﬁnal pose of the character. To be more
precise, blending combines two or more input poses to produce an output pose
for the skeleton.
Blending usually combines two or more poses at a single point in time,
and generates an output at that same moment in time. In this context, blend-
ing is used to combine two or more animations into a host of new animations,
without having to create them manually. For example, by blending an injured
walk animation with an uninjured walk, we can generate various intermedi-
ate levels of apparent injury for our character while he is walking. As another
example, we can blend between an animation in which the character is aim-
ing to the left and one in which he’s aiming to the right, in order to make the
character aim along any desired angle between the two extremes. Blending
can be used to interpolate between extreme facial expressions, body stances,
locomotion modes and so on.
Blending can also be used to ﬁnd an intermediate pose between two known
poses at different points in time. This is used when we want to ﬁnd the pose

576
11. Animation Systems
of a character at a point in time that does not correspond exactly to one of the
sampled frames available in the animation data. We can also use temporal
animation blending to smoothly transition from one animation to another, by
gradually blending from the source animation to the destination over a short
period of time.
11.6.1
LERP Blending
Given a skeleton with N joints, and two skeletal poses Pskel
A
= {(PA)j} |N−1
j=0
and Pskel
B
= {(PB)j} |N−1
j=0 , we wish to ﬁnd an intermediate pose Pskel
LERP be-
tween these two extremes. This can be done by performing a linear interpola-
tion (LERP) between the local poses of each individual joint in each of the two
source poses. This can be written as follows:
(PLERP)j = LERP ((PA)j, (PB)j, β)
(11.5)
= (1 −β)(PA)j + β(PB)j.
The interpolated pose of the whole skeleton is simply the set of interpolated
poses for all of the joints:
Pskel
LERP =

(PLERP)j
	N−1
j=0 .
(11.6)
In these equations, β is called the blend percentage or blend factor. When
β = 0, the ﬁnal pose of the skeleton will exactly match Pskel
A ; when β = 1,
the ﬁnal pose will match Pskel
B . When β is between zero and one, the ﬁnal
pose is an intermediate between the two extremes. This effect is illustrated in
Figure 11.10.
We’ve glossed over one small detail here: We are linearly interpolating
joint poses, which means interpolating 4 × 4 transformation matrices. But, as
we saw in Chapter 4, interpolating matrices directly is not practical. This
is one of the reasons why local poses are usually expressed in SQT format—
doing so allows us to apply the LERP operation deﬁned in Section 4.2.5 to each
component of the SQT individually. The linear interpolation of the translation
component T of an SQT is just a straightforward vector LERP:
(TLERP)j = LERP ((TA)j, (TB)j, β)
(11.7)
= (1 −β)(TA)j + β(TB)j.
The linear interpolation of the rotation component is a quaternion LERP or
SLERP (spherical linear interpolation):
(QLERP)j = normalize (LERP ((QA)j, (QB)j, β))
(11.8)
= normalize ((1 −β)(QA)j + β(QB)j) .

11.6. Animation Blending
577
or
(QSLERP)j = SLERP ((QA)j, (QB)j, β)
(11.9)
= sin ((1 −β)θ)
sin(θ)
(QA)j + sin(βθ)
sin(θ) (QB)j.
Finally, the linear interpolation of the scale component is either a scalar or
vector LERP, depending on the type of scale (uniform or nonuniform scale)
supported by the engine:
(SLERP)j = LERP ((SA)j, (SB)j, β)
(11.10)
= (1 −β)(SA)j + β(SB)j.
or
(SLERP)j = LERP ((SA)j, (SB)j, β)
(11.11)
= (1 −β)(SA)j + β(SB)j.
When linearly interpolating between two skeletal poses, the most natural-
looking intermediate pose is generally one in which each joint pose is inter-
polated independently of the others, in the space of that joint’s immediate
parent. In other words, pose blending is generally performed on local poses. If
we were to blend global poses directly in model space, the results would tend
to look biomechanically implausible.
Because pose blending is done on local poses, the linear interpolation of
any one joint’s pose is totally independent of the interpolations of the other
joints in the skeleton. This means that linear pose interpolation can be per-
formed entirely in parallel on multiprocessor architectures.
11.6.2
Applications of LERP Blending
Now that we understand the basics of LERP blending, let’s have a look at
some typical gaming applications.
11.6.2.1
Temporal Interpolation
As we mentioned in Section 11.4.1.1, game animations are almost never sam-
pled exactly on integer frame indices. Because of variable frame rate, the
player might actually see frames 0.9, 1.85 and 3.02, rather than frames 1, 2
and 3 as one might expect. In addition, some animation compression tech-
niques involve storing only disparate key frames, spaced at uneven intervals
across the clip’s local timeline. In either case, we need a mechanism for ﬁnd-
ing intermediate poses between the sampled poses that are actually present in
the animation clip.
LERP blending is typically used to ﬁnd these intermediate poses. As an
example, let’s imagine that our animation clip contains evenly spaced pose

578
11. Animation Systems
samples at times 0, ∆t, 2∆t, 3∆t and so on. To ﬁnd a pose at time t = 2.18∆t,
we simply ﬁnd the linear interpolation between the poses at times 2∆t and
3∆t, using a blend percentage of β = 0.18.
In general, we can ﬁnd the pose at time t given pose samples at any two
times t1 and t2 that bracket t, as follows:
Pj(t) = LERP (Pj(t1), Pj(t2), β(t))
(11.12)
= (1 −β(t)) Pj(t1) + β(t)Pj(t2),
(11.13)
where the blend factor β(t) can be determined by the ratio
β(t) = t −t1
t2 −t1
.
(11.14)
11.6.2.2
Motion Continuity: Cross-Fading
Game characters are animated by piecing together a large number of ﬁne-
grained animation clips. If your animators are any good, the character will
appear to move in a natural and physically plausible way within each indi-
vidual clip. However, it is notoriously difﬁcult to achieve the same level of
quality when transitioning from one clip to the next. The vast majority of the
“pops” we see in game animations occur when the character transitions from
one clip to the next.
Ideally, we would like the movements of each part of a character’s body
to be perfectly smooth, even during transitions. In other words, the three-
dimensional paths traced out by each joint in the skeleton as it moves should
contain no sudden “jumps.” We call this C0 continuity; it is illustrated in Fig-
ure 11.26.
Not only should the paths themselves be continuous, but their ﬁrst deriva-
tives (velocity) should be continuous as well. This is called C1 continuity (or
continuity of velocity and momentum). The perceived quality and realism
of an animated character’s movement improves as we move to higher- and
higher-order continuity. For example, we might want to achieve C2 continu-
ity, in which the second derivatives of the motion paths (acceleration curves)
are also continuous.
t
Tx7
t
Tx7
discontinuity
C0 continuous
not C0 continuous
Figure 11.26. The channel function on the left has C0 continuity, while the path on the right does
not.

11.6. Animation Blending
579
Strict mathematical continuity up to C1 or higher is often infeasible to
achieve. However, LERP-based animation blending can be applied to achieve
a reasonably pleasing form of C0 motion continuity. It usually also does a
pretty good job of approximating C1 continuity. When applied to transitions
between clips in this manner, LERP blending is sometimes called cross-fading.
LERP blending can introduce unwanted artifacts, such as the dreaded “sliding
feet” problem, so it must be applied judiciously.
To cross-fade between two animations, we overlap the timelines of the
two clips by some reasonable amount, and then blend the two clips together.
The blend percentage β starts at zero at time tstart, meaning that we see only
clip A when the cross-fade begins. We gradually increase β until it reaches a
value of one at time tend. At this point only clip B will be visible, and we can
retire clip A altogether. The time interval over which the cross-fade occurs
(∆tblend = tend −tstart) is sometimes called the blend time.
Types of Cross-Fades
There are two common ways to perform a cross-blended transition:
•
Smooth transition. Clips A and B both play simultaneously as β increases
from zero to one. For this to work well, the two clips must be looping an-
imations, and their timelines must be synchronized so that the positions
of the legs and arms in one clip match up roughly with their positions
in the other clip. (If this is not done, the cross-fade will often look totally
unnatural.) This technique is illustrated in Figure 11.27.
•
Frozen transition. The local clock of clip A is stopped at the moment clip
B starts playing. Thus, the pose of the skeleton from clip A is frozen
while clip B gradually takes over the movement. This kind of transi-
tional blend works well when the two clips are unrelated and cannot be
Clip A
t
Clip B
tstart
tend
Figure 11.27. A smooth transition, in which the local clocks of both clips keep running during the
transition.

580
11. Animation Systems
Clip A
t
Clip B
A’s local timeline 
freezes here
tstart
tend
Figure 11.28. A frozen transition, in which clip A’s local clock is stopped during the transition.
time-synchronized, as they must be when performing a smooth transi-
tion. This approach is depicted in Figure 11.28.
We can also control how the blend factor β varies during the transition.
In Figure 11.27 and Figure 11.28, the blend factor varied linearly with time.
To achieve an even smoother transition, we could vary β according to a cubic
function of time, such as a one-dimensional Bézier. When such a curve is
applied to a currently running clip that is being blended out, it is known as
an ease-out curve; when it is applied to a new clip that is being blended in, it is
known as an ease-in curve. This is shown in Figure 11.29.
The equation for a Bézier ease-in/ease-out curve is given below. It returns
the value of β at any time t within the blend interval. βstart is the blend factor
at the start of the blend interval tstart, and βend is the ﬁnal blend factor at time
tend. The parameter u is the normalized time between tstart and tend, and for
convenience we’ll also deﬁne v = 1−u (the inverse normalized time). Note that
the Bézier tangents Tstart and Tend are taken to be equal to the corresponding
blend factors βstart and βend, because this yields a well-behaved curve for our
Clip A
t
Clip B
tstart
tend
Figure 11.29. A smooth transition, with a cubic ease-in/ease-out curve applied to the blend factor.

11.6. Animation Blending
581
purposes:
let u =
 t −tstart
tend −tstart

and v = 1 −u.
β(t) = (v3)βstart + (3v2u)Tstart + (3vu2)Tend + (u3)βend
= (v3 + 3v2u)βstart + (3vu2 + u3)βend.
Core Poses
This is an appropriate time to mention that motion continuity can actually be
achieved without blending if the animator ensures that the last pose in any
given clip matches the ﬁrst pose of the clip that follows it. In practice, anima-
tors often decide upon a set of core poses—for example, we might have a core
pose for standing upright, one for crouching, one for lying prone and so on.
By making sure that the character starts in one of these core poses at the begin-
ning of every clip and returns to a core pose at the end, C0 continuity can be
achieved by simply ensuring that the core poses match when animations are
spliced together. C1 or higher-order motion continuity can also be achieved
by ensuring that the character’s movement at the end of one clip smoothly
transitions into the motion at the start of the next clip. This can be achieved
by authoring a single smooth animation and then breaking it into two or more
clips.
11.6.2.3
Directional Locomotion
LERP-based animation blending is often applied to character locomotion.
When a real human being walks or runs, he can change the direction in which
he is moving in two basic ways: First, he can turn his entire body to change
direction, in which case he always faces in the direction he’s moving. I’ll call
this pivotal movement, because the person pivots about his vertical axis when
he turns. Second, he can keep facing in one direction while walking forward,
backward or sideways (known as straﬁng in the gaming world) in order to
move in a direction that is independent of his facing direction. I’ll call this
targeted movement, because it is often used in order to keep one’s eye—or one’s
weapon—trained on a target while moving. These two movement styles are
illustrated in Figure 11.30.
Targeted Movement
To implement targeted movement, the animator authors three separate looping
animation clips—one moving forward, one straﬁng to the left, and one straf-
ing to the right. I’ll call these directional locomotion clips. The three directional

582
11. Animation Systems
Targeted
Pivotal
Path of 
Movement
Figure 11.30.
In pivotal movement, the character faces the direction she is moving and pivots
about her vertical axis to turn. In targeted movement, the movement direction need not match
the facing direction.
clips are arranged around the circumference of a semicircle, with forward at
0 degrees, left at 90 degrees and right at −90 degrees. With the character’s
facing direction ﬁxed at 0 degrees, we ﬁnd the desired movement direction on
the semicircle, select the two adjacent movement animations and blend them
together via LERP-based blending. The blend percentage β is determined by
how close the angle of movement is to the angles of two adjacent clips. This
is illustrated in Figure 11.31.
Note that we did not include backward movement in our blend, for a full
circular blend. This is because blending between a sideways strafe and a back-
ward run cannot be made to look natural in general. The problem is that when
straﬁng to the left, the character usually crosses its right foot in front of its left
so that the blend into the pure forward run animation looks correct. Like-
wise, the right strafe is usually authored with the left foot crossing in front of
the right. When we try to blend such strafe animations directly into a back-
ward run, one leg will start to pass through the other, which looks extremely
awkward and unnatural. There are a number of ways to solve this problem.
Strafe
Right
Strafe
Left
Run
Forward
Figure 11.31. Targeted movement can be implemented by blending together looping locomotion
clips that move in each of the four principal directions.

11.6. Animation Blending
583
One feasible approach is to deﬁne two hemispherical blends, one for forward
motion and one for backward motion, each with strafe animations that have
been crafted to work properly when blended with the corresponding straight
run. When passing from one hemisphere to the other, we can play some kind
of explicit transition animation so that the character has a chance to adjust its
gait and leg crossing appropriately.
Pivotal Movement
To implement pivotal movement, we can simply play the forward locomotion
loop while rotating the entire character about its vertical axis to make it turn.
Pivotal movement looks more natural if the character’s body doesn’t remain
bolt upright when it is turning—real humans tend to lean into their turns a
little bit. We could try slightly tilting the vertical axis of the character as a
whole, but that would cause problems with the inner foot sinking into the
ground while the outer foot comes off the ground. A more natural-looking
result can be achieved by animating three variations on the basic forward walk
or run—one going perfectly straight, one making an extreme left turn and one
making an extreme right turn. We can then LERP-blend between the straight
clip and the extreme left turn clip to implement any desired lean angle.
11.6.3
Complex LERP Blends
In a real game engine, characters make use of a wide range of complex blends
for various purposes. It can be convenient to “prepackage” certain commonly
used types of complex blends for ease of use. In the following sections, we’ll
investigate a few popular types of prepackaged complex blends.
11.6.3.1
Generalized One-Dimensional LERP Blending
LERP blending can be easily extended to more than two animation clips, us-
ing a technique I call one-dimensional LERP blending. We deﬁne a new blend
parameter b that lies in any linear range desired (e.g., from −1 to +1, or from
0 to 1, or even from 27 to 136). Any number of clips can be positioned at arbi-
trary points along this range, as shown in Figure 11.32. For any given value of
b, we select the two clips immediately adjacent to it and blend them together
using Equation (11.5). If the two adjacent clips lie at points b1 and b2, then the
blend percentage β can be determined using a technique analogous to that
used in Equation (11.14), as follows:
β(t) = b −b1
b2 −b1
.
(11.15)

584
11. Animation Systems
Clip A
b0
b1
b2
b3
b4
Clip B
Clip C
Clip D Clip E
b
1
2
1
b
b
b
b
−
−
=
β
Figure 11.32. A generalized linear blend between N animation clips.
Strafe
Right
Strafe
Left
Run
Forward
Strafe
Right
b2
Run
Fwd
Strafe
Left
b
b3
b1
Figure 11.33. The directional clips used in targeted movement can be thought of as a special case
of one-dimensional LERP blending.
Targeted movement is just a special case of one-dimensional LERP blend-
ing. We simply straighten out the circle on which the directional animation
clips were placed and use the movement direction angle θ as the parameter
b (with a range of −90 to 90 degrees). Any number of animation clips can be
placed onto this blend range at arbitrary angles. This is shown in Figure 11.33.
11.6.3.2
Simple Two-Dimensional LERP Blending
Sometimes we would like to smoothly vary two aspects of a character’s motion
simultaneously. For example, we might want the character to be capable of
aiming his weapon vertically and horizontally. Or we might want to allow
our character to vary her pace length and the separation of her feet as she
moves. We can extend one-dimensional LERP blending to two dimensions in
order to achieve these kinds of effects.

11.6. Animation Blending
585
bx
by
Figure 11.34. A simple formulation for 2D animation blending between four clips at the corners of
a square region.
If we know that our 2D blend involves only four animation clips, and if
those clips are positioned at the four corners of a square region, then we can
ﬁnd a blended pose by performing two 1D blends. Our generalized blend
factor b becomes a two-dimensional blend vector b =
bx
by

. If b lies within
the square region bounded by our four clips, we can ﬁnd the resulting pose
by following these steps:
1.
Using the horizontal blend factor bx, ﬁnd two intermediate poses, one
between the top two animation clips and one between the bottom two
clips. These two poses can be found by performing two simple one-
dimensional LERP blends.
2.
Using the vertical blend factor by, ﬁnd the ﬁnal pose by LERP-blending
the two intermediate poses together.
This technique is illustrated in Figure 11.34.
11.6.3.3
Triangular Two-Dimensional LERP Blending
The simple 2D blending technique we investigated in the previous section
only works when the animation clips we wish to blend lie at the corners of a
rectangular region. How can we blend between an arbitrary number of clips
positioned at arbitrary locations in our 2D blend space?
Let’s imagine that we have three animation clips that we wish to blend
together. Each clip, designated by the index i, corresponds to a particular
blend coordinate bi =
bix
biy

in our two-dimensional blend space; these
three blend coordinates form a triangle within the blend space. Each of the
three clips deﬁnes a set of joint poses

(Pi)j
	N−1
j=0 , where (Pi)j is the pose

586
11. Animation Systems
Clip A
b0
by
Clip B
Clip C
b
b1
b2
bx
Final
Blend
Figure 11.35. Two-dimensional animation blending between three animation clips.
of joint j as deﬁned by clip i, and N is the number of joints in the skeleton.
We wish to ﬁnd the interpolated pose of the skeleton corresponding to an
arbitrary point b within the triangle, as illustrated in Figure 11.35.
But how can we calculate a LERP blend between three animation clips?
Thankfully, the answer is simple: the LERP function can actually operate on
any number of inputs, because it is really just a weighted average. As with any
weighted average, the weights must add to one. In the case of a two-input
LERP blend, we used the weights β and (1 −β), which of course add to one.
For a three-input LERP, we simply use three weights, α, β and γ = (1−α−β).
Then we calculate the LERP as follows:
(PLERP)j = α (P0)j + β (P1)j + γ (P2)j .
(11.16)
Given the two-dimensional blend vector b, we ﬁnd the blend weights α,
β and γ by ﬁnding the barycentric coordinates of the point b relative to the tri-
angle formed by the three clips in two-dimensional blend space (http://en.
wikipedia.org/wiki/Barycentric_coordinates_%28mathematics%29). In gen-
eral, the barycentric coordinates of a point b within a triangle with vertices
b1, b2 and b3 are three scalar values (α, β, γ) that satisfy the relations
b = αb0 + βb1 + γb2,
(11.17)
and
α + β + γ = 1.
These are exactly the weights we seek for our three-clip weighted average.
Barycentric coordinates are illustrated in Figure 11.36.
Note that plugging the barycentric coordinate (1, 0, 0) into Equation (11.17)
yields b0, while (0, 1, 0) gives us b1 and (0, 0, 1) produces b2. Likewise, plug-
ging these blend weights into Equation (11.16) gives us poses (P0)j, (P1)j
and (P2)j for each joint j, respectively. Furthermore, the barycentric coordi-

11.6. Animation Blending
587
b0
by
b
b1
b2
α
β
γ
bx
Figure 11.36. Various barycentric coordinates within a triangle.
nate ( 1
3, 1
3, 1
3) lies at the centroid of the triangle and gives us an equal blend
between the three poses. This is exactly what we’d expect.
11.6.3.4
Generalized Two-Dimensional LERP Blending
The barycentric coordinate technique can be extended to an arbitrary number
of animation clips positioned at arbitrary locations within the two-dimensional
blend space. We won’t describe it in its entirety here, but the basic idea is
to use a technique known as Delaunay triangulation (http://en.wikipedia.org/
wiki/Delaunay_triangulation) to ﬁnd a set of triangles given the positions
of the various animation clips bi. Once the triangles have been determined,
we can ﬁnd the triangle that encloses the desired point b and then perform
a three-clip LERP blend as described above.
This technique was used in
FIFA soccer by EA Sports in Vancouver, implemented within their proprietary
“ANT” animation framework. It is shown in Figure 11.37.
Clip A
b0
Clip B
b1
Clip C
Clip D
Clip E
Clip F
Clip G
Clip H
Clip I
Clip J
b2
b3
b4
b5
b6
b7
b8
b9
by
bx
Figure 11.37. Delaunay triangulation between an arbitrary number of animation clips positioned
at arbitrary locations in two-dimensional blend space.

588
11. Animation Systems
11.6.4
Partial-Skeleton Blending
A human being can control different parts of his or her body independently.
For example, I can wave my right arm while walking and pointing at some-
thing with my left arm. One way to implement this kind of movement in a
game is via a technique known as partial-skeleton blending.
Recall from Equations (11.5) and (11.6) that when doing regular LERP
blending, the same blend percentage β was used for every joint in the skele-
ton. Partial-skeleton blending extends this idea by permitting the blend per-
centage to vary on a per-joint basis. In other words, for each joint j, we deﬁne
a separate blend percentage βj. The set of all blend percentages for the entire
skeleton

βj
	N−1
j=0 is sometimes called a blend mask because it can be used to
“mask out” certain joints by setting their blend percentages to zero.
As an example, let’s say we want our character to wave at someone using
his right arm and hand. Moreover, we want him to be able to wave whether
he’s walking, running or standing still. To implement this using partial blend-
ing, the animator deﬁnes three full-body animations: Walk, Run and Stand.
The animator also creates a single waving animation, Wave. A blend mask
is created in which the blend percentages are zero everywhere except for the
right shoulder, elbow, wrist and ﬁnger joints, where they are equal to one:
βj =

1
when j within right arm,
0
otherwise.
When Walk, Run or Stand is LERP-blended with Wave using this blend mask,
the result is a character who appears to be walking, running or standing while
waving his right arm.
Partial blending is useful, but it has a tendency to make a character’s
movements look unnatural. This occurs for two basic reasons:
•
An abrupt change in the per-joint blend factors can cause the move-
ments of one part of the body to appear disconnected from the rest of
the body. In our example, the blend factors change abruptly at the right
shoulder joint. Hence the animation of the upper spine, neck and head
are being driven by one animation, while the right shoulder and arm
joints are being entirely driven by a different animation. This can look
odd. The problem can be mitigated somewhat by gradually changing
the blend factors rather than doing it abruptly. (In our example, we
might select a blend percentage of 0.9 at the right shoulder, 0.5 on the
upper spine and 0.2 on the neck and mid-spine.)
•
The movements of a real human body are never totally independent.
For example, one would expect a person’s wave to look more “bouncy”

11.6. Animation Blending
589
and out of control when he or she is running than when he or she is
standing still. Yet with partial blending, the right arm’s animation will
be identical no matter what the rest of the body is doing. This prob-
lem is difﬁcult to overcome using partial blending. Instead, many game
developers have turned to a more natural-looking technique known as
additive blending.
11.6.5
Additive Blending
Additive blending approaches the problem of combining animations in a to-
tally new way. It introduces a new kind of animation called a difference clip,
which, as its name implies, represents the difference between two regular an-
imation clips. A difference clip can be added onto a regular animation clip in
order to produce interesting variations in the pose and movement of the char-
acter. In essence, a difference clip encodes the changes that need to be made to
one pose in order to transform it into another pose. Difference clips are often
called additive animation clips in the game industry. We’ll stick with the term
difference clip in this book because it more accurately describes what is going
on.
Consider two input clips called the source clip (S) and the reference clip (R).
Conceptually, the difference clip is D = S−R. If a difference clip D is added to
its original reference clip, we get back the source clip (S = D+R). We can also
generate animations that are partway between R and S by adding a percent-
age of D to R, in much the same way that LERP blending ﬁnds intermediate
animations between two extremes. However, the real beauty of the additive
blending technique is that once a difference clip has been created, it can be
added to other unrelated clips, not just to the original reference clip. We’ll call
these animations target clips and denote them with the symbol T.
As an example, if the reference clip has the character running normally
and the source clip has him running in a tired manner, then the difference clip
will contain only the changes necessary to make the character look “tired”
while running. If this difference clip is now applied to a clip of the charac-
ter walking, the resulting animation can make the character look tired while
walking. A whole host of interesting and very natural-looking animations can
be created by adding a single difference clip onto various “regular” animation
clips, or a collection of difference clips can be created, each of which produces
a different effect when added to a single target animation.
11.6.5.1
Mathematical Formulation
A difference animation D is deﬁned as the difference between some source
animation S and some reference animation R. So conceptually, the difference

590
11. Animation Systems
pose (at a single point in time) is D = S −R. Of course, we’re dealing with
joint poses, not scalar quantities, so we cannot simply subtract the poses. In
general, a joint pose is a 4 × 4 afﬁne transformation matrix that transforms
points and vectors from the child joint’s local space to the space of its parent
joint. The matrix equivalent of subtraction is multiplication by the inverse
matrix. So given the source pose Sj and the reference pose Rj for any joint j
in the skeleton, we can deﬁne the difference pose Dj at that joint as follows.
(For this discussion, we’ll drop the C →P or j →p(j) subscript, as it is
understood that we are dealing with child-to-parent pose matrices.)
Dj = SjR−1
j .
“Adding” a difference pose Dj onto a target pose Tj yields a new additive
pose Aj. This is achieved by simply concatenating the difference transform
and the target transform as follows:
Aj = DjTj =
 SjR−1
j

Tj.
(11.18)
We can verify that this is correct by looking at what happens when the differ-
ence pose is “added” back onto the original reference pose:
Aj = DjRj
= SjR−1
j Rj
= Sj.
In other words, adding the difference animation D back onto the original ref-
erence animation R yields the source animation S, as we’d expect.
Temporal Interpolation of Difference Clips
As we learned in Section 11.4.1.1, game animations are almost never sampled
on integer frame indices. To ﬁnd a pose at an arbitrary time t, we must of-
ten temporally interpolate between adjacent pose samples at times t1 and t2.
Thankfully, difference clips can be temporally interpolated just like their non-
additive counterparts. We can simply apply Equations (11.12) and (11.14) di-
rectly to our difference clips as if they were ordinary animations.
Note that a difference animation can only be found when the input clips
S and R are of the same duration. Otherwise there would be a period of time
during which either S or R is undeﬁned, meaning D would be undeﬁned as
well.
Additive Blend Percentage
In games, we often wish to blend in only a percentage of a difference anima-
tion to achieve varying degrees of the effect it produces. For example, if a

11.6. Animation Blending
591
difference clip causes the character to turn his head 80 degrees to the right,
blending in 50% of the difference clip should make him turn his head only
40 degrees to the right.
To accomplish this, we turn once again to our old friend LERP. We wish
to interpolate between the unaltered target animation and the new animation
that would result from a full application of the difference animation. To do
this, we extend Equation (11.18) as follows:
Aj = LERP (Tj, DjTj, β)
(11.19)
= (1 −β) (Tj) + β (DjTj) .
As we saw in Chapter 4, we cannot LERP matrices directly. So Equation
(11.16) must be broken down into three separate interpolations for S, Q and T,
just as we did in Equations (11.7) through (11.11).
11.6.5.2
Additive Blending versus Partial Blending
Additive blending is similar in some ways to partial blending. For example,
we can take the difference between a standing clip and a clip of standing while
waving the right arm. The result will be almost the same as using a partial
blend to make the right arm wave. However, additive blends suffer less from
the “disconnected” look of animations combined via partial blending. This
is because, with an additive blend, we are not replacing the animation for
a subset of joints or interpolating between two potentially unrelated poses.
Rather, we are adding movement to the original animation—possibly across
the entire skeleton. In effect, a difference animation “knows” how to change a
character’s pose in order to get him to do something speciﬁc, like being tired,
aiming his head in a certain direction, or waving his arm. These changes can
be applied to a reasonably wide variety of animations, and the result often
looks very natural.
11.6.5.3
Limitations of Additive Blending
Of course, additive animation is not a silver bullet. Because it adds move-
ment to an existing animation, it can have a tendency to over-rotate the joints
in the skeleton, especially when multiple difference clips are applied simulta-
neously. As a simple example, imagine a target animation in which the char-
acter’s left arm is bent at a 90 degree angle. If we add a difference animation
that also rotates the elbow by 90 degrees, then the net effect would be to rotate
the arm by 90 + 90 = 180 degrees. This would cause the lower arm to inter-
penetrate the upper arm—not a comfortable position for most individuals!
Clearly we must be careful when selecting the reference clip and also when
choosing the target clips to which to apply it. Here are some simple rules of
thumb:

592
11. Animation Systems
•
Keep hip rotations to a minimum in the reference clip.
•
The shoulder and elbow joints should usually be in neutral poses in the
reference clip to minimize over-rotation of the arms when the difference
clip is added to other targets.
•
Animators should create a new difference animation for each core pose
(e.g., standing upright, crouched down, lying prone, etc.). This allows
the animator to account for the way in which a real human would move
when in each of these stances.
These rules of thumb can be a helpful starting point, but the only way to
really learn how to create and apply difference clips is by trial and error or by
apprenticing with animators or engineers who have experience creating and
applying difference animations. If your team hasn’t used additive blending
in the past, expect to spend a signiﬁcant amount of time learning the art of
additive blending.
11.6.6
Applications of Additive Blending
11.6.6.1
Stance Variation
One particularly striking application of additive blending is stance variation.
For each desired stance, the animator creates a one-frame difference anima-
tion. When one of these single-frame clips is additively blended with a base
animation, it causes the entire stance of the character to change drastically
while he continues to perform the fundamental action he’s supposed to per-
form. This idea is illustrated in Figure 11.38.
Target +
Difference A
Target +
Difference B
Target Clip
(and Reference)
Figure 11.38. Two single-frame difference animations A and B can cause a target animation clip
to assume two totally different stances. (Character from Uncharted: Drake’s Fortune, © 2007/®
SCEA. Created and developed by Naughty Dog.)

11.6. Animation Blending
593
Target Clip
(and Reference)
Target +
Difference A
Target +
Difference B
Target +
Difference C
Figure 11.39. Additive blends can be used to add variation to a repetitive idle animation. Images
courtesy of Naughty Dog, Inc., © 2014/™SCEA.
11.6.6.2
Locomotion Noise
Real humans don’t run exactly the same way with every footfall—there is
variation in their movement over time. This is especially true if the person
is distracted (for example, by attacking enemies). Additive blending can be
used to layer randomness, or reactions to distractions, on top of an otherwise
entirely repetitive locomotion cycle. This is illustrated in Figure 11.39.
11.6.6.3
Aim and Look-At
Another common use for additive blending is to permit the character to look
around or to aim his weapon. To accomplish this, the character is ﬁrst ani-
mated doing some action, such as running, with his head or weapon facing
straight ahead. Then the animator changes the direction of the head or the
aim of the weapon to the extreme right and saves off a one-frame or multi-
frame difference animation. This process is repeated for the extreme left, up
and down directions. These four difference animations can then be additively
blended onto the original straight-ahead animation clip, causing the character
to aim right, left, up, down or anywhere in between.
The angle of the aim is governed by the additive blend factor of each clip.
For example, blending in 100% of the right additive causes the character to
aim as far right as possible. Blending 50% of the left additive causes him to

594
11. Animation Systems
Target +
Difference Right
Target +
Difference Left
Target Clip
(and Reference)
0% Right
0% Left
100% Right
100% Left
Figure 11.40. Additive blending can be used to aim a weapon. Screenshots courtesy of Naughty
Dog, Inc., © 2014/™SCEA.
aim at an angle that is one-half of his leftmost aim. We can also combine
this with an up or down additive to aim diagonally. This is demonstrated in
Figure 11.40.
11.6.6.4
Overloading the Time Axis
It’s interesting to note that the time axis of an animation clip needn’t be used
to represent time. For example, a three-frame animation clip could be used to
provide three aim poses to the engine—a left aim pose on frame 1, a forward
aim pose on frame 2 and a right aim pose on frame 3. To make the character
aim to the right, we can simply ﬁx the local clock of the aim animation on
frame 3. To perform a 50% blend between aiming forward and aiming right,
we can dial in frame 2.5. This is a great example of leveraging existing features
of the engine for new purposes.
11.7
Post-Processing
Once a skeleton as been posed by one or more animation clips and the results
have been blended together using linear interpolation or additive blending, it
is often necessary to modify the pose prior to rendering the character. This is
called animation post-processing. In this section, we’ll look at a few of the most
common kinds of animation post-processing.

11.7. Post-Processing
595
11.7.1
Procedural Animations
A procedural animation is any animation generated at runtime rather than being
driven by data exported from an animation tool such as Maya. Sometimes,
hand-animated clips are used to pose the skeleton initially, and then the pose
is modiﬁed in some way via procedural animation as a post-processing step.
A procedural animation can also be used as an input to the system in place of
a hand-animated clip.
For example, imagine that a regular animation clip is used to make a ve-
hicle appear to be bouncing up and down on the terrain as it moves. The
direction in which the vehicle travels is under player control. We would like
to adjust the rotation of the front wheels and steering wheel so that they move
convincingly when the vehicle is turning. This can be done by post-processing
the pose generated by the animation. Let’s assume that the original animation
has the front tires pointing straight ahead and the steering wheel in a neutral
position. We can use the current angle of turn to create a quaternion about
the vertical axis that will deﬂect the front tires by the desired amount. This
quaternion can be multiplied with the front tire joints’ Q channel to produce
the ﬁnal pose of the tires. Likewise, we can generate a quaternion about the
axis of the steering column and multiply it into the steering wheel joint’s Q
channel to deﬂect it. These adjustments are made to the local pose, prior to
global pose calculation and matrix palette generation (see Section 11.5).
As another example, let’s say that we wish to make the trees and bushes
in our game world sway naturally in the wind and get brushed aside when
characters move through them. We can do this by modeling the trees and
bushes as skinned meshes with simple skeletons. Procedural animation can
be used, in place of or in addition to hand-animated clips, to cause the joints
to move in a natural-looking way. We might apply one or more sinusoids, or
a Perlin noise function, to the rotation of various joints to make them sway in
the breeze, and when a character moves through a region containing a bush
or grass, we can deﬂect its root joint quaternion radially outward to make it
appear to be pushed over by the character.
11.7.2
Inverse Kinematics
Let’s say we have an animation clip in which a character leans over to pick up
an object from the ground. In Maya, the clip looks great, but in our production
game level, the ground is not perfectly ﬂat, so sometimes the character’s hand
misses the object or appears to pass through it. In this case, we would like
to adjust the ﬁnal pose of the skeleton so that the hand lines up exactly with
the target object. A technique known as inverse kinematics (IK) can be used to
make this happen.

596
11. Animation Systems
Figure 11.41. Inverse kinematics attempts to bring an end effector joint into a target global pose
by minimizing the error between them.
A regular animation clip is an example of forward kinematics (FK). In for-
ward kinematics, the input is a set of local joint poses, and the output is a
global pose and a skinning matrix for each joint. Inverse kinematics goes in
the other direction: The input is the desired global pose of a single joint, which
is known as the end effector. We solve for the local poses of other joints in the
skeleton that will bring the end effector to the desired location.
Mathematically, IK boils down to an error minimization problem. As with
most minimization problems, there might be one solution, many or none at
all. This makes intuitive sense: If I try to reach a doorknob that is on the
other side of the room, I won’t be able to reach it without walking over to it.
IK works best when the skeleton starts out in a pose that is reasonably close
to the desired target. This helps the algorithm to focus in on the “closest”
solution and to do so in a reasonable amount of processing time. Figure 11.41
shows IK in action.
Imagine a two-joint skeleton, each of which can rotate only about a single
axis. The rotation of these two joints can be described by a two-dimensional
angle vector θ =
θ1
θ2

. The set of all possible angles for our two joints
forms a two-dimensional space called conﬁguration space. Obviously, for more-
complex skeletons with more degrees of freedom per joint, conﬁguration space
becomes multidimensional, but the concepts described here work equally well
no matter how many dimensions we have.
Now imagine plotting a three-dimensional graph, where for each combi-
nation of joint rotations (i.e., for each point in our two-dimensional conﬁgu-
ration space), we plot the distance from the end effector to the desired target.
An example of this kind of plot is shown in Figure 11.42. The “valleys” in
this three-dimensional surface represent regions in which the end effector is
as close as possible to the target. When the height of the surface is zero, the
end effector has reached its target. Inverse kinematics, then, attempts to ﬁnd
minima (low points) on this surface.

11.8. Compression Techniques
597
 1
 2
dtarget
Minimum
Figure 11.42. A three-dimensional plot of the distance from the end effector to the target for each
point in two-dimensional conﬁguration space. IK ﬁnds the local minimum.
We won’t get into the details of solving the IK minimization problem here.
You can read more about IK at http://en.wikipedia.org/wiki/Inverse_
kinematics and in Jason Weber’s article, “Constrained Inverse Kinematics”
[42].
11.7.3
Rag Dolls
A character’s body goes limp when he dies or becomes unconscious. In such
situations, we want the body to react in a physically realistic way with its
surroundings. To do this, we can use a rag doll. A rag doll is a collection
of physically simulated rigid bodies, each one representing a semi-rigid part
of the character’s body, such as his lower arm or his upper leg. The rigid
bodies are constrained to one another at the joints of the character in such a
way as to produce natural-looking “lifeless” body movement. The positions
and orientations of the rigid bodies are determined by the physics system and
are then used to drive the positions and orientations of certain key joints in
the character’s skeleton. The transfer of data from the physics system to the
skeleton is typically done as a post-processing step.
To really understand rag doll physics, we must ﬁrst have an understand-
ing of how the collision and physics systems work. Rag dolls are covered in
more detail in Sections 12.4.8.7 and 12.5.3.8.
11.8
Compression Techniques
Animation data can take up a lot of memory. A single joint pose might be
composed of ten ﬂoating-point channels (three for translation, four for rota-
tion and up to three more for scale). Assuming each channel contains a 4-

598
11. Animation Systems
byte ﬂoating-point value, a one-second clip sampled at 30 samples per second
would occupy 4 bytes × 10 channels × 30 samples/second = 1200 bytes per
joint per second, or a data rate of about 1.17 KiB per joint per second. For
a 100-joint skeleton (which is small by today’s standards), an uncompressed
animation clip would occupy 117 KiB per joint per second. If our game con-
tained 1,000 seconds of animation (which is on the low side for a modern
game), the entire data set would occupy a whopping 114.4 MiB. That’s quite
a lot, considering that a PlayStation 3 has only 256 MiB of main RAM and
256 MiB of video RAM. Sure, the PS4 has 8 GiB of RAM. But even still—we
would rather have much richer animations with a lot more variety than waste
memory unnecessarily. Therefore, game engineers invest a signiﬁcant amount
of effort into compressing animation data in order to permit the maximum
richness and variety of movement at the minimum memory cost.
11.8.1
Channel Omission
One simple way to reduce the size of an animation clip is to omit channels
that are irrelevant. Many characters do not require nonuniform scaling, so
the three scale channels can be reduced to a single uniform scale channel. In
some games, the scale channel can actually be omitted altogether for all joints
(except possibly the joints in the face). The bones of a humanoid character
generally cannot stretch, so translation can often be omitted for all joints ex-
cept the root, the facial joints and sometimes the collar bones. Finally, because
quaternions are always normalized, we can store only three components per
quat (e.g., x, y and z) and reconstruct the fourth component (e.g., w) at run-
time.
As a further optimization, channels whose pose does not change over the
course of the entire animation can be stored as a single sample at time t = 0
plus a single bit indicating that the channel is constant for all other values of t.
Channel omission can signiﬁcantly reduce the size of an animation clip. A
100-joint character with no scale and no translation requires only 303 chan-
nels—three channels for the quaternions at each joint, plus three channels for
the root joint’s translation. Compare this to the 1,000 channels that would be
required if all ten channels were included for all 100 joints.
11.8.2
Quantization
Another way to reduce the size of an animation is to reduce the size of each
channel. A ﬂoating-point value is normally stored in 32-bit IEEE format. This
format provides 23 bits of precision in the mantissa and an 8-bit exponent.
However, it’s often not necessary to retain that kind of precision and range in
an animation clip. When storing a quaternion, the channel values are guaran-

11.8. Compression Techniques
599
teed to lie in the range [−1, 1]. At a magnitude of 1, the exponent of a 32-bit
IEEE ﬂoat is zero, and 23 bits of precision give us accuracy down to the sev-
enth decimal place. Experience shows that a quaternion can be encoded well
with only 16 bits of precision, so we’re really wasting 16 bits per channel if we
store our quats using 32-bit ﬂoats.
Converting a 32-bit IEEE ﬂoat into an n-bit integer representation is called
quantization. There are actually two components to this operation: Encoding is
the process of converting the original ﬂoating-point value to a quantized in-
teger representation. Decoding is the process of recovering an approximation
to the original ﬂoating-point value from the quantized integer. (We can only
recover an approximation to the original data—quantization is a lossy compres-
sion method because it effectively reduces the number of bits of precision used
to represent the value.)
To encode a ﬂoating-point value as an integer, we ﬁrst divide the valid
range of possible input values into N equally sized intervals. We then deter-
mine within which interval a particular ﬂoating-point value lies and represent
that value by the integer index of its interval. To decode this quantized value,
we simply convert the integer index into ﬂoating-point format and shift and
scale it back into the original range. N is usually chosen to correspond to the
range of possible integer values that can be represented by an n-bit integer.
For example, if we’re encoding a 32-bit ﬂoating-point value as a 16-bit integer,
the number of intervals would be N = 216 = 65,536.
Jonathan Blow wrote an excellent article on the topic of ﬂoating-point scalar
quantization in the Inner Product column of Game Developer Magazine, a-
vailable at http://number-none.com/product/Scalar%20Quantization/index.
html. The article presents two ways to map a ﬂoating-point value to an inter-
val during the encoding process: We can either truncate the ﬂoat to the next
lowest interval boundary (T encoding), or we can round the ﬂoat to the center
of the enclosing interval (R encoding). Likewise, it describes two approaches to
reconstructing the ﬂoating-point value from its integer representation: We can
either return the value of the left-hand side of the interval to which our original
value was mapped (L reconstruction), or we can return the value of the center
of the interval (C reconstruction). This gives us four possible encode/decode
methods: TL, TC, RL and RC. Of these, TL and RC are to be avoided because
they tend to remove or add energy to the data set, which can often have disas-
trous effects. TC has the beneﬁt of being the most efﬁcient method in terms of
bandwidth, but it suffers from a severe problem—there is no way to represent
the value zero exactly. (If you encode 0.0f, it becomes a small positive value
when decoded.) RL is therefore usually the best choice and is the method
we’ll demonstrate here.

600
11. Animation Systems
The article only talks about quantizing positive ﬂoating-point values, and
in the examples, the input range is assumed to be [0, 1] for simplicity. How-
ever, we can always shift and scale any ﬂoating-point range into the range
[0, 1]. For example, the range of quaternion channels is [−1, 1], but we can
convert this to the range [0, 1] by adding one and then dividing by two.
The following pair of routines encode and decode an input ﬂoating-point
value lying in the range [0, 1] into an n-bit integer, according to Jonathan
Blow’s RL method. The quantized value is always returned as a 32-bit un-
signed integer (U32), but only the least-signiﬁcant n bits are actually used, as
speciﬁed by the nBits argument. For example, if you pass nBits==16, you
can safely cast the result to a U16.
U32 CompressUnitFloatRL(F32 unitFloat, U32 nBits)
{
// Determine the number of intervals based on the
// number of output bits we've been asked to produce.
U32 nIntervals = 1u << nBits;
// Scale the input value from the range [0, 1] into
// the range [0, nIntervals - 1]. We subtract one
// interval because we want the largest output value
// to fit into nBits bits.
F32 scaled = unitFloat * (F32)(nIntervals - 1u);
// Finally, round to the nearest interval center. We
// do this by adding 0.5f and then truncating to the
// next-lowest interval index (by casting to U32).
U32 rounded = (U32)(scaled + 0.5f);
// Guard against invalid input values.
if (rounded > nIntervals - 1u)
rounded = nIntervals - 1u;
return rounded;
}
F32 DecompressUnitFloatRL(U32 quantized, U32 nBits)
{
// Determine the number of intervals based on the
// number of bits we used when we encoded the value.
U32 nIntervals = 1u << nBits;
// Decode by simply converting the U32 to an F32, and
// scaling by the interval size.
F32 intervalSize = 1.0f / (F32)(nIntervals - 1u);
F32 approxUnitFloat = (F32)quantized * intervalSize;

11.8. Compression Techniques
601
return approxUnitFloat;
}
To handle arbitrary input values in the range [min, max], we can use these
routines:
U32 CompressFloatRL(F32 value, F32 min, F32 max,
U32 nBits)
{
F32 unitFloat = (value - min) / (max - min);
U32 quantized = CompressUnitFloatRL(unitFloat,
nBits);
return quantized;
}
F32 DecompressFloatRL(U32 quantized, F32 min, F32 max,
U32 nBits)
{
F32 unitFloat = DecompressUnitFloatRL(quantized,
nBits);
F32 value = min + (unitFloat * (max - min));
return value;
}
Let’s return to our original problem of animation channel compression.
To compress and decompress a quaternion’s four components into 16 bits per
channel, we simply call CompressFloatRL() and DecompressFloatRL()
with min = −1, max = 1 and n = 16:
inline U16 CompressRotationChannel(F32 qx)
{
return (U16)CompressFloatRL(qx, -1.0f, 1.0f, 16u);
}
inline F32 DecompressRotationChannel(U16 qx)
{
return DecompressFloatRL((U32)qx, -1.0f, 1.0f, 16u);
}
Compression of translation channels is a bit trickier than rotations, because
unlike quaternion channels, the range of a translation channel could theoreti-
cally be unbounded. Thankfully, the joints of a character don’t move very far
in practice, so we can decide upon a reasonable range of motion and ﬂag an
error if we ever see an animation that contains translations outside the valid

602
11. Animation Systems
range. In-game cinematics are an exception to this rule—when an IGC is ani-
mated in world space, the translations of the characters’ root joints can grow
very large. To address this, we can select the range of valid translations on
a per-animation or per-joint basis, depending on the maximum translations
actually achieved within each clip. Because the data range might differ from
animation to animation, or from joint to joint, we must store the range with the
compressed clip data. This will add a tiny amount of data to each animation
clip, but the impact is generally negligible.
// We'll use a 2 m range -- your mileage may vary.
F32 MAX_TRANSLATION = 2.0f;
inline U16 CompressTranslationChannel(F32 vx)
{
// Clamp to valid range...
if (vx < -MAX_TRANSLATION)
vx = -MAX_TRANSLATION;
if (vx > MAX_TRANSLATION)
vx = MAX_TRANSLATION;
return (U16)CompressFloatRL(vx,
-MAX_TRANSLATION, MAX_TRANSLATION, 16);
}
inline F32 DecompressTranslationChannel(U16 vx)
{
return DecompressFloatRL((U32)vx,
-MAX_TRANSLATION, MAX_TRANSLATION, 16);
}
11.8.3
Sampling Frequency and Key Omission
Animation data tends to be large for three reasons: ﬁrst, because the pose of
each joint can contain upwards of ten channels of ﬂoating-point data; second,
because a skeleton contains a large number of joints (250 or more for a hu-
manoid character on PS3 or Xbox 360, and more than 800 on some PS4 and
Xbox One games); third, because the pose of the character is typically sam-
pled at a high rate (e.g., 30 frames per second). We’ve seen some ways to
address the ﬁrst problem. We can’t really reduce the number of joints for our
high-resolution characters, so we’re stuck with the second problem. To attack
the third problem, we can do two things:
•
Reduce the sample rate overall. Some animations look ﬁne when exported
at 15 samples per second, and doing so cuts the animation data size in
half.

11.8. Compression Techniques
603
•
Omit some of the samples. If a channel’s data varies in an approximately
linear fashion during some interval of time within the clip, we can omit
all of the samples in this interval except the endpoints. Then, at runtime,
we can use linear interpolation to recover the dropped samples.
The latter technique is a bit involved, and it requires us to store informa-
tion about the time of each sample. This additional data can erode the savings
we achieved by omitting samples in the ﬁrst place. However, some game en-
gines have used this technique successfully.
11.8.4
Curve-Based Compression
One of the most powerful, easiest-to-use and best-thought-out animation APIs
I’ve ever worked with is Granny, by Rad Game Tools. Granny stores anima-
tions not as a regularly spaced sequence of pose samples but as a collection of
nth-order, nonuniform, nonrational B-splines, describing the paths of a joint’s
S, Q and T channels over time. Using B-splines allows channels with a lot of
curvature to be encoded using only a few data points.
Granny exports an animation by sampling the joint poses at regular in-
tervals, much like traditional animation data. For each channel, Granny then
ﬁts a set of B-splines to the sampled data set to within a user-speciﬁed toler-
ance. The end result is an animation clip that is usually signiﬁcantly smaller
than its uniformly sampled, linearly interpolated counterpart. This process is
illustrated in Figure 11.43.
t
Qx1
Figure 11.43. One form of animation compression ﬁts B-splines to the animation channel data.
11.8.5
Selective Loading and Streaming
The cheapest animation clip is the one that isn’t in memory at all. Most games
don’t need every animation clip to be in memory simultaneously. Some clips
apply only to certain classes of character, so they needn’t be loaded during
levels in which that class of character is never encountered. Other clips ap-
ply to one-off moments in the game. These can be loaded or streamed into

604
11. Animation Systems
memory just before being needed and dumped from memory once they have
played.
Most games load a core set of animation clips into memory when the game
ﬁrst boots and keep them there for the duration of the game. These include
the player character’s core move set and animations that apply to objects that
reappear over and over throughout the game, such as weapons or power-ups.
All other animations are usually loaded on an as-needed basis. Some game
engines load animation clips individually, but many package them together
into logical groups that can be loaded and unloaded as a unit.
11.9
Animation System Architecture
Now that we understand the theory that underlies a game’s animation sys-
tem, let’s turn our attention to how such a system is structured from a soft-
ware architecture standpoint. We’ll also investigate what kinds of interfaces
exist between the animation system and the other systems in a typical game
engine.
Most animation systems are comprised of up to three distinct layers:
•
Animation pipeline. For each animating character and object in the game,
the animation pipeline takes one or more animation clips and corre-
sponding blend factors as input, blends them together, and generates
a single local skeletal pose as output. It also calculates a global pose for
the skeleton and a palette of skinning matrices for use by the rendering
engine. Post-processing hooks are usually provided, which permit the
local pose to be modiﬁed prior to ﬁnal global pose and matrix palette
generation. This is where inverse kinematics (IK), rag doll physics and
other forms of procedural animation are applied to the skeleton.
•
Action state machine (ASM). The actions of a game character (standing,
walking, running, jumping, etc.) are usually best modeled via a ﬁnite
state machine, commonly known as the action state machine (ASM). The
ASM subsystem sits atop the animation pipeline and provides a state-
driven animation interface for use by virtually all higher-level game
code. It ensures that characters can transition smoothly from state to
state. In addition, most animation engines permit different parts of the
character’s body to be doing different, independent actions simultane-
ously, such as aiming and ﬁring a weapon while running. This can be
accomplished by allowing multiple independent state machines to con-
trol a single character via state layers.

11.10. The Animation Pipeline
605
•
Animation controllers. In many game engines, the behaviors of a player
or non-player character are ultimately controlled by a high-level sys-
tem of animation controllers. Each controller is custom-tailored to man-
age the character’s behavior when in a particular mode. There might be
one controller handling the character’s actions when he is ﬁghting and
moving around out in the open (“run-and-gun” mode), one for when he
is in cover, one for driving a vehicle, one for climbing a ladder and so
on. These high-level animation controllers allow most if not all of the
animation-related code to be encapsulated, allowing top-level player
control or AI logic to remain uncluttered by animation micromanage-
ment.
Some game engines draw the lines between these layers differently than
we do here, or introduce additional layers. Other engines meld two or more
of the layers into a single system. However, all animation engines need to
perform these tasks in one form or another. In the following sections, we’ll
explore animation architecture in terms of these three layers, noting in our ex-
amples when a particular game engine takes a more or less uniﬁed approach.
11.10
The Animation Pipeline
The operations performed by the low-level animation engine form a pipeline
that transforms its inputs (animation clips and blend speciﬁcations) into the
desired outputs (local and global poses, plus a matrix palette for rendering).
The stages of this pipeline are:
1.
Clip decompression and pose extraction. In this stage, each individual clip’s
data is decompressed, and a static pose is extracted for the time index in
question. The output of this phase is a local skeletal pose for each input
clip. This pose might contain information for every joint in the skeleton
(a full-body pose), for only a subset of joints (a partial pose), or it might be
a difference pose for use in additive blending.
2.
Pose blending. In this stage, the input poses are combined via full-body
LERP blending, partial-skeleton LERP blending and/or additive blend-
ing. The output of this stage is a single local pose for all joints in the
skeleton. This stage is of course only executed when blending more than
one animation clip together—otherwise the output pose from stage 1 can
be used directly.

606
11. Animation Systems
3.
Global pose generation. In this stage, the skeletal hierarchy is walked, and
local joint poses are concatenated in order to generate a global pose for
the skeleton.
4.
Post-processing. In this optional stage, the local and/or global poses of
the skeleton can be modiﬁed prior to ﬁnalization of the pose.
Post-
processing is used for inverse kinematics, rag doll physics and other
forms of procedural animation adjustment.
5.
Recalculation of global poses. Many types of post-processing require global
pose information as input but generate local poses as output. After such
a post-processing step has run, we must recalculate the global pose from
the modiﬁed local pose. Obviously, a post-processing operation that
does not require global pose information can be done between stages 2
and 3, thus avoiding the need for global pose recalculation.
6.
Matrix palette generation. Once the ﬁnal global pose has been generated,
each joint’s global pose matrix is multiplied by the corresponding in-
verse bind pose matrix. The output of this stage is a palette of skinning
matrices suitable for input to the rendering engine.
A typical animation pipeline is depicted in Figure 11.44.
Outputs
Inputs
Decompression
and
Pose Extraction
Blend
Specification
Pose
Blending
Skinning 
Matrix
Calc.
Global
Pose Calc.
Local 
Pose
Rendering
Engine
Matrix
Palette
Post-
Processing
Skeleton
Clip(s)
Local 
Clock(s)
Global
Pose
Game Play 
Systems
Figure 11.44. A typical animation pipeline.

11.10. The Animation Pipeline
607
11.10.1
Data Structures
Every animation pipeline is architected differently, but most operate in terms
of data structures that are similar to the ones described in this section.
11.10.1.1
Shared Resource Data
As with all game engine systems, a strong distinction must be made between
shared resource data and per-instance state information. Each individual character
or object in the game has its own per-instance data structures, but characters
or objects of the same type typically share a single set of resource data. This
shared resource data typically includes the following:
•
Skeleton. The skeleton describes the joint hierarchy and its bind pose.
•
Skinned meshes. One or more meshes can be skinned to a single skeleton.
Each vertex within a skinned mesh contains the indices of one or more
joints within the skeleton, plus weights governing how much inﬂuence
each joint should have on that vertex’s position.
•
Animation clips. Many thousands of animation clips are created for a
character’s skeleton. These may be full-body clips, partial-skeleton clips
or difference clips for use in additive blending.
A UML diagram of these data structures is shown in Figure 11.45. Pay par-
ticular attention to the cardinality and direction of the relationships between
these classes. The cardinality is shown just beside the tip or tail of the re-
lationship arrow between classes—a one represents a single instance of the
class, while an asterisk indicates many instances. For any one type of charac-
ter, there will be one skeleton, one or more meshes and one or more animation
clips. The skeleton is the central unifying element—the skins are attached to
the skeleton but don’t have any relationship with the animation clips. Like-
wise, the clips are targeted at a particular skeleton, but they have no “knowl-
edge” of the skin meshes. Figure 11.46 illustrates these relationships.
Game designers often try to reduce the number of unique skeletons in the
game to a minimum, because each new skeleton generally requires a whole
new set of animation clips. To provide the illusion of many different types of
characters, it is usually better to create multiple meshes skinned to the same
skeleton when possible, so that all of the characters can share a single set of
animations.
This is less of a problem now, thanks to the emergence of high-quality real
time skeletal retargeting techniques in recent years. Retargeting means using
an animation authored for one skeleton to animate a different skeleton. If the
two skeletons are morphologically identical, retargeting may boil down to a

608
11. Animation Systems
1
*
1
*
1
*
-uniqueId : int
-jointCount : int
-joints : SkeletonJoint
Skeleton
-name : string
-parentIndex : int
-invBindPose : Matrix44
SkeletonJoint
1
*
1
*
1
*
-indices : int
-vertices : Vertex
-skeletonId : int
Mesh
-nameId : int
-duration : float
-poseSamples : AnimationPose
AnimationClip
-position : Vector3
-normal : Vector3
-uv : Vector2
-jointIndices : int
-jointWeights : float
Vertex
-scale : Vector3
-rotation : Quaternion
-translation : Vector3
SQT
-jointPoses : SQT
AnimationPose
Figure 11.45. UML diagram of shared animation resources.
simple matter of joint index remapping. But when the two skeletons don’t
match exactly, the retargeting problem becomes more complex. At Naughty
Dog, the animators deﬁne a special pose known as the retarget pose. This pose
captures the essential differences between the bind poses of the source and tar-
get skeletons, allowing the runtime retargeting system to adjust source poses
so they will work more naturally on the target character.
Skeleton
Clip N
...
Skin A
Skin B
Skin C
Clip 1
Clip 2
Clip 3
other skeletons...
...
...
Figure 11.46. Many animation clips and one or more meshes target a single skeleton.

11.10. The Animation Pipeline
609
11.10.1.2
Per-Instance Data
In most games, multiple instances of each character type can appear on-screen
at the same time. Every instance of a particular character type needs its own
private data structures, allowing it to keep track of its currently playing ani-
mation clip(s), a speciﬁcation of how the clips are to be blended together (if
there’s more than one) and its current skeletal pose.
There is no one universally accepted way to represent per-instance ani-
mation data. However, virtually every animation engine keeps track of the
following pieces of information.
•
Clip state. For each playing clip, the following information is maintained:
◦
Local clock. A clip’s local clock describes the point along its local
timeline at which its current pose should be extracted. This may
be replaced by a global start time in some engines. (A comparison
between local and global clocks was provided in Section 11.4.3.)
◦
Playback rate. A clip can be played at an arbitrary rate, denoted R
in Section 11.4.2.
•
Blend speciﬁcation. The blend speciﬁcation is a description of which ani-
mation clips are currently playing and how these clips are to be blended
together. The degree to which each clip contributes to the ﬁnal pose is
controlled by one or more blend weights. There are two primary meth-
ods of describing the set of clips that should be blended together: a ﬂat
weighted average approach and a tree of blend nodes. When the tree ap-
proach is used, the structure of the blend tree is usually treated as a
shared resource, while the blend weights are stored as part of the per-
instance state information.
•
Partial-skeleton joint weights. If a partial-skeleton blend is to be performed,
the degrees to which each joint should contribute to the ﬁnal pose are
speciﬁed via a set of joint weights. In some animation engines, the joint
weights are binary: either a joint contributes or it does not. In other en-
gines, the weights can lie anywhere from zero (no contribution) to one
(full contribution).
•
Local pose. This is typically an array of SQT data structures, one per
joint, holding the ﬁnal pose of the skeleton in parent-relative format.
This array might also be reused to store an intermediate pose that serves
both as the input to and the output of the post-processing stage of the
pipeline.
•
Global pose. This is an array of SQTs, or 4 × 4 or 4 × 3 matrices, one
per joint, that holds the ﬁnal pose of the skeleton in model-space or

610
11. Animation Systems
world-space format. The global pose may serve as an input to the post-
processing stage.
•
Matrix palette. This is an array of 4 × 4 or 4 × 3 matrices, one per joint,
containing skinning matrices for input to the rendering engine.
11.10.2
The Flat Weighted Average Blend Representation
All but the most rudimentary game engines support animation blending in
some form. This means that at any given time, multiple animation clips may
be contributing to the ﬁnal pose of a character’s skeleton. One simple way
to describe how the currently active clips should be blended together is via a
weighted average.
In this approach, every animation clip is associated with a blend weight in-
dicating how much it should contribute to the ﬁnal pose of the character. A ﬂat
list of all active animation clips (i.e., clips whose blend weights are nonzero)
is maintained. To calculate the ﬁnal pose of the skeleton, we extract a pose
at the appropriate time index for each of the N active clips. Then, for each
joint of the skeleton, we calculate a simple N-point weighted average of the
translation vectors, rotation quaternions and scale factors extracted from the
N active animations. This yields the ﬁnal pose of the skeleton.
The equation for the weighted average of a set of N vectors {vi} is as
follows:
vavg =
N−1
X
i=0
wivi
N−1
X
i=0
wi
.
If the weights are normalized, meaning they sum to one, then this equation can
be simpliﬁed to the following:
vavg =
N−1
X
i=0
wivi,
when
N−1
X
i=0
wi = 1.
In the case of N = 2, if we let w0 = (1 −β) and w1 = β, the weighted average
reduces to the familiar equation for the linear interpolation (LERP) between
two vectors:
vavg = w0vA + w1vB
= (1 −β) vA + βvB
= LERP [vA, vB, β] .

11.10. The Animation Pipeline
611
We can apply this same weighted average formulation equally well to quater-
nions by simply treating them as four-element vectors.
11.10.2.1
Example: OGRE
The OGRE animation system works in exactly this way. An Ogre::Entity
represents an instance of a 3D mesh (e.g., one particular character walking
around in the game world).
The Entity aggregates an object called an
Ogre::AnimationStateSet,
which
in
turn
maintains
a
list
of
Ogre::AnimationState objects, one for each active animation.
The
Ogre::AnimationState class is shown in the code snippet below. (A few
irrelevant details have been omitted for clarity.)
/** Represents the state of an animation clip and the
weight of its influence on the overall pose of the
character.
*/
class AnimationState
{
protected:
String
mAnimationName; // reference to
// clip
Real
mTimePos;
// local clock
Real
mWeight;
// blend weight
bool
mEnabled;
// is this anim
// running?
bool
mLoop;
// should the
// anim loop?
public:
/// Gets the name of the animation.
const String& getAnimationName() const;
/// Gets the time position (local clock) for this
/// anim.
Real getTimePosition(void) const;
/// Sets the time position (local clock) for this
/// anim.
void setTimePosition(Real timePos);
/// Gets the weight (influence) of this animation
Real getWeight(void) const;
/// Sets the weight (influence) of this animation
void setWeight(Real weight);

612
11. Animation Systems
/// Modifies the time position, adjusting for
/// animation duration. This method loops if looping
/// is enabled.
void addTime(Real offset);
/// Returns true if the animation has reached the
/// end of local timeline and is not looping.
bool hasEnded(void) const;
/// Returns true if this animation is currently
/// enabled.
bool getEnabled(void) const;
/// Sets whether or not this animation is enabled.
void setEnabled(bool enabled);
/// Sets whether or not this animation should loop.
void setLoop(bool loop) { mLoop = loop; }
/// Gets whether or not this animation loops.
bool getLoop(void) const { return mLoop; }
};
Each AnimationState keeps track of one animation clip’s local clock
and its blend weight. When calculating the ﬁnal pose of the skeleton for a
particular Ogre::Entity, OGRE’s animation system simply loops through
each active AnimationState in its AnimationStateSet. A skeletal pose
is extracted from the animation clip corresponding to each state at the time
index speciﬁed by that state’s local clock. For each joint in the skeleton, an N-
point weighted average is then calculated for the translation vectors, rotation
quaternions and scales, yielding the ﬁnal skeletal pose.
OGRE and the Playback Rate
It is interesting to note that OGRE has no concept of a playback rate (R). If
it did, we would have expected to see a data member like this in the
Ogre::AnimationState class:
Real
mPlaybackRate;
Of course, we can still make animations play more slowly or more quickly
in OGRE by simply scaling the amount of time we pass to the addTime()
function, but unfortunately, OGRE does not support animation time scaling
out of the box.

11.10. The Animation Pipeline
613
11.10.2.2
Example: Granny
The Granny animation system, by Rad Game Tools (http://www.radgametool.
com/granny.html), provides a ﬂat, weighted average animation blending sys-
tem similar to OGRE’s.
Granny permits any number of animations to be
played on a single character simultaneously. The state of each active anima-
tion is maintained in a data structure known as a granny_control. Granny
calculates a weighted average to determine the ﬁnal pose, automatically nor-
malizing the weights of all active clips. In this sense, its architecture is virtu-
ally identical to that of OGRE’s animation system. But where Granny really
shines is in its handling of time. Granny uses the global clock approach dis-
cussed in Section 11.4.3. It allows each clip to be looped an arbitrary number
of times or inﬁnitely. Clips can also be time-scaled; a negative time scale al-
lows an animation to be played in reverse.
11.10.3
Blend Trees
For reasons we’ll explore below, some animation engines represent their blend
speciﬁcations not as a ﬂat weighted average but as a tree of blend operations.
An animation blend tree is an example of what is known in compiler theory
as an expression tree or a syntax tree. The interior nodes of such a tree are opera-
tors, and the leaf nodes serve as the inputs to those operators. (More correctly,
the interior nodes represent the nonterminals of the grammar, while the leaf
nodes represent the terminals.) In the following sections, we’ll brieﬂy revisit
the various kinds of animation blends we learned about in Sections 11.6.3 and
11.6.5 and see how each can be represented by an expression tree.
11.10.3.1
Binary LERP Blend
As we saw in Section 11.6.1, a binary linear interpolation (LERP) blend takes
two input poses and blends them together into a single output pose. A blend
weight β controls the percentage of the second input pose that should appear
at the output, while (1 −β) speciﬁes the percentage of the ﬁrst input pose.
This can be represented by the binary expression tree shown in Figure 11.47.
Figure 11.47. A binary LERP blend, represented by a binary expression tree.

614
11. Animation Systems
For this specific value of 
b, this tree converts to...
 = 0
 = 1
b
b
b
b
b
LERP
Output Pose
b
Clip A
Clip B
Clip C
Clip D
LERP
Clip B
Clip C
Output Pose
Figure 11.48. A multi-input expression tree can be used to represent a generalized 1D blend. Such
a tree can always be transformed into a binary expression tree for any speciﬁc value of the blend
factor b.
11.10.3.2
Generalized One-Dimensional LERP Blend
In Section 11.6.3.1, we learned that it can be convenient to deﬁne a generalized
one-dimensional LERP blend by placing an arbitrary number of clips along a
linear scale. A blend factor b speciﬁes the desired blend along this scale. Such
a blend can be pictured as an n-input operator, as shown in Figure 11.48.
Given a speciﬁc value for b, such a linear blend can always be transformed
into a binary LERP blend. We simply use the two clips immediately adjacent
to b as the inputs to the binary blend and calculate the blend weight β as
speciﬁed in Equation (11.15)
11.10.3.3
Simple Two-Dimensional LERP Blend
In Section 11.6.3.2, we saw how a two-dimensional LERP blend can be realized
by simply cascading the results of two binary LERP blends. Given a desired
two-dimensional blend point b =
bx
by

, Figure 11.49 shows how this kind
of blend can be represented in tree form.
11.10.3.4
Triangular LERP Blend
Section 11.6.3.3 introduced us to triangular LERP blending, using the barycen-
tric coordinates α, β and γ = (1 −α −β) as the blend weights. To represent
this kind of blend in tree form, we need a ternary (three-input) expression tree
node, as shown in Figure 11.50.

11.10. The Animation Pipeline
615
b
LERP
Bottom Left
Bottom Right
LERP
Top Left
Top Right
Output Pose
LERP
b
Figure 11.49. A simple 2D LERP blend, implemented as cascaded binary blends.
Output Pose
Clip A
Clip B
Clip C
Figure 11.50. A triangular 2D LERP blend, represented as a ternary expression tree.
11.10.3.5
Generalized Triangular LERP Blend
In Section 11.6.3.4, we saw that a generalized two-dimensional LERP blend
can be speciﬁed by placing clips at arbitrary locations on a plane. A desired
output pose is speciﬁed by a point b =
bx
by

on the plane. This kind of
blend can be represented as a tree node with an arbitrary number of inputs,
as shown in Figure 11.51.
For this specific value of 
b, this tree converts to...
bE
Triangular
LERP
Output Pose
Clip C
Clip D
Clip E
–
Delaunay
LERP
Output Pose
b
Clip A
Clip B
Clip C
Clip D
Clip E
bA
bB
bC
bD
b
b
Figure 11.51. A generalized 2D blend can be represented by a multi-input expression tree node, but
it can always be converted into a ternary tree via Delaunay triangulation.

616
11. Animation Systems
A generalized triangular LERP blend can always be transformed into a
ternary tree by using Delaunay triangulation to identify the triangle that sur-
rounds the point b. The point is then converted into barycentric coordinates
α, β and γ = (1 −α −β), and these coordinates are used as the blend weights
of a ternary blend node with the three clips at the vertices of the triangle as its
inputs. This is demonstrated in Figure 11.51.
11.10.3.6
Additive Blend
Section 11.6.5 described additive blending. This is a binary operation, so it can
be represented by a binary tree node, as shown in Figure 11.52. A single blend
weight β controls the amount of the additive animation that should appear at
the output—when β = 0, the additive clip does not affect the output at all,
while when β = 1, the additive clip has its maximum effect on the output.
Additive blend nodes must be handled carefully, because the inputs are
not interchangeable (as they are with most types of blend operators). One of
the two inputs is a regular skeletal pose, while the other is a special kind of
pose known as a difference pose (also known as an additive pose). A difference
pose may only be applied to a regular pose, and the result of an additive blend
is another regular pose. This implies that the additive input of a blend node
must always be a leaf node, while the regular input may be a leaf or an interior
node. If we want to apply more than one additive animation to our character,
we must use a cascaded binary tree with the additive clips always applied to
the additive inputs, as shown in Figure 11.53.
Figure 11.52. An additive blend represented as a binary tree.
Clip A
Diff Clip B
+
Diff Clip C
+
Output Pose
Diff Clip D
+
Figure 11.53. In order to additively blend more than one difference pose onto a regular “base”
pose, a cascaded binary expression tree must be used.

11.10. The Animation Pipeline
617
11.10.4
Cross-Fading Architectures
As we saw in Section 11.6.2.2, cross-fading between animations is generally
accomplished by LERP blending from the previous animation to the next one.
Cross-fades can be implemented in one of two ways, depending on whether
your animation engine uses the ﬂat weighted average architecture or the ex-
pression tree architecture. In this section, we’ll take a look at both implemen-
tations.
11.10.4.1
Cross-Fades with a Flat Weighted Average
In an animation engine that employs the ﬂat weighted average architecture,
cross-fades are implemented by adjusting the weights of the clips themselves.
Recall that any clip whose weight wi = 0 will not contribute to the current
pose of the character, while those whose weights are nonzero are averaged
together to generate the ﬁnal pose. If we wish to transition smoothly from
clip A to clip B, we simply ramp up clip B’s weight wB, while simultaneously
ramping down clip A’s weight wA. This is illustrated in Figure 11.54.
Cross-fading in a weighted average architecture becomes a bit trickier when
we wish to transition from one complex blend to another. As an example, let’s
say we wish to transition the character from walking to jumping. Let’s assume
that the walk movement is produced by a three-way average between clips A,
B and C, and that the jump movement is produced by a two-way average
between clips D and E.
We want the character to look like he’s smoothly transitioning from walk-
ing to jumping, without affecting how the walk or jump animations look in-
dividually. So during the transition, we want to ramp down the ABC clips
and ramp up the DE clips while keeping the relative weights of the ABC and
DE clip groups constant. If the cross-fade’s blend factor is denoted by λ, we
can meet this requirement by simply setting the weights of both clip groups to
their desired values and then multiplying the weights of the source group by
(1 −λ) and the weights of the destination group by λ.
t
w
tstart
tend
w
w
Figure 11.54.
A simple cross-fade from clip A to clip B, as implemented in a weighted average
animation architecture.

618
11. Animation Systems
Let’s look at a concrete example to convince ourselves that this will work
properly. Imagine that before the transition from ABC to DE, the nonzero
weights are as follows: wA = 0.2, wB = 0.3 and wC = 0.5. After the transition,
we want the nonzero weights to be wD = 0.33 and wE = 0.66. So, we set the
weights as follows:
wA = (1 −λ)(0.2),
wD = λ(0.33),
wB = (1 −λ)(0.3),
wE = λ(0.66).
(11.20)
wC = (1 −λ)(0.5),
From Equations (11.20), you should be able to convince yourself of the follow-
ing:
1.
When λ = 0, the output pose is the correct blend of clips A, B and C,
with zero contribution from clips D and E.
2.
When λ = 1, the output pose is the correct blend of clips D and E, with
no contribution from A, B or C.
3.
When 0 < λ < 1, the relative weights of both the ABC group and the
DE group remain correct, although they no longer add to one. (In fact,
group ABC’s weights add to (1 −λ), and group DE’s weights add to λ.)
For this approach to work, the implementation must keep track of
the logical groupings between clips (even though, at the lowest level, all
of the clips’ states are maintained in one big, ﬂat array—for example, the
Ogre::AnimationStateSet in OGRE). In our example above, the system
must “know” that A, B and C form a group, that D and E form another group,
and that we wish to transition from group ABC to group DE. This requires
additional metadata to be maintained, on top of the ﬂat array of clip states.
11.10.4.2
Cross-Fades with Expression Trees
Implementing a cross-fade in an expression-tree animation engine is a bit
more intuitive than it is in a weighted average architecture. Whether we’re
transitioning from one clip to another or from one complex blend to another,
the approach is always the same: We simply introduce a new, binary LERP
node at the root of the blend tree for the duration of the cross-fade.
We’ll denote the blend factor of the cross-fade node with the symbol λ as
before. Its top input is the source tree (which can be a single clip or a complex
blend), and its bottom input is the destination tree (again a clip or a complex
blend). During the transition, λ is ramped from zero to one. Once λ = 1, the
transition is complete, and the cross-fade LERP node and its top input tree

11.10. The Animation Pipeline
619
Tree
A
Tree
A
Tree
B
Tree
B
Before
Cross-Fade
During
Cross-Fade
After
Cross-Fade
Figure 11.55. A cross-fade between two arbitrary blend trees A and B.
can be retired. This leaves its bottom input tree as the root of the overall blend
tree, thus completing the transition. This process is illustrated in Figure 11.55.
11.10.5
Animation Pipeline Optimization
Optimization is a crucial aspect of any animation pipeline. Some pipelines
expose all of their nitty-gritty optimization details, effectively placing the re-
sponsibility for proper optimization on the calling code. Others attempt to
encapsulate most of the optimization details behind a convenient API, but
even in these cases, the API still must be structured in a particular way so as
to permit the desired optimizations to be implemented behind the scenes.
Animation pipeline optimizations are usually highly speciﬁc to the archi-
tecture of the hardware on which the game will run. For example, on modern
hardware architectures, memory access patterns can greatly affect the per-
formance of the code. Cache misses and load-hit-store operations must be
avoided to ensure maximum speed. But on other hardware, ﬂoating-point op-
erations might be the bottleneck, in which case the code might be structured
to take maximum advantage of SIMD vector math. Each hardware platform
presents a unique set of optimization challenges to the programmer. As a re-
sult, some animation pipeline APIs are highly speciﬁc to a particular platform.

620
11. Animation Systems
Other pipelines attempt to present an API that can be optimized in different
ways on different processors. Let’s take a look at a few examples.
11.10.5.1
Optimization on the PlayStation 3
As we saw in Section 7.6.1.2, the PlayStation 3 has six specialized processors
known as synergistic processing units (SPU). The SPUs execute most code much
more quickly than the main CPU (known as the power processing unit or PPU).
Each SPU also has a 256 KiB region of ultra-fast local store memory for its exclu-
sive use. The PS3 has a powerful DMA controller capable of moving memory
back and forth between main RAM and the SPUs’ memories in parallel with
computing tasks. If one could write an ideal animation pipeline for the PS3,
as much processing as possible would be executed on the SPUs, and neither
the PPU nor any SPU would ever be idle waiting for a DMA to complete.
The PS3’s architecture prompts us to make use of a batched job API, in
which the game’s requests for animation blends are performed as jobs that
run on the SPUs. The input data for each animation job is moved from main
RAM to the local store of one of the SPUs via the DMA controller. The job then
executes (at lightning speed!) on that SPU, and the results are transferred back
to main RAM via another DMA operation. While one job is running, the DMA
controller and the other SPUs can be kept busy with other animation or non-
animation tasks. This allows the animation system to maximize hardware
utilization on the PS3.
11.10.5.2
Optimization on the Xbox 360, Xbox One and PlayStation 4
Rather than having specialized memory regions and a DMA controller to
move data from region to region, the Xbox, Xbox 360, Xbox One and PlaySta-
tion 4 all employ a heterogeneous uniﬁed memory architecture (hUMA, see
Section 7.6.1.3 for details). In other words, all CPU cores and the GPU all tap
into a single big block of main RAM.
In theory, the hUMA architecture requires a totally different set of opti-
mizations than would be required on the PlayStation 3, and so we might ex-
pect to see very different animation APIs between these two platforms. How-
ever, the Xbox 360, Xbox One and PS4 serve as examples of how optimizations
for one platform can sometimes be beneﬁcial to other platforms as well. As it
turns out, all of these platforms incur massive performance degradation in
the presence of cache misses and load-hit-store memory access patterns. So,
it is beneﬁcial on all systems to keep animation data as localized as possible
in physical RAM. An animation pipeline that processes animations in large
batches and operates on data within relatively small regions of memory (such

11.11. Action State Machines
621
as the SPU’s local store memories on the PS3) will also perform well on a uni-
ﬁed memory architecture. Achieving this kind of synergy between platforms
is not always possible, and every hardware platform requires its own speciﬁc
optimizations. However, when such an opportunity does arise, it is wise to
take advantage of it.
A good rule of thumb is to optimize your engine for the platform with
the most stringent performance restrictions. When your optimized code is
ported to other platforms with fewer restrictions, there’s a good chance that
the optimizations you made will remain beneﬁcial, or at worst will have few
adverse affects on performance. Going in the other direction—porting from
the least stringent platform to the more stringent ones—almost always results
in less-than-optimal performance on the most stringent platform.
11.11
Action State Machines
The low-level pipeline is the equivalent of OpenGL or DirectX for animation—
it is very powerful but can be rather inconvenient for direct use by game code.
Therefore, it is usually convenient to introduce a layer between the low-level
pipeline and the game characters and other clients of the animation system.
This layer is usually implemented as a state machine, known as the action state
machine or the animation state machine (ASM).
The ASM sits on top of the animation pipeline, permitting the actions of
the characters in a game to be controlled in a straightforward, state-driven
manner. The ASM is also responsible for ensuring that transitions from state
to state are smooth and natural-looking. Some animation engines permit mul-
tiple independent state machines to control different aspects of a character’s
movement, such as full-body locomotion, upper-body gestures and facial an-
imations. This can be accomplished by introducing the concept of state lay-
ering. In this section, we’ll explore how a typical animation state machine is
architected.
11.11.1
Animation States
Each state in an ASM corresponds to an arbitrarily complex blend of simul-
taneous animation clips. In a blend tree architecture, each state corresponds
to a particular predeﬁned blend tree. In a ﬂat weighted average architecture,
a state represents a group of clips with a speciﬁc set of relative weights. It is
somewhat more convenient and expressive to think in terms of blend trees, so
we will do so for the remainder of this discussion. However, everything we
describe here can also be implemented using the ﬂat weighted average ap-

622
11. Animation Systems
proach, as long as additive blending or quaternion SLERP operations are not
involved.
The blend tree corresponding to a particular animation state can be as
simple or as complex as required by the game’s design (provided it remains
within the memory and performance limitations of the engine). For example,
an “idle” state might be comprised of a single full-body animation. A “run-
ning” state might correspond to a semicircular blend, with straﬁng left, run-
ning forward and straﬁng right at the −90-degree, 0-degree and +90-degree
points, respectively. The blend tree for a “running while shooting” state might
include a semicircular directional blend, plus additive or partial-skeleton blend
nodes for aiming the character’s weapon up, down, left and right, and addi-
tional blends to permit the character to look around with its eyes, head and
shoulders. More additive animations might be included to control the charac-
ter’s overall stance, gait and foot spacing while locomoting and to provide a
degree of “humanness” through random movement variations.
11.11.1.1
State and Blend Tree Speciﬁcations
Animators, game designers and programmers usually cooperate to create the
animation and control systems for the central characters in a game. These
developers need a way to specify the states that make up a character’s ASM,
to lay out the tree structure of each blend tree, and to select the clips that
will serve as their inputs. Although the states and blend trees could be hard-
coded, most modern game engines provide a data-driven means of deﬁning
animation states. The goal of a data-driven approach is to permit a user to
create new animation states, remove unwanted states, ﬁne-tune existing states
and then see the effects of his or her changes reasonably quickly. In other
words, the central goal of a data-driven animation engine is to enable rapid
iteration.
The means by which the users enter animation state data varies widely.
Some game engines employ a simple, bare-bones approach, allowing anima-
tion states to be speciﬁed in a text ﬁle with a simple syntax. Other engines pro-
vide a slick, graphical editor that permits animation states to be constructed
by dragging atomic components such as clips and blend nodes onto a canvas
and linking them together in arbitrary ways. Such editors usually provide a
live preview of the character so that the user can see immediately how the
character will look in the ﬁnal game. In my opinion, the speciﬁc method cho-
sen has little bearing on the quality of the ﬁnal game—what matters most is
that the user can make changes and see the results of those changes reasonably
quickly and easily.

11.11. Action State Machines
623
11.11.1.2
Custom Blend Tree Node Types
To build an arbitrarily complex blend tree, we really only require four atomic
types of blend nodes: clips, binary LERP blends, binary additive blends and
ternary (triangular) LERP blends. Virtually any blend tree imaginable can be
created as compositions of these atomic nodes.
A blend tree built exclusively from atomic nodes can quickly become large
and unwieldy. As a result, many game engines permit custom compound
node types to be predeﬁned for convenience. The N-dimensional linear blend
node discussed in Sections 11.6.3.4 and 11.10.3.2 is an example of a compound
node. One can imagine myriad complex blend node types, each one address-
ing a particular problem speciﬁc to the particular game being made. A soccer
game might deﬁne a node that allows the character to dribble the ball. A war
game could deﬁne a special node that handles aiming and ﬁring a weapon.
A brawler could deﬁne custom nodes for each ﬁght move the characters can
perform. Once we have the ability to deﬁne custom node types, the sky’s the
limit.
11.11.1.3
Example: The Uncharted/The Last of Us Engine
The animation engine used in Naughty Dog’s Uncharted franchise and The Last
of Us employs a simple, text-based approach to specifying animation states.
For reasons related to Naughty Dog’s rich history with the Lisp language,
state speciﬁcations in the Naughty Dog engine are written in a customized
version of the Scheme programming language (which itself is a Lisp variant).
Two basic state types can be used: simple and complex.
Simple States
A simple state contains a single animation clip. For example:
(define-state simple
:name "pirate-b-bump-back"
:clip "pirate-b-bump-back"
:flags (anim-state-flag no-adjust-to-ground)
)
Don’t let the Lisp-style syntax throw you. All this block of code does is to de-
ﬁne a state named “pirate-b-bump-back” whose animation clip also happens
to be named “pirate-b-bump-back.” The :flags parameter allows users to
specify various Boolean options on the state.

624
11. Animation Systems
Complex States
A complex state contains an arbitrary tree of LERP or additive blends. For
example, the following state deﬁnes a tree that contains a single binary LERP
blend node, with two clips (“walk-l-to-r” and “run-l-to-r”) as its inputs:
(define-state complex
:name "move-l-to-r"
:tree
(anim-node-lerp
(anim-node-clip "walk-l-to-r")
(anim-node-clip "run-l-to-r")
)
)
The :tree argument allows the user to specify an arbitrary blend tree, com-
posed of LERP or additive blend nodes and nodes that play individual ani-
mation clips.
From this, we can see how the (define-state simple ...) example
shown above might really work under the hood—it probably deﬁnes a com-
plex blend tree containing a single “clip” node, like this:
(define-state complex
:name "pirate-b-unimog-bump-back"
:tree (anim-node-clip "pirate-b-unimog-bump-back")
:flags (anim-state-flag no-adjust-to-ground)
)
The following complex state shows how blend nodes can be cascaded into
arbitrarily deep blend trees:
(define-state complex
:name "move-b-to-f"
:tree
(anim-node-lerp
(anim-node-additive
(anim-node-additive
(anim-node-clip "move-f")
(anim-node-clip "move-f-look-lr")
)
(anim-node-clip "move-f-look-ud")
)
(anim-node-additive
(anim-node-additive
(anim-node-clip "move-b")
(anim-node-clip "move-b-look-lr")
)

11.11. Action State Machines
625
(anim-node-clip "move-b-look-ud")
)
)
)
This corresponds to the tree shown in Figure 11.56.
LERP
move-f
move-f-look-lr
+
move-f-look-ud
move-b
move-b-look-lr
+
move-b-look-ud
+
+
Figure 11.56. Blend tree corresponding to the example state “move-b-to-f.”
Custom Tree Syntax
Thanks to the powerful macro language in Scheme, custom blend trees can
also be deﬁned by the user in terms of the basic clip, LERP and additive blend
nodes. This allows us to deﬁne multiple states, each of which has a nearly
identical tree structure but with different input clips or any number of other
variations. For example, the complex blend tree used in the state “move-b-to-
f” shown above could be partially deﬁned via a macro as follows:
(define-syntax look-tree
(syntax-rules ()
((look-tree base-clip look-lr-clip look-ud-clip)
;; This means "whenever the compiler sees
;; code of the form (look-tree b lr ud),
;; replace it with the following code..."
(anim-node-additive
(anim-node-additive
(anim-node-clip base-clip)
(anim-node-clip look-lr-clip)
)
(anim-node-clip look-ud-clip)
)
)
)
)

626
11. Animation Systems
The original “move-b-to-f” state could then be redeﬁned in terms of this
macro as follows:
(define-state complex
:name "move-b-to-f"
:tree
(anim-node-lerp
(look-tree "move-f"
"move-f-look-lr"
"move-f-look-ud")
(look-tree "move-b"
"move-b-look-lr"
"move-b-look-ud")
)
)
The (look-tree ...) macro can be used to deﬁne any number of states
that require this same basic tree structure but want different animation clips
as inputs. They can also combine their “look trees” in any number of ways.
Rapid Iteration
Rapid iteration is achieved in Naughty Dog’s games with the help of three im-
portant tools. An in-game animation viewer allows a character to be spawned
into the game and its animations controlled via an in-game menu. A simple
command-line tool allows animation scripts to be recompiled and reloaded
into the running game on the ﬂy. To tweak a character’s animations, the user
can make changes to the text ﬁle containing the animation state speciﬁcations,
quickly reload the animation states and immediately see the effects of his or
her changes on an animating character in the game. Finally, Naughty Dog
is also developing a host of “live update” tools. For example, animators can
now tweak their animations in Maya and see them update virtually instanta-
neously in the game.
11.11.1.4
Example: Unreal Engine 4
Unreal Engine 4 (UE4) provides its users with a graphical interface to the an-
imation system. As shown in Figure 11.57, an animation blend tree in Unreal
is comprised of a special root node called an AnimTree. This node takes three
kinds of inputs: animations, morphs and special nodes known as skel controls.
The animation input can be connected to the root of an arbitrarily complex
blend tree (which happens to be drawn with poses ﬂowing from right to left—
opposite of the convention we use in this book). The “morph” input allows
morph-target-based animations to drive the character; this is most often used

11.11. Action State Machines
627
Figure 11.57. The Unreal Engine 4 graphical animation editor.
for facial animation. The “skel control” inputs allow various kinds of proce-
dural post-processing, such as inverse kinematics (IK), to be performed on the
pose generated by the animation and/or morph trees.
The UE4 Animation Tree
The Unreal animation tree is essentially a blend tree. Individual animation
clips (called sequences in Unreal) are represented by nodes of type Anim-
Sequence. A sequence node has a single output, which may either be con-
nected directly to the “animation” input of the AnimTree node or to other
complex node types. Unreal provides a wide selection of blend node types
out of the box, including binary blends, four-way two-dimensional blends
(known as blend by aim) and so on. It also provides various special nodes that
are capable of doing things like scaling the playback rate (R) of a clip, mirror-
ing the animation (which turns a right-handed motion into a left-handed one,
for example) and more.
The UE4 animation tree is also highly customizable. A programmer can
create new types of nodes that perform arbitrarily complex operations. So the
Unreal developer is not limited to simple binary and ternary LERP blends.
It is interesting to note that Unreal’s approach to character animation is not
explicitly state-based. Rather than deﬁning multiple states, each with its own

628
11. Animation Systems
local blend tree, the Unreal developer typically builds a single monolithic tree.
The character can be put into different “states” by simply turning on or off
certain parts of the tree. Some game teams implement a system for replacing
portions of the UE4 animation tree dynamically, so that a game’s monolithic
tree can be broken into more manageable subtrees.
The UE4 Post-Processing Tree (Skel Controls)
As we have seen, animation post-processing involves procedurally modifying
the pose of the skeleton that has been generated by the blend tree. In UE4,
skel control nodes are used for this purpose. To use a skel control, the user
ﬁrst creates an input on the AnimTree node corresponding to the joint in the
skeleton that he or she wishes to control procedurally. Then a suitable skel
control node is created, and its output is hooked up to the new input on the
AnimTree node.
Unreal provides a number of skel controls out of the box to perform foot IK
(which ensures that the feet conform to ground contours), procedural “look-
at” (which allows the character to look at arbitrary points in space), other
forms of IK and so on. As with animation nodes, it is quite easy for a pro-
grammer to create custom skel control nodes in order to meet the particular
needs of the game being developed.
11.11.2
Transitions
To create a high-quality animating character, we must carefully manage the
transitions between states in the action state machine to ensure that the splices
between animations do not have a jarring and unpolished appearance. Most
modern animation engines provide a data-driven mechanism for specifying
exactly how transitions should be handled. In this section, we’ll explore how
this mechanism works.
11.11.2.1
Kinds of Transitions
There are many different ways to manage the transition between states. If we
know that the ﬁnal pose of the source state exactly matches the ﬁrst pose of the
destination state, we can simply “pop” from one state to another. Otherwise,
we can cross-fade from one state to the next. Cross-fading is not always a
suitable choice when transitioning from state to state. For example, there is
no way that a cross-fade can produce a realistic transition from lying on the
ground to standing upright. For this kind of state transition, we need one
or more custom animations. This kind of transition is often implemented by
introducing special transitional states into the state machine. These states are

11.11. Action State Machines
629
intended for use only when going from one state to another—they are never
used as a steady-state node. But because they are full-ﬂedged states, they
can be comprised of arbitrarily complex blend trees. This provides maximum
ﬂexibility when authoring custom-animated transitions.
11.11.2.2
Transition Parameters
When describing a particular transition between two states, we generally need
to specify various parameters, controlling exactly how the transition will oc-
cur. These include but are not limited to the following.
•
Source and destination states. To which state(s) does this transition apply?
•
Transition type. Is the transition immediate, cross-faded or performed via
a transitional state?
•
Duration. For cross-faded transitions, we need to specify how long the
cross-fade should take.
•
Ease-in/ease-out curve type. In a cross-faded transition, we may wish to
specify the type of ease-in/ease-out curve to use to vary the blend factor
during the fade.
•
Transition window. Certain transitions can only be taken when the source
animation is within a speciﬁed window of its local timeline. For exam-
ple, a transition from a punch animation to an impact reaction might
only make sense when the arm is in the second half of its swing. If an at-
tempt to perform the transition is made during the ﬁrst half of the swing,
the transition would be disallowed (or a different transition might be se-
lected instead).
11.11.2.3
The Transition Matrix
Specifying transitions between states can be challenging, because the num-
ber of possible transitions is usually very large. In a state machine with n
states, the worst-case number of possible transitions is n2. We can imagine
a two-dimensional square matrix with every possible state listed along both
the vertical and horizontal axes. Such a table can be used to specify all of the
possible transitions from any state along the vertical axis to any other state
along the horizontal axis.
In a real game, this transition matrix is usually quite sparse, because not
all state-to-state transitions are possible. For example, transitions are usually
disallowed from a death state to any other state. Likewise, there is probably
no way to go from a driving state to a swimming state (without going through
at least one intermediate state that causes the character to jump out of his

630
11. Animation Systems
vehicle). The number of unique transitions in the table may be signiﬁcantly
less even than the number of valid transitions between states. This is because
we can often reuse a single transition speciﬁcation between many different
pairs of states.
11.11.2.4
Implementing a Transition Matrix
There are all sorts of ways to implement a transition matrix. We could use a
spreadsheet application to tabulate all the transitions in matrix form, or we
might permit transitions to be authored in the same text ﬁle used to author
our action states. If a graphical user interface is provided for state editing,
transitions could be added to this GUI as well. In the following sections, we’ll
take a brief look at a few transition matrix implementations from real game
engines.
Example: Wild-Carded Transitions in Medal of Honor: Paciﬁc Assault
On Medal of Honor: Paciﬁc Assault (MOHPA), we used the sparseness of the
transition matrix to our advantage by supporting wild-carded transition spec-
iﬁcations. For each transition speciﬁcation, the names of both the source and
destination states could contain asterisks (*) as a wild-card character. This
allowed us to specify a single default transition from any state to any other
state (via the syntax from="*" to="*") and then reﬁne this global default
easily for entire categories of states. The reﬁnement could be taken all the way
down to custom transitions between speciﬁc state pairs when necessary. The
MOHPA transition matrix looked something like this:
<transitions>
<!-- global default -->
<trans from="*" to="*"
type=frozen duration=0.2>
...
<!-- default for any walk to any run -->
<trans from="walk*" to="run*"
type=smooth
duration=0.15>
...
<!-- special handling from any prone to any getting-up
-- action (only valid from 2 sec to 7.5 sec on the

11.11. Action State Machines
631
-- local timeline) -->
<trans from="*prone" to="*get-up"
type=smooth
duration=0.1
window-start=2.0
window-end=7.5>
...
<!-- special case between crouched walking and jumping -->
<trans from="walk-crouch" to="jump"
type=frozen
duration=0.3>
...
</transitions>
Example: First-Class Transitions in Uncharted
In some animation engines, high-level game code requests transitions from
the current state to a new state by naming the destination state explicitly. The
problem with this approach is that the calling code must have intimate knowl-
edge of the names of the states and of which transitions are valid when in a
particular state.
In Naughty Dog’s engine, this problem is overcome by turning state tran-
sitions from secondary implementation details into ﬁrst-class entities. Each
state provides a list of valid transitions to other states, and each transition is
given a unique name. The names of the transitions are standardized in order
to make the effect of each transition predictable. For example, if a transition
is called “walk,” then it always goes from the current state to a walking state
of some kind, no matter what the current state is. Whenever the high-level
animation control code wants to transition from state A to state B, it asks for a
transition by name (rather than requesting the destination state explicitly). If
such a transition can be found and is valid, it is taken; otherwise, the request
fails.
The following example state deﬁnes four transitions named “reload,” “step-
left,” “step-right” and “ﬁre.” The (transition-group ...) line invokes
a previously deﬁned group of transitions; it is useful when the same set of
transitions is to be used in multiple states. The (transition-end ...)
command speciﬁes a transition that is taken upon reaching the end of the
state’s local timeline if no other transition has been taken before then.
(define-state complex
:name "s_turret-idle"

632
11. Animation Systems
:tree (aim-tree
(anim-node-clip "turret-aim-all--base")
"turret-aim-all--left-right"
"turret-aim-all--up-down"
)
:transitions (
(transition "reload" "s_turret-reload"
(range - -) :fade-time 0.2)
(transition "step-left" "s_turret-step-left"
(range - -) :fade-time 0.2)
(transition "step-right" "s_turret-step-right"
(range - -) :fade-time 0.2)
(transition "fire" "s_turret-fire"
(range - -) :fade-time 0.1)
(transition-group "combat-gunout-idle^move")
(transition-end "s_turret-idle")
)
)
The beauty of this approach may be difﬁcult to see at ﬁrst. Its primary
purpose is to allow transitions and states to be modiﬁed in a data-driven
manner, without requiring changes to the C++ source code in many cases.
This degree of ﬂexibility is accomplished by shielding the animation control
code from knowledge of the structure of the state graph. For example, let’s
say that we have ten different walking states (normal, scared, crouched, in-
jured and so on). All of them can transition into a jumping state, but different
kinds of walks might require different jump animations (e.g., normal jump,
scared jump, jump from crouch, injured jump, etc.). For each of the ten walk-
ing states, we deﬁne a transition simply called “jump.” At ﬁrst, we can point
all of these transitions to a single generic “jump” state, just to get things up
and running. Later, we can ﬁne-tune some of these transitions so that they
point to custom jump states. We can even introduce transitional states be-
tween some of the “walk” states and their corresponding “jump” states. All
sorts of changes can be made to the structure of the state graph and the pa-
rameters of the transitions without affecting the C++ source code—as long as
the names of the transitions don’t change.

11.11. Action State Machines
633
11.11.3
State Layers
Most living creatures can do more than one thing at once with their bodies.
For example, a human can walk around with her lower body while looking at
something with her shoulders, head and eyes and making a gesture with her
hands and arms. The movements of different parts of the body aren’t gener-
ally in perfect sync—certain parts of the body tend to “lead” the movements
of other parts (e.g., the head leads a turn, followed by the shoulders, the hips
and ﬁnally the legs). In traditional animation, this well-known technique is
known as anticipation [46].
This kind of movement seems to be at odds with a state-machine-based
approach to animation. After all, we can only be in one state at a time. So,
how can we get different parts of the body to operate independently? One
solution to this problem is to introduce the concept of state layers. Each layer
can be in only one state at a time, but the layers are temporally independent
of one another. The ﬁnal pose of the skeleton is calculated by evaluating the
blend trees on each of the n layers, thus generating n skeletal poses, and then
blending these poses together in a predeﬁned manner. This is illustrated in
Figure 11.58.
Base Layer
State A
State B
State C
Variation Layer (Additive)
D
E
G
Gesture Layer (Additive)
H
I
Gesture Layer (LERP)
J
K
F
Time ( )
Figure 11.58. A layered animation state machine, showing how each layer’s state transitions are
temporally independent.

634
11. Animation Systems
Net blend tree
at time 
Time
H
F
B
K
LERP
+
Tree
B
Tree
F
Tree
H
+
Tree
K
Figure 11.59. A layered state machine converts the blend trees from multiple states into a single,
uniﬁed tree.
The Naughty Dog engine uses a layered state architecture. The layers form
a stack, with the bottom-most layer (called the base layer) always producing
a full-body skeletal pose and each upper layer blending in a new full-body,
partial-skeleton or additive pose on top of the base pose. Two kinds of lay-
ers are supported: LERP and additive. A LERP layer blends its output pose
with the pose generated by the layer(s) below it. An additive layer assumes
that its output pose is always a difference pose and uses additive blending to
combine it with the pose generated by the layer(s) below it. In effect, a layered
state machine converts multiple, temporally independent blend trees (one per
layer) into a single uniﬁed blend tree. This is shown in Figure 11.59.
11.11.4
Control Parameters
From a software engineering perspective, it can be challenging to orchestrate
all of the blend weights, playback rates and other control parameters of a com-

11.11. Action State Machines
635
plex animating character. Different blend weights have different effects on the
way the character animates. For example, one weight might control the char-
acter’s movement direction, while others control its movement speed, hori-
zontal and vertical weapon aim, head/eye look direction and so on. We need
some way of exposing all of these blend weights to the code that is responsible
for controlling them.
In a ﬂat weighted average architecture, we have a ﬂat list of all the ani-
mation clips that could possibly be played on the character. Each clip state
has a blend weight, a playback rate and possibly other control parameters.
The code that controls the character must look up individual clip states by
name and adjust each one’s blend weight appropriately. This makes for a sim-
ple interface, but it shifts most of the responsibility for controlling the blend
weights to the character control system. For example, to adjust the direction in
which a character is running, the character control code must know that the
“run” action is comprised of a group of animation clips, named something
like “StrafeLeft,” “RunForward,” “StrafeRight” and “RunBackward.” It must
look up these clip states by name and manually control all four blend weights
in order to achieve a particular angled run animation. Needless to say, con-
trolling animation parameters in such a ﬁne-grained way can be tedious and
can lead to difﬁcult-to-understand source code.
In a blend tree, a different set of problems arise. Thanks to the tree struc-
ture, the clips are grouped naturally into functional units. Custom tree nodes
can encapsulate complex character motions. These are both helpful advan-
tages over the ﬂat weighted average approach. However, the control param-
eters are buried within the tree. Code that wishes to control the horizontal
look-at direction of the head and eyes needs a priori knowledge of the struc-
ture of the blend tree so that it can ﬁnd the appropriate nodes in the tree in
order to control their parameters.
Different animation engines solve these problems in different ways. Here
are some examples:
•
Node search. Some engines provide a way for higher-level code to ﬁnd
blend nodes in the tree. For example, relevant nodes in the tree can
be given special names, such as “HorizAim” for the node that controls
horizontal weapon aiming. The control code can simply search the tree
for a node of a particular name; if one is found, then we know what
effect adjusting its blend weight will have.
•
Named variables. Some engines allow names to be assigned to the indi-
vidual control parameters. The controlling code can look up a control
parameter by name in order to adjust its value.

636
11. Animation Systems
•
Control structure. In other engines, a simple data structure, such as an
array of ﬂoating-point values or a C struct, contains all of the control
parameters for the entire character. The nodes in the blend tree(s) are
connected to particular control parameters, either by being hard-coded
to use certain struct members or by looking up the parameters by name
or index.
Of course, there are many other alternatives as well. Every animation en-
gine tackles this problem in a slightly different way, but the net effect is always
roughly the same.
11.11.5
Constraints
We’ve seen how action state machines can be used to specify complex blend
trees and how a transition matrix can be used to control how transitions be-
tween states should work. Another important aspect of character animation
control is to constrain the movement of the characters and/or objects in the
scene in various ways. For example, we might want to constrain a weapon
so that it always appears to be in the hand of the character who is carrying it.
We might wish to constrain two characters so that they line up properly when
shaking hands. A character’s feet are often constrained so that they line up
with the ﬂoor, and its hands might be constrained to line up with the rungs
on a ladder or the steering wheel of a vehicle. In this section, we’ll take a brief
look at how these constraints are handled in a typical animation system.
11.11.5.1
Attachments
Virtually all modern game engines permit objects to be attached to one an-
other. At its simplest, object-to-object attachment involves constraining the
position and/or orientation of a particular joint JA within the skeleton of ob-
ject A so that it coincides with a joint JB in the skeleton of object B. An at-
tachment is usually a parent-child relationship. When the parent’s skeleton
moves, the child object is adjusted to satisfy the constraint. However, when
the child moves, the parent’s skeleton is usually not affected. This is illus-
trated in Figure 11.60.
Sometimes it can be convenient to introduce an offset between the parent
joint and the child joint. For example, when placing a gun into a character’s
hand, we could constrain the “Grip” joint of the gun so that it coincides with
the “RightWrist” joint of the character. However, this might not produce the
correct alignment of the gun with the hand. One solution to this problem is
to introduce a special joint into one of the two skeletons. For example, we
could add a “RightGun” joint to the character’s skeleton, make it a child of

11.11. Action State Machines
637
… child 
skeleton
follows
parent
skeleton
moves…
child
skeleton
moves…
… parent 
skeleton
unaffected
Figure 11.60.
An attachment, showing how movement of the parent automatically produces
movement of the child but not vice versa.
the “RightWrist” joint, and position it so that when the “Grip” joint of the
gun is constrained to it, the gun looks like it is being held naturally by the
character. The problem with this approach, however, is that it increases the
number of joints in the skeleton. Each joint has a processing cost associated
with animation blending and matrix palette calculation and a memory cost for
storing its animation keys. So adding new joints is often not a viable option.
We know that an additional joint added for attachment purposes will not
contribute to the pose of the character—it merely introduces an additional
transform between the parent and child joint in an attachment. What we really
want, then, is a way to mark certain joints so that they can be ignored by the
animation blending pipeline but can still be used for attachment purposes.
Such special joints are sometimes called attach points. They are illustrated in
Figure 11.61.
Figure 11.61. An attach point acts like an extra joint between the parent and the child.

638
11. Animation Systems
Attach points might be modeled in Maya just like regular joints or locators,
although many game engines deﬁne attach points in a more convenient man-
ner. For example, they might be speciﬁed as part of the action state machine
text ﬁle or via a custom GUI within the animation authoring tool. This allows
the animators to focus only on the joints that affect the look of the character,
while the power to control attachments is put conveniently into the hands of
the people who need it—the game designers and the engineers.
11.11.5.2
Interobject Registration
The interactions between game characters and their environments is growing
ever more complex and nuanced with each new title. Hence, it is important
to have a system that allows characters and objects to be aligned with one
another when animating. Such a system can be used for in-game cinematics
and interactive gameplay elements alike.
Imagine that an animator, working in Maya or some other animation tool,
sets up a scene involving two characters and a door object. The two charac-
ters shake hands, and then one of them opens the door and they both walk
through it. The animator can ensure that all three actors in the scene line up
perfectly. However, when the animations are exported, they become three
separate clips, to be played on three separate objects in the game world. The
two characters might have been under AI or player control prior to the start of
this animated sequence. How, then, can we ensure that the three objects line
up correctly with one another when the three clips are played back in-game?
Reference Locators
One good solution is to introduce a common reference point into all three
animation clips. In Maya, the animator can drop a locator (which is just a 3D
transform, much like a skeletal joint) into the scene, placing it anywhere that
seems convenient. Its location and orientation are actually irrelevant, as we’ll
see. The locator is tagged in some way to tell the animation export tools that
it is to be treated specially.
When the three animation clips are exported, the tools store the position
and orientation of the reference locator, expressed in coordinates that are rela-
tive to the local object space of each actor, into all three clip’s data ﬁles. Later,
when the three clips are played back in-game, the animation engine can look
up the relative position and orientation of the reference locator in all three
clips. It can then transform the origins of the three objects in such a way as
to make all three reference locators coincide in world space. The reference
locator acts much like an attach point (Section 11.11.5.1) and, in fact, could be

11.11. Action State Machines
639
ymaya
xmaya
Figure 11.62. Original Maya scene containing three actors
and a reference locator.
yA
xA
yB
xB
xC
yC
Figure 11.63. The reference locator is encoded in each
actor’s animation ﬁle.
implemented as one. The net effect—all three actors now line up with one
another, exactly as they had been aligned in the original Maya scene.
Figure 11.62 illustrates how the door and the two characters from the above
example might be set up in a Maya scene. As shown in Figure 11.63, the ref-
erence locator appears in each exported animation clip (expressed in that ac-
tor’s local space). In-game, these local-space reference locators are aligned to
a ﬁxed world-space locator in order to realign the actors, as shown in Fig-
ure 11.64.
yworld
xworld
Figure 11.64. At runtime, the local-space reference transforms are aligned to a world-space refer-
ence locator, causing the actors to line up properly.
Finding the World-Space Reference Location
We’ve glossed over one important detail here—who decides what the world-
space position and orientation of the reference locator should be? Each anima-
tion clip provides the reference locator’s transform in the coordinate space of
its actor. But we need some way to deﬁne where that reference locator should
be in world space.
In our example with the door and the two characters shaking hands, one of
the actors is ﬁxed in the world (the door). So one viable solution is to ask the
door for the location of the reference locator and then align the two characters
to it. The commands to accomplish this might look similar to the following
pseudocode.

640
11. Animation Systems
void playShakingHandsDoorSequence(
Actor& door,
Actor& characterA,
Actor& characterB)
{
// Find the world-space transform of the reference
// locator as specified in the door's animation.
Transform refLoc = getReferenceLocatorWs(door,
"shake-hands-door");
// Play the door's animation in-place. (It's
// already in the correct place.)
playAnimation("shake-hands-door", door);
// Play the two characters' animations relative to
// the world-space reference locator obtained from
// the door.
playAnimationRelativeToReference(
"shake-hands-character-a", characterA, refLoc);
playAnimationRelativeToReference(
"shake-hands-character-b", characterB, refLoc);
}
Another option is to deﬁne the world-space transform of the reference lo-
cator independently of the three actors in the scene. We could place the ref-
erence locator into the world using our world-building tool, for example (see
Section 14.3). In this case, the pseudocode above should be changed to look
something like this:
void playShakingHandsDoorSequence(
Actor& door,
Actor& characterA,
Actor& characterB,
Actor& refLocatorActor)
{
// Find the world-space transform of the reference
// locator by simply querying the transform of an
// independent actor (presumably placed into the
// world manually).
Transform refLoc = getActorTransformWs(
refLocatorActor);
// Play all animations relative to the world-space
// reference locator obtained above.
playAnimationRelativeToReference("shake-hands-door",
door, refLoc);
playAnimationRelativeToReference(

11.11. Action State Machines
641
"shake-hands-character-a", characterA, refLoc);
playAnimationRelativeToReference(
"shake-hands-character-b", characterB, refLoc);
}
11.11.5.3
Grabbing and Hand IK
Even after using an attachment to connect two objects, we sometimes ﬁnd that
the alignment does not look exactly right in-game. For example, a character
might be holding a riﬂe in her right hand, with her left hand supporting the
stock. As the character aims the weapon in various directions, we may notice
that the left hand no longer aligns properly with the stock at certain aim an-
gles. This kind of joint misalignment is caused by LERP blending. Even if the
joints in question are aligned perfectly in clip A and in clip B, LERP blending
does not guarantee that those joints will be in alignment when A and B are
blended together.
One solution to this problem is to use inverse kinematics (IK) to correct the
position of the left hand. The basic approach is to determine the desired tar-
get position for the joint in question. IK is then applied to a short chain of
joints (usually two, three or four joints), starting with the joint in question and
progressing up the hierarchy to its parent, grandparent and so on. The joint
whose position we are trying to correct is known as the end effector. The IK
solver adjusts the orientations of the end effector’s parent joint(s) in order to
get the end effector as close as possible to the target.
The API for an IK system usually takes the form of a request to enable or
disable IK on a particular chain of joints, plus a speciﬁcation of the desired
target point. The actual IK calculation is usually done internally by the low-
level animation pipeline. This allows it to do the calculation at the proper
time—namely, after intermediate local and global skeletal poses have been
calculated but before the ﬁnal matrix palette calculation.
Some animation engines allow IK chains to be deﬁned a priori. For ex-
ample, we might deﬁne one IK chain for the left arm, one for the right arm
and two for the two legs. Let’s assume for the purposes of this example that
a particular IK chain is identiﬁed by the name of its end-effector joint. (Other
engines might use an index or handle or some other unique identiﬁer, but the
concept remains the same.) The function to enable an IK calculation might
look something like this:
void enableIkChain(Actor& actor,
const char* endEffectorJointName,
const Vector3& targetLocationWs);

642
11. Animation Systems
and the function to disable an IK chain might look like this:
void disableIkChain(Actor& actor,
const char* endEffectorJointName);
IK is usually enabled and disabled relatively infrequently, but the world-
space target location must be kept up-to-date every frame (if the target is mov-
ing). Therefore, the low-level animation pipeline always provides some mech-
anism for updating an active IK target point. For example, the pipeline might
allow us to call enableIkChain() multiple times. The ﬁrst time it is called,
the IK chain is enabled, and its target point is set. All subsequent calls sim-
ply update the target point. Another way to keep IK targets up-to-date is to
link them to dynamic objects in the game. For example, an IK target might
be speciﬁed as a handle to a rigid game object, or a joint within an animated
object.
IK is well-suited to making minor corrections to joint alignment when the
joint is already reasonably close to its target. It does not work nearly as well
when the error between a joint’s desired location and its actual location is
large. Note also that most IK algorithms solve only for the position of a joint.
You may need to write additional code to ensure that the orientation of the end
effector aligns properly with its target as well. IK is not a cure-all, and it may
have signiﬁcant performance costs. So always use it judiciously.
11.11.5.4
Motion Extraction and Foot IK
In games, we usually want the locomotion animations of our characters to
look realistic and “grounded.” One of the biggest factors contributing to the
realism of a locomotion animation is whether or not the feet slide around on
the ground. Foot sliding can be overcome in a number of ways, the most com-
mon of which are motion extraction and foot IK.
Motion Extraction
Let’s imagine how we’d animate a character walking forward in a straight
line. In Maya (or his or her animation package of choice), the animator makes
the character take one complete step forward, ﬁrst with the left foot and then
with the right foot. The resulting animation clip is known as a locomotion cycle,
because it is intended to be looped indeﬁnitely, for as long as the character is
walking forward in-game. The animator takes care to ensure that the feet of
the character appear grounded and don’t slide as it moves. The character
moves from its initial location on frame 0 to a new location at the end of the
cycle. This is shown in Figure 11.65.

11.11. Action State Machines
643
Figure 11.65.
In the animation authoring package, the character moves forward in space, and
its feet appear grounded. Image courtesy of Naughty Dog, Inc. (UNCHARTED: Drake’s Fortune
© 2007/® SCEA. Created and developed by Naughty Dog.)
Notice that the local-space origin of the character remains ﬁxed during the
entire walk cycle. In effect, the character is “leaving his origin behind him” as
he takes his step forward. Now imagine playing this animation as a loop. We
would see the character take one complete step forward, and then pop back
to where he was on the ﬁrst frame of the animation. Clearly this won’t work
in-game.
To make this work, we need to remove the forward motion of the character,
so that his local-space origin remains roughly under the center of mass of the
character at all times. We could do this by zeroing out the forward translation
of the root joint of the character’s skeleton. The resulting animation clip would
make the character look like he’s “moonwalking,” as shown in Figure 11.66.
Figure 11.66. Walk cycle after zeroing out the root joint’s forward motion. Image courtesy of
Naughty Dog, Inc. (UNCHARTED: Drake’s Fortune © 2007/® SCEA. Created and developed by
Naughty Dog.)

644
11. Animation Systems
Figure 11.67. Walk cycle in-game, with extracted root motion data applied to the local-space origin
of the character. Image courtesy of Naughty Dog, Inc. (UNCHARTED: Drake’s Fortune © 2007/®
SCEA. Created and developed by Naughty Dog.)
In order to get the feet to appear to “stick” to the ground the way they did
in the original Maya scene, we need the character to move forward by just the
right amount each frame. We could look at the distance the character moved,
divide by the amount of time it took for him to get there, and hence ﬁnd his
average movement speed. But a character’s forward speed is not constant
when walking. This is especially evident when a character is limping (quick
forward motion on the injured leg, followed by slower motion on the “good”
leg), but it is true for all natural-looking walk cycles.
Therefore, before we zero out the forward motion of the root joint, we ﬁrst
save the animation data in a special “extracted motion” channel. This data can
be used in-game to move the local-space origin of the character forward by the
exact amount that the root joint had moved in Maya each frame. The net result
is that the character will walk forward exactly as he was authored, but now
his local-space origin comes along for the ride, allowing the animation to loop
properly. This is shown in Figure 11.67.
If the character moves forward by 4 feet in the animation and the anima-
tion takes one second to complete, then we know that the character is moving
at an average speed of 4 feet/second. To make the character walk at a differ-
ent speed, we can simply scale the playback rate of the walk cycle animation.
For example, to make the character walk at 2 feet/second, we can simply play
the animation at half speed (R = 0.5).
Foot IK
Motion extraction does a good job of making a character’s feet appear ground-
ed when it is moving in a straight line (or, more correctly, when it moves in a
path that exactly matches the path animated by the animator). However, a real
game character must be turned and moved in ways that don’t coincide with

11.11. Action State Machines
645
the original hand-animated path of motion (e.g., when moving over uneven
terrain). This results in additional foot sliding.
One solution to this problem is to use IK to correct for any sliding in the
feet. The basic idea is to analyze the animations to determine during which
periods of time each foot is fully in contact with the ground. At the moment
a foot contacts the ground, we note its world-space location. For all subse-
quent frames while that foot remains on the ground, we use IK to adjust the
pose of the leg so that the foot remains ﬁxed to the proper location. This tech-
nique sounds easy enough, but getting it to look and feel right can be very
challenging. It requires a lot of iteration and ﬁne-tuning. And some natural
human motions—like leading into a turn by increasing your stride—cannot
be produced by IK alone.
In addition, there is a big trade-off between the look of the animations and
the feel of the character, particularly for a human-controlled character. It’s
generally more important for the player character control system to feel re-
sponsive and fun than it is for the character’s animations to look perfect. The
upshot is this: Do not take the task of adding foot IK or motion extraction to
your game lightly. Budget time for a lot of trial and error, and be prepared to
make trade-offs to ensure that your player character not only looks good but
feels good as well.
11.11.5.5
Other Kinds of Constraints
There are plenty of other possible kinds of constraint systems that can be
added to a game animation engine. Some examples include:
•
Look-at. This is the ability for characters to look at points of interest in
the environment. A character might look at a point with only his or her
eyes, with eyes and head, or with eyes, head and a twist of the entire
upper body. Look-at constraints are sometimes implemented using IK
or procedural joint offsets, although a more natural look can often be
achieved via additive blending.
•
Cover registration. This is the ability for a character to align perfectly
with an object that is serving as cover. This is often implemented via the
reference locator technique described above.
•
Cover entry and departure. If a character can take cover, animation blend-
ing and custom entry and departure animations must usually be used
to get the character into and out of cover.
•
Traversal aids. The ability for a character to navigate over, under, around
or through obstacles in the environment can add a lot of life to a game.
This is often done by providing custom animations and using a reference
locator to ensure proper registration with the obstacle being overcome.

646
11. Animation Systems
11.12
Animation Controllers
The animation pipeline provides high-speed animation posing and blending
facilities, but its interface is usually too cumbersome to be used directly by
gameplay code. The action state machine provides a more convenient inter-
face by allowing complex blend trees to be described, often in a data-driven
manner, and then encapsulated within easy-to-understand logical states. Tran-
sitions between states can also be deﬁned, again often in a data-driven way,
so that gameplay code can be written in a ﬁre-and-forget manner, without
having to micromanage every transition. The ASM system may also provide
a layering mechanism, allowing the motion of a character to be described by
multiple state machines running in parallel. But even given the relatively con-
venient interface provided by the action state machine, some game teams ﬁnd
it convenient to introduce a third layer of software, aimed at providing higher-
level control over how characters animate. As such, it is often implemented
as a collection of classes known as animation controllers.
Controllers tend to manage behaviors over relatively long periods of time—
on the order of a few seconds or more. Each animation controller is typically
responsible for one type of gross character behavior, like how to behave when
in cover, how to behave when locomoting from one place to another in the
game world, or how to drive a vehicle. A controller typically orchestrates all
aspects of the character’s animation-related behavior. It adjusts blend factors
to control movement directions, aiming and so on, manages state transitions,
fades layers in and out and does whatever else is needed to make the character
behave as desired.
One beneﬁt of a controller-based design is that all of the code relating to
a particular behavioral category is localized in one place. This design also
permits higher-level gameplay systems, like player mechanics or artiﬁcial in-
telligence, to be written in a much simpler way, because all of the details of
micromanaging the animations can be extracted and hidden within the con-
trollers.
The animation controller layer takes many different forms and is highly
dependent upon the needs of the game and the software design philosophies
of the engineering team. Some teams don’t use animation controllers at all.
On other teams, the animation controllers may be tightly integrated into the
AI and/or player mechanics systems. Still other teams implement a suite of
relatively general-purpose controllers that can be shared between the player
character and the NPCs. For better or for worse, there is no one standard way
to implement animation controllers in the game industry (at least not yet).

12
Collision and Rigid
Body Dynamics
I
n the real world, solid objects are inherently, well...solid. They generally
avoid doing impossible things, like passing through one another, all by
themselves. But in a virtual game world, objects don’t do anything unless we
tell them to, and game programmers must make an explicit effort to ensure
that objects do not pass through one another. This is the role of one of the
central components of any game engine—the collision detection system.
A game engine’s collision system is often closely integrated with a physics
engine. Of course, the ﬁeld of physics is vast, and what most of today’s game
engines call “physics” is more accurately described as a rigid body dynamics
simulation. A rigid body is an idealized, inﬁnitely hard, non-deformable solid
object. The term dynamics refers to the process of determining how these rigid
bodies move and interact over time under the inﬂuence of forces. A rigid body
dynamics simulation allows motion to be imparted to objects in the game in a
highly interactive and naturally chaotic manner—an effect that is much more
difﬁcult to achieve when using canned animation clips to move things about.
A dynamics simulation makes heavy use of the collision detection system
in order to properly simulate various physical behaviors of the objects in the
simulation, including bouncing off one another, sliding under friction, rolling
and coming to rest. Of course, a collision detection system can be used stand-
alone, without a dynamics simulation—many games do not have a “physics”
647

648
12. Collision and Rigid Body Dynamics
system at all. But all games that involve objects moving about in two- or
three-dimensional space have some form of collision detection.
In this chapter, we’ll investigate the architecture of both a typical collision
detection system and a typical physics (rigid body dynamics) system. As we
investigate the components of these two closely interrelated systems, we’ll
take a look at the mathematics and the theory that underlie them.
12.1
Do You Want Physics in Your Game?
Nowadays, most game engines have some kind of physical simulation capa-
bilities. Some physical effects, like rag doll deaths, are simply expected by
gamers. Other effects, like ropes, cloth, hair or complex physically driven ma-
chinery can add that je ne sais quoi that sets a game apart from its competitors.
In recent years, some game studios have started experimenting with advanced
physical simulations, including approximate real-time ﬂuid mechanics effects
and simulations of deformable bodies. But adding physics to a game is not
without costs, and before we commit ourselves to implementing an exhaus-
tive list of physics-driven features in our game, we should (at the very least)
understand the trade-offs involved.
12.1.1
Things You Can Do with a Physics System
Here are just a few of the things you can do or have with a game physics
system.
•
Detect collisions between dynamic objects and static world geometry.
•
Simulate free rigid bodies under the inﬂuence of gravity and other forces.
•
Spring-mass systems.
•
Destructible buildings and structures.
•
Ray and shape casts (to determine line of sight, bullet impacts, etc.).
•
Trigger volumes (determine when objects enter, leave or are inside pre-
deﬁned regions in the game world).
•
Complex machines (cranes, moving platform puzzles and so on).
•
Traps (such as an avalanche of boulders).
•
Drivable vehicles with realistic suspensions.
•
Rag doll character deaths.
•
Powered rag doll: a realistic blend between traditional animation and
rag doll physics.

12.1. Do You Want Physics in Your Game?
649
•
Dangling props (canteens, necklaces, swords), semi-realistic hair, cloth-
ing movements.
•
Cloth simulations.
•
Water surface simulations and buoyancy.
•
Audio propagation.
And the list goes on.
We should note here that in addition to running a physics simulation at
runtime in our game, we can also run a simulation as part of an ofﬂine pre-
processing step in order to generate an animation clip. A number of physics
plug-ins are available for animation tools like Maya.
This is also the ap-
proach taken by the Endorphin1 package by NaturalMotion, Inc. (http://www.
naturalmotion.com/endorphin.htm). In this chapter, we’ll restrict our discus-
sion to runtime rigid body dynamics simulations, but ofﬂine tools are a pow-
erful option, of which we should always remain aware as we plan our game
projects.
12.1.2
Is Physics Fun?
The presence of a rigid body dynamics system in a game does not necessarily
make the game fun. More often than not, the inherently chaotic behavior of a
physics sim can actually detract from the gameplay experience rather than en-
hancing it. The fun derived from physics depends on many factors, including
the quality of the simulation itself, the care with which it has been integrated
with other engine systems, the selection of physics-driven gameplay elements
versus elements that are controlled in a more direct manner, how the physical
elements interact with the goals of the player and the abilities of the player
character, and the genre of game being made.
Let’s take a look at a few broad game genres and how a rigid body dynam-
ics system might ﬁt into each one.
12.1.2.1
Simulations (Sims)
The primary goal of a sim is to accurately reproduce a real-life experience. Ex-
amples include the Flight Simulator, Gran Turismo and NASCAR Racing series
of games. Clearly, the realism provided by a rigid body dynamics system ﬁts
extremely well into these kinds of games.
1NaturalMotion also offers a runtime version of Endorphin called Euphoria.

650
12. Collision and Rigid Body Dynamics
12.1.2.2
Physics Puzzle Games
The whole idea of a physics puzzle is to let the user play around with dynam-
ically simulated toys. So obviously this kind of game relies almost entirely on
physics for its core mechanic. Examples of this genre include Bridge Builder,
The Incredible Machine, the online game Fantastic Contraption and Crayon Physics
for the iPhone.
12.1.2.3
Sandbox Games
In a sandbox game, there may be no objectives at all, or there may be a large
number of optional objectives. The player’s primary objective is usually to
“mess around” and explore what the objects in the game world can be made
to do. Examples of sandbox games include Grand Theft Auto V, Spore, Little-
BigPlanet 2, TearAway and of course Minecraft.
Sandbox games can put a realistic dynamics simulation to good use, es-
pecially if much of the fun is derived from playing with realistic (or semi-
realistic) interactions between objects in the game world. So in these contexts,
physics can be fun in and of itself. However, many games trade realism for an
increased fun factor (e.g., larger-than-life explosions, gravity that is stronger
or weaker than normal, etc.). So the dynamics simulation may need to be
tweaked in various ways to achieve the right “feel.”
12.1.2.4
Goal-Based and Story-Driven Games
A goal-based game has rules and speciﬁc objectives that the player must ac-
complish in order to progress; in a story-driven game, telling a story is of
paramount importance. Integrating a physics system into these kinds of games
can be tricky. We generally give away control in exchange for a realistic simula-
tion, and this loss of control can inhibit the player’s ability to accomplish goals
or the game’s ability to tell the story.
For example, in a character-based platformer game, we want the player
character to move in ways that are fun and easy to control but not necessarily
physically realistic. In a war game, we might want a bridge to explode in a
realistic way, but we also may want to ensure that the debris doesn’t end up
blocking the player’s only path forward. In these kinds of games, physics is
often not necessarily fun, and in fact it can often get in the way of fun when
the player’s goals are at odds with the physically simulated behaviors of the
objects in the game world. Therefore, developers must be careful to apply
physics judiciously and take steps to control the behavior of the simulation
in various ways to ensure it doesn’t get in the way of gameplay. It’s usually
a good idea to provide the player with a way out of difﬁcult situations, too.

12.1. Do You Want Physics in Your Game?
651
A good example of this can be found in the Halo series of games, where the
player can press X to ﬂip over a vehicle that has landed upside-down.
12.1.3
Impact of Physics on a Game
Adding a physics simulation to a game can have all sorts of impacts on the
project and the gameplay. Here are a few examples across various game de-
velopment disciplines.
12.1.3.1
Design Impacts
•
Predictability. The inherent chaos and variability that sets a physically
simulated behavior apart from an animated one is also a source of un-
predictability. If something absolutely must happen a certain way every
time, it’s usually better to animate it than to try to coerce your dynamics
simulation into producing the motion reliably.
•
Tuning and control. The laws of physics (when modeled accurately) are
ﬁxed. In a game, we can tweak the value of gravity or the coefﬁcient
of restitution of a rigid body, which gives back some degree of control.
However, the results of tweaking physics parameters are often indirect
and difﬁcult to visualize. It’s much harder to tweak a force in order to
get a character to move in the desired direction than it is to tweak an
animation of a character walking.
•
Emergent behaviors. Sometimes physics introduces unexpected features
into a game—for example, the rocket-launcher jump trick in Team Fortress
Classic, the high-ﬂying exploding Warthog in Halo and the ﬂying “surf-
boards” in PsyOps.
In general, the game design should usually drive the physics requirements
of a game engine—not the other way around.
12.1.3.2
Engineering Impacts
•
Tools pipeline. A good collision/physics pipeline takes time to build and
maintain.
•
User interface. How does the player control the physics objects in the
world? Does he or she shoot them? Walk into them? Pick them up?
Does he or she hold them using virtual arms, as in Trespasser? Or using
a “gravity gun,” as in Half-Life 2?
•
Collision detection. Collision models intended for use within a dynamics
simulation may need to be more detailed and more carefully constructed
than their non-physics-driven counterparts.

652
12. Collision and Rigid Body Dynamics
•
AI. Pathing may not be predictable in the presence of physically simu-
lated objects. The engine may need to handle dynamic cover points that
can move or blow up. Can the AI use the physics to its advantage?
•
Misbehaved objects. Animation-driven objects can clip slightly through
one another with few or no ill effects. But when driven by a dynamics
simulation, objects may bounce off one another in unexpected ways or
jitter badly. Collision ﬁltering may need to be applied to permit objects
to interpenetrate slightly. Mechanisms may need to be put in place to
ensure that objects settle and go to sleep properly.
•
Rag doll physics. Rag dolls require a lot of ﬁne-tuning and often suffer
from instability in the simulation. An animation may drive parts of a
character’s body into penetration with other collision volumes—when
the character turns into a rag doll, these interpenetrations can cause
enormous instability. Steps must be taken to avoid this.
•
Graphics. Physics-driven motion can have an effect on renderable ob-
jects’ bounding volumes (where they would otherwise be static or more
predictable). The presence of destructible buildings and objects can in-
validate some kinds of precomputed lighting and shadow methods.
•
Networking and multiplayer. Physics effects that do not affect gameplay
may be simulated exclusively (and independently) on each client ma-
chine. However, physics that has an effect on gameplay (such as the
trajectory that a grenade follows) must be simulated on the server and
accurately replicated on all clients.
•
Record and playback. The ability to record gameplay and play it back at a
later time is very useful as a debugging/testing aid, and it can also serve
as a fun game feature. This feature is difﬁcult to implement because it
requires every engine system to behave in a deterministic manner, so
that everything will play out exactly in the same way during playback
as it did when the recording was made. If your physics simulation isn’t
deterministic, this can become a major ﬂy in the ointment.
12.1.3.3
Art Impacts
•
Additional tool and workﬂow complexity. The need to rig up objects with
mass, friction, constraints and other attributes for consumption by the
dynamics simulation makes the art department’s job more difﬁcult as
well.
•
More-complex content. We may need multiple visually identical versions
of an object with different collision and dynamics conﬁgurations for dif-

12.2. Collision/Physics Middleware
653
ferent purposes—for example, a pristine version and a destructible ver-
sion.
•
Loss of control. The unpredictability of physics-driven objects can make
it difﬁcult to control the artistic composition of a scene.
12.1.3.4
Other Impacts
•
Interdisciplinary impacts. The introduction of a dynamics simulation into
your game requires close cooperation between engineering, art, audio
and design.
•
Production impacts. Physics can add to a project’s development costs,
technical and organizational complexity and risk.
Having explored the impacts, most teams today do choose to integrate a
rigid body dynamics system into their games. With some careful planning
and wise choices along the way, adding physics to your game can be reward-
ing and fruitful. And as we’ll see below, third-party middleware is making
physics more accessible than ever.
12.2
Collision/Physics Middleware
Writing a collision system and rigid body dynamics simulation is challenging
and time-consuming work. The collision/physics system of a game engine
can account for a signiﬁcant percentage of the source code in a typical game
engine. That’s a lot of code to write and maintain!
Thankfully, a number of robust, high-quality collision/physics engines are
now available, either as commercial products or in open source form. Some
of these are listed below. For a discussion of the pros and cons of various
physics SDKs, check out the online game development forums (e.g., http://
www.gamedev.net/community/forums/topic.asp?topic_id=463024).
12.2.1
I-Collide, SWIFT, V-Collide and RAPID
I-Collide is an open source collision detection library developed by the Uni-
versity of North Carolina at Chapel Hill (UNC). It can detect intersections be-
tween convex volumes. I-Collide has been replaced by a faster, more feature-
rich library called SWIFT. UNC has also developed collision detection libraries
that can handle complex non-convex shapes, called V-Collide and RAPID.
None of these libraries can be used right out of the box in a game, but they
might provide a good basis upon which to build a fully functional game
collision detection engine. You can read more about I-Collide, SWIFT and

654
12. Collision and Rigid Body Dynamics
the other UNC geometry libraries at http://www.cs.unc.edu/∼geom/I_
COLLIDE/.
12.2.2
ODE
ODE stands for “Open Dynamics Engine” (http://www.ode.org). As its name
implies, ODE is an open source collision and rigid body dynamics SDK. Its
feature set is similar to a commercial product like Havok. Its beneﬁts in-
clude being free (a big plus for small game studios and school projects!) and
the availability of full source code (which makes debugging much easier and
opens up the possibility of modifying the physics engine to meet the speciﬁc
needs of a particular game).
12.2.3
Bullet
Bullet is an open source collision detection and physics library used by both
the game and ﬁlm industries. Its collision engine is integrated with its dy-
namics simulation, but hooks are provided so that the collision system can
be used stand-alone or integrated with other physics engines. It supports
continuous collision detection (CCD)—also known as time of impact (TOI) col-
lision detection—which, as we’ll see below, can be extremely helpful when a
simulation includes small, fast-moving objects. The Bullet SDK is available
for download at http://code.google.com/p/bullet/, and the Bullet wiki is
located at http://www.bulletphysics.com/mediawiki-1.5.8/index.php?title=
Main_Page.
12.2.4
TrueAxis
TrueAxis is another collision/physics SDK. It is free for non-commercial use.
You can learn more about TrueAxis at http://trueaxis.com.
12.2.5
PhysX
PhysX started out as a library called Novodex, produced and distributed by
Ageia as part of their strategy to market their dedicated physics coprocessor. It
was bought by NVIDIA and retooled so that it can run using NVIDIA’s GPUs
as a coprocessor. (It can also run entirely on a CPU, without GPU support.)
It is available at http://www.nvidia.com/object/nvidia_physx.html. Part of
Ageia’s and NVIDIA’s marketing strategy has been to provide the CPU ver-
sion of the SDK entirely for free, in order to drive the physics coprocessor
market forward. Developers can also pay a fee to obtain full source code and
the ability to customize the library as needed. PhysX is now combined with

12.3. The Collision Detection System
655
APEX, NVIDIA’s scalable multiplatform dynamics framework. PhysX/APEX
is available for Windows, Linux, Mac, Android, Xbox 360, PlayStation 3, Xbox
One, PlayStation 4 and Wii.
12.2.6
Havok
Havok is the gold standard in commercial physics SDKs, providing one of the
richest feature sets available and boasting excellent performance characteris-
tics on all supported platforms. (It’s also the most expensive solution.) Havok
is comprised of a core collision/physics engine, plus a number of optional
add-on products including a vehicle physics system, a system for modeling
destructible environments and a fully featured animation SDK with direct in-
tegration into Havok’s rag doll physics system. It is available on Xbox 360,
PlayStation 3, Xbox One, PlayStation 4, PlayStation Vita, Wii, Wii U, Win-
dows 8, Android, Apple Mac and iOS. You can learn more about Havok at
http://www.havok.com.
12.2.7
Physics Abstraction Layer (PAL)
The Physics Abstraction Layer (PAL) is an open source library that allows
developers to work with more than one physics SDK on a single project. It
provides hooks for PhysX (Novodex), Newton, ODE, OpenTissue, Tokamak,
TrueAxis and a few other SDKs. You can read more about PAL at http://www.
adrianboeing.com/pal/index.html.
12.2.8
Digital Molecular Matter (DMM)
Pixelux Entertainment S.A., located in Geneva, Switzerland, has produced a
unique physics engine that uses ﬁnite element methods to simulate the dy-
namics of deformable bodies and breakable objects, called Digital Molecu-
lar Matter (DMM). The engine has both an ofﬂine and a runtime compo-
nent. It was released in 2008 and can be seen in action in LucasArts’ Star
Wars: The Force Unleashed. A discussion of deformable body mechanics is
beyond our scope here, but you can read more about DMM at http://www.
pixeluxentertainment.com.
12.3
The Collision Detection System
The primary purpose of a game engine’s collision detection system is to de-
termine whether any of the objects in the game world have come into contact.

656
12. Collision and Rigid Body Dynamics
To answer this question, each logical object is represented by one or more ge-
ometric shapes. These shapes are usually quite simple, such as spheres, boxes
and capsules. However, more-complex shapes can also be used. The colli-
sion system determines whether or not any of the shapes are intersecting (i.e.,
overlapping) at any given moment in time. So a collision detection system is
essentially a gloriﬁed geometric intersection tester.
Of course, the collision system does more than answer yes/no questions
about shape intersection. It also provides relevant information about the na-
ture of each contact. Contact information can be used to prevent unrealistic
visual anomalies on-screen, such as objects interpenetrating one another. This
is generally accomplished by moving all interpenetrating objects apart prior
to rendering the next frame. Collisions can provide support for an object—one
or more contacts that together allow the object to come to rest, in equilibrium
with gravity and/or any other forces acting on it. Collisions can also be used
for other purposes, such as to cause a missile to explode when it strikes its
target or to give the player character a health boost when he passes through
a ﬂoating health pack. A rigid body dynamics simulation is often the most
demanding client of the collision system, using it to mimic physically realis-
tic behaviors like bouncing, rolling, sliding and coming to rest. But, of course,
even games that have no physics system can still make heavy use of a collision
detection engine.
In this chapter, we’ll go on a brief high-level tour of how collision detec-
tion engines work. For an in-depth treatment of this topic, a number of ex-
cellent books on real-time collision detection are available, including [12], [43]
and [9].
12.3.1
Collidable Entities
If we want a particular logical object in our game to be capable of colliding
with other objects, we need to provide it with a collision representation, describ-
ing the object’s shape and its position and orientation in the game world. This
is a distinct data structure, separate from the object’s gameplay representation
(the code and data that deﬁne its role and behavior in the game) and separate
from its visual representation (which might be an instance of a triangle mesh, a
subdivision surface, a particle effect or some other visual representation).
From the point of view of detecting intersections, we generally favor shapes
that are geometrically and mathematically simple. For example, a rock might
be modeled as a sphere for collision purposes; the hood of a car might be
represented by a rectangular box; a human body might be approximated by a
collection of interconnected capsules (pill-shaped volumes). Ideally, we should

12.3. The Collision Detection System
657
Figure 12.1. Simple geometric shapes are often used to approximate the collision volumes of the
objects in a game.
resort to a more-complex shape only when a simpler representation proves in-
adequate to achieve the desired behavior in the game. Figure 12.1 shows a few
examples of using simple shapes to approximate object volumes for collision
detection purposes.
Havok uses the term collidable to describe a distinct, rigid object that can
take part in collision detection. It represents each collidable with an instance
of the C++ class hkpCollidable. PhysX calls its rigid objects actors and
represents them as instances of the class NxActor. In both of these libraries, a
collidable entity contains two basic pieces of information—a shape and a trans-
form. The shape describes the collidable’s geometric form, and the transform
describes the shape’s position and orientation in the game world. Collidables
need transforms for three reasons:
1.
Technically speaking, a shape only describes the form of an object (i.e.,
whether it is a sphere, a box, a capsule or some other kind of volume).
It may also describe the object’s size (e.g., the radius of a sphere or the
dimensions of a box). But a shape is usually deﬁned with its center
at the origin and in some sort of canonical orientation relative to the
coordinate axes. To be useful, a shape must therefore be transformed in
order to position and orient it appropriately in world space.
2.
Many of the objects in a game are dynamic. Moving an arbitrarily com-
plex shape through space could be expensive if we had to move the fea-
tures of the shape (vertices, planes, etc.) individually. But with a trans-
form, any shape can be moved in space inexpensively, no matter how
simple or complex the shape’s features may be.

658
12. Collision and Rigid Body Dynamics
3.
The information describing some of the more-complex kinds of shapes
can take up a nontrivial amount of memory. So, it can be beneﬁcial to
permit more than one collidable to share a single shape description. For
example, in a racing game, the shape information for many of the cars
might be identical. In that case, all of the car collidables in the game can
share a single car shape.
Any particular object in the game may have no collidable at all (if it doesn’t
require collision detection services), a single collidable (if the object is a simple
rigid body) or multiple collidables (each representing one rigid component of
an articulated robot arm, for example).
12.3.2
The Collision/Physics World
A collision system typically keeps track of all of its collidable entities via a
singleton data structure known as the collision world. The collision world is a
complete representation of the game world designed explicitly for use by the
collision detection system. Havok’s collision world is an instance of the class
hkpWorld. Likewise, the PhysX world is an instance of NxScene. ODE uses
an instance of class dSpace to represent the collision world; it is actually the
root of a hierarchy of geometric volumes representing all the collidable shapes
in the game.
Maintaining all collision information in a private data structure has a num-
ber of advantages over attempting to store collision information with the game
objects themselves. For one thing, the collision world need only contain col-
lidables for those game objects that can potentially collide with one another.
This eliminates the need for the collision system to iterate over any irrelevant
data structures. This design also permits collision data to be organized in the
most efﬁcient manner possible. The collision system can take advantage of
cache coherency to maximize performance, for example. The collision world
is also an effective encapsulation mechanism, which is generally a plus from
the perspectives of understandability, maintainability, testability and the po-
tential for software reuse.
12.3.2.1
The Physics World
If a game has a rigid body dynamics system, it is usually tightly integrated
with the collision system. It typically shares its “world” data structure with
the collision system, and each rigid body in the simulation is usually associ-
ated with a single collidable in the collision system. This design is common-
place among physics engines because of the frequent and detailed collision
queries required by the physics system. It’s typical for the physics system to

12.3. The Collision Detection System
659
actually drive the operation of the collision system, instructing it to run col-
lision tests at least once, and sometimes multiple times, per simulation time
step. For this reason, the collision world is often called the collision/physics
world or sometimes just the physics world.
Each dynamic rigid body in the physics simulation is usually associated
with a single collidable object in the collision system (although not all collid-
ables need be dynamic rigid bodies). For example, in Havok, a rigid body is
represented by an instance of the class hkpRigidBody, and each rigid body
has a pointer to exactly one hkpCollidable. In PhysX, the concepts of col-
lidable and rigid body are comingled—the NxActor class serves both pur-
poses (although the physical properties of the rigid body are stored separately,
in an instance of NxBodyDesc). In both SDKs, it is possible to tell a rigid body
that its location and orientation are to be ﬁxed in space, meaning that it will
be omitted from the dynamics simulation and will serve as a collidable only.
Despite this tight integration, most physics SDKs do make at least some
attempt to separate the collision library from the rigid body dynamics simu-
lation. This permits the collision system to be used as a stand-alone library
(which is important for games that don’t need physics but do need to de-
tect collisions). It also means that a game studio could theoretically replace a
physics SDK’s collision system entirely, without having to rewrite the dynam-
ics simulation. (Practically speaking, this may be a bit harder than it sounds!)
12.3.3
Shape Concepts
A rich body of mathematical theory underlies the everyday concept of shape
(see http://en.wikipedia.org/wiki/Shape). For our purposes, we can think of
a shape simply as a region of space described by a boundary, with a deﬁnite
inside and outside. In two dimensions, a shape has area, and its boundary is
deﬁned either by a curved line or by three or more straight edges (in which
case it’s a polygon). In three dimensions, a shape has volume, and its boundary
is either a curved surface or is composed of polygons (in which case is it called
a polyhedron).
It’s important to note that some kinds of game objects, like terrain, rivers or
thin walls, might be best represented by surfaces. In three-space, a surface is a
two-dimensional geometric entity with a front and a back but no inside or out-
side. Examples include planes, triangles, subdivision surfaces and surfaces
constructed from a group of connected triangles or other polygons. Most col-
lision SDKs provide support for surface primitives and extend the term shape
to encompass both closed volumes and open surfaces.
It’s commonplace for collision libraries to allow surfaces to be given vol-
ume via an optional extrusion parameter. Such a parameter speciﬁes how

660
12. Collision and Rigid Body Dynamics
“thick” a surface should be. Doing this helps reduce the occurrence of missed
collisions between small, fast-moving objects and inﬁnitesimally thin surfaces
(the so-called “bullet through paper” problem—see Section 12.3.5.7).
12.3.3.1
Intersection
We all have an intuitive notion of what an intersection is. Technically speaking,
the term comes from set theory (http://en.wikipedia.org/wiki/Intersection_
(set_theory)). The intersection of two sets is comprised of the subset of mem-
bers that are common to both sets. In geometrical terms, the intersection be-
tween two shapes is just the (inﬁnitely large!) set of all points that lie inside
both shapes.
12.3.3.2
Contact
In games, we’re not usually interested in ﬁnding the intersection in the strictest
sense, as a set of points. Instead, we want to know simply whether or not two
objects are intersecting. In the event of a collision, the collision system will
usually provide additional information about the nature of the contact. This
information allows us to separate the objects in a physically plausible and ef-
ﬁcient way, for example.
Collision systems usually package contact information into a convenient
data structure that can be instanced for each contact detected. For example,
Havok returns contacts as instances of the class hkContactPoint. Contact
information often includes a separating vector—a vector along which we can
slide the objects in order to efﬁciently move them out of collision. It also typ-
ically contains information about which two collidables were in contact, in-
cluding which individual shapes were intersecting and possibly even which
individual features of those shapes were in contact. The system may also re-
turn additional information, such as the velocity of the bodies projected onto
the separating normal.
12.3.3.3
Convexity
One of the most important concepts in the ﬁeld of collision detection is the
distinction between convex and non-convex (i.e., concave) shapes. Technically, a
convex shape is deﬁned as one for which no ray originating inside the shape
will pass through its surface more than once. A simple way to determine if
a shape is convex is to imagine shrink-wrapping it with plastic ﬁlm—if it’s
convex, no air pockets will be left under the ﬁlm. So in two dimensions, cir-
cles, rectangles and triangles are all convex, but Pac Man is not. The concept
extends equally well to three dimensions.

12.3. The Collision Detection System
661
The property of convexity is important because, as we’ll see, it’s generally
simpler and less computationally intensive to detect intersections between
convex shapes than concave ones. See http://en.wikipedia.org/wiki/Convex
for more information about convex shapes.
12.3.4
Collision Primitives
Collision detection systems can usually work with a relatively limited set of
shape types. Some collision systems refer to these shapes as collision primitives
because they are the fundamental building blocks out of which more-complex
shapes can be constructed. In this section, we’ll take a brief look at some of
the most common types of collision primitives.
12.3.4.1
Spheres
The simplest three-dimensional volume is a sphere. And as you might expect,
spheres are the most efﬁcient kind of collision primitive. A sphere is repre-
sented by a center point and a radius. This information can be conveniently
packed into a four-element ﬂoating-point vector—a format that works partic-
ularly well with SIMD math libraries.
12.3.4.2
Capsules
A capsule is a pill-shaped volume, composed of a cylinder and two hemi-
spherical end caps. It can be thought of as a swept sphere—the shape that is
traced out as a sphere moves from point A to point B. (There are, however,
some important differences between a static capsule and a sphere that sweeps
out a capsule-shaped volume over time, so the two are not identical.) Cap-
sules are often represented by two points and a radius (Figure 12.2). Capsules
are more efﬁcient to intersect than cylinders or boxes, so they are often used to
model objects that are roughly cylindrical, such as the limbs of a human body.
r
r
2
1
Figure 12.2. A capsule can be represented by two points and a radius.
12.3.4.3
Axis-Aligned Bounding Boxes
An axis-aligned bounding box (AABB) is a rectangular volume (technically
known as a cuboid) whose faces are parallel to the axes of the coordinate sys-

662
12. Collision and Rigid Body Dynamics
y
x
xmin
xmax
ymin
ymax
Figure 12.3. An axis-aligned box.
tem. Of course, a box that is axis-aligned in one coordinate system will not
necessarily be axis-aligned in another. So we can only speak about an AABB
in the context of the particular coordinate frame(s) with which it aligns.
An AABB can be conveniently deﬁned by two points: one containing the
minimum coordinates of the box along each of the three principal axes and the
other containing its maximum coordinates. This is depicted in Figure 12.3.
The primary beneﬁt of axis-aligned boxes is that they can be tested for
interpenetration with other axis-aligned boxes in a highly efﬁcient manner.
The big limitation of using AABBs is that they must remain axis-aligned at
all times if their computational advantages are to be maintained. This means
that if an AABB is used to approximate the shape of an object in the game,
the AABB will have to be recalculated whenever that object rotates. Even if
an object is roughly box-shaped, its AABB may degenerate into a very poor
approximation to its shape when the object rotates off-axis. This is shown in
Figure 12.4.
y
x
y
x
Figure 12.4.
An AABB is only a good approximation to a box-shaped object when the object’s
principal axes are roughly aligned with the coorindate system’s axes.
12.3.4.4
Oriented Bounding Boxes
If we permit an axis-aligned box to rotate relative to its coordinate system,
we have what is known as an oriented bounding box (OBB). It is often repre-

12.3. The Collision Detection System
663
sented by three half-dimensions (half-width, half-depth and half-height) and
a transformation, which positions the center of the box and deﬁnes its orien-
tation relative to the coordinate axes. Oriented boxes are a commonly used
collision primitive because they do a better job at ﬁtting arbitrarily oriented
objects, yet their representation is still quite simple.
12.3.4.5
Discrete Oriented Polytopes (DOP)
A discrete oriented polytope (DOP) is a more-general case of the AABB and
OBB. It is a convex polytope that approximates the shape of an object. A DOP
can be constructed by taking a number of planes at inﬁnity and sliding them
along their normal vectors until they come into contact with the object whose
shape is to be approximated. An AABB is a 6-DOP in which the plane normals
are taken parallel to the coordinate axes. An OBB is also a 6-DOP in which the
plane normals are parallel to the object’s natural principal axes. A k-DOP
is constructed from an arbitrary number of planes k. A common method of
constructing a DOP is to start with an OBB for the object in question and then
bevel the edges and/or corners at 45 degrees with additional planes in an
attempt to yield a tighter ﬁt. An example of a k-DOP is shown in Figure 12.5.
Figure 12.5. An OBB that has been beveled on all eight corners is known as a 14-DOP.
12.3.4.6
Arbitrary Convex Volumes
Most collision engines permit arbitrary convex volume to be constructed by a
3D artist in a package like Maya. The artist builds the shape out of polygons
(triangles or quads). An ofﬂine tool analyzes the triangles to ensure that they
actually do form a convex polyhedron. If the shape passes the convexity test,
its triangles are converted into a collection of planes (essentially a k-DOP),
represented by k plane equations, or k points and k normal vectors. (If it
is found to be non-convex, it can still be represented by a polygon soup—
described in the next section.) This approach is depicted in Figure 12.6.
Convex volumes are more expensive to intersection-test than the simpler
geometric primitives we’ve discussed thus far. However, as we’ll see in Sec-

664
12. Collision and Rigid Body Dynamics
Figure 12.6. An arbitrary convex volume can be represented by a collection of intersecting planes.
tion 12.3.5.5, certain highly efﬁcient intersection-ﬁnding algorithms such as
GJK are applicable to these shapes because they are convex.
12.3.4.7
Poly Soup
Some collision systems also support totally arbitrary, non-convex shapes.
These are usually constructed out of triangles or other simple polygons. For
this reason, this type of shape is often called a polygon soup, or poly soup for
short. Poly soups are often used to model complex static geometry, such as
terrain and buildings (Figure 12.7).
As you might imagine, detecting collisions with a poly soup is the most
expensive kind of collision test. In effect, the collision engine must test every
individual triangle, and it must also properly handle spurious intersections
with triangle edges that are shared between adjacent triangles. As a result,
most games try to limit the use of poly soup shapes to objects that will not
take part in the dynamics simulation.
Figure 12.7. A poly soup is often used to model complex static surfaces such as terrain or buildings.

12.3. The Collision Detection System
665
Does a Poly Soup Have an Inside?
Unlike convex and simple shapes, a poly soup does not necessarily represent
a volume—it can represent an open surface as well. Poly soup shapes often
don’t include enough information to allow the collision system to differentiate
between a closed volume and an open surface. This can make it difﬁcult to
know in which direction to push an object that is interpenetrating a poly soup
in order to bring the two objects out of collision.
Thankfully, this is by no means an intractable problem. Each triangle in
a poly soup has a front and a back, as deﬁned by the winding order of its
vertices. Therefore, it is possible to carefully construct a poly soup shape so
that all of the polygons’ vertex winding orders are consistent (i.e., adjacent
triangles always “face” in the same direction). This gives the entire poly soup
a notion of “front” and “back.” If we also store information about whether
a given poly soup shape is open or closed (presuming that this fact can be
ascertained by ofﬂine tools), then for closed shapes, we can interpret “front”
and “back” to mean “outside” and “inside” (or vice versa, depending on the
conventions used when constructing the poly soup).
We can also “fake” an inside and outside for certain kinds of open poly
soup shapes (i.e., surfaces). For example, if the terrain in our game is rep-
resented by an open poly soup, then we can decide arbitrarily that the front
of the surface always points away from the Earth. This implies that “front”
should always correspond to “outside.” Practically speaking, to make this
work, we would probably need to customize the collision engine in some way
in order to make it aware of our particular choice of conventions.
12.3.4.8
Compound Shapes
Some objects that cannot be adequately approximated by a single shape can
be approximated well by a collection of shapes. For example, a chair might be
modeled out of two boxes—one for the back of the chair and one enclosing
the seat and all four legs. This is shown in Figure 12.8.
Figure 12.8. A chair can be modeled using a pair of interconnected box shapes.

666
12. Collision and Rigid Body Dynamics
B2
B3
B1
B4
A1
A2
Sphere A
Sphere B
A1
A2
B1
B2
B3
B4
Bounding Volume 
Hierarchies:
Sphere A
Sphere B
Figure 12.9. A collision system need only test the subshapes of a pair of compound shapes when
their convex bounding volumes (in this case, Sphere A and Sphere B) are found to be intersecting.
A compound shape can often be a more-efﬁcient alternative to a poly soup
for modeling non-convex objects; two or more convex volumes can often out-
perform a single poly soup shape. What’s more, some collision systems can
take advantage of the convex bounding volume of the compound shape as a
whole when testing for collisions. In Havok, this is called midphase collision
detection. As the example in Figure 12.9 shows, the collision system ﬁrst tests
the convex bounding volumes of the two compound shapes. If they do not
intersect, the system needn’t test the subshapes for collisions at all.
12.3.5
Collision Testing and Analytical Geometry
A collision system makes use of analytical geometry—mathematical descrip-
tions of three-dimensional volumes and surfaces—in order to detect inter-
sections between shapes computationally. See http://en.wikipedia.org/wiki/
Analytic_geometry for more details on this profound and broad area of re-
search. In this section, we’ll brieﬂy introduce the concepts behind analytical
geometry, show a few common examples and then discuss the generalized
GJK intersection testing algorithm for arbitrary convex polyhedra.
12.3.5.1
Point versus Sphere
We can determine whether a point p lies within a sphere by simply forming
the separation vector s between the point and the sphere’s center c and then
checking its length. If it is greater than the radius of the sphere r, then the
point lies outside the sphere; otherwise, it lies inside:
s = c −p;
if |s| ≤r, then p is inside.

12.3. The Collision Detection System
667
12.3.5.2
Sphere versus Sphere
Determining if two spheres intersect is almost as simple as testing a point
against a sphere. Again, we form a vector s connecting the center points of
the two spheres. We take its length and compare it with the sum of the radii
of the two spheres. If the length of the separating vector is less than or equal
to the sum of the radii, the spheres intersect; otherwise, they do not:
s = c1 −c2;
(12.1)
if |s| ≤(r1 + r2), then spheres intersect.
To avoid the square root operation inherent in calculating the length of
vector s, we can simply square the entire equation. So Equation (12.1) becomes
s = c1 −c2;
|s|2 = s · s;
if |s|2 ≤(r1 + r2)2, then spheres intersect.
12.3.5.3
The Separating Axis Theorem
Most collision detection systems make heavy use of a theorem known as
the separating axis theorem (http://en.wikipedia.org/wiki/Separating_axis
_theorem). It states that if an axis can be found along which the projection of
two convex shapes do not overlap, then we can be certain that the two shapes
do not intersect at all. If such an axis does not exist and the shapes are convex,
then we know for certain that they do intersect. (If the shapes are concave,
then they may not be interpenetrating despite the lack of a separating axis.
This is one reason why we tend to favor convex shapes in collision detection.)
This theorem is easiest to visualize in two dimensions. Intuitively, it says
that if a line can be found, such that object A is entirely on one side of the line
and object B is entirely on the other side, then objects A and B do not overlap.
Such a line is called a separating line, and it is always perpendicular to the sepa-
rating axis. So once we’ve found a separating line, it’s a lot easier to convince
ourselves that the theory is in fact correct by looking at the projections of our
shapes onto the axis that is perpendicular to the separating line.
The projection of a two-dimensional convex shape onto an axis acts like the
shadow that the object would leave on a thin wire. It is always a line seg-
ment, lying on the axis, that represents the maximum extents of the object in
the direction of the axis. We can also think of a projection as a minimum and
maximum coordinate along the axis, which we can write as the fully closed
interval [cmin, cmax]. As you can see in Figure 12.10, when a separating line ex-
ists between two shapes, their projections do not overlap along the separating
axis. However, the projections may overlap along other, non-separating axes.

668
12. Collision and Rigid Body Dynamics
A
B
Non-Separating Axis
Separating Axis
Separating 
Line/Plane
Projection of A
Projection of B
A
B
Figure 12.10. The projections of two shapes onto a separating axis are always two disjoint line
segments. The projections of these same shapes onto a non-separating axis are not necessarily
disjoint. If no separating axis exists, the shapes intersect.
In three dimensions, the separating line becomes a separating plane, but
the separating axis is still an axis (i.e., an inﬁnite line). Again, the projection
of a three-dimensional convex shape onto an axis is a line segment, which we
can represent by the fully closed interval [cmin, cmax].
Some types of shapes have properties that make the potential separating
axes obvious. To detect intersections between two such shapes A and B, we
can project the shapes onto each potential separating axis in turn and then
check whether or not the two projection intervals, [cA
min, cA
max] and [cB
min, cB
max],
are disjoint (i.e., do not overlap). In math terms, the intervals are disjoint
if cA
max < cB
min or if cB
max < cA
min. If the projection intervals along one of the
potential separating axes are disjoint, then we’ve found a separating axis, and
we know the two shapes do not intersect.
One example of this principle in action is the sphere-versus-sphere test. If
two spheres do not intersect, then the axis parallel to the line segment join-
ing the spheres’ center points will always be a valid separating axis (although
other separating axes may exist, depending on how far apart the two spheres
are). To visualize this, consider the limit when the two spheres are just about
to touch but have not yet come into contact. In that case, the only separating
axis is the one parallel to the center-to-center line segment. As the spheres
move apart, we can rotate the separating axis more and more in either direc-
tion. This is shown in Figure 12.11.

12.3. The Collision Detection System
669
Separating
Line/Plane
Separating Axis
Many
Separating Axes
Many
Separating
Lines/Planes
Figure 12.11. When two spheres are an inﬁnitesimal distance apart, the only separating axis lies
parallel to the line segment formed by the two spheres’ center points.
12.3.5.4
AABB versus AABB
To determine whether two AABBs are intersecting, we can again apply the
separating axis theorem. The fact that the faces of both AABBs are guaranteed
to lie parallel to a common set of coordinate axes tells us that if a separating
axis exists, it will be one of these three coordinate axes.
So, to test for intersections between two AABBs, which we’ll call A and B,
we merely inspect the minimum and maximum coordinates of the two boxes
along each axis independently. Along the x-axis, we have the two intervals
[xA
min, xA
max] and [xB
min, xB
max], and we have corresponding intervals for the y-
and z-axes. If the intervals overlap along all three axes, then the two AABBs
are intersecting—in all other cases, they are not. Examples of intersecting and
non-intersecting AABBs are shown in Figure 12.12 (simpliﬁed to two dimen-
y
x
y
x
Figure 12.12. A two-dimensional example of intersecting and non-intersecting AABBs. Notice that
even though the second pair of AABBs are intersecting along the x-axis, they are not intersecting
along the y-axis.

670
12. Collision and Rigid Body Dynamics
sions for the purposes of illustration). For an in-depth discussion of AABB col-
lision, see http://www.gamasutra.com/features/20000203/lander_01.htm.
12.3.5.5
Detecting Convex Collisions: The GJK Algorithm
A very efﬁcient algorithm exists for detecting intersections between arbitrary
convex polytopes (i.e., convex polygons in two dimensions, or convex poly-
hedra in three dimensions). It is known as the GJK algorithm, named after
its inventors, E. G. Gilbert, D. W. Johnson and S. S. Keerthi of the University
of Michigan. Many papers have been written on the algorithm and its vari-
ants, including the original paper (http://ieeexplore.ieee.org/xpl/freeabs_all.
jsp?&arnumber=2083), an excellent SIGGRAPH PowerPoint presentation by
Christer Ericson (http://realtimecollisiondetection.net/pubs/SIGGRAPH04_
Ericson_the_GJK_algorithm.ppt) and another great PowerPoint presentation
by Gino van den Bergen (www.laas.fr/~nic/MOVIE/Workshop/Slides/Gino.
vander.Bergen.ppt). However, the easiest-to-understand (and most entertain-
ing) description of the algorithm is probably Casey Muratori’s instructional
video entitled “Implementing GJK,” available online at http://mollyrocket.
com/849. Because these descriptions are so good, I’ll just give you a feel for
the essence of the algorithm here and then direct you to the Molly Rocket
website and the other references cited above for additional details.
The GJK algorithm relies on a geometric operation known as the Minkowski
difference. This fancy-sounding operation is really quite simple: We take every
point that lies within shape B and subtract it pairwise from every point inside
shape A. The resulting set of points {(Ai −Bj)} is the Minkowski difference.
The useful thing about the Minkowski difference is that, when applied to
two convex shapes, it will contain the origin if and only if those two shapes
intersect. Proof of this statement is a bit beyond our scope, but we can intuit
why it is true by remembering that when we say two shapes A and B intersect,
we really mean that there are points within A that are also within B. During
the process of subtracting every point in B from every point in A, we would
expect to eventually hit one of those shared points that lies within both shapes.
A point minus itself is all zeros, so the Minkowski difference will contain the
origin if (and only if) sphere A and sphere B have points in common. This is
illustrated in Figure 12.13.
The Minkowski difference of two convex shapes is itself a convex shape.
All we care about is the convex hull of the Minkowski difference, not all of the
interior points. The basic procedure of GJK is to try to ﬁnd a tetrahedron (i.e.,
a four-sided shape made out of triangles) that lies on the convex hull of the
Minkwoski difference and that encloses the origin. If one can be found, then
the shapes intersect; if one cannot be found, then they don’t.

12.3. The Collision Detection System
671
Contains the Origin
y
x
A – B
Does not Contain 
the Origin
y
A – B
A
B
A
B
x
Figure 12.13. The Minkowski difference of two intersecting convex shapes contains the origin, but
the Minkowski difference of two non-intersecting shapes does not.
A tetrahedron is just one case of a geometrical object known as a simplex.
But don’t let that name scare you—a simplex is just a collection of points.
A single-point simplex is a point, a two-point simplex is a line segment, a
three-point simplex is a triangle and a four-point simplex is a tetrahedron (see
Figure 12.14).
GJK is an iterative algorithm that starts with a one-point simplex lying
anywhere within the Minkowski difference hull. It then attempts to build
higher-order simplexes that might potentially contain the origin. During each
iteration of the loop, we take a look at the simplex we currently have and
determine in which direction the origin lies relative to it. We then ﬁnd a sup-
porting vertex of the Minkowski difference in that direction—i.e., the vertex
of the convex hull that is closest to the origin in the direction we’re currently
going. We add that new point to the simplex, creating a higher-order simplex
(i.e., a point becomes a line segment, a line segment becomes a triangle and a
Line Segment
Point
Triangle
Tetrahedron
Figure 12.14. Simplexes containing one, two, three and four points.

672
12. Collision and Rigid Body Dynamics
New Point
y
x
New Point
y
x
Search
Direction
Search
Direction
Figure 12.15. In the GJK algorithm, if adding a point to the current simplex creates a shape that
contains the origin, we know the shapes intersect; if there is no supporting vertex that will bring
the simplex any closer to the origin, then the shapes do not intersect.
triangle becomes a tetrahedron). If the addition of this new point causes the
simplex to surround the origin, then we’re done—we know the two shapes
intersect. On the other hand, if we are unable to ﬁnd a supporting vertex that
is closer to the origin than the current simplex, then we know that we can
never get there, which implies that the two shapes do not intersect. This idea
is illustrated in Figure 12.15.
To truly understand the GJK algorithm, you’ll need to check out the papers
and video I referenced previously. But hopefully this description will whet
your appetite for deeper investigation. Or, at the very least, you can impress
your friends by dropping the name “GJK” at parties. (Just don’t try this at job
interviews unless you really do understand the algorithm!)
12.3.5.6
Other Shape-Shape Combinations
We won’t cover any of the other shape-shape intersection combinations here,
as they are covered well in other texts such as [12], [43] and [9]. The key point
to recognize here, however, is that the number of shape-shape combinations is
very large. In fact, for N shape types, the number of pairwise tests required
is O(N 2). Much of the complexity of a collision engine arises because of the
sheer number of intersection cases it must handle. This is one reason why
the authors of collision engines usually try to limit the number of primitive
types—doing so drastically reduces the number of cases the collision detector
must handle. (This is also why GJK is popular—it handles collision detection
between all convex shape types in one fell swoop. The only thing that differs
from shape type to shape type is the support function used in the algorithm.)
There’s also the practical matter of how to implement the code that selects
the appropriate collision-testing function given two arbitrary shapes that are
to be tested. Many collision engines use a double dispatch method (http://en.
wikipedia.org/wiki/Double_dispatch). In single dispatch (i.e., virtual func-

12.3. The Collision Detection System
673
tions), the type of a single object is used to determine which concrete imple-
mentation of a particular abstract function should be called at runtime. Dou-
ble dispatch extends the virtual function concept to two object types. It can
be implemented via a two-dimensional function look-up table keyed by the
types of the two objects being tested. It can also be implemented by arranging
for a virtual function based on the type of object A to call a second virtual
function based on the type of object B.
Let’s take a look at a real-world example. Havok uses objects known as col-
lision agents (classes derived from hkpCollisionAgent) to handle speciﬁc
intersection test cases. Concrete agent classes include hkpSphereSphere-
Agent, hkpSphereCapsuleAgent, hkpGskConvexConvexAgent and so
on. The agent types are referenced by what amounts to a two-dimensional dis-
patch table, managed by the class hkpCollisionDispatcher. As you’d ex-
pect, the dispatcher’s job is to efﬁciently look up the appropriate agent given
a pair of collidables that are to be collision-tested and then call it, passing the
two collidables as arguments.
12.3.5.7
Detecting Collisions between Moving Bodies
Thus far, we’ve considered only static intersection tests between stationary ob-
jects. When objects move, this introduces some additional complexity. Motion
in games is usually simulated in discrete time steps. So one simple approach is
to treat the positions and orientations of each rigid body as stationary at each
time step and use static intersection tests on each “snapshot” of the collision
world. This technique works as long as objects aren’t moving too fast relative
to their sizes. In fact, it works so well that many collision/physics engines,
including Havok, use this approach by default.
However, this technique breaks down for small, fast-moving objects. Imag-
ine an object that is moving so fast that it covers a distance larger than its own
size (measured in the direction of travel) between time steps. If we were to
overlay two consecutive snapshots of the collision world, we’d notice that
there is now a gap between the fast-moving object’s images in the two snap-
shots. If another object happens to lie within this gap, we’ll miss the collision
with it entirely. This problem, illustrated in Figure 12.16, is known as the
“bullet through paper” problem, also known as “tunneling.” The following
sections describe a number of common ways to overcome this problem.
Swept Shapes
One way to avoid tunneling is to make use of swept shapes. A swept shape
is a new shape formed by the motion of a shape from one point to another

674
12. Collision and Rigid Body Dynamics
Figure 12.16. A small, fast-moving object can leave gaps in its motion path between consecutive
snapshots of the collision world, meaning that collisions might be missed entirely.
over time. For example, a swept sphere is a capsule, and a swept triangle is a
triangular prism (see Figure 12.17).
Rather than testing static snapshots of the collision world for intersections,
we can test the swept shapes formed by moving the shapes from their posi-
tions and orientations in the previous snapshot to their positions and orienta-
tions in the current snapshot. This approach amounts to linearly interpolating
the motion of the collidables between snapshots, because we generally sweep
the shapes along line segments from snapshot to snapshot.
Of course, linear interpolation may not be a good approximation of the
motion of a fast-moving collidable. If the collidable is following a curved
path, then theoretically we should sweep its shape along that curved path.
Unfortunately, a convex shape that has been swept along a curve is not itself
convex, so this can make our collision tests much more complex and compu-
tationally intensive.
In addition, if the convex shape we are sweeping is rotating, the result-
ing swept shape is not necessarily convex, even when it is swept along a line
segment. As Figure 12.18 shows, we can always form a convex shape by lin-
early extrapolating the extreme features of the shapes from the previous and
Figure 12.17. A swept sphere is a capsule; a swept triangle is a triangular prism.

12.3. The Collision Detection System
675
Figure 12.18.
A rotating object swept along a line segment does not necessarily generate a convex shape (left). A linear
interpolation of the motion does form a convex shape (right), but it can be a fairly inaccurate approximation of what actually
happened during the time step.
current snapshots—but the resulting convex shape is not necessarily an accu-
rate representation of what the shape really would have done over the time
step. Put another way, a linear interpolation is not appropriate in general for
rotating shapes. So unless our shapes are not permitted to rotate, intersection
testing of swept shapes becomes much more complex and computationally
intensive than its static snapshot-based counterpart.
Swept shapes can be a useful technique for ensuring that collisions are not
missed between static snapshots of the collision world state. However, the
results are generally inaccurate when linearly interpolating curved paths or
rotating collidables, so more-detailed techniques may be required depending
on the needs of the game.
Continuous Collision Detection (CCD)
Another way to deal with the tunneling problem is to employ a technique
known as continuous collision detection (CCD). The goal of CCD is to ﬁnd the
earliest time of impact (TOI) between two moving objects over a given time
interval.
CCD algorithms are generally iterative in nature. For each collidable, we
maintain both its position and orientation at the previous time step and its po-
sition and orientation at the current time. This information can be used to lin-
early interpolate the position and rotation independently, yielding an approx-
imation of the collidable’s transform at any time between the previous and
current time steps. The algorithm then searches for the earliest TOI along the
motion path. A number of search algorithms are commonly used, including
Brian Mirtich’s conservative advancement method, performing a ray cast on the
Minkowski sum, or considering the minimum TOI of individual feature pairs.

676
12. Collision and Rigid Body Dynamics
Erwin Coumans of Sony Computer Entertainment describes some of these
algorithms in http://gamedevs.org/uploads/continuous-collision-detection
-and-physics.pdf along with his own novel variation on the conservative ad-
vancement approach.
12.3.6
Performance Optimizations
Collision detection is a CPU-intensive task for two reasons:
1.
The calculations required to determine whether two shapes intersect are
themselves nontrivial.
2.
Most game worlds contain a large number of objects, and the number
of intersection tests required grows rapidly as the number of objects in-
creases.
To detect intersections between n objects, the brute-force technique would be
to test every possible pair of objects, yielding an O(n2) algorithm. However,
much more efﬁcient algorithms are used in practice. Collision engines typ-
ically employ some form of spatial hashing (http://bit.ly/1fLtX1D), spatial
subdivision or hierarchical bounding volumes in order to reduce the number
of intersection tests that must be performed.
12.3.6.1
Temporal Coherency
One common optimization technique is to take advantage of temporal coherency,
also known as frame-to-frame coherency. When collidables are moving at rea-
sonable speeds, their positions and orientations are usually quite similar from
time step to time step. We can often avoid recalculating certain kinds of in-
formation every frame by caching the results across multiple time steps. For
example, in Havok, collision agents (hkpCollisionAgent) are usually per-
sistent between frames, allowing them to reuse calculations from previous
time steps as long as the motion of the collidables in question hasn’t invali-
dated those calculations.
12.3.6.2
Spatial Partitioning
The basic idea of spatial partitioning is to greatly reduce the number of collid-
ables that need to be checked for intersection by dividing space into a number
of smaller regions. If we can determine (in an inexpensive manner) that a
pair of collidables do not occupy the same region, then we needn’t perform
more-detailed intersection tests on them.

12.3. The Collision Detection System
677
Various hierarchical partitioning schemes, such as octrees, binary space
partitioning trees (BSPs), kd-trees or sphere trees, can be used to subdivide
space for the purposes of collision detection optimization. These trees sub-
divide space in different ways, but they all do so in a hierarchical fashion,
starting with a gross subdivision at the root of the tree and further subdivid-
ing each region until sufﬁciently ﬁne-grained regions have been obtained. The
tree can then be walked in order to ﬁnd and test groups of potentially collid-
ing objects for actual intersections. Because the tree partitions space, we know
that when we traverse down one branch of the tree, the objects in that branch
cannot be colliding with objects in other sibling branches.
12.3.6.3
Broad Phase, Midphase and Narrow Phase
Havok uses a three-tiered approach to prune the set of collidables that need
to be tested for collisions during each time step.
•
First, gross AABB tests are used to determine which collidables are po-
tentially intersecting. This is known as broad phase collision detection.
•
Second, the coarse bounding volumes of compound shapes are tested.
This is known as midphase collision detection. For example, in a com-
pound shape composed of three spheres, the bounding volume might
be a fourth, larger sphere that encloses the other spheres. A compound
shape may contain other compound shapes, so in general a compound
collidable has a bounding volume hierarchy. The midphase traverses
this hierarchy in search of subshapes that are potentially intersecting.
•
Finally, the collidables’ individual primitives are tested for intersection.
This is known as narrow phase collision detection.
The Sweep and Prune Algorithm
In all of the major collision/physics engines (e.g., Havok, ODE, PhysX), broad
phase collision detection employs an algorithm known as sweep and prune
(http://en.wikipedia.org/wiki/Sweep_and_prune). The basic idea is to sort
the minimum and maximum dimensions of the collidables’ AABBs along the
three principal axes, and then check for overlapping AABBs by traversing the
sorted lists. Sweep and prune algorithms can make use of frame-to-frame
coherency (see Section 12.3.6.1) to reduce an O(n log n) sort operation to an
expected O(n) running time. Frame coherency can also aid in the updating of
AABBs when objects rotate.

678
12. Collision and Rigid Body Dynamics
12.3.7
Collision Queries
Another responsibility of the collision detection system is to answer hypo-
thetical questions about the collision volumes in the game world. Examples
include the following:
•
If a bullet travels from the player’s weapon in a given direction, what is
the ﬁrst target it will hit, if any?
•
Can a vehicle move from point A to point B without striking anything
along the way?
•
Find all enemy objects within a given radius of a character.
In general, such operations are known as collision queries.
The most common kind of query is a collision cast, sometimes just called a
cast. (The terms trace and probe are other common synonyms for “cast.”) A cast
determines what, if anything, a hypothetical object would hit if it were to be
placed into the collision world and moved along a ray or line segment. Casts
are different from regular collision detection operations because the entity be-
ing cast is not really in the collision world—it cannot affect the other objects
in the world in any way. This is why we say that a collision cast answers
hypothetical questions about the collidables in the world.
12.3.7.1
Ray Casting
The simplest type of collision cast is a ray cast, although this name is actually a
bit of a misnomer. What we’re really casting is a directed line segment—in other
words, our casts always have a start point (p0) and an end point (p1). The cast
line segment is tested against the collidable objects in the collision world. If it
intersects any of them, the contact point or points are returned.
Ray casting systems typically describe the line segment via its start point
p0 and a delta vector d that, when added to p0, yields the end point p1. Any
point on this line segment can be found via the following parametric equation,
where the parameter t is permitted to vary between zero and one:
p(t) = p0 + td,
t ∈[0, 1].
Clearly, p0 = p(0) and p1 = p(1). In addition, any contact point along the
segment can be uniquely described by specifying the value of the parameter
t corresponding to the contact. Most ray casting APIs return their contact
points as “t values,” or they permit a contact point to be converted into its
corresponding t by making an additional function call.

12.3. The Collision Detection System
679
Most collision detection systems are capable of returning the earliest con-
tact—i.e., the contact point that lies closest to p0 and corresponds to the small-
est value of t. Some systems are also capable of returning a complete list of all
collidables that were intersected by the ray or line segment. The information
returned for each contact typically includes the t value, some kind of unique
identiﬁer for the collidable entity that was hit, and possibly other information
such as the surface normal at the point of contact or other relevant proper-
ties of the shape or surface that was struck. One possible contact point data
structure is shown below.
struct RayCastContact
{
F32
m_t;
// the t value for this
// contact
U32
m_collidableId; // which collidable did we
// hit?
Vector m_normal;
// surface normal at
// contact pt.
// other information...
};
Applications of Ray Casts
Ray casts are used heavily in games. For example, we might want to ask the
collision system whether character A has a direct line of sight to character
B. To determine this, we simply cast a directed line segment from the eyes of
character A to the chest of character B. If the ray hits character B, we know that
A can “see” B. But if the ray strikes some other object before reaching character
B, we know that the line of sight is being blocked by that object. Ray casts
are used by weapon systems (e.g., to determine bullet hits), player mechanics
(e.g., to determine whether or not there is solid ground beneath the character’s
feet), AI systems (e.g., line of sight checks, targeting, movement queries, etc.),
vehicle systems (e.g., to locate and snap the vehicle’s tires to the terrain) and
so on.
12.3.7.2
Shape Casting
Another common query involves asking the collision system how far an imag-
inary convex shape would be able to travel along a directed line segment be-
fore it hits something solid. This is known as a sphere cast when the volume
being cast is a sphere, or a shape cast in general. (Havok calls them linear

680
12. Collision and Rigid Body Dynamics
casts.) As with ray casts, a shape cast is usually described by specifying the
start point p0, the distance to travel d and of course the type, dimensions and
orientation of the shape we wish to cast.
There are two cases to consider when casting a convex shape.
1.
The cast shape is already interpenetrating or contacting at least one other
collidable, preventing it from moving away from its starting location.
2.
The cast shape is not intersecting with anything else at its starting loca-
tion, so it is free to move a nonzero distance along its path.
In the ﬁrst scenario, the collision system typically reports the contact(s)
between the cast shape and all of the collidables with which it is initially in-
terpenetrating. These contacts might be inside the cast shape or on its surface,
as shown in Figure 12.19.
Figure 12.19.
A cast
sphere that starts in
penetration will be un-
able to move, and possi-
bly many contact points
will lie inside the cast
shape in general.
In the second case, the shape can move a nonzero distance along the line
segment before striking something. Presuming that it hits something, it will
usually hit only a single collidable. However, it is possible for a cast shape
to strike more than one collidable simultaneously if its trajectory is just right.
And of course, if the impacted collidable is a non-convex poly soup, the cast
shape may end up touching more than one part of the poly soup simultane-
ously. We can safely say that no matter what kind of convex shape is cast, it
is possible for the cast to generate multiple contact points. The contacts will al-
ways be on the surface of the cast shape in this case, never inside it (because we
know that the cast shape was not interpenetrating anything when it started its
journey). This case is illustrated in Figure 12.20.
As with ray casts, some shape casting APIs report only the earliest contact
experienced by the cast shape, while others allow the shape to continue along
Contact
Contacts
d
d
Figure 12.20. If the starting location of a cast shape is not interpenetrating anything, then the
shape will move a nonzero distance along its line segment, and its contacts (if any) will always be
on its surface.

12.3. The Collision Detection System
681
Contact 1
d
Contact 2
Contact 3
Figure 12.21. A shape casting API might return all contacts instead of only the earliest contact.
its hypothetical path, returning all the contacts it experiences on its journey.
This is illustrated in Figure 12.21.
The contact information returned by a shape cast is necessarily a bit more
complex than it is for a ray cast. We cannot simply return one or more t val-
ues, because a t value only describes the location of the center point of the
shape along its path. It tells us nothing of where, on the surface or interior
of the shape, it came into contact with the impacted collidable. As a result,
most shape casting APIs return both a t value and the actual contact point,
along with other relevant information (such as which collidable was struck,
the surface normal at the contact point, etc.).
Unlike ray casting APIs, a shape casting system must always be capable of
reporting multiple contacts. This is because even if we only report the contact
with the earliest t value, the shape may have touched multiple distinct collid-
ables in the game world, or it may be touching a single non-convex collidable
at more than one point. As a result, collision systems usually return an array
or list of contact point data structures, each of which might look something
like this:
struct ShapeCastContact
{
F32
m_t;
// the t value for this
// contact
U32
m_collidableId; // which collidable did we
// hit?
Point
m_contactPoint; // location of actual
// contact

682
12. Collision and Rigid Body Dynamics
Vector m_normal;
// surface normal at
// contact pt.
// other information...
};
Given a list of contact points, we often want to distinguish between the
groups of contact points for each distinct t value. For example, the earliest
contact is actually described by the group of contact points that all share the
minimum t in the list. It’s important to realize that collision systems may
or may not return their contact points sorted by t. If it does not, it’s almost
always a good idea to sort the results by t manually. This ensures that if one
looks at the ﬁrst contact point in the list, it will be guaranteed to be among the
earliest contact points along the shape’s path.
Applications of Shape Casts
Shape casts are extremely useful in games. Sphere casts can be used to de-
termine whether the virtual camera is in collision with objects in the game
world. Sphere or capsule casts are also commonly used to implement charac-
ter movement. For example, in order to slide the character forward on uneven
terrain, we can cast a sphere or capsule that lies between the character’s feet
in the direction of motion. We can adjust it up or down via a second cast, to
ensure that it remains in contact with the ground. If the sphere hits a very
short vertical obstruction, such as a street curb, it can “pop up” over the curb.
If the vertical obstruction is too tall, like a wall, the cast sphere can be slid
horizontally along the wall. The ﬁnal resting place of the cast sphere becomes
the character’s new location next frame.
12.3.7.3
Phantoms
Sometimes, games need to determine which collidable objects lie within some
speciﬁc volume in the game world. For example, we might want the list of all
enemies that are within a certain radius of the player character. Havok sup-
ports a special kind of collidable object known as a phantom for this purpose.
A phantom acts much like a shape cast whose distance vector d is zero.
At any moment, we can ask the phantom for a list of its contacts with other
collidables in the world. It returns this data in essentially the same format that
would be returned by a zero-distance shape cast.
However, unlike a shape cast, a phantom is persistent in the collision
world. This means that it can take full advantage of the temporal coherency
optimizations used by the collision engine when detecting collisions between

12.3. The Collision Detection System
683
“real” collidables. In fact, the only difference between a phantom and a reg-
ular collidable is that it is “invisible” to all other collidables in the collision
world (and it does not take part in the dynamics simulation). This allows it to
answer hypothetical questions about what objects it would collide with were
it a “real” collidable, but it is guaranteed not to have any effect of the other
collidables—including other phantoms—in the collision world.
12.3.7.4
Other Types of Queries
Some collision engines support other kinds of queries in addition to casts. For
example, Havok supports closest point queries, which are used to ﬁnd the set of
points on other collidables that are closest to a given collidable in the collision
world.
12.3.8
Collision Filtering
It is quite common for game developers to want to enable or disable collisions
between certain kinds of objects. For example, most objects are permitted to
pass through the surface of a body of water—we might employ a buoyancy
simulation to make them ﬂoat, or they might just sink to the bottom, but in
either case we do not want the water’s surface to appear solid. Most collision
engines allow contacts between collidables to be accepted or rejected based on
game-speciﬁc critiera. This is known as collision ﬁltering.
12.3.8.1
Collision Masking and Layers
One common ﬁltering approach is to categorize the objects in the world and
then use a look-up table to determine whether certain categories are permitted
to collide with one another or not. For example, in Havok, a collidable can be
a member of one (and only one) collision layer. The default collision ﬁlter in
Havok, represented by an instance of the class hkpGroupFilter, maintains
a 32-bit mask for each layer, each bit of which tells the system whether or not
that particular layer can collide with one of the other layers.
12.3.8.2
Collision Callbacks
Another ﬁltering technique is to arrange for the collision library to invoke a
callback function whenever a collision is detected. The callback can inspect the
speciﬁcs of the collision and make the decision to either allow or reject the
collision based on suitable criteria. Havok also supports this kind of ﬁltering.
When contact points are ﬁrst added to the world, the contactPointAdded()
callback is invoked. If the contact point is later determined to be valid (it may
not be if an earlier TOI contact was found), the contactPointConfirmed()
callback is invoked. The application may reject contact points in these call-
backs if desired.

684
12. Collision and Rigid Body Dynamics
12.3.8.3
Game-Speciﬁc Collision Materials
Game developers often need to categorize the collidable objects in the game
world, in part to control how they collide (as with collision ﬁltering) and in
part to control other secondary effects, such as the sound that is made or the
particle effect that is generated when one type of object hits another. For exam-
ple, we might want to differentiate between wood, stone, metal, mud, water
and human ﬂesh.
To accomplish this, many games implement a collision shape categoriza-
tion mechanism similar in many respects to the material system used in the
rendering engine. In fact, some game teams use the term collision material to
describe this categorization. The basic idea is to associate with each collid-
able surface a set of properties that deﬁnes how that particular surface should
behave from a physical and collision standpoint. Collision properties can in-
clude sound and particle effects, physical properties like coefﬁcient of restitu-
tion or friction coefﬁcients, collision ﬁltering information and whatever other
information the game might require.
For simple convex primitives, the collision properties are usually associ-
ated with the shape as a whole. For polygon soup shapes, the properties
might be speciﬁed on a per-triangle basis. Because of this latter usage, we
usually try to keep the binding between the collision primitive and its colli-
sion material as compact as possible. A typical approach is to bind collision
primitives to collision materials via an 8-, 16- or 32-bit integer, or a pointer to
the material data. This integer indexes into a global array of data structures
containing the detailed collision properties themselves.
12.4
Rigid Body Dynamics
In a game engine, we are particularly concerned with the kinematics of objects—
how they move over time. Many game engines include a physics system for the
purposes of simulating the motion of the objects in the virtual game world
in a somewhat physically realistic way. Technically speaking, game physics
engines are typically concerned with a particular ﬁeld of physics known as
dynamics. This is the study of how forces affect the movement of objects. Un-
til very recently, game physics systems have been focused almost exclusively
on a speciﬁc subdiscipline known as classical rigid body dynamics. This name
implies that in a game’s physics simulation, two important simplifying as-
sumptions are made:
•
Classical (Newtonian) mechanics.
The objects in the simulation are as-
sumed to obey Newton’s laws of motion. The objects are large enough

12.4. Rigid Body Dynamics
685
that there are no quantum effects, and their speeds are low enough that
there are no relativistic effects.
•
Rigid bodies. All objects in the simulation are perfectly solid and cannot
be deformed. In other words, their shape is constant. This idea meshes
well with the assumptions made by the collision detection system. Fur-
thermore, the assumption of rigidity greatly simpliﬁes the mathematics
required to simulate the dynamics of solid objects.
Game physics engines are also capable of ensuring that the motions of
the rigid bodies in the game world conform to various constraints. The most
common constraint is that of non-penetration—in other words, objects aren’t
allowed to pass through one another. Hence the physics system attempts to
provide realistic collision responses whenever bodies are found to be interpen-
etrating.2 This is one of the primary reasons for the tight interconnection be-
tween the physics engine and the collision detection system.
Most physics systems also allow game developers to set up other kinds of
constraints in order to deﬁne realistic interactions between physically simu-
lated rigid bodies. These may include hinges, prismatic joints (sliders), ball
joints, wheels, “rag dolls” to emulate unconscious or dead characters and so
on.
The physics system usually shares the collision world data structure, and
in fact it usually drives the execution of the collision detection algorithm as
part of its time step update routine. There is typically a one-to-one mapping
between the rigid bodies in the dynamics simulation and the collidables man-
aged by the collision engine. For example, in Havok, an hkpRigidBody ob-
ject maintains a reference to one and only one hkpCollidable (although it
is possible to create a collidable that has no rigid body). In PhysX, the two
concepts are a bit more tightly integrated—an NxActor serves both as a coll-
idable object and as a rigid body for the purposes of the dynamics simulation.
These rigid bodies and their corresponding collidables are usually maintained
in a singleton data structure known as the collision/physics world, or sometimes
just the physics world.
The rigid bodies in the physics engine are typically distinct from the logi-
cal objects that make up the virtual world from a gameplay perspective. The
positions and orientations of game objects can be driven by the physics sim-
ulation. To accomplish this, we query the physics engine every frame for the
transform of each rigid body, and apply it in some way to the transform of
the corresponding game object. It’s also possible for a game object’s motion,
2Or in the case of continuous collision detection, the collision response actually prevents the
penetration from occurring.

686
12. Collision and Rigid Body Dynamics
as determined by some other engine system (such as the animation system or
the character control system) to drive the position and rotation of a rigid body
in the physics world. As mentioned in Section 12.3.1, a single logical game ob-
ject may be represented by one rigid body in the physics world, or by many.
A simple object like a rock, weapon or barrel, might correspond to one rigid
body. But an articulated character or a complex machine might be composed
of many interconnected rigid pieces.
The remainder of this chapter will be devoted to investigating how game
physics engines work. We’ll brieﬂy introduce the theory that underlies rigid
body dynamics simulations. Then we’ll investigate some of the most common
features of a game physics system and have a look at how a physics engine
might be integrated into a game.
12.4.1
Some Foundations
A great many excellent books, articles and slide presentations have been writ-
ten on the topic of classical rigid body dynamics. A solid foundation in ana-
lytical mechanics theory can be obtained from [15]. Even more relevant to our
discussion are texts like [34], [11] and [25], which have been written specif-
ically about the kind of physics simulations done by games.
Other texts,
like [1], [9] and [28], include chapters on rigid body dynamics for games.
Chris Hecker wrote a series of helpful articles on the topic of game physics
for Game Developer Magazine; Chris has posted these and a variety of other
useful resources at http://chrishecker.com/Rigid_Body_Dynamics. An infor-
mative slide presentation on dynamics simulation for games was produced
by Russell Smith, the primary author of ODE; it is available at http://www.
ode.org/slides/parc/dynamics.pdf.
In this section, I’ll summarize the fundamental theoretical concepts that
underlie the majority of game physics engines. This will be a whirlwind tour
only, and by necessity I’ll have to omit some details. Once you’ve read this
chapter, I strongly encourage you to read at least a few of the additional re-
sources cited previously.
12.4.1.1
Units
Most rigid body dynamics simulations operate in the MKS system of units. In
this system, distance is measured in meters (abbreviated “m”), mass is mea-
sured in kilograms (abbreviated “kg”) and time is measured in seconds (ab-
breviated “s”). Hence the name MKS.
You could conﬁgure your physics system to use other units if you wanted
to, but if you do this, you need to make sure everything in the simulation

12.4. Rigid Body Dynamics
687
is consistent. For example, constants like the acceleration due to gravity g,
which is measured in m/s2 in the MKS system, would have to be re-expressed
in whatever unit system you select. Most game teams just stick with MKS to
keep life simple.
12.4.1.2
Separability of Linear and Angular Dynamics
An unconstrained rigid body is one that can translate freely along all three
Cartesian axes and that can rotate freely about these three axes as well. We
say that such a body has six degrees of freedom (DOF).
It is perhaps somewhat surprising that the motion of an unconstrained
rigid body can be separated into two independent components:
•
Linear dynamics. This is a description of the motion of the body when
we ignore all rotational effects. (We can use linear dynamics alone to
describe the motion of an idealized point mass—i.e., a mass that is in-
ﬁnitesimally small and cannot rotate.)
•
Angular dynamics. This is a description of the rotational motion of the
body.
As you can well imagine, this ability to separate the linear and angular
components of a rigid body’s motion is extremely helpful when analyzing or
simulating its behavior. It means that we can calculate a body’s linear motion
without regard to rotation—as if it were an idealized point mass—and then
layer its angular motion on top in order to arrive at a complete description of
the body’s motion.
12.4.1.3
Center of Mass
For the purposes of linear dynamics, an unconstrained rigid body acts as
though all of its mass were concentrated at a single point known as the center
of mass (abbreviated CM, or sometimes COM). The center of mass is essentially
the balancing point of the body for all possible orientations. In other words,
the mass of a rigid body is distributed evenly around its center of mass in all
directions.
For a body with uniform density, the center of mass lies at the centroid of
the body. That is, if we were to divide the body up into N very small pieces,
add up the positions of all these pieces as a vector sum and then divide by
the number of pieces, we’d end up with a pretty good approximation to the
location of the center of mass. If the body’s density is not uniform, the position
of each little piece would need to be weighted by that piece’s mass, meaning

688
12. Collision and Rigid Body Dynamics
that in general the center of mass is really a weighted average of the pieces’
positions. So we have
rCM =
X
∀i
miri
X
∀i
mi
=
X
∀i
miri
m
,
where the symbol m represents the total mass of the body, and the symbol r
represents a radius vector or position vector—i.e., a vector extending from the
world-space origin to the point in question. (These sums become integrals in
the limit as the sizes and masses of the little pieces approach zero.)
The center of mass always lies inside a convex body, although it may actu-
ally lie outside the body if it is concave. (For example, where would the center
of mass of the letter “C” lie?)
12.4.2
Linear Dynamics
For the purposes of linear dynamics, the position of a rigid body can be fully
described by a position vector rCM that extends from the world-space origin
to the center of mass of the body, as shown in Figure 12.22. Since we’re using
the MKS system, position is measured in meters (m). For the remainder of
this discussion, we’ll drop the CM subscripts, as it is understood that we are
describing the motion of the body’s center of mass.
y
x
CM
Figure 12.22. For the purposes of linear dynamics, the position of a rigid body can be fully de-
scribed by the position of its center of mass.
12.4.2.1
Linear Velocity and Acceleration
The linear velocity of a rigid body deﬁnes the speed and direction in which the
body’s CM is moving. It is a vector quantity, typically measured in meters per

12.4. Rigid Body Dynamics
689
second (m/s). Velocity is the ﬁrst time derivative of position, so we can write
v(t) = dr(t)
dt
= ˙r(t),
where the dot over the vector r denotes taking the derivative with respect to
time. Differentiating a vector is the same as differentiating each component
independently, so
vx(t) = drx(t)
dt
= ˙rx(t),
and so on for the y- and z-components.
Linear acceleration is the ﬁrst derivative of linear velocity with respect to
time, or the second derivative of the position of a body’s CM versus time.
Acceleration is a vector quantity, usually denoted by the symbol a. So we can
write
a(t) = dv(t)
dt
= ˙v(t)
= d2r(t)
dt2
= ¨r(t).
12.4.2.2
Force and Momentum
A force is deﬁned as anything that causes an object with mass to accelerate or
decelerate. A force has both a magnitude and a direction in space, so all forces
are represented by vectors. A force is often denoted by the symbol F. When N
forces are applied to a rigid body, their net effect on the body’s linear motion
is found by simply adding up the force vectors:
Fnet =
N
X
i=1
Fi.
Newton’s famous Second Law states that force is proportional to accelera-
tion and mass:
F(t) = ma(t) = m¨r(t).
(12.2)
As Newton’s law implies, force is measured in units of kilogram-meters per
second squared (kg-m/s2). This unit is also called the Newton.
When we multiply a body’s linear velocity by its mass, the result is a quan-
tity known as linear momentum. It is customary to denote linear momentum
with the symbol p:
p(t) = mv(t).

690
12. Collision and Rigid Body Dynamics
When mass is constant, Equation (12.2) holds true. But if mass is not con-
stant, as would be the case for a rocket whose fuel is being gradually used up
and converted into energy, Equation (12.2) is not exactly correct. The proper
formulation is actually as follows:
F(t) = dp(t)
dt
= d
 m(t)v(t)

dt
.
which of course reduces to the more familiar F = ma when the mass is con-
stant and can be brought outside the derivative. Linear momentum is not of
much concern to us. However, the concept of momentum will become rele-
vant when we discuss angular dynamics.
12.4.3
Solving the Equations of Motion
The central problem in rigid body dynamics is to solve for the motion of the
body, given a set of known forces acting on it. For linear dynamics, this means
ﬁnding v(t) and r(t) given knowledge of the net force Fnet(t) and possibly
other information, such as the position and velocity at some previous time.
As we’ll see below, this amounts to solving a pair of ordinary differential
equations—one to ﬁnd v(t) given a(t) and the other to ﬁnd r(t) given v(t).
12.4.3.1
Force as a Function
A force can be constant, or it can be a function of time as shown above. A force
can also be a function of the position of the body, its velocity, or any number
of other quantities. So in general, the expression for force should really be
written as follows:
F
 t, r(t), v(t), . . .

= ma(t).
(12.3)
This can be rewritten in terms of the position vector and its ﬁrst and second
derivatives as follows:
F
 t, r(t), ˙r(t), . . .

= m¨r(t).
For example, the force exerted by a spring is proportional to how far it has
been stretched away from its natural resting position. In one dimension, with
the spring’s resting position at x = 0, we can write
F
 t, x(t)

= −kx(t),
where k is the spring constant, a measure of the spring’s stiffness.

12.4. Rigid Body Dynamics
691
As another example, the damping force exerted by a mechanical viscous
damper (a so-called dashpot) is proportional to the velocity of the damper’s
piston. So in one dimension, we can write
F
 t, v(t)

= −bv(t),
where b is a viscous damping coefﬁcient.
12.4.3.2
Ordinary Differential Equations
In general, an ordinary differential equation (ODE) is an equation involving a
function of one independent variable and various derivatives of that function.
If our independent variable is time and our function is x(t), then an ODE is a
relation of the form
dnx
dtn = f

t, x(t), dx(t)
dt , d2x(t)
dt2
, . . . , dn−1x(t)
dtn−1

.
Put another way, the nth derivative of x(t) is expressed as a function f
whose arguments can be time (t), position (x(t)), and any number of deriva-
tives of x(t) as long as those derivatives are of lower order than n.
As we saw in Equation (12.3), force is a function of time, position and
velocity in general:
¨r(t) = 1
mF
 t, r(t), ˙r(t)

.
This clearly qualiﬁes as an ODE. We wish to solve this ODE in order to ﬁnd
v(t) and r(t).
12.4.3.3
Analytical Solutions
In some rare situations, the differential equations of motion can be solved an-
alytically, meaning that a simple, closed-form function can be found that de-
scribes the body’s position for all possible values of time t. A common example
is the vertical motion of a projectile under the inﬂuence of a constant accelera-
tion due to gravity, a(t) = [0, g, 0], where g = −9.8m/s2. In this case, the ODE
of motion boils down to
¨y(t) = g.
Integrating once yields
˙y(t) = gt + v0,

692
12. Collision and Rigid Body Dynamics
where v0 is the vertical velocity at time t = 0. Integrating a second time yields
the familiar solution
y(t) = 1
2gt2 + v0t + y0,
where y0 is the initial vertical position of the object.
However, analytical solutions are almost never possible in game physics.
This is due in part to the fact that closed-form solutions to some differential
equations are simply not known. Moreover, a game is an interactive simula-
tion, so we usually cannot predict how the forces in a game will behave over
time. This makes it impossible to ﬁnd simple, closed-form expressions for the
positions and velocities of the objects in the game as functions of time.
There are of course exceptions to this rule of thumb. For example, it’s
pretty common to solve for a closed-form expression in order to determine
with what velocity a projectile must be launched in order to hit a predeﬁned
target.
12.4.4
Numerical Integration
For the reasons cited above, game physics engines turn to a technique known
as numerical integration. With this technique, we solve our differential equa-
tions in a time-stepped manner—using the solution from a previous time step
to arrive at the solution for the next time step. The duration of the time step
is usually taken to be (roughly) constant and is denoted by the symbol ∆t.
Given that we know the body’s position and velocity at the current time t1
and that the force is known as a function of time, position and/or velocity, we
wish to ﬁnd the position and velocity at the next time step t2 = t1 + ∆t. In
other words, given r(t1), v(t1) and F(t, r, v), the problem is to ﬁnd r(t2) and
v(t2).
12.4.4.1
Explicit Euler
One of the simplest numerical solutions to an ODE is known as the explicit Eu-
ler method. This is the intuitive approach often taken by new game program-
mers. Let’s assume for the moment that we already know the current velocity
and that we wish to solve the following ODE to ﬁnd the body’s position on
the next frame:
v(t) = ˙r(t).
(12.4)
Using the explicit Euler method, we simply convert the velocity from meters
per second into meters per frame by multiplying by the time delta, and then
we add “one frame’s worth” of velocity onto the current position in order to

12.4. Rigid Body Dynamics
693
ﬁnd the new position on the next frame. This yields the following approxi-
mate solution to the ODE given by Equation (12.4):
r(t2) = r(t1) + v(t1)∆t.
(12.5)
We can take an analogous approach to ﬁnd the body’s velocity next frame
given the net force acting this frame. Hence, the approximate explicit Euler
solution to the ODE
a(t) = Fnet(t)
m
= ˙v(t)
is as follows:
v(t2) = v(t1) + Fnet(t)
m
∆t.
(12.6)
Interpretations of Explicit Euler
What we’re really doing in Equation (12.5) is assuming that the velocity of the
body is constant during the time step. Therefore, we can use the current ve-
locity to predict the body’s position on the next frame. The change in position
∆r between times t1 and t2 is hence ∆r = v(t1)∆t. Graphically, if we imag-
ine a plot of the position of the body versus time, we are taking the slope of
the function at time t1 (which is just v(t1)) and extrapolating it linearly to the
next time step t2. As we can see in Figure 12.23, linear extrapolation does not
necessarily provide us with a particularly good estimate of the true position
at the next time step r(t2), but it does work reasonably well as long as the
velocity is roughly constant.
Figure 12.23 suggests another way to interpret the explicit Euler method—
as an approximation of a derivative. By deﬁnition, any derivative is the quo-
tient of two inﬁnitesimally small differences (in our case, dr/dt). The explicit
Euler method approximates this using the quotient of two ﬁnite differences. In
Δr
Δt
t
r(t1)
rapprox(t2)
r(t2)
r(t)
t1
t2
= v(t1)
Δr
Δt
Figure 12.23. In the explicit Euler method, the slope of r(t) at time t1 is used to linearly extrapolate
from r(t1) to an estimate of the true value of r(t2).

694
12. Collision and Rigid Body Dynamics
other words, dr becomes ∆r and dt becomes ∆t. This yields
dr
dt ≈∆r
∆t ;
v(t1) ≈r(t2) −r(t1)
t2 −t1
,
which again simpliﬁes to Equation (12.5). This approximation is really only
valid when the velocity is constant over the time step. It is also valid in the
limit as ∆t tends toward zero (at which point it becomes exactly right). Obvi-
ously, this same analysis can be applied to Equation (12.6) as well.
12.4.4.2
Properties of Numerical Methods
We’ve implied that the explicit Euler method is not particularly accurate. Let’s
pin this idea down more concretely. A numerical solution to an ordinary dif-
ferential equation actually has three important and interrelated properties:
•
Convergence. As the time step ∆t tends toward zero, does the approxi-
mate solution get closer and closer to the real solution?
•
Order. Given a particular numerical approximation to the solution of
an ODE, how “bad” is the error? Errors in numerical ODE solutions
are typically proportional to some power of the time step duration ∆t,
so they are often written using “big O” notation (e.g., O(∆t2)). We say
that a particular numerical method is of “order n” when its error term is
O(∆t(n+1)).
•
Stability. Does the numerical solution tend to “settle down” over time?
If a numerical method adds energy into the system, object velocities will
eventually “explode,” and the system will become unstable. On the other
hand, if a numerical method tends to remove energy from the system, it
will have an overall damping effect, and the system will be stable.
The concept of order warrants a little more explanation. We usually mea-
sure the error of a numerical method by comparing its approximate equation
with the inﬁnite Taylor series expansion of the exact solution to the ODE. We
then cancel terms by subtracting the two equations. The remaining Taylor
terms represent the error inherent in the method. For example, the explicit
Euler equation is
r(t2) = r(t1) + ˙r(t1)∆t.
The inﬁnite Taylor series expansion of the exact solution is
r(t2) = r(t1) + ˙r(t1)∆t + 1
2¨r(t1)∆t2 + 1
6r(3)(t1)∆t3 + . . . ,

12.4. Rigid Body Dynamics
695
where r(3) represents the third derivative with respect to time. Therefore, the
error is represented by all of the terms after the v∆t term, which is of order
O(∆t2) (because this term dwarfs the other higher-order terms):
E = 1
2¨r(t1)∆t2 + 1
6r(3)(t1)∆t3 + . . .
= O
 ∆t2
.
To make the error of a method explicit, we’ll often write its equation with
the error term added in “big O” notation at the end. For example, the explicit
Euler method’s equation is most accurately written as follows:
r(t2) = r(t1) + ˙r(t1)∆t + O
 ∆t2
.
We say that the explicit Euler method is an “order one” method because it is
accurate up to and including the Taylor series term involving ∆t to the ﬁrst
power. In general, if a method’s error term is O(∆t(n+1)), then it is said to be
an “order n” method.
12.4.4.3
Alternatives to Explicit Euler
The explicit Euler method sees quite a lot of use for simple integration tasks in
games, producing the best results when the velocity is nearly constant. How-
ever, it is not used in general-purpose dynamics simulations because of its
high error and poor stability. There are all sorts of other numerical meth-
ods for solving ODEs, including backward Euler (another ﬁrst-order method),
midpoint Euler (a second-order method) and the family of Runge-Kutta
methods. (The fourth-order Runge-Kutta, often abbreviated “RK4,” is par-
ticularly popular.) We won’t describe these in any detail here, as you can
ﬁnd voluminous amounts of information about them online and in the litera-
ture. The Wikipedia page http://en.wikipedia.org/wiki/Numerical_ordinary
_differential_equations serves as an excellent jumping-off point for learning
these methods.
12.4.4.4
Verlet Integration
The numerical ODE method most often used in interactive games these days is
probably the Verlet method, so I’ll take a moment to describe it in some detail.
There are actually two variants of this method: regular Verlet and the so-called
velocity Verlet. I’ll present both methods here, but I’ll leave the theory and deep
explanations to the myriad papers and Web pages available on the topic. (For
a start, check out http://en.wikipedia.org/wiki/Verlet_integration.)
The regular Verlet method is attractive because it achieves a high order
(low error), is relatively simple and inexpensive to evaluate, and produces a

696
12. Collision and Rigid Body Dynamics
solution for position directly in terms of acceleration in one step (as opposed
to the two steps normally required to go from acceleration to velocity and then
from velocity to position). The formula is derived by adding two Taylor series
expansions, one going forward in time and one going backward in time:
r(t1 + ∆t) = r(t1) + ˙r(t1)∆t + 1
2¨r(t1)∆t2 + 1
6r(3)(t1)∆t3 + O(∆t4);
r(t1 −∆t) = r(t1) −˙r(t1)∆t + 1
2¨r(t1)∆t2 −1
6r(3)(t1)∆t3 + O(∆t4).
Adding these expressions causes the negative terms to cancel with the corre-
sponding positive ones. The result gives us the position at the next time step
in terms of the acceleration and the two (known) positions at the current and
previous time steps. This is the regular Verlet method:
r(t1 + ∆t) = 2r(t1) −r(t1 −∆t) + a(t1)∆t2 + O(∆t4).
In terms of net force, the Verlet method becomes
r(t1 + ∆t) = 2r(t1) −r(t1 −∆t) + Fnet(t1)
m
∆t2 + O(∆t4).
The velocity is conspicuously absent from this expression. However, it can
be found using the following somewhat inaccurate approximation (among
other alternatives):
v(t1 + ∆t) = r(t1 + ∆t) −r(t1)
∆t
+ O(∆t).
12.4.4.5
Velocity Verlet
The more commonly used velocity Verlet method is a four-step process in which
the time step is divided into two parts to facilitate the solution. Given that
a(t1) = 1
mF
 t1, r(t1), v(t1)

is known, we do the following:
1.
Calculate r(t1 + ∆t) = r(t1) + v(t1)∆t + 1
2a(t1)∆t2.
2.
Calculate v(t1 + 1
2∆t) = v(t1) + 1
2a(t1)∆t.
3.
Determine a(t1 + ∆t) = a(t2) = 1
mF
 t2, r(t2), v(t2)

.
4.
Calculate v(t1 + ∆t) = v(t1 + 1
2∆t) + 1
2a(t1 + ∆t)∆t.
Notice in the third step that the force function depends on the position
and velocity on the next time step, r(t2) and v(t2). We already calculated r(t2)
in step 1, so we have all the information we need as long as the force is not
velocity-dependent. If it is velocity-dependent, then we must approximate
the next frame’s velocity, perhaps using the explicit Euler method.

12.4. Rigid Body Dynamics
697
12.4.5
Angular Dynamics in Two Dimensions
Up until now, we’ve focused on analyzing the linear motion of a body’s center
of mass (which acts as if it were a point mass). As I said earlier, an uncon-
strained rigid body will rotate about its center of mass. This means that we
can layer the angular motion of a body on top of the linear motion of its cen-
ter of mass in order to arrive at a complete description of the body’s overall
motion. The study of a body’s rotational motion in response to applied forces
is called angular dynamics.
In two dimensions, angular dynamics works almost identically to linear
dynamics. For each linear quantity, there’s an angular analog, and the math-
ematics works out quite neatly. So let’s investigate two-dimensional angular
dynamics ﬁrst. As we’ll see, when we extend the discussion into three di-
mensions, things get a bit messier, but we’ll burn that bridge when we get to
it!
12.4.5.1
Orientation and Angular Speed
In two dimensions, every rigid body can be treated as a thin sheet of material.
(Some physics texts refer to such a body as a plane lamina.) All linear motion
occurs in the xy-plane, and all rotations occur about the z-axis. (Visualize
wooden puzzle pieces sliding about on an air hockey table.)
The orientation of a rigid body in 2D is fully described by an angle θ, mea-
sured in radians relative to some agreed-upon zero rotation. For example, we
might specify that θ = 0 when a race car is facing directly down the positive
x-axis in world space. This angle is of course a time-varying function, so we
denote it θ(t).
12.4.5.2
Angular Speed and Acceleration
Angular velocity measures the rate at which a body’s rotation angle changes
over time. In two dimensions, angular velocity is a scalar, more correctly
called angular speed, since the term “velocity” really only applies to vectors.
It is denoted by the scalar function ω(t) and measured in radians per second
(rad/s). Angular speed is the derivative of the orientation angle θ(t) with
respect to time:
Angular:
Linear:
ω(t) = dθ(t)
dt
= ˙θ(t)
v(t) = dr(t)
dt
= ˙r(t).

698
12. Collision and Rigid Body Dynamics
And as we’d expect, angular acceleration, denoted α(t) and measured in
radians per second squared (rad/s2), is the rate of change of angular speed:
Angular:
Linear:
α(t) = dω(t)
dt
= ˙ω(t) = ¨θ(t)
a(t) = dv(t)
dt
= ˙v(t) = ¨r(t).
12.4.5.3
Moment of Inertia
The rotational equivalent of mass is a quantity known as the moment of inertia.
Just as mass describes how easy or difﬁcult it is to change the linear velocity
of a point mass, the moment of inertia measures how easy or difﬁcult it is to
change the angular speed of a rigid body about a particular axis. If a body’s
mass is concentrated near an axis of rotation, it will be relatively easier to
rotate about that axis, and it will hence have a smaller moment of inertia than
a body whose mass is spread out away from that axis.
Since we’re focusing on two-dimensional angular dynamics right now, the
axis of rotation is always z, and a body’s moment of inertia is a simple scalar
value. Moment of inertia is usually denoted by the symbol I. We won’t get
into the details of how to calculate the moment of inertia here. For a full
derivation, see [15].
12.4.5.4
Torque
Until now, we’ve assumed that all forces are applied to the center of mass of a
rigid body. However, in general, forces can be applied at arbitrary points on a
body. If the line of action of a force passes through the body’s center of mass,
then the force will produce linear motion only, as we’ve already seen. Other-
wise, the force will introduce a rotational force known as a torque in addition
to the linear motion it normally causes. This is illustrated in Figure 12.24.
F1
F2
Figure 12.24. On the left, a force applied to a body’s CM produces purely linear motion. On the
right, a force applied off-center will give rise to a torque, producing rotational motion as well as
linear motion.

12.4. Rigid Body Dynamics
699
Figure 12.25. Torque is calculated by taking the cross product between a force’s point of applica-
tion in body space (i.e., relative to the center of mass) and the force vector. The vectors are shown
here in two dimensions for ease of illustration; if it could be drawn, the torque vector would be
directed into the page.
We can calculate torque using a cross product. First, we express the lo-
cation at which the force is applied as a vector r extending from the body’s
center of mass to the point of application of the force. (In other words, the
vector r is in body space, where the origin of body space is deﬁned to be the
center of mass.) This is illustrated in Figure 12.25. The torque N caused by a
force F applied at a location r is
N = r × F.
(12.7)
Equation (12.7) implies that torque increases as the force is applied farther
from the center of mass. This explains why a lever can help us to move a
heavy object. It also explains why a force applied directly through the center
of mass produces no torque and no rotation—the magnitude of the vector r is
zero in this case.
When two or more forces are applied to a rigid body, the torque vectors
produced by each one can be summed, just as we can sum forces. So in general
we are interested in the net torque, Nnet.
In two dimensions, the vectors r and F must both lie in the xy-plane, so
N will always be directed along the positive or negative z-axis. As such,
we’ll denote a two-dimensional torque via the scalar Nz, which is just the
z-component of the vector N.
Torque is related to angular acceleration and moment of inertia in much
the same way that force is related to linear acceleration and mass:
Angular:
Linear:
Nz(t) = Iα(t) = I ˙ω(t) = I ¨θ(t)
F(t) = ma(t) = m ˙v(t) = m¨r(t).
(12.8)

700
12. Collision and Rigid Body Dynamics
12.4.5.5
Solving the Angular Equations of Motion in Two Dimensions
For the two-dimensional case, we can solve the angular equations of motion
using exactly the same numerical integration techniques we applied to the
linear dynamics problem. The pair of ODEs that we wish to solve is as follows:
Angular:
Linear:
Nnet(t) = I ˙ω(t)
Fnet(t) = m ˙v(t)
ω(t) = ˙θ(t)
v(t) = ˙r(t),
and their approximate explicit Euler solutions are
Angular:
Linear:
ω(t2) = ω(t1) + I−1Nnet(t1)∆t
v(t2) = v(t1) + m−1Fnet(t1)∆t
θ(t2) = θ(t1) + ω(t1)∆t
r(t2) = r(t1) + v(t1)∆t.
Of course, we could apply any of the other more-accurate numerical meth-
ods as well, such as the velocity Verlet method (I’ve omitted the linear case
here for compactness, but compare this to the steps given in Section 12.4.4.5):
1.
Calculate θ(t1 + ∆t) = θ(t1) + ω(t1)∆t + 1
2α(t1)∆t2.
2.
Calculate ω(t1 + 1
2∆t) = ω(t1) + 1
2α(t1)∆t.
3.
Calculate α(t1 + ∆t) = α(t2) = I−1Nnet
 t2, θ(t2), ω(t2)

.
4.
Calculate ω(t1 + ∆t) = ω(t1 + 1
2∆t) + 1
2α(t1 + ∆t)∆t.
12.4.6
Angular Dynamics in Three Dimensions
Angular dynamics in three dimensions is a somewhat more complex topic
than its two-dimensional counterpart, although the basic concepts are of
course very similar. In the following section, I’ll give a very brief overview
of how angular dynamics works in 3D, focusing primarily on the things that
are typically confusing to someone who is new to the topic. For further in-
formation, check out Glenn Fiedler’s series of articles on the topic, available
at http://gafferongames.com/game-physics/physics-in-3d/. Another help-
ful resource is the paper entitled “An Introduction to Physically Based Mod-
eling” by David Baraff of the Robotics Institute at Carnegie Mellon University,
available at http://www-2.cs.cmu.edu/~baraff/sigcourse/notesd1.pdf.

12.4. Rigid Body Dynamics
701
12.4.6.1
The Inertia Tensor
A rigid body may have a very different distribution of mass about the three co-
ordinate axes. As such, we should expect a body to have different moments of
inertia about different axes. For example, a long thin rod should be relatively
easy to make rotate about its long axis because all the mass is concentrated
very close to the axis of rotation. Likewise, the rod should be relatively more
difﬁcult to make rotate about its short axis because its mass is spread out far-
ther from the axis. This is indeed the case, and it is why a ﬁgure skater spins
faster when she tucks her limbs in close to her body.
In three dimensions, the rotational mass of a rigid body is represented by a
3×3 matrix known as its inertia tensor. It is usually represented by the symbol
I (as before, we won’t describe how to calculate the inertia tensor here; see [15]
for details):
I =


Ixx
Ixy
Ixz
Iyx
Iyy
Iyz
Izx
Izy
Izz

.
The elements lying along the diagonal of this matrix are the moments of
inertia of the body about its three principal axes, Ixx, Iyy and Izz. The off-
diagonal elements are called products of inertia. They are zero when the body
is symmetrical about all three principal axes (as would be the case for a rect-
angular box). When they are nonzero, they tend to produce physically real-
istic yet somewhat unintuitive motions that the average game player would
probably think were “wrong” anyway. Therefore, the inertia tensor is often
simpliﬁed down to the three-element vector
Ixx
Iyy
Izz

in game physics
engines.
12.4.6.2
Orientation in Three Dimensions
In two dimensions, we know that the orientation of a rigid body can be de-
scribed by a single angle θ, which measures rotation about the z-axis (assum-
ing the motion is taking place in the xy-plane). In three dimensions, a body’s
orientation could be represented using three Euler angles
θx
θy
θz

, each
representing the body’s rotation about one of the three Cartesian axes. How-
ever, as we saw in Chapter 4, Euler angles suffer from gimbal lock problems
and can be difﬁcult to work with mathematically. Therefore, the orientation
of a body is more often represented using either a 3 × 3 matrix R or a unit
quaternion q. We’ll use the quaternion form exclusively in this chapter.
Recall that a quaternion is a four-element vector whose x-, y- and z-com-
ponents can be interpreted as a unit vector u lying along the axis of rotation,
scaled by the sine of the half-angle and whose w component is the cosine of

702
12. Collision and Rigid Body Dynamics
the half-angle:
q =
qx
qy
qz
qw

=
q
qw

=

u sin θ
2
cos θ
2

.
A body’s orientation is of course a function of time, so we should write it q(t).
Again, we need to select an arbitrary direction to be our zero rotation.
For example, we might say that by default, the front of every object will lie
along the positive z-axis in world space, with y up and x to the left. Any non-
identity quaternion will serve to rotate the object away from this canonical
world-space orientation. The choice of the canonical orientation is arbitrary,
but of course it’s important to be consistent across all assets in the game.
12.4.6.3
Angular Velocity and Momentum in Three Dimensions
In three dimensions, angular velocity is a vector quantity, denoted by ω(t).
The angular velocity vector can be visualized as a unit-length vector u that
deﬁnes the axis of rotation, scaled by the two-dimensional angular velocity
ωu = ˙θu of the body about the u-axis. Hence,
ω(t) = ωu(t)u = ˙θu(t)u.
In linear dynamics, we saw that if there are no forces acting on a body,
then the linear acceleration is zero, and linear velocity is constant. In two-
dimensional angular dynamics, this again holds true: If there are no torques
acting on a body in two dimensions, then the angular acceleration α is zero,
and the angular speed ω about the z-axis is constant.
Unfortunately, this is not the case in three dimensions. It turns out that
even when a rigid body is rotating in the absence of all forces, its angular
velocity vector ω(t) may not be constant because the axis of rotation can con-
tinually change direction. You can see this effect in action when you try to
spin a rectangular object, like a block of wood, in mid-air in front of you. If
you throw the block so that it is rotating about its shortest axis, it will spin in a
stable way. The orientation of the axis stays roughly constant. The same thing
happens if you try to spin the block about its longest axis. But if you try to spin
the block around the remaining axis (the one that’s neither the shortest nor the
longest), the rotation will be utterly unstable. (Try it! Go steal a wooden block
from a baby and spin it in various ways. On second thought, make sure to
give it back when you’re done.) The axis of rotation itself changes direction
wildly as the object spins. This is illustrated in Figure 12.26.

12.4. Rigid Body Dynamics
703
Figure 12.26. A rectangular object that is spun about its shortest or longest axis has a constant angular velocity vector.
However, when spun about its medium-sized axis, the direction of the angular velocity vector changes wildly.
The fact that the angular velocity vector can change in the absence of
torques is another way of saying that angular velocity is not conserved. How-
ever, a related quantity called the angular momentum does remain constant in
the absence of forces and hence is conserved. Angular momentum is the rota-
tional equivalent of linear momentum:
Angular:
Linear:
L(t) = Iω(t)
p(t) = mv(t).
Like the linear case, angular momentum L(t) is a three-element vector.
However, unlike the linear case, rotational mass (the inertia tensor) is not a
scalar but rather a 3 × 3 matrix. As such, the expression Iω is computed via a
matrix multiplication:


Lx(t)
Ly(t)
Lz(t)

=


Ixx
Ixy
Ixz
Iyx
Iyy
Iyz
Izx
Izy
Izz




ωx(t)
ωy(t)
ωz(t)

.
Because the angular velocity ω is not conserved, we do not treat it as a
primary quantity in our dynamics simulations the way we do the linear ve-
locity v. Instead, we treat angular momentum L as the primary quantity. The
angular velocity is a secondary quantity, determined only after we have de-
termined the value of L at each time step of the simulation.
12.4.6.4
Torque in Three Dimensions
In three dimensions, we still calculate torque as the cross product between
the radial position vector of the point of force application and the force vector
itself (N = r×F). Equation (12.8) still holds, but we always write it in terms of

704
12. Collision and Rigid Body Dynamics
the angular momentum because angular velocity is not a conserved quantity:
N = Iα(t)
= Idω(t)
dt
= d
dt
 Iω(t)

= dL(t)
dt
.
12.4.6.5
Solving the Equations of Angular Motion in Three Dimensions
When solving the equations of angular motion in three dimensions, we might
be tempted to take exactly the same approach we used for linear motion and
two-dimensional angular motion. We might guess that the differential equa-
tions of motion should be written
Angular 3D?
Linear:
Nnet(t) = I ˙ω(t)
Fnet(t) = m ˙v(t)
ω(t) = ˙θ(t)
v(t) = ˙r(t),
and using the explicit Euler method, we might guess that the approximate
solutions to these ODEs would look something like this:
Angular 3D?
Linear:
ω(t2) = ω(t1) + I−1Nnet(t1)∆t
v(t2) = v(t1) + m−1Fnet(t)∆t
θ(t2) = θ(t1) + ω(t1)∆t
r(t2) = r(t1) + v(t1)∆t.
However, this is not actually correct. The differential equations of three-di-
mensional angular motion differ from their linear and two-dimensional an-
gular counterparts in two important ways:
1.
Instead of solving for the angular velocity ω, we solve for the angular
momentum L directly. We then calculate the angular velocity vector as a
secondary quantity using I and L. We do this because angular momen-
tum is conserved, while angular velocity is not.
2.
When solving for the orientation given the angular velocity, we have a
problem: The angular velocity is a three-element vector, while the orien-
tation is a four-element quaternion. How can we write an ODE relating
a quaternion to a vector? The answer is that we cannot, at least not di-
rectly. But what we can do is convert the angular velocity vector into

12.4. Rigid Body Dynamics
705
quaternion form and then apply a slightly odd-looking equation that
relates the orientation quaternion to the angular velocity quaternion.
It turns out that when we express a rigid body’s orientation as a quater-
nion, the derivative of this quaternion is related to the body’s angular velocity
vector in the following way. First, we construct an angular velocity quaternion.
This quaternion contains the three components of the angular velocity vector
in x, y and z, with its w-component set to zero:
ω =
ωx
ωy
ωz
0
.
Now the differential equation relating the orientation quaternion to the angu-
lar velocity quaternion is (for reasons we won’t get into here) as follows:
dω(t)
dt
= ˙q(t) = 1
2ω(t)q(t).
It’s important to remember here that ω(t) is the angular velocity quaternion
as described above and that the product ω(t)q(t) is a quaternion product (see
Section 4.4.2.1 for details).
So, we actually need to write the ODEs of motion as follows (note that I’ve
recast the linear ODEs in terms of linear momentum as well, to underscore
the similarities between the two cases):
Angular 3D:
Linear:
Nnet(t) = ˙L(t)
Fnet(t) = ˙p(t)
ω(t) = I−1L(t)
v(t) = m−1p(t)
ω(t) =
ω(t)
0
v(t) = ˙r(t).
1
2ω(t)q(t) = ˙q(t)
Using the explicit Euler method, the ﬁnal approximate solution to the angular
ODEs in three dimensions is actually as follows:
L(t2) = L(t1) + Nnet(t1)∆t
(vectors)
= L(t1) + ∆t
X
∀i
 ri × Fi(t1)

;
(vectors)
ω(t2) =
I−1L(t2)
0
;
(quaternions)
q(t2) = q(t1) + 1
2ω(t1)q(t1)∆t.
(quaternions)
The orientation quaternion q(t) should be renormalized periodically to re-
verse the effects of the inevitable accumulation of ﬂoating-point error.

706
12. Collision and Rigid Body Dynamics
As always, the explicit Euler method is being used here just as an example.
In a real engine, we would employ velocity Verlet, RK4 or some other more-
stable and more-accurate numerical method.
12.4.7
Collision Response
Everything we’ve discussed so far assumes that our rigid bodies are neither
colliding with anything, nor is their motion constrained in any other way.
When bodies collide with one another, the dynamics simulation must take
steps to ensure that they respond realistically to the collision and that they
are never left in a state of interpenetration after the simulation step has been
completed. This is known as collision response.
12.4.7.1
Energy
Before we discuss collision response, we must understand the concept of en-
ergy. When a force moves a body over a distance, we say that the force does
work. Work represents a change in energy—that is, a force either adds energy
to a system of rigid bodies (e.g., an explosion) or it removes energy from the
system (e.g., friction). Energy comes in two forms. The potential energy V
of a body is the energy it has simply because of where it is relative to a force
ﬁeld such as a gravitational or a magnetic ﬁeld. (For example, the higher up
a body is above the surface of the Earth, the more gravitational potential en-
ergy it has.) The kinetic energy of a body T represents the energy arising from
the fact that it is moving relative to other bodies in a system. The total energy
E = V +T of an isolated system of bodies is a conserved quantity, meaning that
it remains constant unless energy is being drained from the system or added
from outside the system.
The kinetic energy arising from linear motion can be written
Tlinear = 1
2mv2,
or in terms of the linear momentum and velocity vectors:
Tlinear = 1
2p · v.
Analogously, the kinetic energy arising from a body’s rotational motion is as
follows:
Tangular = 1
2L · ω.
Energy and its conservation can be extremely useful concepts when solving
all sorts of physics problems. We’ll see the role that energy plays in the deter-
mination of collision responses in the following section.

12.4. Rigid Body Dynamics
707
12.4.7.2
Impulsive Collision Response
When two bodies collide in the real world, a complex set of events takes place.
The bodies compress slightly and then rebound, changing their velocities and
losing energy to sound and heat in the process. Most real-time rigid body
dynamics simulations approximate all of these details with a simple model
based on an analysis of the momenta and kinetic energies of the colliding ob-
jects, called Newton’s law of restitution for instantaneous collisions with no friction.
It makes the following simplifying assumptions about the collision:
•
The collision force acts over an inﬁnitesimally short period of time, turn-
ing it into what we call an idealized impulse. This causes the velocities
of the bodies to change instantaneously as a result of the collision.
•
There is no friction at the point of contact between the objects’ surfaces.
This is another way of saying that the impulse acting to separate the
bodies during the collision is normal to both surfaces—there is no tan-
gential component to the collision impulse. (This is just an idealization
of course; we’ll get to friction in Section 12.4.7.5.)
•
The nature of the complex submolecular interactions between the bod-
ies during the collision can be approximated by a single quantity known
as the coefﬁcient of restitution, customarily denoted by the symbol ε. This
coefﬁcient describes how much energy is lost during the collision. When
ε = 1, the collision is perfectly elastic, and no energy is lost. (Picture two
billiard balls colliding in mid-air.) When ε = 0, the collision is perfectly
inelastic, also known as perfectly plastic and the kinetic energy of both
bodies is lost. The bodies will stick together after the collision, continu-
ing to move in the direction that their mutual center of mass had been
moving before the collision. (Picture pieces of putty being slammed to-
gether.)
All collision analysis is based around the idea that linear momentum is
conserved. So for two bodies 1 and 2, we can write
p1 + p2 = p′
1 + p′
2,
or
m1v1 + m2v2 = m′
1v′
1 + m′
2v′
2
where the primed symbols represent the momenta and velocities after the col-
lision. The kinetic energy of the system is conserved as well, but we must ac-
count for the energy lost due to heat and sound by introducing an additional
energy loss term Tlost:
1
2m1v2
1 + 1
2m2v2
2 = 1
2m′
1v′
1
2 + 1
2m′
2v′
2
2 + Tlost.

708
12. Collision and Rigid Body Dynamics
n
Body 1
Body 2
p^
Figure 12.27. In a frictionless collision, the impulse acts along a line normal to both surfaces at
the point of contact. This line is deﬁned by the unit normal vector n.
If the collision is perfectly elastic, the energy loss Tlost is zero. If it is perfectly
plastic, the energy loss is equal to the original kinetic energy of the system, the
primed kinetic energy sum becomes zero and the bodies stick together after
the collision.
To resolve a collision using Newton’s law of restitution, we apply an ide-
alized impulse to the two bodies. An impulse is like a force that acts over
an inﬁnitesimally short period of time and thereby causes an instantaneous
change in the velocity of the body to which it is applied. We could denote an
impulse with the symbol ∆p, since it is a change in momentum (∆p = m∆v).
However, most physics texts use the symbol ˆp (pronounced “p-hat”) instead,
so we’ll do the same.
Because we assume that there is no friction involved in the collision, the
impulse vector must be normal to both surfaces at the point of contact. In
other words, ˆp = ˆpn, where n is the unit vector normal to both surfaces. This
is illustrated in Figure 12.27. If we assume that the surface normal points to-
ward body 1, then body 1 experiences an impulse of ˆp, and body 2 experiences
an equal but opposite impulse −ˆp. Hence, the momenta of the two bodies af-
ter the collision can be written in terms of their momenta prior to the collision
and the impulse ˆp as follows:
p′
1 = p1 + ˆp
p′
2 = p2 −ˆp
m1v′
1 = m1v1 + ˆp
m2v′
2 = m2v2 −ˆp
(12.9)
v′
1 = v1 + ˆp
m1
n
v′
2 = v2 + ˆp
m2
n.
The coefﬁcient of restitution provides the key relationship between the rela-
tive velocities of the bodies before and after the collision. Given that the cen-
ters of mass of the bodies have velocities before the collision and afterward,
the coefﬁcient of restitution ε is deﬁned as follows:
(v′
2 −v′
1) = ε(v2 −v1).
(12.10)

12.4. Rigid Body Dynamics
709
Solving Equations (12.9) and (12.10) under the temporary assumption that
the bodies cannot rotate yields
ˆp = ˆpn = (ε + 1)(v2 · n −v1 · n)
1
m1
+ 1
m2
n.
Notice that if the coefﬁcient of restitution is one (perfectly elastic collision)
and if the mass of body 2 is effectively inﬁnite (as it would be for, say, a con-
crete driveway), then (1/m2) = 0, v2 = 0, and this expression reduces to
a reﬂection of the other body’s velocity vector about the contact normal, as
we’d expect:
ˆp = −2m1(v1 · n)n;
v′
1 = p1 + p2
m1
= m1v1 −2m1(v1 · n)n
m1
= v1 −2m1(v1 · n)n.
The solution gets a bit hairier when we take the rotations of the bodies
into account. In this case, we need to look at the velocities of the points of
contact on the two bodies rather than the velocities of their centers of mass,
and we need to calculate the impulse in such a way as to impart a realistic
rotational effect as a result of the collision. We won’t get into the details here,
but Chris Hecker’s article, available at http://chrishecker.com/images/e/e7/
Gdmphys3.pdf, does an excellent job of describing both the linear and the
rotational aspects of collision response. The theory behind collision response
is explained more fully in [15].
12.4.7.3
Penalty Forces
Another approach to collision response is to introduce imaginary forces called
penalty forces into the simulation. A penalty force acts like a stiff damped
spring attached to the contact points between two bodies that have just in-
terpenetrated. Such a force induces the desired collision response over a short
but ﬁnite period of time. Using this approach, the spring constant k effectively
controls the duration of the interpenetration, and the damping coefﬁcient b
acts a bit like the restitution coefﬁcient. When b = 0, there is no damping—no
energy is lost, and the collision is perfectly elastic. As b increases, the collision
becomes more plastic.
Let’s take a brief look at some of the pros and cons of the penalty force ap-
proach to resolving collisions. On the positive side, penalty forces are easy to

710
12. Collision and Rigid Body Dynamics
implement and understand. They also work well when three or more bodies
are interpenetrating each other. This problem is very difﬁcult to solve when
resolving collisions one pair at a time. A good example is the Sony PS3 demo
in which a huge number of rubber duckies are poured into a bathtub—the
simulation was nice and stable despite the very large number of collisions.
The penalty force method is a great way to achieve this.
Unfortunately, because penalty forces respond to penetration (i.e., relative
position) rather than to relative velocity, the forces may not align with the di-
rection we would intuitively expect, especially during a high-speed collision.
A classic example is a car driving head-on into a truck. The car is low while
the truck is tall. Using only the penalty force method, it is easy to arrive at a
situation in which the penalty force is vertical, rather than horizontal as we
would expect given the velocities of the two vehicles. This can cause the truck
to pop its nose up into the air while the car drives under it.
In general, the penalty force technique works well for low-speed impacts,
but it does not work well at all when objects are moving quickly. It is pos-
sible to combine the penalty force method with other collision resolution ap-
proaches in order to strike a balance between stability in the presence of large
numbers of interpenetrations and responsiveness and more-intuitive behav-
ior at high velocities.
12.4.7.4
Using Constraints to Resolve Collisions
As we’ll investigate in Section 12.4.8, most physics systems permit various
kinds of constraints to be imposed on the motion of the bodies in the simula-
tion. If collisions are treated as constraints that disallow object interpenetra-
tion, then they can be resolved by simply running the simulation’s general-
purpose constraint solver. If the constraint solver is fast and produces high-
quality visual results, this can be an effective way to resolve collisions.
12.4.7.5
Friction
Friction is a force that arises between two bodies that are in continuous con-
tact, resisting their movement relative to one another. There are a number of
types of friction. Static friction is the resistance one feels when trying to start
a stationary object sliding along a surface. Dynamic friction is a resisting force
that arises when objects are actually moving relative to one another. Sliding
friction is a type of dynamic friction that resists movement when an object
slides along a surface. Rolling friction is a type of static or dynamic friction
that acts at the point of contact between a wheel or other round object and
the surface it is rolling on. When the surface is very rough, the rolling fric-
tion is exactly strong enough to cause the wheel to roll without sliding, and it

12.4. Rigid Body Dynamics
711
acts as a form of static friction. If the surface is somewhat smooth, the wheel
may slip, and a dynamic form of rolling friction comes into play. Collision fric-
tion is the friction that acts instantaneously at the point of contact when two
bodies collide while moving. (This is the friction force that we ignored when
discussing Newton’s law of restitution in Section 12.4.7.1.) Various kinds of
constraints can have friction as well. For example, a rusted hinge or axle might
resist being turned by introducing a friction torque.
Let’s look at an example to understand the essence of how friction works.
Linear sliding friction is proportional to the component of an object’s weight
that is acting normal to the surface on which it is sliding. The weight of an
object is just the force due to gravity, G = mg, which is always directed down-
ward. The component of this force normal to an inclined surface that makes
an angle θ with the horizontal is just GN = mg cos θ. The friction force f is
then
f = µmg cos θ,
where the constant of proportionality µ is called the coefﬁcient of friction. This
force acts tangentially to the surface, in a direction opposite to the attempted
or actual motion of the object. This is illustrated in Figure 12.28.
Figure 12.28 also shows the component of the gravitational force acting
tangent to the surface, GT = mg sin θ. This force tends to make the object
accelerate down the plane, but in the presence of sliding friction, it is counter-
acted by f. Hence, the net force tangent to the surface is
Fnet = GT −f = mg(sin θ −µ cos θ).
If the angle of inclination is such that the expression in parentheses is zero,
the object will slide at a constant speed (if already moving) or be at rest. If the
expression is greater than zero, the object will accelerate down the surface. If
it is less than zero, the object will decelerate and eventually come to rest.
= m
|
N| =
mg cos
|
T| =
mg sin
| | =
mg cos
Figure 12.28. The force of friction f is proportional to the normal component of the object’s
weight. The proportionality constant µ is called the coefﬁcient of friction.

712
12. Collision and Rigid Body Dynamics
Spurious Contacts 
with Triangle Edge
Figure 12.29. When an object slides between two adjacent triangles, spurious contacts with the
new triangle’s edge can be generated.
12.4.7.6
Welding
An additional problem arises when an object is sliding across a polygon soup.
Recall that a polygon soup is just what its name implies—a soup of essentially
unrelated polygons (usually triangles). As an object slides from one triangle
of this soup to the next, the collision detection system will generate additional
spurious contacts because it will think that the object is about to hit the edge
of the next triangle. This is illustrated in Figure 12.29.
There are a number of solutions to this problem. One is to analyze the set of
contacts and discard ones that appear to be spurious, based on various heuris-
tics and possibly some knowledge of the object’s contacts on a previous frame
(e.g., if we know the object was sliding along a surface and a contact normal
arises that is due to the object being near the edge of its current triangle, then
discard that contact normal). Versions of Havok prior to 4.5 employed this
approach.
Starting with Havok 4.5, a new technique was implemented that essen-
tially annotates the mesh with triangle adjacency information. The collision
detection system therefore “knows” which edges are interior edges and can
discard spurious collisions reliably and quickly. Havok describes this solu-
tion as welding, because in effect the edges of the triangles in the poly soup are
welded to one another.
12.4.7.7
Coming to Rest, Islands and Sleeping
When energy is removed from a simulated system via friction, damping or
other means, moving objects will eventually come to rest. This seems like a
natural consequence of the simulation—something that would just “fall out”
of the differential equations of motion. Unfortunately, in a real computerized
simulation, coming to rest is never quite that simple. Various factors such as
ﬂoating-point error, inaccuracies in the calculation of restitution forces and
numerical instability can cause objects to jitter forever rather than coming to

12.4. Rigid Body Dynamics
713
rest as they should. For this reason, most physics engines use various heuristic
methods to detect when objects are oscillating instead of coming to rest as they
should. Additional energy can be removed from the system to ensure that
such objects eventually settle down, or they can simply be stopped abruptly
once their average velocity drops below a threshold.
When an object really does stop moving (ﬁnds itself in a state of equilib-
rium), there is no reason to continue integrating its equations of motion every
frame. To optimize performance, most physics engines allow dynamic objects
in the simulation to be put to sleep. This excludes them from the simulation
temporarily, although sleeping objects are still active from a collision stand-
point. If any force or impulse begins acting on a sleeping object, or if the
object loses one of the contacts that was holding it in equilibrium, it will be
awoken so that its dynamic simulation can be resumed.
Sleep Criteria
Various criteria can be used to determine whether or not a body qualiﬁes for
sleep. It’s not always easy to make this determination in a robust manner for
all situations. For example, a long pendulum might have very low angular
momentum and yet still be moving visibly on-screen.
The most commonly used criteria for equilibrium detection include:
•
The body is supported. This means it has three or more contact points
(or one or more planar contacts) that allow it to attain equilibrium with
gravity and any other forces that might be affecting it.
•
The body’s linear and angular momentum are below a predeﬁned thresh-
old.
•
A running average of the linear and angular momentum are below a pre-
deﬁned threshold.
•
The total kinetic energy of the body (T =
1
2p · v + 1
2L · ω) is below a
predeﬁned threshold. The kinetic energy is usually mass-normalized
so that a single threshold can be used for all bodies regardless of their
masses.
The motion of a body that is about to go to sleep might be progressively damped
so that it comes to a smooth stop rather than stopping abruptly.
Simulation Islands
Both Havok and PhysX further optimize their performance by automatically
grouping objects that either are interacting or have the potential to interact

714
12. Collision and Rigid Body Dynamics
in the near future into sets called simulation islands. Each simulation island
can be simulated independently of all the other islands—an approach that is
highly conducive to cache coherency optimizations and parallel processing.
Havok and PhysX both put entire islands to sleep rather than individual
rigid bodies. This approach has its pros and cons. The performance boost
is obviously larger when a whole group of interacting objects can be put to
sleep. On the other hand, if even one object in an island is awake, the entire
island is awake. Overall, it seems that the pros tend to outweigh the cons, so
the simulation island design is one we’re likely to continue to see in future
versions of these SDKs.
12.4.8
Constraints
An unconstrained rigid body has six degrees of freedom (DOF): It can trans-
late in three dimensions, and it can rotate about the three Cartesian axes. Con-
straints restrict an object’s motion, reducing its degrees of freedom either par-
tially or completely. Constraints can be used to model all sorts of interesting
behaviors in a game. Here are a few examples:
•
a swinging chandelier (point-to-point constraint);
•
a door that can be kicked, slammed, blown of its hinges (hinge con-
straint);
•
a vehicle’s wheel assembly (axle constraint with damped springs for
suspension);
•
a train or a car pulling a trailer (stiff spring/rod constraint);
•
a rope or chain (chain of stiff springs or rods); and
•
a rag doll (specialized constraints that mimic the behavior of various
joints in the human skeleton).
In the sections that follow, we’ll brieﬂy investigate these and some of the
other most common kinds of constraints typically provided by a physics SDK.
12.4.8.1
Point-to-Point Constraints
Figure
12.30.
A
point-to-point con-
straint requires that
a point on body A
align with a point on
body B.
A point-to-point constraint is the simplest type of constraint. It acts like a ball-
and-socket joint—bodies can move in any way they like, as long as a speciﬁed
point on one body lines up with a speciﬁed point on the other body. This is
illustrated in Figure 12.30.

12.4. Rigid Body Dynamics
715
Figure 12.31. A stiff spring constraint requires that a point on body A be separated from a point
on body B by a user-speciﬁed distance.
12.4.8.2
Stiff Springs
A stiff spring constraint is a lot like a point-to-point constraint except that it
keeps the two points separated by a speciﬁed distance. This kind of constraint
acts like an invisible rod between the two constrained points. Figure 12.31
illustrates this constraint.
12.4.8.3
Hinge Constraints
A hinge constraint limits rotational motion to only a single degree of freedom,
about the hinge’s axis. An unlimited hinge acts like an axle, allowing the con-
strained object to complete an unlimited number of full rotations. It’s common
to deﬁne limited hinges that can only move through a predeﬁned range of an-
gles about the one allowed axis. For example, a one-way door can only move
through a 180 degree arc, because otherwise it would pass through the adja-
cent wall. Likewise, a two-way door is constrained to move through a ±180
degree arc. Hinge constraints may also be given a degree of friction in the
form of a torque that resists rotation about the hinge’s axis. A limited hinge
constraint is shown in Figure 12.32.
Figure 12.32. A limited hinge constraint mimics the behavior of a door.

716
12. Collision and Rigid Body Dynamics
Figure 12.33. A prismatic constraint acts like a piston.
12.4.8.4
Prismatic Constraints
Prismatic constraints act like a piston: A constrained body’s motion is re-
stricted to a single translational degree of freedom. A prismatic constraint
may or may not permit rotation about the translation axis of the piston. Pris-
matic constraints can of course be limited or unlimited and may or may not
include friction. A prismatic constraint is illustrated in Figure 12.33.
12.4.8.5
Other Common Constraint Types
Many other types of constraints are possible, of course. Here are just a few
examples:
•
Planar. Objects are constrained to move in a two-dimensional plane.
•
Wheel.
This is typically a hinge constraint with unlimited rotation,
coupled with some form of vertical suspension simulated via a spring-
damper assembly.
•
Pulley. In this specialized constraint, an imaginary rope passes through
a pulley and is attached to two bodies. The bodies move along the line
of the rope via a leverage ratio.
Constraints may be breakable, meaning that after enough force is applied,
they automatically come apart. Alternatively, the game can turn the constraint
on and off at will, using its own criteria for when the constraint should break.
12.4.8.6
Constraint Chains
Long chains of linked bodies are sometimes difﬁcult to simulate in a stable
manner because of the iterative nature of the constraint solver. A constraint
chain is a specialized group of constraints with information that tells the con-
straint solver how the objects are connected. This allows the solver to deal
with the chain in a more stable manner than would otherwise be possible.

12.4. Rigid Body Dynamics
717
12.4.8.7
Rag Dolls
A rag doll is a physical simulation of the way a human body might move
when it is dead or unconscious and hence entirely limp. Rag dolls are created
by linking together a collection of rigid bodies, one for each semi-rigid part
of the body. For example, we might have capsules for the feet, calves, thighs,
hands, upper and lower arms and head and possibly a few for the torso to
simulate the ﬂexibility of the spine.
The rigid bodies in a rag doll are connected to one another via constraints.
Rag doll constraints are specialized to mimic the kinds of motions the joints
in a real human body can perform. We usually make use of constraint chains
to improve the stability of the simulation.
A rag doll simulation is always tightly integrated with the animation sys-
tem. As the rag doll moves in the physics world, we extract the positions and
rotations of the rigid bodies, and we use this information to drive the positions
and orientations of certain joints in the animated skeleton. So in effect, a rag
doll is really just a form of procedural animation that happens to be driven by
the physics system. (See Chapter 11 for more details on skeletal animation.)
Of course, implementing a rag doll is not quite as simple as I’ve made it
sound here. For one thing, there’s usually not a one-to-one mapping between
the rigid bodies in the rag doll and the joints in the animated skeleton—the
skeleton usually has more joints than the rag doll has bodies. Therefore, we
need a system that can map rigid bodies to joints (i.e., one that “knows” to
which joint each rigid body in the rag doll corresponds). There may be addi-
tional joints between those that are being driven by the rag doll bodies, so the
mapping system must also be capable of determining the correct pose trans-
forms for these intervening joints. This is not an exact science. We must apply
artistic judgment and/or some knowledge of human biomechanics in order
to achieve a natural-looking rag doll.
12.4.8.8
Powered Constraints
Constraints can also be “powered,” meaning that an external engine system
such as the animation system can indirectly control the translations and ori-
entations of the rigid bodies in the rag doll.
Let’s take an elbow joint as an example. An elbow acts pretty much like a
limited hinge, with a little less than 180 degrees of free rotation. (Actually, an
elbow can also rotate axially, but we’ll ignore that for the purposes of this dis-
cussion.) To power this constraint, we model the elbow as a rotational spring.
Such a spring exerts a torque proportional to the spring’s angle of deﬂection
away from some predeﬁned rest angle, N = −k(θ −θrest). Now imagine

718
12. Collision and Rigid Body Dynamics
Bone
Collision 
Capsule
Capsule strikes 
an obstacle
Bone continues 
to move
Figure 12.34. With a powered rag doll constraint, and in the absence of any additional forces or
torques, a rigid body representing the lower arm can be made to exactly track the movements of
an animated elbow joint (left). If an obstacle blocks the motion of the body, it will diverge from
that of the animated elbow joint in a realistic way (right).
changing the rest angle externally, say by ensuring that it always matches the
angle of the elbow joint in an animated skeleton. As the rest angle changes,
the spring will ﬁnd itself out of equilibrium, and it will exert a torque that
tends to rotate the elbow back into alignment with θrest. In the absence of
any other forces or torques, the rigid bodies will exactly track the motion of
the elbow joint in the animated skeleton. But if other forces are introduced
(for example, the lower arm comes in contact with an immovable object), then
these forces will play into the overall motion of the elbow joint, allowing it
to diverge from the animated motion in a somewhat realistic manner. As il-
lustrated in Figure 12.34, this provides the illusion of a human who is trying
her best to move in a certain way (i.e., the “ideal” motion provided by the
animation) but who is sometimes unable to do so due to the limitations of the
physical world (e.g., her arm gets caught on something as she tries to swing it
forward).
12.4.9
Controlling the Motions of Rigid Bodies
Most game designs call for a degree of control over the way rigid bodies move
over and above the way they would move naturally under the inﬂuence of
gravity and in response to collisions with other objects in the scene. For ex-
ample:
•
An air vent applies an upward force to any object that enters its shaft of
inﬂuence.
•
A car is coupled to a trailer and exerts a pulling force on it as it moves.
•
A tractor beam exerts a force on an unwitting spacecraft.

12.4. Rigid Body Dynamics
719
•
An anti-gravity device causes objects to hover.
•
The ﬂow of a river creates a force ﬁeld that causes objects ﬂoating in the
river to move downstream.
And the list goes on. Most physics engines typically provide their users with
a number of ways to exert control over the bodies in the simulation. We’ll
outline the most common of these mechanisms in the following sections.
12.4.9.1
Gravity
Gravity is ubiquitous in most games that take place on the surface of the Earth
or some other planet (or on a spacecraft with simulated gravity). Gravity is
technically not a force but rather a (roughly) constant acceleration, so it af-
fects all bodies equally regardless of their mass. Because of its ubiquitous and
special nature, the magnitude and direction of the gravitational acceleration
is speciﬁed via a global setting in most SDKs. (If you’re writing a space game,
you can always set gravity to zero to eliminate it from the simulation.)
12.4.9.2
Applying Forces
Any number of forces can be applied to the bodies in a game physics simula-
tion. A force always acts over a ﬁnite time interval. (If it acted instantaneously,
it would be called an impulse—more on that in Section 12.4.9.4 below.) The
forces in a game are often dynamic in nature—they often change their direc-
tions and/or their magnitudes every frame. So the force-application function
in most physics SDKs is designed to be called once per frame for the duration
of the force’s inﬂuence. The signature of such a function usually looks some-
thing like this: applyForce(const Vector& forceInNewtons), where
the duration of the force is assumed to be ∆t.
12.4.9.3
Applying Torques
When a force is applied such that its line of action passes through the center of
mass of a body, no torque is generated, and only the body’s linear acceleration
is affected. If it is applied off-center, it will induce both a linear and a rotational
acceleration. A pure torque can be applied to a body as well by applying two
equal and opposite forces to points equidistant from the center of mass. The
linear motions induced by such a pair of forces will cancel each other out
(since for the purposes of linear dynamics, the forces both act at the center
of mass). This leaves only their rotational effects. A pair of torque-inducing
forces like this is known as a couple (http://en.wikipedia.org/wiki/Couple_
(mechanics).
A special function such as applyTorque(const Vector&
torque) may be provided for this purpose. However, if your physics SDK

720
12. Collision and Rigid Body Dynamics
provides no applyTorque() function, you can always write one and have it
generate a suitable couple instead.
12.4.9.4
Applying Impulses
As we saw in Section 12.4.7.2, an impulse is an instantaneous change in veloc-
ity (or actually, a change in momentum). Technically speaking, an impulse
is a force that acts for an inﬁnitesimal amount of time. However, the short-
est possible duration of force application in a time-stepped dynamics simu-
lation is ∆t, which is not short enough to simulate an impulse adequately.
As such, most physics SDKs provide a function with a signature such as
applyImpulse(const Vector& impulse) for the purposes of applying
impulses to bodies.
Of course, impulses come in two ﬂavors—linear and
angular—and a good SDK should provide functions for applying both types.
12.4.10
The Collision/Physics Step
Now that we’ve covered the theory and some of the technical details behind
implementing a collision and physics system, let’s take a brief look at how
these systems actually perform their updates every frame.
Every collision/physics engine performs the following basic tasks during
its update step. Different physics SDKs may perform these phases in different
orders. That said, the technique I’ve seen used most often goes something like
this:
1.
The forces and torques acting on the bodies in the physics world are
integrated forward by ∆t in order to determine their tentative positions
and orientations next frame.
2.
The collision detection library is called to determine if any new contacts
have been generated between any of the objects as a result of their ten-
tative movement. (The bodies normally keep track of their contacts in
order to take advantage of temporal coherency. Hence, at each step of
the simulation, the collision engine need only determine whether any
previous contacts have been lost and whether any new contacts have
been added.)
3.
Collisions are resolved, often by applying impulses or penalty forces or
as part of the constraint-solving step below. Depending on the SDK,
this phase may or may not include continuous collision detection (CCD,
otherwise known as time of impact detection or TOI).
4.
Constraints are satisﬁed by the constraint solver.

12.4. Rigid Body Dynamics
721
At the conclusion of step 4, some of the bodies may have moved away from
their tentative positions as determined in step 1. This movement may cause
additional interpenetrations between objects or cause other previously sat-
isﬁed constraints to be broken. Therefore, steps 1 through 4 (or sometimes
only 2 through 4, depending on how collisions and constraints are resolved)
are repeated until either (a) all collisions have been successfully resolved and
all constraints are satisﬁed, or (b) a predeﬁned maximum number of itera-
tions has been exceeded. In the latter case, the solver effectively “gives up,”
with the hope that things will resolve themselves naturally during subsequent
frames of the simulation. This helps to avoid performance spikes by amortiz-
ing the cost of collision and constraint resolution over multiple frames. How-
ever, it can lead to incorrect-looking behavior if the errors are too large or if
the time step is too long or is inconsistent. Penalty forces can be blended into
the simulation in order to gradually resolve these problems over time.
12.4.10.1
The Constraint Solver
A constraint solver is essentially an iterative algorithm that attempts to satisfy
a large number of constraints simultaneously by minimizing the error between
the actual positions and rotations of the bodies in the physics world and their
ideal positions and rotations as deﬁned by the constraints. As such, constraint
solvers are essentially iterative error-minimization algorithms.
Let’s take a look ﬁrst at how a constraint solver works in the trivial case
of a single pair of bodies connected by a single hinge constraint. During each
step of the physics simulation, the numerical integrator will ﬁnd new tenta-
tive transforms for the two bodies. The constraint solver then evaluates their
relative positions and calculates the error between the positions and orienta-
tions of their shared axis of rotation. If any error is detected, the solver moves
the bodies in such a way as to minimize or eliminate it. Since there are no
other bodies in the system, the second iteration of the step should discover no
new contacts, and the constraint solver will ﬁnd that the one hinge constraint
is now satisﬁed. Hence the loop can exit without further iterations.
When more than one constraint must be satisﬁed simultaneously, more
iterations may be required. During each iteration, the numerical integrator
will sometimes tend to move the bodies out of alignment with their con-
straints, while the constraint solver tends to put them back into alignment.
With luck, and a carefully designed approach to minimizing error in the con-
straint solver, this feedback loop should eventually settle into a valid solution.
However, the solution may not always be exact. This is why, in games with
physics engines, you sometimes witness seemingly impossible behaviors, like
chains that stretch (opening up little gaps between the links), objects that

722
12. Collision and Rigid Body Dynamics
interpenetrate brieﬂy or hinges that momentarily move beyond their allow-
able ranges. The goal of the constraint solver is to minimize error—it’s not
always possible to eliminate it completely.
12.4.10.2
Variations between Engines
The description given above is of course an over-simpliﬁcation of what really
goes on in a physics/collision engine every frame. The way in which the var-
ious phases of computation are performed, and their order relative to one an-
other, may vary from physics SDK to physics SDK. For example, some kinds
of constraints are modeled as forces and torques that are taken care of by the
numerical integration step rather than being resolved by the constraint solver.
Collision may be run before the integration step rather than after. Collisions
may be resolved in any number of different ways. Our goal here is merely to
give you a taste of how these systems work. For a detailed understanding of
how any one SDK operates, you’ll want to read its documentation and prob-
ably also inspect its source code (presuming the relevant bits are available
for you to read). The curious and industrious reader can get a good start by
downloading and experimenting with Open Dynamics Engine (ODE) and/or
PhysX, as these two SDKs are available for free. You can also learn a great deal
from ODE’s wiki, which is available at http://opende.sourceforge.net/wiki/
index.php/Main_Page.
12.5
Integrating a Physics Engine into Your Game
Obviously, a collision/physics engine is of little use by itself—it must be inte-
grated into your game engine. In this section, we’ll discuss the most common
interface points between the collision/physics engine and the rest of the game
code.
12.5.1
Linking Game Objects and Rigid Bodies
The rigid bodies and collidables in the collision/physics world are nothing
more than abstract mathematical descriptions. In order for them to be useful
in the context of a game, we need to link them in some way to their visual
representations on-screen. Usually, we don’t draw the rigid bodies directly
(except for debugging purposes). Instead, the rigid bodies are used to describe
the shape, size, and physical behavior of the logical objects that make up the
virtual game world. We’ll discuss game objects in depth in Chapter 15, but for
the time being, we’ll rely on our intuitive notion of what a game object is—
a logical entity in the game world, such as a character, a vehicle, a weapon,

12.5. Integrating a Physics Engine into Your Game
723
Debug Draw
Drive
Update
Submit
Figure 12.35. Rigid bodies are linked to their visual representations by way of game objects. An
optional direct rendering path is usually provided so that the locations of the rigid bodies can be
visualized for debugging purposes.
a ﬂoating power-up and so on. So the linkage between a rigid body in the
physics world and its visual representation on-screen is usually indirect, with
the logical game object serving as the hub that links the two together. This is
illustrated in Figure 12.35.
In general, a game object is represented in the collision/physics world by
zero or more rigid bodies. The following list describes three possible scenarios:
•
Zero rigid bodies. Game objects without any rigid bodies in the physics
world act as though they are not solid, because they have no collision
representation at all. Decorative objects with which the player or non-
player characters cannot interact, such as birds ﬂying overhead or por-
tions of the game world that can be seen but never reached, might have
no collision. This scenario can also apply to objects whose collision de-
tection is handled manually (without the help of the collision/physics
engine) for some reason.
•
One rigid body. Most simple game objects need only be represented by a
single rigid body. In this case, the shape of the rigid body’s collidable is
chosen to closely approximate the shape of the game object’s visual rep-
resentation, and the rigid body’s position and orientation exactly match
the position and orientation of the game object itself.
•
Multiple rigid bodies. Some complex game objects are represented by
multiple rigid bodies in the collision/physics world. Examples include
characters, machinery, vehicles or any object that is composed of multi-
ple solid pieces that can move relative to one another. Such game objects
usually make use of a skeleton (i.e., a hierarchy of afﬁne transforms) to

724
12. Collision and Rigid Body Dynamics
track the locations of their component pieces (although other means are
certainly possible as well). The rigid bodies are usually linked to the
joints of the skeleton in such a way that the position and orientation of
each rigid body corresponds to the position and orientation of one of
the joints. The joints in the skeleton might be driven by an animation, in
which case the associated rigid bodies simply come along for the ride.
Alternatively, the physics system might drive the locations of rigid bod-
ies and hence indirectly control the locations of the joints. The mapping
from joints to rigid bodies may or may not be one-to-one—some joints
might be controlled entirely by animation, while others are linked to
rigid bodies.
The linkage between game objects and rigid bodies must be managed by
the engine, of course. Typically, each game object will manage its own rigid
bodies, creating and destroying them when necessary, adding and removing
them from the physics world as needed, and maintaining the connection be-
tween each rigid body’s location and the location of the game object and/or
one of its joints. For complex game objects consisting of multiple rigid bodies,
a wrapper class of some kind may be used to manage them. This insulates
the game objects from the nitty-gritty details of managing a collection of rigid
bodies and allows different kinds of game objects to manage their rigid bodies
in a consistent way.
12.5.1.1
Physics-Driven Bodies
If our game has a rigid body dynamics system, then presumably we want
the motions of at least some of the objects in the game to be driven entirely
by the simulation. Such game objects are called physics-driven objects. Bits of
debris, exploding buildings, rocks rolling down a hillside, empty magazines
and shell casings—these are all examples of physics-driven objects.
A physics-driven rigid body is linked to its game object by stepping the
simulation and then querying the physics system for the body’s position and
orientation. This transform is then applied either to the game object as a whole
or to a joint or some other data structure within the game object.
Example: Building a Safe with a Detachable Door
When physics-driven rigid bodies are linked to the joints of a skeleton, the
bodies are often constrained to produce a desired kind of motion. As an exam-
ple, let’s look at how a safe with a detachable door might be modeled.
Visually, let’s assume that the safe consists of a single triangle mesh with
two submeshes, one for the housing and one for the door. A two-joint skeleton

12.5. Integrating a Physics Engine into Your Game
725
is used to control the motions of these two pieces. The root joint is bound to
the housing of the safe, while the child joint is bound to the door in such a
way that rotating the door joint causes the door submesh to swing open and
shut in a suitable way.
The collision geometry for the safe is broken into two independent pieces
as well, one for the housing and one for the door. These two pieces are used
to create two totally separate rigid bodies in the collision/physics world. The
rigid body for the safe’s housing is attached to the root joint in the skeleton,
and the door’s rigid body is linked to the door joint. A hinge constraint is
then added to the physics world to ensure that the door body swings properly
relative to the housing when the dynamics of the two rigid bodies are simu-
lated. The motions of the two rigid bodies representing the housing and the
door are used to update the transforms of the two joints in the skeleton. Once
the skeleton’s matrix palette has been generated by the animation system, the
rendering engine will end up drawing the housing and door submeshes in the
locations of the rigid bodies within the physics world.
If the door needs to be blown off at some point, the constraint can be bro-
ken, and impulses can be applied to the rigid bodies to send them ﬂying.
Visibly, it will appear to the human player that the door and the housing have
become separate objects. But in reality, it’s still a single game object and a
single triangle mesh with two joints and two rigid bodies.
12.5.1.2
Game-Driven Bodies
In most games, certain objects in the game world need to be moved about in
a non-physical way. The motions of such objects might be determined by an
animation or by following a spline path, or they might be under the control
of the human player. We often want these objects to participate in collision
detection—to be capable of pushing the physics-driven objects out of their
way, for example—but we do not want the physics system to interfere with
their motion in any way. To accommodate such objects, most physics SDKs
provide a special type of rigid body known as a game-driven body. (Havok calls
these “key framed” bodies.)
Game-driven bodies do not experience the effects of gravity. They are also
considered to be inﬁnitely massive by the physics system (usually denoted by
a mass of zero, since this is an invalid mass for a physics-driven body). The
assumption of inﬁnite mass ensures that forces and collision impulses within
the simulation can never change the velocity of a game-driven body.
To move a game-driven body around in the physics world, we cannot sim-
ply set its position and orientation every frame to match the location of the
corresponding game object. Doing so would introduce discontinuities that

726
12. Collision and Rigid Body Dynamics
would be very difﬁcult for the physical simulation to resolve. (For example,
a physics-driven body might ﬁnd itself suddenly interpenetrating a game-
driven body, but it would have no information about the game-driven body’s
momentum with which to resolve the collision.) As such, game-driven bodies
are usually moved using impulses—instantaneous changes in velocity that,
when integrated forward in time, will position the bodies in the desired places
at the end of the time step. Most physics SDKs provide a convenience func-
tion that will calculate the linear and angular impulses required in order to
achieve a desired position and orientation on the next frame. When moving
a game-driven body, we do have to be careful to zero out its velocity when it
is supposed to stop. Otherwise, the body will continue forever along its last
nonzero trajectory.
Example: Animated Safe Door
Let’s continue our example of the safe with a detachable door. Imagine that
we want a character to walk up to the safe, dial the combination, open the
door, deposit some money and close and lock the door again. Later, we want
a different character to get the money in a rather less-civilized manner—by
blowing the door off the safe. To do this, the safe would be modeled with an
additional submesh for the dial and an additional joint that allows the dial to
be rotated. No rigid body is required for the dial, however, unless of course
we want it to ﬂy off when the door explodes.
During the animated sequence of the person opening and closing the safe,
its rigid bodies can be put into game-driven mode. The animation now drives
the joints, which in turn drive the rigid bodies. Later, when the door is to be
blown off, we can switch the rigid bodies into physics-driven mode, break the
hinge constraint, apply the impulse and watch the door ﬂy.
As you’ve probably already noticed, the hinge constraint is not actually
needed in this particular example. It would only be required if the door is to
be left open at some point and we want to see the door swinging naturally in
response to the safe being moved or the door being bumped.
12.5.1.3
Fixed Bodies
Most game worlds are composed of both static geometry and dynamic objects.
To model the static components of the game world, most physics SDKs pro-
vide a special kind of rigid body known as a ﬁxed body. Fixed bodies act a bit
like game-driven bodies, but they do not take part in the dynamics simulation
at all. They are, in effect, collision-only bodies. This optimization can give a
big performance boost to most games, especially those whose worlds contain

12.5. Integrating a Physics Engine into Your Game
727
only a small number of dynamic objects moving around within a large static
world.
12.5.1.4
Havok’s Motion Type
In Havok, all types of rigid body are represented by instances of the class hkp-
RigidBody. Each instance contains a ﬁeld that speciﬁes its motion type. The
motion type tells the system whether the body is ﬁxed, game-driven (what
Havok calls “key framed”) or physics-driven (what Havok calls “dynamic”).
If a rigid body is created with the ﬁxed motion type, its type can never be
changed. Otherwise, the motion type of a body can be changed dynamically at
runtime. This feature can be incredibly useful. For example, an object that is in
a character’s hand would be game-driven. But as soon as the character drops
or throws the object, it would be changed to physics-driven so the dynamics
simulation can take over its motion. This is easily accomplished in Havok by
simply changing the motion type at the moment of release.
The motion type also doubles as a way to give Havok some hints about
the inertia tensor of a dynamic body. As such, the “dynamic” motion type is
broken into subcategories such as “dynamic with sphere inertia,” “dynamic
with box inertia” and so on. Using the body’s motion type, Havok can de-
cide to apply various optimizations based on assumptions about the internal
structure of the inertia tensor.
12.5.2
Updating the Simulation
The physics simulation must of course be updated periodically, usually once
per frame. This does not merely involve stepping the simulation (numerically
integrating, resolving collisions and applying constraints). The linkages be-
tween the game objects and their rigid bodies must be maintained as well.
If the game needs to apply any forces or impulses to any of the rigid bod-
ies, this must also be done every frame. The following steps are required to
completely update the physics simulation:
•
Update game-driven rigid bodies. The transforms of all game-driven rigid
bodies in the physics world are updated so that they match the trans-
forms of their counterparts (game objects or joints) in the game world.
•
Update phantoms. A phantom shape acts like a game-driven collidable
with no corresponding rigid body. It is used to perform certain kinds
of collision queries. The locations of all phantoms are updated prior to
the physics step, so that they will be in the right places when collision
detection is run.

728
12. Collision and Rigid Body Dynamics
•
Update forces, apply impulses and adjust constraints. Any forces being ap-
plied by the game are updated. Any impulses caused by game events
that occurred this frame are applied. Constraints are adjusted if neces-
sary. (For example, a breakable hinge might be checked to determine if
it has been broken; if so, the physics engine is instructed to remove the
constraint.)
•
Step the simulation.
We saw in Section 12.4.10 that the collision and
physics engines must both be updated periodically. This involves nu-
merically integrating the equations of motion to ﬁnd the physical state of
all bodies on the next frame, running the collision detection algorithm to
add and remove contacts from all rigid bodies in the physics world, re-
solving collisions and applying constraints. Depending on the SDK, these
update phases may be hidden behind a single atomic step() function,
or it may be possible to run them individually.
•
Update physics-driven game objects. The transforms of all physics-driven
objects are extracted from the physics world, and the transforms of the
corresponding game objects or joints are updated to match.
•
Query phantoms. The contacts of each phantom shape are read after the
physics step and used to make decisions.
•
Perform collision cast queries. Ray casts and shape casts are kicked off, ei-
ther synchronously or asynchronously. When the results of these queries
become available, they are used by various engine systems to make de-
cisions.
These tasks are usually performed in the order shown above, with the ex-
ception of ray and shape casts, which can theoretically be done at any time
during the game loop. Clearly it makes sense to update game-driven bod-
ies and apply forces and impulses prior to the step, so that the effects will
be “seen” by the simulation. Likewise, physics-driven game objects should
always be updated after the step, to ensure that we’re using the most up-to-
date body transforms. Rendering typically happens after everything else in
the game loop. This ensures that we are rendering a consistent view of the
game world at a particular instant in time.
12.5.2.1
Timing Collision Queries
In order to query the collision system for up-to-date information, we need to
run our collision queries (ray and shape casts) after the physics step has run
during the frame. However, the physics step is usually run toward the end
of the frame, after the game logic has made most of its decisions and the new

12.5. Integrating a Physics Engine into Your Game
729
locations of any game-driven physics bodies have been determined. When,
then, should collision queries be run?
This question does not have an easy answer. We have a number of options,
and most games end up using some or all of them:
•
Base decisions on last frame’s state. In many cases, decisions can be made
correctly based on last frame’s collision information. For example, we
might want to know whether or not the player was standing on some-
thing last frame, in order to decide whether or not he should start falling
this frame. In this case, we can safely run our collision queries prior to
the physics step.
•
Accept a one-frame lag. Even if we really want to know what is happen-
ing this frame, we may be able to tolerate a one-frame lag in our collision
query results. This is usually only true if the objects in question aren’t
moving too fast. For example, we might move one object forward in
time and then want to know whether or not that object is now in the
player’s line of sight. A one-frame-off error in this kind of query may
not be noticeable to the player. If this is the case, we can run the collision
query prior to the physics step (returning collision information from the
previous frame) and then use these results as if they were an approxima-
tion to the collision state at the end of the current frame.
•
Run the query after the physics step. Another approach is to run certain
queries after the physics step. This is feasible when the decisions being
made based on the results of the query can be deferred until late in the
frame. For example, a rendering effect that depends on the results of a
collision query could be implemented this way.
12.5.2.2
Single-Threaded Updating
A very simple single-threaded game loop might look something like this:
F32 dt = 1.0f/30.0f;
for (;;) // main game loop
{
g_hidManager->poll();
g_gameObjectManager->preAnimationUpdate(dt);
g_animationEngine->updateAnimations(dt);
g_gameObjectManager->postAnimationUpdate(dt);
g_physicsWorld->step(dt);
g_animationEngine->updateRagDolls(dt);

730
12. Collision and Rigid Body Dynamics
g_gameObjectManager->postPhysicsUpdate(dt);
g_animationEngine->finalize();
g_effectManager->update(dt);
g_audioEngine->udate(dt);
// etc.
g_renderManager->render();
dt = calcDeltaTime();
}
In this example, our game objects are updated in three phases: once before
animation runs (during which they can queue up new animations, for exam-
ple), once after the animation system has calculated ﬁnal local poses and a
tentative global pose (but before the ﬁnal global pose and matrix palette has
been generated) and once after the physics system has been stepped.
•
The locations of all game-driven rigid bodies are generally updated in
preAnimationUpdate() or postAnimationUpdate(). Each game-
driven body’s transform is set to match the location of either the game
object that owns it or a joint in the owner’s skeleton.
•
The location of each physics-driven rigid body is generally read in post-
PhysicsUpdate() and used to update the location of either the game
object or one of the joints in its skeleton.
One important concern is the frequency with which you are stepping the
physics simulation. Most numerical integrators, collision detection algorithms
and constraint solvers operate best when the time between steps (∆t) is con-
stant. It’s usually a good idea to step your physics/collision SDK with an ideal
1/30 second or 1/60 second time delta and then govern the frame rate of your
overall game loop. If your game drops below its target frame rate, it’s better
to let the physics slow down visually than to try to adjust the simulation time
step to match the actual frame rate.
12.5.3
Example Uses of Collision and Physics in a Game
To make our discussion of collision and physics more concrete, let’s take a
high-level look at a few common examples of how collision and/or physics
simulations are commonly used in real games.

12.5. Integrating a Physics Engine into Your Game
731
12.5.3.1
Simple Rigid Body Game Objects
Many games include simple physically simulated objects like weapons, rocks
that can be picked up and thrown, empty magazines, furniture, objects on
shelves that can be shot and so on. Such objects might be implemented by
creating a custom game object class and giving it a reference to a rigid body in
the physics world (e.g., hkpRigidBody if we’re using Havok). Or we might
create an add-on component class that handles simple rigid body collision and
physics, allowing this feature to be added to virtually any type of game object
in the engine.
Simple physics objects usually change their motion type at runtime. They
are game-driven when being held in a character’s hand and physics-driven
when in free fall after having been dropped.
12.5.3.2
Bullet Traces
Whether or not you approve of game violence, the fact remains that laser guns
and projectile weapons of one form or another are a big part of most games.
Let’s look at how these are typically implemented.
Sometimes projectiles are implemented using ray casts. On the frame that
the weapon is ﬁred, we shoot off a ray cast, determine what object was hit and
immediately impart the impact to the affected object.
Unfortunately, the ray cast approach does not account for the travel time
of the projectile. It also does not account for the slight downward trajectory
caused by the inﬂuence of gravity. If these details are important to the game,
we can model our projectiles using real rigid bodies that move through the
collision/physics world over time. This is especially useful for slower-moving
projectiles, like thrown objects or rockets.
The thrown bricks in Naughty
Dog’s The Last of Us used such an approach.
There are plenty of issues to consider and deal with when implementing
laser beams and projectiles. A few of the most common ones are discussed
below.
Bullet Ray Casting
When using ray casting to check for bullet hits, the question arises: Does the
ray come from the camera focal point or from the tip of the gun in the player
character’s hands? This is especially problematic in a third-person shooter,
where the ray coming out of the player’s gun usually does not align with the
ray coming from the camera focal point through the reticle in the center of the
screen. This can lead to situations in which the reticle appears to be on top of a
target, yet the third-person character is clearly behind an obstacle and would

732
12. Collision and Rigid Body Dynamics
not be able to shoot that target from his point of view. Various “tricks” must
usually be employed to ensure that the player feels like he or she is shooting
what he or she is aiming at while maintaining plausible visuals on the screen.
Mismatches between Collision and Visible Geometry
Mismatches between collision geometry and visible geometry can lead to situ-
ations in which the player can see the target through a small crack or just over
the edge of some other object, and yet the collision geometry is solid, so the
bullet cannot reach the target. (This is usually only a problem for the player
character.) One solution to this problem is to use a render query instead of
a collision query to determine if the ray actually hit the target. For example,
during one of the rendering passes, we could generate a texture in which each
pixel stores the unique identiﬁer of the game object to which it corresponds.
We can then query this texture to determine whether or not an enemy char-
acter or other suitable target occupies the pixel(s) underneath the weapon’s
reticle.
Aiming in a Dynamic Environment
AI-controlled characters may need to “lead” their shots if projectiles take a
ﬁnite amount of time to reach their targets.
Impact Effects
When bullets hit their targets, we may want to trigger a sound or a particle
effect, lay down a decal or perform other tasks.
In the Unreal engine, this is accomplished via a system of physical materials.
Visible geometry can be tagged not only with visual materials, but with phys-
ical materials as well. The former deﬁnes how the surface looks, the latter
deﬁnes how it reacts to physical interactions, including impact sounds, bullet
“squib” particle effects, decals and so on. (See http://udn.epicgames.com/
Three/PhysicalMaterialSystem.html for more details.)
At Naughty Dog, we use a very similar system: Collision geometry can be
tagged with polygon attributes (PATs for short), which deﬁne certain physical
behaviors like footstep sounds. But bullet impacts are treated in a special
way, because we need them to interact directly with visible geometry rather
than the crude collision geo. As such, visible materials can be tagged with an
optional bullet effect that deﬁnes which bullet squib, impact sound and decal
should be laid down for each possible type of projectile that might impact that
surface.

12.5. Integrating a Physics Engine into Your Game
733
12.5.3.3
Grenades
Grenades in games are sometimes implemented as free-moving physics ob-
jects. However, this leads to a signiﬁcant loss of control. Some control can be
regained by imposing various artiﬁcial forces or impulses on the grenade. For
example, we could apply an extreme air drag once the grenade bounces for
the ﬁrst time, in an attempt to limit the distance it can bounce away from its
target.
Some game teams actually go so far as to manage the grenade’s motion en-
tirely manually. The arc of a grenade’s trajectory can be calculated beforehand,
using a series of ray casts to determine what target it would hit if released. The
trajectory can even be shown to the player via some kind of on-screen display.
When the grenade is thrown, the game moves it along its arc and can then
carefully control the bounce so that it never goes too far away from its target,
while still looking natural.
12.5.3.4
Explosions
In a game, an explosion typically has a few components: some kind of visual
effect like a ﬁreball and smoke, audio effects to mimic the sound of the explo-
sion and its impacts with objects in the world, and a growing damage radius
that affects any objects in its wake.
When an object ﬁnds itself in the radius of an explosion, its health is typ-
ically reduced, and we often also want to impart some motion to mimic the
effect of the shock wave. This might be done via an animation. (For exam-
ple, the reaction of a character to an explosion might best be done this way.)
We might also wish to allow the impact reaction to be driven entirely by the
dynamics simulation. We can accomplish this by having the explosion apply
impulses to any suitable objects within its radius. It’s pretty easy to calculate
direction of these impulses—they are typically radial, calculated by normal-
izing the vector from the center of the explosion to the center of the impacted
object and then scaling this vector by the magnitude of the explosion (and
perhaps falling off as the distance from the epicenter increases).
Explosions may interact with other engine systems as well. For exam-
ple, we might want to impart a “force” to the animated foliage system, caus-
ing grass, plants and trees to momentarily bend as a result of the explosion’s
shock wave.
12.5.3.5
Destructible Objects
Destructible objects are commonplace in many games. These objects are pecu-
liar because they start out in an undamaged state in which they must appear

734
12. Collision and Rigid Body Dynamics
to be a single cohesive object, and yet they must be capable of breaking into
many separate pieces. We may want the pieces to break off one by one, al-
lowing the object to be “whittled down” gradually, or we may only require a
single catastrophic explosion.
Deformable body simulations like DMM can handle destruction naturally.
However, we can also implement breakable objects using rigid body dynam-
ics. This is typically done by dividing a model into a number of breakable
pieces and assigning a separate rigid body to each one. This is the approach
taken by Havok Destruction, for example.
For reasons of performance optimization and/or visual quality, we might
decide to use special “undamaged” versions of the visual and collision geom-
etry, each of which is constructed as a single solid piece. This model can be
swapped out for the damaged version when the object needs to start breaking
apart. In other cases, we may want to model the object as separate pieces at
all times. This might be appropriate if the object is a stack of bricks or a pile
of pots and pans, for example.
To model a multi-piece object, we could simply stack a bunch of rigid bod-
ies and let the physics simulation take care of it. This can be made to work
in good-quality physics engines (although it’s not always trivial to get right).
However, we may want some Hollywood-style effects that cannot be achieved
with a simple stack of rigid bodies.
For example, we may want to deﬁne the structure of the object. Some
pieces might be indestructible, like the base of a wall or the chassis of a car.
Others might be non-structural—they just fall off when hit by bullets or other
objects. Still other pieces might be structural—if they are hit, not only do they
fall, but they also impart forces to other pieces lying on top of them. Some
pieces could be explosive—when they are hit, they create secondary explosions
or propagate damage throughout the structure. We may want some pieces to
act as valid cover points for characters but not others. This implies that our
breakable object system may have some connections to the cover system.
We might also want our breakable objects to have a notion of health. Dam-
age might build up until eventually the whole thing collapses, or each piece
might have a health, requiring multiples shots or impacts before it is allowed
to break. Constraints might also be employed to allow broken pieces to hang
off the object rather than coming away from it completely.
We may also want our structures to take time to collapse completely. For
example, if a long bridge is hit by an explosion at one end, the collapse should
slowly propagate from one end to the other so that the bridge looks massive.
This is another example of a feature the physics system won’t give you for
free—it would just wake up all rigid bodies in the simulation island simulta-

12.5. Integrating a Physics Engine into Your Game
735
neously. These kinds of effects can be implemented through judicious use of
the game-driven motion type.
12.5.3.6
Character Mechanics
For a game like bowling, pinball or Marble Madness, the “main character” is
a ball that rolls around in an imaginary game world. For this kind of game,
we could very well model the ball as a free-moving rigid body in the physics
simulation and control its movements by applying forces and impulses to it
during gameplay.
In character-based games, however, we usually don’t take this kind of ap-
proach. The locomotion of a humanoid or animal character is usually far too
complex to be controlled adequately with forces and impulses. Instead, we
usually model characters as a set of game-driven capsule-shaped rigid bodies,
each one linked to a joint in the character’s animated skeleton. These bodies
are primarily used for bullet hit detection or to generate secondary effects such
as when a character’s arm bumps an object off a table. Because these bodies
are game-driven, they won’t avoid interpenetrations with immovable objects
in the physics world, so it is up to the animator to ensure that the character’s
movements appear believable.
To move the character around in the game world, most games use sphere
or capsule casts to probe in the direction of desired motion. Collisions are
resolved manually. This allows us to do cool stuff like:
•
having the character slide along walls when he runs into them at an
oblique angle;
•
allowing the character to “pop up” over low curbs rather than getting
stuck;
•
preventing the character from entering a “falling” state when he walks
off a low curb;
•
preventing the character from walking up slopes that are too steep (most
games have a cutoff angle after which the character will slide back rather
than being able to walk up the slope); and
•
adjusting animations to accommodate collisions.
As an example of this last point, if the character is running directly into a
wall at a roughly 90 degree angle, we can let the character “moonwalk” into
the wall forever, or we can slow down his animation. We can also do some-
thing even more slick, like playing an animation in which the character sticks
out his hand and touches the wall and then idles sensibly until the movement
direction changes.

736
12. Collision and Rigid Body Dynamics
Figure 12.36. Havok’s character controller models a character as a capsule-shaped phantom. The
phantom maintains a noise-reduced collision manifold (a collection of contact planes) that can
be used by the game to make movement decisions.
Havok provides a character controller system that handles many of these
things. In Havok’s system, illustrated in Figure 12.36, a character is modeled
as a capsule phantom that is moved each frame to ﬁnd a potential new loca-
tion. A collision contact manifold (i.e., a collection of contact planes, cleaned
up to eliminate noise) is maintained for the character. This manifold can be
analyzed each frame in order to determine how best to move the character,
adjust animations and so on.
12.5.3.7
Camera Collision
In many games, the camera follows the player’s character or vehicle around
in the game world, and it can often be rotated or controlled in limited ways
by the person playing the game. It’s important in such games to never permit
the camera to interpenetrate geometry in the scene, as this would break the
illusion of realism. The camera system is therefore another important client of
the collision engine in many games.
The basic idea behind most camera collision systems is to surround the
virtual camera with one or more sphere phantoms or sphere cast queries that
can detect when it is getting close to colliding with something. The system
can respond by adjusting the camera’s position and/or orientation in some
way to avoid the potential collision before the camera actually passes through
the object in question.

12.5. Integrating a Physics Engine into Your Game
737
This sounds simple enough, but it is actually an incredibly tricky problem
requiring a great deal of trial and error to get right. To give you a feel for how
much effort can be involved, many game teams have a dedicated engineer
working on the camera system for the entire duration of the project. We can’t
possibly cover camera collision detection and resolution in any depth here,
but the following list should give you a sense of some of the most pertinent
issues to be aware of:
•
Zooming the camera in to avoid collisions works well in a wide variety
of situations. In a third-person game, you can zoom all the way in to
a ﬁrst-person view without causing too much trouble (other than mak-
ing sure the camera doesn’t interpenetrate the character’s head in the
process).
•
It’s usually a bad idea to drastically change the horizontal angle of the
camera in response to collisions, as this tends to mess with camera-
relative player controls. However, some degree of horizontal adjustment
can work well, depending on what the player is expected to be doing
at the time. If she is aiming at a target, she’ll be angry with you if you
throw off her aim to bring the camera out of collision. But if she’s just lo-
comoting through the world, the change in camera orientation may feel
entirely natural. Because of this, you might want to allow adjustments
to the horizontal angle of the camera only when the main character is
not in the heat of a battle.
•
You can adjust the vertical angle of the camera to some degree, but it’s
important not to do too much of this, or the player will lose track of the
horizon and end up looking down onto the top of the player character’s
head!
•
Some games allow the camera to move along an arc lying in a vertical
plane, perhaps described by a spline. This permits a single HID control
such as the vertical deﬂection of the left thumb stick to control both the
zoom and the vertical angle of the camera in an intuitive way. (The cam-
era in Uncharted and The Last of Us works this way.) When the camera
comes into collision with objects in the world, it can be automatically
moved along this same arc to avoid the collision, the arc might be com-
pressed horizontally, or any number of other approaches might be taken.
•
It’s important to consider not only what’s behind and beside the camera
but what is in front of it as well. For example, what should happen if
a pillar or another character comes between the camera and the player
character? In some games, the offending object becomes translucent; in
others, the camera zooms in or swings around to avoid the collision.

738
12. Collision and Rigid Body Dynamics
This may or may not feel good to the person playing the game! How
you handle these kinds of situations can make or break the perceived
quality of your game.
Even after taking account of these and many other problematic situations,
your camera may not look or feel right! Always budget plenty of time for trial
and error when implementing a camera collision system.
12.5.3.8
Rag Doll Integration
In Section 12.4.8.7, we learned how special types of constraints can be used
to link a collection of rigid bodies together to mimic the behavior of a limp
(dead or unconscious) human body. In this section, we’ll investigate a few of
the issues that arise when integrating rag doll physics into your game.
As we saw in Section 12.5.3.6, the gross movements of a conscious charac-
ter are usually determined by performing shape casts or moving a phantom
shape around in the game world. The detailed movements of the character’s
body are typically driven by animations. Game-driven rigid bodies are some-
times attached to the limbs for the purposes of weapons targeting or to allow
the character to knock over other objects in the world.
When a character becomes unconscious, the rag doll system kicks in. The
character’s limbs are modeled as capsule-shaped rigid bodies connected via
constraints and linked to joints in the character’s animated skeleton.
The
physics system simulates the motions of these bodies, and we update the
skeletal joints to match, thereby allowing physics to move the character’s
body.
The set of rigid bodies used for rag doll physics might not be the same
ones afﬁxed to the character’s limbs when it was alive. This is because the
two collision models have very different requirements. When the character
is alive, its rigid bodies are game-driven, so we don’t care if they interpene-
trate. And in fact, we usually want them to overlap, so there aren’t any holes
through which an enemy character might shoot. But when the character turns
into a rag doll, it’s important that the rigid bodies do not interpenetrate, as
this would cause the collision resolution system to impart large impulses that
would tend to make the limbs explode outward! For these reasons, it’s actu-
ally quite common for characters to have entirely different collision/physics
representations depending on whether they’re conscious or unconscious.
Another issue is how to transition from the conscious state to the uncon-
scious state. A simple LERP blend between animation-generated and physics-
generated poses usually doesn’t work very well, because the physics pose
very quickly diverges from the animation pose. (A blend between two totally

12.5. Integrating a Physics Engine into Your Game
739
unrelated poses usually doesn’t look natural.) As such, we may want to use
powered constraints during the transition (see Section 12.4.8.8).
Characters often interpenetrate background geometry when they are con-
scious (i.e., when their rigid bodies are game-driven). This means that the
rigid bodies might be inside another solid object when the character transi-
tions to rag doll (physics-driven) mode. This can give rise to huge impulses
that cause rather wild-looking rag doll behavior in-game. To avoid these prob-
lems, it is best to author death animations carefully, so that the character’s
limbs are kept out of collision as best as possible. It’s also important to detect
collisions via phantoms or collision callbacks during the game-driven mode
so that you can drop the character into rag doll mode the moment any part of
his body touches something solid.
Even when these steps are taken, rag dolls have a tendency to get stuck
inside other objects. Single-sided collision can be an incredibly important fea-
ture when trying to make rag dolls look good. If a limb is partly embedded
in a wall, it will tend to be pushed out of the wall rather than staying stuck
inside it. However, even single-sided collision doesn’t solve all problems. For
example, when the character is moving quickly or if the transition to rag doll
isn’t executed properly, one rigid body in the rag doll can end up on the far
side of a thin wall. This causes the character to hang in mid-air rather than
falling properly to the ground.
Another rag doll feature that can be useful is the ability for unconscious
characters to regain consciousness and get back up. To implement this, we
need a way to search for a suitable “stand up” animation. We want to ﬁnd an
animation whose pose on frame zero most closely matches the rag doll’s pose
after it has come to rest (which is totally unpredictable in general). This can
be done by matching the poses of only a few key joints, like the upper thighs
and the upper arms. Another approach is to manually guide the rag doll into
a pose suitable for getting up by the time it comes to rest, using powered
constraints.
As a ﬁnal note, we should mention that setting up a rag doll’s constraints
can be a tricky business. We generally want the limbs to move freely but with-
out doing anything biomechanically impossible. This is one reason special-
ized types of constraints are often used when constructing rag dolls. Nonethe-
less, you shouldn’t assume that your rag dolls will look great without some
effort. High-quality physics engines like Havok provide a rich set of content
creation tools that allow an artist to set up constraints within a DCC package
like Maya and then test them in real time to see how they might look in-game.
All in all, getting rag doll physics to work in your game isn’t particularly
difﬁcult, but getting it to look good can take a lot of work! As with many things

740
12. Collision and Rigid Body Dynamics
in game programming, it’s a good idea to budget plenty of time for trial and
error, especially when it’s your ﬁrst time working with rag dolls.
12.6
Advanced Physics Features
A rigid body dynamics simulation with constraints can cover an amazing
range of physics-driven effects in a game. However, such a system clearly has
its limitations. Recent research and development is seeking to expand physics
engines beyond constrained rigid bodies. Here are just a few examples:
•
Deformable bodies. As hardware capabilities improve and more-efﬁcient
algorithms are developed, physics engines are beginning to provide sup-
port for deformable bodies. DMM is an excellent example of such an
engine.
•
Cloth. Cloth can be modeled as a sheet of point masses, connected by
stiff springs. Cloth is notoriously difﬁcult to get right, as many difﬁ-
culties arise with respect to collision between cloth and other objects,
numerical stability of the simulation, etc. That being said, many games
and third-party physics SDKs like Havok now provide powerful and
well-behaved cloth simulations for use in games and other real-time ap-
plications.
•
Hair. Hair can be modeled by a large number of small physically sim-
ulated ﬁlments, or a simpler approach can be used in which sheets of
cloth are texture-mapped to look like hair, and the cloth simulation is
tuned to make the character’s hair move in a believable way. This is
how Ellie’s hair in The Last of Us works. Hair simulation and rendering
remains an active area of research, and the quality of hair in games will
certainly continue to improve.
•
Water surface simulations and buoyancy. Games have been doing water
surface simulations and buoyancy for some time now. Buoyancy can be
implemented via a special-case system (not part of the physics engine
per se), or it can be modeled via forces within the physics simulation.
Organic movement of the water surface is often a rendering effect only
and does not affect the physics simulation at all. From the point of view
of physics, the water surface is often modeled as a plane. For large dis-
placements in the water surface, the entire plane might be moved. How-
ever, some game teams and researchers are pushing the limits of these
simulations, allowing for dynamic water surfaces, waves that crest, re-
alistic current simulations and more.

12.6. Advanced Physics Features
741
•
General ﬂuid dynamics simulations. Right now, ﬂuid dynamics falls pri-
marily into the realm of specialized simulation libraries. However, this
is an active area of research and development, and some games already
make use of ﬂuid simulations to produce some astounding visual ef-
fects.
•
Physically based audio synthesis. When physically simulated objects col-
lide, bounce, roll and slide, it’s important to be able to generate ap-
propriate audio to reinforce the believability of the simulation. These
sounds can be created in games via controlled playback of pre-recorded
audio clips. But dynamic synthesis of such sounds is becoming a viable
alternative, and is currently an active area of research.
•
GPGPU. As GPUs become more and more powerful, there has been a
shift toward harnessing their awesome parallel processing power for
tasks other than graphics. One obvious application of general-purpose
GPU (GPGPU) computing is for collision and physics simulation. For
example, for the PlayStation 4 version of The Last of Us, Naughty Dog’s
cloth simulation engine was ported to run entirely on the GPU.

This page intentionally left blank
This page intentionally left blank

13
Audio
I
f you’ve ever watched a horror ﬁlm with your speakers muted, you know
just how important audio is to immersiveness. (If not, try it! It’s a real
ear-opener.) Be it a ﬁlm or a video game, sound can quite literally make the
difference between a gripping, emotional, unforgettable multimedia experi-
ence and a lackluster yawnfest.
Modern games immerse the player in a realistic (or a semi-realistic but
stylized) virtual environment. The graphics engine is charged with the task of
reproducing as accurately and believably as possible what the player would
actually see, if he or she were present within this virtual environment (while
remaining true to the art style of the game). In exactly the same sense, the
audio engine is charged with the task of accurately and believably repro-
ducing what the player would actually hear, if he or she were present in the
game world (while remaining true to the ﬁction and tonal style of the game).
Sound programmers today use the term audio rendering engine to underscore
its many parallels with the graphics rendering engine.
In this chapter, we’re going to explore both the theory and practice of cre-
ating audio for a AAA game. We’ll introduce an area of mathematics called
signal processing theory that underlies almost every aspect of digital audio tech-
nology, including digital sound recording and playback, ﬁltering, reverb and
other digital signal processor (DSP) effects. We’ll explore game audio from
a software engineering standpoint, by investigating of a number of widely
743

744
13. Audio
used audio APIs, breaking down the components that comprise a typical au-
dio rendering engine and learning how the audio system is interconnected
with other game engine systems. We’ll also see how environmental acous-
tic modeling and character dialog were handled in Naughty Dog’s recent hit
game, The Last of Us. So hold on tight, keep your hands inside the car at all
times and enjoy the noisy ride!
13.1
The Physics of Sound
Sound is a compression wave that travels through the air (or some other com-
pressible medium). A sound wave gives rise to alternating regions of air com-
pression and decompression (also known as rarefaction) relative to the average
atmospheric pressure. As such, we measure the amplitude of a sound wave in
units of pressure. In SI units, pressure is measured in Pascals, abbreviated Pa.
One Pascal is the force of one Newton applied over an area of one square
meter (1 Pa = 1 N/m2 = 1 kg/(m · s2)).
The instantaneous acoustic pressure is the ambient atmospheric pressure (con-
sidered a constant for our purposes) plus the perturbation caused by the sound
wave at one speciﬁc instant in time:
pinst = patmos + psound.
Of course, sound is a dynamic phenomenon—the sound pressure varies over
time. We can plot the instantaneous sound pressure as a function of time,
pinst(t). In signal processing theory—the area of mathematics that underlies vir-
tually every aspect of digital audio technology—such a time-varying function
is called a signal. Figure 13.1 illustrates a typical sound wave signal p(t), oscil-
lating about the average atmospheric pressure.
p(t)
t
patmos
+
–
Figure 13.1. A signal p(t) can be used to model the time-varying acoustic pressure of a sound.

13.1. The Physics of Sound
745
T
t
p(t)
Figure 13.2. The period T of an arbitrary periodic signal is the minimum time between repeated
patterns in the waveform.
13.1.1
Properties of Sound Waves
When a musical instrument plays a long steady note, the resulting sound
pressure signal is periodic, meaning the waveform consists of a repeating pat-
tern characteristic to that particular kind of instrument. The period T of any
repreating pattern describes the minimum amount of time that passes be-
tween successive instances of the pattern. For example, for a sinusoidal sound
wave the period measures the time between successive peaks or troughs. In
SI units, period is typically measured in seconds (s). This is illustrated in Fig-
ure 13.2.
The frequency of a wave is just the inverse of its period (f = 1/T). Fre-
quency is measured in Hertz (Hz), which means “cycles per second.” A “cy-
cle” is technically a dimensionless quantity, so the Hertz is the inverse of the
second (Hz = 1/s).
Many scientists and mathematicians make use of a quantity known as the
angular frequency, typically denoted by the symbol ω. The angular frequency is
just the rate of oscillation measured in radians per second instead of cycles per
second. Since one complete circular rotation is 2π radians, ω = 2πf = 2π/T.
Angular frequency is very useful when analyzing sinusoidal waves, because
a circular motion in two dimensions gives rise to a sinusoidal motion when
projected onto a single dimensional axis.
The amount by which a periodic signal such as a sine wave is shifted left
or right along the time axis is known as its phase. Phase is a relative term. For
example, sin(t) is really just a version of cos(t) that has been phase-shifted by
+ π
2 along the t axis (i.e., sin(t) = cos(t−π
2 )). Likewise cos(t) is just sin(t) phase-
shifted by −π
2 (i.e., cos(t) = sin(t + π
2 )). Phase is illustrated in Figure 13.3.
The speed v at which a sound wave propagates through its medium de-
pends upon the material and physical properties of the medium, including

746
13. Audio
t
sin(t)
cos(t)
/2
/2
– /2
Figure 13.3. The sine and cosine functions are just phase-shifted versions of one another.
phase (solid, gas or liquid), temperature, pressure and density. In 20°C dry
air, the speed of sound is approximately 343.2 m/s, which is 767.7 mph or
1235.6 km/h.
The wavelength λ of a sinusoidal wave measures the spatial distance be-
tween successive peaks or troughs. It depends in part on the freequency of
the wave, but because it is a spatial measurement it also depends on the speed
of the wave. Speciﬁcally, λ = v/f where v is the speed of the wave (mea-
sured in m/s) and f is the frequency (measured in Hz or 1/s). The seconds
in the numerator and denominator cancel one another, and we are left with
wavelength measured in units of meters.
13.1.2
Perceived Loudness and the Decibel
In order to judge the “loudness” of the sounds we hear, our ears continuously
average the amplitude of the incoming sound signal over a short, sliding time
window. This averaging effect is modeled well by a quantity known as the
effective sound pressure. This is deﬁned as the root mean square (RMS) of the
instantaneous sound pressure measured over a speciﬁc interval of time.
If we were to take a series of n discrete sound pressure measurements pi,
equally spaced in time, the RMS sound pressure prms would be
prms =
v
u
u
t 1
n
n
X
i=1
p2
i .
(13.1)
However, our ears take pressure measurements continuously, rather that at
discrete points in time. If we imagine measuring the instantaneous sound
pressure continuously, starting at time T1 and lasting until time T2, the sum-
mation in Equation (13.1) would become an integral as follows:
prms =
s
1
T2 −T1
Z T2
T1
(p(t))2 dt.
(13.2)

13.1. The Physics of Sound
747
However, the story doesn’t end here. Perceived loudness is actually pro-
portional to the acoustic intensity I, which is itself proportional to the square of
the RMS sound pressure:
I ∝p2
rms.
Humans can perceive a very wide range of sound pressure variations—
from the ﬂutter of a piece of paper falling to the ground to the boom of an
aircraft breaking mach one. In order to manage such a wide dynamic range,
we normally measure sound intensity in units of decibels (dB). The decibel is
a logarithmic unit that expresses the ratio between two values. By employing
a logarithmic scale, the decibel allows a wide range of measurements to be
represented by a relatively narrow range of values. A decibel is actually one-
tenth of a bel, a unit named in honor of Alexander Graham Bell.
When sound intensity is measured in decibels, it is called sound pressure
level (SPL) and represented by the symbol Lp. Sound pressure level is deﬁned
as the ratio of the acoustic intensity (i.e., the squared pressure) of a sound
relative to a reference intensity pref that represents the lower limit of human
hearing. So we have:
Lp = 10 log10
p2
rms
p2
ref

dB
= 20 log10
prms
pref

dB,
where the 20 arises because when we take the square outside the logarithm, it
becomes a multiplication by two. The commonly used reference sound pres-
sure in air is pref = 20 Pa (RMS). For more information on sound pressure, see
http://en.wikipedia.org/wiki/Sound_pressure.
By the way, if you’re feeling a bit rusty, the following identities may help
to refresh your memory on logarithms. In Equations (13.3), b, x and y are
positive real numbers with b ̸= 1, c and d are any real numbers, c = logb x,
and d = logb y (or written another way, bc = x and bd = y).
logb x = c
when
bc = x
(deﬁnition);
logb 1 = 0
because
b0 = 1;
logb b = 1
because
b1 = b;
logb(x · y) = logb x + logb y
because
bc · bd = bc+d;
(13.3)
logb(x/y) = logb x −logb y
because
bc/bd = bc−d;
logb xd = d logb x
because
(bc)d = bcd.

748
13. Audio
Figure 13.4. The human ear is most sensitive in the frequency range between 2 and 5 kHz. As the
frequency decreases or increases beyond this range, more and more acoustic pressure is required
to produce the same perception of “loudness.”
13.1.2.1
Equal-Loudness Contours
The human ear does not have the same response to sound waves of different
frequencies. The human ear is most sensitive in the frequency range between
2 and 5 kHz. As the frequency decreases or increases beyond this range, more
and more acoustic intensity (i.e., pressure) is required to produce the same
perception of “loudness.”
Figure 13.4 shows a number of equal-loudness countours, each correspond-
ing to a different perceived loudness level. These curves show that more pres-
sure is required at low and high frequencies to achieve the same perceived
loudness than is needed at mid-range frequencies. Or put another way, if
we were to keep the amplitude of an acoustic pressure wave the same while
varying the frequency, the human ear would actually perceive the lower and
higher frequencies as “less loud” than the mid-range frequencies. The lowest
equal-loudness contour represents the quietest audible tone and is also known
as the absolute threshold of hearing. The highest contour is the threshold of
pain.
13.1.2.2
The Audible Frequency Band
A typical adult can hear sounds with frequencies as low as 20 Hz and as high
as 20,000 Hz (20 kHz) (although the upper limit generally decreases with age).
The equal-loudness contours help to explain why the human ear can perceive
sounds only within this limited “band” of frequencies. As the frequency be-

13.1. The Physics of Sound
749
comes lower or higher, more and more acoustic pressure is required to pro-
duce the same perceived loudness. As the frequency approaches the lower or
upper limits of human hearing, the countours become asympotically vertical,
meaning we’d need an effectively inﬁnite acoustic pressure to produce any
perception of loudness at all. Or put another way, human audio perception
drops off to effectively zero outside the audible frequency band.
13.1.3
Sound Wave Propagation
Like any kind of wave, an acoustic pressure wave propagates through space
and can be absorbed or reﬂected by surfaces, diffracted around corners and
through narrow “slits,” and refracted as it passes across the boundary between
different transmission media. Sound waves exhibit no polarization1 because
the acoustic pressure oscillation occurs in the direction of wave travel (this is
known as a longitudinal wave), rather than perpendicular to it as with light
waves (a transverse wave). In games, we typically model the absorption, re-
ﬂection and sometimes the diffraction (e.g., bending slightly around corners)
of our virtual sound waves, but we generally ignore refraction effects because
these effects are not easily noticed by a human listener.
13.1.3.1
Fall-Off with Distance
In an open space with otherwise perfectly still air, and assuming a sound
source that radiates equally in all directions, the intensity of the sound pres-
sure wave it produces falls off with distance, following a 1/r2 law, while pres-
sure follows a 1/r law.
p(r) ∝1
r ;
I(r) ∝1
r2 .
Here, r measures the radial distance of the listener or microphone from the
sound source, and both pressure and intensity are expressed as functions of r.
More precisely, the sound pressure level for a spherically radiating (omni-
directional) sound wave in open space can be written as follows:
Lp(r) = Lp(0) + 10 log10

1
4πr2

dB
= Lp(0) −10 log10
 4πr2
dB.
where Lp(r) is the SPL at the listener as a function of its radial distance from
the sound source, and Lp(0) represents the unattenuated or “natural” sound
intensity of the source.
1Sound waves in solids can be transverse and therefore can exhibit polarization.

750
13. Audio
Figure 13.5. Three types of sound sources and their sound radiation patterns (in two dimensions
for ease of illustration). From left to right: omnidirectional, conical and directional.
Sound sources are not always omnidirectional. For example, when a large,
ﬂat wall reﬂects sound waves, it acts like a purely directional sound source—
the reﬂected waves propagate in a single direction, and the pressure wave-
fronts are essentially parallel.
A bullhorn projects sound in a particular direction but with a conical fall-
off, meaning that the intensity of the sound waves is maximum along the cen-
terline of the projection “cone,” but falls off as the angle between the listener
and this centerline increases.
Various sound radiation patterns are illustrated in Figure 13.5.
13.1.3.2
Atmospheric Absorption
The 1/r fall-off of sound pressure with distance arises because energy is dis-
sipated as the waveform expands geometrically. This fall-off affects sounds
of all frequencies equally. Sound intensity also falls off with distance due to
energy absorption by the atmosphere. Atmospheric absorption effects are not
uniform across the entire frequency spectrum. In general, the absorption ef-
fect becomes greater as the frequency of the sound increases.
I’m reminded of a story I heard when I was in high school: A woman was
walking down a quiet village street at night. She heard a sporadic sequence of
low tones with long, silent gaps between them. Curious what might be mak-
ing these strange tones, she walked toward them. As she walked, the tones
became louder and the spaces between the tones seemed to get shorter. After
a few minutes’ walk, the tones had resolved into a beautiful piece of music.
The woman arrived at an open window to discover a viola player practicing
within. The musician stopped playing to say “hello,” and the woman asked
him why he had been playing random notes a few minutes before. He replied,
“I haven’t been playing random notes—I’ve been playing this piece the whole
time.” The explanation for what the woman heard, of course, is that lower-

13.1. The Physics of Sound
751
frequency sounds can be heard over longer distances than higher-frequency
sounds due to atmospheric absorption.
You can learn more about sound
wave propagation at http://www.sfu.ca/sonic-studio/handbook/Sound_
Propagation.html.
Other factors also affect the intensity of sound waves as they propagate
through their medium. In general, fall-off depends on distance, frequency,
temperature and humidity. See http://sengpielaudio.com/calculator-air.htm
for an online calculator that lets you experiment with the effects of these
factors.
13.1.3.3
Phase Shift and Interference
When multiple sound waves overlap in space, their amplitudes add together—
this is called superposition. Consider two periodic sound waves with the same
frequency. (The simplest example would be two sinusoids.) If the waves are
in phase—that is, their peaks and troughs line up—then the waves will posi-
tively reinforce each other, and the result is a wave with larger amplitude than
either of the original waves. Likewise, if the waves are out of phase, the peaks
of one wave can tend to cancel the troughs of the other and vice versa, and the
result is a wave with lower (or even zero) amplitude.
When multiple waves interact, we call this interference. Constructive inter-
ference describes the case in which the waves reinforce one another and the
amplitude increases. Destructive interference occurs when the waves cancel
each other out, resulting in lower amplitude.
The frequency of the waves has an important effect on this phenomenon:
If the frequencies of the two waves match closely, the interference simply in-
creases or decreases the overall amplitude. If the frequencies differ signiﬁ-
cantly, we can get an effect called beating, wherein the frequency difference
causes alternating periods of the waves being in and out of phase, resulting in
alternating periods of higher and lower amplitude.
Interference can occur between two totally unrelated sound signals, or it
can occur if a single sound signal takes multiple paths from the source to the
listener. In the latter case, the difference in path lengths introduces a phase
shift that can cause either constructive or destructive interference, depending
on the amount of the phase shift.
Comb Filtering
Interference can lead to an effect known as comb ﬁltering. This is caused when
sound waves reﬂect off surfaces in such a way as to either almost completely
cancel or completely reinforce certain frequencies. The result is a frequency

752
13. Audio
response (see Section 13.2.5.7) with lots of narrow peaks and troughs, which
when plotted look a bit like a comb (hence the name). This effect can have a
big impact on audio reproduction and recording—sometimes it is an undesir-
able artifact, and sometimes it is used as a tool. The existence of comb ﬁltering
is also one of the key reasons why it is generally better to spend money on
acoustic room treatment than to spend money on high-end audio equipment:
If the room exhibits comb effects, you’re wasting your time trying to get a ﬂat
response from your gear. See http://www.realtraps.com/video_comb.htm for
a great video by Ethan Winer on the subject.
13.1.3.4
Reverb and Echo
In any environment containing sound-reﬂective surfaces, a listener generally
receives three kinds of sound waves from a sound source:
•
Direct (dry). Sound waves that arrive at the listener via a direct, un-
obstructed path from the source are collectively known as direct or dry
sound.
•
Early reﬂections (echo). Sound waves that arrive at the listener via an
indirect path, after being reﬂected from and partially absorbed by sur-
rounding surfaces, take a longer time to reach the listener because their
path is longer. As such, there will be a delay between the arrival of the di-
rect sound waves and the arrival of the reﬂected waves. The ﬁrst group
of reﬂected sound waves arriving at the ear have only interacted with
one or two surfaces. As such, they are relatively “clean” signals, and we
perceive them as distinct new “copies” of the sound or echos.
•
Late reverberations (tail). Once the sound waves have bounced around the
listening space more than a few times, they superimpose and interfere
with one another so much that the brain can no longer detect distinct
echos. These are known as late reverberations or the diffuse tail. The prop-
erties of the reﬂective surfaces cause the amplitudes of the waves to be
attenuated by varying amounts. And because the reﬂected sound waves
are delayed, phase shifts occur causing the waves to interfere with one
another. This causes certain frequencies to be attenuated relative to the
others. When we speak of the acoustics of a space, we are largely speak-
ing about the effects of late reverberations on the perceived “quality” or
“timbre” of the sound.
Collectively, the echos and the tail combine with the dry sound to create what
is known as wet sound. Figure 13.6 illustrates the wet and dry components of
a single abrupt clap.

13.1. The Physics of Sound
753
t
p(t)
Dry
Wet
Early
Reflections
Late Reverberations
(Diffuse Tail)
1600 ms
100 ms
50 ms
0 ms
Figure 13.6. Direct sound waves, early reﬂections and late reverberations.
The early reﬂections and late reverberations provide the brain with a wealth
of cues that tell us quite a lot about the type of space we’re in. The pre-delay is
the time interval between the arrival of the direct sound waves and the arrival
of the very ﬁrst reﬂected waves. From the pre-delay, the brain can determine
the approximate size of the room or space in which we are listening. The decay
is the time it takes for the reﬂected sound waves to die away. This tells our
brains how much of the sound has been absorbed by the surroundings, and
so indirectly tells us something about the materials that make up the space
we’re in. For example, a small tiled bathroom would produce late reverbera-
tions with a very short pre-delay (due to its small size) and a long decay (due
to the tile’s ability to reﬂect sound waves efﬁciently, with little absorption). A
large granite-walled room like Grand Central Terminal (a.k.a. Grand Central
Station) in New York City will have a much longer pre-delay and a lot more
echos, but the decay will be similar to that of the tiled bathroom.
If we were to hang curtains in that bathroom, or if the walls were covered
with wood panels instead of tile, the pre-delay would remain the same, but
the decay, along with other factors such as density (how closely spaced in time
the individual reﬂections are) and diffusion (the rate at which the reﬂections
increase in density over time), would change. This explains how a person
can guess where they are even when blindfolded, or how the blind can learn
to navigate with only a cane to aid them. Sound provides us with a lot of
information about our surroundings!
The term reverb is used to describe the quality of a sound in terms of
its wet components. In the early days of audio recording, sound engineers
had little control over reverb, relying entirely on the shape and construction
of the room in which the recording was made.
Later, simple artiﬁcial re-

754
13. Audio
verb devices were created, from the use of a speaker and microphone in a
bathroom by Bill Putman Sr. (founder of Universal Audio), to the use of a
long metal plate or spring to introduce a delay in a sound signal, to modern
digital techniques. Today, digital signal processor (DSP) chips and/or soft-
ware are used not only to recreate natural reverb effects in recorded sound
effects and music, but also to augment recordings with all sorts of interest-
ing effects that are not normally heard in nature.
We’ll learn more about
digital signal processing in Section 13.2. You can read more about reverb at
http://www.uaudio.com/blog/the-basics-of-reverb.
An anechoic chamber is a room especially designed to entirely eliminate
reﬂected sound waves. This is accomplished by lining the walls, ﬂoor and
ceiling of the room with thick corrugated foam padding that absorbs essen-
tially all of the reﬂected sound waves. As a result, only the direct (dry) sound
reaches the listener or microphone. Sound in an anechoic chamber has a com-
pletely “dead” timbre. Anechoic chambers are useful for recording “pure”
sounds that contain no reverb. Such pure sounds are often perfect candidates
for input into a digital signal processing pipeline, giving a sound designer
maximum ﬂexibility to control the timbre of the sound.
13.1.3.5
Sound in Motion: The Doppler Effect
If you’ve ever stood at a railway crossing when a train goes by, you’ve heard
the Doppler effect in action. The sound of the train seems higher pitched when
it is approaching you, and becomes lower pitched as it races off into the dis-
tance. Sound waves travel at a roughly constant speed through the air, but the
sound source (in this case, the train) is also moving. The sound waves that are
moving in the same direction as the train become “squashed together,” and
the waves that are moving opposite to the motion of the train become “spread
out,” each by an amount proportional to the difference between the speed of
sound in air and the speed of the train through the air. The frequency of the
squashed waves is therefore increased, because the space between the peaks
and troughs of the sound waves has been effectively reduced, resulting in a
higher-pitched sound. Likewise, the frequency of the spread-out waves is de-
creased, resulting in a lower-pitched sound. The Doppler effect was named
after the Austrian physicist Christian Doppler, who identiﬁed it in 1842.
The Doppler effect also occurs when the listener is moving but the sound
source is stationary. In general, the Doppler shift is dependent upon the rel-
ative velocity (as a vector) between the listener and the sound source. In one
dimension, the Doppler shift amounts to a change in frequency, and can be

13.1. The Physics of Sound
755
quantiﬁed as follows:
f ′ =
 c + vl
c + vs

f,
where f is the original frequency, f ′ is the Doppler-shifted (observed) fre-
quency at the listener, c is the speed of sound in air and vl and vs are the
speeds of the listener and sound source, respectively. If the speeds of the
sound sources are very small relative to the speed of sound, we can approxi-
mate this relationship as follows:
f ′ =
1 + (vl −vs)
c

f
=
1 + ∆v
c

f.
This expression makes the relative velocity, ∆v, apparent. The Doppler effect
can be easily visualized by looking at the following animated GIF: http://en.
wikipedia.org/wiki/File:Dopplereffectsourcemovingrightatmach0.7.gif.
13.1.4
Perception of Position
The human auditory system has evolved to allow a reasonably accurate per-
ception of the position of sounds in the space around us. A number of factors
contribute to our perception of sound position:
•
Fall-off with distance provides us with a rough idea of how far away the
source of a sound is. In order for this to work, we must have some idea
of the loudness of the sound when heard at close range to serve as a
“baseline.”
•
Atmospheric absorption causes the higher frequencies in a sound to drop
out as the source moves farther away from the listener. This can serve
as an important cue in perceiving the difference between, for example, a
person speaking at normal volume but far away and a person speaking
at reduced volume up close.
•
Having two ears, one on the left and one on the right, gives us a great
deal of positional information. A sound that is to our right will sound
louder in the right ear than in the left. An interaural time difference (ITD)
of approximately one millisecond also arises, because a sound to one
side of your head will take just a little bit longer to reach the opposite ear.
Finally, the head itself obstructs sounds, so the ear opposite to the sound
source will perceive a slightly mufﬂed version of the sound reaching the
near ear. This is known as interaural intensity difference (IID).

756
13. Audio
•
Ear shape has an effect as well. Our ears are cupped slightly forward,
so sounds coming from behind us are very slightly mufﬂed relative to
those coming from in front of us.
•
The head-related transfer function (HRTF) is a mathematical model of the
minute effects that the folds of our ears (the pinnae) have on sounds com-
ing from different directions.
13.2
The Mathematics of Sound
Signal processing and systems theory is the area of mathematics that lies at
the heart of virtually all modern audio technology. It is also extensively used
in a wide variety of other technological and engineering endeavors, including
image processing and machine vision, aeronautics, electronics, ﬂuid dynam-
ics, and the list goes on. In this section, we’ll embark on a whirlwind tour of
the key concepts in signals and systems theory, because it will help us to un-
derstand some of the more advanced topics in game audio later in the chapter.
(It’s also an important area of mathematical theory that can beneﬁt any game
programmer—so what the heck!) An in-depth treatment of the topic can be
obtained from [36].
13.2.1
Signals
A signal is any function of one or more independent variables, typically de-
scribing the behavior of some kind of physical phenomenon. In Section 13.1,
we used the signal p(t) to represent the time-varying acoustic pressure of an
audio compression wave. Of course, many other kinds of signals are possi-
ble. A signal v(t) might represent the voltage produced by a microphone over
time, while w(t) might model the time-varying water pressure in a system of
pipes, or we could use f(t) to represent the varying population of foxes in an
ecosystem.
In studying signal theory, we often refer to the independent variable as
“time” and represent it with the symbol t—but of course the independent
variable might represent some other quantity, and there may be more than
one independent variable. For example, one can think of a 2D greyscale im-
age as a signal i(x, y), where the two independent variables, x and y, represent
the orthogonal coordinate axes, and the signal value i represents the intensity
of the greyscale image at each pixel. A color image could be similarly rep-
resented by three signals, r(x, y) for the red channel, g(x, y) for the green
channel and b(x, y) for the blue channel.

13.2. The Mathematics of Sound
757
13.2.1.1
Be Discrete, Continuously
The 2D image examples above bring to light an important distinction between
two fundamental kinds of signal: continuous and discrete.
•
If the independent variable is a real number (t ∈R), we call the signal a
continuous-time signal. In this chapter, we’ll use the symbol t to represent
continuous “time,” and we’ll use round parentheses for our functional
notation (e.g., x(t)) to remind us that we’re working with a continuous-
time signal.
•
If the independent variable(s) is an integer (n ∈I), we call the signal a
discrete-time signal. We’ll use the symbol n to represent discrete “time,”
and we’ll use square brackets for our functional notation (e.g., x[n]) to
remind us that we’re working with a discrete-time signal. Note that the
value of a discrete-time signal might still be a real number (x[n] ∈R)—
the only thing the term “discrete-time signal” prescribes is that the inde-
pendent variable is an integer (n ∈I).
In Figure 13.1, we saw that we can visualize a continuous-time signal as an
ordinary function plot, with time t on the horizontal axis and the signal value
p(t) on the vertical axis. We can plot a discrete-time signal x[n] in similar
fashion, although the function’s values are only deﬁned for integer values of
the independent variable n (see Figure 13.7). One common way to think of a
discrete-time signal is as a sampled version of a continuous-time signal. The
sampling process (also known as digitization or analog-to-digital conversion) lies
at the heart of digital audio recording and playback. See Section 13.3.2.1 for
more information on sampling.
x[n]
n
Figure 13.7. The value of a discrete-time signal x[n] is deﬁned only for integer values of n.

758
13. Audio
x(t)
t
x(–t)
t
x(t – s)
t
x(2t)
t
s
Figure 13.8. Simple manipulations of a signal’s independent variable.
13.2.2
Manipulating Signals
It will be important in the following discussions for us to understand some
basic ways to manipulate a signal by making changes to its independent vari-
able. For example, to reﬂect a signal about t = 0, we simply replace t with −t
in the signal’s equation. To time-shift the entire signal to the right (i.e., in the
positive direction) by a distance s, we replace t with t −s in the signal’s equa-
tion. (Time shifting to the left/negative direction is accomplished by replacing
t with t+s.) We can also expand or compress the domain of the signal by scal-
ing the independent variable. These simple transformations are illustrated in
Figure 13.8.
13.2.3
Linear Time-Invariant (LTI) Systems
In the context of signal processing theory, a system is deﬁned as any device or
process that transforms an input signal into a new output signal. The math-
ematical concept of a system can be used to describe, analyze and manipu-
late many real-world systems that arise in audio processing, including micro-
phones, speakers, analog-to-digital converters, reverb units, equalizers and
ﬁlters and even the acoustics of a room.
As a simple example, an ampliﬁer is a system that increases the amplitude
of its input signal by a factor A known as the gain of the amp. Given an in-
put signal x(t), such an ampliﬁcation system would produce an output signal
y(t) = Ax(t).

13.2. The Mathematics of Sound
759
A time-invariant system is one for which a time shift in the input signal
causes an equal time shift in the output signal. In other words, the behavior
of the system does not change over time.
A linear system is one that possesses the property of superposition. This
means that if an input signal consists of a weighted sum of other signals, then
the output is a weighted sum of the individual outputs that would have been
produced, had each of the other signals been fed through the system indepen-
dently.
Linear time-invariant (LTI) systems are extremely useful for two reasons.
First, their behaviors are well understood and relatively easy to work with
mathematically. Second, many real physical systems in the ﬁelds of audio
propagation theory, electronics, mechanics, ﬂuid ﬂow, etc. can be modeled ac-
curately using LTI systems. As such, we will restrict ourselves to a discussion
of LTI systems for our purposes of understanding audio technology.
Figure 13.9. A system as
a black box.
We can visualize any system as a black box with an input signal and an
output signal, as shown in Figure 13.9.
Using this black-box notation, simple systems can be conveniently inter-
connected to construct more-complex systems. For example:
•
The output of system A could be connected to the input of system B,
yielding a composite system that performs operation A followed by op-
eration B. This is called a serial connection.
•
The outputs of two systems could be added together.
•
The output of a system could be fed back into an earlier input, yielding
what is known as a feedback loop.
See Figure 13.10 for examples of all of these kinds of connections.
Serial
A
x(t)
B
y(t)
Feedback Loop
y(t)
x(t)
+
–a
d
dt
y(t)
Parallel
y(t)
A
B
+
x(t)
a
b
Figure 13.10. Various ways to interconnect systems. In the serial connection, y(t) = B(A(x(t))).
In the parallel connection, y(t) = aA(x(t)) −bB(x(t)). In the feedback loop, y(t) = x(t) −a ˙y(t).

760
13. Audio
One very important property of all LTI systems is that their interconnec-
tions are order-independent. So if we have a serial connection of system A fol-
lowed by system B, we can reverse the order of the two systems and the output
will remain unchanged.
13.2.4
Impulse Response of an LTI System
It’s all ﬁne and dandy to talk about systems that convert an input signal into
an output signal, and it’s even pretty intuitive to draw diagrams of system
interconnections. But how can we describe the operation of a system mathe-
matically?
Recall from Section 13.2.3 that for a linear system, if the input consists of
a linear combination (weighted sum) of input signals, the output will be a
linear combination (weighted sum) of the individual outputs (had each of the
input signals been fed into the system independently). So, if we can ﬁgure
out a way to represent an arbitrary input signal as a weighted sum of very
simple signals, we should be able to describe the behavior of the system by
describing only its response to those very simple signals.
13.2.4.1
The Unit Impulse
If we are going to describe an input signal as a linear combination of simple
signals, the question arises: Which simple signal shall we use? For reasons
that will become clear in a moment, our signal of choice is going to be the unit
impulse. This signal is one of a family of related functions known as singularity
functions because they all contain at least one discontinuity or “singularity.”
In discrete time, the unit impulse δ[n] is as simple as it gets: It is a signal
whose value is zero everywhere except at n = 0, where its value is one:
δ[n] =
 1
if n = 0,
0
otherwise.
The discrete-time unit impulse is illustrated in Figure 13.11.
Figure 13.11. The unit impulse in discrete time.

13.2. The Mathematics of Sound
761
b t
t
T
1/T
t
b t
T
Figure 13.12. The unit impulse can be deﬁned as the limit of
a box function b(t) whose width approaches zero.
Figure 13.13. The value of the unit impulse function δ(t) is
zero everywhere except at t = 0, where it is inﬁnite. It is
drawn as an arrow of unit height to indicate that the area
under the curve is 1.
In continuous time, the unit impulse δ(t) is a bit trickier to deﬁne. It is a
function whose value is zero everywhere except at t = 0, where its value is
inﬁnite—but the area under the curve is equal to one.
To see how such a strange beast of a function might be formally deﬁned,
imagine a “box” function b(t), whose value is zero everwhere except in the
interval [0, T), where its value is 1/T. The area under this curve is just the
area of the box, width times height, or T × 1
T = 1. Now imagine the limit
as T →0. As this happens, the width of the box approaches zero and its
height approaches inﬁnity, but its area remains equal to 1. This is shown in
Figure 13.12.
The unit impulse function is typically denoted by the symbol δ(t). It can
be formally deﬁned as follows:
δ(t) = lim
T →0 b(t),
where
b(t) =
 1/T
if t ≥0 and t < T,
0
otherwise.
As shown in Figure 13.13, we typically plot the unit impulse by drawing
an arrow whose height represents the area under the curve (since the actual
“height” of the function at t = 0 is inﬁnite).
13.2.4.2
Using an Impulse Train to Represent a Signal
Now that we know what the unit impulse signal is, let’s see if we can describe
an arbitrary signal x[n] as a linear combination of unit impulses. (Spoiler alert:
It turns out we can.)
The function δ[n−k] is a time-shifted discrete unit impulse, whose value is
zero everywhere except at time n = k, where it is equal to one. In other words,

762
13. Audio
the unit impulse δ[n−k] is “positioned” at time k. Consider an impulse at one
particular value of k (say, k = 3). Let’s make sure that the “height” of that
impulse matches the value of the original function at k = 3 by “scaling” the
impulse by x[3], yielding x[3]δ[n −3]. If we rinse and repeat for all possible
values of k, we get a train of impulses of the form x[k]δ[n −k]. Adding all
these scaled, time-shifted impulse functions together is just another way of
writing the original signal x[n]:
x[n] =
+∞
X
k=−∞
x[k]δ[n −k].
(13.4)
We won’t give a rigorous proof here, but it’s probably not too hard to be-
lieve that doing this in continuous time works in pretty much the same way.
The only difﬁculty is that for continuous time, the sum in Equation (13.4) be-
comes an integral. Let’s imagine an inﬁnite sequence of time-shifted unit im-
pulses δ(t−τ), each one located at a different time τ. We can build up an arbi-
trary signal x(t) in an analogous fashion to the discrete-time case, as follows:
x(t) =
Z +∞
τ=−∞
x(τ)δ(t −τ) dτ.
(13.5)
13.2.4.3
Convolution
Equation (13.4) tells us how to represent a signal x[n] as a linear combination
of simple, time-shifted unit impulse signals δ[n −k]. Let’s imagine putting
just one of these weighted impulse inputs (x[k]δ[n −k]) through the system.
It doesn’t matter which one we choose, so let’s select the one at k = 0. This
gives us the input signal x[0]δ[n].
We’ll use the notation x[n] =⇒y[n] to indicate that an input signal x[n]
is being transformed by an LTI system into an output signal y[n]. So we can
write:
x[0]δ[n] =⇒y[n].
The value of x[0] is just a constant, so because we’re dealing with a linear sys-
tem, the output y[n] will just be that same constant times the system’s response
to the unit impulse δ[n]. Let’s use the signal h[n] to represent the system’s re-
sponse to a “bare” unit impulse: δ[n] =⇒h[n]. The signal h[n] is called the
impulse response of the system. So we can write the system’s response to our
simple input signal as follows:
x[0]δ[n] =⇒x[0]h[n].
The concept of impulse response is illustrated in Figure 13.14.

13.2. The Mathematics of Sound
763
h[n]
n
n
[n]
h(t)
t
t
(t)
Figure 13.14. Examples of the impulse response of a system in discrete and continuous time.
The response of an LTI system to a time-shifted unit impulse is just a time-
shifted impulse response (δ[n −k] =⇒h[n −k]). So for values of k other than
zero, everything works out exactly the same except that now the input and
output signals are both time-shifted by k:
x[k]δ[n −k] =⇒x[k]h[n −k].
To ﬁnd the system’s response to the entire input signal x[n], we just sum
up the responses to each individual time-shifted component, like this:
+∞
X
k=−∞
x[k] δ[n −k] =⇒
+∞
X
k=−∞
x[k] h[n −k].
In other words, the output of our system can be written as follows:
y[n] =
+∞
X
k=−∞
x[k] h[n −k].
(13.6)
This very important equation is known as the convolution sum. It’s conve-
nient to introduce a new mathematical operator ∗to represent the operation
of convolution:
x[n] ∗h[n] =
+∞
X
k=−∞
x[k] h[n −k].
(13.7)
Equations (13.6) and (13.7) give us a way to calculate an LTI system’s re-
sponse y[n] to any arbitrary input signal x[n], given only the impulse response
of the system, h[n]. In other words, for LTI systems, the impulse response
signal h[n] completely describes the system. Pretty cool stuff.

764
13. Audio
Convolution in Continuous Time
In our discussions above, we worked in discrete time to keep things simple. In
continuous time, everything works out in pretty much the same way. The only
difference is that summations become integrals, and we need to remember to
include the differential dτ in our equations.
When we apply an arbitrary signal x(t) to the input of a continuous-time
LTI system, the output signal can be written as follows:
y(t) =
Z +∞
τ=−∞
x(τ) h(t −τ) dτ.
(13.8)
As before, we’ll use the operator ∗as a shorthand for convolution:
x(t) ∗h(t) =
Z +∞
τ=−∞
x(τ) h(t −τ) dτ.
(13.9)
Analogous to the convolution sum, the integral in Equations (13.8) and (13.9)
is known as the convolution integral.
13.2.4.4
Visualizing Convolution
Integrate to find area 
under this curve.
x( )
h(t – )
t
h( )
x( ) h(t – )
Figure 13.15. Visualization
of the convolution oper-
ation in continuous time.
Let’s try to visualize the convolution operation in the continuous time case. To
evaluate y(t) = x(t) ∗h(t) for one speciﬁc value of t (say, t = 4), we perform
the following steps as illustrated in Figure 13.15:
1.
Plot x(τ), using τ as the time variable because t is ﬁxed (at t = 4 in this
example).
2.
Plot h(t −τ). We can rewrite this as h(−τ + t). Because τ is negated,
we know that the impulse response has been ﬂipped about τ = 0. And
because we’ve added t to the independent variable, we know the signal
has been shifted to the left by t = 4 units.
3.
Multiply the two signals together across the entire τ axis.
4.
Integrate from −∞to +∞along the τ axis to ﬁnd the area under the
resulting curve. This is the value of y(t) at this one speciﬁc value of t (in
this example, t = 4).
Remember that we must repeat this procedure for every possible value of t in
order to determine the complete output signal y(t).

13.2. The Mathematics of Sound
765
13.2.4.5
Some Properties of Convolution
The properties of the convolution operation are surprisingly analogous to
those of ordinary multiplication. Convolution is:
•
commutative: x(t) ∗h(t) = h(t) ∗x(t);
•
associative: x(t) ∗
 h1(t) ∗h2(t)

=
 x(t) ∗h1(t)

∗h2(t); and
•
distributive: x(t) ∗
 h1(t) + h2(t)

=
 x(t) ∗h1(t)

+
 x(t) ∗h2(t)

.
13.2.5
The Frequency Domain and the Fourier Transform
In order to arrive at the concepts of impulse response and convolution, we
described a signal as a weighted sum of unit impulses. We can also represent
a signal as a weighted sum of sinusoids. Representing a signal in this manner
essentially breaks it up into its frequency components. This will allow us to
derive another incredibly powerful mathematical tool—the Fourier transform.
13.2.5.1
The Sinusoidal Signal
A sinusoidal signal is produced when a circular motion in two dimensions is
projected onto a single axis. An audio signal in the form of a sinusoid pro-
duces a “pure” tone at one speciﬁc frequency.
The most basic sinusoidal signal is the sine (or cosine) function. The signal
x(t) = sin t takes on the value 0 at t = 0, π and 2π, has a value of 1 at t = π
2
and has a value of −1 at t = 3π
2 .
The most-general form of a real-valued sinusoidal signal is
x(t) = A cos(ω0t + φ).
(13.10)
Here, A represents the amplitude of the sine wave (i.e., the peaks and troughs
of the cosine wave hit maximum and minimum values of A and −A, respec-
tively). The angular frequency is ω0, measured in radians/second (see Section
13.1.1 for a discussion of frequency and angular frequency). φ represents a
phase offset (also measured in radians) that shifts the cosine wave to the left or
right along the time axis.
When A = 1, ω0 = 1 and φ = 0, Equation (13.10) reduces to x(t) = cos t.
When φ =
π
2 , the expression becomes x(t) = sin t. The cos function repre-
sents the projection of a circular motion onto the horizontal axis, while sin
represents its projection onto the vertical axis.
13.2.5.2
The Complex Exponential Signal
The cosine function isn’t actually the best tool for representing a signal as a
sum of sinusoids. The math is much simpler and more elegant if we make use

766
13. Audio
of complex numbers instead. In order to understand how this works, we need
to review complex math, and take a look at how multiplication of complex
numbers works. So bear with me here—all will become clear by the time
we’re done.
A Brief Review of Complex Numbers
You’ll probably remember from high-school math class that a complex num-
ber is a kind of two-dimensional quantity consisting of a real part and an
imaginary part. Any complex number can be written as follows: c = a + jb,
where a and b are real numbers and j = √−1 is the imaginary unit. The real
part of c is a = Re(c), and its imaginary part is b = Im(c).
You can visualize a complex number as a kind of “vector” [a, b] in a two-
dimensional space known as the Argand plane. It’s important to remember,
however, that complex numbers and vectors are not interchangeable—their
mathematical behaviors are quite different.
We deﬁne the magnitude of a complex number as the length of its 2D “vec-
tor” representation in the complex plane: |c| =
√
a2 + b2. The angle the vector
makes with the real axis is known as its argument: arg c = tan−1(b/a). (The
argument of a complex number is sometimes called its phase. As we’ll see, the
term “phase” is closely related to the phase offset φ in Equation (13.10).) The
magnitude and argument of a complex number are depicted in Figure 13.16.
Figure 13.16. The magnitude |c| =
√
a2 + b2 of a complex number is its length in the complex
plane, and its argument arg c = tan−1(b/a) is the angle it makes with the Re axis.
Complex Multiplication and Rotation
We won’t get into all of the properties of complex numbers here. Check out
http://www.math.wisc.edu/∼angenent/Free-Lecture-Notes/freecomplexnu
mbers.pdf for an in-depth discussion of complex number theory. However,
there is one mathematical operation that does concern us here: the operation
of complex multiplication.

13.2. The Mathematics of Sound
767
Complex numbers are multiplied algebraically (no dot or cross products
here):
c1c2 = (a1 + jb1)(a2 + jb2)
= (a1a2) + j(a1b2 + a2b1) + j2b1b2
= (a1a2 −b1b2) + j(a1b2 + a2b1).
(13.11)
If you work out2 the magnitude and argument (angle) of the product c1c2,
you’ll ﬁnd that the magnitude is equal to the product of the two input magni-
tudes, and the argument is the sum of the input arguments:
|c1c2| = |c1||c2|;
arg(c1c2) = arg c1 + arg c2.
(13.12)
The fact that multiplication of complex numbers causes their angles (ar-
guments) to add means that complex multiplcation produces a rotation in the
complex plane. If the magnitude of c1 is unity (|c1| = 1), then the magnitude
of the product will be equal to the magnitude of c2 (|c1c2| = |c2|). In this case,
the product represents a pure rotation of c2 by an angle equal to arg c1 (see Fig-
ure 13.17). If |c1| ̸= 1, then the product’s magnitude will be scaled by |c1|, and
the result is that c2 undergoes a spiral motion in the complex plane.
Im
Re
arg c = 30
1
arg c = 80
2
c1
c2
+1
–1
+j
–j
c c
1   2
arg c c = 110
= 30  + 80
1   2
|c | = |c | = 1
1                2
Figure 13.17. Multiplying two complex numbers together that both have magnitudes of 1 produces
a pure rotation in the complex plane.
2Yikes—this sounds an awful lot like an exercise for the reader...

768
13. Audio
0
2
1
3
Figure 13.18. Multiplying the imaginary number j = √−1 by itself acts like rotating a unit vector
by 90 degrees in the complex plane.
This explains why unit-length quaternions operate as rotations in 3D space!
A quaternion is essentially a four-dimensional complex number, with one real
part and three imaginary parts. So a quaternion follows the same basic rules in
three dimensions that a regular complex number follows in two dimensions.
The fact that complex multiplication produces a rotation makes sense when
we consider what happens when we multiply j by itself many times:
1 × j = j,
j × j =
√
−1
√
−1 = −1,
−1 × j = −j,
−j × j = 1,
. . .
So multiplying j by itself is like rotating a unit vector by 90 degrees in the
complex plane. In fact, multiplying any complex number by j has the effect of
rotating it by 90 degrees. This is illustrated in Figure 13.18.
The Complex Exponential and Euler’s Formula
For any complex number c with |c| = 1, the function f(n) = cn, with n tak-
ing on a sequence of increasing positive real values, will trace out a circular
path in the complex plane. Any circular path in two dimensions traces out a
sine curve along the vertical axis, and a corresponding cosine curve along the
horizontal axis. This is illustrated in Figure 13.19.
Raising a complex number to a real power (cn) produces rotation in the
complex plane, and therefore yields a sinusoid when projected onto any axis
in the plane. As it turns out, we can also get this rotational effect by raising a

13.2. The Mathematics of Sound
769
Im
Re
+1
–1
+j
–j
|c| = 1
c5
c4
c3
c
n
Im(c ) = sin(n arg c)
n
c2
Figure 13.19. Multiplying a complex number by itself repeatedly traces out a circular path in the
Argand plane, producing a sinusoid when projected onto any axis through the origin.
real number to an complex power (nc). This means that we can write Equation
(13.10) in terms of complex numbers as follows:
ejω0t = cos ω0t + j sin ω0t, t ∈R;
(13.13)
Re

ejω0t
= cos ω0t;
Im

ejω0t
= sin ω0t,
where e ≈2.71828 is the real transcendental number that deﬁnes the base of
the natural logarithm function.
Equation (13.13) is one of the most important equations in all of mathe-
matics. It is known as Euler’s formula. Why it works is a bit of a mystery (even
to some seasoned mathematicians). The theorem can be explained by looking
at the Taylor series expansion of ejt, or by considering the derivative of ex and
then allowing x to become a complex number. But for our purposes, it should
sufﬁce to rely on the intuitions we gained from looking at how complex mul-
tiplication results in rotation in the complex plane.
13.2.5.3
The Fourier Series
Now that we have the mathematical tools we need to represent sinusoids as
complex numbers, let’s turn our attention again to the task of representing a
signal as a sum of sinusoids.

770
13. Audio
Doing this is easiest when the signal is periodic. In this case, we can write
the signal as a sum of harmonically related sinusoids:
x(t) =
+∞
X
k=−∞
akej(kω0)t.
(13.14)
We call this the Fourier series representation of the signal. Here, the complex
exponential functions ej(kω0)t are the sinusoidal components from which we
are building up the signal. These components are harmonically related, in that
each one has a frequency that is an integer multiple k of the so-called funda-
mental frequency ω0. The coefﬁcients ak represent the “amount” of each har-
monic present in the signal x(t).
13.2.5.4
The Fourier Transform
A full explanation of this topic is beyond the scope of this book, but for our
purposes it will sufﬁce to state (without any proof whatsoever!) that any rea-
sonably well-behaved signal,3 even signals that are non-periodic, can be rep-
resented as a linear combination of sinusoids. In general, an arbitrary sig-
nal may contain components at any frequency, not just frequencies that are
harmonically related. As such, the discrete set of harmonic coefﬁcients ak in
Equation (13.14) becomes a continuum of values representing “how much” of
each frequency the signal contains.
We can envision a new function X(ω) whose independent variable is the
frequency ω rather than time t, and whose value represents the amount of each
frequency present in the original signal x(t). We say that x(t) is the time domain
representation of the signal, while X(ω) is its frequency domain representation.
Mathematically, we can ﬁnd the frequency domain representation of a sig-
nal from its time domain representation, and vice versa, by using the Fourier
transform:
X(ω) =
Z +∞
−∞
x(t)e−jωtdt;
(13.15)
x(t) = 1
2π
Z +∞
−∞
X(ω)ejωtdω.
(13.16)
If you compare Equation (13.16) to the Fourier series from Equation (13.14),
you can see the similarity. Rather than describing the “amounts” of the fre-
quency components via a discrete series of coefﬁcients ak, we’re now describ-
ing them using the continuous function X(ω). But in both cases we’re repre-
senting x(t) as a “sum” of sinusoids.
3All signals that meet the so-called Dirichlet conditions have Fourier transforms and are there-
fore “reasonably well-behaved” for our purposes.

13.2. The Mathematics of Sound
771
x(t) = e –at
|X( )| = 
a2 + 
2
1
t
arg X( ) = tan–1
a
Frequency Domain 
(Bode Plot)
Time Domain
Figure 13.20. The Fourier transform yields a complex-valued frequency domain signal. A Bode
plot is used to visualize this complex-valued signal in terms of its magnitude and its phase (or
argument).
13.2.5.5
Bode Plots
In general the Fourier transform of a real-valued signal is a complex-valued sig-
nal (X(ω) ∈C). When visualizing the Fourier transform, we often draw it
using two plots. For example, we might plot its real and imaginary compo-
nents. Or we might plot its magnitude and its argument (angle) on two dif-
ferent plots—a visualization known as a Bode plot (pronounced “Boh-dee”).
Figure 13.20 shows an example of a signal and its Bode plot.
13.2.5.6
The Fast Fourier Transform (FFT)
A collection of fast algorithms exist for calculating the Fourier transform in
discrete time. This family of algorithms is called, aptly enough, the fast Fourier
transform or FFT. You can read more about the FFT at http://en.wikipedia.org/
wiki/Fast_Fourier_transform.
13.2.5.7
Fourier Transforms and Convolution
It is interesting to note that convolution in the time domain corresponds to
multiplication in the frequency domain and vice versa. Given a system whose
impulse response is h(t), we know that we can ﬁnd the output of the system

772
13. Audio
y(t) in response to an input x(t) as follows:
y(t) = x(t) ∗h(t).
In the frequency domain, given the Fourier transforms of the impulse re-
sponse H(ω) and the input X(ω), we can ﬁnd the Fourier transform of the
output as follows:
Y (ω) = X(ω)H(ω).
This result is pretty incredible, and it’s also very handy. Sometimes it is more
convenient to perform a convolution on the time axis using a system’s im-
pulse response h(t), while at other times it’s more convenient to perform a
multiplication in the frequency domain using the system’s frequency response
H(ω).
As it turns out, LTI systems exhibit a property called duality, which says
that you can reverse the roles of time and frequency and virtually the same
mathematical rules continue to apply. So, for example, we can understand
how signal modulation (the multiplication of one signal by another) works in
the time domain by looking at what happens when we convolve the Fourier
transforms of the two signals on the frequency axis. Having two ways of
tackling a problem is always better than one!
13.2.5.8
Filtering
The Fourier transform allows us to visualize the set of frequencies that make
up virtually any audio signal. A ﬁlter is an LTI system that attenuates a se-
lected range of input frequencies while leaving all other frequencies unaltered.
A low-pass ﬁlter retains low frequencies while attenuating high frequencies. A
high-pass ﬁlter does the opposite, retaining high frequencies and attenuating
lower frequencies. A band-pass ﬁlter attenuates both low and high frequencies
but retains frequencies within a limited passband. A notch ﬁlter does the oppo-
site, retaining low and high frequencies but attenuating frequencies within a
limited stopband.
Filters are used in a stereo system’s equalizer, by attenuating or boosting
speciﬁc frequencies based on user inputs. Filters can also be used to atten-
uate noise, if the spectra of the noise signal and the desirable signal occupy
different regions of the frequency axis. For example, if a high-frequency noise
signal is adversely affecting a lower-frequency voice or music signal, a low-
pass ﬁlter could be used to eliminate the noise.
The frequency response H(ω) of an ideal ﬁlter looks like a rectangular box,
with a value of one in the passband and zero in the stopband. When we

13.2. The Mathematics of Sound
773
c
c
Figure 13.21. The frequency response H(ω) for an ideal ﬁlter has a value of one in the passband
and zero in the stopband.
multiply this by the Fourier transform of our input signal X(ω), the output
Y (ω) = X(ω)H(ω) will have its passband frequencies preserved exactly, and
its stopband frequencies all set to zero. The frequency response for an ideal
ﬁlter is shown in Figure 13.21.
Of course, an ideal ﬁlter that completely passes certain frequencies and
completely suppresses others may not be desirable. The frequency responses
of most real-world ﬁlters have a gradual fall-off between the passband and
stopband. This aids ﬁltering in situations where there is no single, clear-cut
line between the desirable frequencies and the unwanted frequencies. The
frequency response of a low-pass ﬁlter with a gradual fall-off is shown in Fig-
ure 13.22.
arg H( )
20 log  |H( )|
10
0 dB
–20 dB
–40 dB
0
– /4
– /2
0.1/RC
1/RC
10/RC
100/RC
0.1/RC
1/RC
10/RC
100/RC
Figure 13.22. The frequency response H(ω) for an RC (resistor-capacitor) low-pass ﬁlter with a
gradual fall-off. Both the horizontal and vertical axes of both plots are drawn using a logarithmic
scale.

774
13. Audio
The equalizer (EQ) found on most high-ﬁdelity audio equipment permits
the user to adjust the amount of bass, mid-range and treble that is output. An
EQ is really just a collection of ﬁlters tuned to different frequency ranges and
applied in series to an audio signal.
Filtering theory is an immense ﬁeld of study, so we can’t possibly do it
justice here. For a great deal more information, see [36, Chapter 6].
13.3
The Technology of Sound
Before we can fully understand the software that comprises a game’s audio
engine, we need a ﬁrm grasp of audio hardware and technology and of the
terminology used by industry professionals to describe it.
13.3.1
Analog Audio Technology
The earliest audio hardware was based on analog electronics. This was the
easiest way to record, manipulate and play back audio compression waves,
because sound is itself an analog physical phenomenon. In this section, we’ll
brieﬂy explore some key analog audio technologies.
13.3.1.1
Microphones
A microphone (also known as a “mic” or “mike”) is a transducer that converts
an audio compression wave into an electronic signal. Microphones make use
of various technologies in order to convert the mechanical pressure variations
of a sound wave into an equivalent signal based on variations in electric volt-
age. A dynamic microphone uses electromagnetic induction, while a condenser
microphone utilizes changes in capacitance. Other types of mics use piezoelec-
tric generation or light modulation to produce a voltage signal.
Different microphones have different sensitivity patterns, known as polar
patterns. These patterns describe how sensitive the mic is to sound at various
angles about its central axis. An omnidirectional mic is equally sensitive in all
directions. A bidirectional mic has two sensitivity “lobes” in the shape of a
ﬁgure eight. A cardioid mic has essentially a unidirectional sensitivity proﬁle,
so named because of its somewhat heart-shaped polar pattern. Some common
microphone polar patterns are illustrated in Figure 13.23.
13.3.1.2
Speakers
A speaker is basically a microphone operated in reverse—it is a transducer that
converts a varying input voltage signal into vibrations in a membrane, which

13.3. The Technology of Sound
775
270
90
0
180
–25 dB
–20 dB
–15 dB
–10 dB
–5 dB
270
90
0
180
–25 dB
–20 dB
–15 dB
–10 dB
–5 dB
270
90
0
180
–25 dB
–20 dB
–15 dB
–10 dB
–5 dB
Figure 13.23. Three typical microphone polar patterns, clockwise from upper left: omnidirectional, cardioid and bidirectional.
in turn gives rise to air pressure variations that result in a sound pressure
wave.
13.3.1.3
Speaker Layouts: Stereo
Sound systems usually support multiple speaker output channels. A stereo
device such as an iPod, the sound system in your car or your grandpa’s portable
“boom box” supports at least two speakers for the left and right stereo chan-
nels. Some high-ﬁdelity stereo systems also boast two additional “tweeters”—
tiny speakers that are capable of reproducing the highest-frequency sounds

776
13. Audio
within the left and right channels. This allows the two main speakers to be
larger, and therefore better at covering the bass. Some stereo systems also
support a subwoofer or LFE (low-frequency effects) speaker. Such systems
are sometimes called 2.1 systems—two for the left and right, and “dot one”
for the LFE speaker.
Headphones Versus Speakers
It’s important to distinguish between stereo speakers in an open room and
stereo headphones. Stereo speakers in a room will typically be positioned in
front of the listener and offset to either side. This means that the sound waves
coming from the left speaker are actually received by the right ear as well,
and vice versa. The waves from the more-distant speaker will arrive at the
ear with a slight time delay (phase shift) and a slight attenuation. The phase-
shifted sound waves from the more-distant speaker will tend to interfere with
those coming from the closer speaker. The sound system should take this
interference into account in order to produce the highest quality sound.
Headphones, on the other hand, come in direct contact with the ears, so
the left and right channels are perfectly isolated and do not interfere with
one another. Also, because headphones deliver sound almost directly to the
ear canal, the head-related transfer effects (HRTF) of the shape of the ears
themselves do not come into play (see Section 13.1.4), meaning that somewhat
less spatial information is received by the listener.
13.3.1.4
Speaker Layouts: Surround Sound
Home theater surround sound systems typically come in two ﬂavors: 5.1 and
7.1. As you undoubtedly guessed, these numbers refer to the ﬁve or seven
“main” speakers, plus the one subwoofer. The goal of a surround sound sys-
tem is to immerse the listener in a realistic soundscape, by providing positional
information as well as high-ﬁdelity sound reproduction (see Section 13.1.4).
The main speaker channels in a 5.1 system are: center, front left, front right,
rear left and rear right. A 7.1 system adds two additional speakers, surround
left and surround right, which are intended to be placed directly to either side
of the listener. Dolby Digital AC-3 and DTS are two popular surround sound
technologies. The speaker layout of a typical 7.1 home theater is shown in
Figure 13.24.
Dolby Surround, Dolby Pro Logic and Dolby Pro Logic II are technologies
for expanding a stereo source signal into 5.1 surround sound. A stereo signal
lacks the positional information necessary to drive the 5.1 speaker conﬁgu-
ration directly. But using these Dolby technologies, an approximation of the

13.3. The Technology of Sound
777
Figure 13.24. Speaker arrangement for a 7.1 surround sound home theater system.
missing positional information can be generated heuristically using various
cues found within the original stereo source signal.
13.3.1.5
Analog Signal Levels
Audio voltage signals may be transmitted at various voltage levels. A mi-
crophone usually produces a low-amplitude voltage signal—these are called
mic-level signals. For connections between components, higher-voltage line-
level signals are used. There’s a big difference between professional audio
equipment and consumer electronics when it comes to line-level voltages.
Professional gear is usually designed to work with line levels ranging from
2.191 V (volts) peak-to-peak for a nominal signal up to a maximum voltage
of 3.472 V peak-to-peak. The peak-to-peak voltage of a “line level” signal on
consumer equipment varies quite a bit, but most consumer devices output up
to 1.0 V peak-to-peak, and have inputs that can handle up to 2.0 V signals. It’s
important to match the levels of input and output signals when connecting
audio equipment. Passing a voltage that is too high for the device to handle
will cause clipping of the signal. And passing a voltage that is too low will
result in audio that sounds quieter than it should.

778
13. Audio
13.3.1.6
Ampliﬁers
The small voltages produced by a microphone must be ampliﬁed in order to
drive speakers with enough force to produce audible sound waves. An am-
pliﬁer is an analog electronic circuit that produces at its output a nearly exact
replica of its input signal, but with the amplitude of the signal increased signif-
icantly. An amp essentially increases the power content of a signal. It does this
by drawing from some kind of power source, and driving the increased volt-
age produced by this power source in such a way as to mimic the behavior of
the input signal over time. In other words, an amp modulates the output of its
power source to match its much lower-voltage input signal.
The core technology behind an ampliﬁer is the transistor—that well-known
and utterly ingenious device that sits at the heart of many modern electronic
devices, including its crowning achievement: the computer. A transistor makes
use of a semiconducting material in order to link the voltages between two
otherwise isolated, independent circuits. As such, a low-voltage signal can
be used to drive a higher-voltage circuit. This is exactly what is required of
an ampliﬁer. We won’t get into the details of how transistors and ampliﬁers
work under the hood here. But if you’re curious, you can whet your whis-
tle with this great YouTube video on how the very ﬁrst transistor worked:
https://www.youtube.com/watch?v=RdYHljZi7ys. And you can read more
about ampliﬁer circuits here: http://en.wikipedia.org/wiki/Ampliﬁer.
The gain A of an ampliﬁcation system is deﬁned as the ratio of output
power Pout to input power Pin. Like sound pressure level, gain is typically
measured in decibels:
A = 10 log10
Pout
Pin

dB.
13.3.1.7
Volume/Gain Controls
A volume control is basically an inverse ampliﬁer, also known as an attenua-
tor. Rather than increasing the amplitude of an electrical signal, it decreases the
amplitude, while keeping all other aspects of the waveform intact. In a home
theater system, the D/A converter produces a voltage signal with a very small
amplitude. The power amp boosts this signal up to the maximum “safe” out-
put power, beyond which the sound produced by your speakers would begin
to clip and distort (or even damage your hardware). The volume control then
attenuates this maximum output power to produce sound at the desired lis-
tening volume.
A volume control is much simpler to make than an ampliﬁer. One can be
constructed by introducing a variable resistor, also known as a potentiometer,

13.3. The Technology of Sound
779
into the circuit somewhere between the ampliﬁer’s output and the speakers.
When the resistance is at its minimum (at or very close to zero), the ampli-
tude of the input signal isn’t changed, and a sound of maximum volume is
produced. When the resistance is at its maximum setting, the input signal’s
amplitude is maximally attenuated, and a sound of minimum volume is pro-
duced.
If your stereo system at home reports the volume in decibels, you’ve prob-
ably noticed that the values are always negative. This is because the volume
control is attenuating the output of the power amp. The volume meter is still
measured like a gain, but the “input” power is the maximum power of the
amp, and the “output” power is the volume selected by the user:
A = 10 log10
Pvolume
Pmax

dB,
which will be negative as long as Pvolume < Pmax.
13.3.1.8
Analog Wiring and Connectors
An analog monophonic audio voltage signal can be carried by a pair of wires;
a stereo signal requires three wires (two channels plus a common ground).
The wiring can be internal to a device, in which case it is usually called a
bus. Wiring can also be external, for use in connecting different devices to one
another.
External wiring is typically connected to audio hardware either via a di-
rect “clip” or screw-post connector, of the kind found on high-end speakers,
or via various standardized connectors. Examples include RCA jacks, large
TRS (tip/ring/sleeve) jacks (the kind used by telephone operators in the early
1900s), TRS mini-jacks (found on your iPod, mobile phone and most PC sound
cards), keyed jacks (found most often on high-quality microphones and power
amps), and the list goes on.
Audio wiring is available in a wide range of quality levels. Thicker-guage
wiring offers less resistance and therefore can transmit signals over farther
distances without unacceptable levels of attenuation. Optional shielding can
help reduce noise. And of course the choice of which metal to use in the
construction of the wires and connectors can make a difference in the quality
of the wiring as well.
13.3.2
Digital Audio Technology
The introduction of the compact disc (CD) marked a turning point in the au-
dio industry toward digital audio storage and processing. Digital technology

780
13. Audio
opens up a great many new possibilities, from reducing the size and increas-
ing the capacity of storage media, to using powerful computer hardware and
software to synthesize and manipulate audio in previously unimagined ways.
Today, analog audio storage devices are a thing of the past, and analog audio
signals are typically employed only where necessary—at the microphone and
the speaker.
As we saw in Section 13.2.1.1, the distinction between analog and digital
audio technologies corresponds exactly to the distinction between continuous-
time and discrete-time signals in the study of signal processing theory.
13.3.2.1
Analog-to-Digital Conversion: Pulse-Code Modulation
To record audio for use in a digitial system, such as a computer or game con-
sole, the time-varying voltage of an analog audio signal must ﬁrst be con-
verted into digital form. Pulse-code modulation (PCM) is the standard method
for encoding a sampled analog sound signal so that it can be stored in a com-
puter’s memory, transmitted over a digital telephony network or burned onto
a compact disc.
In pulse-code modulation, voltage measurements are taken at regular time
intervals. The voltage measurements may be stored in ﬂoating-point format,
or they may be quantized so that each measurement can be stored in an inte-
ger with a ﬁxed number of bits (typically 8, 16, 24 or 32). The sequence of
measured voltage values are then stored into an array in memory, or written
out to a long-term storage medium. The process of measuring a single analog
voltage and converting it to quantized numeric form is called analog-to-digital
conversion or A/D conversion. Specialized hardware is typically used to per-
form A/D conversions. When we repeat this process at regular time intervals,
it is called sampling. A hardware or software component that performs A/D
conversion and/or sampling is referred to as an A/D converter or ADC.
In math terms, given the continuous-time audio signal p(t), we construct
the sampled version p[n] such that for each sample, p[n] = p(nTs), where n is a
non-negative integer used to index the samples, and Ts is the amount of time
between each sample, known as the sampling period. The basics of sampling
are illustrated in Figure 13.25.
The digital signal that results from PCM sampling has two important prop-
erties:
•
Sampling rate. This is the frequency at which the voltage measurements
(samples) are taken. In principle an analog signal can be recorded dig-
itally without any loss of ﬁdelity, provided that it is sampled at a fre-
quency twice that of the highest-frequency component present in the

13.3. The Technology of Sound
781
s
s
Figure 13.25. A discrete-time signal can be thought of as a sampled version of a continuous-time
signal.
original signal. This somewhat astounding and incredibly useful fact
is known as the Shannon-Nyquist sampling theorem. As we saw in Section
13.1.2.2, humans can only hear sounds within a limited band of frequen-
cies (from 20 Hz to 20 kHz). So all audio signals of interest to human
beings are band-limited, and can be faithfully recorded using a sampling
rate of a little over 40 kHz. (Voice signals occupy a narrower band of
frequencies, from 300 Hz to 3.4 kHz, so digital telephony systems can
get away with a sampling frequency of only 8 kHz.)
•
Bit depth. This describes the number of bits used to represent each quan-
tized voltage measurement. Quantization error is the error introduced by
rounding the measured voltage values to the nearest quantized value.
All other things being equal, a greater bit depth results in lower quan-
tization error, and therefore yields a higher-quality audio recording. A
bit depth of 16 is typical among uncompressed audio data formats. Bit
depth is sometimes referred to as resolution.
The Shannon-Nyquist Sampling Theorem
The Shannon-Nyquist sampling theorem states that if a band-limited continuous-
time signal (i.e., a signal whose Fourier transform is zero everywhere outside
a limited band of frequencies) is sampled to produce its discrete-time coun-
terpart, the original continuous-time signal can be recovered exactly from the
discrete signal, provided that the sampling rate is high enough. The mini-
mum sampling frequency for which this relation holds is called the Nyquist
frequency.
ωs > 2 ωmax,
where
ωs = 2π
Ts
.
Clearly, it is the existence of this theorem that allows digital technology to be
used in audio processing. Without it, digital audio would be doomed never

782
13. Audio
to sound as good as analog audio, and computers would not be playing the
signiﬁcant role in the production of high-ﬁdelity audio that they do today.
We won’t get into all of the gory details of why the sampling theorem
works. But we can gain some insight by realizing that the act of sampling
a signal at regularly spaced intervals in time causes its frequency spectrum
(Fourier transform) to be duplicated over and over along the frequency axis.
The higher the sampling frequency, the more “spaced out” these copies of the
signal’s frequency spectrum will be. So if the original signal is band-limited,
and if the sampling frequency is high enough, we can guarantee that the
copies of the frequency spectrum will be spaced far enough apart so as not
to overlap with one another. When this happens, we can recover the original
frequency spectrum exactly via a low-pass ﬁlter that ﬁlters out all of the copies
of the spectrum except the original. However, if the sampling frequency is too
low, the spectrum copies will overlap with one another. This is called aliasing,
and it prevents us from exactly recovering the original signal’s spectrum. See
Figure 13.26 for an illustration of aliased and unaliased sampling.
...
...
...
...
X( )
X ( )
s
X ( )
s
max
max
s
s
s
s
s
max
Aliasing
s
max
Figure 13.26. The frequency spectrum of a band-limited signal is zero everywhere except within
a limited frequency band (top). If the sampling frequency exceeds the Nyquist frequency, the
spectrum copies do not overlap and the original signal can be recovered exactly (middle). If the
sampling frequency is too low, the spectrum copies overlap and aliasing results (bottom).

13.3. The Technology of Sound
783
13.3.2.2
Digital-to-Analog Conversion: Demodulation
When a digital sound signal is to be played back, a process opposite to that of
analog-to-digital conversion is required. We call this, sensibly enough, digital-
to-analog conversion or D/A conversion for short. It is also termed demodulation
because it undoes the effects of pulse-code modulation. A digital-to-analog
conversion circuit is called a DAC.
D/A conversion hardware generates an analog voltage corresponding to
each sampled voltage value in a digital signal, as represented by an array of
quantized PCM values in memory. If we drive this hardware with new values
periodically, at the rate at which the samples were measured during PCM, and
presuming that the sample rate was high enough as per the Shannon-Nyquist
sampling theorem, the analog voltage signal produced should exactly match
the original voltage signal.
Practically speaking, when we drive an analog voltage circuit with a se-
quence of discrete voltage levels, unwanted high-frequency oscillations are
often introduced as the hardware tries to rapidly change from one voltage
level to another. D/A hardware typically includes a low-pass or band-pass
ﬁlter to remove these unwanted oscillations, thereby ensuring an accurate re-
production of the original analog signal. For more information on ﬁltering,
see Section 13.2.5.8.
13.3.2.3
Digital Audio Formats and Codecs
Various data formats exist for storing PCM audio data on disc or transmitting
it over the Internet. Each format has its history, and its pros and cons. Some
formats such as AVI are actually “container” formats, which can encapsulate
digital audio signals in more than one data format.
Some audio formats store the PCM data in an uncompressed form. Oth-
ers utilize various forms of data compression to reduce the required ﬁle size
or transmission bandwidth. Some compression schemes are lossy, meaning
that some of the ﬁdelity of the original signal is lost in the compression/de-
compression process. Other compression schemes are lossless, meaning that
the original PCM data can be recovered exactly after a round-trip compres-
sion/decompression cycle.
Let’s take a look at a few of the most common audio data formats.
•
Raw header-less PCM data is sometimes used in situations where the
meta-information about the signal, such as the sample rate and bit depth,
are known a priori.
•
Linear PCM (LPCM) is an uncompressed audio format that can support
up to eight channels of audio at a 48 kHz or 96 kHz sampling frequency,

784
13. Audio
and 16, 20 or 24 bits per sample. The “linear” in LPCM refers to the fact
that the amplitude measurements are taken on a linear scale (as opposed
to, say, a logarithmic scale).
•
WAV is an uncompressed ﬁle format created by Microsoft and IBM. Its
use is commonplace on the Windows operating system. Its correct name
is “waveform audio ﬁle format” although it is also rarely referred to as
“audio for windows.” The WAV ﬁle format is actually one of a family
of formats known as resource interchange ﬁle format (RIFF). The contents
of a RIFF ﬁle are arranged in chunks, each with a four-character code
(FOURCC) that deﬁnes the contents of the chunk and a chunk size ﬁeld.
The bitstream in a WAV ﬁle conforms to the linear pulse-code modula-
tion (LPCM) format. WAV ﬁles can also contain compressed audio, but
they are most commonly used for storing uncompressed audio data.
•
WMA (Windows Media Audio) is proprietary audio compression tech-
nology designed by Microsoft as an alternative to MP3. See http://en.
wikipedia.org/wiki/Windows_Media_Audio for details.
•
AIFF (audio interchange ﬁle format) is a format developed by Apple
Computer, Inc. and used widely on Macintosh computers. Like a WAV/
RIFF ﬁle, an AIFF ﬁle typically contains uncompressed PCM data, and
is comprised of chunks, each prefaced by a four-character code and a
size ﬁeld. AIFF-C is a compressed variant of the AIFF format.
•
MP3 is a lossy compressed audio ﬁle format that has become the de
facto standard on most digital audio players, and is also widely used
by games and multimedia systems and services. The full name of this
format is actually MPEG-1 or MPEG-2 audio layer III. MP3 compression
can result in ﬁles that are one-tenth the size, but with very little per-
ceptual difference from the original uncompressed audio. These results
are achieved by making use of perceptual coding—a technique that elimi-
nates portions of the audio signal that are beyond the perception of most
people anyway.
•
ATRAC stands for Adaptive Transform Acoustic Coding—a family of
proprietary audio compression techniques developed by Sony. The for-
mat was originally developed to allow Sony’s MiniDisc media to con-
tain audio with the same running time as a CD while occupying signiﬁ-
cantly less space and undergoing an imperceptable degradation in qual-
ity. See http://en.wikipedia.org/wiki/Adaptive_Transform_Acoustic_
Coding for more details.
•
Ogg Vorbis is an open source ﬁle format that offers lossy compression.
Ogg refers to a “container” format that is commonly used in conjunction
with the Vorbis data format.

13.3. The Technology of Sound
785
•
Dolby Digital (AC-3) is a lossy compression format supporting channel
formats from mono to 5.1 surround sound.
•
DTS is a collection of theater audio technologies developed by DTS,
Inc.
DTS Coherent Acoustics is a digital audio format transportable
through S/PDIF interfaces (see Section 13.3.2.5) and used on DVDs and
Laserdiscs.
•
VAG is a proprietary audio ﬁle format available for use by all PlaySta-
tion 3 developers. It makes use of adaptive differential PCM (ADPCM), an
analog-to-digital conversion scheme based on PCM. Differential PCM
(DPCM) stores the deltas between samples rather than the absolute val-
ues of the samples themselves, in order to allow the signal to be com-
pressed more effectively. Adaptive DPCM varies the sample rate dy-
namically in order to further improve the achievable compression ratio.
•
MPEG-4 SLS, MPEG-4 ALS and MPEG-4 DST are formats that offer loss-
less compression.
This list is by no means comprehensive. In fact, there are a dizzying num-
ber of audio ﬁle formats, and an even longer list of compression/decompres-
sion algorithms. For an introduction to the fascinating world of audio data
formats, check out our old friend Wikipedia: http://en.wikipedia.org/wiki/
Digital_audio_format. The “PlayStation 3 Secrets” website also provides some
excellent information on audio formats: http://www.edepot.com/playstation3.
html#PS3_Audio.
13.3.2.4
Parallel and Interleaved Audio Data
One way to organize multi-channel audio data is to store the samples for each
monophonic channel into a separate buffer. In this case, you’d need six par-
allel buffers to describe a 5.1 audio signal. This arrangement is shown in Fig-
ure 13.27.
FR[n+1]
FL[n+1]
C[n+1]
FR[n+2]
FL[n+2]
C[n+2]
...
...
...
Parallel
RR[n+1]
RL[n+1]
LFE[n+1]
RR[n+2]
RL[n+2]
LFE[n+2]
...
...
...
FR[n]
FL[n]
C[n]
RR[n]
RL[n]
LFE[n]
Figure 13.27. Six-channel (5.1) PCM bus data in parallel format.

786
13. Audio
Multi-channel audio data can also be interleaved within a single buffer. In
this case, all of the samples for each time index are grouped together in a
predeﬁned order. Figure 13.28 depicts an interleaved PCM buffer containing
a six-channel (5.1) audio signal.
13.3.2.5
Digital Wiring and Connectors
RR[n]
RL[n]
FR[n]
FL[n]
C[n]
FL[n+1]
C[n+1]
LFE[n]
FR[n+1]
Figure 13.28.
Six-
channel (5.1) PCM bus
data
in
interleaved
format.
S/PDIF (Sony/Philips Digital Interconnect Format) is an interconnect technol-
ogy that transmits audio signals digitally, thereby eliminating the possibility
of noise being introduced by analog wiring. The S/PDIF standard is physi-
cally realized either via a coaxial cable connection (also called S/PDIF) or a
ﬁber-optic connection (known as TOSLINK).
Regardless of the physical interface (S/PDIF coaxial or TOSLINK optical),
the S/PDIF transport protocol is limited to 2-channel 24-bit LPCM uncom-
pressed audio at standard sampling rates ranging from 32kHz to 192kHz.
However, not all equipment works at all sample rates. The same physical in-
terfaces can be also be used to transport bitstream-encoded audio (e.g., Dolby
Digital or DTS lossy compressed data) at bitrates ranging from 32 kpbs to
640 kbps for Dolby Digital and 768 kbps to 1536 kbps for DTS, respectively.
Uncompressed multi-channel LPCM (i.e., greater than two stereo chan-
nels) can only be sent over an HDMI (high-deﬁnition multimedia interface)
connection on consumer audio equipment. HDMI connectors are used for
transmission of both uncompressed digital video and either compressed or
uncompressed digital audio signals. HDMI supports up to a 36.86 Mbps bi-
trate for multi-channel or bitstream audio. However, HDMI bitrates for audio
vary depending on the video mode—only 720p/50 Hz modes or higher are
capable of utilizing the full audio bandwidth. See the HDMI speciﬁcation un-
der the section “video dependency” for more information on this. Apple’s
DisplayPort and Thunderbolt connectors are other high-bandwidth alterna-
tives similar in many respects to HDMI.
USB connections are sometimes used to send audio signals. On most game
consoles, the USB output is intended only to drive headphones.
Wireless audio connections are also possible. The Bluetooth standard is
the most commonly used method of transmitting audio signals wirelessly.
13.4
Rendering Audio in 3D
Thus far, we’ve learned about the physics of sound, the mathematics of signal
processing and the various technologies that are used to record and play back
sounds. In this section, we’ll explore how all of this theory and technology

13.4. Rendering Audio in 3D
787
can be put to use in a game engine, in order to produce realistic, immersive
soundscapes for our games.
Any game that takes place in a virtual 3D world requires some sort of 3D
audio rendering engine. A high-quality 3D audio system should provide the
player with a rich, immersive and believable soundscape that matches what’s
going on in this 3D world, while supporting the story and remaining true to
the tonal design of the game.
•
The inputs to this system are the myriad 3D sounds that emanate from all
over the game world: footsteps, speech, the sound of objects bumping
into one another, gunﬁre, ambient sounds like wind or rainfall and so
on.
•
Its output is a handful of sound channels that, when played in the speak-
ers, reproduce as believably as possible what the player would actually
hear if he or she were really there in the virtual game world.
Ideally we’d like our audio engine to produce its output in full 7.1 or 5.1 sur-
round sound, because this gives the ears the richest possible set of positional
cues. However, audio engines must also support stereo output for players
who don’t have fancy home theater systems—or who just want to play their
game using headphones so they don’t wake their neighbors.
A game’s audio engine is also responsible for playing sounds that do not
originate in the virtual world.
Examples include the music track, sounds
made by the in-game menu system, a narrator’s voice-over, the voice of the
player character (especially in ﬁrst-person shooters) and possibly certain am-
bient sounds. We call these 2D sounds. Such sounds are designed to be played
“directly” in the speakers, after having been mixed with the outputs of the 3D
spatialization engine.
13.4.1
Overview of 3D Sound Rendering
The primary tasks performed by the 3D audio engine are as follows:
•
Sound synthesis is the process of generating the sound signals that cor-
respond to the events occurring in the game world. These might be
produced by playing back pre-recorded sound clips, or they might be
procedurally generated at runtime.
•
Spatialization produces the illusion that each 3D sound is coming from
the proper location in the game world, from the point of view of the
listener. Spatialization is accomplished by controlling the amplitude of
each sound wave (i.e., its gain or volume) in two ways:

788
13. Audio
◦
Distance-based attenuation controls the overall volume of a sound in
order to provide an indication of its radial distance from the listener.
◦
Pan controls a sound’s relative volume in each of the available speak-
ers in order to provide an indication of direction from which the
sound is arriving.
•
Acoustical modeling heightens the realism of the rendered soundscape by
mimicking the early reﬂections and late reverberations that characterize
the listening space, and by accounting for the presence of obstacles that
partially or completely block the path between the sound source and
the listener. Some sound engines also model the frequency-dependent
effects of atmospheric absorption (Section 13.1.3.2) and/or HRTF effects
(Section 13.1.4).
•
Doppler shifting may also be applied to account for any relative move-
ment between a sound source and the listener.
•
Mixing is the process of controlling the relative volumes of all the 2D
and 3D sounds in our game. The mix is driven in part by physics and in
part by aesthetic choices made by the game’s sound designers.
13.4.2
Modeling the Audio World
In order to render the soundscape of a virtual world, we must ﬁrst describe
that world to the engine. The “audio world model” consists of the following
elements:
•
3D sound sources. Each 3D sound in the game world consists of a mono-
phonic audio signal, emanating from a speciﬁc position. We must also
provide the engine with its velocity, radiation pattern (omnidirectional,
conical, planar) and range (beyond which the sound is inaudible).
•
Listener.
The listener is a “virtual microphone” located in the game
world. It is deﬁned by its position, velocity and orientation.
•
Environmental model. This model either describes the geometry and prop-
erties of the surfaces and objects present in the virtual world, and/or it
describes the acoustic properties of the listening spaces in which game-
play takes place.
The positions of the source and listener are used for distance-based attenu-
ation; the radiation pattern of the sound source also factors into the distance-
based attenuation calcuation. The orientation of the listener deﬁnes a refer-
ence frame in which the angular position of the sound is calculated. This angle
in turn determines the pan—the relative volumes of the sound in the ﬁve or

13.4. Rendering Audio in 3D
789
seven main speakers of 5.1 or 7.1 surround sound, respectively. The relative
velocity of source and listener is used when applying a Doppler shift. And last
but not least, the environmental model is used for modeling the acoustics of the
listening space and to account for partial or complete blockage of the sound
path.
13.4.3
Distance-Based Attenuation
Distance-based attenuation reduces the volume of a 3D sound as the radial
distance between it and the listener increases.
13.4.3.1
Fall-Off Min and Max
The number of sound sources in a typical game world is very large. Due to
hardware and CPU bandwidth limitations, we couldn’t possibly render them
all. And we wouldn’t want to, because thanks to distance-based fall-off all
sounds beyond a certain distance from the listener can’t be heard anyway. For
this reason, each sound source is usually annoted with fall-off (FO) parame-
ters.
The fall-off min (“FO min” for short) is a minimum radius, which we’ll
denote rmin, within which the sound doesn’t fall off at all and is heard at full
volume. The fall-off max or “FO max” is a maximum radius, denoted rmax,
beyond which the sound source is considered to be silent and can therefore be
ignored. Between the FO min and FO max, we need to blend smoothly from
full volume down to zero.
13.4.3.2
Blending to Zero
One way to blend from maximum volume down to zero is to use a linear
ramp between FO min and FO max. Depending on the type of sound, a linear
fall-off might sound just ﬁne.
In Section 13.1.3.1, we learned that sound intensity, which is closely related
to our perception of “loudness,” falls off with radial distance according to a
1/r2 rule. Gain, which is proportional to the amplitude of the sound pressure,
falls of as 1/r. So really the right thing to do is to use a 1/r curve to blend the
gain of a sound from full volume down to zero.
One problem with the function 1/r is that it is asymptotic—it never quite
reaches zero, no matter how large r gets. We can ﬁx this by shifting the curve
slightly downward so that it crosses the r axis at rmax. Or we can simply clamp
the sound intensity to zero for all r > rmax.

790
13. Audio
13.4.3.3
Bending the Rules
When making The Last of Us, Naughty Dog’s sound department discovered
that attenuating character dialog using the 1/r2 rule caused speech to be-
come unintelligible too quickly for characters that were only a modest dis-
tance away. This was a serious problem, especially during the stealth sections
of gameplay, where hearing the enemies’ ambient conversations was impor-
tant both as a tactical tool and as a means of advancing the storyline.
To solve this problem, the sound department at Naughty Dog utilized a
sophisticated fall-off curve that causes dialog to roll off more slowly near the
listener, more quickly in the mid-range, and then more slowly again as the
distance to the listener grows very large. This allows speech to be audible
over longer distances, while still retaining a natural-sounding fall-off.
The dialog fall-off curves were also adjusted dynamically at runtime, based
on the current “tension level” of the game (i.e., whether the enemies are un-
aware of the player, are searching for him or are engaged in direct combat
with him). This is what allows the voices in The Last of Us to project over
longer distances during stealth gameplay, while not rising to overpowering
levels when combat breaks out.
Finally, a “sweetener” reverb could be optionally enabled to allow char-
acter voices to bleed around corners, even when the direct path is 100% ob-
structed. This tool is incredibly helpful in situations where modeling realistic
fall-off is less important than ensuring that the player can hear a conversation
clearly.
There are all sorts of ways to “cheat” when designing your 3D audio model.
But no matter what you do, always remember this simple lesson: Never be
afraid to do whatever it takes to satisfy the needs of your game. Don’t worry—
the laws of physics won’t be offended!
13.4.3.4
Atmospheric Attenuation
As we saw in Section 13.1.3.2, low-pitched sounds are attenuated by the atmo-
sphere less than high-pitched sounds. Some games, including Naughty Dog’s
The Last of Us, model this phenomenon by applying a low-pass ﬁlter to each
3D sound whose passband slides toward lower and lower frequencies as the
distance between the sound source and the listener increases.
13.4.4
Pan
Panning is a technique used to provide the illusion that a 3D sound is coming
from a particular direction. By controlling the volume (i.e., gain) of the sound
in each of the available speakers, we can induce the perception of a phantom

13.4. Rendering Audio in 3D
791
Figure 13.29. Speaker layout for 7.1 pan.
image of the sound in three-dimensional space. This method of panning is
called amplitude panning because we are providing angular information to the
listener by adjusting only the amplitudes of the sound waves produced at
each speaker (as opposed to using phase offsets, reverb or ﬁltering to provide
positional cues). It is sometimes referred to as IID panning because it relies
on the perceptual effects of interaural intensity difference (IID) to produce a
sound’s phantom image.
The term “pan” comes from early technology that used a “panoramic po-
tentiometer” (variable resistor) or “pan pot” to control the relative volumes
of the left and right speakers of a stereo system. Dialing the pan pot to one
extreme would produce sound only in the left speaker; dialing it to the other
extreme would drive the right speaker exclusively; and centering the pan pot
dial would distribute the sound equally to both speakers.
To understand how pan works, we envision our listener located at the cen-
ter of a circle. The speakers are positioned at various points on the circumfer-
ence of this circle, so we’ll call it the speaker circle in this book. The radius of
the circle approximates the average distance between the listener and any one
speaker.
For a stereo sound system, the front and right speakers are located roughly
at ±45 degrees to the left and right of center. For stereo headpones, they are
positioned at ±90 degrees (and the radius is much smaller). For 7.1 surround
sound, we consider only the seven “main” speakers, as the LFE channel pro-
vides no positional cues. These speakers are located roughly as shown in
Figure 13.29. When panning to a 5.1 system, we simply omit the surround left
and surround right speakers.

792
13. Audio
s
2
1
s
2
1
Figure 13.30.
Treating the sound as a point source, the pan blend percentage β is calculated
between the two speakers immediately adjacent to the source.
For the time being, let’s treat each 3D sound as a point source. To pan a
sound, we ﬁrst determine its azimuthal (horizontal) angle. The azimuthal an-
gle must be measured in the local space of the listener, so that an angle of zero
corresponds to the position directly in front of the listener. Next, we ﬁgure out
which two speakers around the circumference of our circle are adjacent to this
azimuthal angle. We convert the angle into a percentage of the arc between
the two speakers. Finally, we use this percentage to determine the gains of the
sound in each speaker.
To formulate this mathematically, let’s use the symbol θs for the azimuthal
angle of the sound. We’ll call the angles of the two adjacent speakers θ1 and
θ2. The percentage β is then calculated as follows:
β = θs −θ1
θ2 −θ1
.
This calculation is illustrated in Figure 13.30.
13.4.4.1
Constant Gain Panning
Our ﬁrst instinct might be to use the percentage β to perform a simple linear
interpolation between the gains of the two speakers. Given the gain A of the
unpanned sound, the gains of that sound as played in each speaker would be
calculated as follows:
A1 = (1 −β)A;
A2 = βA.

13.4. Rendering Audio in 3D
793
This is known as constant gain panning, because the net gain A = A1 + A2 is
constant, independent of the values of θs and β.
The main problem with constant gain panning is that it does not produce
the perception of constant loudness as the sound moves around the acoustic
ﬁeld. Gain controls the amplitude of the sound pressure wave, and there-
fore controls the sound pressure level (SPL). However, as we learned in Section
13.1.2, human perception of loudness is actually proportional to the intensity
or power of a sound wave, both of which vary as the square of the SPL.
As an illustration of the problem, imagine that our sound is panned to
the halfway point between our two speakers. Constant gain panning would
have us set the gains A1 and A2 to 1
2A each. But this yields a total power of
A2
1 + A2
2 = ( 1
2A)2 + ( 1
2A)2 = 1
2A2. In other words, the loudness of the sound
will be one-half of what it would have been, had the sound been panned to
only the left or the right speaker.
13.4.4.2
The Constant Power Pan Law
Clearly in order to keep the perception of loudness constant as a sound’s im-
age moves about the listener, we need to keep the power constant. This rule is
known as the constant power pan law, or just the pan law for short.
There’s a very easy way to implement the constant power pan law. Instead
of linearly interpolating the gains, we use the sine and cosine of the blend
percentage β to calculate them:
A1 = sin( π
2 β)A;
A2 = cos( π
2 β)A.
Consider again a sound image that is panned to halfway between the two
speakers (β = 1
2). With constant power panning, the two speakers’ gains will
be set to A1 = A2 =
1
√
2A. This yields a total power of A2
1 + A2
2 = ( 1
√
2A)2 +
( 1
√
2A)2 = A2. This works for any value of β, so the power A2 is constant no
matter where our sound image is placed around the circle.
Sound designers often apply a “3 dB rule” to account for the pan law:
If a sound is to be mixed equally to two speakers, the gain in each speaker
should be reduced by 3 dB relative to the gain that would be used if the
sound were to be played in only one speaker. The value −3 dB arises be-
cause log10

1
√
2

≈−0.15. Voltage gain (or amplitude gain) is deﬁned as
20 log10(Aout/Ain), and 20 × −0.15 = −3 dB. (The 20 in front of the logarithm
arises because a decibel is one-tenth of a bel, multiplied by two to account for
the fact that we’re dealing with A2 and not A.)

794
13. Audio
13.4.4.3
Headroom
Panning causes sounds to be rendered entirely by one speaker in some situa-
tions, and by two (or more, as we’ll see) speakers in others. Let’s say a sound
is being played equally by two adjacent speakers, and its volume is so loud
that each speaker is outputting its maximum power. What happens when
that sound pans around to only one speaker? The answer is that we’d proba-
bly blow out the speaker, because our constant power pan law requires us to
use more gain for one speaker than for two.
To prevent this problem, we need to artiﬁcially lower the maximum gains
of our sounds across the board, such that the worst-case scenario of playing
the sound in one speaker won’t overdrive that speaker. The practice of artiﬁ-
cially reducing the maximum range of volume is known as “leaving oneself
some headroom.”
The concept of headroom also applies to mixing. When two or more sounds
are mixed, their amplitudes add up. By leaving some headroom in our mix,
we can accomodate worst-case scenarios where a large number of high-volume
sounds play simultaneously.
13.4.4.4
To Center or Not To Center?
In cinema, the center channel was historically used for speech; only the sound
effects would be panned to the other speakers around the room. The idea
behind this practice was that the characters in the movie are usually on-screen
when they speak, so the audience expects to hear their voices front-and-center.
This approach has the nice side-effect of separating out the speech from the
rest of the sounds in the ﬁlm, meaning that loud sound effects won’t use up
all the available headroom and drown out the dialog.
In 3D games, the situation is quite different. The player generally wants
to hear dialog coming from the “correct” location around him or her. If the
player swings the camera by 180 degrees, the dialog should likewise swing
about the sound ﬁeld by 180 degrees. As such, games usually do not assign
all dialog to the center speaker; instead, it is included in the pan for both
sound effects and dialog.
Of course, this brings us back to the headroom problem—loud gunﬁre can
now completely drown out the speech. At Naughty Dog, we overcame this
problem by “splitting the difference” and always playing some of the dialog
in the center channel, as well as panning some of it to the rest of the speakers
along with the sound effects.

13.4. Rendering Audio in 3D
795
13.4.4.5
Focus
When the source of a sound is far away from the listener, we can treat it as a
point source. We simply calculate a single azimuthal angle and feed it into our
constant power panning system. However, when a sound source approaches
or actually enters into the circle that deﬁnes the radial distance of the speakers
from the listener, it can no longer be accurately modeled as a point source
represented by a single angle.
Consider the case of moving toward and past a sound source. At ﬁrst, the
sound source appears entirely in the front speakers. As it passes the listener,
we somehow need to transfer the sound to the rear speakers. If we model the
sound as a point source, our only option is to “pop” the sound from the fronts
to the rears.
Ideally we’d like the sound’s image to gradually “spread out” around the
speaker circle as it approaches. That way, as it nears the listener, we can start
playing more of it in the side speakers. When the sound source is coincident
with the listener, it can be played in all seven (or ﬁve) speakers. And once it
passes, we can smoothly transition the sound to the rears, dropping the front
gains to zero as it recedes behind the listener.
We can do this kind of thing and more if we model a sound source not
as a point on the speaker circle but as an arc. Looking at it another way, we
can think of each sound source as having an arbitrary shape in 3D space, and
its projection onto the speaker circle subtends a certain angle, deﬁning a “pie
wedge” shape within the circle. This is analogous to the concept of solid angle
often used in the calculation of ambient occlusion in 3D graphics—see http://
en.wikipedia.org/wiki/Solid_angle for details.
We’ll call the angle subtended by an extended sound source the focus angle,
and we’ll denote it α. A point source can be thought of as an “edge case” in
which α = 0. The focus angle is depicted in Figure 13.31.
To render a sound with a nonzero focus angle, we must ﬁrst determine
the subset of speakers that either intersect its projected arc on the speaker
circle, or are immediately adjacent to the arc. Then we must divide the sound’s
intensity/power among these speakers in order to induce the perception of a
phantom image that extends across the projected arc.
We can divide the sound amongst the relevant speakers in various ways.
For example, we could arrange for all the speakers that lie within the focus
“pie slice” to receive equal maximum power, and then apportion less of the
sound to the two speakers immediately adjacent the arc to create a fall-off.
But no matter how we do it, we must remember to always obey the constant
power pan law. So, we must set the gains in such a way that the sum of their

796
13. Audio
Figure 13.31. The focus angle α deﬁnes the projection of an extended sound source on the speaker
circle.
squares (i.e., the sum of their powers) equals the squared gain of the original
unpanned sound source.
13.4.4.6
Dealing with Verticality
In both stereo and surround sound set-ups, the speakers all lie roughly in a
horizontal plane. This arrangement makes it tricky to position sounds above
or below the plane of the listener’s ears.
The ideal of course would be to model a true “periphonic” sound ﬁeld by
using a spherical speaker arrangement. The little-known Ambisonic technol-
ogy (http://en.wikipedia.org/wiki/Ambisonics is capable of accomodating
both planar and spherical speaker arrangements. However, it is not supported
by any game console—at least not yet. So we’ll need to make do with a planar
speaker arrangement.
It turns out that the concept of focus can be leveraged to simulate some de-
gree of verticality in our sound imagery. We simply project all sounds onto the
horizontal plane, and then use a nonzero focus angle for any sounds whose
projections fall too close to or within the speaker circle. An elevated sound
that is far away will be rendered in virtually the same way as one that is not
elevated. But as the elevated sound passes overhead, we blend it across mul-
tiple speakers, thereby producing a phantom image within the speaker circle.
If we combine this with distance-based attenuation and frequency-dependent
atmospheric absorptions, we can provide the listener with enough cues to
make the sound seem to be located above or below the listener.

13.4. Rendering Audio in 3D
797
13.4.4.7
Further Reading on Pan
The basics of the constant power pan law can be found here: http://www.
rs-met.com/documents/tutorials/PanRules.pdf. The following site is also a
great resource on the topic: http://www.music.miami.edu/programs/mue/
Research/jwest/Chap_3/Chap_3_IID_Based_Panning_Methods.html.
The paper entitled “Spatial Sound Generation and Perception by Ampli-
tude Panning Techniques” by Ville Pukki of the Helsinki University of Tech-
nology, available at https://aaltodoc.aalto.ﬁ/bitstream/handle/123456789/
2345/isbn9512255324.pdf?sequence=1, provides a clear description of the spa-
tialization problem and outlines the vector based amplitude panning (VBAP)
method, as well as providing an extensive bibliography for further reading.
David Griesinger’s paper, “Stereo and Surround Panning in Practice,”
also makes for a very interesting read; it is available at http://www.
davidgriesinger.com/pan_laws.pdf. David’s website is chock full of research
on sound perception and audio reproduction technologies.
13.4.5
Propagation, Reverb and Acoustics
Even if we were to implement distance-based attenuation, pan and Doppler,
our 3D sound engine still wouldn’t be able to generate a realistic soundscape.
This is because a lot of the auditory cues we humans use to sort out what
kind of space we’re in come from the early reﬂections, late reverberations and
head-related transfer function (HRTF) effects caused by sound waves taking
multiple paths to reach our ears. The term “sound propagation modeling” can
be applied to any technique that is designed to take into account the ways in
which sound waves propagate through a space.
Many different approaches are used, both in research and in interactive
media and games. These technologies fall into three basic categories:
•
geometric analysis attempts to model the actual pathways taken by sound
waves,
•
perceptually based models focus on reproducing what the ear perceives
using an LTI system model of the acoustics of a listening space, and
•
ad hoc methods employ various kinds of approximations to produce rea-
sonably accurate acoustics with minimal data and/or processing band-
width.
The following paper does a good job of surveying many of the techniques
that fall into the ﬁrst two categories: http://www-sop.inria.fr/reves/Nicolas.
Tsingos/publis/presence03.pdf. In this section, we’ll brieﬂy discuss LTI sys-

798
13. Audio
tems modeling, and then turn our attention to a few ad hoc methods, because
they tend to be more practical for use in real games.
13.4.5.1
Modeling Propagation Effects with an LTI System
Imagine that I am standing in a room containing various objects made of var-
ious materials. A sound is made in the room. It reﬂects and diffracts and
bounces around the room, and eventually reaches my ears. If you think about
it, it doesn’t really matter which speciﬁc paths those sound waves took. The
only thing that affects my perception is the speciﬁc superposition of the dry
direct sound waves and the various time-shifted and possibly mufﬂed or oth-
erwise altered wet indirect waves.
It turns out that all of these effects can be modeled with a linear time-
invariant (LTI) system. Theoretically, if we could measure the impulse response
of the room for a given pair of points that represent the source of the sound
and the listener, we can determine exactly how any sound we might play at
that source location should sound if heard at the listener position. All we need
to do is convolve the dry sound with the impulse response!
pwet(t) = pdry(t) ∗h(t).
This technique seems like a silver bullet at ﬁrst blush. However, it is ac-
tually more difﬁcult and less practical than it may at ﬁrst seem. It’s pretty
easy to determine the impulse response of a space in real life—you can record
the sound of a short “click” that approximates the unit impulse δ(t), and the
recorded signal will approximate h(t). But in a virtual space, we’d need to
perform a complex and expensive simulation of each play space in order to
determine h(t). Also, to model the room’s acoustics accurately, we’d need
to perform this calculation for a large number of source-listener point pairs
throughout the game world, and once calculated the size of this data would
be immense. Finally, the operation of convolution is itself not inexpensive,
and game consoles and sound cards have in the past lacked the horsepower
to do this for every sound in the game in real time.
Modern gaming hardware is getting more powerful all the time, and a
convolution-based approach to propagation modeling is becoming more fea-
sible. For example, Micah Taylor et al. created a real-time demo of convolu-
tion reverb that produced promising results—see http://software.intel.com/
en-us/articles/interactive-geometric-sound-propagation-and-rendering. That
said, most games still don’t use this approach, but instead they rely on various
ad hoc methods and approximations to model environmental reverb.

13.4. Rendering Audio in 3D
799
Figure 13.32. It’s a good idea to cross-blend between reverb settings based on the position of the
listener.
13.4.5.2
Reverb Regions
One common approach to modeling the wet characteristics of a play space is
to annotate the game world with manually placed regions, each of which is
tagged with appropriate reverb settings such as pre-delay, decay, density and
diffusion. See Section 13.1.3.4 for a discussion of these parameters. As the
virtual listener moves through these regions, we can light up the appropriate
reverb mode: If the player enters a large tiled room, we can bump up the
echos; when the player enters a small closet, we can virtually eliminate the
reverb to produce a very dry sound.
It’s a good idea to smoothly cross-blend between reverb settings as the
listener moves through the play space. We can use simple linear interpola-
tion to perform this cross-blend for each parameter. The blend percentage is
best calculated using a measure of how far “into” the region the listener is.
For example, imagine moving between an outdoor space and an indoor space
through a doorway. We could deﬁne a region around the doorway within
which the blend occurs. If the listener is entirely outside the blend region, the
blend percentage should yield 100% of the outdoor reverb settings and 0% of
the indoor settings. If the listener is standing at the halfway point within the
blend region, we’d want a 50/50 mix of the reverb settings. Once the listener
passes out of the blend region inside the building, we’ll have reached a 0%
outdoor / 100% indoor blend. This idea is illustrated in Figure 13.32.
13.4.5.3
Obstruction, Occlusion and Exclusion
When using regions to deﬁne the acoustics of our play spaces, we typically as-
sign a single impulse response function or a single collection of reverb settings
to each region. This captures the essence of each play space (e.g., large tiled
hall, small closet lined with coats, ﬂat outdoor plain, etc.). But it results in a
less-than-perfect reproduction of the acoustics that arise due to obstacles. For

800
13. Audio
sound
indirect
direct
sound
indirect
sound
indirect
direct
direct
Figure 13.33. From top to bottom: occlusion, obstruction and exclusion.
example, imagine a square room with a large pillar in the center. If a sound
source is located in the corner of the room, a listener will perceive a very dif-
ferent timbre as he or she moves about the room, depending on whether the
direct path is obstructed by the pillar or not. If we use a single set of reverb
parameters for this room, we cannot capture these subtleties.
To address this problem, we can attempt to model the geometry and mate-
rial properties of the environment in some way, determine how sound waves
are affected by the obstacles in their path, and then use the results of this anal-
ysis to alter the “base” reverb settings associated with the room.
Figure 13.33 shows the three ways in which the objects and surfaces in the
game world can affect the transmission of sound waves:
•
Occlusion. This describes a situation in which there exists no unfettered
path from the sound source to the listener. A listener might still be able

13.4. Rendering Audio in 3D
801
to hear a fully occluded sound, if for example there is only a thin wall
or door between it and the source of the sound. Either the dry and wet
components of an occluded sound are both attenuated and/or mufﬂed,
or the sound is entirely silent from the point of view of the listener.
•
Obstruction. This describes a case in which the direct path between the
sound and the listener is blocked, but an indirect path is available. Ob-
struction can occur for example when a sound source passes behind a
car, pillar or other obstacle. The dry component of an obstructed sound
is either entirely absent or greatly mufﬂed, and the wet component may
be altered as well to account for the sound waves having to take a longer,
more reﬂected path to the listener.
•
Exclusion. This describes a case in which there is a free direct path be-
tween source and listener, but the indirect path is compromised in some
way. This can happen if a sound is produced in one room and passes
through narrow opening such as a door or window to reach the listener.
In an exclusion situation, the dry component of the sound remains unal-
tered but the wet component is attenuated, mufﬂed or, for very narrow
openings, entirely absent.
Analyzing the Direct Path
Determining whether the direct path is blocked or not is not difﬁcult. We
simply cast a ray (see Section 12.3.7.1) from the listener to each sound source.
If it is blocked, the direct path is occluded. If not, it is free.
If we wish to model sound transmission through walls and other obstacles,
ray casting can still be used. We cast a ray from source to listener, and for each
contact we query the material properties of the impacted surface to determine
how much of the sound’s energy it absorbs. If it allows some energy to pass
through, we can cast another ray starting on the other side of the obstacle
and continue tracing the path to the listener. Once all of the sound’s energy
has been absorbed, we can conclude that the sound cannot be heard. But if
the ray makes it all the way to the listener without losing all sound energy,
we can attenuate the gain of the dry sound component by the corresponding
amount to simulate transmission of the sound.
Analyzing the Indirect Path
Determining whether the indirect path is occluded is a much more difﬁcult
problem. Ideally, we’d perform some kind of search (A* perhaps) to determine
whether or not a path exists from the source to the listener, and also how
much attenuation and reﬂection is introduced by each viable path. In practice,

802
13. Audio
this path tracing method is rarely used because it is processor- and memory-
intensive. And at the end of the day, we game programmers aren’t really
interested in creating physically accurate simulations that will win us Nobel
prizes in physics. We merely want to produce a soundscape that is immersive
and believable.
Never fear, all is not lost. There are all sorts of ways in which we can obtain
an approximate model of the indirect path of a sound. For example, if we are
using reverb regions to model the overall acoustics of the various spaces in
our game (see Section 13.4.5.2), we could leverage these regions to determine
whether an indirect path exists. For example, we could use some simple rules
of thumb:
1.
If the source and listener are in the same region, assume an indirect path
exists.
2.
If the source and listener are in different regions, assume the indirect
path is occluded.
Using these assumptions combined with the results of our direct path ray cast,
we can differentiate between the four cases: free, occluded, obstructed or ex-
cluded.
Accounting for Diffraction
When any wave passes through a narrow opening or interacts with a corner,
it spreads out as shown in Figure 13.34. We call this phenomenon diffraction.
Because of diffraction, sounds can be heard around corners as if a direct path
existed, as long as the angular difference between the direct path and curved
path is not too great.
One way to determine whether sound can diffract in order to reach the lis-
tener is to cast a few “curved” rays around the central “direct” ray. Most colli-
sion engines don’t support curved path tracing, but we can emulate a curved
Figure 13.34. Diffraction causes the dry component of a sound to be clearly audible even when
the direct path is blocked.

13.4. Rendering Audio in 3D
803
Figure 13.35. Curved ray casts can be approximated using multiple straight-line rays.
path by using multiple straight-line ray casts. Figure 13.35 shows a simple ex-
ample, in which ﬁve rays are cast from the sound source to the listener—one
direct ray, plus two “curved” traces comprised of two straight-line ray casts
each. Technically speaking we’re employing a piecewise-linear approximation to
each curved path we wish to trace.
If the direct ray is occluded but the curved traces can “see” the listener,
this tells us that the listener is within the “diffraction region” around a nearby
corner, and should hear the sound as if it is not occluded.
Applying the Model Using Reverb and Gain
Thus far, we’ve discussed how to determine whether the direct and indirect
paths are blocked or not. This analysis can also tell us something about the
acoustic impact of an occlusion or obstruction. (For example, sound pass-
ing through a wall can be mufﬂed; sound taking a long “bouncy” path might
introduce a lot of reverb.) The question now is: How do we apply this knowl-
edge when rendering the sound?
One simple approach is to simply attenuate the dry and wet components
of the sound individually, based on whether the direct or indirect paths are
totally or partially blocked, respectively. To ﬁnesse the results, we can also
apply more or less reverb to each component of the sound, based on whatever
heuristic information we gathered when determining the path(s) taken by the
sound. The needs of every game are different, so this is one of those times
when trial and error is your best and only option!
Blending Obstructed Sounds
If you were to go off and implement everything we talked about in the sec-
tions above, you’d notice a glaring problem. As a sound source moves be-
tween the four states described above—for example, from being free to being
obstructed—the timbre and loudness of the sound will seem to “pop.” There
are a number of ways to smooth out such transitions. You could apply a little

804
13. Audio
hysteresis, meaning that you delay the response of the sound system to changes
in the obstructed state of each sound, and then use this short delay window
to smoothly cross-blend between the two sets of reverb settings. But the delay
might be noticeable, so this isn’t an ideal solution.
For the Uncharted series and The Last of Us, Naughty Dog’s senior sound
programmer Jonathan Lanier invented a proprietary system that he called sto-
castic propagation modeling. Without giving away any trade secrets, I can tell
you that this system involves casting a bunch of rays to each sound source,
some direct and some indirect, and accumulating these hit/miss results over
many frames. From this data, we generate a probabalistic model of the de-
gree of occlusion experienced by both the dry and wet components of each
sound source. This allows us to smoothly transition a sound from being fully
obstructed to fully free without noticeable “pops.”
13.4.5.4
Sound Portals in The Last of Us
For The Last of Us, Naughty Dog needed a way to model the actual pathways
that sounds take through the environment. If an enemy NPC is speaking
while standing in a long hallway that connects to the room the player is in,
we wanted to be able to hear the sound of his voice coming from the doorway,
not “through the wall” along a straight-line path.
To do this, we used a network of interconnected regions. There were two
kinds of regions: rooms and portals. For each sound source, we found a path
from the listener to the sound by using connectivity information provided by
the sound designer when laying out the regions. If both the sound source and
listener were in the same room, we’d use the tried and true method of per-
forming obstruction/occlusion/exclusion analysis that we used on the Un-
charted series. But if the sound source was in a room directly connected to
the listener’s room via a portal, we would play the sound as if it were located
in the portal region. We found that we only needed to go “one hop” in the
room connectivity graph to make this work for all real situations that arose
in the game. Obviously I’m leaving out a lot of important details here, but
Figure 13.36 illustrates the basics of how this system worked.
13.4.5.5
Further Reading on Environmental Acoustics
Audio propagation modeling and acoustics analysis are areas of active re-
search, and more and more advanced techniques are being applied in the
game industry as hardware capabilities continue to improve. A few links are
listed below to whet your appetite, but a Google search for “sound propaga-
tion” or “acoustics modeling” will provide many more hours of enjoyment!

13.4. Rendering Audio in 3D
805
Portal
Region
Fake
Sound
Source
Figure 13.36. The portal-based audio propagation model used in The Last of Us by Naughty Dog,
Inc.
•
“Real-Time Sound Propagation in Video Games” by Jean-François Guay
of Ubisoft Montreal:
http://gdcvault.com/play/1015492/Real-time
-Sound-Propagation-in;
•
“Modern Audio Technologies in Games” presented at GDC 2003 by A.
Menshikov: http://ixbtlabs.com/articles2/sound-technology;
•
“3D Sound in Games” by Jake Simpson: http://www.gamedev.net/page/
resources/_/technical/game-programming/3d-sound-in-games-r1130.
13.4.6
Doppler Shift
As we saw in Section 13.1.3.5, the Doppler effect is a change in frequency
that’s dependent upon the relative velocity between source and listener: vrel =
vsource −vlistener. This frequency change can be approximated by simply time-
scaling the sound signal. This results in the “chipmunk effect” with which
Alvin and the Chipmunks have made us all so familiar—by speeding up a
sound, the pitch also rises. Because our sound signals are digital (i.e., sam-
pled discrete-time signals), this kind of time scaling can be accomplished via
sample rate conversion (see Section 13.5.4.4). However, this is not strictly the
correct thing to do, because the speeding up or slowing down of the sound
can become noticable.
The ideal solution is to apply a pitch shift without affecting the time axis.
This can be done in a number of ways, including the phase vocoder and time do-
main harmonic scaling approaches. A complete description of these techniques

806
13. Audio
is beyond our scope here, but you can read more about them at http://www.
dspdimension.com/admin/time-pitch-overview.
Time-independent pitch shifting technology is an extremely powerful thing
to have in your audio engine, in part because it also allows you to perform
frequency-independent time scaling. So not only can you alter the pitch of
sounds without changing timing for Doppler, you can also speed up or slow
down sounds without altering their pitch for all sorts of other cool effects.
13.5
Audio Engine Architecture
To this point, we’ve discussed the concepts and methodologies behind 3D
sound rendering, and the theory and technologies that underlie them. In this
section, we’ll turn our attention to the architecture of the software and hard-
ware components used to implement a 3D audio rendering engine.
As with most computer systems, a game engine’s audio rendering soft-
ware is typically arranged into a “stack” of layered hardware and software
components (see Figure 13.37).
•
Hardware inevitably serves as the foundation of this structure, providing
at minimum the necessary circuitry to drive the digital or analog speaker
outputs that connect our PC or game console to a pair of headphones,
a TV or a surround sound home theater system. Audio hardware may
also provide “acceleration” to the software above it in the stack by sup-
plying codecs, mixers, reverb tanks, effects units, waveform synthesiz-
ers and/or DSP chips in silicon. This hardware is often called the sound
card because PCs sometimes provide their audio capabilities via a plug-
in peripheral card.
•
On a personal computer, the hardware is typically encapsulated in a
driver layer, allowing the OS to support sound cards from a wide range
of vendors.
Figure 13.37. The audio hardware/software “stack.”

13.5. Audio Engine Architecture
807
Figure 13.38. A multi-channel mixer console by Focusrite with support for 72 inputs and 48 out-
puts.
•
On both PCs and game consoles, the hardware and drivers are usually
wrapped in a low-level application programming interface (API) designed
to free the programmer from having to deal with the minutia of control-
ling the hardware and drivers directly.
•
The 3D audio engine itself is built on top of these foundations.
The feature set presented to the programmer by the audio hardware/soft-
ware stack is usually modeled after the feature set of a multi-channel mixer con-
sole (http://en.wikipedia.org/wiki/Mixing_console) of the sort used in record-
ing studios and at live concerts (see Figure 13.38). A mixer board can accept
a relatively large number of audio inputs obtained from microphones and/or
electronic instruments. The input sounds can be ﬁltered and equalized, and
reverb and other effects can be applied to them. The console is then used
to mix all of the signals together, setting the relative volumes of the sounds
as desired by the sound designer. The ﬁnal mixed output is routed to the
speakers (for a live performance) or to the individual channels of a multi-track
recording.
In the same sense, the audio HW/SW stack must accept a large number of
inputs (2D and 3D sounds), process them in various ways, mix them together
so that their relative gains are set appropriately and ﬁnally pan these signals
to the speaker output channels to produce the illusion of a three-dimensional
soundscape for the human player.

808
13. Audio
13.5.1
The Audio Processing Pipeline
As we learned in Section 13.4.1, the process of rendering a 3D sound involves
a number of discrete steps:
•
For each 3D sound, a “dry” digital (PCM) signal must be synthesized.
•
Distance-based attenuation is applied to provide a sense of distance
from the listener, and reverb is applied to the signal to model the acous-
tics of the virtual listening space and to provide spatialization cues to
the listener. This produces a new “wet” signal.
•
The wet and dry signals are panned (independently) to one or more
speakers in order to produce the ﬁnal “image” of each signal in three-
dimensional space.
•
The panned multi-channel signals of all the 3D sounds are mixed to-
gether into a single multi-channel signal, which is either sent through
a parallel bank of DACs and amps to drive the analog speaker outputs
or sent directly to a digital output such as HDMI or S/PDIF.
Clearly, we think of the process of rendering 3D audio as a pipeline. And
because a game world typically contains a large number of sound sources,
multiple instances of this pipeline are in ﬂight simultaneously. For this reason,
the audio processing pipeline is sometimes called the audio processing graph. It
truly is a graph of interconnected components, ultimately culminating in the
handful of speaker channels that comprise the ﬁnal mixed, panned output.
Figure 13.39 presents a high-level view of the audio graph.
wet
dry
Synth
Distance
Attenuation
Reverb
Pan
Pan
wet
dry
Synth
Distance
Attenuation
Reverb
Pan
Pan
Mixer
7.1 Out
LFE 
Gen
6-Channel
Figure 13.39. The audio processing graph (pipeline).

13.5. Audio Engine Architecture
809
13.5.2
Concepts and Terminology
Before we can explore the audio processing pipeline in any depth, we need
to become familiar with a few concepts and the terminology used to describe
them.
13.5.2.1
Voices
Each 2D or 3D sound passing through the audio rendering graph is called
a voice. This term comes from the early days of electronic music: A synthe-
sizer would produce musical notes via a set of waveform generators called
“voices.”
A synthesizer contains a limited number of waveform generator circuits,
so electronic musicians speak of how many simultaneous voices their synth
can produce. In the same sense, a game’s audio rendering engine typically
has a limited number of codecs, reverb units and so on. The maximum num-
ber of voices supported by a particular audio HW/SW stack is dictated by
the number of independent parallel pathways through the audio graph. This
number is generally bounded by limited memory resources, limited hardware
resources and/or processing power limitations. This number is sometimes re-
ferred to as the degree of polyphony supported by the system.
2D Voices
A game’s audio rendering pipeline must also be capable of handling 2D
sounds, such as music, menu sound effects, narrator voice-overs and so on.
2D voices are also processed by the audio rendering pipeline. The main things
that differentiate 2D sound processing from 3D processing are:
•
2D sounds originate as multi-channel signals, one for each available
speaker, whereas 3D sounds originate as dry monophonic signals. As
such, 2D sounds do not pass through a pan pot.
•
A 2D sound may contain “baked” reverb or other effects. If so, the sound
may not make use of the reverb capabilities of the rendering engine.
As such, 2D sounds typically enter the pipeline just prior to the master mixer,
where they are combined with the 3D sounds to produce the ﬁnal “mix.”
13.5.2.2
Buses
The interconnections between the components that make up the audio graph
are called buses. In electronics, a bus is a circuit whose primary purpose is to
connect other circuits to one another. In software, a bus is nothing more than

810
13. Audio
Codec
Pre-Send
Filter
Pan
7-Channel
Gain
Post-Send
Filter
wet
Reverb
Pan
wet
Reverb
Pan
dry
wet
Reverb
Pan
Distortion
Figure 13.40. The pipeline through which an individual 3D voice passes on its way through the
audio graph.
a logical construct that describes the presence of an interconnection between
components.
13.5.3
The Voice Bus
Figure 13.40 presents a more-detailed view of the pipeline of components
through which a single 3D voice passes as it is rendered by the audio engine.
In the following sections, we’ll explore each of these components in detail and
learn why they are interconnected in the way that they are.
13.5.3.1
Sound Synthesis: Codecs
An audio signal passes through the rendering graph in digital form. The
term synthesis describes the process of generating these digital signals. Au-
dio signals may be synthesized by simply “playing back” a pre-recorded au-
dio clip. They might also be procedurally generated, perhaps by combining
one or more fundamental waveforms (sinusoid, square wave, sawtooth, etc.),
and/or by applying various ﬁlters to a harmonically rich noise signal. Since
most games use pre-recorded audio clips almost exclusively, we’ll restrict our
discussion to them here.
Pre-recorded audio clips can be provided to the game engine in any one
of the myriad compressed and uncompressed audio ﬁle formats in use today
(see Section 13.3.2.3). Raw PCM data is the “canonical” format accepted by
the various components in the audio processing graph. Therefore, a device

13.5. Audio Engine Architecture
811
or software component known as a codec is used to convert each source audio
clip into a raw PCM data stream. The codec interprets the source data format,
decompresses the data if necessary, and then transmits it onto the voice bus
for its journey through the audio processing graph.
13.5.3.2
Gain Control
The loudness of each source sound in the 3D world can be controlled in a
number of ways: When recording the audio clip, we can set the recording
levels to produce a sound at the desired loudness. We can process the clip in
an ofﬂine tool to adjust its gain. At runtime, we can also dynamically adjust
the volume of the clip using a gain control component within the audio graph.
See Section 13.3.1.7 for a detailed discussion of gain control.
13.5.3.3
Aux Sends
When a sound engineer at a recording studio or live concert wants to apply
effects to a sound, he or she can route the sound out of the multi-channel mix-
ing console, through an effects “pedal,” and then back into the mixing board
for further processing. These outputs are known as auxiliary send outputs, or
aux sends for short.
Within the audio processing graph, the term “aux send” is used in an anal-
ogous manner: It describes a bifurcation point in the pipeline, splitting the
signal into two parallel signals. One of these signals is for the dry component
of the sound. The other is piped through a reverb/effects component to create
the wet component of the sound.
13.5.3.4
Reverb
The wet signal path is typically routed through a component that adds early
reﬂections and late reverberations. Reverb might be implemented using a
convolution, as described in Section 13.4.5.1. If convolution is not practical in
real time, either because the console or PC lacks DSP hardware or because the
game’s CPU and/or memory budgets are insufﬁcient, reverb can be imple-
mented using a reverb tank. This is essentially a buffering system that caches
time-delayed copies of a sound that are then mixed with the original to mimic
early reﬂections and/or late reverberations, combined with a ﬁlter to mimic
the interference effects and general attenuation of high-frequency components
in the reﬂected sound waves.
13.5.3.5
Pre-Send Filter
The voice pipeline typically includes a ﬁlter that is applied before the aux send
bifurcation, and therefore applies to both the dry and wet components of the

812
13. Audio
sound. This is called a pre-send ﬁlter. It is generally used to model phenomena
that occur at the source of the sound. For example, we could mimic the sound
of someone wearing a gas mask with a pre-send ﬁlter.
13.5.3.6
Post-Send Filter
Another ﬁlter is typically provided after the aux send bifurcation. As such,
this ﬁlter only applies to the dry component of the sound. This ﬁlter can be
useful for modeling the mufﬂing effect of an obstruction/occlusion on the di-
rect sound path. At Naughty Dog, we also use a post-send ﬁlter to implement
the frequency-speciﬁc fall-off that occurs due to atmospheric absorption (see
Section 13.1.3.2).
13.5.3.7
Pan Pots
The dry and wet components of a 3D sound are monophonic signals through-
out their journey along the voice bus. At the very end of the pipeline, each one
of these two mono signals must be panned to the two stereo speakers/head-
phones or the ﬁve or seven surround sound speakers. For this reason, every
3D voice bus terminates in two or more pan pots, one for the dry signal and
one or more for the wet. The components may be panned differently. The dry
signal is panned according to the actual location of the source. The wet signal,
however, may be panned with a wider focus to simulate the way in which
reﬂected sound waves tend to impinge on the listener’s head from various di-
rections. If the sound is coming from a narrow doorway, the focus of the wet
signal might be only a few degrees. But if the listener is standing in the cen-
ter of a cavernous hall, the wet signal should probably be given a 360-degree
focus (i.e., it should be rendered in all speakers equally).
13.5.4
Master Mixer
Each pan pot’s output is a multi-channel bus, containing signals for each of
the desired output channels (stereo or surround sound). A game typically
has a large number of 3D sounds playing simultaneously. The master mixer
takes all of these multi-channel inputs and mixes them together into a single
multi-channel signal for output to the speakers.
Depending on the speciﬁcs of the implementation, the master mixer might
be implemented in hardware, or it might live entirely in software. If the mas-
ter mix is performed in hardware, the sound card designer has the option of
performing an analog mix or a digital mix. (Software can only do digital mix-
ing, for obvious reasons.)

13.5. Audio Engine Architecture
813
13.5.4.1
Analog Mixing
An analog mixer is essentially just an summation circuit—the amplitudes of
the individual input signals are added together, and the resultant wave’s am-
plitude is then attenuated to fall back within the desired signal voltage range.
13.5.4.2
Digital Mixing
Mixing can also be performed digitally by software running on a dedicated
DSP chip or a general-purpose CPU. A digital mixer takes multiple PCM data
streams as its inputs, and produces a single PCM data stream at its output.
A digital mixer’s job is a little more complicated than that of an analog
mixer, because the collection of PCM channels it is combining may have been
recorded at different sample rates and/or different bit depths. Two processes
known as sample depth conversion and sample rate conversion must be executed
on all of the mixer’s input signals to bring them into a common format. Once
this has been done, mixing again becomes trivial. At each time index, the
values of all the input samples are simply added together, and the ﬁnal out-
put amplitude is adjusted if necessary to bring the combined signal into the
desired volume range.
13.5.4.3
Sample Depth Conversion
If the bit depths of the mixer’s input signals differ, sample depth conversion can
be used to convert them to a common format. This operation is trivial. We
simply de-quantize the input sample values into ﬂoating-point format, and
then re-quantize each one at the desired output bit depth. See Section 11.8.2
for all the gory details on quantization.
13.5.4.4
Sample Rate Conversion
If the sample rates of the input signals differ, sample rate conversion must be
used to convert them all into the desired output sample rate prior to mixing.
In principle, this involves converting the signal into analog form, and then
resampling it at the desired rate (which could be done using D/A and A/D
hardware). In practice, analog sample rate conversion tends to introduce un-
wanted noise, so the conversion is almost always accomplished by running a
direct digital-to-digital algorithm directly on the PCM data stream.
An understanding of signal processing theory (see Section 13.2) is neces-
sary to fully understand how these algorithms function, and a full discussion
is beyond our scope here. But in certain simple cases, the concept is easy
enough to grasp. For example, if we are doubling the sample rate, we can in-
terpolate adjacent samples and insert these values as new samples, thereby

814
13. Audio
Pre-
Amp
LFE
Gen
EQ
Compressor
7.1 Out
Vol.
7.1-Channel
7-Channel
Figure 13.41. A typical master output bus.
doubling the number of samples. It’s not quite as simple as this—one must
take care to avoid introducing aliasing into the resulting signal, for example.
See http://en.wikipedia.org/wiki/Sample_rate_conversion for a detailed dis-
cussion of sample rate conversion.
13.5.5
The Master Output Bus
Once the voices have been mixed, they are processed by the master output bus.
This is a collection of components that process the output prior to sending it
to the speakers. A typical master output bus is depicted in Figure 13.41, and
its components are described brieﬂy below. Every audio engine does things
a bit differently, and not all engines support all of the components described
below. Some engines may also introduce additional components not shown
here.
•
Pre-amp. The pre-amp allows the master signal’s gain to be trimmed
prior to passing through the remainder of the output bus.
•
LFE generator. As we mentioned in Section 13.4.4, a pan pot only drives
the two, ﬁve or seven “main” speakers of a stereo or surround sound
system. The LFE (subwoofer) channel does not contribute to the posi-
tioning of a sound’s 3D image. An LFE generator is a component that
extracts the very lowest frequencies of the ﬁnal mixed signal and uses
this to drive the LFE channel.
•
Equalizer. Most audio engines provide some kind of equalizer (EQ). As
described in Section 13.2.5.8, an EQ allows speciﬁc frequency bands in
the signal to be boosted or attenuated individually. A typical EQ divides
the spectrum up into anywhere from four to tens of individually tunable
bands.

13.5. Audio Engine Architecture
815
•
Compressor. A compressor performs dynamic range compression (DRC) on
the audio signal. A compressor reduces the volume of the loudest por-
tions of the signal and/or increases the volume of the quietest moments.
It does this automatically by analyzing the input signal’s volume char-
acteristics and adjusting the compression dynamically. See http://en.
wikipedia.org/wiki/Dynamic_range_compression for a detailed discus-
sion of DRC.
•
Master gain control. This component allows the overall volume of the
entire game to be controlled.
•
Outputs. The output of the master bus is a collection of line-level analog
signals corresponding to the speaker channels and/or a digital HDMI
or S/PDIF multi-channel signal, suitable for transmission to a TV or a
home theater system.
13.5.6
Implementing a Bus
13.5.6.1
Analog Buses
An analog bus is implemented via a number of parallel electronic connections.
To carry a monophonic audio signal, we need two parallel wires or “lines” on
the circuit board: one to carry the voltage signal itself, and one to serve as
ground.
An analog bus operates pretty much instantaneously. The output signal
from an upstream component is immediately consumed by the next down-
stream component, because the signal is a continuous physical phenomenon.
Such circuits are quite simple. The only real complication is ensuring that the
voltage levels and impedances of the input and output signals match.
13.5.6.2
Digital Buses
One could imagine using simple digital circuitry to build instantaneous con-
nections between our digital components. However, this would require the
connected components to run in perfect lock-step: At the exact moment that
the sender produces a byte of data, the receiver would have to consume it.
Otherwise, the byte would be lost.
To overcome the synchronization problems inherent in connecting two
digital components, ring buffers are typically used at the input and/or out-
put of each component. A ring buffer is a buffer that can be shared by two
clients—one reader and one writer. To make this work, we maintain two point-
ers or indices within the buffer, called the read head and the write head. The
reader consumes data at the read head, advancing it forward in the buffer as
data is consumed, and wrapping when the end of the buffer is reached. The

816
13. Audio
writer stores data into the buffer at the write head, advancing and wrapping
as well. Neither head is permitted to “pass” the other, which guarantees that
the two clients cannot conﬂict with one other (i.e., reading data that hasn’t
been written yet, or writing over top of data that is currently being read).
The simplest way to connect, say, the digital output of a codec to the digital
input of a DAC is to use a shared ring buffer. The codec writes to the exact same
buffer read by the DAC.
While simple, the shared buffer approach only works when the two com-
ponents have access to the same physical memory. This is trivial to do when
the components are running in threads on a single CPU. To make a shared
memory approach work between two separate operating system processes,
each of which has its own private virtual memory space, the OS needs to pro-
vide a mechanism for mapping the same physical memory into each process’
virtual address space. This is usually only possible when the two processes
are running on the same core, or on different cores within a multicore com-
puter.
If the two components are running on different cores that cannot share
memory (as would be the case if one were running on the PC and the other
on a plug-in sound card, for example), then each component needs its own
input or output buffer. Data must be copied from the output buffer of one
component to the input buffer of the other. This might be accomplished via
a direct memory access controller (DMAC), as is the case when transferring data
between the PPU and the SPUs on the PS3. It might also be accomplished via
a specialized bus, such as the ubiquitous PCI Express (PCIe) bus that is used
to connect the main CPU core(s) to plug-in peripheral cards on a PC.
13.5.6.3
Bus Latency
In order to play sound, the game or applicaton must feed audio data peri-
odically into the codecs that ultimately drive the speaker outputs. We call
this servicing the audio. The rate at which the game or app services its audio
is crucial to proper sound production: If packets are sent too infrequently, the
buffers will underﬂow, meaning that the device consumes all of the data before
a new packet arrives. This causes the audio to drop out brieﬂy while the soft-
ware catches up. If packets are sent too often, the PCM buffers can overﬂow,
causing packets to be lost. This causes the audio to seem to “skip.”
The size of the input and output buffers that comprise a digital bus dic-
tates the latency of the sound system—in other words, how much delay is
introduced by the bus. If the buffers are very small, latency is minimized,
but this places a greater burden on the CPU because it must feed the buffers
more frequently. Likewise, larger buffers translate into less CPU load but also

13.5. Audio Engine Architecture
817
higher latency. We usually measure the latency of a piece of audio hardware
in milliseconds, rather than measuring the size of the buffers in bytes. This
is done because buffer size depends upon the data format and the degree of
compression supported by the codec, but the latency is really what we care
about when trying to produce high-ﬁdelity sound.
How much latency is acceptable? This depends on the application. Profes-
sional audio systems require very short latencies—on the order of 0.5 ms. This
is because audio signals are often fed through a network of audio hardware
before being synchronized with each other and often to a video signal as well.
Every time latency is introduced by the hardware, accurate synchronization
becomes more difﬁcult.
Game consoles, on the other hand, can tolerate a longer latency. In a game,
all we care about is synchronizing the audio and the graphics. If the game is
rendering at 60 FPS, this translates into 1/60 = 16.6 ms per frame. As long
as the audio isn’t delayed by more than 16 ms, we know it will be in sync
with graphics rendered for that same frame. (In fact, many games use double
or triple buffering for their rendering engines, which introduces one or two
frames of delay between the time the game requests that a frame be drawn
and when that frame will actually appear on the TV screen. The television
may also introduce a delay. As such, a triple-buffered 60 Hz game can actually
tolerate an audio latency of 3×16 = 48 ms or more.) The PlayStation 3’s DMA
controller runs every 5.5 ms, so PS3 audio systems are typically conﬁgured
such that the audio buffers can hold an integer multiple of 5.5 ms worth of
audio.
13.5.7
Asset Management
13.5.7.1
Audio Clips
The most atomic audio asset is a clip—a single digital sound asset with its own
local timeline (analogous to an animation clip). A clip is sometimes called
a sound buffer, because the digital sample data is stored in a buffer. A clip
might encapsulate monophonic audio data (typical for 3D sound assets), or it
might contain multi-channel audio (typically used for 2D assets or stero sound
sources in 3D). A clip may be stored in any of the audio ﬁle formats supported
by your engine.
13.5.7.2
Sound Cues
A sound cue is a collection of audio clips plus metadata that describes how
they should be processed and played. Cues are usually the primary means
by which the game can request sounds to be played. (Playing individual clips

818
13. Audio
may or may not be supported by the engine.) Cues also serve as a convenient
division-of-labor mechanism: The sound designers can craft the cues using an
ofﬂine tool, without having to worry about how or when they’ll be played in-
game. And the game programmers can play the cues conveniently in response
to relevant events in the game, without having to worry about micromanaging
the details of playback.
There are many ways in which the collection of clips in a cue could be
interpreted and played back. A cue might contain clips representing the six
channels of a pre-mixed 5.1 music recording. A cue might also collect up a
set of raw sounds, from which a random selection can be made, for the sake
of variety. A cue might also be set up to play its collection of raw sounds
in a predeﬁned sequence. A cue typically speciﬁes whether the sound(s) it
encapsulates represent a one-shot sound or a looping sound.
Some audio engines permit a cue to provide one or more optional sound
clips that only play if the main sound is interrupted midway through playing.
For example, a vocal cue might include a “glottal stop” sound that is played
only if the person’s line of dialog is interrupted. This feature can also be used
to provide a distinct “tail” sound that is played when a looping cue is stopped.
For example, a looping machine gun sound cue might use this “tail” clip fea-
ture to produce a suitable echoing fall-off sound when the ﬁring ceases.
A cue’s metadata might include whether it is intended to be played in
3D or 2D; the FO min, FO max and fall-off curve of the sound source; group
membership (see Section 13.5.8.1); and possibly any special effects, ﬁltering
or equalization that should be applied when the sound is played. In Sony’s
Scream engine—the sound engine used by Naughty Dog in its Uncharted se-
ries and The Last of Us—a cue can contain arbitrary script code that allows a
sound designer to completely control how the encapsulated sound asset(s) are
played when the cue plays.
Playing a Cue
Every audio engine that supports the concept of cues provides an API for
playing them. This API is usually the primary way—and sometimes the only
way—for the game code to request that a sound be played.
The cue playing API generally allows the programmer to specify whether
the cue should be played as a 2D or 3D sound, to provide 3D position and
velocity parameters, to specify whether the sound should loop or play only
once, and to specify whether the source buffer is in-memory or streamed. The
API usually also allows us to control the volume of the sound and possibly
other aspects of playback.

13.5. Audio Engine Architecture
819
Most APIs return a sound handle to the caller. This handle allows the pro-
gram to keep track of a sound as it is playing, so that it can be modiﬁed or
canceled before the sound ends. A sound handle is usually implemented
under the hood as an index into a global handle table, rather than as a raw
pointer to the data that descibes the sound instance. That way, if the sound
ends naturally, the handle can be “nulled out” automatically. A handle mech-
anism can also be used to make the system thread-safe—if one thread kills the
sound, other threads that have handles to the sound will automatically see
their handles become invalid.
13.5.7.3
Sound Banks
A 3D audio engine manages a lot of assets. The game world contains a large
number of objects. Each object can generate a variety of sounds. And in ad-
dition to 3D sound effects, we have music, speech, menu sound effects and so
on.
All of this audio data takes up an immense amount of space, so we can’t
keep it all in memory at once. On the other hand, the individual audio clips
are too ﬁne-grained and too numerous for them to be managed on an indi-
vidual basis. As such, most game engines package their sound clips and cues
into coarse units called sound banks.
Some sound banks are loaded when the game starts up and left in memory
forever. For example, the collection of sounds made by the player character
are always needed, so we could keep them in memory indeﬁnitely. Other
banks might be loaded and unloaded dynamically as the needs of the game
change. For example, the sounds in level A might not be used in level B, so
we can load the “A” bank only when level A is being played. For example,
in Naughty Dog’s The Last of Us, the sounds of rain, ﬂowing water and the
creaking of beams on the verge of collapse were only loaded when the player
was in the tilted building in Boston.
Some sound engines permit banks to be relocated in memory. This feature
can entirely eliminate the memory fragmentation problems that would other-
wise arise as lots of differently sized banks are loaded and unloaded during
gameplay. See Section 5.2.2.2 for more information on memory relocation.
13.5.7.4
Streaming Sounds
Some sounds are so long in duration that they cannot be conveniently stored
in memory all at once. Music and speech are common examples. For these
kinds of sounds, many game engines support streaming audio.
Streaming audio is possible because when playing a sound, the only in-
formation we actually need is the signal data at and around the current time

820
13. Audio
index. To implement streaming, we maintain a relatively small ring buffer
for each streaming sound. Prior to playing the sound, we pre-load a small
chunk of it into the buffer, then play the sound normally. The audio pipeline
consumes the data from the ring buffer as it plays, making room for us to
load more data. As long as we keep ﬁlling the buffer with data before it is all
consumed, our sound will play seamlessly.
13.5.8
Mixing Your Game
If we were to play every sound coming from every game object, properly at-
tenuated and spatialized and acoustically modeled, using all the techniques
and technology we’ve discussed thus far, what would be the result?
We
might expect the answer to this question to be “an incredibly immersive and
believable soundscape that wins awards and makes us rich!” But what we’d
actually get is cacophony.
What separates a good game from a great game is the mix—what you hear,
how much of it you hear, and just as importantly what you don’t hear. The
goal of a game’s sound designer is to produce a ﬁnal mix that:
•
sounds realistic and emersive;
•
isn’t too distracting, annoying or difﬁcult to listen to;
•
conveys all information relevant to gameplay and/or story effectively;
and
•
maintains a mood and tonal color that is always appropriate, given the
events taking place in the game and the overall design of the game.
All sorts of different kinds of sounds must come together in the game’s
mix. These include music, speech, ambient sounds like rain, wind, insects or
the creak of an old building, sound effects such as weapons ﬁre, explosions
and vehicles, and the bumps, slides and rolling sounds made by physically
simulated objects.
Various techniques are employed to ensure the mix of the game meets
these goals. We’ll explore a few of them in the following sections.
13.5.8.1
Groups
The most obvious thing we can do to improve the mix of our game is to set
the levels of all the source sounds in the 3D world appropriately. The impor-
tant thing here is to ensure that each sound’s gain is appropriate relative to
the other sounds in the game. For example, footsteps should be quieter than
gunﬁre.

13.5. Audio Engine Architecture
821
In some games, the loudness of certain sounds needs to change dynami-
cally. Often we want to control an entire category of sounds at once. For ex-
ample, during a frenetic ﬁght sequence, we might want to bring up the levels
of the music and the weapons, and drop the volume of ancillary sound effects.
Or during a quiet moment with characters talking to one another, we might
want to boost the speech a little and tone down ambient sounds to ensure the
dialog can be heard.
For this reason, many audio engines support the concept of groups—a con-
cept “stolen” from our old friend the multi-channel mixing console. On a mix-
ing board, a collection of sound inputs can be routed to an intermediate mixer
circuit, combining them into a single “group signal.” The gain of this signal
can then be controlled with a single knob on the board, thereby allowing the
sound engineer to control the loudness of all input signals at once.
In the software world, groups are implemented by simply categorizing
sound cues, rather than physically mixing their signals together. For example,
we can classify a cue as being music, a sound effect, a weapon, a line of speech
and so on. Then, the engine can provide a means of controlling the gains of all
sounds in each category with a single control value. Groups usually also allow
entire categories of sound to be paused, restarted and muted conveniently
with a simple API call.
Some sound engines do also provide a mechanism for physically mixing
groups of audio signals into a single signal, just as is done when working with
groups on a mixing console. In Sony’s Scream engine, this is called generating
a pre-master submix. After the relative gains of the signals in the group have
been locked down by the submix, the resulting signal can be routed through
additional ﬁlters or other processing stages. This gives the sound designer
even more control over the mix of the game.
13.5.8.2
Ducking
Ducking is a technique in which the volume/gain of certain sounds are tem-
porarily reduced in order to make other sounds more audible. For example,
when a character is speaking, the level of background noise could be reduced
automatically to make the dialog more audible.
Ducking can be triggered in numerous ways. The presence of one particu-
lar type of sound might be used to duck another category of sounds. A game
event might trigger a duck programmatically. Any triggering mechanism that
is deemed appropriate can be used to initiate a duck.
The reduction of volume caused by a duck is typically accomplished via
the group categorization system: When one category of sound is playing, it
can automatically duck one or more other categories by various amounts. Or

822
13. Audio
the game code can call a function to duck a group of sounds programmatically.
Ducking can also be performed by routing one sound signal into the side-
chain input of the dynamic range compressor (DRC) on a different voice’s bus.
Recall from Section 13.5.5 that a DRC analyzes the volume characteristics of a
signal and automatically compresses the loudness of the signal appropriately.
When a side-chain input is connected to a DRC, it analyzes the side-chain sig-
nal when deciding how to adjust the volume. So, we can arrange for increased
loudness in one signal to cause a decrease in the dynamic range of another sig-
nal.
13.5.8.3
Bus Presets and Mix Snapshots
Many sound engines allow the sound designer to set up conﬁguration param-
eters, save them off and then recall and apply them conveniently at runtime.
In Sony’s Scream engine, these come in two basic ﬂavors: bus presets and mix
snapshots.
A bus preset is a set of conﬁguration parameters that control aspects of the
components on a single bus (voice bus or master output bus). For example,
a bus preset might describe one particular reverb set-up that mimics, say, the
acoustics of a large open hall, or the interior of a car, or a small broom closet.
Or a bus preset might control the DRC settings on the master output bus.
Many such presets can be created by the sound designer, and the appropriate
ones activated at runtime as the game requires.
A mix snapshot is the same kind of idea applied to gain control. The gains
of the various channels within a group can be established a priori and then
applied at runtime as needed.
13.5.8.4
Instance Limiting
Instance limiting is a means of controlling the number of sounds that are per-
mitted to play simultaneously. For example, even though 20 NPCs are all ﬁr-
ing their guns at once, we might only play the three or four gun sounds that
are nearest to the listener. Instance limiting is important for two reasons: First,
it’s a great way to prevent cacophony. Second, a sound engine typically sup-
ports only a ﬁxed number of simultaneous voices, either because of hardware
limitations (e.g., the sound card only has N codecs) or because of memory or
processor bandwidth limitations in the software, so we must use them wisely.
Per-Group Limits
Instance limiting is sometimes applied differently to different groups of
sounds. For example, we might specify that we should play up to four guns

13.5. Audio Engine Architecture
823
simultaneously, hear up to three people speaking at a time and allow up to
ﬁve other sound effects to play at once, plus up to two overlapping music
tracks.
Prioritization and Voice Stealing
In a 3D game with lots of dynamic elements, there may be more sounds play-
ing at any given time than the system has voices for. Some sound engines sup-
port a large number (or even an inﬁnite number) of virtual voices. Each virtual
voice represents a sound that is technically playing, but that can be muted
or stopped temporarily so it ceases to occupy valuable hardware or software
resources. The engine uses various criteria to dynamically determine which
virtual voices should be mapped to “real” voices at any given moment.
One of the simplest ways to limit the number of sounds playing simul-
taneously is to assign a maximum radius to every 3D sound source. As we
saw in Section 13.4.3.1, this is the FO max radius. If the listener is beyond
this distance from the sound, it is considered inaudible and its virtual voice
is temporarily muted or stopped, freeing its resources for use by other voices.
The process of automatically silencing virtual voices is called voice stealing.
Another common approach is to assign each cue or group of cues a priority.
When too many virtual voices are playing at once, those with lower priorities
can be silenced (stolen) in favor of higher priority voices.
Sound engines may also provide various other mechanisms for controlling
the details of the voice stealing algorithm. For example, a cue might be given
a minimum play time, after which its voice is permitted to be stolen. Sounds
might be faded out rather than cut off abruptly when their voice is stolen.
And some cues might be temporarily marked as “unstealable” to ensure that
they play, even despite their priority settings.
13.5.8.5
Mixing In-Game Cinematics
Under normal gameplay conditions, the listener or “virtual microphone” is
typically positioned at or near the location of the camera, and the sound
sources are modeled where they really are in the environment. Distance-based
attenuation, direct and indirect sound path determination, voice limiting—all
are determined using these realistic positions.
However, during an in-game cinematic—a portion of the game in which
player control is suspended so that a story moment can take place—the cam-
era often pans out away from the player’s head. This kind of thing tends to
wreak havoc with our 3D audio system. We could just keep the listener/mic
locked to the location of the camera; but this is not always appropriate. For

824
13. Audio
example, if there’s a long shot of two characters speaking, we probably still
want to mix so that the characters’ voices can be heard, even though physi-
cally speaking they are too far away to be heard. In this case, we might want
to detach the listener from the camera, and artiﬁcially position it nearer to the
characters.
Mixing in-game cinematics is a lot closer to mixing a ﬁlm. As such, a sound
engine needs to be capable of “breaking the rules” and doing things that aren’t
necessarily physically realistic.
13.5.9
Audio Engine Survey
It should be evident by now that creating a 3D audio engine is a massive
undertaking. Luckily for us, lots of people have already put a great deal of
effort into this task, and the result is a wide range of audio software that we
can use pretty much out of the box. This ranges from low-level sound libraries
all the way to fully featured 3D audio rendering engines.
In the following sections, we’ll survey a few of the most common audio
libraries and engines. Some of these are speciﬁc to a particular target platform,
while others are cross-platform.
13.5.9.1
Windows: The Universal Audio Architecture
In the early days of PC gaming, the feature set and architecture of PC sound
cards varied a great deal from platform to platform and vendor to vendor.
Microsoft attempted to encapsulate all of this diversity within its DirectSound
API, supported by the Windows Driver Model (WDM) and the Kernel Audio
Mixer (KMixer) driver. However, because vendors could not agree on a com-
mon feature set or set of standard interfaces, the same functionality would
often be realized in very different ways on different sound cards. This re-
quired the operating system to manage a very large number of incompatible
driver interfaces.
For Windows Vista and beyond, Microsoft introduced a new standard
called the Universal Audio Architecture (UAA). Only a limited set of hard-
ware features are supported by the standard UAA driver API—all remain-
ing features are implemented in software (although hardware manufacturers
are still free to provide additional “hardware acceleration” features, as long
as they provide custom drivers to expose them). Although the introduction
of UAA limited the competitive advantage of prominent sound card vendors
like Creative Labs, it did have the desired effect of creating a solid, feature-rich
standard, which could be used by games and PC applications in a convenient
way.

13.5. Audio Engine Architecture
825
The UAA standard had another positive effect on the user’s aural experi-
ence. In the old DirectSound days, a game could take complete control of the
sound card, meaning that sounds coming from the OS or other applications
such as an email program would be “locked out” and their sounds would not
play while the game was running. The new UAA architecture allowed the OS
to claim ultimate control over the ﬁnal mix heard through the PC’s speakers.
Multiple applications could ﬁnally share the sound card in a reasonable and
consistent way.
See http://en.wikipedia.org/wiki/Technical_features_new_to_Windows_
Vista#Audio_stack_architecture
and
http://msdn.microsoft.com/en-us/
library/windows/hardware/gg463030.aspx for more information on UAA.
The UAA is implemented on Windows by the so-called Windows Audio
Session API, or WASAPI for short. This API is not really intended for use by
games. It supports most advanced audio processing features in software only,
with limited support for hardware acceleration. Instead, games usually make
use of the XAudio2 API, which is described in the next section.
13.5.9.2
XAudio2
XAudio 2 is the high-powered low-level API that provides access to the au-
dio hardware on Xbox 360, Xbox One and Windows. It replaces DirectAudio
and provides access to a wide range of hardware-accelerated features includ-
ing programmable DSP effects, submixing, support for a wide range of com-
pressed and uncompressed audio formats, and multirate processing to lighten
the load on the main CPU(s).
Atop the XAudio2 API sits a 3D audio rendering library called X3DAudio.
A powerful ofﬂine tool called the “cross-platform audio creation tool” or XACT
for short is used to author assets for use with XAudio2 and X3DAudio. XACT
assets can also be used from within Microsoft’s XNA game development plat-
form.
These APIs are also available on the Windows platform for use by
PC games. You can read more about XACT at http://msdn.microsoft.com/
en-us/library/ff827592.aspx.
A screenshot of the XACT audio tool is shown in Figure 13.42.
13.5.9.3
Scream and BoomRangBuss
On the PS3 and PS4, Naughty Dog uses Sony’s 3D audio engine Scream and
its synth library, BoomRangBuss.
The audio hardware on a PlayStation 3 is very much like a UAA-compliant
audio device, supporting up to eight channels of audio for full 7.1 surround
sound support, plus a hardware mixer and HDMI, S/PDIF, analog and USB/
Bluetooth outputs. This audio hardware is encapsulated by a collection of

826
13. Audio
Figure 13.42. Microsoft’s XACT audio authoring tool for Windows and Xbox platforms.
OS-level libraries including libaudio, libsynth and libmixer. On top of these
libraries, game makers are free to implement their own audio software stacks.
Sony also provides a powerful 3D-capable audio stack of its own called Scream
which game studios can use “out of the box.” Scream is available on the
PS3, PS4 and PSVita platforms. Its architecture mimics a fully featured multi-
channel mixer console.
On top of Scream, Naughty Dog has implemented a proprietary 3D envi-
ronmental audio system for use on the Uncharted series and its latest new IP,
The Last of Us. This system provides stochastic obstruction/occlusion mod-
eling and a portal-based audio rendering system that permits rendering a
highly realistic soundscape.
Advanced Linux Sound Architecture
The Linux equivalent of the UAA driver model is called the Advanced Linux
Sound Architecture (ALSA). This Linux kernel component replaced the orig-
inal Open Sound System (OSSv3) as the standard way to expose audio func-
tionality to applications and games. See http://www.alsa-project.org/main/
index.php/Main_Page for more information on ALSA.

13.5. Audio Engine Architecture
827
QNX Sound Architecture
QNX Sound Architecture (QSA) is a driver-level audio API for the QNX Neu-
trino real-time OS. As a game programmer, you’ll probably never use QNX.
But its documentation does provide an excellent picture of the concepts
and the typical feature set of audio hardware. See http://www.qnx.com/
developers/docs/6.5.0/index.jsp?topic=%2Fcom.qnx.doc.neutrino_audio%2
Fmixer.html for these docs.
13.5.9.4
Multiplatform 3D Audio Engines
A number of powerful, ready-to-use cross-platform 3D audio engines are avail-
able. We’ll outline the most well-known of these below.
•
OpenAL is a cross-platform 3D audio rendering API that has been de-
liberately designed to mimic the design of the OpenGL graphics library.
Early versions of the library were open source, but it is now licensed
software. A number of vendors provide implementations of the Ope-
nAL API spec, including OpenAL Soft (http://kcat.strangesoft.net/
openal.html and AeonWave-OpenAL (http://www.adalin.com).
•
AeonWave 4D is a low-cost audio library for Windows and Linux by
Adalin B.V.
•
FMOD Studio is an audio authoring tool that features a “pro audio” look
and feel (http://www.fmod.org). A full-featured runtime 3D audio API
allows assets created in FMOD Studio to be rendered in real time on the
Windows, Mac, iOS and Android platforms.
•
Miles Sound System is a popular audio middleware solution by Rad Game
Tools (http://www.radgametools.com/miles.htm). It provides a power-
ful audio processing graph and is available on virtually every gaming
platform imaginable.
•
Wwise is a 3D audio rendering engine by Audiokinetic (https://www.
audiokinetic.com). It is notably not based around the concepts and fea-
tures of a multi-channel mixing console, but rather presents the sound
designer and programmer with a unique interface based on game ob-
jects and events.
•
UnrealEngine of course provides its own 3D audio engine and powerful
integrated tool chain (http://www.unrealengine.com). For an in-depth
look at Unreal’s audio feature set and tools, see [40].

828
13. Audio
13.6
Game-Speciﬁc Audio Features
On top of the 3D audio rendering pipeline, games typically implement all
sorts of game-speciﬁc features and systems. Some examples include:
•
Split-screen support. Multiplayer games that support split-screen play
must provide some mechanism that allows multiple listeners in the 3D
game world to share a single set of speakers in the living room.
•
Physics-driven audio. Games that support dynamic, physically simulated
objects like debris, destructible objects and rag dolls require a means of
playing appropriate audio in response to impacts, sliding, rolling and
breaking.
•
Dynamic music system. Many story-driven games require the music to
adapt in real time to the mood and tension of events in the game.
•
Character dialog system. AI-driven characters seem a great deal more re-
alistic when they speak to one another and to the player’s character.
•
Sound synthesis. Some engines continue to provide the ability to synthe-
size sounds “from scratch” by combining various kinds of waveforms
(sinusoid, square, sawtooth, etc.) at various volumes and frequencies.
Advanced synthesis techniques are also becoming practical for use in
real-time games. For example:
◦
Musical instrument synthesizers reproduce the natural sound of an
analog musical instrument without the use of pre-recorded audio.
◦
Physically based sound synthesis encompasses a broad range of tech-
niques that attempt to accurately reproduce the sound that would
be made by an object as it physically interacts with a virtual en-
vironment.
Such systems make use of the contact, momentum,
force, torque and deformation information available from a mod-
ern physics simulation engine, in concert with the properties of the
material from which the object is made and its geometric shape,
in order to synthesize suitable sounds for impacts, sliding, rolling,
bending and so on. Here are just a few links to research on this fas-
cinating topic: http://gamma.cs.unc.edu/research/sound, http://
gamma.cs.unc.edu/AUDIO_MATERIAL, http://www.cs.cornell.
edu/projects/sound, and https://ccrma.stanford.edu/∼bilbao/
booktop/node14.html.
◦
Vehicle engine synthesizers aim to reproduce the sounds made by
a vehicle, given inputs such as the acceleration, RPM and load
placed on a virtual engine, and the mechanical movements of the

13.6. Game-Speciﬁc Audio Features
829
vehicle. (The vehicle chase sequences in Naughty Dog’s three Un-
charted games all used various forms of dynamic engine modeling,
although technically these systems were not synthesizers, because
they produced their output by cross-fading between various pre-
recorded sounds.)
◦
Articulatory speech synthesizers produce human speech “from scratch”
via a 3D model of the human vocal tract. VocalTractLab (http://
www.vocaltractlab.de) is a free tool that allows students to learn
about and experiment with vocal synthesis.
•
Crowd Modeling. Games that feature crowds of people (audiences, city
dwellers, etc.) require some means of rendering the sound of that crowd.
This is not as simple as playing lots and lots of human voices over top
of one another. Instead, it is usually necessary to model the crowd as
multiple layers of sounds, including a background ambience plus indi-
vidual vocalizations.
We can’t possibly cover everything from the above list in one chapter.
But let’s spend a few more pages covering some of the most common game-
speciﬁc features.
13.6.1
Supporting Split-Screen
Supporting split-screen multiplayer is a tricky problem, because you have
multiple listeners in the virtual game world, but they must share a single set
of speakers in the player’s living room. If you simply pan the sounds mul-
tiple times, once for each listener, and then mix the results into the speakers
evenly, the result won’t always sound sensible. There is no perfect solution:
For example, if player A is standing right next to an explosion and player B
is standing far away, the person playing player B will still hear the explosion
loud and clear. The best a game can do is cobble together a hybrid solution, in
which some sounds are handled in a “physically correct” way and others are
“fudged” in order to produce the most sensible-sounding experience for the
players.
13.6.2
Character Dialog
Even if we’ve created characters for our game that look like photographs of
real human beings, and even if they move in astoundingly realistic ways, they
still won’t seem real to the player until they can speak realistically. Speech
communicates information crucial to gameplay. It’s a central story-telling tool.
And it cements the emotional bond between the human player the characters

830
13. Audio
in the game. Speech can also be the deciding factor in the player’s perception
of intelligence among the AI-controlled characters in a game.
At the Game Developer’s Conference (GDC) in 2002, Chris Butcher and
Jaime Griesemer of Bungie gave a talk entitled, “The Illusion of Intelligence:
The Integration of AI and Level Design in Halo” (http://bit.ly/1g7FbhD). In
their talk, they shared an anecdote about how important speech can be to com-
municating the motivations of an AI-driven character to the player. In Halo,
when the Elite leader of a squad of Covenant was killed, the grunts would all
run away in fear. In playtest after playtest, no one seemed to understand that
it was the killing of the Elite that had triggered the grunts to ﬂee. Finally, the
grunts were given lines of dialong saying something to the effect of, “Leader
dead—run away!” Only then did play testers start to really grok what was
going on!
In this section, we’ll explore some of the fundamental subsystems you’ll
ﬁnd in the character dialog system of pretty much any character-based game.
We’ll also discuss some of the speciﬁc techniques and technologies used by
Naughty Dog to create rich, realistic conversations in their latest game, The
Last of Us. For more information and in-game videos of Naughty Dog’s char-
acter dialog system in action, check out the talk I gave at GDC 2014 entitled,
“Context-Aware Character Dialog in The Last of Us,” available in PDF and
QuickTime formats at http://www.gameenginebook.com.
13.6.2.1
Giving a Character a Voice
It’s easy enough to give a game character a voice—simply play an appropriate
pre-recorded sound whenever the character needs to speak. However, things
are never quite that simple. The dialog system in a game engine is typically a
reasonably complex beast. Here are just a few of the reasons why:
•
We need a way to catalog all the possible lines of dialog that each char-
acter might be called upon to say, and give each of these lines some kind
of unique id so that the game can trigger them when needed.
•
We need to make sure that each uniquely identiﬁable character in the
game speaks with a recognizable and consistent voice. For example,
each of the hunters in the Pittsburgh section of The Last of Us was as-
signed to one of eight unique voices, so that no two hunters in a battle
would sound the same.
•
We may not know a priori which character is going to be called upon
to say a speciﬁc line, so we often need to record the same line multiple
times, spoken by various voice actors, so that the appropriate voice can
be used to say the line when needed.

13.6. Game-Speciﬁc Audio Features
831
•
We also usually want a lot of variety in the things that are said. So most
dialog systems provide a means of selecting speciﬁc lines at random
from a pool of possibilities.
•
Speech audio assets tend to be of relatively long duration, which means
they occupy a lot of memory. Many lines of dialog are part of cinematic
sequences, and hence are only spoken once in the entire game. For these
reasons, it’s usually wasteful to store speech assets in memory. Instead,
it’s typical for speech audio assets to be streamed on demand (see Section
13.5.7.4).
Usually other vocal sounds, like the “efforts” made when lifting something
heavy, jumping over an obstacle or getting punched in the gut, are handled
by the same system that handles spoken dialog. This is done largely because
a character’s efforts need to match his or her spoken voice. So we may as well
leverage the dialog system to produce effort sounds as well.
13.6.2.2
Deﬁning a Line of Dialog
Most dialog systems introduce a level of indirection between a request to speak
and the choice of the particular audio clip to play. The game programmer or
designer requests logical lines of dialog, each of which is represented by a
unique identiﬁer such as a string, or better yet a hashed string id (see Section
5.4.3.1). The sound designers can then “ﬁll out” each logical line with one
or more audio clips in order to provide the necessary variety both in voice
quality and in terms of what exactly is said.
For example, let’s imagine a logical line in which the character says some-
thing to the effect of, “I’m out of ammo.” We’ll assign this logical dialog line
the unique id 'line-out-of-ammo, where the leading single quote indi-
cates a hashed string id. Let’s assume also that there are ten different char-
acters that might say this line: the player character (call him “drake”), the
player’s sidekick (call her “elena”) and up to eight enemy characters (call
them “pirate-a” through “pirate-h”). We’ll need some kind of data structure
to deﬁne all the physical audio assets that make up this one logical line of
dialog.
At Naughty Dog, sound designers use the Scheme programming language
to deﬁne logical dialog lines using a custom syntax. We’ll use a similar syntax
in our example below. However, the speciﬁcs of the implementation are not
important here. All we’re interested in is the structure of the data itself:
(define-dialog-line 'line-out-of-ammo
(character 'drake
(lines

832
13. Audio
drk-out-of-ammo-01
;; "Dammit, I'm out!"
drk-out-of-ammo-02
;; "Crap, need more bullets."
drk-out-of-ammo-03
;; "Oh, now I'm REALLY mad."
)
)
(character 'elena
(lines
eln-out-of-ammo-01
;; "Help, I'm out!"
eln-out-of-ammo-02
;; "Got any more bullets?"
)
)
(character 'pirate-a
(lines
pira-out-of-ammo-01
;; "I'm out!"
pira-out-of-ammo-02
;; "Need more ammo!"
;; ...
)
)
;; ...
(character 'pirate-h
(lines
pirh-out-of-ammo-01
;; "I'm out!"
pirh-out-of-ammo-02
;; "Need more ammo!"
;; ...
)
)
)
Rather than deﬁne your dialog lines in one monolithic data structure like
the one shown above, it’s usually better to break the lines out into separate
ﬁles by character. For example, all of Drake’s lines can be managed in one ﬁle,
Elena’s in another ﬁle and all the pirates’ lines can be stored in a third ﬁle.
This helps to prevent the sound designers from stepping on each others’ toes.
It also means that we can manage our memory more efﬁciently. For example,
if there are no pirates in a given section of the game, there’s no need to keep
the data for the pirates’ dialog lines in memory. It’s also a good idea to split
the dialog data up by level, for the same reason.
13.6.2.3
Playing a Line of Dialog
Given this data, the dialog system can easily convert a request for a logical
line of dialog such as 'line-out-of-ammo into a speciﬁc audio clip. It sim-
ply looks up the character’s speciﬁc voice id in the table, and then makes a
random choice amongst the various possible lines for that character.

13.6. Game-Speciﬁc Audio Features
833
It’s usually a good idea to put in place some kind of mechanism to ensure
that lines aren’t repeated too often. One way to accomplish this is to store the
indices of the various lines in an array and then randomly shufﬂe its contents.
To select a line, we simply cycle through the shufﬂed array in order. Once
all possible lines have been exhausted, we reshufﬂe the array, taking care that
the most recently played line doesn’t end up in the ﬁrst slot. This prevents all
repetition while keeping the line selections sounding random.
Dialog line requests are typically made by gameplay code in C++, Java,
C# or whatever language your game is written in. Game designers may also
request lines of dialog via script (Lua, Python, etc.) The dialog system’s API
is usually designed with simplicity of use in mind. If an AI programmer or
game designer has to jump through a lot of hoops just to get a line of dialog
to play, you may discover that your characters are uncannily silent! It’s best
to provide a simple, ﬁre-and-forget interface. Leave all the hard work to the
programmer who is crafting the dialog system.
For example, in Uncharted 3: Drake’s Deception, C++ code could request a
character to play a line of dialog by calling a simple PlayDialog() member
function of the Npc class. These calls would be peppered throughout the AI
decision-making code in order to trigger appropriate lines of dialog at key
moments in the game. For example:
void Skill::OnEvent(const Event& evt)
{
Npc* pNpc = GetSelf(); // grab a pointer to the NPC
switch (evt.GetMessage())
{
case SID('player-seen'):
// play a line of dialog...
pNpc->PlayDialog(SID('line-player-seen'));
// ... and move to closest cover
pNpc->MoveTo(GetClosestCover());
break;
// ...
}
// ...
}
13.6.2.4
Priority and Interruption
What happens if a character is asked to speak while he’s already speaking?
What if he receives more than one speech command on the same frame? In
both cases, a priority system is a good way to resolve ambiguities.

834
13. Audio
To implement such a system, we simply assign each line of dialog to a
priority level. When a request to say a line of dialog comes in, the system
looks at the priority of the currently playing line if any, and the priorities of the
line or lines that have been requested this frame. It ﬁnds the highest priority
line among these. If the currently playing line “wins,” it continues to play and
the requested lines are ignored. If one of the requested lines is higher priority
than the current line, or if the character isn’t speaking yet, the new line plays,
interrupting the current line if necessary.
Implementing the interruption of the speech itself is actually a bit tricky.
We can’t perform a cross-fade (i.e., fade the volume of the playing sound
down and the new sound up) because this sounds strange and wrong when
applied to the speech of a single character. Ideally, we’d want to play at least
some kind of glottal stop sound just prior to starting the new line. It might
even be appropriate to play a short phrase indicating that the character is sur-
prised and/or annoyed by the interruption, and then play the new line of dia-
log. The dialog system in The Last of Us doesn’t do any of these fancy things. It
simply stops the current line and immediately plays the new one. This sounds
pretty good most of the time. Of course, each game has its own unique speech
patterns, and what works in one game may not work well in another. So as
the saying goes, “Your milage may vary.”
13.6.2.5
Conversations
In The Last of Us, Naughty Dog wanted the enemy NPCs to sound like they’re
having real conversations with one another. This meant that the characters
would need to be capable of saying relatively long chains of lines, with back-
and-forth banter between two or more characters.
Conversations in The Last of Us are constructed from logical segments. Each
segment corresponds to one logical line, spoken by one particular actor in
the conversation. Each segment is given a unique id, and the segments are
chained together into a conversation via these ids. As an example, let’s see
how we would deﬁne the following conversation:
A:
“Hey, did you ﬁnd anything?”
B:
“No, I’ve been looking for an hour and I ain’t found nothin’.”
A:
“Well then shut up and keep looking!”
This conversation could be expressed in the Naughty Dog conversation sys-
tem as follows:

13.6. Game-Speciﬁc Audio Features
835
(define-conversation-segment 'conv-searching-for-stuff-01
:rule []
:line 'line-did-you-find-anything
;; "Hey, did you find anything?"
:next-seg 'conv-searching-for-stuff-02
)
(define-conversation-segment 'conv-searching-for-stuff-02
:rule []
:line 'line-nope-not-yet
;; "I've been looking for an hour..."
:next-seg 'conv-searching-for-stuff-03
)
(define-conversation-segment 'conv-searching-for-stuff-03
:rule []
:line 'line-shut-up-keep-looking
;; "Well then shut up and keep looking!"
)
This syntax might seem a bit verbose at ﬁrst glace. But as we’ll see in Sec-
tion 13.6.2.8, breaking the conversation out like this gives us a great deal of
ﬂexibility. For example, it allows branching conversations to be deﬁned in a
natural and reasonably convenient way.
13.6.2.6
Interrupting Conversations
We saw in Section 13.6.2.4 that a simple priority system can be used to handle
interruptions and to resolve contention when more than one logical line is
requested simultaneously.
When conversations are in play, a priority system can still be used. But its
implementation is a bit more complex in this case. For example, imagine a
conversation between characters A and B. A says his line, then B says her
line while A waits his turn. During the time that B is speaking, A is asked
to play an entirely different line of dialog. He’s not tehnically speaking, so
by the rules for dialog prioritization, applied to each character individually,
there would be no problem and the line would play. But this could sound
very jarring, depending on what’s being said.
A:
“Hey, did you ﬁnd anything?”
B:
“No, I’ve been looking for an hour and...”
A:
“Look, a shiny object!”
(interruption by an unrelated line of dialog)
B:
“...I ain’t found nothin’.”

836
13. Audio
To overcome this issue on The Last of Us, we introduced the concept of
conversations as “ﬁrst-class entities.” When a conversation is started, the sys-
tem “knows” that each of the characters is involved in that conversation, even
when he or she isn’t speaking. Each conversation has a priority, and the prior-
itization rules are applied to entire conversations, not the individual lines on
a per-character basis. So for example, when charater A is asked to say, “Look,
a shiny object!” the system knows that he is currently involved in the “Hey,
did you ﬁnd anything?” conversation. Persumably the line “Look, a shiny
object!” is at the same or lower priority as the current conversation, so the
interruption isn’t allowed.
If the interrupting line is something higher priority like, “Holy cow, he’s
pointing a gun at us!” then the line is allowed to interrupt the existing conver-
sation. In that case, all of the characters in the conversation are interrupted.
The result is an interruption that sounds natural and intelligent.
A:
“Hey, did you ﬁnd anything?”
B:
“No, I’ve been looking for an hour and...”
A:
“Holy cow, he’s pointing a gun at us!”
(interruption by a higher-priority conversation)
B:
“Get him!”
(The original conversation is interrupted by the new one, and A and B go into
combat mode.)
13.6.2.7
Exclusivity
On The Last of Us, we also introduced the concept of exclusivity. Any line of
dialog or conversation can be marked as either non-exclusive, faction-exclusive
or globally exclusive. This mark-up controls how interruptions work for the
given line or conversation.
•
A non-exclusive line or conversation is permitted to play over top of other
lines or conversations. For example, during a search for the player, it’s
not a big problem if one hunter is mumbling to himself, “Huh, there’s
nothing over here.” while another hunter is saying, “I’m getting tired
of this.” The two hunters aren’t speaking to each other, so the overlap
sounds perfectly natural.
•
A faction-exclusive line or conversation interrupts all other lines or con-
versations within that character’s faction. For example, if the player
(Joel) is spotted during a search, the hunter that saw him might say,

13.6. Game-Speciﬁc Audio Features
837
“He’s over here!” The other hunters should immediately stop speaking,
because we want to make it seem as if the hunters can hear one another,
and also to communicate to the player that their collective focus has
shifted. However, if Joel’s companion Ellie is whispering a warning to
him at the time, we probably do not want to interrupt her. She is not part
of the hunter gang, and what she has to say to Joel is relevant whether
or not the hunters have spotted him.
•
A globally exclusive line or conversation interrupts all other lines, across
faction boundaries. This is useful in situations in which every character
within earshot should react to hearing whatever is being said.
13.6.2.8
Choices and Branching Conversations
It’s often desirable to allow conversations to play out in different ways de-
pending on what the player does, on what decisions the AI characters make
and/or on other aspects of game world state. When authoring or editing con-
versations, the writers and sound designers would like to have control not
only over which lines are said, but also over the logical conditions that control
which branch of the conversation will be taken at any given moment during
gameplay. This puts the creative power in the hands of the people who need
it, instead of forcing them to work through a programmer.
Naughty Dog implemented such a system for use on The Last of Us. It
was inspired in part by an earlier system developed by Valve and described
by Elan Ruskin in his talk, “Rule Databased for Contextual Dialog and Game
Logic,” which he delivered at the Game Developer’s Conference in 2012. The
talk is available here: http://www.gdcvault.com/play/1015317/AI-driven
-Dynamic-Dialog-through. Naughty Dog’s conversation system differs from
Valve’s in a number of signiﬁcant ways, but the core idea behind both systems
is similar. We’ll describe the Naughty Dog system here, since that’s the system
with which the author has the most experience.
In Naughty Dog’s conversation system, each segment of a conversation
can consist of one or more alternative lines of dialog. Each alternative within
the segment carries with it a selection rule. If the rule evaluates to true, that
alternative is selected; if the rule evaluates to false, the alternative is ignored.
A rule is comprised of one or more criteria. Each criterion is a simple logi-
cal expression that evaluates to a Boolean. The expressions ('health > 5)
and ('player-death-count == 1) are examples of criteria. If more than
one criterion is provided within a rule, they are logically combined using the
Boolean AND operator. A rule only evaluates to true when all of its criteria
evaluate to true.

838
13. Audio
Here’s an example of one segment of a conversation, with three alterna-
tives that depend upon the health of the character doing the talking:
(define-conversation-segment 'conv-player-hit-by-bullet
(
:rule [ ('health < 25) ]
:line 'line-i-need-a-doctor
;; "I'm bleeding bad... need a doctor!"
)
(
:rule [ ('health < 75) ]
:line 'line-im-in-trouble
;; "Now I'm in real trouble."
)
(
:rule [ ] ;; no criteria acts as an "else" case
:line 'line-that-was-close
;; "Ah! Can't let that happen again!"
)
)
Branching Dialog
By breaking a conversation into segments, each of which contains one or more
alternative lines, we open up the possibility of crafting branching conversa-
tions. For example, let’s consider a conversation in which Ellie (the player’s
companion in The Last of Us) asks Joel (the player character) if he’s all right
when he’s been shot at. If the player wasn’t actually hit by the bullet, the
conversation goes like this:
Ellie:
“Are you OK?”
Joel:
“Yeah, I’m ﬁne.”
Ellie:
“Geez. Keep your head down!”
If Joel has been hit, the conversation plays out differently:
Ellie:
“Are you OK?”
Joel:
“(panting) Not exactly.”
Ellie:
“You’re bleeding!”
We can express this branching conversation using the conversation syntax de-
scribed above:

13.6. Game-Speciﬁc Audio Features
839
(define-conversation-segment 'conv-shot-at--start
(
:rule [ ]
:line 'line-are-you-ok ;; "Are you OK?"
:next-seg 'conv-shot-at--health-check
:next-speaker 'listener ;; *** see comments below
)
)
(define-conversation-segment 'conv-shot-at--health-check
(
:rule [ (('speaker 'shot-recently) == false) ]
:line 'line-yeah-im-fine ;; "Yeah, I'm fine."
:next-seg 'conv-shot-at--not-hit
:next-speaker 'listener ;; *** see comments below
)
(
:rule [ (('speaker 'shot-recently) == true) ]
:line 'line-not-exactly ;; "(panting) Not exactly."
:next-seg 'conv-shot-at--hit
:next-speaker 'listener ;; *** see comments below
)
)
(define-conversation-segment 'conv-shot-at--not-hit
(
:rule [ ]
:line 'line-keep-head-down ;; "Geez. Keep your head down!"
)
)
(define-conversation-segment 'conv-shot-at--hit
(
:rule [ ]
:line 'line-youre-bleeding ;; "You're bleeding!"
)
)
Speaker and Listener
There’s a subtle aspect to what’s going on in the branching converstion above.
At any given moment in a two-person conversation, one person is the speaker
and the other is the listener. The roles of speaker and listener ping-pong back
and forth as the conversation progresses. In the ﬁrst segment of the conver-
sation, 'conv-shot-at--start, Ellie is the speaker and Joel is the listener.
When we chain to the next segment, 'conv-shot-at--health-check, we

840
13. Audio
specify the value 'listener for the ﬁeld :next-speaker. This tells the
system to use the current listener (Joel) as the next segment’s speaker, thereby
reversing the roles. In that segment, we check whether the speaker has been
shot recently via the criteria (('speaker 'shot-recently) == false)
and (('speaker 'shot-recently) == true).
But now Joel is the
speaker, so everything works out as we’d expect.
An abstract speaker/listener system doesn’t seem all that useful for a con-
versation between two principal characters like Joel and Ellie. But by keeping
the deﬁnition of the conversation abstract, we gain a signiﬁcant amount of
ﬂexibility. For one thing, we could use the same conversation speciﬁcation
to deﬁne a conversation in which Joel asks Ellie if she’s OK. This works be-
cause the entire conversation is deﬁned in a way that is independent of which
character is saying each line. Moreover, for enemy characters it’s absolutely
essential that conversations be deﬁned in a generic manner, because we don’t
know which speciﬁc characters will be doing the speaking a priori. For enemy
battle chatter, we typically select a pair of characters dynamically and ﬁre off
the conversation. It has to work, no matter which two characters are selected.
The speaker/listener system can be extended to two- or three-person con-
versations. The Naughty Dog conversation system supported up to three lis-
teners, although the vast majority of our conversations were between only
two characters.
Fact Dictionaries
The criteria within a rule reference symbolic quantities like 'health and
'player-death-count. These symbolic quantities are implemented under
the hood as entires in a dictionary data structure—basically a table containing
key-value pairs. We call these fact dictionaries. An example of a fact dictionary
is shown in Table 13.1.
You may have noticed in Table 13.1 that each value in the dictionary has an
associated data type. In other words, the values in the dictionary are variants.
A variant is a data object that is capable of holding values of various types,
Key
Value
Data Type
'name
'ellie
StringId
'faction
'buddy
StringId
'health
82
int
'is-joels-friend
true
bool
...
...
...
Table 13.1. An example of a fact dictionary.

13.6. Game-Speciﬁc Audio Features
841
much like a union in C or C++. However, unlike a union, a variant also
stores information about the type of data it currently contains. This allows us
to validate the type of a value prior to using it. It also lets us convert data from
one type to another. For example, if our variant holds the integer value 42,
we could ask the variant to return it to us as the ﬂoating-point value 42.0f
instead.
In The Last of Us, each character has its own fact dictionary containing facts
about the character itself like health, weapon type, awareness level and so on.
Each “faction” of characters also has a fact dictionary. This allows us to ex-
press facts about the faction as a whole, like how many characters remain
alive within the group. Finally, there is a singleton “global” fact dictionary
that contains information about the game as a whole, without respect to fac-
tion. Things like the amount of time spent playing, the name of the current
level or how many times the player has retried a particular task are all things
that can go into the global fact dictionary.
Criterion Syntax
When writing a criterion, the syntax allows for facts to be pulled from any
dictionary by name. For example, (('self 'health) > 5) tells the sys-
tem to grab the fact dictionary of the character itself, look up the value of the
'health fact in that dictionary and then check if it is greater than 5. Likewise,
(('global 'seconds-playing) <= 23.5) instructs the system to look
up the 'seconds-playing fact from the global fact dictionary, and check
that it is less than or equal to 23.5 seconds.
If the user doesn’t specify a dictionary explicity, as in ('health > 5),
the system searches for the named fact by following a predeﬁned search order.
Check the character’s fact dictionary ﬁrst. If that fails, try to ﬁnd it in the
dictionary that matches the character’s faction. Finally, if all else fails, look
for the fact in the global dictionary. This “search path” feature allows sound
designers to be as brief as possible when writing criteria (albeit with the loss
of some speciﬁcity and clarity in the rules).
13.6.2.9
Context-Sensitive Dialog
In The Last of Us, we wanted to have enemy characters call out the location of
the player in an intelligent way. If the player is hiding in a store, the enemies
should shout, “He’s in the store!” If he’s hiding behind a car, we want the bad
guys to say, “He’s behind that car!” This makes the characters sound incredi-
bly intelligent, yet it turns out to be a relatively simple thing to implement.
To make this work, the sound designers mark up our game worlds with
regions. Each region is tagged with one of two kinds of location tags. A speciﬁc

842
13. Audio
general: store
general: garage
general: street
specific: counter
specific: soda
machine
specific: car
specific: tree
specific:
cabinet
NPC2
Player
NPC1
“He’s by 
that tree!”
“He’s out in 
the street!”
Figure 13.43. General and speciﬁc regions for context-sensitive dialog line selection.
tag marks the region with a very specifc location like “behind the counter”
or “by the cash register.” A general tag marks the region with a more general
location like “in the store” or “in the street.”
To determine which line of dialog to play, the system determines within
which region the player is located, and within which region the enemy NPC
is located. If they are both in the same general region, the player’s speciﬁc tag
is used to select dialog lines. When the NPC and player reside in different
general regions, we fall back to using the player’s general region tag to select
the line. So if the enemy and the player are both in the store, we might select a
line like, “He’s by the window!” But if the NPC is in the store and the player
is out in the street, we might hear the NPC say, “He’s out in the street! Get
him!” See Figure 13.43 for an illustration of how this system works.
This very simple system proved incredibly powerful. It was difﬁcult to set
up due to the sheer number of combinations of lines that had to be recorded
and conﬁgured, but the ﬁnal result in-game was worth the effort.
13.6.2.10
Dialog Actions
Lines of dialog delivered without body language usually look uncanny and
unrealistic. Some dialog lines are delivered as part of a full-body animation—

13.6. Game-Speciﬁc Audio Features
843
an in-game cinematic for example. But some lines must be delivered while the
character is busy doing something else, like walking, running or ﬁring their
weapon. Ideally we’d like to spice up such lines of dialog with some gestures
to breathe life into them.
On The Last of Us, we implemented a gesture system using additive anima-
tion technology (see Section 11.6.5). These gestures could be explicitly called
out by C++ code or script. In addition, each line of dialog could have a script
associated with it whose timeline was synchronized with the audio. This al-
lowed us to trigger gestures at precise moments during key lines of dialog.
13.6.3
Music
Music is an incredibly important aspect of pretty much any good game. It
sets the tone, drives the player’s sense of tension, and can make (or break) an
emotional scene. A game engine’s music system is typically charged with the
following duties:
•
Provide the ability to play back music tracks as streaming audio clips
(because music clips are almost always too large to ﬁt in memory).
•
Provide musical variety.
•
Match the music to the events occurring in the game.
•
Seamlessly transition from one piece of music to the next.
•
Mix the audio with the other sounds in the game in a suitable and pleas-
ing manner.
•
Allow music to be temporarily ducked to enhance the audibility of spe-
ciﬁc sounds or conversations in-game.
•
Permit brief pieces of music or sound effects known as stingers to tem-
porarily interrupt the currently playing music track.
•
Allow music to be paused and restarted. (You don’t need a full orchestra
playing a grandiose theme during every single second of gameplay, you
know!)
We generally expect the music to change to match the changing levels of
tension and/or emotional moods of the events happening in the game. One
way to accomplish this is to create multiple playlists, each of which is intended
for a different level of tension or emotional mood in the game. Each playlist
contains one or more pieces of music, from which selections may be made
randomly or sequentially. As the tension and mood changes in the game—as
battles begin and end, touching cutscenes come and go and so on—the music
system detects these changes and selects new music playlists as appropriate.

844
13. Audio
Some games implement a “stack” of music selections at increasing tension
levels—calm music for when no enemies are around, tense music when the
player is approaching an unsuspecting group of enemies, startling music at
ﬁrst contact and fast-paced music during battle.
Stingers are another way to match the music to the events in the game. A
stinger is a short musical clip or sound effect that can temporarily interrupt
the currently playing music track, or play over top of it while the main track’s
volume is ducked down. For example, the ﬁrst time the player makes line-
of-sight with a new enemy, we might want to play an ominous “rumbling”
sound to give the player a cue that danger is near. Or when the player dies,
we may want to quickly switch to a snippet of “death music.” Both of these
are situations in which a stinger might be used.
Transitioning smoothly between different music streams is somewhat of a
challenge. We cannot blindly cross-blend between totally unrelated pieces of
music and expect it to always sound good. The tempos of the two pieces may
not match, and the “beat” of one piece of music might not line up with that
of the next. The key is to time each transition properly. A rapid cross-fade
can be useful if the tempos don’t match; a longer cross-fade might work well
if the tempos are nearly identical. This takes some trial and error to get right.
Even getting a piece of music to loop properly requires some tweaking by the
sound engineer.
The topic of game music is a broad one, and we can’t really do it justice
here. If you are interested in learning more, [40] is a great book to start with.

Part IV
Gameplay

This page intentionally left blank
This page intentionally left blank

14
Introduction to
Gameplay Systems
U
p until now, everything we’ve talked about in this book has focused
on technology. We’ve learned that a game engine is a complex, layered
software system built on top of the hardware, drivers and operating system
of the target machine. We’ve seen how low-level engine systems provide ser-
vices that are required by the rest of the engine; how human interface devices
such as joypads, keyboards, mice and other devices can allow a human player
to provide inputs to the engine; how the rendering engine produces 3D im-
ages on-screen; how the animation system allows characters and objects to
move naturally; how the collision system detects and resolves interpenetra-
tions between shapes; how the physics simulation causes objects to move in
physically realistic ways; how the 3D audio engine renders a believable and
immersive soundscape for our game world. But despite the wide range of
powerful features provided by these components, if we were to put them all
together, we still wouldn’t have a game!
A game is deﬁned not by its technology but by its gameplay. Gameplay
can be deﬁned as the overall experience of playing a game. The term game
mechanics pins down this idea a bit more concretely—it is usually deﬁned as
the set of rules that govern the interactions between the various entities in the
game. It also deﬁnes the objectives of the player(s), criteria for success and fail-
ure, the player character’s abilities, the number and types of non-player entities
847

848
14. Introduction to Gameplay Systems
that exist within the game’s virtual world and the overall ﬂow of the gaming
experience as a whole. In many games, these elements are intertwined with a
compelling story and a rich cast of characters. However, story and characters
are deﬁnitely not a necessary part of every video game, as evidenced by wildly
successful puzzle games like Tetris. In their paper, “A Survey of ‘Game’ Porta-
bility”
(http://www.dcs.shef.ac.uk/intranet/research/resmes/CS0705.pdf),
Ahmed BinSubaih, Steve Maddock and Daniela Romano of the University of
Shefﬁeld refer to the collection of software systems used to implement game-
play as a game’s G-factor. In the next three chapters, we’ll explore the crucial
tools and engine systems that deﬁne and manage the game mechanics (a.k.a.
gameplay, a.k.a. G-factor) of a game.
14.1
Anatomy of a Game World
Gameplay designs vary widely from genre to genre and from game to game.
That said, most 3D games, and a good number of 2D games as well, conform
more or less to a few basic structural patterns. We’ll discuss these patterns
in the following sections, but please keep in mind that there are bound to be
games out there that do not ﬁt neatly into this mold.
14.1.1
World Elements
Most video games take place in a two- or three-dimensional virtual game world.
This world is typically comprised of numerous discrete elements. Generally,
these elements fall into two categories: static elements and dynamic elements.
Static elements include terrain, buildings, roads, bridges and pretty much
anything that doesn’t move or interact with gameplay in an active way. Dy-
namic elements include characters, vehicles, weaponry, ﬂoating power-ups
and health packs, collectible objects, particle emitters, dynamic lights, invisi-
ble regions used to detect important events in the game, splines that deﬁne the
paths of objects and so on. This breakdown of the game world is illustrated in
Figure 14.1.
Gameplay is generally concentrated within the dynamic elements of a game.
Clearly, the layout of the static background plays a crucial role in how the
game plays out. For example, a cover-based shooter wouldn’t be very much
fun if it were played in a big, empty, rectangular room. However, the software
systems that implement gameplay are primarily concerned with updating the
locations, orientations and internal states of the dynamic elements, since they
are the elements that change over time. The term game state refers to the cur-
rent state of all dynamic game world elements taken as a whole.

14.1. Anatomy of a Game World
849
Figure 14.1. A game world from The Last of Us (© 2013/™SCEA. Created and developed by Naughty
Dog, PlayStation 3) showing static and dynamic elements.
The ratio of dynamic to static elements also varies from game to game.
Most 3D games consist of a relatively small number of dynamic elements
moving about within a relatively large static background area. Other games,
like the arcade classic Asteroids or the Xbox 360 retro hit Geometry Wars, have
no static elements to speak of (other than a black screen). The dynamic ele-
ments of a game are usually more expensive than the static elements in terms
of CPU resources, so most 3D games are constrained to a limited number of
dynamic elements. However, the higher the ratio of dynamic to static ele-
ments, the more “alive” the game world can seem to the player. As gaming
hardware becomes more and more powerful, games are achieving higher and
higher dynamic-to-static ratios.
It’s important to note that the distinction between the dynamic and static
elements in a game world is often a bit blurry. For example, in the arcade game
Hydro Thunder, the waterfalls were dynamic in the sense that their textures
were animated, they had dynamic mist effects at their bases, and they could be
placed into the game world and positioned by a game designer independently
of the terrain and water surface. However, from an engineering standpoint,
waterfalls were treated as static elements because they did not interact with

850
14. Introduction to Gameplay Systems
the boats in the race in any way (other than to obscure the player’s view of
hidden boost power-ups and secret passageways). Different game engines
draw different lines between static and dynamic elements, and some don’t
draw a distinction at all (i.e., everything is potentially a dynamic element).
The distinction between static and dynamic serves primarily as an opti-
mization tool—we can do less work when we know that the state of an object
isn’t going to change. For example, when we know a mesh is static and will
never move, its lighting can be precomputed in the form of static vertex light-
ing, light maps, shadow maps, static ambient occlusion information or pre-
computed radiance transfer (PRT) spherical harmonics coefﬁcients. Virtually
any computation that must be done at runtime for a dynamic world element
is a good candidate for precomputation or omission when applied to a static
element.
Games with destructible environments are an example of how the line be-
tween the static and dynamic elements in a game world can blur. For in-
stance, we might deﬁne three versions of every static element—an undam-
aged version, a damaged version, and a fully destroyed version. These back-
ground elements act like static world elements most of the time, but they can
be swapped dynamically during an explosion to produce the illusion of be-
coming damaged. In reality, static and dynamic world elements are just two
extremes along a gamut of possible optimizations. Where we draw the line
between the two categories (if we draw one at all) shifts as our optimization
methodologies change and adapt to the needs of the game design.
14.1.1.1
Static Geometry
The geometry of a static world element is often deﬁned in a tool like Maya. It
might be one giant triangle mesh, or it might be broken up into discrete pieces.
The static portions of the scene are sometimes built out of instanced geometry.
Instancing is a memory conservation technique in which a relatively small
number of unique triangle meshes are rendered multiple times throughout
the game world, at different locations and orientations, in order to provide the
illusion of variety. For example, a 3D modeler might create ﬁve different kinds
of short wall sections and then piece them together in random combinations
in order to construct miles of unique-looking walls.
Static visual elements and collision data might also be constructed from
brush geometry. This kind of geometry originated with the Quake family of
engines. A brush describes a shape as a collection of convex volumes, each
bounded by a set of planes. Brush geometry is fast and easy to create and
integrates well into a BSP-tree-based rendering engine. Brushes can be really
useful for rapidly blocking out the contents of a game world. This allows

14.1. Anatomy of a Game World
851
gameplay to be tested early, when it is cheap to do so. If the layout proves its
worth, the art team can either texture map and ﬁne-tune the brush geometry
or replace it with more-detailed custom mesh assets. On the other hand, if
the level requires redesign, the brush geometry can be easily revised without
creating a lot of extra work for the art team.
14.1.2
World Chunks
When a game takes place in a very large virtual world, it is typically divided
into discrete playable regions, which we’ll call world chunks. Chunks are also
known as levels, maps, stages or areas. The player can usually see only a hand-
ful of chunks at any given moment while playing the game, and he or she
progresses from chunk to chunk as the game unfolds.
Originally, the concept of “levels” was invented as a mechanism to provide
greater variety of gameplay within the memory limitations of early gaming
hardware. Only one level could exist in memory at a time, but the player
could progress from level to level for a much richer overall experience. Since
then, game designs have branched out in many directions, and linear level-
based games are much less common today. Some games are essentially still
linear, but the delineations between world chunks are usually not as obvious
to the player as they once were. Other games use a star topology, in which the
player starts in a central hub area and can access other areas at random from
the hub (perhaps only after they have been unlocked). Others use a graph-
like topology, where areas are connected to one another in arbitrary ways.
Still others provide the illusion of a vast, open world, and use level-of-detail
(LOD) techniques to reduce memory overhead and improve performance.
Despite the richness of modern game designs, all but the smallest of game
worlds are still divided into chunks of some kind. This is done for a number of
reasons. First of all, memory limitations are still an important constraint (and
will be until game machines with inﬁnite memory hit the market!). World
chunks are also a convenient mechanism for controlling the overall ﬂow of the
game. Chunks can serve as a division-of-labor mechanism as well; each chunk
can be constructed and managed by a relatively small group of designers and
artists. World chunks are illustrated in Figure 14.2.
14.1.3
High-Level Game Flow
A game’s high-level ﬂow deﬁnes a sequence, tree or graph of player objectives.
Objectives are sometimes called tasks, stages, levels (a term that can also apply
to world chunks) or waves (if the game is primarily about defeating hordes of
attacking enemies). The high-level ﬂow also provides the deﬁnition of success
for each objective (e.g., clear all the enemies and get the key) and the penalty

852
14. Introduction to Gameplay Systems
Figure 14.2. Many game worlds are divided into chunks for various reasons, including memory
limitations, the need to control the ﬂow of the game through the world, and as a division-of-
labor mechanism during development.
for failure (e.g., go back to the start of the current area, possibly losing a “life”
in the process). In a story-driven game, this ﬂow might also include various
in-game movies that serve to advance the player’s understanding of the story
as it unfolds. These sequences are sometimes called cut-scenes, in-game cine-
matics (IGC) or noninteractive sequences (NIS). When they are rendered ofﬂine
and played back as a full-screen movie, such sequences are usually called full-
motion videos (FMV).
Early games mapped the objectives of the player one-to-one to particular
world chunks (hence the dual meaning of the term “level”). For example, in
Donkey Kong, each new level presents Mario with a new objective (namely, to
reach the top of the structure and progress to the next level). However, this
one-to-one mapping between world chunks and objectives is less popular in
modern game design. Each objective is associated with one or more world
chunks, but the coupling between chunks and objectives remains deliberately
loose. This kind of design offers the ﬂexibility to alter game objectives and
world subdivision independently, which is extremely helpful from a logistic
and practical standpoint when developing a game. Many games group their
objectives into coarser sections of gameplay, often called chapters or acts. A
typical gameplay architecture is shown in Figure 14.3.

14.2. Implementing Dynamic Elements: Game Objects
853
Chapter 1
Chunk 1
Chunk 2
Chunk 3
Objective 1B
Objective 1A
Objective 1C
Optional
Objective 1D
Objective 1E
Objective 1G
Optoinal
Objective 1F
Chapter 2
Chunk 4
Chunk 5
Chunk 6
Chunk 7
Objective 2B
Objective 2A
Objective 2C
Objective 2D
Objective 2G
Optoinal
Objective 2H
Optional
Objective 2F
Optional
Objective 2E
Objective 2I
Figure 14.3. Gameplay objectives are typically arranged in a sequence, tree or graph, and each one maps to one or more
game world chunks.
14.2
Implementing Dynamic Elements:
Game Objects
The dynamic elements of a game are usually designed in an object-oriented
fashion. This approach is intuitive and natural and maps well to the game
designer’s notion of how the world is constructed. He or she can visual-
ize characters, vehicles, ﬂoating health packs, exploding barrels and myriad
other dynamic objects moving about in the game. So it is only natural to
want to be able to create and manipulate these elements in the game world
editor. Likewise, programmers usually ﬁnd it natural to implement dynamic
elements as largely autonomous agents at runtime. In this book, we’ll use the
term game object (GO) to refer to virtually any dynamic element within a game
world. However, this terminology is by no means standard within the indus-
try. Game objects are commonly referred to as entities, actors or agents, and the
list of terms goes on.

854
14. Introduction to Gameplay Systems
As is customary in object-oriented design, a game object is essentially a
collection of attributes (the current state of the object) and behaviors (how the
state changes over time and in response to events). Game objects are usually
classiﬁed by type. Different types of objects have different attribute schemas
and different behaviors. All instances of a particular type share the same at-
tribute schema and the same set of behaviors, but the values of the attributes
differ from instance to instance. (Note that if a game object’s behavior is data-
driven, say through script code or via a set of data-driven rules governing the
object’s responses to events, then behavior too can vary on an instance-by-
instance basis.)
The distinction between a type and an instance of a type is a crucial one.
For example, the game of Pac-Man involves four game object types: ghosts,
pellets, power pills and Pac-Man. However, at any moment in time, there
may be up to four instances of the type “ghost,” 50–100 instances of the type
“pellet,” four “power pill” instances and one instance of the “Pac-Man” type.
Most object-oriented systems provide some mechanism for the inheritance
of attributes, behavior or both. Inheritance encourages code and design reuse.
The speciﬁcs of how inheritance works vary widely from game to game, but
most game engines support it in some form.
14.2.1
Game Object Models
In computer science, the term object model has two related but distinct mean-
ings. It can refer to the set of features provided by a particular programming
language or formal design language. For example, we might speak of the
C++ object model or the OMT object model. It can also refer to a speciﬁc object-
oriented programming interface (i.e., a collection of classes, methods and in-
terrelationships designed to solve a particular problem). One example of this
latter usage is the Microsoft Excel object model, which allows external programs
to control Excel in various ways. (See http://en.wikipedia.org/wiki/Object_
model for further discussion of the term object model.)
In this book, we will use the term game object model to describe the facilities
provided by a game engine in order to permit the dynamic entities in the
virtual game world to be modeled and simulated. In this sense, the term game
object model has aspects of both of the deﬁnitions given above:
•
A game’s object model is a speciﬁc object-oriented programming inter-
face intended to solve the particular problem of simulating the speciﬁc
set of entities that make up a particular game.
•
Additionally, a game’s object model often extends the programming lan-
guage in which the engine was written. If the game is implemented in

14.2. Implementing Dynamic Elements: Game Objects
855
a non-object-oriented language like C, object-oriented facilities can be
added by the programmers. And even if the game is written in an object-
oriented language like C++, advanced features like reﬂection, persis-
tence and network replication are often added. A game object model
sometimes melds the features of multiple languages. For example, a
game engine might combine a compiled programming language such as
C or C++ with a scripting language like Python, Lua or Pawn and pro-
vide a uniﬁed object model that can be accessed from either language.
14.2.2
Tool-Side Design versus Runtime Design
The object model presented to the designers via the world editor (discussed
in Section 14.4) needn’t be the same object model used to implement the game
at runtime.
•
The tool-side game object model might be implemented at runtime us-
ing a language with no native object-oriented features at all, like C.
•
A single GO type on the tool side might be implemented as a collection
of classes at runtime (rather than as a single class as one might at ﬁrst
expect).
•
Each tool-side GO might be nothing more than a unique id at runtime,
with all of its state data stored in tables or collections of loosely coupled
objects.
Therefore, a game really has two distinct but closely interrelated object models:
•
The tool-side object model is deﬁned by the set of game object types seen by
the designers within the world editor.
•
The runtime object model is deﬁned by whatever set of language con-
structs and software systems the programmers have used to implement
the tool-side object model at runtime. The runtime object model might
be identical to the tool-side model or map directly to it, or it might be
entirely different than the tool-side model under the hood.
In some game engines, the line between the tool-side and runtime designs
is blurred or nonexistent. In others, it is very well delineated. In some engines,
the implementation is actually shared between the tools and the runtime. In
others, the runtime implementation looks almost totally alien relative to the
tool-side view of things. Some aspects of the implementation almost always
creep up into the tool-side design, and game designers must be cognizant of
the performance- and memory-consumption impacts of the game worlds they

856
14. Introduction to Gameplay Systems
construct and the gameplay rules and object behaviors they design. That said,
virtually all game engines have some form of tool-side object model and a
corresponding runtime implementation of that object model.
14.3
Data-Driven Game Engines
In the early days of game development, games were largely hard-coded by
programmers. Tools, if any, were primitive. This worked because the amount
of content in a typical game was miniscule, and the bar wasn’t particularly
high, thanks in part to the primitive graphics and sound of which early game
hardware was capable.
Today, games are orders of magnitude more complex, and the quality bar
is so high that game content is often compared to the computer-generated ef-
fects in Hollywood blockbusters. Game teams have grown much larger, but
the amount of game content is growing faster than team size. In the eighth
generation of consoles, deﬁned by the Xbox One and the PlayStation 4, game
teams routinely speak of the need to produce ten times the content with teams
that are not that much larger than in the previous generation. This trend
means that a game team must be capable of producing very large amounts
of content in an extremely efﬁcient manner.
Engineering resources are often a production bottleneck because high-
quality engineering talent is limited and expensive and because engineers
tend to produce content much more slowly than artists and game designers
(due to the complexities inherent in computer programming). Most teams
now believe that it’s a good idea to put at least some of the power to cre-
ate content directly into the hands of the folks responsible for producing that
content—namely the designers and the artists. When the behavior of a game
can be controlled, in whole or in part, by data provided by artists and design-
ers rather than exclusively by software produced by programmers, we say the
engine is data driven.
Data-driven architectures can improve team efﬁciency by fully leveraging
all staff members to their fullest potential and by taking some of the heat off
the engineering team. It can also lead to improved iteration times. Whether a
developer wants to make a slight tweak to the game’s content or completely
revise an entire level, a data-driven design allows the developer to see the
effects of the changes quickly, ideally with little or no help from an engineer.
This saves valuable time and can permit the team to polish their game to a
very high level of quality.
That being said, it’s important to realize that data-driven features often
come at a heavy cost. Tools must be provided to allow game designers and

14.4. The Game World Editor
857
artists to deﬁne game content in a data-driven manner. The runtime code
must be changed to handle the wide range of possible inputs in a robust way.
Tools must also be provided in-game to allow artists and designers to preview
their work and troubleshoot problems. All of this software requires signiﬁcant
time and effort to write, test and maintain.
Sadly, many teams make a mad rush into data-driven architectures with-
out stopping to study the impacts of their efforts on their particular game de-
sign and the speciﬁc needs of their team members. In their haste, such teams
often dramatically overshoot the mark, producing overly complex tools and
engine systems that are difﬁcult to use, bug-ridden and virtually impossible
to adapt to the changing requirements of the project. Ironically, in their efforts
to realize the beneﬁts of a data-driven design, a team can easily end up with
signiﬁcantly lower productivity than the old-fashioned hard-coded methods.
Every game engine should have some data-driven components, but a game
team must exercise extreme care when selecting which aspects of the engine
to data-drive. It’s crucial to weigh the costs of creating a data-driven or rapid-
iteration feature against the amount of time the feature is expected to save
the team over the course of the project. It’s also incredibly important to keep
the KISS mantra (“keep it simple, stupid”) in mind when designing and im-
plementing data-driven tools and engine systems. To paraphrase Albert Ein-
stein, everything in a game engine should be made as simple as possible, but
no simpler.
14.4
The Game World Editor
We’ve already discussed data-driven asset-creation tools, such as Maya, Pho-
toshop, Havok content tools and so on. These tools generate individual assets
for consumption by the rendering engine, animation system, audio system,
physics system and so on. The analog to these tools in the gameplay space
is the game world editor—a tool (or a suite of tools) that permits game world
chunks to be deﬁned and populated with static and dynamic elements.
All commercial game engines have some kind of world editor tool.
•
A well-known tool called Radiant is used to create maps for the Quake
and Doom family of engines. A screenshot of Radiant is shown in Fig-
ure 14.4.
•
Valve’s Source engine, the engine that drives Half-Life 2, The Orange Box,
Team Fortress 2, the Portal series, the Left 4 Dead series and most recently
Titanfall provides an editor called Hammer (previously distributed under

858
14. Introduction to Gameplay Systems
Figure 14.4. The Radiant world editor for the Quake and Doom family of engines.
the names Worldcraft and The Forge). Figure 14.5 shows a screenshot of
Hammer.
•
Crytek’s CryENGINE 3 provides a powerful suite of world creation and
editing tools. These tools support real-time editing of multiplatform
game environments simultaneously, both in 2D and true stereoscopic
3D. Crytek’s Sandbox editor is depicted in Figure 14.6.
The game world editor generally permits the initial states of game objects
(i.e., the values of their attributes) to be speciﬁed. Most game world editors
also give their users some sort of ability to control the behaviors of the dynamic
objects in the game world. This control might be via data-driven conﬁguration
parameters (e.g., object A should start in an invisible state, object B should
immediately attack the player when spawned, object C is ﬂammable, etc.),
or behavioral control might be via a scripting language, thereby shifting the
game designers’ tasks into the realm of programming. Some world editors
even allow entirely new types of game objects to be deﬁned, with little or no
programmer intervention.

14.4. The Game World Editor
859
Figure 14.5. Valve’s Hammer editor for the Source engine.
Figure 14.6. The Sandbox editor for CryENGINE 3. (See Color Plate XXIV.)

860
14. Introduction to Gameplay Systems
14.4.1
Typical Features of a Game World Editor
The design and layout of game world editors vary widely, but most editors
provide a reasonably standard set of features. These include, but are certainly
not limited to, the following.
14.4.1.1
World Chunk Creation and Management
The unit of world creation is usually a chunk (also known as a level or map—
see Section 14.1.2). The game world editor typically allows new chunks to
be created and existing chunks to be renamed, broken up, combined or de-
stroyed. Each chunk can be linked to one or more static meshes and/or other
static data elements such as AI navigation maps, descriptions of ledges that
can be grabbed by the player, cover point deﬁnitions and so on. In some
engines, a chunk is deﬁned by a single background mesh and cannot exist
without one. In other engines, a chunk may have an independent existence,
perhaps deﬁned by a bounding volume (e.g., AABB, OBB or arbitrary polyg-
onal region), and can be populated by zero or more meshes and/or brush
geometry (see Section 1.7.2.1).
Some world editors provide dedicated tools for authoring terrain, water
and other specialized static elements. In other engines, these elements might
be authored using standard DCC applications but tagged in some way to in-
dicate to the asset conditioning pipeline and/or the runtime engine that they
are special. (For example, in the Uncharted series and The Last of Us, water
was authored as a triangle mesh, but it was mapped with a special material
that indicated that it was to be treated as water.) Sometimes, special world
elements are created and edited in a separate, stand-alone tool. For example,
the height ﬁeld terrain in Medal of Honor: Paciﬁc Assault was authored using
a customized version of a tool obtained from another team within Electronic
Arts because this was more expedient than trying to integrate a terrain editor
into Radiant, the world editor being used on the project at the time.
14.4.1.2
Game World Visualization
It’s important for the user of a game world editor to be able to visualize the
contents of the game world. As such, virtually all game world editors provide
a three-dimensional perspective view of the world and/or a two-dimensional
orthographic projection. It’s common to see the view pane divided into four
sections, three for top, side and front orthographic elevations and one for the
3D perspective view.
Some editors provide these world views via a custom rendering engine
integrated directly into the tool. Other editors are themselves integrated into

14.4. The Game World Editor
861
a 3D geometry editor like Maya or 3ds Max, so they can simply leverage the
tool’s viewports. Still other editors are designed to communicate with the
actual game engine and use it to render the 3D perspective view. Some editors
are even integrated into the engine itself.
14.4.1.3
Navigation
Clearly, a world editor wouldn’t be of much use if the user weren’t able to
move around within the game world. In an orthographic view, it’s important
to be able to scroll and zoom in and out. In a 3D view, various camera control
schemes are used. It may be possible to focus on an individual object and
rotate around it. It may also be possible to switch into a “ﬂy through” mode
where the camera rotates about its own focal point and can be moved forward,
backward, up and down and panned left and right.
Some editors provide a host of convenience features for navigation. These
include the ability to select an object and focus in on it with a single key press,
the ability to save various relevant camera locations and then jump between
them, various camera movement speed modes for coarse navigation and ﬁne
camera control, a Web-browser-like navigation history that can be used to
jump around the game world and so on.
14.4.1.4
Selection
A game world editor is primarily designed to allow the user to populate a
game world with static and dynamic elements. As such, it’s important for the
user to be able to select individual elements for editing. Some editors only
allow a single object to be selected at a time, while more-advanced editors
permit multiobject selections. Objects might be selected via a rubber-band
box in the orthographic view or by ray cast style picking in the 3D view. Many
editors also display a list of all world elements in a scrolling list or tree view
so that objects can be found and selected by name. Some world editors also
allow selections to be named and saved for later retrieval.
Game worlds are often quite densely populated. As such, it can sometimes
be difﬁcult to select a desired object because other objects are in the way. This
problem can be overcome in a number of ways. When using a ray cast to select
objects in 3D, the editor might allow the user to cycle through all of the objects
that the ray is currently intersecting rather than always selecting the nearest
one. Many editors allow the currently selected object(s) to be temporarily
hidden from view. That way, if you don’t get the object you want the ﬁrst
time, you can always hide it and try again. As we’ll see in the next section,
layers can also be an effective way to reduce clutter and improve the user’s
ability to select objects successfully.

862
14. Introduction to Gameplay Systems
14.4.1.5
Layers
Some editors also allow objects to be grouped into predeﬁned or user-deﬁned
layers. This can be an incredibly useful feature, allowing the contents of the
game world to be organized sensibly. Entire layers can be hidden or shown to
reduce clutter on-screen. Layers might be color-coded for easy identiﬁcation.
Layers can be an important part of a division-of-labor strategy, as well. For
example, when the lighting team is working on a world chunk, they can hide
all of the elements in the scene that are not relevant to lighting.
What’s more, if the game world editor is capable of loading and saving
layers individually, conﬂicts can be avoided when multiple people are work-
ing on a single world chunk at the same time. For example, all of the lights
might be stored in one layer, all of the background geometry in another and all
AI characters in a third. Since each layer is totally independent, the lighting,
background and NPC teams can all work simultaneously on the same world
chunk.
14.4.1.6
Property Grid
The static and dynamic elements that populate a game world chunk typically
have various properties (also known as attributes) that can be edited by the
user. Properties might be simple key-value pairs and be limited to simple
atomic data types like Booleans, integers, ﬂoating-point numbers and strings.
In some editors, more-complex properties are supported, including arrays of
data and nested compound data structures. More-complex data types may be
supported too, such as vectors, RGB colors and references to external assets
(audio ﬁles, meshes, animations, etc.)
Most world editors display the attributes of the currently selected object(s)
in a scrollable property grid view. An example of a property grid is shown in
Figure 14.7. The grid allows the user to see the current values of each attribute
and edit the values by typing, using check boxes or drop-down combo boxes,
dragging spinner controls up and down and so on.
Editing Multiobject Selections
In editors that support multiobject selection, the property grid may support
multiobject editing as well. This advanced feature displays an amalgam of
the attributes of all objects in the selection. If a particular attribute has the
same value across all objects in the selection, the value is shown as-is, and
editing the value in the grid causes the property value to be updated in all
selected objects. If the attribute’s value differs from object to object within the
selection, the property grid typically shows no value at all. In this case, if a

14.4. The Game World Editor
863
Figure 14.7. A typical property grid.
new value is typed into the ﬁeld in the grid, it will overwrite the values in all
selected objects, bringing them all into agreement.
Another problem arises when the selection contains a heterogeneous col-
lection of objects (i.e., objects whose types differ). Each type of object can
potentially have a different set of attributes, so the property grid must dis-
play only those attributes that are common to all object types in the selection.
This can still be useful, however, because game object types often inherit from
a common base type. For example, most objects have a position and orienta-
tion. In a heterogeneous selection, the user can still edit these shared attributes
even though more-speciﬁc attributes are temporarily hidden from view.
Free-Form Properties
Normally, the set of properties associated with an object, and the data types
of those properties, are deﬁned on a per-object-type basis. For example, a ren-
derable object has a position, orientation, scale and mesh, while a light has
position, orientation, color, intensity and light type. Some editors also allow
additional “free-form” properties to be deﬁned by the user on a per-instance

864
14. Introduction to Gameplay Systems
basis. These properties are usually implemented as a ﬂat list of key-value
pairs. The user is free to choose the name (key) of each free-form property,
along with its data type and its value. This can be incredibly useful for proto-
typing new gameplay features or implementing one-off scenarios.
14.4.1.7
Object Placement and Alignment Aids
Some object properties are treated in a special way by the world editor. Typi-
cally the position, orientation and scale of an object can be controlled via spe-
cial handles in the orthographic and perspective viewports, just like in Maya
or Max. In addition, asset linkages often need to be handled in a special way.
For example, if we change the mesh associated with an object in the world, the
editor should display this mesh in the orthographic and 3D perspective view-
ports. As such, the game world editor must have special knowledge of these
properties—it cannot treat them generically, as it can most object properties.
Many world editors provide a host of object placement and alignment aids
in addition to the basic translation, rotation and scale tools. Many of these
features borrow heavily from the feature sets of commercial graphics and 3D
modeling tools like Photoshop, Maya, Visio and others. Examples include
snap to grid, snap to terrain, align to object and many more.
14.4.1.8
Special Object Types
Just as some object properties must be handled in a special way by the world
editor, certain types of objects also require special handling. Examples in-
clude:
•
Lights. The world editor usually uses special icons to represent lights,
since they have no mesh. The editor may attempt to display the light’s
approximate effect on the geometry in the scene as well, so that design-
ers can move lights around in real time and get a reasonably good feel
for how the scene will ultimately look.
•
Particle emitters. Visualization of particle effects can also be problematic
in editors that are built on a stand-alone rendering engine. In this case,
particle emitters might be displayed using icons only, or some attempt
might be made to emulate the particle effect in the editor. Of course, this
is not a problem if the editor is in-game or can communicate with the
running game for live tweaking.
•
Sound sources. As we discussed in Chapter 13, a 3D rendering engine
models sound sources as 3D points or volumes. It may be convenient
to provide specialized editing tools for these in the world editor. For
example, sound designers will ﬁnd it helpful if they can visualize the

14.4. The Game World Editor
865
maximum radius of an omnidirectional sound emitter, or the direction
vector and cone of a directional emitter.
•
Regions. A region is a volume of space that is used by the game to detect
relevant events such as objects entering or leaving the volume or to de-
mark areas for various purposes. Some game engines restrict regions to
being modeled as spheres or oriented boxes, while others may permit ar-
bitrary convex polygonal shapes when viewed from above, with strictly
horizontal sides. Still others might allow regions to be constructed out
of more-complex geometry, such as k-DOPs (see Section 12.3.4.5). If re-
gions are always spherical then the designers might be able to make do
with a “Radius” property in the property grid, but to deﬁne or modify
the extents of an arbitrarily shaped region, a special-case editing tool is
almost certainly required.
•
Splines. A spline is a three-dimensional curve deﬁned by a set of con-
trol points and possibly tangent vectors at the points, depending on the
type of mathematical curve used. Catmull-Rom splines are commonly
used because they are fully deﬁned by a set of control points (without
tangents), and the curve always passes through all of the control points.
But no matter what type of splines are supported, the world editor typ-
ically needs to provide the ability to display the splines in its viewports,
and the user must be able to select and manipulate individual control
points. Some world editors actually support two selection modes—a
“coarse” mode for selecting objects in the scene and a “ﬁne” mode for
selecting the individual components of a selected object, such as the con-
trol points of a spline or the vertices of a region.
•
Nav meshes for AI. In many games, NPCs navigate by running path-
ﬁnding algorithms within the navigable regions of the game world. These
navigable regions must be deﬁned, and the world editor usually plays
a central role in allowing AI designers to create, visualize and edit these
regions. For example, a nav mesh is a 2D triangle mesh that provides a
simple description of the boundaries of the navigable region, as well as
providing connectivity information to the path ﬁnder.
•
Other custom data. Of course, every game has its own speciﬁc data re-
quirements. The world editor may be called upon to provide custom
visualization and editing facilities for these pieces of data. Examples in-
clude a description of the “affordances” (windows, doorways, possible
points of attack or defense) within a play space for use by the AI system,
or geometric features that describe things like cover points or grabbable
ledges for use by the player character and/or NPCs.

866
14. Introduction to Gameplay Systems
14.4.1.9
Saving and Loading World Chunks
Of course, no world editor would be complete if it were unable to load and
save world chunks. The granularity with which world chunks can be loaded
and saved differs widely from engine to engine. Some engines store each
world chunk in a single ﬁle, while others allow individual layers to be loaded
and saved independently. Data formats also vary across engines. Some use
custom binary formats, others text formats like XML or JSON. Each design
has its pros and cons, but every editor provides the ability to load and save
world chunks in some form—and every game engine is capable of loading
world chunks so that they can be played at runtime.
14.4.1.10
Rapid Iteration
A good game world editor usually supports some degree of dynamic tweak-
ing for rapid iteration. Some editors run within the game itself, allowing the
user to see the effects of his or her changes immediately. Others provide a
live connection from the editor to the running game. Still other world editors
operate entirely ofﬂine, either as a stand-alone tool or as a plug-in to a DCC
application like Lightwave or Maya. These tools sometimes permit modiﬁed
data to be reloaded dynamically into the running game. The speciﬁc mech-
anism isn’t important—all that matters is that users have a reasonably short
round-trip iteration time (i.e., the time between making a change to the game
world and seeing the effects of that change in-game). It’s important to realize
that iterations don’t have to be instantaneous. Iteration times should be com-
mensurate with the scope and frequency of the changes being made. For ex-
ample, we might expect tweaking a character’s maximum health to be a very
fast operation, but when making major changes to the lighting environment
for an entire world chunk, a much longer iteration time might be acceptable.
14.4.2
Integrated Asset Management Tools
In some engines, the game world editor is integrated with other aspects of
game asset database management, such as deﬁning mesh and material prop-
erties, deﬁning animations, blend trees, animation state machines, setting up
collision and physical properties of objects, managing texture resources and
so on. (See Section 6.2.1.2 for a discussion of the game asset database.)
Perhaps the best-known example of this design in action is UnrealEd, the
editor used to create content for games built on the Unreal Engine. UnrealEd
is integrated directly into the game engine, so any changes made in the editor
are made directly to the dynamic elements in the running game. This makes
rapid iteration very easy to achieve. But UnrealEd is much more than a game

14.4. The Game World Editor
867
Figure 14.8. UnrealEd’s Generic Browser provides access to the entire game asset database.
world editor—it is actually a complete content-creation package. It manages
the entire database of game assets, from animations to audio clips to triangle
meshes to textures to materials and shaders and much more. UnrealEd pro-
vides its user with a uniﬁed, real-time, WYSIWYG view into the entire asset
database, making it a powerful enabler of any rapid, efﬁcient game develop-
ment process. A few screenshots from UnrealEd are shown in Figures 14.8
and 14.9.
14.4.2.1
Data Processing Costs
In Section 6.2.1, we learned that the asset conditioning pipeline (ACP) con-
verts game assets from their various source formats into the formats required
by the game engine. This is typically a two-step process. First, the asset is
exported from the DCC application to a platform-independent intermediate
format that only contains the data that is relevant to the game. Second, the
asset is processed into a format that is optimized for a speciﬁc platform. On
a project targeting multiple gaming platforms, a single platform-independent
asset gives rise to multiple platform-speciﬁc assets during this second phase.
One of the key differences between tools pipelines is the point at which this
second platform-speciﬁc optimization step is performed. UnrealEd performs
it when assets are ﬁrst imported into the editor. This approach pays off in

868
14. Introduction to Gameplay Systems
Figure 14.9. UnrealEd also provides a world editor.
rapid iteration time when iterating on level design. However, it can make the
cost of changing source assets like meshes, animations, audio assets and so
on more painful. Other engines like the Source engine and the Quake engine
pay the asset optimization cost when baking out the level prior to running
the game. Halo gives the user the option to change raw assets at any time;
they are converted into an optimized form when they are ﬁrst loaded into the
engine, and the results are cached to prevent the optimization step from being
performed needlessly every time the game is run.

15
Runtime Gameplay
Foundation Systems
15.1
Components of the Gameplay
Foundation System
M
ost game engines provide a suite of runtime software components that
together provide a framework upon which a game’s unique rules, ob-
jectives and dynamic world elements can be constructed. There is no stan-
dard name for these components within the game industry, but we will refer
to them collectively as the engine’s gameplay foundation system. If a line be-
tween engine and game can reasonably be drawn between the game engine
and the game itself, then these systems lie just beneath this line. In theory, one
can construct gameplay foundation systems that are for the most part game-
agnostic. However, in practice, these systems almost always contain genre-
or game-speciﬁc details. In fact, the line between the engine and the game
can probably be best visualized as one big blur—a gradient that arcs across
these components as it links the engine to the game. In some game engines,
one might even go so far as to consider the gameplay foundation systems as
lying entirely above the engine-game line. The differences between game en-
gines are most acute when it comes to the design and implementation of their
gameplay components. That said, there are a surprising number of common
patterns across engines, and those commonalities will be the topic of our dis-
cussions here.
869

870
15. Runtime Gameplay Foundation Systems
Every game engine approaches the problem of gameplay software design
a bit differently. However, most engines provide the following major subsys-
tems in some form:
•
Runtime game object model. This is an implementation of the abstract
game object model advertised to the game designers via the world edi-
tor.
•
Level management and streaming. This system loads and unloads the con-
tents of the virtual worlds in which gameplay takes place. In many en-
gines, level data is streamed into memory during gameplay, thus pro-
viding the illusion of a large seamless world (when in fact it is broken
into discrete chunks).
•
Real-time object model updating. In order to permit the game objects in the
world to behave autonomously, each object must be updated periodi-
cally. This is where all of the disparate systems in a game engine truly
come together into a cohesive whole.
•
Messaging and event handling. Most game objects need to communicate
with one another. This is usually done via an abstract messaging system.
Inter-object messages often signal changes in the state of the game world
called events. So the messaging system is referred to as the event system
in many studios.
•
Scripting. Programming high-level game logic in a language like C or
C++ can be cumbersome. To improve productivity, allow rapid itera-
tion, and put more power into the hands of the non-programmers on
the team, a scripting language is often integrated into the game engine.
This language might be text-based, like Python or Lua, or it might be a
graphical language, like Unreal’s Kismet.
•
Objectives and game ﬂow management. This subsystem manages the player’s
objectives and the overall ﬂow of the game. This is usually described
by a sequence, tree or graph of player objectives. Objectives are often
grouped into chapters, especially if the game is highly story-driven as
many modern games are. The game ﬂow management system manages
the overall ﬂow of the game, tracks the player’s accomplishment of ob-
jectives and gates the player from one area of the game world to the next
as the objectives are accomplished. Some designers refer to this as the
“spine” of the game.
Of these major systems, the runtime object model is probably the most
complex. It typically provides most, if not all, of the following features:
•
Spawning and destroying game objects dynamically. The dynamic elements
in a game world often need to come and go during gameplay. Health

15.1. Components of the Gameplay Foundation System
871
packs disappear once they have been picked up, explosions appear and
then dissipate and enemy reinforcements mysteriously come from around
a corner just when you think you’ve cleared the level. Many game en-
gines provide a system for managing the memory and other resources
associated with dynamically spawned game objects. Other engines sim-
ply disallow dynamic creation or destruction of game objects altogether.
•
Linkage to low-level engine systems. Every game object has some kind of
linkage to one or more underlying engine systems. Most game objects
are visually represented by renderable triangle meshes. Some have par-
ticle effects. Many generate sounds. Some animate. Many have colli-
sion, and some are dynamically simulated by the physics engine. One
of the primary responsibilities of the gameplay foundation system is to
ensure that every game object has access to the services of the engine
systems upon which it depends.
•
Real-time simulation of object behaviors. At its core, a game engine is a real-
time dynamic computer simulation of an agent-based model. This is just
a fancy way of saying that the game engine needs to update the states
of all the game objects dynamically over time. The objects may need to
be updated in a very particular order, dictated in part by dependencies
between the objects, in part by their dependencies on various engine
subsystems, and in part because of the interdependencies between those
engine subsystems themselves.
•
Ability to deﬁne new game object types. Every game’s requirements change
and evolve as the game is developed. It’s important that the game object
model be ﬂexible enough to permit new object types to be added easily
and exposed to the world editor. In an ideal world, it should be pos-
sible to deﬁne a new type of object in an entirely data-driven manner.
However, in many engines, the services of a programmer are required
in order to add new game object types.
•
Unique object ids. Typical game worlds contain hundreds or even thou-
sands of individual game objects of various types. At runtime, it’s im-
portant to be able to identify or search for a particular object. This means
each object needs some kind of unique identiﬁer. A human-readable
name is the most convenient kind of id, but we must be wary of the
performance costs of using strings at runtime. Integer ids are the most
efﬁcient choice, but they are very difﬁcult for human game developers
to work with. Arguably the best solution is to use hashed string ids (see
Section 5.4.3.1) as our object identiﬁers, as they are as efﬁcient as integers
but can be converted back into string form for ease of reading.

872
15. Runtime Gameplay Foundation Systems
•
Game object queries. The gameplay foundation system must provide some
means of ﬁnding objects within the game world. We might want to ﬁnd
a speciﬁc object by its unique id, or all the objects of a particular type, or
we might want to perform advanced queries based on arbitrary criteria
(e.g., ﬁnd all enemies within a 20 m radius of the player character).
•
Game object references.
Once we’ve found the objects, we need some
mechanism for holding references to them, either brieﬂy within a sin-
gle function or for much longer periods of time. An object reference
might be as simple as a pointer to a C++ class instance, or it might
be something more sophisticated, like a handle or a reference-counted
smart pointer.
•
Finite state machine support. Many types of game objects are best modeled
as ﬁnite state machines (FSM). Some game engines provide the ability
for a game object to exist in one of many possible states, each with its
own attributes and behavioral characteristics.
•
Network replication. In a networked multiplayer game, multiple game
machines are connected together via a LAN or the Internet. The state of
a particular game object is usually owned and managed by one machine.
However, that object’s state must also be replicated (communicated) to
the other machines involved in the multiplayer game so that all players
have a consistent view of the object.
•
Saving and loading / object persistence. Many game engines allow the cur-
rent states of the game objects in the world to be saved to disk and later
reloaded. This might be done to support a “save anywhere” save-game
system or as a way of implementing network replication, or it might
simply be the primary means of loading game world chunks that were
authored in the world editor tool. Object persistence usually requires
certain language features, such as runtime type identiﬁcation (RTTI), reﬂec-
tion and abstract construction. RTTI and reﬂection provide software with
a means of determining an object’s type, and what attributes and methods
its class provides, dynamically at runtime. Abstract construction allows
instances of a class to be created without having to hard-code the name
of the class—a very useful feature when serializing an object instance
into memory from disk. If RTTI, reﬂection and abstract construction are
not natively supported in your language of choice, these features can be
added manually.
We’ll spend the remainder of this chapter delving into each of these sub-
systems in depth.

15.2. Runtime Object Model Architectures
873
15.2
Runtime Object Model Architectures
In the world editor, the game designer is presented with an abstract game
object model, which deﬁnes the various types of dynamic elements that can
exist in the game, how they behave and what kinds of attributes they have.
At runtime, the gameplay foundation system must provide a concrete imple-
mentation of this object model. This is by far the largest component of any
gameplay foundation system.
The runtime object model implementation may or may not bear any re-
semblance to the abstract tool-side object model. For example, it might not be
implemented in an object-oriented programming language at all, or it might
use a collection of interconnected class instances to represent a single abstract
game object. Whatever its design, the runtime object model must provide a
faithful reproduction of the object types, attributes and behaviors advertised
by the world editor.
The runtime object model is the in-game manifestation of the abstract tool-
side object model presented to the designers in the world editor. Designs vary
widely, but most game engines follow one of two basic architectural styles:
•
Object-centric. In this style, each tool-side game object is represented at
runtime by a single class instance or a small collection of interconnected
instances. Each object has a set of attributes and behaviors that are encap-
sulated within the class (or classes) of which the object is an instance.
The game world is just a collection of game objects.
•
Property-centric. In this style, each tool-side game object is represented
only by a unique id (implemented as an integer, hashed string id or
string). The properties of each game object are distributed across many
data tables, one per property type, and keyed by object id (rather than
being centralized within a single class instance or collection of intercon-
nected instances). The properties themselves are often implemented as
instances of hard-coded classes. The behavior of a game object is implic-
itly deﬁned by the collection of properties of which it is composed. For
example, if an object has the “Health” property, then it can be damaged,
lose health and eventually die. If an object has the “MeshInstance” prop-
erty, then it can be rendered in 3D as an instance of a triangle mesh.
There are distinct advantages and disadvantages to each of these architec-
tural styles. We’ll investigate each one in some detail and note where one style
has signiﬁcant potential beneﬁts over the other as they arise.

874
15. Runtime Gameplay Foundation Systems
15.2.1
Object-Centric Architectures
In an object-centric game world object architecture, each logical game object
is implemented as an instance of a class, or possibly a collection of intercon-
nected class instances. Under this broad umbrella, many different designs are
possible. We’ll investigate a few of the most common designs in the following
sections.
15.2.1.1
A Simple Object-Based Model in C: Hydro Thunder
Game object models needn’t be implemented in an object-oriented language
like C++ at all. For example, the arcade hit Hydro Thunder, by Midway Home
Entertainment in San Diego, was written entirely in C. Hydro employed a very
simple game object model consisting of only a few object types:
•
boats (player- and AI-controlled),
•
ﬂoating blue and red boost icons,
•
ambient animated objects (animals on the side of the track, etc.),
•
the water surface,
•
ramps,
•
waterfalls,
•
particle effects,
•
race track sectors (two-dimensional polygonal regions connected to one
another that together deﬁne the watery region in which boats could
race),
•
static geometry (terrain, foliage, buildings along the sides of the track,
etc.), and
•
two-dimensional heads-up display (HUD) elements.
A few screenshots of Hydro Thunder are shown in Figure 15.1. Notice the
hovering boost icons in both screenshots and the shark swimming by in the
left image (an example of an ambient animated object).
Hydro had a C struct named World_t that stored and managed the con-
tents of a game world (i.e., a single race track). The world contained pointers
to arrays of various kinds of game objects. The static geometry was a single
mesh instance. The water surface, waterfalls and particle effects were each
represented by custom data structures. The boats, boost icons and other dy-
namic objects in the game were represented by instances of a general-purpose
struct called WorldOb_t (i.e., a world object). This was Hydro’s equivalent of
a game object as we’ve deﬁned it in this chapter.

15.2. Runtime Object Model Architectures
875
Figure 15.1.
Screenshots from the arcade smash Hydro Thunder, developed by Midway Home
Entertainment in San Diego.
The WorldOb_t data structure contained data members encoding the po-
sition and orientation of the object, the 3D mesh used to render it, a set of colli-
sion spheres, simple animation state information (Hydro only supported rigid
hierarchical animation), physical properties like velocity, mass and buoyancy,
and other data common to all of the dynamic objects in the game. In addi-
tion, each WorldOb_t contained three pointers: a void* “user data” pointer,
a pointer to a custom “update” function and a pointer to a custom “draw”
function. So while Hydro Thunder was not object-oriented in the strictest sense,
the Hydro engine did extend its non-object-oriented language (C) to support
rudimentary implementations of two important OOP features: inheritance and
polymorphism. The user data pointer permitted each type of game object to
maintain custom state information speciﬁc to its type while inheriting the fea-
tures common to all world objects. For example, the Banshee boat had a dif-
ferent booster mechanism than the Rad Hazard, and each booster mechanism
required different state information to manage its deployment and stowing
animations. The two function pointers acted like virtual functions, allowing
world objects to have polymorphic behaviors (via their “update” functions)
and polymorphic visual appearances (via their “draw” functions).
struct WorldOb_s
{
Orient_t m_transform;
/* position/rotation */
Mesh3d*
m_pMesh;
/* 3D mesh */
/* ... */
void*
m_pUserData;
/* custom state */
void
(*m_pUpdate)(); /* polymorphic update */
void
(*m_pDraw)();
/* polymorphic draw */
};
typedef struct WorldOb_s WorldOb_t;

876
15. Runtime Gameplay Foundation Systems
Figure 15.2. A hypothetical class hierarchy for the game Pac-Man.
15.2.1.2
Monolithic Class Hierarchies
It’s natural to want to classify game object types taxonomically. This tends
to lead game programmers toward an object-oriented language that supports
inheritance. A class hierarchy is the most intuitive and straightforward way to
represent a collection of interrelated game object types. So it is not surprising
that the majority of commercial game engines employ a class hierarchy based
technique.
Figure 15.2 shows a simple class hierarchy that could be used to implement
the game Pac-Man. This hierarchy is rooted (as many are) at a common class
called GameObject, which might provide some facilities needed by all object
types, such as RTTI or serialization. The MovableObject class represents
any object that has a position and orientation. RenderableObject gives
the object an ability to be rendered (in the case of traditional Pac-Man, via a
sprite, or in the case of a modern 3D Pac-Man game, perhaps via a triangle
mesh). From RenderableObject are derived classes for the ghosts, Pac-
Man, pellets and power pills that make up the game. This is just a hypothetical
example, but it illustrates the basic ideas that underlie most game object class
hierarchies—namely that common, generic functionality tends to exist at the
root of the hierarchy, while classes toward the leaves of the hierarchy tend to
add increasingly speciﬁc functionality.
A game object class hierarchy usually begins small and simple, and in that
form, it can be a powerful and intuitive way to describe a collection of game
object types. However, as class hierarchies grow, they have a tendency to
deepen and widen simultaneously, leading to what I call a monolithic class
hierarchy. This kind of hierarchy arises when virtually all classes in the game
object model inherit from a single, common base class. The Unreal Engine’s
game object model is a classic example, as Figure 15.3 illustrates.

15.2. Runtime Object Model Architectures
877
Figure 15.3. An excerpt from the game object class hierarchy in the Unreal engine.
15.2.1.3
Problems with Deep, Wide Hierarchies
Monolithic class hierarchies tend to cause problems for the game develop-
ment team for a wide range of reasons. The deeper and wider a class hier-
archy grows, the more extreme these problems can become. In the following
sections, we’ll explore some of the most common problems caused by wide,
deep class hierarchies.
Understanding, Maintaining and Modifying Classes
The deeper a class lies within a class hierarchy, the harder it is to understand,
maintain and modify. This is because to understand a class, you really need
to understand all of its parent classes as well. For example, modifying the

878
15. Runtime Gameplay Foundation Systems
behavior of an innocuous-looking virtual function in a derived class could
violate the assumptions made by any one of the many base classes, leading to
subtle, difﬁcult-to-ﬁnd bugs.
Inability to Describe Multidimensional Taxonomies
A hierarchy inherently classiﬁes objects according to a particular system of
criteria known as a taxonomy. For example, biological taxonomy (also known
as alpha taxonomy) classiﬁes all living things according to genetic similarities,
using a tree with eight levels: domain, kingdom, phylum, class, order, family,
genus and species. At each level of the tree, a different criterion is used to
divide the myriad life forms on our planet into more and more reﬁned groups.
One of the biggest problems with any hierarchy is that it can only classify
objects along a single “axis”—according to one particular set of criteria—at
each level of the tree. Once the criteria have been chosen for a particular hier-
archy, it becomes difﬁcult or impossible to classify along an entirely different
set of “axes.” For example, biological taxonomy classiﬁes objects according to
genetic traits, but it says nothing about the colors of the organisms. In order
to classify organisms by color, we’d need an entirely different tree structure.
In object-oriented programming, this limitation of hierarchical classiﬁca-
tion often manifests itself in the form of wide, deep and confusing class hier-
archies. When one analyzes a real game’s class hierarchy, one often ﬁnds that
its structure attempts to meld a number of different classiﬁcation criteria into a
single class tree. In other cases, concessions are made in the class hierarchy to
accommodate a new type of object whose characteristics were not anticipated
when the hierarchy was ﬁrst designed. For example, imagine the seemingly
logical class hierarchy describing different types of vehicles, depicted in Fig-
ure 15.4.
Vehicle
Motorcycle
SpeedBoat
Car
Truck
Hovercraft
Yacht
LandVehicle
WaterVehicle
Figure 15.4. A seemingly logical class hierarchy describing various kinds of vehicles.

15.2. Runtime Object Model Architectures
879
What happens when the game designers announce to the programmers
that they now want the game to include an amphibious vehicle? Such a vehicle
does not ﬁt into the existing taxonomic system. This may cause the program-
mers to panic or, more likely, to “hack” their class hierarchy in various ugly
and error-prone ways.
Multiple Inheritance: The Deadly Diamond
One solution to the amphibious vehicle problem is to utilize C++’s multiple
inheritance (MI) features, as shown in Figure 15.5. At ﬁrst glance, this seems
like a good solution. However, multiple inheritance in C++ poses a number
of practical problems. For example, multiple inheritance can lead to an object
that contains multiple copies of its base class’ members—a condition known
as the “deadly diamond” or “diamond of death.” (See Section 3.1.1.3 for more
details.)
The difﬁculties in building an MI class hierarchy that works and that is
understandable and maintainable usually outweigh the beneﬁts. As a result,
most game studios prohibit or severely limit the use of multiple inheritance
in their class hierarchies.
Figure 15.5. A diamond-shaped class hierarchy for amphibious vehicles.
Mix-In Classes
Some teams do permit a limited form of MI, in which a class may have any
number of parent classes but only one grandparent. In other words, a class
may inherit from one and only one class in the main inheritance hierarchy,
but it may also inherit from any number of mix-in classes (stand-alone classes
with no base class). This permits common functionality to be factored out
into a mix-in class and then spot-patched into the main hierarchy wherever
it is needed. This is shown in Figure 15.6. However, as we’ll see below, it’s
usually better to compose or aggregate such classes than to inherit from them.

880
15. Runtime Gameplay Foundation Systems
GameObject
+GetHealth()
+ApplyDamage()
+IsDead()
+OnDeath()
MHealth
+PickUp()
+Drop()
+IsBeingCarried()
MCarryable
NPC
Player
Tank
Jeep
Pistol
MG
Canteen
Ammo
Character
Vehicle
Weapon
Item
Figure 15.6. A class hierarchy with mix-in classes. The MHealth mix-in class adds the notion of
health and the ability to be killed to any class that inherits it. The MCarryable mix-in class allows
an object that inherits it to be carried by a Character.
The Bubble-Up Effect
When a monolithic class hierarchy is ﬁrst designed, the root class or classes are
usually very simple, each one exposing only a minimal feature set. However,
as more and more functionality is added to the game, the desire to share code
between two or more unrelated classes begins to cause features to “bubble up”
the hierarchy.
For example, we might start out with a design in which only wooden
crates can ﬂoat in water. However, once our game designers see those cool
ﬂoating crates, they begin to ask for other kinds of ﬂoating objects, like charac-
ters, bits of paper, vehicles and so on. Because “ﬂoating versus non-ﬂoating”
was not one of the original classiﬁcation criteria when the hierarchy was de-
signed, the programmers quickly discover the need to add ﬂotation to classes
that are totally unrelated within the class hierarchy. Multiple inheritance is
frowned upon, so the programmers decide to move the ﬂotation code up the
hierarchy, into a base class that is common to all objects that need to ﬂoat.
The fact that some of the classes that derive from this common base class
cannot ﬂoat is seen as less of a problem than duplicating the ﬂotation code
across multiple classes. (A Boolean member variable called something like
m_bCanFloat might even be added to make the distinction clear.) The ulti-
mate result is that ﬂotation eventually becomes a feature of the root object in
the class hierarchy (along with pretty much every other feature in the game).
The Actor class in Unreal is a classic example of this “bubble-up effect.” It
contains data members and code for managing rendering, animation, physics,

15.2. Runtime Object Model Architectures
881
world interaction, audio effects, network replication for multiplayer games,
object creation and destruction, actor iteration (i.e., the ability to iterate over
all actors meeting a certain criteria and perform some operation on them),
and message broadcasting. Encapsulating the functionality of various engine
subsystems is difﬁcult when features are permitted to “bubble up” to the root-
most classes in a monolithic class hierarchy.
15.2.1.4
Using Composition to Simplify the Hierarchy
Perhaps the most prevalent cause of monolithic class hierarchies is over-use
of the “is-a” relationship in object-oriented design. For example, in a game’s
GUI, a programmer might decide to derive the class Window from a class
called Rectangle, using the logic that GUI windows are always rectangular.
However, a window is not a rectangle—it has a rectangle, which deﬁnes its
boundary. So a more workable solution to this particular design problem is to
embed an instance of the Rectangle class inside the Window class, or to give
the Window a pointer or reference to a Rectangle.
In object-oriented design, the “has-a” relationship is known as composition.
In composition, a class A either contains an instance of class B directly, or con-
tains a pointer or reference to an instance of B. Strictly speaking, in order for
the term “composition” to be applicable, class A must own class B. This means
that when an instance of class A is created, it automatically creates an instance
of class B as well; when that instance of A is destroyed, its instance of B is de-
stroyed, too. We can also link classes to one another via a pointer or reference
without having one of the classes manage the other’s lifetime. In that case, the
technique is usually called aggregation.
Converting Is-A to Has-A
Converting “is-a” relationships into “has-a” relationships can be a useful tech-
nique for reducing the width, depth and complexity of a game’s class hier-
archy. To illustrate, let’s take a look at the hypothetical monolithic hierar-
chy shown in Figure 15.7. The root GameObject class provides some ba-
sic functionality required by all game objects (e.g., RTTI, reﬂection, persis-
tence via serialization, network replication, etc.). The MovableObject class
represents any game object that has a transform (i.e., a position, orientation
and optional scale). RenderableObject adds the ability to be rendered on-
screen. (Not all game objects need to be rendered—for example, an invisible
TriggerRegion class could be derived directly from MovableObject.) The
CollidableObject class provides collision information to its instances. The
AnimatingObject class grants to its instances the ability to be animated via

882
15. Runtime Gameplay Foundation Systems
a skeletal joint hierarchy. Finally, the PhysicalObject gives its instances the
ability to be physically simulated (e.g., a rigid body falling under the inﬂuence
of gravity and bouncing around in the game world).
Figure 15.7.
A hy-
pothetical game ob-
ject class hierarchy
using
only
inheri-
tance to associate
the classes.
One big problem with this class hierarchy is that it limits our design
choices when creating new types of game objects. If we want to deﬁne an
object type that is physically simulated, we are forced to derive its class
from PhysicalObject even though it may not require skeletal anima-
tion.
If we want a game object class with collision, it must inherit from
CollidableObject even though it may be invisible and hence not require
the services of RenderableObject.
A second problem with the hierarchy shown in Figure 15.7 is that it is
difﬁcult to extend the functionality of the existing classes.
For example,
let’s imagine we want to support morph target animation, so we derive two
new classes from AnimatingObject called SkeletalObject and Morph-
TargetObject. If we wanted both of these new classes to have the abil-
ity to be physically simulated, we’d be forced to refactor PhysicalObject
into two nearly identical classes, one derived from SkeletalObject and
one from MorphTargetObject, or turn to multiple inheritance.
One solution to these problems is to isolate the various features of a
GameObject into independent classes, each of which provides a single, well-
deﬁned service. Such classes are sometimes called components or service objects.
A componentized design allows us to select only those features we need for
each type of game object we create. In addition, it permits each feature to
be maintained, extended or refactored without affecting the others. The indi-
vidual components are also easier to understand, and easier to test, because
they are decoupled from one another. Some component classes correspond
directly to a single engine subsystem, such as rendering, animation, collision,
physics, audio, etc. This allows these subsystems to remain distinct and well-
encapsulated when they are integrated together for use by a particular game
object.
Figure 15.8 shows how our class hierarchy might look after refactoring it
into components. In this revised design, the GameObject class acts like a hub,
containing pointers to each of the optional components we’ve deﬁned. The
MeshInstance component is our replacement for the RenderableObject
class—it represents an instance of a triangle mesh and encapsulates the knowl-
edge of how to render it. Likewise, the AnimationController compo-
nent replaces AnimatingObject, exposing skeletal animation services to the
GameObject. Class Transform replaces MovableObject by maintaining
the position, orientation and scale of the object. The RigidBody class repre-
sents the collision geometry of a game object and provides its GameObject

15.2. Runtime Object Model Architectures
883
GameObject
Transform
MeshInstance
AnimationController
RigidBody
1
1
1
1
1
1
1
1
Figure 15.8. Our hypothetical game object class hierarchy, refactored to favor class composition
over inheritance.
with an interface into the low-level collision and physics systems, replacing
CollidableObject and PhysicalObject.
Component Creation and Ownership
In this kind of design, it is typical for the “hub” class to own its compo-
nents, meaning that it manages their lifetimes. But how should a GameObject
“know” which components to create? There are numerous ways to solve this
problem, but one of the simplest is to provide the root GameObject class
with pointers to all possible components. Each unique type of game object is
deﬁned as a derived class of GameObject. In the GameObject constructor,
all of the component pointers are initially set to NULL. Each derived class’s
constructor is then free to create whatever components it may need. For con-
venience, the default GameObject destructor can clean up all of the com-
ponents automatically. In this design, the hierarchy of classes derived from
GameObject serves as the primary taxonomy for the kinds of objects we want
in our game, and the component classes serve as optional add-on features.
One possible implementation of the component creation and destruction
logic for this kind of hierarchy is shown below. However, it’s important to
realize that this code is just an example—implementation details vary widely,
even between engines that employ essentially the same kind of class hierarchy
design.
class GameObject
{
protected:

884
15. Runtime Gameplay Foundation Systems
// My transform (position, rotation, scale).
Transform
m_transform;
// Standard components:
MeshInstance*
m_pMeshInst;
AnimationController* m_pAnimController;
RigidBody*
m_pRigidBody;
public:
GameObject()
{
// Assume no components by default.
// Derived classes will override.
m_pMeshInst = NULL;
m_pAnimController = NULL;
m_pRigidBody = NULL;
}
~GameObject()
{
// Automatically delete any components created by
// derived classes. (Deleting null pointers OK.)
delete m_pMeshInst;
delete m_pAnimController;
delete m_pRigidBody;
}
// ...
};
class Vehicle : public GameObject
{
protected:
// Add some more components specific to Vehicles...
Chassis* m_pChassis;
Engine*
m_pEngine;
// ...
public:
Vehicle()
{
// Construct standard GameObject components.
m_pMeshInst = new MeshInstance;
m_pRigidBody = new RigidBody;
// NOTE: We'll assume the animation controller
// must be provided with a reference to the mesh

15.2. Runtime Object Model Architectures
885
// instance so that it can provide it with a
// matrix palette.
m_pAnimController
= new AnimationController(*m_pMeshInst);
// Construct vehicle-specific components.
m_pChassis = new Chassis(*this,
*m_pAnimController);
m_pEngine = new Engine(*this);
}
~Vehicle()
{
// Only need to destroy vehicle-specific
// components, as GameObject cleans up the
// standard components for us.
delete m_pChassis;
delete m_pEngine;
}
};
15.2.1.5
Generic Components
Another more ﬂexible (but also trickier to implement) alternative is to provide
the root game object class with a generic linked list of components. The com-
ponents in such a design usually all derive from a common base class—this
allows us to iterate over the linked list and perform polymorphic operations,
such as asking each component what type it is or passing an event to each
component in turn for possible handling. This design allows the root game
object class to be largely oblivious to the component types that are available
and thereby permits new types of components to be created without modify-
ing the game object class in many cases. It also allows a particular game object
to contain an arbitrary number of instances of each type of component. (The
hard-coded design permits only a ﬁxed number, determined by how many
pointers to each component exist within the game object class.)
This kind of design is illustrated in Figure 15.9. It is trickier to implement
than a hard-coded component model because the game object code must be
written in a totally generic way. The component classes can likewise make no
assumptions about what other components might or might not exist within
the context of a particular game object. The choice between hard-coding the
component pointers or using a generic linked list of components is not an
easy one to make. Neither design is clearly superior—they each have their
pros and cons, and different game teams take different approaches.

886
15. Runtime Gameplay Foundation Systems
Asterisk indicates zero 
or more instances 
(e.g., linked list).
Figure 15.9. A linked list of components can provide ﬂexibility by allowing the hub game object to
be unaware of the details of any particular component.
15.2.1.6
Pure Component Models
What would happen if we were to take the componentization concept to its
extreme? We would move literally all of the functionality out of our root
GameObject class into various component classes. At this point, the game
object class would quite literally be a behavior-less container, with a unique id
and a bunch of pointers to its components, but otherwise containing no logic
of its own. So why not eliminate the class entirely? One way to do this is to
give each component a copy of the game object’s unique id. The components
are now linked together into a logical grouping by id. Given a way to quickly
look up any component by id, we would no longer need the GameObject
“hub” class at all. I will use the term pure component model to describe this
kind of architecture. It is illustrated in Figure 15.10.
-m_uniqueId : int = 72
GameObject
-m_uniqueId : int = 72
Transform
-m_uniqueId : int = 72
MeshInstance
-m_uniqueId : int = 72
AnimationController
-m_uniqueId : int = 72
RigidBody
Figure 15.10. In a pure component model, a logical game object is comprised of many components,
but the components are linked together only indirectly, by sharing a unique id.

15.2. Runtime Object Model Architectures
887
A pure component model is not quite as simple as it ﬁrst sounds, and it
is not without its share of problems. For one thing, we still need some way
of deﬁning the various concrete types of game objects our game needs and
then arranging for the correct component classes to be instantiated whenever
an instance of the type is created. Our GameObject hierarchy used to handle
construction of components for us. Instead, we might use a factory pattern,
in which we deﬁne factory classes, one per game object type, with a virtual
construction function that is overridden to create the proper components for
each game object type. Or we might turn to a data-driven model, where the
game object types are deﬁned in a text ﬁle that can be parsed by the engine
and consulted whenever a type is instantiated.
Another issue with a components-only design is inter-component commu-
nication. Our central GameObject acted as a “hub,” marshalling communi-
cations between the various components. In pure component architectures,
we need an efﬁcient way for the components making up a single game object
to talk to one another. This could be done by having each component look up
the other components using the game object’s unique id. However, we prob-
ably want a much more efﬁcient mechanism—for example the components
could be prewired into a circular linked list.
In the same sense, sending messages from one game object to another is
difﬁcult in a pure componentized model. We can no longer communicate with
the GameObject instance, so we either need to know a priori with which
component we wish to communicate, or we must multicast to all components
that make up the game object in question. Neither option is ideal.
Pure component models can and have been made to work on real game
projects. These kinds of models have their pros and cons, but again, they are
not clearly better than any of the alternative designs. Unless you’re part of a
research and development effort, you should probably choose the architecture
with which you are most comfortable and conﬁdent, and which best ﬁts the
needs of the particular game you are building.
15.2.2
Property-Centric Architectures
Programmers who work frequently in an object-oriented programming lan-
guage tend to think naturally in terms of objects that contain attributes (data
members) and behaviors (methods, member functions). This is the object-
centric view:
•
Object1
◦
Position = (0, 3, 15)
◦
Orientation = (0, 43, 0)

888
15. Runtime Gameplay Foundation Systems
•
Object2
◦
Position = (−12, 0, 8)
◦
Health = 15
•
Object3
◦
Orientation = (0, −87, 10)
However, it is possible to think primarily in terms of the attributes, rather
than the objects. We deﬁne the set of all properties that a game object might
have. Then for each property, we build a table containing the values of that
property corresponding to each game object that has it. The property values
are keyed by the objects’ unique ids. This is what we will call the property-
centric view:
•
Position
◦
Object1 = (0, 3, 15)
◦
Object2 = (−12, 0, 8)
•
Orientation
◦
Object1 = (0, 43, 0)
◦
Object3 = (0, −87, 10)
•
Health
◦
Object2 = 15
Property-centric object models have been used very successfully on many
commercial games, including Deus Ex 2 and the Thief series of games. See
Section 15.2.2.5 for more details on exactly how these projects designed their
object systems.
A property-centric design is more akin to a relational database than an ob-
ject model. Each attribute acts like a table in a relational database, with the
game objects’ unique id as the primary key. Of course, in object-oriented de-
sign, an object is deﬁned not only by its attributes, but also by its behavior. If all
we have are tables of properties, then where do we implement the behavior?
The answer to this question varies somewhat from engine to engine, but most
often the behaviors are implemented in one or both of the following places:
•
in the properties themselves, and/or
•
via script code.
Let’s explore each of these ideas further.

15.2. Runtime Object Model Architectures
889
15.2.2.1
Implementing Behavior via Property Classes
Each type of property can be implemented as a property class. Properties can
be as simple as a single Boolean or ﬂoating-point value or as complex as a
renderable triangle mesh or an AI “brain.” Each property class can provide
behaviors via its hard-coded methods (member functions). The overall be-
havior of a particular game object is determined by the aggregation of the
behaviors of all its properties.
For example, if a game object contains an instance of the Health property,
it can be damaged and eventually destroyed or killed. The Health object
can respond to any attacks made on the game object by decrementing the ob-
ject’s health level appropriately. A property object can also communicate with
other property objects within the same game object to produce cooperative
behaviors. For example, when the Health property detects and responds
to an attack, it could possibly send a message to the AnimatedSkeleton
property, thereby allowing the game object to play a suitable hit reaction ani-
mation. Similarly, when the Health property detects that the game object is
about to die or be destroyed, it can talk to the RigidBodyDynamics property
to activate a physics-driven explosion or a “rag doll” dead body simulation.
15.2.2.2
Implementing Behavior via Script
Another option is to store the property values as raw data in one or more
database-like tables and use script code to implement a game object’s behav-
iors. Every game object could have a special property called something like
ScriptId, which, if present, speciﬁes the block of script code (script func-
tion, or script object if the scripting language is itself object-oriented) that will
manage the object’s behavior. Script code could also be used to allow a game
object to respond to events that occur within the game world. See Section 15.7
for more details on event systems and Section 15.8 for a discussion of game
scripting languages.
In some property-centric engines, a core set of hard-coded property classes
are provided by the engineers, but a facility is provided allowing game de-
signers and programmers to implement new property types entirely in script.
This approach was used successfully on the Dungeon Siege project, for exam-
ple.
15.2.2.3
Properties versus Components
It’s important to note that many of the authors cited in Section 15.2.2.5 use
the term “component” to refer to what I call a “property object” here. In Sec-

890
15. Runtime Gameplay Foundation Systems
tion 15.2.1.4, I used the term “component” to refer to a subobject in an object-
centric design, which isn’t quite the same as a property object.
However, property objects are very closely related to components in many
ways. In both designs, a single logical game object is made up of multiple sub-
objects. The main distinction lies in the roles of the subobjects. In a property-
centric design, each subobject deﬁnes a particular attribute of the game object
itself (e.g., health, visual representation, inventory, a particular magic power,
etc.), whereas in a component-based (object-centric) design, the subobjects of-
ten represent linkages to particular low-level engine subsystems (renderer, an-
imation, collision and dynamics, etc.) This distinction is so subtle as to be
virtually irrelevant in many cases. You can call your design a pure component
model (Section 15.2.1.6) or a property-centric design as you see ﬁt, but at the end
of the day, you’ll have essentially the same result—a logical game object that
is comprised of, and derives its behavior from, a collection of subobjects.
15.2.2.4
Pros and Cons of Property-Centric Designs
There are a number of potential beneﬁts to an attribute-centric approach. It
tends to be more memory-efﬁcient, because we need only store attribute data
that is actually in use (i.e., there are never game objects with unused data
members). It is also easier to construct such a model in a data-driven manner—
designers can deﬁne new attributes easily, without recompiling the game,
because there are no game object class deﬁnitions to be changed. Program-
mers need only get involved when entirely new types of properties need to be
added (presuming the property cannot be added via script).
A property-centric design can also be more cache-friendly than an object-
centric model, because data of the same type is stored contiguously in memory.
This is a commonplace optimization technique on modern gaming hardware,
where the cost of accessing memory is far higher than the cost of executing
instructions and performing calculations. (For example, on the PlayStation 3,
the cost of a single cache miss is equivalent to the cost of executing literally
thousands of CPU instructions.) By storing data contiguously in RAM, we
can reduce or eliminate cache misses, because when we access one element of
a data array, a large number of its neighboring elements are loaded into the
same cache line. This approach to data design is sometimes called the struct of
arrays technique, in contrast to the more-traditional array of structs approach.
The differences between these two memory layouts are illustrated by the code
snippet below. (Note that we wouldn’t really implement a game object model
in exactly this way—this example is meant only to illustrate the way in which
a property-centric design tends to produce many contiguous arrays of like-
typed data, rather than a single array of complex objects.)

15.2. Runtime Object Model Architectures
891
static const U32 MAX_GAME_OBJECTS = 1024;
// Traditional array-of-structs approach.
struct GameObject
{
U32
m_uniqueId;
Vector
m_pos;
Quaternion
m_rot;
float
m_health;
// ...
};
GameObject g_aAllGameObjects[MAX_GAME_OBJECTS];
// Cache-friendlier struct-of-arrays approach.
struct AllGameObjects
{
U32
m_aUniqueId[MAX_GAME_OBJECTS];
Vector
m_aPos[MAX_GAME_OBJECTS];
Quaternion
m_aRot[MAX_GAME_OBJECTS];
float
m_aHealth[MAX_GAME_OBJECTS];
// ...
};
AllGameObjects g_allGameObjects;
Attribute-centric models have their share of problems as well. For exam-
ple, when a game object is just a grab bag of properties, it becomes much
more difﬁcult to enforce relationships between those properties. It can be hard
to implement a desired large-scale behavior merely by cobbling together the
ﬁne-grained behaviors of a group of property objects. It’s also much trickier
to debug such systems, as the programmer cannot slap a game object into the
watch window in the debugger in order to inspect all of its properties at once.
15.2.2.5
Further Reading
A number of interesting PowerPoint presentations on the topic of property-
centric architectures have been given by prominent engineers in the game in-
dustry at various game development conferences.
•
Rob Fermier, “Creating a Data Driven Engine,” Game Developer’s Con-
ference, 2002.
•
Scott Bilas, “A Data-Driven Game Object System,” Game Developer’s
Conference, 2002.

892
15. Runtime Gameplay Foundation Systems
•
Alex Duran, “Building Object Systems: Features, Tradeoffs, and Pit-
falls,” Game Developer’s Conference, 2003.
•
Jeremy Chatelaine, “Enabling Data Driven Tuning via Existing Tools,”
Game Developer’s Conference, 2003.
•
Doug Church, “Object Systems,” presented at a game development con-
ference in Seoul, Korea, 2003; conference organized by Chris Hecker,
Casey Muratori, Jon Blow and Doug Church. http://chrishecker.com/
images/6/6f/ObjSys.ppt.
15.3
World Chunk Data Formats
As we’ve seen, a world chunk generally contains both static and dynamic world
elements. The static geometry might be represented by one big triangle mesh,
or it might be comprised of many smaller meshes. Each mesh might be in-
stanced multiple times—for example, a single door mesh might be reused for
all of the doorways in the chunk. The static data usually includes collision
information stored as a triangle soup, a collection of convex shapes and/or
other simpler geometric shapes like planes, boxes, capsules or spheres. Other
static elements include volumetric regions that can be used to detect events or
delineate areas within the game world, an AI navigation mesh, a set of line seg-
ments delineating edges within the background geometry that can be grabbed
by the player character and so on. We won’t get into the details of these data
formats here, because we’ve already discussed most of them in previous sec-
tions.
The dynamic portion of the world chunk contains some kind of represen-
tation of the game objects within that chunk. A game object is deﬁned by
its attributes and its behaviors, and an object’s behaviors are determined either
directly or indirectly by its type. In an object-centric design, the object’s type
directly determines which class(es) to instantiate in order to represent the ob-
ject at runtime. In a property-centric design, a game object’s behavior is deter-
mined by the amalgamation of the behaviors of its properties, but the type still
determines which properties the object should have (or one might say that an
object’s properties deﬁne its type). So, for each game object, a world chunk
data ﬁle generally contains:
•
The initial values of the objects’ attributes. The world chunk deﬁnes the
state of each game object as it should exist when ﬁrst spawned into the
game world. An object’s attribute data can be stored in a number of
different formats. We’ll explore a few popular formats below.

15.3. World Chunk Data Formats
893
•
Some kind of speciﬁcation of the object’s type. In an object-centric engine,
this might be a string, a hashed string id or some other unique type id.
In a property-centric design, the type might be stored explicitly, or it
might be deﬁned implicitly by the collection of properties/attributes of
which the object is comprised.
15.3.1
Binary Object Images
One way to store a collection of game objects into a disk ﬁle is to write a binary
image of each object into the ﬁle, exactly as it looks in memory at runtime.
This makes spawning game objects trivial. Once the game world chunk has
been loaded into memory, we have ready-made images of all our objects, so
we simply let them ﬂy.
Well, not quite. Storing binary images of “live” C++ class instances is prob-
lematic for a number of reasons, including the need to handle pointers and
virtual tables in a special way, and the possibility of having to endian-swap the
data within each class instance. (These techniques are described in detail in
Section 6.2.2.9.) Moreover, binary object images are inﬂexible and not robust
to making changes. Gameplay is one of the most dynamic and unstable as-
pects of any game project, so it is wise to select a data format that supports
rapid development and is robust to frequent changes. As such, the binary
object image format is not usually a good choice for storing game object data
(although this format can be suitable for more stable data structures, like mesh
data or collision geometry).
15.3.2
Serialized Game Object Descriptions
Serialization is another means of storing a representation of a game object’s in-
ternal state to a disk ﬁle. This approach tends to be more portable and simpler
to implement than the binary object image technique. To serialize an object out
to disk, the object is asked to produce a stream of data that contains enough
detail to permit the original object to be reconstructed later. When an object is
serialized back into memory from disk, an instance of the appropriate class is
created, and then the stream of attribute data is read in order to initialize the
new object’s internal state. If the original serialized data stream was complete,
the new object should be identical to the original for all intents and purposes.
Serialization is supported natively by some programming languages. For
example, C# and Java both provide standardized mechanisms for serializing
object instances to and from an XML text format. The C++ language unfortu-
nately does not provide a standardized serialization facility. However, many
C++ serialization systems have been successfully built, both inside and out-

894
15. Runtime Gameplay Foundation Systems
side the game industry. We won’t get into all the details of how to write a
C++ object serialization system here, but we’ll describe the data format and a
few of the main systems that need to be written in order to get serialization to
work in C++.
Serialization data isn’t a binary image of the object. Instead, it is usually
stored in a more-convenient and more-portable format. XML is a popular for-
mat for object serialization because it is well-supported and standardized, it is
somewhat human-readable and it has excellent support for hierarchical data
structures, which arise frequently when serializing collections of interrelated
game objects. Unfortunately, XML is notoriously slow to parse, which can
increase world chunk load times. For this reason, some game engines use a
proprietary binary format that is faster to parse and more compact than XML
text.
Many game engines (and non-game object serialization systems) have
turned to the text-based JSON data format (http://www.json.org) as an alter-
native to XML. JSON is also used ubiquitously for data communication over
the World Wide Web. For example, the Facebook API communicates exclu-
sively using JSON.
The mechanics of serializing an object to and from disk are usually imple-
mented in one of two basic ways:
•
We can introduce a pair of virtual functions called something like
SerializeOut() and SerializeIn() in our base class and arrange
for each derived class to provide custom implementations of them that
“know” how to serialize the attributes of that particular class.
•
We can implement a reﬂection system for our C++ classes. We can then
write a generic system that can automatically serialize any C++ object
for which reﬂection information is available.
Reﬂection is a term used by the C# language, among others. In a nutshell,
reﬂection data is a runtime description of the contents of a class. It stores infor-
mation about the name of the class, what data members it contains, the types
of each data member and the offset of each member within the object’s mem-
ory image, and it also contains information about all of the class’s member
functions. Given reﬂection information for an arbitrary C++ class, we could
quite easily write a general-purpose object serialization system.
The tricky part of a C++ reﬂection system is generating the reﬂection data
for all of the relevant classes. This can be done by encapsulating a class’s data
members in #define macros that extract relevant reﬂection information by
providing a virtual function that can be overridden by each derived class in

15.3. World Chunk Data Formats
895
order to return appropriate reﬂection data for that class, by hand-coding a
reﬂection data structure for each class, or via some other inventive approach.
In addition to attribute information, the serialization data stream invari-
ably includes the name or unique id of each object’s class or type. The class id
is used to instantiate the appropriate class when the object is serialized into
memory from disk. A class id can be stored as a string, a hashed string id, or
some other kind of unique id.
Unfortunately, C++ provides no way to instantiate a class given only its
name as a string or id. The class name must be known at compile time, and
so it must be hard-coded by a programmer (e.g., new ConcreteClass). To
work around this limitation of the language, C++ object serialization systems
invariably include a class factory of some kind. A factory can be implemented
in any number of ways, but the simplest approach is to create a data table that
maps each class name/id to some kind of function or functor object that has
been hard-coded to instantiate that particular class. Given a class name or id,
we simply look up the corresponding function or functor in the table and call
it to instantiate the class.
15.3.3
Spawners and Type Schemas
Both binary object images and serialization formats have an Achilles heel.
They are both deﬁned by the runtime implementation of the game object types
they store, and hence they both require the world editor to contain intimate
knowledge of the game engine’s runtime implementation. For example, in
order for the world editor to write out a binary image of a heterogeneous
collection of game objects, it must either link directly with the runtime game
engine code, or it must be painstakingly hand-coded to produce blocks of
bytes that exactly match the data layout of the game objects at runtime. Seri-
alization data is less-tightly coupled to the game object’s implementation, but
again, the world editor either needs to link with runtime game object code in
order to gain access to the classes’ SerializeIn() and SerializeOut()
functions, or it needs access to the classes’ reﬂection information in some way.
The coupling between the game world editor and the runtime engine code
can be broken by abstracting the descriptions of our game objects in an imple-
mentation-independent way. For each game object in a world chunk data
ﬁle, we store a little block of data, often called a spawner. A spawner is a
lightweight, data-only representation of a game object that can be used to
instantiate and initialize that game object at runtime. It contains the id of
the game object’s tool-side type. It also contains a table of simple key-value
pairs that describe the initial attributes of the game object. These attributes

896
15. Runtime Gameplay Foundation Systems
often include a model-to-world transform, since most game objects have a
distinct position, orientation and scale in world space. When the game object
is spawned, the appropriate class or classes are instantiated, as determined by
the spawner’s type. These runtime objects can then consult the dictionary of
key-value pairs in order to initialize their data members appropriately.
A spawner can be conﬁgured to spawn its game object immediately upon
being loaded, or it can lie dormant until asked to spawn at some later time
during the game. Spawners can be implemented as ﬁrst-class objects, so they
can have a convenient functional interface and can store useful metadata in
addition to object attributes. A spawner can even be used for purposes other
than spawning game objects. For example, in both Uncharted and The Last of
Us, designers used spawners to deﬁne important points or coordinate axes
in the game world. These were called position spawners or locator spawners.
Locators have many uses in a game, such as:
•
deﬁning points of interest for an AI character,
•
deﬁning a set of coordinate axes relative to which a set of animations
can be played in perfect synchronization,
•
deﬁning the location at which a particle effect or audio effect should
originate,
•
deﬁning waypoints along a race track,
and the list goes on.
15.3.3.1
Object Type Schemas
A game object’s attributes and behaviors are deﬁned by its type. In a game
world editor that employs a spawner-based design, a game object type can
be represented by a data-driven schema that deﬁnes the collection of attributes
that should be visible to the user when creating or editing an object of that
type. At runtime, the tool-side object type can be mapped in either a hard-
coded or data-driven way to a class or collection of classes that must be in-
stantiated in order to spawn a game object of the given type.
Type schemas can be stored in a simple text ﬁle for consumption by the
world editor and for inspection and editing by its users.
For example, a
schema ﬁle might look something like this:
enum LightType
{
Ambient, Directional, Point, Spot
}

15.3. World Chunk Data Formats
897
type Light
{
String
UniqueId;
LightType
Type;
Vector
Pos;
Quaternion
Rot;
Float
Intensity : min(0.0), max(1.0);
ColorARGB
DiffuseColor;
ColorARGB
SpecularColor;
// ...
}
type Vehicle
{
String
UniqueId;
Vector
Pos;
Quaternion
Rot;
MeshReference
Mesh;
Int
NumWheels : min(2), max(4);
Float
TurnRadius;
Float
TopSpeed : min(0.0);
// ...
}
//...
The above example brings a few important details to light. You’ll notice
that the data types of each attribute are deﬁned, in addition to their names.
These can be simple types like strings, integers and ﬂoating-point values, or
they can be specialized types like vectors, quaternions, ARGB colors, or ref-
erences to special asset types like meshes, collision data and so on. In this
example, we’ve even provided a mechanism for deﬁning enumerated types,
like LightType. Another subtle point is that the object type schema pro-
vides additional information to the world editor, such as what type of GUI
element to use when editing the attribute. Sometimes an attribute’s GUI re-
quirements are implied by its data type—strings are generally edited with a
text ﬁeld, Booleans via a check box, vectors via three text ﬁelds for the x-, y-
and z-coordinates or perhaps via a specialized GUI element designed for ma-
nipulating vectors in 3D. The schema can also specify meta-information for
use by the GUI, such as minimum and maximum allowable values for integer
and ﬂoating-point attributes, lists of available choices for drop-down combo
boxes and so on.
Some game engines permit object type schemas to be inherited, much like
classes. For example, every game object needs to know its type and must have

898
15. Runtime Gameplay Foundation Systems
a unique id so that it can be distinguished from all the other game objects at
runtime. These attributes could be speciﬁed in a top-level schema, from which
all other schemas are derived.
15.3.3.2
Default Attribute Values
As you can well imagine, the number of attributes in a typical game object
schema can grow quite large. This translates into a lot of data that must be
speciﬁed by the game designer for each instance of each game object type he
or she places into the game world. It can be extremely helpful to deﬁne default
values in the schema for many of the attributes. This permits game designers
to place “vanilla” instances of a game object type with little effort but still
permits him or her to ﬁne-tune the attribute values on speciﬁc instances as
needed.
One inherent problem with default values arises when the default value
of a particular attribute changes. For example, our game designers might
have originally wanted Orcs to have 20 hit points. After many months of
production, it might be decided that Orcs should have a more powerful 30 hit
points by default. Any new Orcs placed into a game world will now have 30
hit points unless otherwise speciﬁed. But what about all the Orcs that were
placed into game world chunks prior to the change? Do we need to ﬁnd all of
these previously created Orcs and manually change their hit points to 30?
Ideally, we’d like to design our spawner system so that changes in de-
fault values automatically propagate to all preexisting instances that have not
had their default values overridden explicitly. One easy way to implement
this feature is to simply omit key-value pairs for attributes whose value does
not differ from the default value. Whenever an attribute is missing from the
spawner, the appropriate default can be used. (This presumes that the game
engine has access to the object type schema ﬁle, so that it can read in the at-
tributes’ default values. Either that or the tool can do it—in which case, prop-
agating new default values requires a simple rebuild of all world chunk(s) af-
fected by the change.) In our example, most of the preexisting Orc spawners
would have had no HitPoints key-value pair at all (unless of course one of
the spawner’s hit points had been changed from the default value manually).
So when the default value changes from 20 to 30, these Orcs will automatically
use the new value.
Some engines allow default values to be overridden in derived object types.
For example, the schema for a type called Vehicle might deﬁne a default
TopSpeed of 80 miles per hour. A derived Motorcycle type schema could
override this TopSpeed to be 100 miles per hour.

15.4. Loading and Streaming Game Worlds
899
15.3.3.3
Some Beneifts of Spawners and Type Schemas
The key beneﬁts of separating the spawner from the implementation of the
game object are simplicity, ﬂexibility and robustness. From a data management
point of view, it is much simpler to deal with a table of key-value pairs than it
is to manage a binary object image with pointer ﬁx-ups or a custom serialized
object format. The key-value pairs approach also makes the data format ex-
tremely ﬂexible and robust to changes. If a game object encounters key-value
pairs that it is not expecting to see, it can simply ignore them. Likewise, if the
game object is unable to ﬁnd a key-value pair that it needs, it has the option
of using a default value instead. This makes a key-value pair data format ex-
tremely robust to changes made by both the designers and the programmers.
Spawners also simplify the design and implementation of the game world
editor, because it only needs to know how to manage lists of key-value pairs
and object type schemas. It doesn’t need to share code with the runtime game
engine in any way, and it is only very loosely coupled to the engine’s imple-
mentation details.
Spawners and archetypes give game designers and programmers a great
deal of ﬂexibility and power. Designers can deﬁne new game object type
schemas within the world editor with little or no programmer intervention.
The programmer can implement the runtime implementation of these new
object types whenever his or her schedule allows it. The programmer does
not need to immediately provide an implementation of each new object type
as it is added in order to avoid breaking the game. New object data can exist
safely in the world chunk ﬁles with or without a runtime implementation, and
runtime implementations can exist with or without corresponding data in the
world chunk ﬁle.
15.4
Loading and Streaming Game Worlds
To bridge the gap between the ofﬂine world editor and our runtime game
object model, we need a way to load world chunks into memory and un-
load them when they are no longer needed. The game world loading sys-
tem has two main responsibilities: to manage the ﬁle I/O necessary to load
game world chunks and other needed assets from disk into memory and to
manage the allocation and deallocation of memory for these resources. The
engine also needs to manage the spawning and destruction of game objects as
they come and go in the game, both in terms of allocating and deallocating
memory for the objects and ensuring that the proper classes are instantiated
for each game object. In the following sections, we’ll investigate how game

900
15. Runtime Gameplay Foundation Systems
worlds are loaded and also have a look at how object spawning systems typi-
cally work.
15.4.1
Simple Level Loading
The most straightforward game world loading approach, and the one used by
all of the earliest games, is to allow one and only one game world chunk (a.k.a.
level) to be loaded at a time. When the game is ﬁrst started, and between pairs
of levels, the player sees a static or simply animated two-dimensional loading
screen while he or she waits for the level to load.
Memory management in this kind of design is quite straightforward. As
we mentioned in Section 6.2.2.7, a stack-based allocator is very well-suited to
a one-level-at-a-time world loading design. When the game ﬁrst runs, any
resource data that is required across all game levels is loaded at the bottom of
the stack. We’ll call these load and stay resident assets (LSR) for the purposes
of this discussion. The location of the stack pointer is recorded after the LSR
assets have been fully loaded. Each game world chunk, along with its asso-
ciated mesh, texture, audio, animation and other resource data, is loaded on
top of the LSR assets on the stack. When the level has been completed by the
player, all of its memory can be freed by simply resetting the stack pointer to
the top of the LSR asset block. At this point, a new level can be loaded in its
place. This is illustrated in Figure 15.11.
While this design is very simple, it has a number of drawbacks. For one
thing, the player only sees the game world in discrete chunks—there is no
way to implement a vast, contiguous, seamless world using this technique.
Another problem is that during the time the level’s resource data is being
loaded, there is no game world in memory. So, the player is forced to watch a
two-dimensional loading screen of some sort.
15.4.2
Toward Seamless Loading: Air Locks
The best way to avoid boring level-loading screens is to permit the player
to continue playing the game while the next world chunk and its associated
resource data are being loaded. One simple approach would be to divide
the memory that we’ve set aside for game world assets into two equally sized
blocks. We could load level A into one memory block, allow the player to start
playing level A and then load level B into the other block using a streaming
ﬁle I/O library (i.e., the loading code would run in a separate thread). The big
problem with this technique is that it cuts the size of each level in half relative
to what would be possible with a one-level-at-a-time approach.

15.4. Loading and Streaming Game Worlds
901
Load LSR data, then obtain marker.
Load-and-
stay-resident
(LSR) data
Load level A.
LSR data
Level A’s
resources
Unload level A, free back to marker.
LSR data
Load level B.
LSR data
Level B’s
resources
Figure 15.11.
A stack-based memory allocator is extremely well-suited to a one-level-at-a-time
world loading system.
We can achieve a similar effect by dividing the game world memory into
two unequally sized blocks—a large block that can contain a “full” game
world chunk and a small block that is only large enough to contain a tiny
world chunk. The small chunk is sometimes known as an “air lock.”
When the game starts, a “full” chunk and an “air lock” chunk are loaded.
The player progresses through the full chunk and into the air lock, at which
point some kind of gate or other impediment ensures that the player can nei-
ther see the previous full world area nor return to it. The full chunk can then
be un-loaded, and a new full-sized world chunk can be loaded. During the
load, the player is kept busy doing some task within the air lock. The task
might be as simple as walking from one end of a hallway to the other, or it
could be something more engaging, like solving a puzzle or ﬁghting some
enemies.

902
15. Runtime Gameplay Foundation Systems
Asynchronous ﬁle I/O is what enables the full world chunk to be loaded
while the player is simultaneously playing in the air lock region. See Section
6.1.3 for more details. It’s important to note that an air lock system does not
free us from displaying a loading screen whenever a new game is started,
because during the initial load there is no game world in memory in which to
play. However, once the player is in the game world, he or she needn’t see a
loading screen ever again, thanks to air locks and asynchronous data loading.
Halo for the Xbox used a technique similar to this. The large world areas
were invariably connected by smaller, more conﬁned areas. As you play Halo,
watch for conﬁned areas that prevent you from back-tracking—you’ll ﬁnd one
roughly every 5–10 minutes of gameplay. Jak 2 for the PlayStation 2 used the
air lock technique as well. The game world was structured as a hub area (the
main city) with a number of offshoot areas, each of which was connected to
the hub via a small, conﬁned air lock region.
15.4.3
Game World Streaming
Many game designs call for the player to feel like he or she is playing in a
huge, contiguous, seamless world. Ideally, the player should not be conﬁned
to small air lock regions periodically—it would be best if the world simply
unfolded in front of the player as naturally and believably as possible.
Modern game engines support this kind of seamless world by using a tech-
nique known as streaming. World streaming can be accomplished in various
ways. The main goals are always (a) to load data while the player is engaged
in regular gameplay tasks and (b) to manage the memory in such a way as to
eliminate fragmentation while permitting data to be loaded and unloaded as
needed as the player progresses through the game world.
Recent consoles and PCs have a lot more memory than their predecessors,
so it is now possible to keep multiple world chunks in memory simultane-
ously. We could imagine dividing our memory space into, say, three equally
sized buffers. At ﬁrst, we load world chunks A, B and C into these three
buffers and allow the player to start playing through chunk A. When he or
she enters chunk B and is far enough along that chunk A can no longer be
seen, we can unload chunk A and start loading a new chunk D into the ﬁrst
buffer. When B can no longer be seen, it can be dumped and chunk E loaded.
This recycling of buffers can continue until the player has reached the end of
the contiguous game world.
The problem with a coarse-grained approach to world streaming is that
it places onerous restrictions on the size of a world chunk. All chunks in
the entire game must be roughly the same size—large enough to ﬁll up the
majority of one of our three memory buffers but never any larger.

15.4. Loading and Streaming Game Worlds
903
One way around this problem is to employ a much ﬁner-grained sub-
division of memory.
Rather than streaming relatively large chunks of the
world, we can divide every game asset, from game world chunks to fore-
ground meshes to textures to animation banks, into equally sized blocks of
data. We can then use a chunky, pool-based memory allocation system like the
one described in Section 6.2.2.7 to load and unload resource data as needed
without having to worry about memory fragmentation. This is essentially
the technique employed by Naughty Dog’s Uncharted / The Last of Us engine.
(Although Naughty Dog’s implementation also employs some sophisticated
techniques for making use of what would otherwise be unused space at the
ends of under-full chunks.)
15.4.3.1
Determining Which Resources to Load
One question that arises when using a ﬁne-grained chunky memory allocator
for world streaming is how the engine will know what resources to load at
any given moment during gameplay. In Uncharted and The Last of Us, we
used a relatively simple system of level load regions to control the loading and
unloading of assets.
All of the Uncharted games and The Last of Us are set in multiple, geograph-
ically distinct, contiguous game worlds. For example, Uncharted: Drake’s For-
tune takes place in a jungle and on an island. Each of these worlds exists in
a single, consistent world space, but they are divided up into numerous ge-
ographically adjacent chunks. A simple convex volume known as a region
encompasses each of the chunks; the regions overlap each other somewhat.
Each region contains a list of the world chunks that should be in memory
when the player is in that region.
At any given moment, the player is within one or more of these regions. To
determine the set of world chunks that should be in memory, we simply take
the union of the chunk lists from each of the regions enclosing the Nathan
Drake character. The level loading system periodically checks this master
chunk list and compares it against the set of world chunks that are currently
in memory. If a chunk disappears from the master list, it is unloaded, thereby
freeing up all of the allocation blocks it occupied. If a new chunk appears
in the list, it is loaded into any free allocation blocks that can be found. The
level load regions and world chunks are designed in such a way as to ensure
that the player never sees a chunk disappear when it is unloaded and that
there’s enough time between the moment at which a chunk starts loading and
the moment its contents are ﬁrst seen by the player to permit the chunk to be
fully streamed into memory. This technique is illustrated in Figure 15.12.

904
15. Runtime Gameplay Foundation Systems
1
2
3
4
Level 1
Level 2
Level 2
Level 3
Level 3
Level 4
Figure 15.12. A game world divided into chunks. Level load regions, each with a requested chunk
list, are arranged in such a way as to guarantee that the player never sees a chunk pop in or out
of view.
15.4.3.2
PlayGo on the PlayStation 4
Sony’s latest console, the PlayStation 4, includes a new feature called PlayGo
that makes the process of downloading a game (as opposed to buying it on
Blu-ray) a lot less painful than it has traditionally been. PlayGo works by
downloading only the minimum subset of data required in order to play the
ﬁrst section of the game. The PS4 downloads the rest of the game’s content in
the background, while the player continues to experience the game without
interruption. In order for this to work well, the game must of course support
seamless level streaming, as we’ve described above.
15.4.4
Memory Management for Object Spawning
Once a game world has been loaded into memory, we need to manage the
process of spawning the dynamic game objects in the world. Most game en-
gines have some kind of game object spawning system that manages the in-
stantiation of the class or classes that make up each game object and handles
destruction of game objects when they are no longer needed. One of the cen-
tral jobs of any object spawning system is to manage the dynamic allocation
of memory for newly spawned game objects. Dynamic allocation can be slow,
so steps must be taken to ensure allocations are as efﬁcient as possible. And
because game objects come in a wide variety of sizes, dynamically allocating
them can cause memory to become fragmented, leading to premature out-of-
memory conditions. There are a number of different approaches to game ob-
ject memory management. We’ll explore a few common ones in the following
sections.
15.4.4.1
OffLine Memory Allocation for Object Spawning
Some game engines solve the problems of allocation speed and memory frag-
mentation in a rather draconian way, by simply disallowing dynamic mem-
ory allocation during gameplay altogether. Such engines permit game world

15.4. Loading and Streaming Game Worlds
905
chunks to be loaded and unloaded dynamically, but they spawn in all dy-
namic game objects immediately upon loading a chunk. Thereafter, no game
objects can be created or destroyed. You can think of this technique as obey-
ing a “law of conservation of game objects.” No game objects are created or
destroyed once a world chunk has been loaded.
This technique avoids memory fragmentation because the memory require-
ments of all the game objects in a world chunk are (a) known a priori and
(b) bounded. This means that the memory for the game objects can be al-
located ofﬂine by the world editor and included as part of the world chunk
data itself. All game objects are therefore allocated out of the same memory
used to load the game world and its resources, and they are no more prone
to fragmentation than any other loaded resource data. This approach also has
the beneﬁt of making the game’s memory usage patterns highly predictable.
There’s no chance that a large group of game objects is going to spawn into
the world unexpectedly, and cause the game to run out of memory.
On the downside, this approach can be quite limiting for game designers.
Dynamic object spawning can be simulated by allocating a game object in the
world editor but instructing it to be invisible and dormant when the world
is ﬁrst loaded. Later, the object can “spawn” by simply activating itself and
making itself visible. But the game designers have to predict the total number
of game objects of each type that they’ll need when the game world is ﬁrst
created in the world editor. If they want to provide the player with an inﬁnite
supply of health packs, weapons, enemies or some other kind of game object,
they either need to work out a way to recycle their game objects, or they’re
out of luck.
15.4.4.2
Dynamic Memory Management for Object Spawning
Game designers would probably prefer to work with a game engine that sup-
ports true dynamic object spawning. Although this is more difﬁcult to imple-
ment than a static game object spawning approach, it can be implemented in
a number of different ways.
Again, the primary problem is memory fragmentation. Because different
types of game objects (and sometimes even different instances of the same
type of object) occupy different amounts of memory, we cannot use our fa-
vorite fragmentation-free allocator—the pool allocator. And because game
objects are generally destroyed in a different order than that in which they
were spawned, we cannot use a stack-based allocator either. Our only choice
appears to be a fragmentation-prone heap allocator. Thankfully, there are
many ways to deal with the fragmentation problem. We’ll investigate a few
common ones in the following sections.

906
15. Runtime Gameplay Foundation Systems
One Memory Pool per Object Type
If the individual instances of each game object type are all guaranteed to oc-
cupy the same amount of memory, we could consider using a separate mem-
ory pool for each object type. Actually, we only need one pool per unique game
object size, so object types of the same size can share a single pool.
Doing this allows us to completely avoid memory fragmentation, but one
limitation of this approach is that we need to maintain lots of separate pools.
We also need to make educated guesses about how many of each type of object
we’ll need. If a pool has too many elements, we end up wasting memory; if it
has too few, we won’t be able to satisfy all of the spawn requests at runtime,
and game objects will fail to spawn.
Small Memory Allocators
We can transform the idea of one pool per game object type into something
more workable by allowing a game object to be allocated out of a pool whose
elements are larger than the object itself.
This can reduce the number of
unique memory pools we need signiﬁcantly, at the cost of some potentially
wasted memory in each pool.
For example, we might create a set of pool allocators, each one with ele-
ments that are twice as large as those of its predecessor—perhaps 8, 16, 32,
64, 128, 256 and 512 bytes. We can also use a sequence of element sizes that
conforms to some other suitable pattern or base the list of sizes on allocation
statistics collected from the running game.
Whenever we try to allocate a game object, we search for the smallest pool
whose elements are larger than or equal to the size of the object we’re allocat-
ing. We accept that for some objects, we’ll be wasting space. In return, we al-
leviate all of our memory fragmentation problems—a reasonably fair trade. If
we ever encounter a memory allocation request that is larger than our largest
pool, we can always turn it over to the general-purpose heap allocator, know-
ing that fragmentation of large memory blocks is not nearly as problematic as
fragmentation involving tiny blocks.
This type of allocator is sometimes called a small memory allocator. It can
eliminate fragmentation (for allocations that ﬁt into one of the pools). It can
also speed up memory allocations signiﬁcantly for small chunks of data,
because a pool allocation involves two pointer manipulations to remove the
element from the linked list of free elements—a much less-expensive opera-
tion than a general-purpose heap allocation.

15.4. Loading and Streaming Game Worlds
907
Memory Relocation
Another way to eliminate fragmentation is to attack the problem directly. This
approach is known as memory relocation. It involves shifting allocated memory
blocks down into adjacent free holes to remove fragmentation. Moving the
memory is easy, but because we are moving “live” allocated objects, we need
to be very careful about ﬁxing up any pointers into the memory blocks we
move. See Section 5.2.2.2 for more details.
15.4.5
Saved Games
Many games allow the player to save his or her progress, quit the game and
then load up the game at a later time in exactly the state he or she left it. A
saved game system is similar to the world chunk loading system in that it is
capable of loading the state of the game world from a disk ﬁle or memory card.
But the requirements of this system differ somewhat from those of a world
loading system, so the two are usually distinct (or overlap only partially).
To understand the differences between the requirements of these two sys-
tems, let’s brieﬂy compare world chunks to saved game ﬁles. World chunks
specify the initial conditions of all dynamic objects in the world, but they also
contain a full description of all static world elements. Much of the static in-
formation, such as background meshes and collision data, tends to take up a
lot of disk space. As such, world chunks are sometimes comprised of multi-
ple disk ﬁles, and the total amount of data associated with a world chunk is
usually large.
A saved game ﬁle must also store the current state information of the game
objects in the world. However, it does not need to store a duplicate copy of
any information that can be determined by reading the world chunk data.
For example, there’s no need to save out the static geometry in a saved game
ﬁle. A saved game need not store every detail of every object’s state either.
Some objects that have no impact on gameplay can be omitted altogether. For
the other game objects, we may only need to store partial state information.
As long as the player can’t tell the difference between the state of the game
world before and after it has been saved and reloaded (or if the differences
are irrelevant to the player), then we have a successful saved game system.
As such, saved game ﬁles tend to be much smaller than world chunk ﬁles and
may place more of an emphasis on data compression and omission. Small ﬁle
sizes are especially important when numerous saved game ﬁles must ﬁt onto
the tiny memory cards that were used on older consoles. But even today, with
consoles that are equipped with large hard drives and linked to a cloud save

908
15. Runtime Gameplay Foundation Systems
system, it’s still a good idea to keep the size of a saved game ﬁle as small as
possible.
15.4.5.1
Checkpoints
One approach to save games is to limit saves to speciﬁc points in the game,
known as checkpoints. The beneﬁt of this approach is that most of the knowl-
edge about the state of the game is saved in the current world chunk(s) in the
vicinity of each checkpoint. This data is always exactly the same, no matter
which player is playing the game, so it needn’t be stored in the saved game.
As a result, saved game ﬁles based on checkpoints can be extremely small. We
might need to store only the name of the last checkpoint reached, plus perhaps
some information about the current state of the player character, such as the
player’s health, number of lives remaining, what items he has in his inven-
tory, which weapon(s) he has and how much ammo each one contains. Some
games based on checkpoints don’t even store this information—they start the
player off in a known state at each checkpoint. Of course, the downside of a
game based on checkpoints is the possibility of user frustration, especially if
checkpoints are few and far between.
15.4.5.2
Save Anywhere
Some games support a feature known as save anywhere. As the name implies,
such games permit the state of the game to be saved at literally any point dur-
ing play. To implement this feature, the size of the saved game data ﬁle must
increase signiﬁcantly. The current locations and internal states of every game
object whose state is relevant to gameplay must be saved and then restored
when the game is loaded again later.
In a save-anywhere design, a saved game data ﬁle contains basically the
same information as a world chunk, minus the world’s static components. It
is possible to utilize the same data format for both systems, although there
may be factors that make this infeasible. For example, the world chunk data
format might be designed for ﬂexibility, but the saved game format might be
compressed to minimize the size of each saved game.
As we’ve mentioned, one way to reduce the amount of data that needs to
be stored in a saved game ﬁle is to omit certain irrelevant game objects and
to omit some irrelevant details of others. For example, we needn’t remember
the exact time index within every animation that is currently playing or the
exact momentums and velocities of every physically simulated rigid body. We
can rely on the imperfect memories of human gamers and save only a rough
approximation to the game’s state.

15.5. Object References and World Queries
909
15.5
Object References and World Queries
Every game object generally requires some kind of unique id so that it can be
distinguished from the other objects in the game, found at runtime, serve as a
target of inter-object communication and so on. Unique object ids are equally
helpful on the tool side, as they can be used to identify and ﬁnd game objects
within the world editor.
At runtime, we invariably need various ways to ﬁnd game objects. We
might want to ﬁnd an object by its unique id, by its type, or by a set of arbi-
trary criteria. We often need to perform proximity-based queries, for example
ﬁnding all enemy aliens within a 10 m radius of the player character.
Once a game object has been found via a query, we need some way to refer
to it. In a language like C or C++, object references might be implemented
as pointers, or we might use something more sophisticated, like handles or
smart pointers. The lifetime of an object reference can vary widely, from the
scope of a single function call to a period of many minutes. In the following
sections, we’ll ﬁrst investigate various ways to implement object references.
Then we’ll explore the kinds of queries we often require when implementing
gameplay and how those queries might be implemented.
15.5.1
Pointers
In C or C++, the most straightforward way to implement an object reference
is via a pointer (or a reference in C++). Pointers are powerful and are just
about as simple and intuitive as you can get. However, pointers suffer from a
number of problems:
•
Orphaned objects. Ideally, every object should have an owner—another
object that is responsible for managing its lifetime—creating it and then
deleting it when it is no longer needed. But pointers don’t give the pro-
grammer any help in enforcing this rule. The result can be an orphaned
object—an object that still occupies memory but is no longer needed or
referenced by any other object in the system.
•
Stale pointers. If an object is deleted, ideally we should null-out any and
all pointers to that object. If we forget to do so, however, we end up
with a stale pointer—a pointer to a block of memory that used to be
occupied by a valid object but is now free memory. If anyone tries to
read or write data through a stale pointer, the result can be a crash or
incorrect program behavior. Stale pointers can be difﬁcult to track down
because they may continue to work for some time after the object has
deleted. Only much later, when a new object is allocated on top of the
stale memory block, does the data actually change and cause a crash.

910
15. Runtime Gameplay Foundation Systems
•
Invalid pointers. A programmer is free to store any address in a pointer,
including a totally invalid address. A common problem is dereferencing
a null pointer. These problems can be guarded against by using asser-
tion macros to check that pointers are never null prior to dereferencing
them. Even worse, if a piece of data is misinterpreted as a pointer, deref-
erencing it can cause the program to read or write an essentially random
memory address. This usually results in a crash or other major problem
that can be very tough to debug.
Many game engines make heavy use of pointers, because they are by far
the fastest, most efﬁcient and easiest-to-work-with way to implement object
references. However, experienced programmers are always wary of pointers,
and some game teams turn to more sophisticated kinds of object references,
either out of a desire to use safer programming practices or out of necessity.
For example, if a game engine relocates allocated data blocks at runtime to
eliminate memory fragmentation (see Section 5.2.2.2), simple pointers cannot
be used. We either need to use a type of object reference that is robust to
memory relocation, or we need to manually ﬁx up any pointers into every
relocated memory block at the time it is moved.
15.5.2
Smart Pointers
A smart pointer is a small object that acts like a pointer for most intents and pur-
poses but avoids most of the problems inherent with native C/C++ pointers.
At its simplest, a smart pointer contains a native pointer as a data member and
provides a set of overloaded operators that make it act like a pointer in most
ways. Pointers can be dereferenced, so the * and -> operators are overloaded
to return the address as expected. Pointers can undergo arithmetic operations,
so the +, -, ++ and -- operators are also overloaded appropriately.
Because a smart pointer is an object, it can contain additional metadata
and/or take additional steps not possible with a regular pointer. For example,
a smart pointer might contain information that allows it to recognize when the
object to which it points has been deleted and start returning a NULL address
if so.
Smart pointers can also help with object lifetime management by cooper-
ating with one another to determine the number of references to a particular
object. This is called reference counting. When the number of smart point-
ers that reference a particular object drops to zero, we know that the object
is no longer needed, so it can be automatically deleted. This can free the pro-
grammer from having to worry about object ownership and orphaned objects.
Reference counting usually also lies at the core of the “garbage collection” sys-
tems found in modern programming languages like Java and Python.

15.5. Object References and World Queries
911
Smart pointers have their share of problems. For one thing, they are rel-
atively easy to implement, but they are extremely tough to get right. There
are a great many cases to handle, and the std::auto_ptr class provided
by the standard C++ library is widely recognized to be inadequate in many
situations. The Boost C++ template library provides six different varieties of
smart pointers:
•
scoped_ptr. A pointer to a single object with one owner.
•
scoped_array. A pointer to an array of objects with one owner.
•
shared_ptr. A pointer to an object whose lifetime is shared by multi-
ple owners.
•
shared_array. A pointer to an array of objects whose lifetimes are
shared by multiple owners.
•
weak_ptr. A pointer that does not own or automatically destroy the
object it references (whose lifetime is assumed to be managed by a
shared_ptr).
•
intrusive_ptr. A pointer that implements reference counting by as-
suming that the pointed-to object will maintain the reference count itself.
Intrusive pointers are useful because they are the same size as a native
C++ pointer (because no reference-counting apparatus is required) and
because they can be constructed directly from native pointers.
Properly implementing a smart pointer class can be a daunting task. Have
a glance at the Boost smart pointer documentation (http://www.boost.org/
doc/libs/1_36_0/libs/smart_ptr/smart_ptr.htm) to see what I mean. All sorts
of issues come up, including:
•
type safety of smart pointers,
•
the ability for a smart pointer to be used with an incomplete type,
•
correct smart pointer behavior when an exception occurs, and
•
runtime costs, which can be high.
I worked on a project that attempted to implement its own smart pointers, and
we were ﬁxing all sorts of nasty bugs with them up until the very end of the
project. My personal recommendation is to stay away from smart pointers,
or if you must use them, use a mature implementation such as Boost’s rather
than trying to roll your own.
15.5.3
Handles
A handle acts like a smart pointer in many ways, but it is simpler to implement
and tends to be less prone to problems. A handle is basically an integer index

912
15. Runtime Gameplay Foundation Systems
NULL
NULL
Object1
Object2
Object3
Object4
Object5
Handle Table
m_handleIndex == 6
0
1
2
3
4
5
6
Handle to Object 5
Figure 15.13. A handle table contains raw object pointers. A handle is simply an index into this
table.
into a global handle table. The handle table, in turn, contains pointers to the
objects to which the handles refer. To create a handle, we simply search the
handle table for the address of the object in question and store its index in the
handle. To dereference a handle, the calling code simply indexes the appro-
priate slot in the handle table and dereferences the pointer it ﬁnds there. This
is illustrated in Figure 15.13.
Because of the simple level of indirection afforded by the handle table,
handles are much safer and more ﬂexible than raw pointers. If an object is
deleted, it can simply null out its entry in the handle table. This causes all
existing handles to the object to be immediately and automatically converted
to null references. Handles also support memory relocation. When an object
is relocated in memory, its address can be found in the handle table and up-
dated appropriately. Again, all existing handles to the object are automatically
updated as a result.
A handle can be implemented as a raw integer. However, the handle table
index is usually wrapped in a simple class so that a convenient interface for
creating and dereferencing the handle can be provided.
Handles are prone to the possibility of referencing a stale object. For ex-
ample, let’s say we create a handle to object A, which occupies slot 17 in the
handle table. Later, object A is deleted, and slot 17 is nulled out. Later still,
a new object B is created, and it just happens to occupy slot 17 in the handle
table. If there are still any handles to object A lying around when object B is
created, they will suddenly start referring to object B (instead of null). This is
certainly not desirable behavior.
One simple solution to the stale handle problem is to include a unique
object id in each handle. That way, when a handle to object A is created, it

15.5. Object References and World Queries
913
contains not only slot index 17, but the object id “A.” When object B takes
A’s place in the handle table, any leftover handles to A will agree on the han-
dle index but disagree on the object id. This allows stale object A handles to
continue to return null when dereferenced rather than returning a pointer to
object B unexpectedly.
The following code snippet shows how a simple handle class might be
implemented. Notice that we’ve also included the handle index in the Game-
Object class itself—this allows us to create new handles to a GameObject
very quickly without having to search the handle table for its address to de-
termine its handle index.
// Within the GameObject class, we store a unique id,
// and also the object's handle index, for efficient
// creation of new handles.
class GameObject
{
private:
// ...
GameObjectId m_uniqueId;
// object's unique id
U32
m_handleIndex;
// speedier handle
// creation
friend class GameObjectHandle;
// access to id and
// index
// ...
public:
GameObject() // constructor
{
// The unique id might come from the world editor,
// or it might be assigned dynamically at runtime.
m_uniqueId = AssignUniqueObjectId();
// The handle index is assigned by finding the
// first free slot in the handle table.
m_handleIndex = FindFreeSlotInHandleTable();
// ...
}
// ...
};

914
15. Runtime Gameplay Foundation Systems
// This constant defines the size of the handle table,
// and hence the maximum number of game objects that can
// exist at any one time.
static const U32 MAX_GAME_OBJECTS = 1024;
// This is the global handle table -- a simple array of
// pointers to GameObjects.
static GameObject* g_apGameObject[MAX_GAME_OBJECTS];
// This is our simple game object handle class.
class GameObjectHandle
{
private:
U32 m_handleIndex;
// index into the handle
// table
GameObjectId m_uniqueId;
// unique id avoids stale
// handles
public:
explicit GameObjectHandle(GameObject& object) :
m_handleIndex(object.m_handleIndex),
m_uniqueId(object.m_uniqueId)
{
}
// This function dereferences the handle.
GameObject* ToObject() const
{
GameObject* pObject
= g_apGameObject[m_handleIndex];
if (pObject != NULL
&&
pObject->m_uniqueId == m_uniqueId)
{
return pObject;
}
return NULL;
}
};
This example is functional but incomplete. We might want to implement copy
semantics, provide additional constructor variants and so on. The entries in
the global handle table might contain additional information, not just a raw
pointer to each game object. And of course, a ﬁxed size handle table imple-
mentation like this one isn’t the only possible design; handle systems vary
somewhat from engine to engine.

15.5. Object References and World Queries
915
We should note that one fortunate side beneﬁt of a global handle table is
that it gives us a ready-made list of all active game objects in the system. The
global handle table can be used to quickly and efﬁciently iterate over all game
objects in the world, for example. It can also make implementing other kinds
of queries easier in some cases.
15.5.4
Game Object Queries
Every game engine provides at least a few ways to ﬁnd game objects at run-
time. We’ll call these searches game object queries. The simplest type of query is
to ﬁnd a particular game object by its unique id. However, a real game engine
makes many other types of game object queries. Here are just a few examples
of the kinds of queries a game developer might want to make:
•
Find all enemy characters with line of sight to the player.
•
Iterate over all game objects of a certain type.
•
Find all destructible game objects with more than 80% health.
•
Transmit damage to all game objects within the blast radius of an explo-
sion.
•
Iterate over all objects in the path of a bullet or other projectile, in nearest-
to-farthest order.
This list could go on for many pages, and of course its contents are highly
dependent upon the design of the particular game being made.
For maximum ﬂexibility in performing game object queries, we could imag-
ine a general-purpose game object database, complete with the ability to for-
mulate arbitrary queries using arbitrary search criteria. Ideally, our game
object database would perform all of these queries extremely efﬁciently and
rapidly, making maximum use of whatever hardware and software resources
are available.
In reality, such an ideal combination of ﬂexibility and blinding speed is
generally not possible. Instead, game teams usually determine which types
of queries are most likely to be needed during development of the game,
and specialized data structures are implemented to accelerate those particular
types of queries. As new queries become necessary, the engineers either lever-
age preexisting data structures to implement them, or they invent new ones
if sufﬁcient speed cannot be obtained. Here are a few examples of specialized
data structures that can accelerate speciﬁc types of game object queries:
•
Finding game objects by unique id. Pointers or handles to the game objects
could be stored in a hash table or binary search tree keyed by unique id.

916
15. Runtime Gameplay Foundation Systems
•
Iterating over all objects that meet a particular criterion. The game objects
could be presorted into linked lists based on various criteria (presuming
the criteria are known a priori). For example, we might construct a list of
all game objects of a particular type, maintain a list of all objects within
a particular radius of the player, etc.
•
Finding all objects in the path of a projectile or with line of sight to some target
point. The collision system is usually leveraged to perform these kinds
of game object queries. Most collision systems provide fast ray casts,
and some also provide the ability to cast other shapes such as spheres
or arbitrary convex volumes into the world to determine what they hit.
(See Section 12.3.7.)
•
Finding all objects within a given region or radius. We might consider stor-
ing our game objects in some kind of spatial hash data structure. This
could be as simple as a horizontal grid placed over the entire game
world or something more sophisticated, such as a quadtree, octree, kd-
tree or other data structure that encodes spatial proximity.
15.6
Updating Game Objects in Real Time
Every game engine, from the simplest to the most complex, requires some
means of updating the internal state of every game object over time. The state
of a game object can be deﬁned as the values of all its attributes (sometimes
called its properties, and called data members in the C++ language). For exam-
ple, the state of the ball in Pong is described by its (x, y) position on the screen
and its velocity (speed and direction of travel). Because games are dynamic,
time-based simulations, a game object’s state describes its conﬁguration at one
speciﬁc instant in time. In other words, a game object’s notion of time is discrete
rather than continuous. (However, as we’ll see, it’s helpful to think of the ob-
jects’ states as changing continuously and then being sampled discretely by
the engine, because it helps you to avoid some common pitfalls.)
In the following discussions, we’ll use the symbol Si(t) to denote the state
of object i at an arbitrary time t. The use of vector notation here is not strictly
mathematically correct, but it reminds us that a game object’s state acts like
a heterogeneous n-dimensional vector, containing all sorts of information of
various data types. We should note that this usage of the term “state” is not
the same as the states in a ﬁnite state machine. A game object may very well
be implemented in terms of one—or many—ﬁnite state machines, but in that
case, a speciﬁcation of the current state of each FSM would merely be a part
of the game object’s overall state vector S(t).

15.6. Updating Game Objects in Real Time
917
Most low-level engine subsystems (rendering, animation, collision, physics,
audio and so on) require periodic updating, and the game object system is no
exception. As we saw in Chapter 7, updating is usually done via a single
master loop called the game loop (or possibly via multiple game loops, each
running in a separate thread). Virtually all game engines update game object
states as part of their main game loop—in other words, they treat the game ob-
ject model as just another engine subsystem that requires periodic servicing.
Game object updating can therefore be thought of as the process of de-
termining the state of each object at the current time Si(t) given its state at a
previous time Si(t−∆t). Once all object states have been updated, the current
time t becomes the new previous time (t −∆t), and this process repeats for as
long as the game is running. Usually, one or more clocks are maintained by the
engine—one that tracks real time exactly and possibly others that may or may
not correspond to real time. These clocks provide the engine with the abso-
lute time t and/or with the change in time ∆t from iteration to iteration of the
game loop. The clock that drives the updating of game object states is usually
permitted to diverge from real time. This allows the behaviors of the game ob-
jects to be paused, slowed down, sped up or even run in reverse—whatever
is required in order to suit the needs of the game design. These features are
also invaluable for debugging and development of the game.
As we learned in Chapter 1, a game object updating system is an exam-
ple of what is known as a dynamic, real-time, agent-based computer simulation in
computer science. Game object updating systems also exhibit some aspects of
discrete event simulations (see Section 15.7 for more details on events). These
are well-researched areas of computer science, and they have many appli-
cations outside the ﬁeld of interactive entertainment. Games are one of the
more-complex kinds of agent-based simulation—as we’ll see, updating game
object states over time in a dynamic, interactive virtual environment can be
surprisingly difﬁcult to get right. Game programmers can learn a lot about
game object updating by studying the wider ﬁeld of agent-based and discrete
event simulations. And researchers in those ﬁelds can probably learn a thing
or two from game engine design as well!
As with all high-level game engine systems, every engine takes a slightly
(or sometimes radically) different approach. However, as before, most game
teams encounter a common set of problems, and certain design patterns tend
to crop up again and again in virtually every engine. In this section, we’ll
investigate these common problems and some common solutions to them.
Please bear in mind that game engines may exist that employ very differ-
ent solutions to the ones described here, and some game designs face unique
problems that we can’t possibly cover here.

918
15. Runtime Gameplay Foundation Systems
15.6.1
A Simple Approach (That Doesn’t Work)
The simplest way to update the states of a collection of game objects is to
iterate over the collection and call a virtual function, named something like
Update(), on each object in turn. This is typically done once during each
iteration of the main game loop (i.e., once per frame). Game object classes
can provide custom implementations of the Update() function in order to
perform whatever tasks are required to advance the state of that type of object
to the next discrete time index. The time delta from the previous frame can
be passed to the update function so that objects can take proper account of
the passage of time. At its simplest, then, our Update() function’s signature
might look something like this:
virtual void Update(float dt);
For the purposes of the following discussions, we’ll assume that our en-
gine employs a monolithic object hierarchy, in which each game object is rep-
resented by a single instance of a single class. However, we can easily ex-
tend the ideas here to virtually any object-centric design. For example, to
update a component-based object model, we could call Update() on every
component that makes up each game object, or we could call Update() on
the “hub” object and let it update its associated components as it sees ﬁt. We
can also extend these ideas to property-centric designs, by calling some sort
of Update() function on each property instance every frame.
They say that the devil is in the details, so let’s investigate two important
details here. First, how should we maintain the collection of all game objects?
And second, what kinds of things should the Update() function be respon-
sible for doing?
15.6.1.1
Maintaining a Collection of Active Game Objects
The collection of active game objects is often maintained by a singleton man-
ager class, perhaps named something like GameWorld or GameObject-
Manager. The collection of game objects generally needs to be dynamic, be-
cause game objects are spawned and destroyed as the game is played. Hence
a linked list of pointers, smart pointers or handles to game objects is one sim-
ple and effective approach. (Some game engines disallow dynamic spawning
and destroying of game objects; such engines can use a statically sized array of
game object pointers, smart pointers or handles rather than a linked list.) As
we’ll see below, most engines use more-complex data structures to keep track
of their game objects rather than just a simple, ﬂat linked list. But for the time
being, we can visualize the data structure as a linked list for simplicity.

15.6. Updating Game Objects in Real Time
919
15.6.1.2
Responsibilities of the Update() Function
A game object’s Update() function is primarily responsible for determining
the state of that game object at the current discrete time index Si(t) given its
previous state Si(t −∆t). Doing this may involve applying a rigid body dy-
namics simulation to the object, sampling a preauthored animation, reacting
to events that have occurred during the current time step and so on.
Most game objects interact with one or more engine subsystems. They
may need to animate, be rendered, emit particle effects, play audio, collide
with other objects and static geometry and so on. Each of these systems has
an internal state that must also be updated over time, usually once or a few
times per frame. It might seem reasonable and intuitive to simply update all of
these subsystems directly from within the game object’s Update() function.
For example, consider the following hypothetical update function for a Tank
object:
virtual void Tank::Update(float dt)
{
// Update the state of the tank itself.
MoveTank(dt);
DeflectTurret(dt);
FireIfNecessary();
// Now update low-level engine subsystems on behalf
// of this tank. (NOT a good idea... see below!)
m_pAnimationComponent->Update(dt);
m_pCollisionComponent->Update(dt);
m_pPhysicsComponent->Update(dt);
m_pAudioComponent->Update(dt);
m_pRenderingComponent->draw();
}
Given that our Update() functions are structured like this, the game loop
could be driven almost entirely by the updating of the game objects, like this:
while (true)
{
PollJoypad();
float dt = g_gameClock.CalculateDeltaTime();
for (each gameObject)
{
// This hypothetical Update() function updates
// all engine subsystems!

920
15. Runtime Gameplay Foundation Systems
gameObject.Update(dt);
}
g_renderingEngine.SwapBuffers();
}
However attractive the simple approach to object updating shown above
may seem, it is usually not viable in a commercial-grade game engine. In
the following sections, we’ll explore some of the problems with this simplis-
tic approach and investigate common ways in which each problem can be
solved.
15.6.2
Performance Constraints and Batched Updates
Most low-level engine systems have extremely stringent performance con-
straints. They operate on a large quantity of data, and they must do a large
number of calculations every frame as quickly as possible. As a result, most
engine systems beneﬁt from batched updating. For example, it is usually far
more efﬁcient to update a large number of animations in one batch than it is
to update each object’s animation interleaved with other unrelated operations,
such as collision detection, physical simulation and rendering.
In most commercial game engines, each engine subsystem is updated di-
rectly or indirectly by the main game loop rather than being updated on a
per-game object basis from within each object’s Update() function. If a game
object requires the services of a particular engine subsystem, it asks that sub-
system to allocate some subsystem-speciﬁc state information on its behalf. For
example, a game object that wishes to be rendered via a triangle mesh might
request the rendering subsystem to allocate a mesh instance for its use. (As
described in Section 10.1.1.5, a mesh instance represents a single instance of
a triangle mesh—it keeps track of the position, orientation and scale of the
instance in world space whether or not it is visible, per-instance material data
and any other per-instance information that may be relevant.) The rendering
engine maintains a collection of mesh instances internally. It can manage the
mesh instances however it sees ﬁt in order to maximize its own runtime per-
formance. The game object controls how it is rendered by manipulating the
properties of the mesh instance object, but the game object does not control
the rendering of the mesh instance directly. Instead, after all game objects
have had a chance to update themselves, the rendering engine draws all visi-
ble mesh instances in one efﬁcient batch update.
With batched updating, a particular game object’s Update() function,
such as that of our hypothetical tank object, might look more like this:

15.6. Updating Game Objects in Real Time
921
virtual void Tank::Update(float dt)
{
// Update the state of the tank itself.
MoveTank(dt);
DeflectTurret(dt);
FireIfNecessary();
// Control the properties of my various engine
// subsystem components, but do NOT update
// them here...
if (justExploded)
{
m_pAnimationComponent->PlayAnimation("explode");
}
if (isVisible)
{
m_pCollisionComponent->Activate();
m_pRenderingComponent->Show();
}
else
{
m_pCollisionComponent->Deactivate();
m_pRenderingComponent->Hide();
}
// etc.
}
The game loop then ends up looking more like this:
while (true)
{
PollJoypad();
float dt = g_gameClock.CalculateDeltaTime();
for (each gameObject)
{
gameObject.Update(dt);
}
g_animationEngine.Update(dt);
g_physicsEngine.Simulate(dt);
g_collisionEngine.DetectAndResolveCollisions(dt);
g_audioEngine.Update(dt);
g_renderingEngine.RenderFrameAndSwapBuffers();
}

922
15. Runtime Gameplay Foundation Systems
Batched updating provides many performance beneﬁts, including but not
limited to:
•
Maximal cache coherency.
Batched updating allows an engine subsys-
tem to achieve maximum cache coherency because its per-object data
is maintained internally and can be arranged in a single, contiguous re-
gion of RAM.
•
Minimal duplication of computations. Global calculations can be done once
and reused for many game objects rather than being redone for each
object.
•
Reduced reallocation of resources. Engine subsystems often need to allocate
and manage memory and/or other resources during their updates. If
the update of a particular subsystem is interleaved with those of other
engine subsystems, these resources must be freed and reallocated for
each game object that is processed. But if the updates are batched, the
resources can be allocated once per frame and reused for all objects in
the batch.
•
Efﬁcient pipelining. Many engine subsystems perform a virtually identi-
cal set of calculations on each and every object in the game world. When
updates are batched, new optimizations become possible, and special-
ized hardware resources can be leveraged. For example, the PlaySta-
tion 3 provides a battery of high-speed microprocessors known as SPUs,
each of which has its own private high-speed memory area. When pro-
cessing a batch of animations, the pose of one character can be calculated
while we simultaneously DMA the data for the next character into SPU
memory. This kind of parallelism cannot be achieved when processing
each object in isolation.
Performance beneﬁts aren’t the only reason to favor a batch updating ap-
proach. Some engine subsystems simply don’t work at all when updated on
a per-object basis. For example, if we are trying to resolve collisions within
a system of multiple dynamic rigid bodies, a satisfactory solution cannot be
found in general by considering each object in isolation. The interpenetrations
between these objects must be resolved as a group, either via an iterative ap-
proach or by solving a linear system.
15.6.3
Object and Subsystem Interdependencies
Even if we didn’t care about performance, a simplistic per-object updating ap-
proach breaks down when game objects depend on one another. For example,

15.6. Updating Game Objects in Real Time
923
a human character might be holding a cat in her arms. In order to calcu-
late the world-space pose of the cat’s skeleton, we ﬁrst need to calculate the
world-space pose of the human. This implies that the order in which objects
are updated is important to the proper functioning of the game.
Another related problem arises when engine subsystems depend on one an-
other. For example, a rag doll physics simulation must be updated in concert
with the animation engine. Typically, the animation system produces an in-
termediate, local-space skeletal pose. These joint transforms are converted to
world space and applied to a system of connected rigid bodies that approxi-
mate the skeleton within the physics system. The rigid bodies are simulated
forward in time by the physics system, and then the ﬁnal resting places of the
joints are applied back to their corresponding joints in the skeleton. Finally,
the animation system calculates the world-space pose and skinning matrix
palette. So once again, the updating of the animation and physics systems
must occur in a particular order in order to produce correct results. These
kinds of inter-subsystem dependencies are commonplace in game engine de-
sign.
15.6.3.1
Phased Updates
To account for inter-subsystem dependencies, we can explicitly code our en-
gine subsystem updates in the proper order within the main game loop. For
example, to handle the interplay between the animation system and rag doll
physics, we might write something like this:
while (true) // main game loop
{
// ...
g_animationEngine.CalculateIntermediatePoses(dt);
g_ragdollSystem.ApplySkeletonsToRagDolls();
g_physicsEngine.Simulate(dt); // runs ragdolls too
g_collisionEngine.DetectAndResolveCollisions(dt);
g_ragdollSystem.ApplyRagDollsToSkeletons();
g_animationEngine.FinalizePoseAndMatrixPalette();
// ...
}
We must be careful to update the states of our game objects at the right
time during the game loop. This is often not as simple as calling a single
Update() function per game object per frame. Game objects may depend
upon the intermediate results of calculations performed by various engine

924
15. Runtime Gameplay Foundation Systems
subsystems. For example, a game object might request that animations be
played prior to the animation system running its update. However, that same
object may also want to procedurally adjust the intermediate pose generated
by the animation system prior to that pose being used by the rag doll physics
system and/or the ﬁnal pose and matrix palette being generated. This implies
that the object must be updated twice, once before the animation calculates its
intermediate poses and once afterward.
Many game engines allow their game objects to run update logic at mul-
tiple points during the frame. For example, the Naughty Dog engine (The
Last of Us, Uncharted) updates game objects three times—once before anima-
tion blending, once after animation blending but prior to ﬁnal pose generation
and once after ﬁnal pose generation. This can be accomplished by providing
each game object class with three virtual functions that act as “hooks.” In such
a system, the game loop ends up looking something like this:
while (true) // main game loop
{
// ...
for (each gameObject)
{
gameObject.PreAnimUpdate(dt);
}
g_animationEngine.CalculateIntermediatePoses(dt);
for (each gameObject)
{
gameObject.PostAnimUpdate(dt);
}
g_ragdollSystem.ApplySkeletonsToRagDolls();
g_physicsEngine.Simulate(dt); // runs ragdolls too
g_collisionEngine.DetectAndResolveCollisions(dt);
g_ragdollSystem.ApplyRagDollsToSkeletons();
g_animationEngine.FinalizePoseAndMatrixPalette();
for (each gameObject)
{
gameObject.FinalUpdate(dt);
}
// ...
}

15.6. Updating Game Objects in Real Time
925
We can provide our game objects with as many update phases as we see
ﬁt. But we must be careful, because iterating over all game objects and calling
a virtual function on each one can be expensive. Also, not all game objects re-
quire all update phases—iterating over objects that don’t require a particular
phase is a pure waste of CPU bandwidth.
Actually, the above example isn’t completely realistic. Iterating directly
over all game objects to call their PreAnimUpdate(), PostAnimUpdate()
and FinalUpdate() hook functions would be highly inefﬁcient, because
only a small percentage of the objects might actually need to perform any
logic in each hook. It’s also an inﬂexible design, because only game objects
are supported—if we wanted to update a particle system during the post-
animation phase, we’d be out of luck. Finally, such a design would lead to
unnecessary coupling between the low-level engine systems and the game ob-
ject system.
A generic callback mechanism would be a much better design choice. In
such a design, the animation system would provide a facility by which any
client code (game objects or any other engine system) could register a callback
function for each of the three update phases (pre-animation, post-animation
and ﬁnal). The animation system would iterate through all registered call-
backs and call them, without any “knowledge” of game objects per se. This
design maximizes performance, because only those clients that actually need
updates register callbacks and are called each frame. It also maximizes ﬂex-
ibility and eliminates unnecessary coupling between the game object system
and other engine subsystems, because any client is allowed to register a call-
back, not just game objects.
15.6.3.2
Bucketed Updates
In the presence of inter-object dependencies, the phased updates technique de-
scribed above must be adjusted a little. This is because inter-object dependen-
cies can lead to conﬂicting rules governing the order of updating. For exam-
ple, let’s imagine that object B is being held by object A. Further, let’s assume
that we can only update object B after A has been fully updated, including the
calculation of its ﬁnal world-space pose and matrix palette. This conﬂicts with
the need to batch animation updates of all game objects together in order to
allow the animation system to achieve maximum throughput.
Inter-object dependencies can be visualized as a forest of dependency trees.
The game objects with no parents (no dependencies on any other object) rep-
resent the roots of the forest. An object that depends directly on one of these
root objects resides in the ﬁrst tier of children in one of the trees in the forest.
An object that depends on a ﬁrst-tier child becomes a second-tier child and so
on. This is illustrated in Figure 15.14.

926
15. Runtime Gameplay Foundation Systems
Depends
On
Figure 15.14. Inter-object update order dependencies can be viewed as a forest of dependency
trees.
One solution to the problem of conﬂicting update order requirements is to
collect objects into independent groups, which we’ll call buckets here for lack
of a better name. The ﬁrst bucket consists of all root objects in the forest. The
second bucket is comprised of all ﬁrst-tier children. The third bucket contains
all second-tier children and so on. For each bucket, we run a complete update
of the game objects and the engine systems, complete with all update phases.
Then we repeat the entire process for each bucket until there are no more
buckets.
In theory, the depths of the trees in our dependency forest are unbounded.
However, in practice, they are usually quite shallow. For example, we might
have characters holding weapons, and those characters might or might not be
riding on a moving platform or a vehicle. To implement this, we only need
three tiers in our dependency forest, and hence only three buckets: one for
platforms/vehicles, one for characters and one for the weapons in the charac-
ters’ hands. Many game engines explicitly limit the depth of their dependency
forest so that they can use a ﬁxed number of buckets (presuming they use a
bucketed approach at all—there are of course many other ways to architect a
game loop).
Here’s what a bucketed, phased, batched update loop might look like:
enum Bucket
{
kBucketVehiclesPlatforms,

15.6. Updating Game Objects in Real Time
927
kBucketCharacters,
kBucketAttachedObjects,
kBucketCount
};
void UpdateBucket(Bucket bucket)
{
// ...
for (each gameObject in bucket)
{
gameObject.PreAnimUpdate(dt);
}
g_animationEngine.CalculateIntermediatePoses
(bucket, dt);
for (each gameObject in bucket)
{
gameObject.PostAnimUpdate(dt);
}
g_ragdollSystem.ApplySkeletonsToRagDolls(bucket);
g_physicsEngine.Simulate(bucket, dt); // ragdolls etc.
g_collisionEngine.DetectAndResolveCollisions
(bucket, dt);
g_ragdollSystem.ApplyRagDollsToSkeletons(bucket);
g_animationEngine.FinalizePoseAndMatrixPalette
(bucket);
for (each gameObject in bucket)
{
gameObject.FinalUpdate(dt);
}
// ...
}
void RunGameLoop()
{
while (true)
{
// ...
UpdateBucket(kBucketVehiclesAndPlatforms);
UpdateBucket(kBucketCharacters);
UpdateBucket(kBucketAttachedObjects);

928
15. Runtime Gameplay Foundation Systems
// ...
g_renderingEngine.RenderSceneAndSwapBuffers();
}
}
In practice, things might be a bit more complex than this. For example,
some engine subsystems like the physics engine might not support the con-
cept of buckets, perhaps because they are third-party SDKs or because they
cannot be practically updated in a bucketed manner. However, this bucketed
update is essentially what we used at Naughty Dog to implement all of the
games in the Uncharted series as well as The Last of Us. So it’s a method that
has proven to be practical and reasonably efﬁcient.
15.6.3.3
Object State Inconsistencies and One-Frame-Off Lag
Let’s revisit game object updating, but this time thinking in terms of each
object’s local notion of time. We said in Section 15.6 that the state of game
object i at time t can be denoted by a state vector Si(t). When we update
a game object, we are converting its previous state vector Si(t1) into a new
current state vector Si(t2) (where t2 = t1 + ∆t).
In theory, the states of all game objects are updated from time t1 to time
t2 instantaneously and in parallel, as depicted in Figure 15.15. However, in
practice, we can only update the objects one by one—we must loop over each
game object and call some kind of update function on each one in turn. If
we were to stop the program halfway through this update loop, half of our
game objects’ states would have been updated to Si(t2), while the remaining
half would still be in their previous states Si(t1). This implies that if we were
to ask two of our game objects what the current time is during the update
loop, they may or may not agree! What’s more, depending on where exactly
we interrupt the update loop, the objects may all be in a partially updated
state. For example, animation pose blending may have been run, but physics
and collision resolution may not yet have been applied. This leads us to the
following rule:
The states of all game objects are consistent before and after the
update loop, but they may be inconsistent during it.
This is illustrated in Figure 15.16.
The inconsistency of game object states during the update loop is a major
source of confusion and bugs, even among professionals within the game in-
dustry. The problem rears its head most often when game objects query one

15.6. Updating Game Objects in Real Time
929
t1
t
SA
ObjectA
SA
ObjectB
SB
ObjectC
SC
ObjectD
SD
t2
SB
SC
SD
t
Figure 15.15. In theory, the states of all game objects are updated instantaneously and in parallel
during each iteration of the game loop.
another for state information during the update loop (which implies that there
is a dependency between them). For example, if object B looks at the velocity
of object A in order to determine its own velocity at time t, then the program-
mer must be clear about whether he or she wants to read the previous state of
object A, SA(t1), or the new state, SA(t2). If the new state is needed but object
A has not yet been updated, then we have an update order problem that can
lead to a class of bugs known as one-frame-off lags. In this type of bug, the state
of one object lags one frame behind the states of its peers, which manifests
itself on-screen as a lack of synchronization between game objects.
t1
t
SA
ObjectA
ObjectB
SA
ObjectC
ObjectD
SC
t2
SB
SD
SB
SC
Figure 15.16. In practice, the states of the game objects are updated one by one. This means that
at some arbitrary moment during the update loop, some objects will think the current time is t2
while others think it is still t1. Some objects may be only partially updated, so their states will be
internally inconsistent. In effect, the state of such an object lies at a point between t1 and t2.

930
15. Runtime Gameplay Foundation Systems
15.6.3.4
Object State Caching
As described above, one solution to this problem is to group the game ob-
jects into buckets (Section 15.6.3.2). One problem with a simple bucketed up-
date approach is that it imposes somewhat arbitrary limitations on the way in
which game objects are permitted to query one another for state information.
If a game object A wants the updated state vector SB(t2) of another object B,
then object B must reside in a previously updated bucket. Likewise, if object A
wants the previous state vector SB(t1) of object B, then object B must reside
in a yet-to-be-updated bucket. Object A should never ask for the state vector
of an object within its own bucket, because, as we stated in the rule above,
those state vectors may be only partially updated. Or at best, there’s some
uncertainty as to whether you’re accessing the other object’s state at time t1 or
time t2.
One way to improve consistency is to arrange for each game object to cache
its previous state vector Si(t1) while it is calculating its new state vector Si(t2)
rather than overwriting it in-place during its update. This has two immediate
beneﬁts. First, it allows any object to safely query the previous state vector of
any other object without regard to update order. Second, it guarantees that
a totally consistent state vector (Si(t1)) will always be available, even during
the update of the new state vector. To my knowledge there is no standard
terminology for this technique, so I’ll call it state caching for lack of a better
name.
Another beneﬁt of state caching is that we can linearly interpolate between
the previous and next states in order to approximate the state of an object at
any moment between these two points in time. The Havok physics engine
maintains the previous and current state of every rigid body in the simulation
for just this purpose.
The downside of state caching is that it consumes twice the memory of the
update-in-place approach. It also only solves half the problem, because while
the previous states at time t1 are fully consistent, the new states at time t2 still
suffer from potential inconsistency. Nonetheless, the technique can be useful
when applied judiciously.
This technique lies at the heart of pure functional programming—a paradigm
in which all operations are performed by functions with a clear input and
output, and all data is considered constant and immutable (see Section 15.8.2).
15.6.3.5
Time-Stamping
One easy and low-cost way to improve the consistency of game object states
is to time-stamp them. It is then a trivial matter to determine whether a game

15.6. Updating Game Objects in Real Time
931
object’s state vector corresponds to its conﬁguration at a previous time or the
current time. Any code that queries the state of another game object during
the update loop can assert or explicitly check the time stamp to ensure that
the proper state information is being obtained.
Time-stamping does not address the inconsistency of states during the
update of a bucket. However, we can set a global or static variable to re-
ﬂect which bucket is currently being updated. Presumably every game object
“knows” in which bucket it resides. So we can check the bucket of a queried
game object against the currently updating bucket and assert that they are
not equal in order to guard against inconsistent state queries. (Of course, this
technique only works when our object update loops are single-threaded.)
15.6.4
Designing for Parallelism
In Section 7.6, we introduced a number of approaches that allow a game en-
gine to take advantage of the parallel processing resources that have become
the norm in recent gaming hardware. How, then, does parallelism affect the
way in which game object states are updated?
15.6.4.1
Parallelizing the Game Object Model Itself
Game object models are notoriously difﬁcult to parallelize for a few reasons.
Game objects tend to be highly interdependent upon one another and upon
the data used and/or generated by numerous engine subsystems. Game ob-
jects communicate with one another, sometimes multiple times during the up-
date loop, and the pattern of communication can be unpredictable and highly
sensitive to the inputs of the player and the events that are occurring in the
game world. This makes it difﬁcult to process game object updates in a con-
current manner (using multiple threads or multiple CPU cores) because the
amount of thread synchronization that would be required to support inter-
object communication is usually prohibitive from a performance standpoint.
And the commonplace practice of peeking directly into a foreign game ob-
ject’s state vector makes it impossible to DMA a game object to the isolated
memory of a coprocessor, such as the PlayStation 3’s SPU, for updating.
That said, game object updating can theoretically be done in parallel. To
make it practical, we’d need to carefully design the entire object model to
ensure that game objects never peek directly into the state vectors of other
game objects.
All inter-object communication would have to be done via
message-passing, and we’d need an efﬁcient system for passing messages be-
tween game objects even when those objects reside in totally separate mem-

932
15. Runtime Gameplay Foundation Systems
ory spaces or are being processed by different physical CPU cores. Some re-
search has been done into using a distributed programming language, such as
Ericsson’s Erlang language (http://www.erlang.org), to code game object mod-
els. Such languages provide built-in support for parallel processing and mes-
sage passing and handle context switching between threads much more efﬁ-
ciently and quickly than in a language like C or C++, and their programming
idioms help programmers to never “break the rules” that allow concurrent,
distributed, multiple-agent designs to function properly and efﬁciently.
Multithreaded game object updating is yet another place where the state
caching technique described in Section 15.6.3.4 can be of use. Without state
caching, we would need to use a mutex or critical section lock whenever one
object needs to directly read the state data of another object. This is because
the other object’s state vector might be in the process of being updated in
another thread at the moment we try to read it, leading to difﬁcult-to-ﬁnd
bugs. But with state caching, we can access the previous state vectors of other
objects without the need for a lock, because the previous state vectors are
guaranteed to be constant during the update.
15.6.4.2
Interfacing with Concurrent Engine Subsystems
Clearly, the most performance-critical parts of our engine—such as render-
ing, animation, audio and physics—are the ones that will beneﬁt most from
parallel processing. So, whether or not our game object model is being up-
dated in a single thread, or across multiple cores, it needs to be able to interface
with low-level engine systems that are almost certainly multithreaded. This,
in turn, means that game programmers need to avoid certain programming
paradigms that may have served them well in the pre-parallel-processing era
and adopt some new ones in their place.
Probably the most important shift a game programmer must make is to
begin thinking asynchronously. As described in Section 7.6.5, this means that
when a game object requires a time-consuming operation to be performed,
it should avoid calling a blocking function—a function that does its work di-
rectly in the context of the calling thread, thereby blocking that thread until
the work has been completed. Instead, whenever possible, large or expen-
sive jobs should be requested by calling a non-blocking function—a function
that sends the request to be executed by another thread, core or processor and
then immediately returns control to the calling function. The main game loop
can proceed with other unrelated work, including updating other game ob-
jects, while the original object waits for the results of its request. Later in the
same frame, or in next frame, that game object can pick up the results of its
request and make use of them.

15.7. Events and Message-Passing
933
Batching is another shift in thinking for game programmers. As we men-
tioned in Section 15.6.2, it is more efﬁcient to collect similar tasks into batches
and perform them en masse than it is to run each task independently. This
applies to the process of updating game object states as well. For example, if
a game object needs to cast 100 rays into the collision world for various pur-
poses, it is best if those ray cast requests can be queued up and executed as
one big batch. If an existing game engine is being retroﬁtted for parallelism,
this often requires code to be rewritten so that it batches requests rather than
doing them individually.
One particularly tricky aspect of converting synchronous, unbatched code
to use an asynchronous, batched approach is determining when during the
game loop (a) to kick off the request and (b) to wait for and utilize the results.
In doing this, it is often helpful to ask ourselves the following questions:
•
How early can we kick off this request? The earlier we make the request, the
more likely it is to be done when we actually need the results—and this
maximizes CPU utilization by helping to ensure that the main thread is
never idle waiting for an asynchronous request to complete. So for any
given request, we should determine the earliest point during the frame
at which we have enough information to kick it off, and kick it there.
•
How long can we wait before we need the results of this request? Perhaps we
can wait until later in the update loop to do the second half of an op-
eration. Perhaps we can tolerate a one-frame lag and use last frame’s
results to update the object’s state this frame. (Some subsystems like AI
can tolerate even longer lag times because they update only every few
seconds.) In many circumstances, code that uses the results of a request
can in fact be deferred until later in the frame, given a little thought,
some code refactoring, and possibly some additional caching of inter-
mediate data.
15.7
Events and Message-Passing
Games are inherently event-driven. An event is anything of interest that hap-
pens during gameplay. An explosion going off, the player being sighted by
an enemy, a health pack getting picked up—these are all events. Games gen-
erally need a way to (a) notify interested game objects when an event occurs
and (b) arrange for those objects to respond to interesting events in various
ways—we call this handling the event. Different types of game objects will
respond in different ways to an event. The way in which a particular type
of game object responds to an event is a crucial aspect of its behavior, just as

934
15. Runtime Gameplay Foundation Systems
important as how the object’s state changes over time in the absence of any
external inputs. For example, the behavior of the ball in Pong is governed in
part by its velocity, in part by how it reacts to the event of striking a wall or
paddle and bouncing off, and in part by what happens when the ball is missed
by one of the players.
15.7.1
The Problem with Statically Typed Function Binding
One simple way to notify a game object that an event has occurred is to sim-
ply call a method (member function) on the object. For example, when an
explosion goes off, we could query the game world for all objects within the
explosion’s damage radius and then call a virtual function named something
like OnExplosion() on each one. This is illustrated by the following pseu-
docode:
void Explosion::Update()
{
// ...
if (ExplosionJustWentOff())
{
GameObjectCollection damagedObjects;
g_world.QueryObjectsInSphere(GetDamageSphere(),
damagedObjects);
for (each object in damagedObjects)
{
object.OnExplosion(*this);
}
}
// ...
}
The call to OnExplosion() is an example of statically typed late function
binding. Function binding is the process of determining which function im-
plementation to invoke at a particular call location—the implementation is,
in effect, bound to the call. Virtual functions, such as our OnExplosion()
event-handling function, are said to be late-bound. This means that the com-
piler doesn’t actually know which of the many possible implementations of
the function is going to be invoked at compile time—only at runtime, when
the type of the target object is known, will the appropriate implementation
be invoked. We say that a virtual function call is statically typed because the
compiler does know which implementation to invoke given a particular object

15.7. Events and Message-Passing
935
type. It knows, for example, that Tank::OnExplosion() should be called
when the target object is a Tank and that Crate::OnExplosion() should
be called when the object is a Crate.
The problem with statically typed function binding is that it introduces
a degree of inﬂexibility into our implementation. For one thing, the virtual
OnExplosion() function requires all game objects to inherit from a common
base class. Moreover, it requires that base class to declare the virtual function
OnExplosion(), even if not all game objects can respond to explosions. In
fact, using statically typed virtual functions as event handlers would require
our base GameObject class to declare virtual functions for all possible events
in the game! This would make adding new events to the system difﬁcult. It
precludes events from being created in a data-driven manner—for example,
within the world editing tool. It also provides no mechanism for certain types
of objects, or certain individual object instances, to register interest in certain
events but not others. Every object in the game, in effect, “knows” about
every possible event, even if its response to the event is to do nothing (i.e., to
implement an empty, do-nothing event handler function).
What we really need for our event handlers, then, is dynamically typed late
function binding. Some programming languages support this feature natively
(e.g., C#’s delegates). In other languages, the engineers must implement it
manually. There are many ways to approach this problem, but most boil down
to taking a data-driven approach. In other words, we encapsulate the notion
of a function call in an object and pass that object around at runtime in order
to implement a dynamically typed late-bound function call.
15.7.2
Encapsulating an Event in an Object
An event is really comprised of two components: its type (explosion, friend
injured, player spotted, health pack picked up, etc.) and its arguments. The
arguments provide speciﬁcs about the event. (How much damage did the
explosion do? Which friend was injured? Where was the player spotted?
How much health was in the health pack?) We can encapsulate these two
components in an object, as shown by the following rather over-simpliﬁed
code snippet:
struct Event
{
const U32 MAX_ARGS = 8;
EventType
m_type;
U32
m_numArgs;
EventArg
m_aArgs[MAX_ARGS];
};

936
15. Runtime Gameplay Foundation Systems
Some game engines call these things messages or commands instead of events.
These names emphasize the idea that informing objects about an event is es-
sentially equivalent to sending a message or command to those objects.
Practically speaking, event objects are usually not quite this simple. We
might implement different types of events by deriving them from a root event
class, for example. The arguments might be implemented as a linked list or a
dynamically allocated array capable of containing arbitrary numbers of argu-
ments, and the arguments might be of various data types.
Encapsulating an event (or message) in an object has many beneﬁts:
•
Single event handling function. Because the event object encodes its type
internally, any number of different event types can be represented by an
instance of a single class (or the root class of an inheritance hierarchy).
This means that we only need one virtual function to handle all types of
events (e.g., virtual void OnEvent(Event& event);).
•
Persistence. Unlike a function call, whose arguments go out of scope
after the function returns, an event object stores both its type and its
arguments as data. An event object therefore has persistence. It can be
stored in a queue for handling at a later time, copied and broadcast to
multiple receivers and so on.
•
Blind event forwarding. An object can forward an event that it receives to
another object without having to “know” anything about the event. For
example, if a vehicle receives a Dismount event, it can forward it to all
of its passengers, thereby allowing them to dismount the vehicle, even
though the vehicle itself knows nothing about dismounting.
This idea of encapsulating an event/message/command in an object is
commonplace in many ﬁelds of computer science. It is found not only in
game engines but in other systems like graphical user interfaces, distributed
communication systems and many others. The well-known “Gang of Four”
design patterns book [17] calls this the Command design pattern.
15.7.3
Event Types
There are many ways to distinguish between different types of events. One
simple approach in C or C++ is to deﬁne a global enum that maps each event
type to a unique integer.
enum EventType
{
EVENT_TYPE_LEVEL_STARTED,
EVENT_TYPE_PLAYER_SPAWNED,

15.7. Events and Message-Passing
937
EVENT_TYPE_ENEMY_SPOTTED,
EVENT_TYPE_EXPLOSION,
EVENT_TYPE_BULLET_HIT,
// ...
}
This approach enjoys the beneﬁts of simplicity and efﬁciency (since integers
are usually extremely fast to read, write and compare). However, it also suf-
fers from two problems. First, knowledge of all event types in the entire game
is centralized, which can be seen as a form of broken encapsulation (for better
or for worse—opinions on this vary). Second, the event types are hard-coded,
which means new event types cannot easily be deﬁned in a data-driven man-
ner. Third, enumerators are just indices, so they are order-dependent. If some-
one accidentally adds a new event type in the middle of the list, the indices
of all subsequent event ids change, which can cause problems if event ids
are stored in data ﬁles. As such, an enumeration-based event typing system
works well for small demos and prototypes but does not scale very well at all
to real games.
Another way to encode event types is via strings. This approach is totally
free-form, and it allows a new event type to be added to the system by merely
thinking up a name for it. But it suffers from many problems, including a
strong potential for event name conﬂicts, the possibility of events not work-
ing because of a simple typo, increased memory requirements for the strings
themselves, and the relatively high cost of comparing strings relative to that
of comparing integers. Hashed string ids can be used instead of raw strings
to eliminate the performance problems and increased memory requirements,
but they do nothing to address event name conﬂicts or typos. Nonetheless,
the extreme ﬂexibility and data-driven nature of a string- or string-id-based
event system is considered worth the risks by many game teams, including
Naughty Dog.
Tools can be implemented to help avoid some of the risks involved in us-
ing strings to identify events. For example, a central database of all event type
names could be maintained. A user interface could be provided to permit new
event types to be added to the database. Naming conﬂicts could be automat-
ically detected when a new event is added, and the user could be disallowed
from adding duplicate event types. When selecting a preexisting event, the
tool could provide a sorted list in a drop-down combo box rather than requir-
ing the user to remember the name and type it manually. The event database
could also store metadata about each type of event, including documentation
about its purpose and proper usage and information about the number and
types of arguments it supports. This approach can work really well, but we

938
15. Runtime Gameplay Foundation Systems
should not forget to account for the costs of setting up and maintaining such
a system, as they are not insigniﬁcant.
15.7.4
Event Arguments
The arguments of an event usually act like the argument list of a function,
providing information about the event that might be useful to the receiver.
Event arguments can be implemented in all sorts of ways.
We might derive a new type of Event class for each unique type of event.
The arguments can then be hard-coded as data members of the class. For
example:
class ExplosionEvent : public Event
{
Point
m_center;
float
m_damage;
float
m_radius;
};
Another approach is to store the event’s arguments as a collection of vari-
ants. A variant is a data object that is capable of holding more than one type
of data. It usually stores information about the data type that is currently be-
ing stored, as well as the data itself. In an event system, we might want our
arguments to be integers, ﬂoating-point values, Booleans or hashed string ids.
So in C or C++, we could deﬁne a variant class that looks something like this:
struct Variant
{
enum Type
{
TYPE_INTEGER,
TYPE_FLOAT,
TYPE_BOOL,
TYPE_STRING_ID,
TYPE_COUNT // number of unique types
};
Type
m_type;
union
{
I32
m_asInteger;
F32
m_asFloat;
bool
m_asBool;
U32
m_asStringId;
};
};

15.7. Events and Message-Passing
939
The collection of variants within an Event might be implemented as an
array with a small, ﬁxed maximum size (say 4, 8 or 16 elements). This imposes
an arbitrary limit on the number of arguments that can be passed with an
event, but it also side-steps the problems of dynamically allocating memory
for each event’s argument payload, which can be a big beneﬁt, especially in
memory-constrained console games.
The collection of variants might be implemented as a dynamically sized
data structure, like a dynamically sized array (like std::vector) or a linked
list (like std::list).
This provides a great deal of additional ﬂexibility
over a ﬁxed size design, but it incurs the cost of dynamic memory alloca-
tion. A pool allocator could be used to great effect here, presuming that each
Variant is the same size.
15.7.4.1
Event Arguments as Key-Value Pairs
A fundamental problem with an indexed collection of event arguments is order
dependency. Both the sender and the receiver of an event must “know” that
the arguments are listed in a speciﬁc order. This can lead to confusion and
bugs. For example, a required argument might be accidentally omitted or an
extra one added.
This problem can be avoided by implementing event arguments as key-
value pairs. Each argument is uniquely identiﬁed by its key, so the arguments
can appear in any order, and optional arguments can be omitted altogether.
The argument collection might be implemented as a closed or open hash table,
with the keys used to hash into the table, or it might be an array, linked list or
binary search tree of key-value pairs. These ideas are illustrated in Table 15.1.
The possibilities are numerous, and the speciﬁc choice of implementation is
largely unimportant as long as the game’s particular requirements have been
effectively and efﬁciently met.
Type
Table 15.1. The arguments of an event object can be implemented as a collection of key-value
pairs. The keys prevent order-dependency problems because each event argument is uniquely
identiﬁed by its key.

940
15. Runtime Gameplay Foundation Systems
15.7.5
Event Handlers
When an event, message or command is received by a game object, it needs
to respond to the event in some way. This is known as handling the event, and
it is usually implemented by a function or a snippet of script code called an
event handler. (We’ll have more to learn about game scripting later on.)
Often an event handler is a single native virtual function or script func-
tion that is capable of handling all types of events (e.g., virtual void On-
Event(Event& event)). In this case, the function usually contains some
kind of switch statement or cascaded if/else-if clause to handle the various
types of events that might be received. A typical event handler function might
look something like this:
virtual void SomeObject::OnEvent(Event& event)
{
switch (event.GetType())
{
case SID('EVENT_ATTACK'):
RespondToAttack(event.GetAttackInfo());
break;
case SID('EVENT_HEALTH_PACK'):
AddHealth(event.GetHealthPack().GetHealth());
break;
// ...
default:
// Unrecognized event.
break;
}
}
Alternatively, we might implement a suite of handler functions, one for
each type of event (e.g., OnThis(), OnThat(), ...). However, as we dis-
cussed above, a proliferation of event handler functions can be problematic.
A Windows GUI toolkit called Microsoft Foundation Classes (MFC) was
well-known for its message maps—a system that permitted any Windows mes-
sage to be bound at runtime to an arbitrary non-virtual or virtual function.
This avoided the need to declare handlers for all possible Windows messages
in a single root class, while at the same time avoiding the big switch statement
that is commonplace in non-MFC Windows message-handling functions. But
such a system is probably not worth the hassle—a switch statement works
really well and is simple and clear.

15.7. Events and Message-Passing
941
15.7.6
Unpacking an Event’s Arguments
The example above glosses over one important detail—namely, how to ex-
tract data from the event’s argument list in a type-safe manner. For example,
event.GetHealthPack() presumably returns a HealthPack game object,
which in turn we presume provides a member function called GetHealth().
This implies that the root Event class “knows” about health packs (as well as,
by extension, every other type of event argument in the game!) This is prob-
ably an impractical design. In a real engine, there might be derived Event
classes that provide convenient data-access APIs such as GetHealthPack().
Or the event handler might have to unpack the data manually and cast them
to the appropriate types. This latter approach raises type safety concerns, al-
though practically speaking it usually isn’t a huge problem because the type
of the event is always known when the arguments are unpacked.
15.7.7
Chains of Responsibility
Game objects are almost always dependent upon one another in various ways.
For example, game objects usually reside in a transformation hierarchy, which
allows an object to rest on another object or be held in the hand of a charac-
ter. Game objects might also be made up of multiple interacting components,
leading to a star topology or a loosely connected “cloud” of component ob-
jects. A sports game might maintain a list of all the characters on each team.
In general, we can envision the interrelationships between game objects as
one or more relationship graphs (remembering that a list and a tree are just spe-
cial cases of a graph). A few examples of relationship graphs are shown in
Figure 15.17.
Attachment
Graph
Event1
Event3
Component
Graph
Event2
Team
Graph
Team
Carter
Evan
Quinn
Cooper
ObjectA
ComponentA2
ComponentA1
ComponentA3
Clip
Weapon
Character
Vehicle
Figure 15.17. Game objects are interrelated in various ways, and we can draw graphs depicting
these relationships. Any such graph might serve as a distribution channel for events.

942
15. Runtime Gameplay Foundation Systems
It often makes sense to be able to pass events from one object to the next
within these relationship graphs. For example, when a vehicle receives an
event, it may be convenient to pass the event to all of the passengers riding on
the vehicle, and those passengers may wish to forward the event to the objects
in their inventories. When a multicomponent game object receives an event, it
may be necessary to pass the event to all of the components so that they all get
a crack at handling it. Or when an event is received by a character in a sports
game, we might want to pass it on to all of his or her teammates as well.
The technique of forwarding events within a graph of objects is a common
design pattern in object-oriented, event-driven programming, sometimes re-
ferred to as Chain of Responsibility [17]. Usually, the order in which the event
is passed around the system is predetermined by the engineers. The event is
passed to the ﬁrst object in the chain, and the event handler returns a Boolean
or an enumerated code indicating whether or not it recognized and handled
the event. If the event is consumed by a receiver, the process of event for-
warding stops; otherwise, the event is forwarded on to the next receiver in
the chain. An event handler that supports Chain of Responsibility style event
forwarding might look something like this:
virtual bool SomeObject::OnEvent(Event& event)
{
// Call the base class' handler first.
if (BaseClass::OnEvent(event))
{
return true;
}
// Now try to handle the event myself.
switch (event.GetType())
{
case SID('EVENT_ATTACK'):
RespondToAttack(event.GetAttackInfo());
return false; // OK to forward this event to others.
case SID('EVENT_HEALTH_PACK'):
AddHealth(event.GetHealthPack().GetHealth());
return true; // I consumed the event; don't forward.
// ...
default:
return false; // I didn't recognize this event.
}
}

15.7. Events and Message-Passing
943
When a derived class overrides an event handler, it can be appropriate
to call the base class’s implementation as well if the class is augmenting but
not replacing the base class’s response. In other situations, the derived class
might be entirely replacing the response of the base class, in which case the
base class’s handler should not be called. This is another kind of responsibility
chain.
Event forwarding has other applications as well. For example, we might
want to multicast an event to all objects within a radius of inﬂuence (for an
explosion, for example). To implement this, we can leverage our game world’s
object query mechanism to ﬁnd all objects within the relevant sphere and then
forward the event to all of the returned objects.
15.7.8
Registering Interest in Events
It’s reasonably safe to say that most objects in a game do not need to respond
to every possible event. Most types of game objects have a relatively small set
of events in which they are “interested.” This can lead to inefﬁciencies when
multicasting or broadcasting events, because we need to iterate over a group
of objects and call each one’s event handler, even if the object is not interested
in that particular kind of event.
One way to overcome this inefﬁciency is to permit game objects to regis-
ter interest in particular kinds of events. For example, we could maintain one
linked list of interested game objects for each distinct type of event, or each
game object could maintain a bit array, in which the setting of each bit corre-
sponds to whether or not the object is interested in a particular type of event.
By doing this, we can avoid calling the event handlers of any objects that do
not care about the event.
Even better, we might be able to restrict our original game object query to
include only those objects that are interested in the event we wish to multi-
cast. For example, when an explosion goes off, we can ask the collision sys-
tem for all objects that are within the damage radius and that can respond
to Explosion events. This can save time overall, because we avoid iterating
over objects that we know aren’t interested in the event we’re multicasting.
Whether or not such an approach will produce a net gain depends on how the
query mechanism is implemented and the relative costs of ﬁltering the objects
during the query versus ﬁltering them during the multicast iteration.
15.7.9
To Queue or Not to Queue
Most game engines provide a mechanism for handling events immediately
when they are sent. In addition to this, some engines also permit events to
be queued for handling at an arbitrary future time. Event queuing has some

944
15. Runtime Gameplay Foundation Systems
attractive beneﬁts, but it also increases the complexity of the event system
signiﬁcantly and poses some unique problems. We’ll investigate the pros and
cons of event queuing in the following sections and learn how such systems
are implemented in the process.
15.7.9.1
Some Beneﬁts of Event Queuing
Control Over When Events are Handled
We have seen that we must be careful to update engine subsystems and game
objects in a speciﬁc order to ensure correct behavior and maximize runtime
performance. In the same sense, certain kinds of events may be highly sen-
sitive to exactly when within the game loop they are handled. If all events
are handled immediately upon being sent, the event handler functions end
up being called in unpredictable and difﬁcult-to-control ways throughout the
course of the game loop. By deferring events via an event queue, the engi-
neers can take steps to ensure that events are only handled when it is safe and
appropriate to do so.
Ability to Post Events into the Future
When an event is sent, the sender can usually specify a delivery time—for
example, we might want the event to be handled later in the same frame, next
frame or some number of seconds after it was sent. This feature amounts to
an ability to post events into the future, and it has all sorts of interesting uses.
We can implement a simple alarm clock by posting an event into the future.
A periodic task, such as blinking a light every two seconds, can be executed
by posting an event whose handler performs the periodic task and then posts
a new event of the same type one time period into the future.
To implement the ability to post events into the future, each event is
stamped with a desired delivery time prior to being queued. An event is only
handled when the current game clock matches or exceeds its delivery time.
An easy way to make this work is to sort the events in the queue in order of
increasing delivery time. Each frame, the ﬁrst event on the queue can be in-
spected and its delivery time checked. If the delivery time is in the future, we
abort immediately because we know that all subsequent events are also in the
future. But if we see an event whose delivery time is now or in the past, we
extract it from the queue and handle it. This continues until an event is found
whose delivery time is in the future. The following pseudocode illustrates this
process:
// This function is called at least once per frame. Its
// job is to dispatch all events whose delivery time is
// now or in the past.

15.7. Events and Message-Passing
945
void EventQueue::DispatchEvents(F32 currentTime)
{
// Look at, but don't remove, the next event on the
// queue.
Event* pEvent = PeekNextEvent();
while (pEvent
&& pEvent->GetDeliveryTime() <= currentTime)
{
// Remove the event from the queue.
RemoveNextEvent();
// Dispatch it to its receiver's event handler.
pEvent->Dispatch();
// Peek at the next event on the queue (again
// without removing it).
pEvent = PeekNextEvent();
}
}
Event Prioritization
Even if our events are sorted by delivery time in the event queue, the order
of delivery is still ambiguous when two or more events have exactly the same
delivery time. This can happen more often than you might think, because it is
quite common for events’ delivery times to be quantized to an integral num-
ber of frames. For example, if two senders request that events be dispatched
“this frame,” “next frame” or “in seven frames from now,” then those events
will have identical delivery times.
One way to resolve these ambiguities is to assign priorities to events. When-
ever two events have the same time stamp, the one with higher priority should
always be serviced ﬁrst. This is easily accomplished by ﬁrst sorting the event
queue by increasing delivery times and then sorting each group of events with
identical delivery times in order of decreasing priority.
We could allow up to four billion unique priority levels by encoding our
priorities in a raw, unsigned 32-bit integer, or we could limit ourselves to only
two or three unique priority levels (e.g., low, medium and high). In every
game engine, there exists some minimum number of priority levels that will
resolve all real ambiguities in the system. It’s usually best to aim as close to
this minimum as possible. With a very large number of priority levels, it can
become a small nightmare to ﬁgure out which event will be handled ﬁrst in
any given situation. However, the needs of every game’s event system are
different, and your mileage may vary.

946
15. Runtime Gameplay Foundation Systems
15.7.9.2
Some Problems with Event Queuing
Increased Event System Complexity
In order to implement a queued event system, we need more code, additional
data structures and more-complex algorithms than would be necessary to im-
plement an immediate event system. Increased complexity usually translates
into longer development times and a higher cost to maintain and evolve the
system during development of the game.
Deep-Copying Events and Their Arguments
With an immediate event handling approach, the data in an event’s arguments
need only persist for the duration of the event handling function (and any
functions it may call). This means that the event and its argument data can
reside literally anywhere in memory, including on the call stack. For example,
we could write a function that looks something like this:
void SendExplosionEventToObject(GameObject& receiver)
{
// Allocate event args on the call stack.
Point centerPoint(-2.0f, 31.5f, 10.0f);
F32
damage = 5.0f;
F32
radius = 2.0f;
// Allocate the event on the call stack.
Event event("Explosion");
event.SetArgFloat("Damage", damage);
event.SetArgPoint("Center", &centerPoint);
event.SetArgFloat("Radius", radius);
// Send the event, which causes the receiver's event
// handler to be called immediately, as shown below.
event.Send(receiver);
//{
//
receiver.OnEvent(event);
//}
}
When an event is queued, its arguments must persist beyond the scope of
the sending function. This implies that we must copy the entire event object
prior to storing the event in the queue. We must perform a deep-copy, meaning
that we copy not only the event object itself but its entire argument payload
as well, including any data to which it may be pointing. Deep-copying the
event ensures that there are no dangling references to data that exist only

15.7. Events and Message-Passing
947
in the sending function’s scope, and it permits the event to be stored indef-
initely. The example event-sending function shown above still looks basically
the same when using a queued event system, but as you can see in the itali-
cized code below, the implementation of the Event::Queue() function is a
bit more complex than its Send() counterpart:
void SendExplosionEventToObject(GameObject& receiver)
{
// We can still allocate event args on the call
// stack.
Point centerPoint(-2.0f, 31.5f, 10.0f);
F32
damage = 5.0f;
F32
radius = 2.0f;
// Still OK to allocate the event on the call stack
Event event("Explosion
event.SetArgFloat("Damage", damage);
event.SetArgPoint("Center", &centerPoint);
event.SetArgFloat("Radius", radius);
// This stores the event in the receiver's queue for
// handling at a future time. Note how the event
// must be deep-copied prior to being enqueued, since
// the original event resides on the call stack and
// will go out of scope when this function returns.
event.Queue(receiver);
//{
//
Event* pEventCopy = DeepCopy(event);
//
receiver.EnqueueEvent(pEventCopy);
//}
}
Dynamic Memory Allocation for Queued Events
Deep-copying of event objects implies a need for dynamic memory allocation,
and as we’ve already noted many times, dynamic allocation is undesirable in
a game engine due to its potential cost and its tendency to fragment memory.
Nonetheless, if we want to queue events, we’ll need to dynamically allocate
memory for them.
As with all dynamic allocation in a game engine, it’s best if we can select a
fast and fragmentation-free allocator. We might be able to use a pool allocator,
but this will only work if all of our event objects are the same size and if their
argument lists are comprised of data elements that are themselves all the same
size. This may well be the case—for example, the arguments might each be
a Variant, as described above. If our event objects and/or their arguments

948
15. Runtime Gameplay Foundation Systems
can vary in size, a small memory allocator might be applicable. (Recall that a
small memory allocator maintains multiple pools, one for each of a few pre-
determined small allocation sizes.) When designing a queued event system,
always be careful to take dynamic allocation requirements into account.
Other designs are possible, of course. For example, at Naughty Dog we
allocate queued events as relocatable memory blocks. See Section 5.2.2.2 for
more information on relocatable memory.
Debugging Difﬁculties
With queued events, the event handler is not called directly by the sender of
that event. So, unlike in immediate event handling, the call stack does not
tell us where the event came from. We cannot walk up the call stack in the
debugger to inspect the state of the sender or the circumstances under which
the event was sent. This can make debugging deferred events a bit tricky, and
things get even more difﬁcult when events are forwarded from one object to
another.
Some engines store debugging information that forms a paper trail of the
event’s travels throughout the system, but no matter how you slice it, event
debugging is usually much easier in the absence of queuing.
Event queuing also leads to interesting and hard-to-track-down race condi-
tion bugs. We may need to pepper multiple event dispatches throughout our
game loop, to ensure that events are delivered without incurring unwanted
one-frame delays yet still ensuring that game objects are updated in the proper
order during the frame. For example, during the animation update, we might
detect that a particular animation has run to completion. This might cause an
event to be sent whose handler wants to play a new animation. Clearly, we
want to avoid a one-frame delay between the end of the ﬁrst animation and
the start of the next. To make this work, we need to update animation clocks
ﬁrst (so that the end of the animation can be detected and the event sent);
then we should dispatch events (so that the event handler has a chance to re-
quest a new animation), and ﬁnally we can start animation blending (so that
the ﬁrst frame of the new animation can be processed and displayed). This is
illustrated in the code snippet below:
while (true) // main game loop
{
// ...
// Update animation clocks. This may detect the end
// of a clip, and cause EndOfAnimation events to
// be sent.
g_animationEngine.UpdateLocalClocks(dt);

15.7. Events and Message-Passing
949
// Next, dispatch events. This allows an
// EndOfAnimation event handler to start up a new
// animation this frame if desired.
g_eventSystem.DispatchEvents();
// Finally, start blending all currently playing
// animations (including any new clips started
// earlier this frame).
g_animationEngine.StartAnimationBlending();
// ...
}
15.7.10
Some Problems with Immediate Event Sending
Not queuing events also has its share of issues. For example, immediate event
handling can lead to extremely deep call stacks. Object A might send object B
an event, and in its event handler, B might send another event, which might
send another event, and another and so on. In a game engine that supports
immediate event handling, it’s not uncommon to see a call stack that looks
something like this:
...
ShoulderAngel::OnEvent()
Event::Send()
Characer::OnEvent()
Event::Send()
Car::OnEvent()
Event::Send()
HandleSoundEffect()
AnimationEngine::PlayAnimation()
Event::Send()
Character::OnEvent()
Event::Send()
Character::OnEvent()
Event::Send()
Character::OnEvent()
Event::Send()
Car::OnEvent()
Event::Send()
Car::OnEvent()
Event::Send()
Car::Update()
GameWorld::UpdateObjectsInBucket()
Engine::GameLoop()
main()

950
15. Runtime Gameplay Foundation Systems
A deep call stack like this can exhaust available stack space in extreme
cases (especially if we have an inﬁnite loop of event sending), but the real
crux of the problem here is that every event handler function must be written
to be fully re-entrant. This means that the event handler can be called recur-
sively without any ill side-effects. As a contrived example, imagine a function
that increments the value of a global variable. If the global is supposed to be
incremented only once per frame, then this function is not re-entrant, because
multiple recursive calls to the function will increment the variable multiple
times.
15.7.11
Data-Driven Event/Message-Passing Systems
Event systems give the game programmer a great deal of ﬂexibility over and
above what can be accomplished with the statically typed function calling
mechanisms provided by languages like C and C++. However, we can do
better. In our discussions thus far, the logic for sending and receiving events
is still hard-coded and therefore under the exclusive control of the engineers.
If we could make our event system data-driven, we could extend its power
into the hands of our game designers.
There are many ways to make an event system data-driven. Starting with
the extreme of an entirely hard-coded (non-data-driven) event system, we
could imagine providing some simple data-driven conﬁgurability. For ex-
ample, designers might be allowed to conﬁgure how individual objects, or
entire classes of object, respond to certain events. In the world editor, we can
imagine selecting an object and then bringing up a scrolling list of all possible
events that it might receive. For each one, the designer could use drop-down
combo boxes and check boxes to control if, and how, the object responds, by
selecting from a set of hard-coded, predeﬁned choices. For example, given
the event “PlayerSpotted,” AI-controlled characters might be conﬁgured to do
one of the following actions: run away, attack or ignore the event altogether.
The event systems of some real commercial game engines are implemented in
essentially this way.
At the other end of the gamut, our engine might provide the game design-
ers with a simple scripting language (a topic we’ll explore in detail in Section
15.8). In this case, the designer can literally write code that deﬁnes how a
particular kind of game object will respond to a particular kind of event. In
a scripted model, the designers are really just programmers (working with a
somewhat less powerful but also easier-to-use and hopefully less error-prone
language than the engineers), so anything is possible. Designers might deﬁne
new types of events, send events and receive and handle events in arbitrary
ways. This is what we do at Naughty Dog.

15.7. Events and Message-Passing
951
The problem with a simple, conﬁgurable event system is that it can severely
limit what the game designers are capable of doing on their own, without the
help of a programmer. On the other hand, a fully scripted solution has its
own share of problems: Many game designers are not professional software
engineers by training, so some designers ﬁnd learning and using a scripting
language a daunting task. Designers are also probably more prone to intro-
ducing bugs into the game than their engineer counterparts, unless they have
practiced scripting or programming for some time. This can lead to some
nasty surprises during alpha.
As a result, some game engines aim for a middle ground. They employ so-
phisticated graphical user interfaces to provide a great deal of ﬂexibility with-
out going so far as to provide users with a full-ﬂedged, free-form scripting
language. One approach is to provide a ﬂowchart-style graphical program-
ming language. The idea behind such a system is to provide the user with a
limited and controlled set of atomic operations from which to choose but with
plenty of freedom to wire them up in arbitrary ways. For example, in response
to an event like “PlayerSpotted,” the designer could wire up a ﬂowchart that
causes a character to retreat to the nearest cover point, play an animation, wait
5 seconds, and then attack. A GUI can also provide error-checking and valida-
tion to help ensure that bugs aren’t inadvertently introduced. Unreal’s Kismet
is an example of such a system—see the following section for more details.
15.7.11.1
Data Pathway Communication Systems
One of the problems with converting a function-call-like event system into a
data-driven system is that different types of events tend to be incompatible.
For example, let’s imagine a game in which the player has an electromagnetic
pulse gun. This pulse causes lights and electronic devices to turn off, scares
small animals and produces a shock wave that causes any nearby plants to
sway. Each of these game object types may already have an event response
that performs the desired behavior. A small animal might respond to the
“Scare” event by scurrying away. An electronic device might respond to the
“TurnOff” event by turning itself off. And plants might have an event handler
for a “Wind” event that causes them to sway. The problem is that our EMP
gun is not compatible with any of these objects’ event handlers. As a result,
we end up having to implement a new event type, perhaps called “EMP,” and
then write custom event handlers for every type of game object in order to
respond to it.
One solution to this problem is to take the event type out of the equation
and to think solely in terms of sending streams of data from one game object to
another. In such a system, every game object has one or more input ports to

952
15. Runtime Gameplay Foundation Systems
Animal
Scare
Foliage
Sway
Radio
TurnOn
Invert
In
Out
EMP Gun
Fire
Figure 15.18. The EMP gun produces a 1 at its “Fire” output when ﬁred. This can be connected to
any input port that expects a Boolean value, in order to trigger the behavior associated with that
input.
which a data stream can be connected, and one or more output ports through
which data can be sent to other objects. Provided we have some way of wiring
these ports together, such as a graphical user interface in which ports can be
connected to each other via rubber-band lines, then we can construct arbitrar-
ily complex behaviors. Continuing our example, the EMP gun would have
an output port, perhaps named “Fire,” that sends a Boolean signal. Most of
the time, the port produces the value 0 (false), but when the gun is ﬁred, it
sends a brief (one-frame) pulse of the value 1 (true). The other game objects in
the world have binary input ports that trigger various responses. The animals
might have a “Scare” input, the electronic devices a “TurnOn” input and the
foliage objects a “Sway” input. If we connect the EMP gun’s “Fire” output
port to the input ports of these game objects, we can cause the gun to trigger
the desired behaviors. (Note that we’d have to pipe the gun’s “Fire” output
through a node that inverts its input, prior to connecting it to the “TurnOn”
input of the electronic devices.
This is because we want them to turn off
when the gun is ﬁring.) The wiring diagram for this example is shown in
Figure 15.18.
Programmers decide what kinds of port(s) each type of game object will
have. Designers using the GUI can then wire these ports together in arbitrary
ways in order to construct arbitrary behaviors in the game. The programmers
also provide various other kinds of nodes for use within the graph, such as a
node that inverts its input, a node that produces a sine wave or a node that
outputs the current game time in seconds.
Various types of data might be sent along a data pathway. Some ports
might produce or expect Boolean data, while others might be coded to pro-
duce or expect data in the form of a unit ﬂoat. Still others might operate

15.7. Events and Message-Passing
953
Figure 15.19. Unreal Engine 4’s Kismet.
on 3D vectors, colors, integers and so on. It’s important in such a system
to ensure that connections are only made between ports with compatible data
types, or we must provide some mechanism for automatically converting data
types when two differently typed ports are connected together. For example,
connecting a unit-ﬂoat output to a Boolean input might automatically cause
any value less than 0.5 to be converted to false, and any value greater than or
equal to 0.5 to be converted to true. This is the essence of GUI-based event
systems like Unreal Engine 4’s Kismet. A screenshot of Kismet is shown in
Figure 15.19.
15.7.11.2
Some Pros and Cons of GUI-Based Programming
The beneﬁts of a graphical user interface over a straightforward, text-ﬁle-
based scripting language are probably pretty obvious: ease of use, a grad-
ual learning curve with the potential for in-tool help and tool tips to guide
the user, and plenty of error-checking. The downsides of a ﬂowchart style
GUI include the high cost to develop, debug, and maintain such a system, the

954
15. Runtime Gameplay Foundation Systems
additional complexity, which can lead to annoying or sometimes schedule-
killing bugs, and the fact that designers are sometimes limited in what they
can do with the tool. A text-ﬁle-based programming language has some dis-
tinct advantages over a GUI-based programming system, including its rela-
tive simplicity (meaning that it is much less prone to bugs), the ability to eas-
ily search and replace within the source code, and the freedom of each user to
choose the text editor with which they are most comfortable.
15.8
Scripting
A scripting language can be deﬁned as a programming language whose pri-
mary purpose is to permit users to control and customize the behavior of a
software application. For example, the Visual Basic language can be used to
customize the behavior of Microsoft Excel; both MEL language and Python
language can be used to customize the behavior of Maya. In the context of
game engines, a scripting language is a high-level, relatively easy-to-use pro-
gramming language that provides its users with convenient access to most of
the commonly used features of the engine. As such, a scripting language can
be used by programmers and non-programmers alike to develop a new game
or to customize—or “mod”—an existing game.
15.8.1
Runtime versus Data Deﬁnition
We should be careful to make an important distinction here. Game scripting
languages generally come in two ﬂavors:
•
Data-deﬁnition languages. The primary purpose of a data-deﬁnition lan-
guage is to permit users to create and populate data structures that are
later consumed by the engine. Such languages are often declarative (see
below) and are either executed or parsed ofﬂine or at runtime when the
data is loaded into memory.
•
Runtime scripting language. Runtime scripting languages are intended
to be executed within the context of the engine at runtime. These lan-
guages are usually used to extend or customize the hard-coded func-
tionality of the engine’s game object model and/or other engine sys-
tems.
In this section, we’ll focus primarily on using a runtime scripting language
for the purpose of implementing gameplay features by extending and cus-
tomizing the game’s object model.

15.8. Scripting
955
15.8.2
Programming Language Characteristics
In our discussion of scripting languages, it will be helpful for us all to be
on the same page with regard to programming language terminology. There
are all sorts of programming languages out there, but they can be classiﬁed
approximately according to a relatively small number of criteria. Let’s take a
brief look at these criteria:
•
Interpreted versus compiled languages. The source code of a compiled lan-
guage is translated by a program called a compiler into machine code,
which can be executed directly by the CPU. In contrast, the source code
of an interpreted language is either parsed directly at runtime or is pre-
compiled into platform-independent byte code, which is then executed
by a virtual machine (VM) at runtime. A virtual machine acts like an em-
ulation of an imaginary CPU, and byte code acts like a list of machine
language instructions that are consumed by this virtual CPU. The bene-
ﬁt of a virtual machine is that it can be quite easily ported to almost any
hardware platform and embedded within a host application like a game
engine. The biggest cost we pay for this ﬂexibility is execution speed—a
virtual machine usually executes its byte code instructions much more
slowly than the native CPU executes its machine language instructions.
•
Imperative languages. In an imperative language, a program is described
by a sequence of instructions, each of which performs an operation and/
or changes the state of data in memory. C and C++ are imperative lan-
guages.
•
Declarative languages.
A declarative language describes what is to be
done but does not specify exactly how the result should be obtained.
That decision is left up to the people implementing the language. Pro-
log is an example of a declarative language. Mark-up languages like
HTML and TeX can also be classiﬁed as declarative languages.
•
Functional languages. Functional languages, which are technically a sub-
set of declarative languages, aim to avoid state altogether. In a func-
tional language, programs are deﬁned by a collection of functions. Each
function produces its results with no side-effects (i.e., it causes no ob-
servable changes to the system, other than to produce its output data).
A program is constructed by passing input data from one function to the
next until the ﬁnal desired result has been generated. These languages
tend to be well-suited to implementing data processing pipelines. They
also offer distinct advantages when implementing multithreaded appli-
cations, because with no mutable state, a functional language requires

956
15. Runtime Gameplay Foundation Systems
no mutex locking. OCaml, Haskell and F# are examples of functional
languages.
•
Procedural versus object-oriented languages. In a procedural language, the
primary atom of program construction is the procedure (or function). These
procedures and functions perform operations, calculate results and/or
change the state of various data structures in memory.
In constrast,
an object-oriented language’s primary unit of program construction is
the class, a data structure that is tightly coupled with a set of proce-
dures/functions that “know” how to manage and manipulate the data
within that data structure.
•
Reﬂective languages. In a reﬂective language, information about the data
types, data member layouts, functions and hierarchical class relation-
ships in the system is available for inspection at runtime. In a non-
reﬂective language, the majority of this meta-information is known only
at compile time; only a very limited amount of it is exposed to the run-
time code. C# is an example of a reﬂective language, while C and C++
are examples of non-reﬂective languages.
15.8.2.1
Typical Characteristics of Game Scripting Languages
The characteristics that set a game scripting language apart from its native pro-
gramming language brethren include:
•
Interpreted. Most game scripting languages are interpreted by a virtual
machine, not compiled. This choice is made in the interest of ﬂexibility,
portability and rapid iteration (see below). When code is represented as
platform-independent byte code, it can easily be treated like data by the
engine. It can be loaded into memory just like any other asset rather than
requiring help from the operating system (as is necessary with a DLL on
a PC platform or a PRX on the PlayStation 3, for example). Because the
code is executed by a virtual machine rather than directly by the CPU,
the game engine is afforded a great deal of ﬂexibility regarding how and
when script code will be run.
•
Lightweight. Most game scripting languages have been designed for use
in an embedded system. As such, their virtual machines tend to be sim-
ple, and their memory footprints tend to be quite small.
•
Support for rapid iteration. Whenever native code is changed, the program
must be recompiled and relinked, and the game must be shut down and
rerun in order to see the effects of the changes (unless your develop-
ment environment supports some form of edit-and-continue). On the
other hand, when script code is changed, the effects of the changes can

15.8. Scripting
957
usually be seen very rapidly. Some game engines permit script code
to be reloaded on the ﬂy, without shutting down the game at all. Oth-
ers require the game to be shut down and rerun. But either way, the
turnaround time between making a change and seeing its effects in-
game is usually much faster than when making changes to the native
language source code.
•
Convenience and ease of use. Scripting languages are often customized to
suit the needs of a particular game. Features can be provided that make
common tasks simple, intuitive and less error-prone. For example, a
game scripting language might provide functions or custom syntax for
ﬁnding game objects by name, sending and handling events, pausing
or manipulating the passage of time, waiting for a speciﬁed amount of
time to pass, implementing ﬁnite state machines, exposing tweakable
parameters to the world editor for use by the game designers, or even
handling network replication for multiplayer games.
15.8.3
Some Common Game Scripting Languages
When implementing a runtime game scripting system, we have one funda-
mental choice to make: Do we select a third-party commercial or open source
language and customize it to suit our needs, or do we design and implement
a custom language from scratch?
Creating a custom language from scratch is usually not worth the hassle
and the cost of maintenance throughout the project. It can also be difﬁcult or
impossible to hire game designers and programmers who are already familiar
with a custom, in-house language, so there’s usually a training cost as well.
However, this is clearly the most ﬂexible and customizable approach, and that
ﬂexibility can be worth the investment.
For many studios, it is more convenient to select a reasonably well-known
and mature scripting language and extend it with features speciﬁc to your
game engine. There are a great many third-party scripting languages from
which to choose, and many are mature and robust, having been used in a
great many projects both within and outside the game industry.
In the following sections, we’ll explore a number of custom game script-
ing languages and a number of game-agnostic languages that are commonly
adapted for use in game engines.
15.8.3.1
QuakeC
Id Software’s John Carmack implemented a custom scripting language for
Quake, known as QuakeC (QC). This language was essentially a simpliﬁed

958
15. Runtime Gameplay Foundation Systems
variant of the C programming language with direct hooks into the Quake en-
gine. It had no support for pointers or deﬁning arbitrary structs, but it could
manipulate entities (Quake’s name for game objects) in a convenient manner,
and it could be used to send and receive/handle game events. QuakeC is an
interpreted, imperative, procedural programming language.
The power that QuakeC put into the hands of gamers is one of the fac-
tors that gave birth to what is now known as the mod community. Script-
ing languages and other forms of data-driven customization allow gamers to
turn many commercial games into all sorts of new gaming experiences—from
slight modiﬁcations on the original theme to entirely new games.
15.8.3.2
UnrealScript
Probably the best-known example of an entirely custom scripting language is
Unreal Engine’s UnrealScript. This language is based on a C++-like syntacti-
cal style, and it supports most of the concepts that C and C++ programmers
have become accustomed to, including classes, local variables, looping, arrays
and structs for data organization, strings, hashed string ids (called FName in
Unreal) and object references (but not free-form pointers). In addition, Un-
realScript provides a number of extremely powerful game-speciﬁc features,
which we’ll explore brieﬂy below. UnrealScript is an interpreted, imperative,
object-oriented language.
Ability to Extend the Class Hierarchy
This is perhaps UnrealScript’s biggest claim to fame. The Unreal object model
is essentially a monolithic class hierarchy, with add-on components provid-
ing interfaces to various engine systems. The root classes in the hierarchy are
known as native classes, because they are implemented in the native C++ lan-
guage. But UnrealScript’s real power comes from the fact that it can be used
to derive new classes that are implemented entirely in script.
This may not sound like a big deal until you try to imagine how you would
implement such a thing! In effect, UnrealScript redeﬁnes and extends C++’s
native object model, which is really quite astounding. For native Unreal classes,
the UnrealScript source ﬁles (normally named with the extension .uc) take the
place of C++’s header ﬁles (.h ﬁles) as the primary deﬁnition of each class—the
UnrealScript compiler actually generates the C++ .h ﬁles from the .uc ﬁles, and
the programmer implements the classes in regular .cpp source ﬁles. Doing this
allows the UnrealScript compiler to introduce additional features into every
Unreal class, and these features permit new script-only classes to be deﬁned
by users that inherit from native classes or other script-only classes.

15.8. Scripting
959
Latent Functions
Latent functions are functions whose execution may span multiple frames of
gameplay. A latent function can execute some instructions and then “go to
sleep” waiting for an event or for a speciﬁed amount of time to pass. When
the relevant event occurs or the time period elapses, the function is “woken
up” by the engine, and it continues executing where it left off. This feature
is highly useful for managing behaviors in the game that depend upon the
passage of time.
Convenient Linkage to UnrealEd
The data members of any UnrealScript-based class can be optionally marked
with a simple annotation, indicating that that data member is to be made
available for viewing and editing in Unreal’s world editor, UnrealEd. No GUI
programming is required. This makes data-driven game design extremely
easy (as long as UnrealEd’s built-in data member editing GUI suits your needs,
of course).
Network Replication for Multiplayer Games
Individual data elements in UnrealScript can be marked for replication. In
Unreal networked games, each game object exists in its full form on one par-
ticular machine; all the other machines have a lightweight version of the object
known as a remote proxy. When you mark a data member for replication, you
are telling the engine that you want that data to be replicated from the master
object to all of the remote proxies. This allows a programmer or designer to
easily control which data should be made available across the network. This
indirectly controls the amount of network bandwidth required by the game.
15.8.3.3
Lua
Lua is a well-known and popular scripting language that is easy to integrate
into an application such as a game engine. The Lua website (http://www.lua.
org/about.html) calls the language the “leading scripting language in games.”
According to the Lua website, Lua’s key beneﬁts are:
•
Robust and mature. Lua has been used on numerous commercial prod-
ucts, including Adobe’s Photoshop Lightroom, and many games, includ-
ing World of Warcraft.
•
Good documentation. Lua’s reference manual [21] is complete and un-
derstandable and is available online and in book formats. A number of
books have been written about Lua, including [22] and [45].

960
15. Runtime Gameplay Foundation Systems
•
Excellent runtime performance. Lua executes its byte code more quickly
and efﬁciently than many other scripting languages.
•
Portable. Out of the box, Lua runs on all ﬂavors of Windows and UNIX,
mobile devices and embedded microprocessors.
Lua is written in a
portable manner, making it easy to adapt to new hardware platforms.
•
Designed for embedded systems. Lua’s memory footprint is very small (ap-
proximately 350 KiB for the interpreter and all libraries).
•
Simple, powerful and extensible. The core Lua language is very small and
simple, but it is designed to support meta-mechanisms that extend its
core functionality in virtually limitless ways. For example, Lua itself
is not an object-oriented language, but OOP support can and has been
added via a meta-mechanism.
•
Free. Lua is open source and is distributed under the very liberal MIT
license.
Lua is a dynamically typed language, meaning that variables don’t have
types—only values do. (Every value carries its type information along with
it.) Lua’s primary data structure is the table, also known as an associative
array. A table is essentially a list of key-value pairs with an optimized ability
to index into the array by key.
Lua provides a convenient interface to the C language—the Lua virtual
machine can call and manipulate functions written in C as easily as it can
those written in Lua itself.
Lua treats blocks of code, called chunks, as ﬁrst-class objects that can be
manipulated by the Lua program itself. Code can be executed in source code
format or in precompiled byte code format. This allows the virtual machine to
execute a string that contains Lua code, just as if the code were compiled into
the original program. Lua also supports some powerful advanced program-
ming constructs, including coroutines. This is a simple form of cooperative
multitasking, in which each thread must yield the CPU to other threads ex-
plicitly (rather than being time-sliced as in a preemptive multithreading sys-
tem).
Lua does have some pitfalls. For example, its ﬂexible function binding
mechanism makes it possible (and quite easy) to redeﬁne an important global
function like sin() to perform a totally different task (which is usually not
something one intends to do). But all in all, Lua has proven itself to be an
excellent choice for use as a game scripting language.

15.8. Scripting
961
15.8.3.4
Python
Python is a procedural, object-oriented, dynamically typed scripting language
designed with ease of use, integration with other programming languages,
and ﬂexibility in mind. Like Lua, Python is a common choice for use as a
game scripting language. According to the ofﬁcial Python website (http://
www.python.org), some of Python’s best features include:
•
Clear and readable syntax. Python code is easy to read, in part because
the syntax enforces a speciﬁc indentation style. (It actually parses the
whitespace used for indentation in order to determine the scope of each
line of code.)
•
Reﬂective language. Python includes powerful runtime introspection fa-
cilities. Classes in Python are ﬁrst-class objects, meaning they can be
manipulated and queried at runtime, just like any other object.
•
Object-oriented. One advantage of Python over Lua is that OOP is built
into the core language. This makes integrating Python with a game’s
object model a little easier.
•
Modular. Python supports hierarchical packages, encouraging clean sys-
tem design and good encapsulation.
•
Exception-based error handling. Exceptions make error-handling code in
Python simpler, more elegant and more localized than similar code in a
non-exception-based language.
•
Extensive standard libraries and third-party modules. Python libraries exist
for virtually every task imaginable. (Really!)
•
Embeddable. Python can be easily embedded into an application, such as
a game engine.
•
Extensive documentation. There’s plenty of documentation and tutorials
on Python, both online and in book form. A good place to start is the
Python website, http://www.python.org.
Python syntax is reminiscent of C in many respects (for example, its use
of the = operator for assignment and == for equality testing). However, in
Python, code indentation serves as the only means of deﬁning scope (as opposed
to C’s opening and closing braces). Python’s primary data structures are the
list—a linearly indexed sequence of atomic values or other nested lists—and
the dictionary—a table of key-value pairs. Each of these two data structures
can hold instances of the other, allowing arbitrarily complex data structures to
be constructed easily. In addition, classes—uniﬁed collections of data elements
and functions—are built right into the language.

962
15. Runtime Gameplay Foundation Systems
Python supports duck typing, which is a style of dynamic typing in which
the functional interface of an object determines its type (rather than being de-
ﬁned by a static inheritance hierarchy). In other words, any class that sup-
ports a particular interface (i.e., a collection of functions with speciﬁc signa-
tures) can be used interchangeably with any other class that supports that
same interface. This is a powerful paradigm: In effect, Python supports poly-
morphism without requiring the use of inheritance. Duck typing is similar
in some respects to C++ template metaprogramming, although it is arguably
more ﬂexible because the bindings between caller and callee are formed dy-
namically, at runtime. Duck typing gets its name from the well-known phrase
(attributed to James Whitcomb Riley), “If it walks like a duck and quacks like a
duck, I would call it a duck.” See http://en.wikipedia.org/wiki/Duck_typing
for more information on duck typing.
In summary, Python is easy to use and learn, embeds easily into a game
engine, integrates well with a game’s object model, and can be an excellent
and powerful choice as a game scripting language.
15.8.3.5
Pawn/Small/Small-C
Pawn is a lightweight, dynamically typed, C-like scripting language created
by Marc Peter. The language was formerly known as Small, which itself was
an evolution of an earlier subset of the C language called Small-C, written by
Ron Cain and James Hendrix. It is an interpreted language—the source code
is compiled into byte code (also known as P-code), which is interpreted by a
virtual machine at runtime.
Pawn was designed to have a small memory footprint and to execute its
byte code very quickly. Unlike C, Pawn’s variables are dynamically typed.
Pawn also supports ﬁnite state machines, including state-local variables. This
unique feature makes it a good ﬁt for many game applications. Good online
documentation is available for Pawn (http://www.compuphase.com/pawn/
pawn.htm). Pawn is open source and can be used free of charge under the
Zlib/libpng license (http://www.opensource.org/licenses/zlib-license.php).
Pawn’s C-like syntax makes it easy to learn for any C/C++ programmer
and easy to integrate with a game engine written in C. Its ﬁnite state machine
support can be very useful for game programming. It has been used success-
fully on a number of game projects, including Freaky Flyers by Midway. Pawn
has shown itself to be a viable game scripting language.
15.8.4
Architectures for Scripting
Script code can play all sorts of roles within a game engine. There’s a gamut
of possible architectures, from tiny snippets of script code that perform sim-

15.8. Scripting
963
ple functions on behalf of an object or engine system to high-level scripts that
manage the operation of the game. Here are just a few of the possible archi-
tectures:
•
Scripted callbacks. In this approach, the engine’s functionality is largely
hard-coded in the native programming language, but certain key bits
of functionality are designed to be customizable. This is often imple-
mented via a hook function or callback—a user-supplied function that is
called by the engine for the purpose of allowing customization. Hook
functions can be written in the native language, of course, but they can
also be written in a scripting language. For example, when updating
game objects during the game loop, the engine might call an optional
callback function that can be written in script. This gives users the op-
portunity to customize the way in which the game object updates itself
over time.
•
Scripted event handler. An event handler is really just a special type of
hook function whose purpose is to allow a game object to respond to
some relevant occurrence within the game world (e.g., responding to
an explosion going off) or within the engine itself (e.g., responding to
an out-of-memory condition). Many game engines allow users to write
event handler hooks in script as well as in the native language.
•
Extending game object types, or deﬁning new ones, with script. Some script-
ing languages allow game object types that have been implemented in
the native language to be extended via script. In fact, callbacks and event
handlers are examples of this on a small scale, but the idea can be ex-
tended even to the point of allowing entirely new types of game objects
to be deﬁned in script. This might be done via inheritance (i.e., deriving
a class written in script from a class written in the native language) or
via composition (i.e., attaching an instance of a scripted class to a native
game object).
•
Scripted components or properties.
In a component- or property-based
game object model, it only makes sense to permit new components or
property objects to be constructed partially or entirely in script. This
approach was used by Gas Powered Games for Dungeon Siege.
The
game object model was property-based, and it was possible to imple-
ment properties in either C++ or Gas Powered Games’ custom scripting
language, Skrit (http://ds.heavengames.com/library/dstk/skrit/skrit).
By the end of the project, they had approximately 148 scripted property
types and 21 native C++ property types.

964
15. Runtime Gameplay Foundation Systems
•
Script-driven engine. Script might be used to drive an entire engine sys-
tem. For example, the game object model could conceivably be written
entirely in script, calling into the native engine code only when it re-
quires the services of lower-level engine components.
•
Script-driven game. Some game engines actually ﬂip the relationship be-
tween the native language and the scripting language on its head. In
these engines, the script code runs the whole show, and the native en-
gine code acts merely as a library that is called to access certain high-
speed features of the engine.
The Panda3D engine (http://www.
panda3d.org) is an example of this kind of architecture. Panda3D games
can be written entirely in the Python language, and the native engine
(implemented in C++) acts like a library that is called by script code.
(Panda3D games can also be written entirely in C++.)
15.8.5
Features of a Runtime Game Scripting Language
The primary purpose of many game scripting languages is to implement game-
play features, and this is often accomplished by augmenting and customizing
a game’s object model. In this section, we’ll explore some of the most common
requirements and features of such a scripting system.
15.8.5.1
Interface with the Native Programming Language
In order for a scripting language to be useful, it must not operate in a vacuum.
It’s imperative for the game engine to be able to execute script code, and it’s
usually equally important for script code to be capable of initiating operations
within the engine as well.
A runtime scripting language’s virtual machine (VM) is generally embed-
ded within the game engine. The engine initializes the virtual machine, runs
script code whenever required, and manages those scripts’ execution. The
unit of execution varies depending on the speciﬁcs of the language and the
game’s implementation.
•
In a functional scripting language, the function is often the primary unit
of execution. In order for the engine to call a script function, it must look
up the byte code corresponding to the name of the desired function and
spawn a virtual machine to execute it (or instruct an existing VM to do
so).
•
In an object-oriented scripting language, classes are typically the primary
unit of execution. In such a system, objects can be spawned and de-
stroyed, and methods (member functions) can be invoked on individual
class instances.

15.8. Scripting
965
It’s usually beneﬁcial to allow two-way communication between script
and native code. Therefore, most scripting languages allowing native code to
be invoked from script as well. The details are language- and implementation-
speciﬁc, but the basic approach is usually to allow certain script functions to
be implemented in the native language rather than in the scripting language.
To call an engine function, script code simply makes an ordinary function
call. The virtual machine detects that the function has a native implementa-
tion, looks up the corresponding native function’s address (perhaps by name
or via some other kind of unique function identiﬁer), and calls it. For exam-
ple, some or all of the member functions of a Python class or module can be
implemented using C functions. Python maintains a data structure, known as
a method table, that maps the name of each Python function (represented as a
string) to the address of the C function that implements it.
Case Study: Naughty Dog’s DC Language
As an example, let’s have a brief look at how Naughty Dog’s runtime scripting
language, a language called DC, was integrated into the engine.
DC is a variant of the Scheme language (which is itself a variant of Lisp lan-
guage). Chunks of executable code in DC are known as script lambdas, which
are the approximate equivalent of functions or code blocks in the Lisp family
of languages. A DC programmer writes script lambdas and identiﬁes them by
giving them globally unique names. The DC compiler converts these script
lambdas into chunks of byte code, which are loaded into memory when the
game runs and can be looked up by name using a simple functional interface
in C++.
Once the engine has a pointer to a chunk of script lambda byte code, it
can execute the code by calling a “virtual machine execution” function in the
engine and passing the byte code pointer to it. The function itself is surpris-
ingly simple. It spins in a loop, reading byte code instructions one-by-one,
and executing each instruction. When all instructions have been executed, the
function returns.
The virtual machine contains a bank of registers, which can hold any kind
of data the script may want to deal with. This is implemented using a variant
data type—a union of all the data types (see 15.7.4 for a discussion of variants).
Some instructions cause data to be loaded into a register; others cause the
data held in a register to be looked up and used. There are instructions for
performing all of the mathematical operations available in the language, as
well as instructions for performing conditional checks—implementations of
DC’s (if ...), (when ...) and (cond ...) instructions and so on.

966
15. Runtime Gameplay Foundation Systems
The virtual machine also supports a function call stack. Script lambdas in
DC can call other script lambdas (i.e., functions) that have been deﬁned by a
script programmer via DC’s (defun ...) syntax. Just like any procedural
programming language, a stack is needed to keep track of the states of the
registers and the return address when one function calls another. In the DC
virtual machine, the call stack is literally a stack of register banks—each new
function gets its own private bank of registers. This prevents us from having
to save off the state of the registers, call the function, and then restore the reg-
isters when the called function returns. When the virtual machine encounters
a byte code instruction that tells it to call another script lambda, the byte code
for that script lambda is looked up by name, a new stack frame is pushed, and
execution continues at the ﬁrst instruction of that script lambda. When the vir-
tual machine encounters a return instruction, the stack frame is popped from
the stack, along with the return “address” (which is really just the index of the
byte code instruction in the calling script lambda after the one that called the
function in the ﬁrst place).
The following pseudocode should give you a feel for what the core in-
struction-processing loop of the DC virtual machine looks like:
void DcExecuteScript(DCByteCode* pCode)
{
DCStackFrame* pCurStackFrame
= DcPushStackFrame(pCode);
// Keep going until we run out of stack frames (i.e.,
// the top-level script lambda "function" returns).
while (pCurStackFrame != NULL)
{
// Get the next instruction. We will never run
// out, because the return instruction is always
// last, and it will pop the current stack frame
// below.
DCInstruction& instr
= pCurStackFrame->GetNextInstruction();
// Perform the operation of the instruction.
switch (instr.GetOperation())
{
case DC_LOAD_REGISTER_IMMEDIATE:
{
// Grab the immediate value to be loaded
// from the instruction.
Variant& data = instr.GetImmediateValue();

15.8. Scripting
967
// Also determine into which register to
// put it.
U32 iReg = instr.GetDestRegisterIndex();
// Grab the register from the stack frame.
Variant& reg
= pCurStackFrame->GetRegister(iReg);
// Store the immediate data into the
// register.
reg = data;
}
break;
// Other load and store register operations...
case DC_ADD_REGISTERS:
{
// Determine the two registers to add. The
// result will be stored in register A.
U32 iRegA = instr.GetDestRegisterIndex();
U32 iRegB = instr.GetSrcRegisterIndex();
// Grab the 2 register variants from the
// stack.
Variant& dataA
= pCurStackFrame->GetRegister(iRegA);
Variant& dataB
= pCurStackFrame->GetRegister(iRegB);
// Add the registers and store in
// register A.
dataA = dataA + dataB;
}
break;
// Other math operations...
case DC_CALL_SCRIPT_LAMBDA:
{
// Determine in which register the name of
// the script lambda to call is stored.
// (Presumably it was loaded by a previous
// load instr.)
U32 iReg = instr.GetSrcRegisterIndex();

968
15. Runtime Gameplay Foundation Systems
// Grab the appropriate register, which
// contains the name of the lambda to call.
Variant& lambda
= pCurStackFrame->GetRegister(iReg);
// Look up the byte code of the lambda by
// name.
DCByteCode* pCalledCode
= DcLookUpByteCode(lambda.AsStringId());
// Now "call" the lambda by pushing a new
// stack frame.
if (pCalledCode)
{
pCurStackFrame
= DcPushStackFrame(pCalledCode);
}
}
break;
case DC_RETURN:
{
// Just pop the stack frame. If we're in
// the top lambda on the stack, this
// function will return NULL, and the loop
// will terminate.
pCurStackFrame = DcPopStackFrame();
}
break;
// Other instructions...
// ...
} // end switch
} // end while
}
In the above example, we assume that the global functions DcPushStack
Frame() and DcPopStackFrame() manage the stack of register banks for
us in some suitable way and that the global function DcLookUpByteCode()
is capable of looking up any script lambda by name. We won’t show imple-
mentations of those functions here, because the purpose of this example is
simply to show how the inner loop of a script virtual machine might work,
not to provide a complete functional implementation.
DC script lambdas can also call native functions—i.e., global functions
written in C++ that serve as hooks into the engine itself. When the virtual

15.8. Scripting
969
machine comes across an instruction that calls a native function, the address
of the C++ function is looked up by name using a global table that has been
hard-coded by the engine programmers. If a suitable C++ function is found,
the arguments to the function are taken from registers in the current stack
frame, and the function is called. This implies that the C++ function’s argu-
ments are always of type Variant. If the C++ function returns a value, it too
must be a Variant, and its value will be stored into a register in the current
stack frame for possible use by subsequent instructions.
The global function table might look something like this:
typedef Variant DcNativeFunction(U32 argCount,
Variant* aArgs);
struct DcNativeFunctionEntry
{
StringId
m_name;
DcNativeFunction*
m_pFunc;
};
DcNativeFunctionEntry g_aNativeFunctionLookupTable[] =
{
{ SID('get-object-pos'), DcGetObjectPos },
{ SID('animate-object'), DcAnimateObject },
// etc.
};
A native DC function implementation might look something like the fol-
lowing. Notice how the Variant arguments are passed to the function as
an array. The function must verify that the number of arguments passed to it
equals the number of arguments it expects. It must also verify that the types
of the argument(s) are as expected and be prepared to handle errors that the
DC script programmer may have made when calling the function.
Variant DcGetObjectPos(U32 argCount, Variant* aArgs)
{
// Set up a default return value.
Variant result;
result.SetAsVector(Vector(0.0f, 0.0f, 0.0f));
if (argCount != 1)
{
DcErrorMessage("get-object-pos: "
"Invalid arg count.\n");
return result;
}

970
15. Runtime Gameplay Foundation Systems
if (aArgs[0].GetType() != Variant::TYPE_STRING_ID)
{
DcErrorMessage("get-object-pos: "
"Expected string id.\n");
return result;
}
StringId objectName = aArgs[0].AsStringId();
GameObject* pObject
= GameObject::LookUpByName(objectName);
if (pObject == NULL)
{
DcErrorMessage("get-object-pos: "
"Object '%s' not found.\n",
objectName.ToDebugString());
return result;
}
result.SetAsVector(pObject->GetPosition());
return result;
}
Note that the function StringId::ToDebugString() performs a re-
verse look-up to convert a string id back to its original string. This requires
the game engine to maintain some kind of database mapping each string id to
its original string. During development such a database can make life much
easier, but because it consumes a lot of memory, the database should be omit-
ted from the ﬁnal shipped product. (The function name ToDebugString()
reminds us that the reverse conversion from string id back to string should
only be performed for debugging purposes—the game itself must never rely
on this functionality!)
15.8.5.2
Game Object References
Script functions often need to interact with game objects, which themselves
may be implemented partially or entirely in the engine’s native language. The
native language’s mechanisms for referencing objects (e.g., pointers or refer-
ences in C++) won’t necessarily be valid in the scripting language. (It may
not support pointers at all, for example.) Therefore, we need to come up with
some reliable way for script code to reference game objects.
There are a number of ways to accomplish this. One approach is to refer
to objects in script via opaque numeric handles. The script code can obtain
object handles in various ways. It might be passed a handle by the engine,

15.8. Scripting
971
or it might perform some kind of query, such as asking for the handles of all
game objects within a radius of the player or looking up the handle that cor-
responds to a particular object name. The script can then perform operations
on the game object by calling native functions and passing the object’s handle
as an argument. On the native language side, the handle is converted back
into a pointer to the native object, and then the object can be manipulated as
appropriate.
Numeric handles have the beneﬁt of simplicity and should be easy to sup-
port in any scripting language that supports integer data. However, they can
be unintuitive and difﬁcult to work with. Another alternative is to use the
names of the objects, represented as strings, as our handles. This has some
interesting beneﬁts over the numeric handle technique. For one thing, strings
are human-readable and intuitive to work with. There is a direct correspon-
dence to the names of the objects in the game’s world editor. In addition,
we can choose to reserve certain special object names and give them “magic”
meanings. For example, in Naughty Dog’s scripting language, the reserved
name “self” always refers to the object to which the currently running script
is attached. This allows game designers to write a script, attach it to an object
in the game, and then use the script to play an animation on the object by
simply writing (animate 'self name-of-animation).
Using strings as object handles has its pitfalls, of course. Strings typically
occupy more memory than integer ids. And because strings vary in length,
dynamic memory allocation is required in order to copy them. String compar-
isons are slow. Script programmers are apt to make mistakes when typing the
names of game objects, which can lead to bugs. In addition, script code can
be broken if someone changes the name of an object in the game world editor
but forgets to update the name of the object in script.
Hashed string ids overcome most of these problems by converting any
strings (regardless of length) into an integer. In theory, hashed string ids enjoy
the best of both worlds—they can be read by users just like strings, but they
have the runtime performance characteristics of an integer. However, for this
to work, your scripting language needs to support hashed string ids in some
way. Ideally, we’d like the script compiler to convert our strings into hashed
ids for us. That way, the runtime code doesn’t have to deal with the strings at
all, only the hashed ids (except possibly for debugging purposes—it’s nice to
be able to see the string corresponding to a hashed id in the debugger). How-
ever, this isn’t always possible in all scripting languages. Another approach is
to allow the user to use strings in script and convert them into hashed ids at
runtime, whenever a native function is called.
Naughty Dog’s DC scripting language leverages the concept of “symbols,”
which are native to the Scheme programming language, to encode its string

972
15. Runtime Gameplay Foundation Systems
ids. Writing 'foo—or more verbosely, (quote foo)—in DC/Scheme cor-
responds to the string id SID('foo') in C++.
15.8.5.3
Receiving and Handling Events in Script
Events are a ubiquitous communication mechanism in most game engines.
By permitting event handler functions to be written in script, we open up a
powerful avenue for customizing the hard-coded behavior of our game.
Events are usually sent to individual objects and handled within the con-
text of that object. Hence, scripted event handlers need to be associated with
an object in some way. Some engines use the game object type system for this
purpose—scripted event handlers can be registered on a per-object-type ba-
sis. This allows different types of game objects to respond in different ways to
the same event but ensures that all instances of each type respond in a consis-
tent and uniform way. The event handler functions themselves can be simple
script functions, or they can be members of a class if the scripting language is
object-oriented. In either case, the event handler is typically passed a handle
to the particular object to which the event was sent, much as C++ member
functions are passed the this pointer.
In other engines, scripted event handlers are associated with individual
object instances rather than with object types. In this approach, different in-
stances of the same type might respond differently to the same event.
There are all sorts of other possibilities, of course. For example, in Naughty
Dog’s engine (used to create the Uncharted series and The Last of Us), scripts
are objects in their own right. They can be associated with individual game
objects, they can be attached to regions (convex volumes that are used to trig-
ger game events), or they can exist as stand-alone objects in the game world.
Each script can have multiple states (that is, scripts are ﬁnite state machines in
the Naughty Dog engine). In turn, each state can have one or more event han-
dler code blocks. When a game object receives an event, it has the option of
handling the event in native C++. It also checks for an attached script object,
and if one is found, the event is sent to that script’s current state. If the state
has an event handler for the event, it is called. Otherwise, the script simply
ignores the event.
15.8.5.4
Sending Events
Allowing scripts to handle game events that are generated by the engine is
certainly a powerful feature. Even more powerful is the ability to generate
and send events from script code either back to the engine or to other scripts.
Ideally, we’d like to be able not only to send predeﬁned types of events
from script but to deﬁne entirely new event types in script. Implementing

15.8. Scripting
973
this is trivial if event types are strings or string ids. To deﬁne a new event
type, the script programmer simply comes up with a new event type name
and types it into his or her script code. This can be a highly ﬂexible way for
scripts to communicate with one another. Script A can deﬁne a new event
type and send it to Script B. If Script B deﬁnes an event handler for this type
of event, we’ve implemented a simple way for Script A to “talk” to Script B. In
some game engines, event- or message-passing is the only supported means
of inter-object communication in script. This can be an elegant yet powerful
and ﬂexible solution.
15.8.5.5
Object-Oriented Scripting Languages
Some scripting languages are inherently object-oriented. Others do not sup-
port objects directly but provide mechanisms that can be used to implement
classes and objects. In many engines, gameplay is implemented via an object-
oriented game object model of some kind. So it makes sense to permit some
form of object-oriented programming in script as well.
Deﬁning Classes in Scripts
A class is really just a bunch of data with some associated functions. So any
scripting language that permits new data structures to be deﬁned, and pro-
vides some way to store and manipulate functions, can be used to implement
classes. For example, in Lua, a class can be built out of a table that stores data
members and member functions.
Inheritance in Script
Object-oriented languages do not necessarily support inheritance. However,
if this feature is available, it can be extremely useful, just as it is in native
programming languages like C++.
In the context of game scripting languages, there are two kinds of in-
heritance: deriving scripted classes from other scripted classes and deriv-
ing scripted classes from native classes. If your scripting language is object-
oriented, chances are the former is supported out of the box. However, the
latter is tough to implement even if the scripting language supports inheri-
tance. The problem is bridging the gap between two languages and two low-
level object models. We won’t get into the details of how this might be im-
plemented here, as the implementation is bound to be speciﬁc to the pair of
languages being integrated. UnrealScript is the only scripting language I’ve
seen that allows scripted classes to derive from native classes in a seamless
way.

974
15. Runtime Gameplay Foundation Systems
Composition/Aggregation in Script
We don’t need to rely on inheritance to extend a hierarchy of classes—we can
also use composition or aggregation to similar effect. In script, then, all we
really need is a way to deﬁne classes and associate instances of those classes
with objects that have been deﬁned in the native programming language. For
example, a game object could hold a pointer or reference to an optional com-
ponent written entirely in script. We can delegate certain key functionality
to the script component, if it exists. The script component might have an
Update() function that is called whenever the game object is updated, and
the scripted component might also be permitted to register some of its mem-
ber functions/methods as event handlers. When an event is sent to the game
object, it calls the appropriate event handler on the scripted component, thus
giving the script programmer an opportunity to modify or extend the behav-
ior of the natively implemented game object.
15.8.5.6
Scripted Finite State Machines
Many problems in game programming can be solved naturally using ﬁnite
state machines (FSMs). For this reason, some engines build the concept of
FSMs right into the core game object model. In such engines, every game
object can have one or more states, and it is the states—not the game ob-
ject itself—that contain the update function, event handler functions and so
on. Simple game objects can be created by deﬁning a single state, but more-
complex game objects have the freedom to deﬁne multiple states, each with a
different update and event-handling behavior.
If your engine supports a state-driven game object model, it makes a lot of
sense to provide ﬁnite state machine support in the scripting language as well.
And of course, even if the core game object model doesn’t support ﬁnite state
machines natively, one can still provide state-driven behavior by using a state
machine on the script side. An FSM can be implemented in any programming
language by using class instances to represent states, but some scripting lan-
guages provide tools especially for this purpose. An object-oriented scripting
language might provide custom syntax that allows a class to contain multiple
states, or it might provide tools that help the script programmer easily aggre-
gate state objects together within a central hub object and then delegate the
update and event-handling functions to it in a straightforward way. But even
if your scripting language provides no such features, you can always adopt a
methodology for implementing FSMs and follow those conventions in every
script you write.

15.8. Scripting
975
15.8.5.7
Multithreaded Scripts
It’s often useful to be able to execute multiple scripts in parallel. This is espe-
cially true on today’s highly parallelized hardware architectures. If multiple
scripts can run at the same time, we are in effect providing parallel threads of
execution in script code, much like the threads provided by most multitasking
operating systems. Of course, the scripts may not actually run in parallel—if
they are all running on a single CPU, the CPU must take turns executing each
one. However, from the point of view of the script programmer, the paradigm
is one of parallel multithreading.
Most scripting systems that provide parallelism do so via cooperative multi-
tasking. This means that a script will execute until it explicitly yields to another
script. This is in contrast with a preemptive multitasking approach, in which the
execution of any script could be interrupted at any time to permit another
script to execute.
One simple approach to cooperative multitasking in script is to permit
scripts to explicitly go to sleep, waiting for something relevant to happen.
A script might wait for a speciﬁed number of seconds to elapse, or it might
wait until a particular event is received. It might wait until another thread
of execution has reached a predeﬁned synchronization point. Whatever the
reason, whenever a script goes to sleep, it puts itself on a list of sleeping script
threads and tells the virtual machine that it can start executing another eligible
script. The system keeps track of the conditions that will wake up each sleep-
ing script—when one of these conditions becomes true, the script or scripts
waiting on the condition are woken up and allowed to continue executing.
To see how this works in practice, let’s look at an example of a multi-
threaded script. This script manages the animations of two characters and
a door. The two characters are instructed to walk up to the door—each one
might take a different, and unpredictable, amount of time to reach it. We’ll
put the script’s threads to sleep while they wait for the characters to reach the
door. Once they both arrive at the door, one of the two characters opens the
door, which it does by playing an “open door” animation. Note that we don’t
want to hard-code the duration of the animation into the script itself. That
way, if the animators change the animation, we won’t have to go back and
modify our script. So we’ll put the threads to sleep again while the wait for
the animation to complete. A script that accomplishes this is shown below,
using a simple C-like pseudocode syntax.
function DoorCinematic
{
thread Guy1
{

976
15. Runtime Gameplay Foundation Systems
// Ask guy1 to walk to the door.
CharacterWalkToPoint(guy1, doorPosition);
// Go to sleep until he gets there.
WaitUntil(CHARACTER_ARRIVAL);
// OK, we're there. Tell the other threads
// via a signal.
RaiseSignal("Guy1Arrived");
// Wait for the other guy to arrive as well.
WaitUntil(SIGNAL, "Guy2Arrived");
// Now tell guy1 to play the "open door"
// animation.
CharacterAnimate(guy1, "OpenDoor");
WaitUntil(ANIMATION_DONE);
// OK, the door is open. Tell the other threads.
RaiseSignal("DoorOpen");
// Now walk thru the door.
CharacterWalkToPoint(guy1, beyondDoorPosition);
}
thread Guy2
{
// Ask guy2 to walk to the door.
CharacterWalkToPoint(guy2, doorPosition);
// Go to sleep until he gets there.
WaitUntil(CHARACTER_ARRIVAL);
// OK, we're there. Tell the other threads
// via a signal.
RaiseSignal("Guy2Arrived");
// Wait for the other guy to arrive as well.
WaitUntil(SIGNAL, "Guy1Arrived");
// Now wait until guy1 opens the door for me.
WaitUntil(SIGNAL, "DoorOpen");
// OK, the door is open. Now walk thru the door.
CharacterWalkToPoint(guy2, beyondDoorPosition);
}
}

15.8. Scripting
977
In the above, we assume that our hypothetical scripting language provides
a simple syntax for deﬁning threads of execution within a single function. We
deﬁne two threads, one for Guy1 and one for Guy2.
The thread for Guy1 tells the character to walk to the door and then goes to
sleep waiting for his arrival. We’re hand-waving a bit here, but let’s imagine
that the scripting language magically allows a thread to go to sleep, waiting
until a character in the game arrives at a target point to which he was re-
quested to walk. In reality, this might be implemented by arranging for the
character to send an event back to the script and then waking the thread up
when the event arrives.
Once Guy1 arrives at the door, his thread does two things that warrant
further explanation. First, it raises a signal called “Guy1Arrived.” Second, it
goes to sleep waiting for another signal called “Guy2Arrived.” If we look at
the thread for Guy2, we see a similar pattern, only reversed. The purpose of
this pattern of raising a signal and then waiting for another signal is used to
synchronize the two threads.
In our hypothetical scripting language, a signal is just a Boolean ﬂag with a
name. The ﬂag starts out false, but when a thread calls RaiseSignal(name),
the named ﬂag’s value changes to true. Other threads can go to sleep, wait-
ing for a particular named signal to become true. When it does, the sleeping
thread(s) wake up and continue executing. In this example, the two threads
are using the “Guy1Arrived” and “Guy2Arrived” signals to synchronize with
one another. Each thread raises its signal and then waits for the other thread’s
signal. It does not matter which signal is raised ﬁrst—only when both signals
have been raised will the two threads wake up. And when they do, they will
be in perfect synchronization. Two possible scenarios are illustrated in Fig-
ure 15.20, one in which Guy1 arrives ﬁrst, the other in which Guy2 arrives
Walk
Signal
Wait
Walk
Signal
(No Wait)
Guy1
Guy2
Sync
Walk
Signal
Wait
Walk
Signal
(No Wait)
Guy1
Guy2
Sync
Figure 15.20. Two examples showing how a simple pattern of raising one signal and then waiting
on another can be used to synchronize a pair of script threads.

978
15. Runtime Gameplay Foundation Systems
ﬁrst. As you can see, the order in which the signals are raised is irrelevant,
and the threads always end up in sync after both signals have been raised.
15.9
High-Level Game Flow
A game object model provides the foundations upon which a rich and en-
tertaining collection of game object types can be implemented with which to
populate our game worlds. However, by itself, a game object model only per-
mits us to deﬁne the kinds of objects that exist in our game world and how
they behave individually. It says nothing of the player’s objectives, what hap-
pens if he or she completes them, and what fate should befall the player if he
or she fails.
For this, we need some kind of system to control high-level game ﬂow.
This is often implemented as a ﬁnite state machine. Each state usually repre-
sents a single player objective or encounter and is associated with a particular
locale within the virtual game world. As the player completes each task, the
state machine advances to the next state, and the player is presented with a
new set of goals. The state machine also deﬁnes what should happen in the
event of the player’s failure to accomplish the necessary tasks or objectives.
Often, failure sends the player back to the beginning of the current state, so he
or she can try again. Sometimes after enough failures, the player has run out
of “lives” and will be sent back to the main menu, where he or she can choose
to play a new game. The ﬂow of the entire game, from the menus to the ﬁrst
“level” to the last, can be controlled through this high-level state machine.
The task system used in Naughty Dog’s Jak and Daxter and Uncharted fran-
chises is an example of such a state-machine-based system. It allows for linear
sequences of states (called tasks at Naughty Dog). It also permits parallel tasks,
where one task branches out into two or more parallel tasks, which eventu-
ally merge back into the main task sequence. This parallel task feature sets
the Naughty Dog task graph apart from a regular state machine, since state
machines typically can only be in one state at a time.

Part V
Conclusion

This page intentionally left blank
This page intentionally left blank

16
You Mean There’s More?
C
ongratulations!
You’ve reached the end of your journey through the
landscape of game engine architecture in one piece (and hopefully none
the worse for wear). With any luck, you’ve learned a great deal about the
major components that comprise a typical game engine. But of course, every
journey’s end is another’s beginning. There’s a great deal more to be learned
about each and every topic covered within these pages. As technology and
computing hardware continue to improve, more things will become possible
in games—and more engine systems will be invented to support them. What’s
more, this book’s focus was on the game engine itself. We haven’t even begun
to discuss the rich world of gameplay programming, a topic that could ﬁll
many more volumes.
In the following brief sections, I’ll identify a few of the engine and game-
play systems we didn’t have room to cover in any depth in this book, and I’ll
suggest some resources for those who wish to learn more about them.
16.1
Some Engine Systems We Didn’t Cover
16.1.1
Movie Player
Most games include a movie player for displaying prerendered movies, also
known as full-motion video (FMV). The basic components of the movie player
981

982
16. You Mean There’s More?
are an interface to the streaming ﬁle I/O system (see Section 6.1.3), a codec to
decode the compressed video stream, and some form of synchronization with
the audio playback system for the sound track.
A number of different video encoding standards and corresponding codecs
are available, each one suited to a particular type of application. For example,
video CDs (VCD) and DVDs use MPEG-1 and MPEG-2 (H.262) codecs, re-
spectively. The H.261 and H.263 standards are designed primarily for online
video conferencing applications. Games often use standards like MPEG-4 part
2 (e.g., DivX), MPEG-4 Part 10 / H.264, Windows Media Video (WMV) or Bink
Video (a standard designed speciﬁcally for games by Rad Game Tools, Inc.).
See http://en.wikipedia.org/wiki/Video_codec and http://www.radgame
tools.com/bnkmain.htm for more information on video codecs.
16.1.2
Multiplayer Networking
Although we have touched on a number of aspects of multiplayer game ar-
chitecture and networking (e.g., Sections 1.6.14, 7.7 and 15.8.3.2), this book’s
coverage of the topic is far from complete. For an in-depth treatment of mul-
tiplayer networking, see [3].
16.2
Gameplay Systems
A game is of course much more than just its engine. On top of the game-
play foundation layer (discussed in Chapter 15), you’ll ﬁnd a rich assortment
of genre- and game-speciﬁc gameplay systems. These systems tie the myr-
iad game engine technologies described in this book together into a cohesive
whole, breathing life into the game.
16.2.1
Player Mechanics
Player mechanics are of course the most important gameplay system. Each
genre is deﬁned by a general style of player mechanics and gameplay, and
of course every game within a genre has its own speciﬁc designs. As such,
player mechanics is a huge topic. It involves the integration of human in-
terface device systems, motion simulation, collision detection, animation and
audio, not to mention integration with other gameplay systems like the game
camera, weapons, cover, specialized traversal mechanics (ladders, swinging
ropes, etc.), vehicle systems, puzzle mechanics and so on.
Clearly player mechanics are as varied as the games themselves, so there’s
no one place you can go to learn all about them. It’s best to tackle this topic
by studying a single genre at a time. Play games and try to reverse-engineer

16.2. Gameplay Systems
983
their player mechanics. Then try to implement them yourself! And as a very
modest start on reading, you can check out [7, Section 4.11] for a discussion of
Mario-style platformer player mechanics.
16.2.2
Cameras
A game’s camera system is almost as important as the player mechanics. In
fact, the camera can make or break the gameplay experience. Each genre tends
to have its own camera control style, although of course every game within
a particular genre does it a little bit differently (and some very differently).
See [6, Section 4.3] for some basic game camera control techniques. In the
following paragraphs, I’ll brieﬂy outline some of the most prevalent kinds of
cameras in 3D games, but please note that this is far from a complete list.
•
Look-at cameras. This type of camera rotates about a target point and can
be moved in and out relative to this point.
•
Follow cameras. This type of camera is prevalent in platformer, third-
person shooter and vehicle-based games. It acts much like a look-at
camera focused on the player character/avatar/vehicle, but its motion
typically lags the player. A follow camera also includes advanced col-
lision detection and avoidance logic and provides the human player
with some degree of control over the camera’s orientation relative to
the player avatar.
•
First-person cameras. As the player character moves about in the game
world, a ﬁrst-person camera remains afﬁxed to the character’s virtual
eyes. The player typically has full control over the direction in which the
camera should be pointed, either via mouse or joypad control. The look
direction of the camera also translates directly into the aim direction of
the player’s weapon, which is typically indicated by a set of disembod-
ied arms and a gun attached to the bottom of the screen, and a reticle at
the center of the screen.
•
RTS cameras. Real-time strategy and god games tend to employ a camera
that ﬂoats above the terrain, looking down at an angle. The camera can
be panned about over the terrain, but the pitch and yaw of the camera
are usually not under direct player control.
•
Cinematic cameras. Most three-dimensional games have at least some
cinematic moments in which the camera ﬂies about within the scene in
a more ﬁlmic manner rather than being tethered to an object in the game.
These camera movements are typically controlled by the animators.

984
16. You Mean There’s More?
16.2.3
Artiﬁcial Intelligence
Another major component of most character-based games is artiﬁcial intelli-
gence (AI). At its lowest level, an AI system is usually founded in technologies
like basic path ﬁnding (which commonly makes use of the well-known A*
algorithm), perception systems (line of sight, vision cones, knowledge of the
environment, etc.) and some form of memory or knowledge.
On top of these foundations, character control logic is implemented. A
character control system determines how to make the character perform spe-
ciﬁc actions like locomoting, navigating unusual terrain features, using weapons,
driving vehicles, taking cover and so on. It typically involves complex inter-
faces to the collision, physics and animation systems within the engine. Char-
acter control is discussed in detail in Sections 11.11 and 11.12.
Above the character control layer, an AI system typically has goal setting
and decision making logic, and possibly also emotional state modeling, group
behaviors (coordination, ﬂanking, crowd and ﬂocking behaviors, etc.), and
perhaps some advanced features like an ability to learn from past mistakes or
adapt to a changing environment.
Of course, the term “artiﬁcial intelligence” is one of the biggest misnomers
around in the game industry. Game AI is always more of a smoke and mirrors
job than an attempt at truly mimicking human intelligence. Your AI characters
might have all sorts of complex internal emotional states and ﬁnely tuned
perception of the game world. But if the player cannot perceive the characters’
motivations, it’s all for naught.
AI programming is a rich topic, and we certainly have not done it justice in
this book. For more information, see [16], [6, Section 3], [7, Section 3] and [42,
Section 3]. Another good starting point is the GDC 2002 talk entitled, “The
Illusion of Intelligence: The Integration of AI and Level Design in Halo,” by
Chris Butcher and Jaime Griesemer of Bungie (http://bit.ly/1g7FbhD). And
while you’re online, search for “game AI programming” too. You’ll ﬁnd all
sorts of links to talks, papers and books on game AI. The web sites http://
aigamedev.com and http://www.gameai.com are great resources as well.
16.2.4
Other Gameplay Systems
Clearly there’s a lot more to a game than just player mechanics, cameras
and AI. Some games have drivable vehicles, implement specialized types of
weaponry, allow the player to destroy the environment with the help of a
dynamic physics simulation, let the player create his or her own characters,
build custom levels, require the player to solve puzzles, .... Of course, the
list of genre- and game-speciﬁc features, and all of the specialized software

16.2. Gameplay Systems
985
systems that implement them, could go on forever. Gameplay systems are as
rich and varied as games are. Perhaps this is where your next journey as a
game programmer will begin!

This page intentionally left blank
This page intentionally left blank

Bibliography
[1]
Tomas Akenine-Moller, Eric Haines and Naty Hoffman. Real-Time Rendering,
Third Edition. Wellesley, MA: A K Peters, 2008.
[2]
Andrei Alexandrescu. Modern C++ Design: Generic Programming and Design Pat-
terns Applied. Reading, MA: Addison-Wesley, 2001.
[3]
Grenville Armitage, Mark Claypool and Philip Branch. Networking and Online
Games: Understanding and Engineering Multiplayer Internet Games. New York, NY:
John Wiley and Sons, 2006.
[4]
James Arvo (editor). Graphics Gems II. San Diego, CA: Academic Press, 1991.
[5]
Grady Booch, Robert A. Maksimchuk, Michael W. Engel, Bobbi J. Young, Jim
Conallen and Kelli A. Houston. Object-Oriented Analysis and Design with Applica-
tions, Third Edition. Reading, MA: Addison-Wesley, 2007.
[6]
Mark DeLoura (editor). Game Programming Gems. Hingham, MA: Charles River
Media, 2000.
[7]
Mark DeLoura (editor). Game Programming Gems 2. Hingham, MA: Charles River
Media, 2001.
[8]
Philip Dutré, Kavita Bala and Philippe Bekaert. Advanced Global Illumination, Sec-
ond Edition. Wellesley, MA: A K Peters, 2006.
[9]
David H. Eberly. 3D Game Engine Design: A Practical Approach to Real-Time Com-
puter Graphics. San Francisco, CA: Morgan Kaufmann, 2001.
[10]
David H. Eberly. 3D Game Engine Architecture: Engineering Real-Time Applications
with Wild Magic. San Francisco, CA: Morgan Kaufmann, 2005.
987

988
Bibliography
[11]
David H. Eberly. Game Physics. San Francisco, CA: Morgan Kaufmann, 2003.
[12]
Christer Ericson. Real-Time Collision Detection. San Francisco, CA: Morgan Kauf-
mann, 2005.
[13]
Randima Fernando (editor). GPU Gems: Programming Techniques, Tips and Tricks
for Real-Time Graphics. Reading, MA: Addison-Wesley, 2004.
[14]
James D. Foley, Andries van Dam, Steven K. Feiner and John F. Hughes.
Computer Graphics: Principles and Practice in C, Second Edition. Reading, MA:
Addison-Wesley, 1995.
[15]
Grant R. Fowles and George L. Cassiday. Analytical Mechanics, Seventh Edition.
Paciﬁc Grove, CA: Brooks Cole, 2005.
[16]
John David Funge. AI for Games and Animation: A Cognitive Modeling Approach.
Wellesley, MA: A K Peters, 1999.
[17]
Erich Gamma, Richard Helm, Ralph Johnson and John M. Vlissiddes. Design
Patterns: Elements of Reusable Object-Oriented Software. Reading, MA: Addison-
Wesley, 1994.
[18]
Andrew S. Glassner (editor). Graphics Gems I. San Francisco, CA: Morgan Kauf-
mann, 1990.
[19]
Paul S. Heckbert (editor). Graphics Gems IV. San Diego, CA: Academic Press,
1994.
[20]
Maurice Herlihy and Nir Shavit. The Art of Multiprocessor Programming. San Fran-
cisco, CA: Morgan Kaufmann, 2008.
[21]
Roberto Ierusalimschy, Luiz Henrique de Figueiredo and Waldemar Celes. Lua
5.1 Reference Manual. Lua.org, 2006.
[22]
Roberto Ierusalimschy. Programming in Lua, Second Edition. Lua.org, 2006.
[23]
Isaac Victor Kerlow. The Art of 3-D Computer Animation and Imaging (Second Edi-
tion). New York, NY: John Wiley and Sons, 2000.
[24]
David Kirk (editor). Graphics Gems III. San Francisco, CA: Morgan Kaufmann,
1994.
[25]
Danny Kodicek. Mathematics and Physics for Game Programmers. Hingham, MA:
Charles River Media, 2005.
[26]
Raph Koster. A Theory of Fun for Game Design. Phoenix, AZ: Paraglyph, 2004.
[27]
John Lakos. Large-Scale C++ Software Design. Reading, MA: Addison-Wesley,
1995.
[28]
Eric Lengyel. Mathematics for 3D Game Programming and Computer Graphics, Sec-
ond Edition. Hingham, MA: Charles River Media, 2003.
[29]
Tuoc V. Luong, James S. H. Lok, David J. Taylor and Kevin Driscoll. International-
ization: Developing Software for Global Markets. New York, NY: John Wiley & Sons,
1995.
[30]
Steve Maguire. Writing Solid Code: Microsoft’s Techniques for Developing Bug-Free
C Programs. Bellevue, WA: Microsoft Press, 1993.

Bibliography
989
[31]
Scott Meyers. Effective C++: 55 Speciﬁc Ways to Improve Your Programs and Designs,
Third Edition. Reading, MA: Addison-Wesley, 2005.
[32]
Scott Meyers. More Effective C++: 35 New Ways to Improve Your Programs and De-
signs. Reading, MA: Addison-Wesley, 1996.
[33]
Scott Meyers. Effective STL: 50 Speciﬁc Ways to Improve Your Use of the Standard
Template Library. Reading, MA: Addison-Wesley, 2001.
[34]
Ian Millington. Game Physics Engine Development. San Francisco, CA: Morgan
Kaufmann, 2007.
[35]
Hubert Nguyen (editor). GPU Gems 3. Reading, MA: Addison-Wesley, 2007.
[36]
Alan V. Oppenheim and Alan S. Willsky. Signals and Systems. Englewood Cliffs,
NJ: Prentice-Hall, 1983.
[37]
Alan W. Paeth (editor). Graphics Gems V. San Francisco, CA: Morgan Kaufmann,
1995.
[38]
C. Michael Pilato, Ben Collins-Sussman and Brian W. Fitzpatrick. Version Control
with Subversion, Second Edition. Sebastopol, CA: O’Reilly Media, 2008. (Com-
monly known as “The Subversion Book.” Available online at http://svnbook.
red-bean.com.)
[39]
Matt Pharr (editor). GPU Gems 2: Programming Techniques for High-Performance
Graphics and General-Purpose Computation. Reading, MA: Addison-Wesley, 2005.
[40]
Richard Stevens and Dave Raybould. The Game Audio Tutorial: A Practical Guide
to Sound and Music for Interactive Games. Burlington, MA: Focal Press, 2011.
[41]
Bjarne Stroustrup. The C++ Programming Language, Special Edition (Third Edi-
tion). Reading, MA: Addison-Wesley, 2000.
[42]
Dante Treglia (editor). Game Programming Gems 3. Hingham, MA: Charles River
Media, 2002.
[43]
Gino van den Bergen. Collision Detection in Interactive 3D Environments. San Fran-
cisco, CA: Morgan Kaufmann, 2003.
[44]
Alan Watt. 3D Computer Graphics, Third Edition. Reading, MA: Addison Wesley,
1999.
[45]
James Whitehead II, Bryan McLemore and Matthew Orlando. World of Warcraft
Programming: A Guide and Reference for Creating WoW Addons. New York, NY:
John Wiley & Sons, 2008.
[46]
Richard Williams. The Animator’s Survival Kit. London, UK: Faber & Faber, 2002.

This page intentionally left blank
This page intentionally left blank

K15874
The highly recommended first edition of Game Engine Architecture provided a 
complete guide to the theory and practice of game engine software development. 
Updating the content to match today’s landscape of game engine architecture, this 
second edition continues to thoroughly cover the major components that make up a 
typical commercial game engine.
New to the Second Edition
• Information on new topics, including the latest variant of the C++ programming 
language, C++11, and the architecture of the eighth generation of gaming consoles, 
the Xbox One and PlayStation 4
• New chapter on audio technology covering the fundamentals of the physics, 
mathematics, and technology that go into creating an AAA game audio engine
• Updated sections on multicore programming, pipelined CPU architecture and 
optimization, localization, pseudovectors and Grassman algebra, dual quaternions, 
SIMD vector math, memory alignment, and anti-aliasing 
• Insight into the making of Naughty Dog’s latest hit, The Last of Us
The book presents the theory underlying various subsystems that comprise a 
commercial game engine as well as the data structures, algorithms, and software 
interfaces that are typically used to implement them. It primarily focuses on the engine 
itself, including a host of low-level foundation systems, the rendering engine, the 
collision system, the physics simulation, character animation, and audio. An in-depth 
discussion on the “gameplay foundation layer” delves into the game’s object model, 
world editor, event system, and scripting system. The text also touches on some 
aspects of gameplay programming, including player mechanics, cameras, and AI.
An awareness-building tool and a jumping-off point for further learning, Game Engine 
Architecture, Second Edition gives you a solid understanding of both the theory and 
common practices employed within each of the engineering disciplines covered. The 
book will help you on your journey through this fascinating and multifaceted field.
Game Engine Architecture
S E C O N D  E D I T I O N
Computer Game Development


www.allitebooks.com

Springer Optimization and Its Applications
VOLUME 62
Managing Editor
Panos M. Pardalos (University of Florida)
Editor–Combinatorial Optimization
Ding-Zhu Du (University of Texas at Dallas)
Advisory Board
J. Birge (University of Chicago)
C.A. Floudas (Princeton University)
F. Giannessi (University of Pisa)
H.D. Sherali (Virginia Polytechnic and State University)
T. Terlaky (McMaster University)
Y. Ye (Stanford University)
Aims and Scope
Optimization has been expanding in all directions at an astonishing rate
during the last few decades. New algorithmic and theoretical techniques
have been developed, the diffusion into other disciplines has proceeded at
a rapid pace, and our knowledge of all aspects of the ﬁeld has grown even
more profound. At the same time, one of the most striking trends in opti-
mization is the constantly increasing emphasis on the interdisciplinary na-
ture of the ﬁeld. Optimization has been a basic tool in all areas of applied
mathematics, engineering, medicine, economics, and other sciences.
The series Springer Optimization and Its Applications publishes under-
graduate and graduate textbooks, monographs and state-of-the-art exposi-
tory work that focus on algorithms for solving optimization problems and
also study applications involving such problems. Some of the topics covered
include nonlinear optimization (convex and nonconvex), network ﬂow prob-
lems, stochastic optimization, optimal control, discrete optimization, multi-
objective programming, description of software packages, approximation
techniques and heuristic approaches.
For further volumes:
http://www.springer.com/series/7393
www.allitebooks.com

  
www.allitebooks.com

Ding-Zhu Du  • Ker-I Ko
Design and Analysis
of Approximation Algorithms
Xiaodong Hu
•
www.allitebooks.com

Ding-Zhu Du 
Ker-I Ko 
Department of Computer Science  
Department of Computer Science 
University of Texas at Dallas 
State University of New York at Stony Brook 
Richardson, TX 75080 
Stony Brook, NY 11794 
USA 
USA 
dzdu@utdallas.edu 
keriko@cs.sunysb.edu 
 
Xiaodong Hu 
Institute of Applied Mathematics  
Academy of Mathematics and Systems Science 
Chinese Academy of Sciences  
Beijing 100190 
China 
xdhu@amss.ac.cn 
ISSN 1931-6828 
ISBN 978-1-4614-1700-2 
e-ISBN 978-1-4614-1701-9 
DOI 10.1007/978-1-4614-1701-9 
Springer New York Dordrecht Heidelberg London 
 
Library of Congress Control Number: 
 
¤ Springer Science+Business Media, LLC 2012 
All rights reserved. This work may not be translated or copied in whole or in part without the written 
permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, 
NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use in 
connection with any form of information storage and retrieval, electronic adaptation, computer soft-
ware, or by similar or dissimilar methodology now known or hereafter developed is forbidden. 
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they 
are not identified as such, is not to be taken as an expression of opinion as to whether or not they are 
subject to proprietary rights. 
 
Printed on acid-free paper 
 
Springer is part of Springer Science+Business Media (www.springer.com) 
2011942512
www.allitebooks.com

Preface
An approximation algorithm is an efﬁcient algorithm that produces solutions to an
optimization problem that are guaranteed to be within a ﬁxed ratio of the optimal
solution. Instead of spending an exponential amount of time ﬁnding the optimal
solution, an approximation algorithm settles for near-optimal solutions within poly-
nomial time in the input size. Approximation algorithms have been studied since the
mid-1960s. Their importance was, however, not fully understood until the discov-
ery of the NP-completeness theory. Many well-known optimization problems have
been proved, under reasonable assumptions in this theory, to be intractable, in the
sense that optimal solutions to these problems are not computable within polyno-
mial time. As a consequence, near-optimal approximation algorithms are the best
one can expect when trying to solve these problems.
In the past decade, the area of approximation algorithms has experienced an ex-
plosive rate of growth. This growth rate is partly due to the development of related
research areas, such as data mining, communication networks, bioinformatics, and
computational game theory. These newly established research areas generate a large
number of new, intractable optimization problems, most of which have direct appli-
cations to real-world problems, and so efﬁcient approximate solutions to them are
actively sought after.
In addition to the external, practical need for efﬁcient approximation algorithms,
there is also an intrinsic, theoretical motive behind the research of approximation
algorithms. In the design of an exact-solution algorithm, the main, and often only,
measure of the algorithm’s performance is its running time. This ﬁxed measure of-
ten limits our choice of techniques in the algorithm’s design. For an approximation
algorithm, however, there is an equally important second measure, that is, the per-
formance ratio of the algorithm, which measures how close the approximation al-
v
www.allitebooks.com

vi
Preface
gorithm’s output is to the optimal solution. This measure adds a new dimension to
the design and analysis of approximation algorithms. Namely, we can now study the
tradeoff between the running time and the performance ratio of approximation algo-
rithms, and apply different design techniques to achieve different tradeoffs between
these two measures. In addition, new theoretical issues about the approximation to
an optimization problem need to be addressed: What is the performance ratio of an
approximation algorithm for this problem based on certain types of design strategy?
What is the best performance ratio of any polynomial-timeapproximation algorithm
for this problem? Does the problem have a polynomial-time approximation scheme
or a fully polynomial-time approximation scheme? These questions are not only of
signiﬁcance in practice for the design of approximation algorithms; they are also of
great theoretical interest, with intriguing connections to the NP-completeness the-
ory.
Motivated by these theoretical questions and the great number of newly discov-
ered optimization problems, people have developed many new design techniques
for approximation algorithms, including the greedy strategy, the restriction method,
the relaxation method, partition, local search, power graphs, and linear and semidef-
inite programming. A comprehensive survey of all these methods and results in a
single book is not possible. We instead provide in this book an intensive study of the
main methods, with abundant applications following our discussion of each method.
Indeed, this book is organized according to design methods instead of application
problems. Thus, one can study approximation algorithms of the same nature to-
gether, and learn about the design techniques in a more uniﬁed way. To this end, the
book is arranged in the following way: First, in Chapter 1, we give a brief introduc-
tion to the concept of NP-completeness and approximation algorithms. In Chapter
2, we give an in-depth analysis of the greedy strategy, including greedy algorithms
with submodular potential functions and those with nonsubmodular potential func-
tions. In Chapters 3, 4, and 5, we cover various restriction methods, including par-
tition and Guillotine cut methods, with applications to many geometric problems.
In the next four chapters, we study the relaxation methods. In addition to a general
discussion of the relaxation method in Chapter 6, we devote three chapters to ap-
proximation algorithms based on linear and semideﬁnite programming, including
the primal-dual schema and its equivalence with the local ratio method. Finally, in
Chapter 10, we present various inapproximability results based on recent work in
the NP-completeness theory. A number of examples and exercises are provided for
each design technique. They are drawn from diverse areas of research, including
communication network design, optical networks, wireless ad hoc networks, sensor
networks, bioinformatics, social networks, industrial engineering, and information
management systems.
This book has grown out of lecture notes used by the authors at the University
of Minnesota, University of Texas at Dallas, Tsinghua University, Graduate School
of Chinese Academy of Sciences, Xi’an Jiaotong University, Zhejiang University,
East China Normal University, Dalian University of Technology, Xinjiang Univer-
sity, Nankai University, Lanzhou Jiaotong University, Xidian University, and Harbin
Institute of Technology. In a typical one-semester class for ﬁrst-year graduate stu-
www.allitebooks.com

Preface
vii
dents, one may cover the ﬁrst two chapters, one or two chapters on the restriction
method, two or three chapters on the relaxation method, and Chapter 10. With more
advanced students, one may also teach a seminar course focusing on one of the
greedy, restriction, or relaxation methods, based on the corresponding chapters of
this book and supplementary material from recent research papers. For instance, a
seminar on combinatorial optimization emphasizing approximations based on linear
and semideﬁnite programming can be organized using Chapters 7, 8, and 9.
This book has beneﬁted much from the help of our friends, colleagues, and stu-
dents. We are indebted to Peng-Jun Wan, Weili Wu, Xiuzhen Cheng, Jie Wang, Yin-
feng Xu, Zhao Zhang, Deying Li, Hejiao Huang, Hong Zhu, Guochuan Zhang, Wei
Wang, Shugang Gao, Xiaofeng Gao, Feng Zou, Ling Ding, Xianyue Li, My T. Thai,
Donghyun Kim, J. K. Willson, and Roozbeh Ebrahimi Soorchaei, who made much-
valued suggestions and corrections to the earlier drafts of the book. We are also
grateful to Professors Frances Yao, Richard Karp, Ronald Graham, and Fan Chung
for their encouragement. Special thanks are due to Professor Andrew Yao and the
Institute for Theoretical Computer Science, Tsinghua University, for the generous
support and stimulating environment they provided for the ﬁrst two authors during
their numerous visits to Tsinghua University.
Dallas, Texas
Ding-Zhu Du
Stony Brook, New York
Ker-I Ko
Beijing, China
Xiaodong Hu
August 2011
www.allitebooks.com

  
www.allitebooks.com

Contents
Preface
v
1
Introduction
1
1.1
Open Sesame
1
1.2
Design Techniques for Approximation Algorithms
8
1.3
Heuristics Versus Approximation
13
1.4
Notions in Computational Complexity
14
1.5
NP-Complete Problems
17
1.6
Performance Ratios
23
Exercises
28
Historical Notes
33
2
Greedy Strategy
35
2.1
Independent Systems
35
2.2
Matroids
40
2.3
Quadrilateral Condition on Cost Functions
43
2.4
Submodular Potential Functions
49
2.5
Applications
59
2.6
Nonsubmodular Potential Functions
66
Exercises
75
Historical Notes
80
3
Restriction
81
3.1
Steiner Trees and Spanning Trees
82
3.2
k-Restricted Steiner Trees
86
3.3
Greedy k-Restricted Steiner Trees
89
ix
www.allitebooks.com

x
Contents
3.4
The Power of Minimum Spanning Trees
102
3.5
Phylogenetic Tree Alignment
110
Exercises
115
Historical Notes
121
4
Partition
123
4.1
Partition and Shifting
123
4.2
Boundary Area
129
4.3
Multilayer Partition
136
4.4
Double Partition
142
4.4.1 A Weighted Covering Problem
142
4.4.2 A 2-Approximation for WDS-UDG on a Small Cell
146
4.4.3 A 6-Approximation for WDS-UDG on a Large Cell
151
4.4.4 A (6 + ε)-Approximation for WDS-UDG
155
4.5
Tree Partition
157
Exercises
160
Historical Notes
164
5
Guillotine Cut
165
5.1
Rectangular Partition
165
5.2
1-Guillotine Cut
170
5.3
m-Guillotine Cut
175
5.4
Portals
184
5.5
Quadtree Partition and Patching
191
5.6
Two-Stage Portals
201
Exercises
205
Historical Notes
208
6
Relaxation
211
6.1
Directed Hamiltonian Cycles and Superstrings
211
6.2
Two-Stage Greedy Approximations
219
6.3
Connected Dominating Sets in Unit Disk Graphs
223
6.4
Strongly Connected Dominating Sets in Digraphs
228
6.5
Multicast Routing in Optical Networks
235
6.6
A Remark on Relaxation Versus Restriction
238
Exercises
240
Historical Notes
243
7
Linear Programming
245
7.1
Basic Properties of Linear Programming
245
7.2
Simplex Method
252
7.3
Combinatorial Rounding
259
7.4
Pipage Rounding
267
7.5
Iterated Rounding
272
7.6
Random Rounding
280

Contents
xi
Exercises
289
Historical Notes
295
8
Primal-Dual Schema and Local Ratio
297
8.1
Duality Theory and Primal-Dual Schema
297
8.2
General Cover
303
8.3
Network Design
310
8.4
Local Ratio
315
8.5
More on Equivalence
325
Exercises
332
Historical Notes
336
9
Semideﬁnite Programming
339
9.1
Spectrahedra
339
9.2
Semideﬁnite Programming
341
9.3
Hyperplane Rounding
345
9.4
Rotation of Vectors
352
9.5
Multivariate Normal Rounding
358
Exercises
363
Historical Notes
369
10
Inapproximability
371
10.1 Many–One Reductions with Gap
371
10.2 Gap Ampliﬁcation and Preservation
376
10.3 APX-Completeness
380
10.4 PCP Theorem
388
10.5 (ρ ln n)-Inapproximability
391
10.6 nc-Inapproximability
396
Exercises
399
Historical Notes
405
Bibliography
407
Index
425

  

1
Introduction
It is the mark of an educated mind to rest satisﬁed with
the degree of precision which the nature of the subject admits
and not to seek exactness where only an approximation is possible.
— Aristotle
A man only becomes wise when he begins to calculate
the approximate depth of his ignorance.
— Gian Carlo Menotti
When exact solutions are hard to compute, approximation algorithms can help. In
this chapter, we introduce the basic notions of approximation algorithms. We study
a simple optimization problem to demonstrate the tradeoff between the time com-
plexity and performance ratio of its approximation algorithms. We also present a
brief introduction to the general theory of computational complexity and show how
to apply this theory to classify optimization problems according to their approxima-
bility.
1.1
Open Sesame
As legend has it, Ali Baba pronounced the magic words “open sesame” and found
himself inside the secret cave of the Forty Thieves, with all their precious treasures
laid before him. After the initial excitement subsided, Ali Baba quickly realized
that he had a difﬁcult optimization problem to solve: He had only brought a single
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
1
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9_1, 
© Springer Science+Business Media, LLC 2012 

2
Introduction
knapsack with him. Which items in the cave should he put in the knapsack in order
to maximize the total value of his ﬁnd?
In modern terminology, what Ali Baba faced is a resource management problem.
In this problem, one is given a ﬁxed amount S of resources (the total volume of the
knapsack) and a set of n tasks (the collection of treasures in the cave). Completing
each task requires a certain amount of resources and gains a certain amount of proﬁt.
The problem is to maximize the total proﬁt, subject to the condition that the total
resources used do not exceed S. Formally, we can describe Ali Baba’s problem as
follows:
Given n items I1, I2, . . ., In, a volume si and a value ci for each item
Ii, 1 ≤i ≤n, and an integer S, ﬁnd a subset A of items that maximizes
the total value 
Ii∈A ci, subject to the condition that the total volume

Ii∈A si does not exceed S.
We can introduce, for each 1 ≤i ≤n, a 0–1 variable xi to represent item Ii in
the following sense:
xi =
1,
if Ii ∈A,
0,
if Ii ̸∈A.
Then, Ali Baba’s problem can be reformulated as a 0–1 integer programming prob-
lem:
KNAPSACK: Given 2n + 1 positive integers S, s1, s2, . . ., sn and
c1, c2, . . ., cn,
maximize
c(x) = c1x1 + c2x2 + · · · + cnxn,
subject to
s1x1 + s2x2 + · · · + snxn ≤S,
x1, x2, . . ., xn ∈{0, 1}.
Notation. (1) In this book, we will use the following notation about an optimization
problem Π: On an input instance I of Π, we write Opt(I) to denote the optimal
solution of the instance I, and opt(I) to denote the optimum value of the objective
function on input I. When there is no confusion, we write Opt and opt for Opt(I)
and opt(I), respectively. In addition, for convenience, we often write, for an ob-
jective function f(x), f∗to denote the optimum value of the function f, and x∗
to denote the value of x that achieves the optimum value f∗. For instance, for the
problem KNAPSACK above, we write opt or c∗to denote the maximum value of c(x)
under the given constraints, and Opt or x∗to denote the value of (x1, x2, . . ., xn)
that makes n
i=1 cixi = c∗.
(2) For the sets of numbers, we write N to denote the set of natural numbers (i.e.,
the set of nonnegative integers), Z the set of integers, Z+ the set of positive integers,
R the set of real numbers, and R+ the set of positive integers.
Following the above convention, let opt denote the optimum value of the objec-
tive function c(x). Without loss of generality, we may assume that sk ≤S for all

1.1 Open Sesame
3
k = 1, . . ., n. In fact, if sk > S, then we must have xk = 0, and so we need not
consider the kth item at all. This assumption implies that opt ≥max1≤k≤n ck.
There are many different approaches to attacking the KNAPSACK problem. First,
let us use the dynamic programming technique to ﬁnd the exact solutions for KNAP-
SACK.
To simplify the description of the algorithm, we ﬁrst deﬁne some notations. For
any subset I ⊆{1, . . ., n}, let SI denote the sum 
k∈I sk. For each pair (i, j),
with 1 ≤i ≤n, 0 ≤j ≤n
i=1 ci, if there exists a set I ⊆{1, 2, . . ., n} such that

k∈I ck = j and SI ≤S, then let a(i, j) denote such a set I with the minimum
SI. If such an index subset I does not exist, then we say that a(i, j) is undeﬁned,
and write a(i, j) = nil.
Using the above notation, it is clear that opt = max{j | a(n, j) ̸= nil}. There-
fore, it sufﬁces to compute all values of a(i, j). The following algorithm is based on
this idea.1
Algorithm 1.A (Exact Algorithm for KNAPSACK)
Input: Positive integers S, s1, s2, . . ., sn, c1, c2, . . ., cn.
(1) Let csum ←
n

i=1
ci.
(2) For j ←0 to csum do
if j = 0 then a(1, j) ←∅
else if j = c1 then a(1, j) ←{1} else a(1, j) ←nil.
(3) For i ←2 to n do
for j ←0 to csum do
if [a(i −1, j −ci) ̸= nil] and [Sa(i−1,j−ci) ≤S −si]
and [a(i −1, j) ̸= nil ⇒Sa(i−1,j) > Sa(i−1,j−ci) + si]
then a(i, j) ←a(i −1, j −ci) ∪{i}
else a(i, j) ←a(i −1, j).
(4) Output c∗←max{j | a(n, j) ̸= nil}.
It is not hard to verify that this algorithm always ﬁnds the optimal solutions to
KNAPSACK (see Exercise 1.1).
Next, we consider the time complexity of Algorithm 1.A. Since Ali Baba had to
load the treasures and leave the cave before the Forty Thieves came back, he needed
an efﬁcient algorithm. It is easy to see that, for any I ⊆{1, . . ., n}, it takes time
O(n logS) to compute SI.2 Thus, Algorithm 1.A runs in time O(n3M log(MS))
where M = max{ck | 1 ≤k ≤n} (note that csum = O(nM)). We note that
1We use the standard pseudocodes to describe an algorithm; see, e.g., Cormen et al. [2001].
2In the rest of the book, we write log k to denote log2 k.

4
Introduction
the input size of the problem is n log M + log S (assuming that the input integers
are written in the binary form). Therefore, Algorithm 1.A is not a polynomial-time
algorithm. It is actually a pseudo-polynomial-timealgorithm, in the sense that it runs
in time polynomial in the maximum input value but not necessarily polynomial in
the input size. Since the input value could be very large, a pseudo polynomial-time
algorithm is usually not considered as an efﬁcient algorithm. To be sure, if Ali Baba
tried to run this algorithm, then the Forty Thieves would deﬁnitely have come back
before he got the solution—even if he could calculate as fast as a modern digital
computer.
As a compromise, Ali Baba might ﬁnd a fast approximation algorithm more use-
ful. For instance, the following is such an approximation algorithm, which uses a
simple greedy strategy that selects the heaviest item (i.e., the item with the greatest
density ci/si) ﬁrst.
Algorithm 1.B (Greedy Algorithm for KNAPSACK)
Input: Positive integers S, s1, s2, . . ., sn, c1, c2, . . ., cn.
(1) Sort all items in the nonincreasing order of ci/si. Without loss of generality,
assume that c1/s1 ≥c2/s2 ≥· · · ≥cn/sn.
(2) If
n

i=1
si ≤S then output cG ←
n

i=1
ci
else k ←max

j

j

i=1
si ≤S <
j+1

i=1
si

;
output cG ←max

ck+1,
k

i=1
ci

.
It is clear that this greedy algorithm runs in time O(n log(nMS)) and hence is
very efﬁcient. The following theorem shows that it produces an approximate solu-
tion not very far from the optimum.
Theorem 1.1 Let opt be the optimal solution of the problem KNAPSACK and cG
the approximate solution obtained by Algorithm 1.B. Then opt ≤2cG (and we say
that the performance ratio of Algorithm 1.B is bounded by the constant 2).
Proof. For convenience, write c∗for opt. If n
i=1 si ≤S, then cG = c∗. Thus, we
may assume n
i=1 si > S. Let k be the integer found by Algorithm 1.B in step (2).
We claim that
k

i=1
ci ≤c∗<
k+1

i=1
ci.
(1.1)
The ﬁrst half of the above inequality holds trivially. For the second half, we note
that, in step (1), we sorted the items according to their density, ci/si. Therefore, if
we are allowed to cut each item into smaller pieces, then the most efﬁcient way of
using the knapsack is to load the ﬁrst k items, plus a portion of the (k + 1)st item
that ﬁlls the knapsack, because replacing any portion of these items by other items

1.1 Open Sesame
5
decreases the total density of the knapsack. This shows that the maximum total value
c∗we can get is less than k+1
i=1 ci.
We can also view the above argument in terms of linear programming. That is,
if we replace the constraints xi ∈{0, 1} by 0 ≤xi ≤1, then we obtain a linear
program which has the maximum objective function value ˆc ≥c∗. It is easy to check
that the following assignment is an optimal solution to this linear program3:
xj =
⎧
⎪
⎨
⎪
⎩
1,
for j = 1, 2, . . ., k,
S −k
i=1 si
/sk+1,
for j = k + 1,
0,
for j = k + 2, . . ., n.
Therefore,
c∗≤ˆc =
k

i=1
ci + ck+1
sk+1

S −
k

i=1
si

<
k

i=1
ci + ck+1
sk+1
sk+1 =
k+1

i=1
ci.
Finally, it is obvious that, from (1.1), we have
cG = max

ck+1,
k

i=1
ci

≥1
2
k+1

i=1
ci > c∗
2 .
□
The above two algorithms demonstrate an interesting tradeoff between the run-
ning time and the accuracy of an algorithm: If we sacriﬁce a little in the accuracy
of the solution, we may get a much more efﬁcient algorithm. Indeed, we can further
explore this idea of tradeoff and show a spectrum of approximation algorithms with
different running time and accuracy.
First, we show how to generalize the above greedy algorithm to get better ap-
proximate solutions—with worse, but still polynomial, running time. The idea is
as follows: We divide all items into two groups: those with values ci ≤a and
those with ci > a, where a is a ﬁxed parameter. Note that in any feasible solution
I ⊆{1, 2, . . ., n}, there can be at most opt/a ≤2cG/a items that have values
ci greater than a. So we can perform an exhaustive search over all index subsets
I ⊆{1, 2, . . ., n} of size at most 2cG/a from the second group as follows: For
each subset I, use the greedy strategy on the ﬁrst group to get a solution of the to-
tal volume no greater than S −SI, and combine it with I to get an approximate
solution. From Theorem 1.1, we know that our error is bounded by the value of a
single item of the ﬁrst group, which is at most a. In addition, we note that there are
at most n2cG/a index subsets of the second group to be searched through, and so the
running time is still a polynomial function in the input size.
In the following, we write |A| to denote the size of a ﬁnite set A.
Algorithm 1.C (Generalized Greedy Algorithm for KNAPSACK)
Input: Positive integers S, s1, s2, . . ., sn, c1, c2, . . ., cn, and a constant 0 < ε < 1.
3See Chapter 7 for a more complete treatment of linear programming.

6
Introduction
(1) Run Algorithm 1.B on the input to get value cG.
(2) Let a ←εcG.
(3) Let Ia ←{i | 1 ≤i ≤n, ci ≤a}. (Without loss of generality, assume that
Ia = {1, . . ., m}, where m ≤n.)
(4) Sort the items in Ia in the nonincreasing order of ci/si. Without loss of gener-
ality, assume that c1/s1 ≥c2/s2 ≥· · · ≥cm/sm.
(5) For each I ⊆{m + 1, m + 2, . . ., n} with |I| ≤2/ε do
if

i∈I
si > S then c(I) ←0
else if
m

i=1
si ≤S −

i∈I
si
then c(I) ←
m

i=1
ci +

i∈I
ci
else k ←max

j

j

i=1
si ≤S −

i∈I
si <
j+1

i=1
si

;
c(I) ←
k

i=1
ci +

i∈I
ci.
(6) Output cGG ←max{c(I) | I ⊆{m + 1, m + 2, . . ., n}, |I| ≤2/ε}.
Theorem 1.2 Let opt be the optimal solution to KNAPSACK and cGG the approxi-
mation obtained by Algorithm 1.C. Then opt ≤(1 + ε)cGG. Moreover, Algorithm
1.C runs in time O(n1+2/ε log(nMS)).
Proof. For convenience, write c∗= opt and let I∗= Opt be the optimal index set;
that is, 
i∈I∗ci = c∗and 
i∈I∗si ≤S. Deﬁne I = {i ∈I∗| ci > a}. We have
already shown that |I| ≤c∗/a ≤2cG/a = 2/ε. Therefore, in step (5) of Algorithm
1.C, the index set I will eventually be set to I. Then, the greedy strategy, as shown
in the proof of Theorem 1.1, will ﬁnd c(I) with the property
c(I) ≤c∗≤c(I) + a.
Since cGG is the maximum c(I), we get
c(I) ≤cGG ≤c∗≤c(I) + a ≤cGG + a.
Let IG denote the set obtained by Algorithm 1.B on the input. Let IG = {i ∈IG |
ci > a}. Then |IG| ≤cG/a = 1/ε. So, we will process set IG in step (5) and get
c(IG) = cG. It means cGG ≥cG, and so
c∗≤cGG + a = cGG + εcG ≤(1 + ε)cGG.

1.1 Open Sesame
7
Note that there are at most n2/ε index sets I of size |I| ≤2/ε. Therefore, the
running time of Algorithm 1.C is O(n1+2/ε log(nMS)).
□
By Theorem 1.2, for any ﬁxed ε > 0, Algorithm 1.C runs in time O(n1+2/ε
log(nMS)) and hence is a polynomial-time algorithm. As ε decreases to zero, how-
ever, the running time increases exponentially with respect to 1/ε. Can we slow
down the speed of increase of the running time with respect to 1/ε? The answer is
yes. The following is such an approximation algorithm:
Algorithm 1.D (Polynomial Tradeoff Approximation for KNAPSACK)
Input: Positive integers S, s1, s2, . . ., sn, c1, c2, . . ., cn, and an integer h > 0.
(1) For k ←1 to n do
c′
k ←
ckn(h + 1)
M

, where M = max
1≤i≤n ci.
(2) Run Algorithm 1.A on the following instance of KNAPSACK:
maximize
c′
1x1 + c′
2x2 + · · · + c′
nxn
subject to
s1x1 + s2x2 + · · · + snxn ≤S,
x1, x2, . . ., xn ∈{0, 1}.
(1.2)
Let (x∗
1, . . ., x∗
n) be the optimal solution found by Algorithm 1.A (i.e., the
index set corresponding to the optimum value opt′ = (c′)∗of (1.2)).
(3) Output cPT ←c1x∗
1 + · · · + cnx∗
n.
Theorem 1.3 The solution obtained by Algorithm 1.D satisﬁes the relationship
opt
cPT
≤1 + 1
h,
where opt is the optimal solution to the input instance.
Proof. For convenience, let c∗= opt and I∗= Opt be the optimal index set of the
input instance; that is, c∗= 
k∈I∗ck. Also, let J∗be the index set found in step
(2); that is, J∗= {k | 1 ≤k ≤n, x∗
k = 1}. Then, we have
cPT =

k∈J∗
ck =

k∈J∗
ckn(h + 1)
M
·
M
n(h + 1)
≥

k∈J∗
ckn(h + 1)
M

·
M
n(h + 1)
=
M
n(h + 1)

k∈J∗
c′
k ≥
M
n(h + 1)

k∈I∗
c′
k
≥
M
n(h + 1)

k∈I∗
ckn(h + 1)
M
−1

≥c∗−
M
h + 1 ≥c∗

1 −
1
h + 1

.
www.allitebooks.com

8
Introduction
In the above, the second inequality holds because J∗is the optimal solution to
the modiﬁed instance of KNAPSACK; and the last inequality holds because M =
max1≤i≤n{ci} ≤c∗. Thus,
c∗
cPT
≤
1
1 −1/(h + 1) = 1 + 1
h.
□
We note that in step (2), the running time for Algorithm 1.A on the modiﬁed
instance is O(n3M ′ log(M ′S)), where M ′ = max{c′
k | 1 ≤k ≤n} ≤n(h + 1).
Therefore, the total running time of Algorithm 1.D is O(n4h log(nhS)), which is
a polynomial function with respect to n, logS, and h = 1/ε. Thus, the tradeoff
between running time and approximation ratio of Algorithm 1.D is better than that
of the generalized greedy algorithm.
From the above analysis, we learned that if we turn our attention from the opti-
mal solutions to the approximate solutions, then we may ﬁnd many new ideas and
techniques to attack the problem. Indeed, the design and analysis of approximation
algorithms are very different from that of exact (or, optimal) algorithms. It is a cave
with a mother lode of hidden treasures. Let us say “Open Sesame” and ﬁnd out what
they are.
1.2
Design Techniques for Approximation Algorithms
What makes the design and analysis of approximation algorithms so different from
that of algorithms that search for exact solutions?4
First, they study different types of problems. Algorithms that look for exact solu-
tions work only for tractable problems, but approximation algorithms apply mainly
to intractable problems. By tractable problems, we mean, in general, problems that
can be solved exactly in polynomial time in the input size. While tractable prob-
lems, such as the minimum spanning-tree problem, the shortest-path problem, and
maximum matching are the main focus of most textbooks for algorithms, most in-
tractable problems are not discussed in these books. On the other hand, a great
number of problems we encounter in the research literature, such as the traveling
salesman problem, scheduling, and integer programming, are intractable. That is,
no polynomial-time exact algorithms have been found for them so far. In addition,
through the study of computational complexity theory, most of these problems have
proven unlikely to have polynomial-time exact algorithms at all. Therefore, approx-
imation algorithms seem to be the only resort.
Second, and more importantly, they emphasize different aspects of the perfor-
mance of the algorithms. For algorithms that look for exact solutions, the most im-
portant issue is the efﬁciency, or the running time, of the algorithms. Data structures
and design techniques are introduced mainly to improve the running time. For ap-
proximation algorithms, the running time is, of course, still an important issue. It,
4We call such algorithms exact algorithms.

1.2 Design Techniques for Approximation Algorithms
9
however, has to be considered together with the performance ratio (the estimate of
how close the approximate solutions are to the optimal solutions) of the algorithms.
As we have seen in the study of the KNAPSACK problem, the tradeoff between the
running time and performance ratio is a critical issue in the analysis of approxi-
mation algorithms. Many design techniques for approximation algorithms aim to
improve the performance ratio with the minimum extra running time.
To illustrate this point, let us take a closer look at approximation algorithms.
First, we observe that, in general, an optimization problem may be formulated in
the following form:
minimize (or, maximize)
f(x1, x2, . . ., xn)
subject to
(x1, x2, . . ., xn) ∈Ω,
(1.3)
where f is a real-valued function and Ω a subset of Rn. We call the function f
the objective function and set Ω the feasible domain (or, the feasible region) of the
problem.
The design of approximation algorithms for such a problem can roughly be di-
vided into two steps. In the ﬁrst step, we convert the underlying intractable problem
into a tractable variation by perturbing the input values, the objective function, or
the feasible domain of the original problem. In the second step, we design an ef-
ﬁcient exact algorithm for the tractable variation and, if necessary, convert its so-
lution back to an approximate solution for the original problem. For instance, in
Algorithm 1.D, we ﬁrst perturb the inputs ci into smaller c′
i, and thus converted the
original KNAPSACK problem into a tractable version of KNAPSACK in which the
maximum parameter c′
i is no greater than n(h+1). Then, in the second step, we use
the technique of dynamic programming to solve the tractable version in polynomial
time, and use the optimal solution (x∗
1, x∗
2, . . . , x∗
n) with the tractable version of
KNAPSACK as an approximate solution to the original instance of KNAPSACK.
It is thus clear that in order to design good approximation algorithms, we must
know how to perturb the original intractable problem to a tractable variation such
that the solution to the tractable problem is closely related to that of the original
problem. A number of techniques for such perturbation have been developed. The
perturbation may act on the objective functions, as in the greedy strategy and the
local search method. It may involve changes to the feasible domain, as in the tech-
niques of restriction and relaxation. It may sometimes also perform some operations
on the inputs, as in the technique of power graphs. These techniques are very differ-
ent from the techniques for the design of efﬁcient exact algorithms, such as divide
and conquer, dynamic programming, and linear programming. The study of these
design techniques forms an important part of the theory of approximation algo-
rithms. Indeed, this book is organized according to the classiﬁcation of these design
techniques. In the following, we give a brief overview of these techniques and the
organization of the book (see Figure 1.1).
In Chapter 2, we present a theory of greedy strategies, in which we demonstrate
how to use the notions of independent systems and submodular potential functions

10
Introduction
Chapter 1
Introduction



=
ZZZ
~
Chapter 2
Greedy Strategy
ZZZ
~



=
Chapter 3
Restriction
?
Chapter 4
Partition
?
Chapter 5
Guillotine Cut
Chapter 6
Relaxation
?
Chapter 7
Linear Programming
?
Chapter 8
Primal-Dual Schema
and Local Ratio
?
Chapter 9
Semideﬁnite Programming
Chapter 10
Inapproximability
Figure 1.1:
Relationships among chapters.
to analyze the performance of greedy algorithms. Due to space limits, we will omit
the related but more involved method of local search.
The technique of restriction is studied in Chapters 3–5. The basic idea of re-
striction is very simple: If we narrow down the feasible domain, the solutions may
become easier to ﬁnd. There are many different ways to restrict the feasible domains,
depending on the nature of the problems. We present some simple applications in
Chapter 3. Two of the most important techniques of restriction, partition and Guil-
lotine cut, are then studied in detail in Chapters 4 and 5, respectively.
In Chapters 6–9, we study the technique of relaxation. In contrast to restriction,
the technique of relaxation is to enlarge the feasible domain to include solutions
which are considered infeasible in the original problem so that different design tech-
niques can be applied. A common implementation of the relaxation technique is as
follows: First, we formulate the problem into an integer programming problem (i.e.,
a problem in the form of (1.3) with Ω ⊆Zn). Then, we relax this integer program
into a linear program by removing the integral constraints on the variables. After we
solve this relaxed linear program, we round the real-valued solution into integers and
use them as the approximate solution to the original problem. Linear programming,

1.2 Design Techniques for Approximation Algorithms
11
solution for
the original problem
?
estimation
solution for
the restricted problem
solution for
the relaxed problem
?
estimation
solution for
the original problem
Figure 1.2:
Analysis of approximation algorithms based on restriction and relax-
ation.
the primal-dual method, and the local ratio method are the main techniques in this
approach. We study these techniques in Chapters 7 and 8. In addition to the linear
programming technique, it has recently been found that semideﬁnite programming
can also be applied in such a relaxation approach. We present the theory of semidef-
inite programming and its application to approximation algorithms in Chapter 9.
We remark that an important step in the analysis of approximation algorithms
is the estimation of the errors created by the perturbation of the feasible domain.
For the algorithms based on the restriction and relaxation techniques, this error es-
timation often uses similar methods. To analyze an algorithm designed with the
restriction technique, one usually takes an optimal solution for the original problem
and modiﬁes it to meet the restriction, and then estimates the errors that occurred
in the modiﬁcation. For the algorithms designed with the relaxation technique, the
key part of the analysis is about rounding the solution, or estimating the errors that
occurred in the transformation from the solution for the relaxed problem to the so-
lution for the original problem. Therefore, in both cases, a key step in the analysis is
the estimation of the change of solutions from those in a larger (or, relaxed) domain
to those in a smaller (or, restricted) domain (see Figure 1.2).
To explain this observation more clearly, let us consider a minimization problem
minx∈Ω f(x) as deﬁned in (1.3), where x denotes a vector (x1, x2, . . ., xn) in Rn.
Assume that x∗∈Ω satisﬁes f(x∗) = minx∈Ω f(x). Suppose we restrict the fea-
sible domain to a subregion Γ of Ω and ﬁnd an optimal solution y∗for the restricted
problem; that is, f(y∗) = minx∈Γ f(x). Then, we may analyze the performance
of y∗as an approximate solution to the original problem in the following way (see
Figure 1.3):
(1) Consider a minimum solution x∗of minx∈Ω f(x).
(2) Modify x∗to obtain a feasible solution y of minx∈Γ f(x).

12
Introduction
y*
x*
Ω
Γ
y
Figure 1.3:
Analysis of the restriction and relaxation approximations.
(3) Estimate the value of f(y)/f(x∗), and use it as an upper bound for the per-
formance ratio for the approximate solution y∗, since y ∈Γ implies
f(y∗)
f(x∗) ≤f(y)
f(x∗).
Similarly, consider the problem minx∈Γ f(x). Suppose we relax the feasible re-
gion Γ to a bigger region Ω, and ﬁnd the optimal solution x∗for the relaxed prob-
lem; that is, f(x∗) = minx∈Ω f(x). Then, we can round x∗into a solution y ∈Γ
and use it as an approximate solution to the original problem. The analysis of this
relaxation algorithm can now be done as follows:
• Estimate the value f(y)/f(x∗), and use it as an upper bound for the perfor-
mance ratio for the approximate solution y, since, for any optimal solutiony∗
for the original problem, we have f(x∗) ≤f(y∗), and hence
f(y)
f(y∗) ≤f(y)
f(x∗).
Thus, in both cases, the analysis of the performance of the approximate solution is
reduced to the estimation of the ratio f(y)/f(x∗).
Notice, however, a critical difference in the above analyses. In the case of the
restriction algorithms, the change from x∗to y is part of the analysis of the algo-
rithm, and we are not concerned with the time complexity of this change. On the
other hand, in the case of the relaxation algorithms, this change is a step in the ap-
proximation algorithm, and has to be done in polynomial time. As a consequence,
while the method of rounding for the analysis of the relaxation algorithms may, in
general, be applied to the analysis of the restriction algorithms, the converse may
not be true; that is, the analysis techniques developed for the restriction algorithms
are not necessarily extendable to the analysis of the relaxation algorithms.

1.3 Heuristics Versus Approximation
13
1.3
Heuristics Versus Approximation
In the literature, the word “heuristics” often appears in the study of intractable prob-
lems and is sometimes used interchangeably with the word “approximation.” In this
book, however, we will use it in a different context and distinguish it from approxi-
mation algorithms. The ﬁrst difference between heuristics and approximation is that
approximation algorithms usually have guaranteed (worst-case) performance ratios,
while heuristic algorithms may not have such guarantees. In other words, approxi-
mations are usually justiﬁed with theoretical analysis, while heuristics often appeal
to empirical data.
The second difference is that approximation usually applies to optimization prob-
lems, while heuristics may also apply to decision problems. Let us look at an exam-
ple. First, we deﬁne some terminologies about Boolean formulas. A Boolean for-
mula is a formula formed by operations ∨(OR), ∧(AND), and ¬ (NOT) over Boolean
constants 0 (FALSE) and 1 (TRUE) and Boolean variables. For convenience, we also
use + for OR and · for AND, and write ¯x to denote ¬x. An assignment to a Boolean
formula φ is a function mapping each Boolean variable in φ to a Boolean constant
0 or 1. A truth assignment is an assignment that makes the resulting formula TRUE.
We say a Boolean formula is satisﬁable if it has a truth assignment. For instance,
the Boolean formula
(v1¯v2 + ¯v1v3¯v4 + v2¯v3)(¯v1¯v3 + ¯v2¯v4)
over the variables v1, . . ., v4 is satisﬁable, since the assignment τ(v1) = τ(v3) = 1
and τ(v2) = τ(v4) = 0 is a truth assignment for it.
Now, consider the following problem.
SATISFIABILITY (SAT): Given a Boolean formula, determine whether
it is satisﬁable.
This is not an optimization problem. Therefore, it does not make much sense
to try to develop an approximation algorithm for this problem, though there are a
number of heuristics, such as the resolution method, developed for this problem.
Such heuristics may work efﬁciently for a large subset of the input instances, but
they do not guarantee to solve all instances in polynomial time.
Although approximations and heuristics are different concepts, their ideas and
techniques can often be borrowed from each other. Theoretical analysis of approx-
imation algorithms could provide interesting ideas for heuristic algorithms. In ad-
dition, for some decision problem, we may ﬁrst convert it into an equivalent opti-
mization problem, and then adapt the approximation algorithms for the optimization
problem to heuristic algorithms for the original decision problem. For instance, we
may use the approximation algorithms for integer programming to develop a heuris-
tic algorithm for SAT as follows.
We ﬁrst convert the problem SAT into an optimization problem. Let v1, v2, . . .,
vn be Boolean variables and v the vector (v1, v2, . . ., vn) in {0, 1}n. Let y1, y2, . . .,
yn be real variables and y the vector (y1, y2, . . ., yn) in Rn. For each Boolean func-
tion f(v), we deﬁne a real function Ff(y) recursively as follows:

14
Introduction
(1) Initially,if f(v) = vi, then set Ff(y) ←yi; if f(v) = 0, then set Ff(y) ←0;
and if f(v) = 1 , then set Ff(y) ←1.
(2) Inductively, if f(v) = g(v) ∨h(v), then set Ff(y) ←Fg(y) + Fh(y) −
Fg(y) · Fh(y); if f(v) = g(v) ∧h(v), then set Ff(y) ←Fg(y) · Fh(y); and
if f(v) = ¬g(v), then set Ff(y) ←1 −Fg(y).
The above construction converts the decision problem SAT into an equivalent
optimization problem, in the sense that a Boolean formula f(v) is satisﬁable if
and only if the following 0–1 integer program has a positive maximum objective
function value:
maximize
Ff(y)
subject to
y ∈{0, 1}n.
Although this new problem is still intractable, it is nevertheless an optimization
problem, and the approximation techniques for 0–1 integer programming are appli-
cable. These approximation algorithms could then be studied and developed into a
heuristic for the decision version of SAT.
Historically, heuristic algorithms have appeared much earlier than approximation
algorithms. The ﬁrst documented approximation algorithm was discovered by Gra-
ham [1966] for a scheduling problem, while heuristic algorithms probably existed,
at least in the informal form, as early as the concept of algorithms was developed.
The existence of the rich families of heuristics and their wide applications encourage
us to develop them into new approximation algorithms. For instance, an important
idea for many heuristics is to link the discrete space of a combinatorial optimiza-
tion problem to the continuous space of a nonlinear optimization problem through
geometric, analytic, or algebraic techniques, and then to apply the nonlinear opti-
mization algorithms to the combinatorial optimization problems. Researchers have
found that this approach often leads to very fast and effective heuristics for combi-
natorial optimization problems of a large scale. However, most of these heuristics,
with a few exceptions such as the interior point method for linear programming,
though working well in practice, do not have a solid theoretical foundation. Theo-
retical analyses for these algorithms could provide new, surprising approximation
algorithms.
1.4
Notions in Computational Complexity
Roughly speaking, the main reason for studying approximation algorithms is to ﬁnd
efﬁcient, but not necessarily optimal, solutions to intractable problems. We have
informally deﬁned an intractable problem to be a problem which does not have a
polynomial-time algorithm. From the theoretical standpoint, there are, in this in-
formal deﬁnition, several important issues that have not been clearly addressed. For
instance, why do we identify polynomial-time computability with tractability? Does
polynomial-time computability depend on the computational model that we use to
implement the algorithm? How do we determine, in general, whether a problem has
a polynomial-time algorithm? These fundamental issues have been carefully exam-

1.4 Computational Complexity
15
ined in the theory of computational complexity. We present, in this and the next
sections, a brief summary of this theory. The interested reader is referred to Du and
Ko [2000] for more details.
The time complexity of an algorithm refers to the running time of the algorithm
as a function of the input size. As a convention, in the worst-case analysis, we take
the maximum running time over all inputs of the same size n as the time complexity
of the algorithm on size n. In order to estimate the running time of an algorithm, we
must specify the computational model in which the algorithm is implemented. Sev-
eral standard computational models have been carefully studied. Here, we consider
only two simple models: the pseudocode and the Turing machine.
We have already used pseudocodes to express algorithms in Section 1.1. Pseu-
docodes are an informal high-level programming language, similar to standard pro-
gramming languages such as Pascal, C, and Java, without complicated language
constructs such as advanced data structures and parameter-passing schemes in pro-
cedure calls. It is an abstract programming language in the sense that each variable
in a procedure represents a memory location that holds an integer or a real number,
without a size limit. We assume the reader is familiar with such high-level program-
ming languages and understands the basic syntax and semantics of pseudocodes.
The reader who is not familiar with pseudocodes is referred to any standard algo-
rithm textbook.
When an algorithm is expressed in the form of a program in pseudocode, it is
natural to use the number of statements or the number of arithmetic and comparison
operations as the basic measure for the time complexity of the algorithm. This time
complexity measure is simple to estimate but does not reﬂect the exact complexity of
the algorithm. For instance, consider the following simple procedure that computes
the function f(a, m) = am, where a and m are two positive integers:
b ←1;
For k ←1 to m do b ←b · a;
Output b.
It is not hard to see that, on any input (a, m), the number of operations to be executed
in the above algorithm is O(m), independent of the size n of the other input number
a. However, a detailed analysis shows that the size of b increases from 1 bit to about
nm bits in the computation of the algorithm, and yet we counted only one unit of
time for the multiplication of b and a, no matter how large b is. This does not seem
to reﬂect the real complexity of the algorithm. A more accurate estimate of the
time complexity should take into account the size of the operands of the arithmetic
operations. For instance, the logarithmic cost measure counts O(log n) units of time
for each arithmetic or comparison operation that is executed on operands whose
values are at most n. Thus, the time complexity of the above algorithm for am,
under the logarithmic cost measure, would be O(m2 loga).
We note that even using the logarithmic cost measure does not give the time com-
plexity of the algorithm completely correctly. Indeed, the logarithmic cost measure
is based on the assumption that arithmetic or comparison operations on operands of
n bits can be executed in O(n) units of time (in other words, these operations can be

16
Introduction
implemented in linear time). This assumption is plausible for simple operations, but
not for more complicated operations such as multiplication and division. Indeed, no
linear-time multiplication algorithm is known. The best algorithm known today for
multiplying two n-bit integers requires Ω(n logn) units of time. Therefore, the log-
arithmic cost measure tends to underestimate the complexity of an algorithm with
heavy multiplications.
To more accurately reﬂect the exact complexity of an algorithm, we usually use
a primitive computational model, called the Turing machine. We refer the reader to
textbooks of theory of computation, for instance, Du and Ko [2000], for the deﬁni-
tion of a Turing machine. Here, it sufﬁces to summarize that (1) all input, output,
and temporary data of the computation of a Turing machine are stored on a ﬁnite
number of tapes, with one single character stored in one cell of the tape, and (2) each
instruction of the Turing machine works on one cell of the tape, either changing the
character stored in the cell or moving its tape head to one of its neighboring cells.
That is, the complexity measure of the Turing machine is a bit-operation measure,
which most closely represents our intuitive notion of time complexity measure.
The instructions of Turing machines are very simple and so it makes the anal-
ysis of the computation of a Turing machine easier. In particular, it allows us to
prove lower bounds of a problem, which is difﬁcult to do for more complicated
computational models. However, one might suspect whether we can implement so-
phisticated algorithms with, for instance, advanced data structures and complicated
recursive calls in such a simplistic machine and, even if so, whether the imple-
mentation is as efﬁcient as more general models. It turns out that Turing machines,
though primitive, can simulate all known computational models efﬁciently in the
following sense: For any algorithm that can be implemented in the model in ques-
tion with time complexity t(n), there is a Turing machine implementing this algo-
rithm in time p(t(n)), where p is a polynomial function depending on the model
but independent of the algorithms. In fact, a widely accepted hypothesis, called the
extended Church–Turing thesis, states that a Turing machine can simulate any rea-
sonable deterministic computational model within polynomial time. In other words,
polynomial-time computability is a notion that is independent of the computational
models used to implement the algorithms.
Based on the extended Church–Turing thesis, we now formally identify the class
of tractable problems with the following complexity class:
P: the class of all decision problems that are solvable in polynomial time by a
deterministic Turing machine.
In other words, we say a problem is tractable if there is a Turing machine M that
solves the problem in polynomial time in the input size (i.e., M runs in time O(nk),
where n is the input size and k is a constant). We note that the composition of
two polynomial functions is still a polynomial function. Thus, the combination of
two polynomial-time algorithms is still a polynomial-time algorithm. This reﬂects
the intuition that the combination of two tractable algorithms should be considered
tractable.

1.5 NP-Complete Problems
17
Now, let us go back to our choice of using pseudocodes to describe algorithms.
From the above discussion, we may assume (and, in fact, prove) that the logarithmic
cost measure of a pseudocode procedure and the bit-operation complexity of an
equivalent Turing machine program are within a polynomial factor. Therefore, in
order to demonstrate that a problem is tractable, we can simply present the algorithm
in a pseudocode procedure and perform a simple time analysis of the procedure. On
the other hand, to show that a problem is intractable, we usually use Turing machines
as the computational model.
1.5
NP-Complete Problems
In the study of computational complexity, an optimization problem is usually for-
mulated into an equivalent decision problem, whose answer is either YES or NO.
For instance, we can formulate the problem KNAPSACK into the following decision
problem:
KNAPSACKD: Given 2n + 2 integers: S, K, s1, s2, . . ., sn, c1, c2, . . .,
cn, determine whether there is a sequence (x1, x2, . . ., xn) ∈{0, 1}n
such that n
i=1 sixi ≤S and n
i=1 cixi ≥K.
It is not hard to see that KNAPSACK and KNAPSACKD are equivalent, in the sense
that they are either both tractable or both intractable.
Proposition 1.4 The optimizationproblem KNAPSACK is polynomial-timesolvable
if and only if the decision problem KNAPSACKD is polynomial-time solvable.
Proof. Suppose the optimization problem KNAPSACK is polynomial-time solvable.
Then, we can solve the decision problem KNAPSACKD by ﬁnding the optimal so-
lution opt of the corresponding KNAPSACK instance and then answering YES if and
only if opt ≥K.
Conversely, suppose KNAPSACKD is solvable in polynomial time by a Turing
machine M. Assume that M runs in time O(N k), where N is the input size and k
is a constant. Now, on input I = (S, s1, . . ., sn, c1, . . ., cn) to the problem KNAP-
SACK, we can binary search for the maximum K such that M answers YES on
input (S, K, s1, . . ., sn, c1, . . ., cn). This maximum value K is exactly the opti-
mal solution opt for input I of the problem KNAPSACK. Note that K satisﬁes
K ≤M2 = n
i=1 ci. Thus, the above binary search needs to simulate M for at
most ⌊log M2 + 1⌋= O(N) times, where N is the size of input I. So, we can solve
KNAPSACK in time O(N k+1).
□
From the discussion of the last section, in order to prove a problem intractable, we
need to show that (the decision version of) the problem is not in P. Unfortunately, for
a great number of optimization problems, there is strong evidence, both empirical
and mathematical, suggesting that they are likely intractable, but no one is able
to ﬁnd a formal proof that they are not in P. Most of these problems, however,
share a common property called NP-completeness. That is, they can be solved by
www.allitebooks.com

18
Introduction
nondeterministic algorithms in polynomial time and, furthermore, if any of these
problems is proved to be not in P, then all of these problems are not in P.
A nondeterministic algorithm is an algorithm that can make nondeterministic
moves. In a nondeterministic move, the algorithm can assign a value of either 0 or
1 to a variable nondeterministically, so that the computation of the algorithm after
this step branches into two separate computation paths, each using a different value
for the variable. Suppose a nondeterministic algorithm executes nondeterministic
moves k times. Then it may generate 2k different deterministic computation paths,
some of which may output YES and some of which may output NO. We say the non-
deterministic algorithm accepts the input (i.e., answers YES) if at least one of the
computation paths outputs YES; and the nondeterministic algorithm rejects the in-
put if all computation paths output NO. (Thus, the actions of accepting and rejecting
an input by a nondeterministic algorithm A are not symmetric: If we change each
answer YES of a computation path to answer NO, and each NO to YES, the collective
solution of A does not necessarily change from accepting to rejecting.) On each in-
put x accepted by a nondeterministic algorithm A, the running time of A on x is the
length of the shortest computation path on x that outputs YES. The time complexity
of algorithm A is deﬁned as the function
tA(n) = the maximum running time on any x of length n that is accepted
by the algorithm A.
For instance, the following is a nondeterministic algorithm for KNAPSACK (more
precisely, for the decision problem KNAPSACKD):
Algorithm 1.E (Nondeterministic Algorithm for KNAPSACKD)
Input: Positive integers S, s1, s2, . . ., sn, c1, c2, . . ., cn, and an integer K > 0.
(1) For i ←1 to n do
nondeterministically select a value 0 or 1 for xi.
(2) If n
i=1 xisi ≤S and n
i=1 xici ≥K then output YES
else output NO.
It is clear that the above algorithm works correctly. Indeed, it contains 2n dif-
ferent computation paths, each corresponding to one choice of (x1, x2, . . ., xn) ∈
{0, 1}n. If one choice of (x1, x2, . . ., xn) satisﬁes the condition of step (2), then the
algorithm accepts the input instance; otherwise, it rejects. In addition, we note that
in this algorithm, all computation paths have the same running time, O(n). Thus,
this is a linear-time nondeterministic algorithm.
The nondeterministic Turing machine is the formalism of nondeterministic al-
gorithms. Corresponding to the deterministic complexity class P is the following
nondeterministic complexity class:
NP: the class of all decision problems that are computable by a nondeterministic
Turing machine in polynomial time.
We note that in a single path of a polynomial-timenondeterministic algorithm, there
can be at most a polynomial number of nondeterministic moves. It is not hard to see

1.5 NP-Complete Problems
19
that we can always move the nondeterministic moves to the beginning of the al-
gorithm without changing its behavior. Thus, all polynomial-time nondeterministic
algorithms MN have the following common form:
Assume that the input x has n bits.
(1) Nondeterministically select a string y = y1y2 · · · yp(n) ∈{0, 1}∗, where p
is a polynomial function.
(2) Run a polynomial-time deterministic algorithm MD on input (x, y).
Suppose MD answers YES on input (x, y); then we say y is a witness of the
instance x. Thus, a problem Π is in NP if there is a two-step algorithm for Π in
which the ﬁrst step nondeterministically selects a potential witness y of polynomial
size, and the second step deterministically veriﬁes that y is indeed a witness. We
call such an algorithm a guess-and-verify algorithm.
As another example, let us show that the problem SAT is in NP.
Algorithm 1.F (Nondeterministic Algorithm for SAT)
Input: A Boolean formula φ over Boolean variables v1, v2, . . ., vn.
(1) Guess n Boolean values b1, b2, . . ., bn.
(2) Verify (deterministically) that the formula φ is TRUE under the assignment
τ(vi) = bi, for i = 1, . . ., n. If so, output YES; otherwise, output NO.
The correctness of the above algorithm is obvious. To show that SAT is in NP,
we only need to check that the veriﬁcation of whether a Boolean formula containing
no variables is TRUE can be done in deterministic polynomial time.
We have seen that problems in NP, such as KNAPSACK and SAT, have sim-
ple polynomial-time nondeterministic algorithms. However, we do not know of any
physical devices to implement the nondeterministic moves in the algorithms. So,
what is the exact relationship between P and NP? This is one of the most important
open questions in computational complexity theory. On the one hand, we do not
know how to ﬁnd efﬁcient deterministic algorithms to simulate a nondeterministic
algorithm. A straightforward simulation by the deterministic algorithm that runs the
veriﬁcation step over all possible guesses would take an exponential amount of time.
On the other hand, though many people believe that there is no polynomial-time de-
terministic algorithm for every problem in NP, no one has yet found a formal proof
for that.
Without a proof for P ̸= NP, how do we demonstrate that a problem in NP is
likely to be intractable? The notion of NP-completeness comes to help.
For convenience, we write in the following x ∈A to denote that the answer
to the input x for the decision problem A is YES (that is, we identify the decision
problem with the set of all input instances which have the answer YES). We say a
decision problem A is polynomial-time reducible to a decision problem B, denoted
by A ≤P
m B, if there is a polynomial-time computable function f from instances of
A to instances of B (called the reduction function from A to B) such that x ∈A if
and only if f(x) ∈B. Intuitively, a reduction function f reduces the membership

20
Introduction
problem of whether x ∈A to the membership problem of whether f(x) ∈B. Thus,
if there is a polynomial-time algorithm to solve problem B, we can combine the
function f with this algorithm to solve problem A.
Proposition 1.5 (a) If A ≤P
m B and B ∈P, then A ∈P.
(b) If A ≤P
m B and B ≤P
m C, then A ≤P
m C.
The above two properties justify the use of the notation ≤P
m between deci-
sion problems: It is a partial ordering for the hardness of the problems (modulo
polynomial-time computability).
We can now deﬁne the term NP-completeness: We say a decision problem A is
NP-hard if, for any B ∈NP, B ≤P
m A. We say A is NP-complete if A is NP-hard
and, in addition, A ∈NP. That is, an NP-complete problem A is one of the hardest
problems in NP with respect to the reduction ≤P
m. For an optimization problem
A, we also say A is NP-hard (or, NP-complete) if its (polynomial-time equivalent)
decision version AD is NP-hard (or, respectively, NP-complete).
It follows immediately from Proposition 1.5 that if an NP-complete problem is
in P, then P = NP. Thus, in view of our inability to solve the P vs. NP question, the
next best way to prove a problem intractable is to show that it is NP-complete (and
so it is most likely not in P, unless P = NP).
Among all problems, SAT was the ﬁrst problem proved NP-complete. It was
proved by Cook [1971], who showed that for any polynomial-time nondeterminis-
tic Turing machine, its computation on any input x can be encoded by a Boolean
formula φx of polynomially bounded length such that the formula φx is satisﬁable
if and only if M accepts x. This proof is called a generic reduction, since it works
directly with the computation of a nondeterministic Turing machine. In general, it
does not require a generic reduction to prove a new problem A to be NP-complete.
Instead, by Proposition 1.5(b), we can use any problem B that is already known to
be NP-complete and only need to prove that B ≤P
m A. For instance, we can prove
that KNAPSACKD is NP-complete by reducing the problem SAT to it.
Theorem 1.6 KNAPSACKD is NP-complete.
Proof. We have already seen that KNAPSACKD is in NP. We now prove that
KNAPSACKD is complete for NP. In order to do this, we introduce a subproblem
3-SAT of SAT. In a Boolean formula, a variable or the negation of a variable is
called a literal. An elementary sum of literals is called a clause. A Boolean formula
is in 3-CNF (conjunctive normal form) if it is a product of a ﬁnite number of clauses,
each being the sum of exactly three literals. For instance, the following is a 3-CNF
formula:
(v1 + ¯v2 + ¯v3)(¯v1 + v3 + v4)(v2 + ¯v3 + ¯v4).
The problem 3-SAT asks whether a given 3-CNF Boolean formula is satisﬁable.
This problem is a restrictive form of the problem SAT, but it is also known to be
NP-complete. Indeed, there is a simple way of transforming a Boolean formula φ
into a new 3-CNF formula ψ such that φ is satisﬁable if and only if ψ is satisﬁable.

1.5 NP-Complete Problems
21
We omit the proof and refer the reader to textbooks on complexity theory. In the
following, we present a proof for 3-SAT ≤P
m KNAPSACKD.
Let φ be a 3-CNF formula that is of the form C1C2 · · · Cm, where each Cj is a
clause with three literals. Assume that φ contains Boolean variables v1, v2, . . ., vn.
We are going to deﬁne a list of 2n +2m integers c1, c2, . . ., c2n+2m, plus an integer
K. All integers ci and the integer K are of value between 0 and 10n+m. These
integers will satisfy the following property:
φ is satisﬁable ⇐⇒

∃x1, x2, . . ., x2n+2m ∈{0, 1}
 2n+2m

i=1
cixi = K.
(1.4)
Now, let S = K, si = ci for i = 1, 2, . . ., 2n+2m. Then, it follows that the formula
φ is satisﬁable if and only if the instance (S, K, s1, . . ., s2n+2m, c1, . . ., c2n+2m)
to the problem KNAPSACKD has the answer YES. Therefore, this construction is a
reduction function for 3-SAT ≤P
m KNAPSACKD.
We now describe the construction of these integers and prove that they satisfy
property (1.4). First, we note that each integer is between 0 and 10n+m, and so it
has a unique decimal representation of exactly n + m digits (with possible leading
zeroes). We will deﬁne each integer digit by digit, with the kth digit indicating the
kth most signiﬁcant digit. First, we deﬁne the ﬁrst n digits of K to be 1 and the last
m digits to be 3. That is,
K = 11 · · ·11



n
33 · · ·33



m
.
Next, for each i = 1, 2, . . ., n, we deﬁne the integer ci as follows: The ith digit and
the (n + j)th digits, for all 1 ≤j ≤m such that Cj contains the literal ¯vi, of ci are
1 and all other digits are 0. For instance, if ¯v3 occurs in C1, C5, and Cm, then
c3 = 00100 · · ·0



n
100010 · · ·01



m
.
Similarly, for i = 1, 2, . . ., n, the integer cn+i is deﬁned as follows: The ith digit
and the (n + j)th digits, for all 1 ≤j ≤m such that Cj contains the literal vi, of
cn+i are 1 and all other digits are 0.
Finally, for j = 1, 2, . . ., m, we deﬁne c2n+2j−1 = c2n+2j as follows: Their
(n + j)th digit is 1 and all other digits are 0. This completes the deﬁnition of the
integers.
Now, we need to show that these integers satisfy property (1.4). First, we observe
that for any k, 1 ≤k ≤n + m, there are at most ﬁve integers among ct’s whose kth
digit is nonzero, and each nonzero digit must be 1. Thus, to get the sum K, we must
choose, for each i = 1, 2, . . ., n, exactly one integer among ct’s whose ith digit is
1, and, for each j = 1, 2, . . ., m, exactly three integers whose (n + j)th digit is 1.
The ﬁrst part of this condition implies that we must choose, for each i = 1, 2, . . ., n,
exactly one of ci or cn+i.
Now, assume that φ has a truth assignment τ on variables v1, v2, . . ., vn. We
deﬁne the sequence (x1, x2, . . ., x2n+2m) as follows:

22
Introduction
(1) For each i = 1, 2, . . ., n, let xn+i = 1 −xi = τ(vi).
(2) For each j = 1, 2, . . ., m, deﬁne x2n+2j−1 and x2n+2j as follows: If τ sat-
isﬁes all three literals of Cj, then x2n+2j−1 = x2n+2j = 0; if τ satisﬁes
exactly two literals of Cj, then x2n+2j−1 = 1 and x2n+2j = 0; and if τ
satisﬁes exactly one literal of Cj, then x2n+2j−1 = x2n+2j = 1.
Then it is easy to verify that 2n+2m
i=1
cixi = K.
Next, assume that there exists a sequence (x1, x2, . . . , x2n+2m) ∈{0, 1}2n+2m
such that 2n+2m
i=1
cixi = K. Then, from our earlier observation, we see that ex-
actly one of xi and xn+i has value 1. Deﬁne τ(vi) = xn+i. We claim that τ satisﬁes
each clause Cj, 1 ≤j ≤m. Since the (n + j)th digit of the sum 2n+2m
i=1
cixi
is equal to 3, and since there are at most two integers among the last 2m integers
whose (n+j)th digit is 1, there must be an integer k ≤2n such that xk = 1 and the
(n + j)th digit of ck is 1. Suppose 1 ≤k ≤n; then it means that τ(vk) = 0, and Cj
contains the literal ¯vk. Thus, τ satisﬁes Cj. On the other hand, if n + 1 ≤k ≤2n,
then we know that τ(vk−n) = 1, and Cj contains the literal vk−n; and so τ also
satisﬁes Cj. This completes the proof of property (1.4).
Finally, we remark that the above construction of these integers from the formula
φ is apparently polynomial-time computable. Thus, this reduction is a polynomial-
time reduction.
□
In addition to the above two problems, thousands of problems from many seem-
ingly unrelated areas have been proven to be NP-complete in the past four decades.
These results demonstrate the importance and universality of the concept of NP-
completeness. In the following, we list a few problems that are frequently used to
prove a new problem being NP-complete.
VERTEX COVER (VC): Given an undirected graph G = (V, E) and a
positive integer K, determine whether there is a set C ⊆V of size ≤K
such that, for every edge {u, v} ∈E, C ∩{u, v} ̸= ∅. (Such a set C is
called a vertex cover of G.)
HAMILTONIAN CIRCUIT (HC): Given an undirected graph G = (V ,
E), determine whether there is a simple cycle that passes through each
vertex exactly once. (Such a cycle is called a Hamiltonian circuit.)
PARTITION: Given n positive integers a1, a2, . . ., an, determine whe-
ther there is a partition of these integers into two parts that have the
equal sum. (This is a subproblem of KNAPSACK.)
SET COVER (SC): Given a family C of subsets of I = {1, 2, . . ., n}
and a positive integer K, determine whether there is a subfamily C′ of
C of at most K subsets such that 
A∈C′ A = I.
For instance, from the problem HC, we can easily prove that (the decision ver-
sions of) the following optimization problems are also NP-complete. We leave their
proofs as exercises.

1.6 Performance Ratios
23
TRAVELING SALESMAN PROBLEM (TSP): Given a complete graph
and a distance function that gives a positive integer as the distance be-
tween every pair of vertices, ﬁnd a Hamiltonian circuit with the mini-
mum total distance.
MAXIMUM HAMILTONIAN CIRCUIT (MAX-HC): Given a complete
graph and a distance function, ﬁnd a Hamiltonian circuit with the max-
imum total distance.
MAXIMUM DIRECTED HAMILTONIAN PATH (MAX-DHP): Given a
complete directed graph and a distance function, ﬁnd a Hamiltonian
path with the maximum total distance. (A Hamiltonian path is a simple
path that passes through each vertex exactly once.)
1.6
Performance Ratios
As we pointed out earlier, the two most important criteria in the study of approxi-
mation algorithms are efﬁciency and the performance ratio. By efﬁciency, we mean
polynomial-time computability. By performance ratio, we mean the ratio of the ob-
jective function values between the approximate and optimal solutions. More pre-
cisely, for any optimization problem Π and any input instance I, let opt(I) denote
the objective function value of the optimal solution to instance I, and A(I) the ob-
jective function value produced by an approximation algorithm A on instance I.
Then, for a minimization problem, we deﬁne the performance ratio of an approxi-
mation algorithm A to be
r(A) = sup
I
A(I)
opt(I)
and, for a maximization problem, we deﬁne it to be
r(A) = sup
I
opt(I)
A(I) ,
where I ranges over all possible input instances. Thus, for any approximation algo-
rithm A, r(A) ≥1, and, in general, the smaller the performance ratio is, the better
the approximation algorithm is.
For instance, consider the maximization problem KNAPSACK again. Let opt(I)
be the maximum value of the objective function on input instance I, and cG(I) and
cGG(I) the objective function values obtained by Algorithms 1.B and 1.C, respec-
tively, on instance I. Then, by Theorems 1.1 and 1.2, the performance ratios of these
two algorithms (denoted by A1B and A1C) are
r(A1B) = sup
I
opt(I)
cG(I) ≤2
and
r(A1C) = sup
I
opt(I)
cGG(I) ≤1 + ε.

24
Introduction
That is, both of these algorithms achieve a constant approximation ratio, but Algo-
rithm 1.C has a better ratio.
As another example, consider the famous TRAVELING SALESMAN PROBLEM
(TSP) deﬁned in the last section. We assume that the distance between any two
vertices is positive. In addition,we assume that the given distance function d satisﬁes
the triangle inequality (abbr. Δ-inequality); that is,
d(a, b) + d(b, c) ≥d(a, c),
for any three vertices a, b, and c. Then, there is a simple approximation algorithm for
TSP that ﬁnds a tour (i.e., a Hamiltonian circuit) with the total distance within twice
of the optimum. This algorithm uses two basic linear-time algorithms on graphs:
Minimum Spanning-Tree Algorithm: Given a connected graph G
with a distance function d on all edges, this algorithm ﬁnds a mini-
mum spanning tree T of the graph G. (T is a minimum spanning tree of
G if T is a connected subgraph of G with the minimum total distance.)
Euler Tour Algorithm: Given a connected graph G in which each ver-
tex has an even degree, this algorithm ﬁnds an Euler tour, i.e., a cycle
that passes through each edge in G exactly once.
Algorithm 1.G (Approximation Algorithm for TSP with Δ-Inequality)
Input: A complete graph G = (V, E), where V = {1, 2, . . ., n}, and a distance
function d : V × V →N that satisﬁes the triangle inequality.
(1) Find a minimum spanning tree T of G.
(2) Change each edge e in T to two (parallel) edges between the same pair of
vertices. Call the resulting graph H.
(3) Find an Euler tour P of H.
(4) Output the Hamiltonian circuit Q that is obtained by visiting each vertex once
in the order of their ﬁrst occurrence in P . (That is, Q is the shortcut of P that
skips a vertex if it has already been visited. See Figure 1.4.)
We ﬁrst note that, after step (2), each vertex in graph H has an even degree and
hence the Euler Tour Algorithm can ﬁnd an Euler tour of H in linear time. Thus,
Algorithm 1.G is well deﬁned. Next, we verify that its performance ratio is bounded
by 2. This is easy to see from the following three observations:
(a) The total distance of the minimum spanning tree T must be less than that of
any Hamiltonian circuit C, since we can obtain a spanning tree by removing
an edge from C.
(b) The total distance of P is exactly twice that of T, and so at most twice that of
the optimal solution.

1.6 Performance Ratios
25
(c)
(b)
(a)
8
7
6
5
4
3
2
1
1
2
4
3
14
13
12
11
10
9
8
7
6
5
Figure 1.4:
Algorithm 1.G: (a) the minimum spanning tree; (b) the Euler tour;
and (c) the shortcut.
(c) By the triangle inequality, the total distance of the shortcut Q is no greater
than that of tour P .
Christoﬁdes [1976] introduced a new idea into this approximation algorithm and
improved the performance ratio to 3/2. This new idea requires another basic graph
algorithm:
Minimum Perfect Matching Algorithm: Given a complete graph G
of an even number of vertices and a distance function d on edges, this
algorithm ﬁnds a perfect matching with the minimum total distance. (A
matching of a graph is a subset M of the edges such that each vertex
occurs in at most one edge in M. A perfect matching of a graph is a
matching M with each vertex occurring in exactly one edge in M.)
Algorithm 1.H (Christoﬁdes’s Algorithm for TSP with Δ-Inequality)
Input: A complete graph G = (V, E), where V = {1, 2, . . ., n}, and a distance
function d : V × V →N that satisﬁes the triangle inequality.
(1) Find a minimum spanning tree T = (V, ET) of G.
(2) Let V ′ be the set of all vertices in T of odd degrees;
Let G′ = (V ′, E′) be the subgraph of G induced by vertex set V ′;
Find a minimum perfect matching M for G′;
Add the edges in M to tree T (with possible parallel edges between
two vertices) to form a new graph H′.
[See Figure 1.5(b).]
(3) Find an Euler tour P ′ of H′.
(4) Output the shortcut Q of the tour P ′ as in step (4) of Algorithm 1.G.
It is clear that after adding the matching M to tree T, each vertex in graph H′
has an even degree. Thus, step (3) of Algorithm 1.H is well deﬁned. Now, we note

26
Introduction
(b)
(a)
1
11
10
9
8
6
5
4
3
2
7
8
7
1
2
3
(c)
6
5
4
Figure 1.5:
Christoﬁdes’s approximation: (a) the minimum spanning tree; (b) the
minimum matching (shown in broken lines) and the Euler tour; and (c) the shortcut.
that the total distance of the matching M is at most one half of that of a minimum
Hamiltonian circuit C′ in G′, since we can remove alternating edges from C′ to
obtain a perfect matching. Also, by the triangle inequality, the total distance of the
minimum Hamiltonian circuit in G′ is no greater than that of the minimum Hamil-
tonian circuit in G. Therefore, the total distance of the tour P ′, as well as that of Q,
is at most 3/2 of the optimal solution. That is, the performance ratio of Algorithm
1.H is bounded by 3/2.
Actually, the performance ratio of Christoﬁdes’s approximation can be shown to
be exactly 3/2. Consider the graph G of Figure 1.6. Graph G has 2n + 1 vertices
v0, v1, . . ., v2n on the Euclidean space R2, with the distance d(vi, vi+1) = 1 for
i = 0, 1, . . ., 2n −1, and d(vi, vi+2) = 1 + a for i = 0, 1, . . ., 2n −2, where
0 < a < 1/2. It is clear that the minimum spanning tree T of G is the path from
v0 to v2n containing all edges of distance 1. There are only two vertices, v0 and
v2n, having odd degrees in tree T. Thus, the traveling salesman tour produced by
Christoﬁdes’s algorithm is the cycle (v0, v1, v2, . . ., v2n, v0), whose total distance
is 2n + n(1 + a) = 3n + na. Moreover, it is easy to see that the minimum traveling
salesman tour consists of all horizontal edges plus the two outside nonhorizontal
edges, whose total distance is (2n −1)(1 + a) + 2 = 2n + 1 + (2n −1)a. So, if we
let A1H denote Christoﬁdes’s algorithm, we get, in this instance I,
A1H(I)
opt(I) =
3n + na
2n + 1 + (2n −1)a,
which approaches 3/2 as a goes to 0 and n goes to inﬁnity. It follows that r(A1H) =
3/2.
Theorem 1.7 For the subproblem of TSP with the triangle inequality, as well as
the subproblem of TSP on Euclidean space, the Christoﬁdes’s approximation A1H
has the performance ratio r(A1H) = 3/2.
For simplicity, we say an approximation algorithm A is an α-approximation if
r(A) ≤α for some constant α ≥1. Thus, we say Christoﬁdes’s algorithm is a

1.6 Performance Ratios
27
1+
1
v
v
v
v
v
v
1
1
1
1
1
1
1
1
1+
1+
1+
1+
1+
1+
a
2
a
n
a
0
a
1
a
3
a
2
a
2n
−1
Figure 1.6:
A worst case of Christoﬁdes’s approximation.
(3/2)-approximation for TSP with the triangle inequality, but not an α-approxima-
tion for any α < 3/2.
An approximation algorithm with a constant performance ratio is also called a
bounded approximation or a linear approximation. An optimization problem Π is
said to have a polynomial-time approximation scheme (PTAS) if, for any k > 0,
there exists a polynomial-time approximation algorithm Ak for Π with performance
ratio r(Ak) ≤1 + 1/k. Furthermore, if the running time of the algorithm Ak in the
approximation scheme is a polynomial function in n+1/k, where n is the input size,
then the scheme is called a fully polynomial-time approximation scheme (FPTAS).
For instance, the generalized greedy algorithm (Algorithm 1.C) is a PTAS, and the
polynomial tradeoff approximation (Algorithm 1.D) is an FPTAS for KNAPSACK.
In this book, our main concern is to ﬁnd efﬁcient approximations to intractable
problems with the best performance ratios. However, some optimization problems
are so hard that they don’t even have any polynomial-time bounded approximations.
In these cases, we also need to prove that such approximations do not exist. Since
most optimization problems are NP-complete, they hence have polynomial-timeop-
timal algorithms if P = NP. So, when we try to prove that a bounded approximation
does not exist, we must assume that P ̸= NP. Very often, we simply prove that the
problem of ﬁnding a bounded approximation (or, an α-approximation for some ﬁxed
constant α) itself is NP-hard. The following is a simple example. We will present a
more systematic study of this type of inapproximability results in Chapter 10.
Theorem 1.8 If P ̸= NP, then there is no polynomial-time approximation algo-
rithm for TSP (without the restriction of the triangle inequality) with a constant
performance ratio.
Proof. For any ﬁxed integer K > 1, we will construct a reduction from the problem
HC to the problem of ﬁnding a K-approximation for TSP.5 That is, we will construct
a mapping from each instance G of the problem HC to an instance (H, d) of TSP,
such that the question of whether G has a Hamiltonian circuit can be determined
from any traveling salesman tour for (H, d) whose total distance is within K times
of the length of the shortest tour.
5Note that TSP is not a decision problem. So, the reduction here has a more general form than that
deﬁned in Section 1.5.
www.allitebooks.com

28
Introduction
For any graph G = (V, E), with |V | = n, let H be the complete graph over the
vertex set V . Deﬁne the distance between two vertices u, v ∈V as follows:
d(u, v) =
1,
if {u, v} ∈E,
n(K + 1),
otherwise.
Now, assume that C is a traveling salesman tour of the instance (H, d) whose
total distance is at most K times of the length of the shortest tour. If the total distance
of C is less than n(K + 1), then we know that all edges in C are of distance 1 and
so they are all in E. Thus, C is a Hamiltonian circuit of G. On the other hand, if
the total distance of C is greater than or equal to n(K + 1), this implies that the
minimum traveling salesman tour has total distance at least n(K + 1)/K and hence
greater than n. It implies that the minimum traveling salesman tour must contain an
edge not in E. Thus, G has no Hamiltonian circuit.
Thus, if there is a polynomial-time K-approximation for TSP, we can then use it
to solve the problem HC, which is NP-complete. It follows that P = NP.
□
Exercises
1.1 Prove that Algorithm 1.A always ﬁnds the optimal solution for KNAPSACK.
More precisely, prove by induction that if there is a subset A ⊆{1, . . ., i} such that

k∈A ck = j and 
k∈A sk ≤S, then the value a(i, j) obtained at the end of step
(3) of Algorithm 1.A satisﬁes a(i, j) ̸= nil and a(i, j) has the minimum total cost

k∈a(i,j) sk among such sets A.
1.2 Formulate the following logic puzzles into satisﬁability instances and solve
them:
(a) Three men named Lewis, Miller, and Nelson ﬁll the positions of accountant,
cashier, and clerk in a department store. If Nelson is the cashier, Miller is
the clerk. If Nelson is the clerk, Miller is the accountant. If Miller is not the
cashier, Lewis is the clerk. If Lewis is the accountant, Nelson is the clerk.
What is each man’s job?
(b) Messrs. Spinnaker, Buoy, Luff, Gybe, and Windward are yacht owners.
Each has a daughter, and each has named his yacht after the daughter of
one of the others. Mr. Spinnaker’s yacht, the Iris, is named after Mr. Buoy’s
daughter. Mr. Buoy’sown yacht is the Daffodil; Mr. Windward’s yacht is the
Jonquil; Mr. Gybe’s, the Anthea. Daffodil is the daughter of the owner of
the yacht that is named after Mr. Luff’s daughter. Mr. Windward’s daughter
is named Lalage. Who is Jonquil’s father?
1.3 For any Boolean function f, Ff(y) is deﬁned as in Section 1.3. Prove that
for y ∈{0, 1}n, 0 ≤Ff(y) ≤1.
1.4 For a 3-CNF formula φ = C1C2 · · ·Cm over Boolean variables x1, x2, . . .,
xn, let x be the vector (x1, x2, . . ., xn) in {0, 1}n. For each variable xj, 1 ≤j ≤n,

Exercises
29
deﬁne a corresponding real variable yj, and let y be the vector (y1, y2, . . ., yn) in
Rn. Deﬁne a function f1 : Rn →R as follows: First, for each pair (i, j), with
1 ≤i ≤m and 1 ≤j ≤n, deﬁne a literal function
qij(yj) =
⎧
⎪
⎨
⎪
⎩
(yj −1)2,
if xj is in clause Ci,
(yj + 1)2,
if ¯xj is in clause Ci,
1,
neither xj nor ¯xj is in Ci,
and, for each 1 ≤i ≤m, deﬁne a clause function ci(y) = n
j=1 qij(yj). Finally,
deﬁne f1 to be the sum of the clause functions: f1(y) = m
i=1 ci(y).
Deﬁne a correspondence between x and y as follows:
xj =
⎧
⎪
⎨
⎪
⎩
1,
if yj = 1,
0,
if yj = −1,
undeﬁned,
otherwise.
Then it is clear that φ is satisﬁable if and only if the minimum value of f1(y) is 0.
Now, deﬁne f(y) = f1(y)+n
j=1(y2
j −1)2, and consider the following minimiza-
tion problem:
minimize f(y).
Show that the objective function f(y) satisﬁes the following properties:
(a) There exists y such that f(y) = 0 if and only if there exists y such that
f(y) < 1.
(b) At every minimum point y∗, f(y∗) is strictly convex.
1.5 Consider the greedy algorithm for KNAPSACK that selects the most valuable
item ﬁrst. That is, in Algorithm 1.B, replace the ordering c1/s1 ≥c2/s2 ≥· · · ≥
cn/sn by c1 ≥c2 ≥· · · ≥cn. Show that this greedy algorithm is not a linear
approximation.
1.6 Give an example to show that the performance ratio of Algorithm 1.G for
TSP with the triangle inequality cannot be any constant smaller than 2.
1.7 When the distance function in TSP is allowed to be asymmetric, i.e., possibly
d(u, v) ̸= d(v, u), the problem is called DIRECTED TSP. Give an example to show
that Christoﬁdes’s approximation (Algorithm 1.H) does not work for DIRECTED
TSP with triangle inequality.
1.8
(a) Suppose there exists an algorithmthat can compute the maximum value
opt of the objective function for KNAPSACK. Can you use this algorithm
as a subroutine to design an algorithm computing an optimal solution for
KNAPSACK (i.e., the 0-1 vector (x∗
1, x∗
2, . . ., x∗
n) such that n
i=1 cix∗
i =
opt) in polynomial time, provided that the time spent by the subroutine is
not counted?

30
Introduction
(b) Suppose there exists an algorithmthat can compute the distance of the short-
est tour for TSP. Can you use this algorithm as a subroutine to design an
algorithm computing an optimal solution for TSP (i.e., the shortest tour)
in polynomial time, provided that the time spent by the subroutine is not
counted?
(c) Suppose there exists an algorithm that can compute a value within a factor
α from the distance of the shortest tour for TSP, where α is a constant. Can
you use this algorithm as a subroutine to design an algorithm computing an
optimal solution for TSP in polynomial time, provided that the time spent
by the subroutine is not counted?
1.9 Show that for any ε
>
0, there exists a polynomial-time (2 + ε)-
approximation for MAX-HC and there exists a polynomial-time 2-approximation
for MAX-DHP. [Hint: Use the polynomial-time Maximum Matching Algorithm.]
1.10 Consider the following problem:
MINIMUM VERTEX COVER (MIN-VC): Given an undirected graph G,
ﬁnd a vertex cover of the minimum size.
(a) Design a polynomial-time 2-approximation for the problem [Hint: Use the
polynomial-time Maximum Matching Algorithm.]
(b) Show that MIN-VC in bipartite graphs can be solved in polynomial time.
1.11 A subset S of vertices in a graph G = (V, E) is independent if no edges
exist between any two vertices in S.
(a) Show that I is a maximum independent set of graph G = (V, E) if and only
if V −I is a minimum vertex cover of G.
(b) Give an example to show that if C is a vertex cover within a factor of 2 from
the minimum, then V −C is still an independent set but may not be within
a factor of 2 from the maximum.
1.12 Find a polynomial-time 2-approximation for the following problem:
STEINER MINIMUM TREE (SMT): Given a graph G = (V, E) with a
distance function on E, and a subset S ⊆V , compute a shortest tree
interconnecting the vertices in S.
1.13 There are n jobs J1, J2, . . ., Jn and m identical machines. Each job Ji,
1 ≤i ≤n, needs to be processed in a machine without interruption for a time
period pi. Consider the problem of ﬁnding a scheduling to ﬁnish all jobs with the
m machines in the minimum time. Graham [1966] proposed a simple algorithm
for this problem: Put n jobs in an arbitrary order; whenever a machine becomes
available, assign it the next job. Show that Graham’s algorithm is a polynomial-time
2-approximation.

Exercises
31
1.14 There are n students in a late-night study group. The time has come to order
pizzas. Each student has his or her own list of preferred toppings (e.g., mushroom,
pepperoni, onions, garlic, sausage, etc.), and each pizza may have only one topping.
Answer the following questions:
(a) If each student wants to eat at least one half of a pizza with the topping on
his or her preferred list, what is the complexity of computing the minimum
number of pizzas to order to make everyone happy?
(b) If everyone wants to eat at least one third of a pizza with the topping on
his or her preferred list, what is the complexity of computing the minimum
number of pizzas to order to make everyone happy?
1.15 Assume that C is a collection of subsets of a set X. We say a set Y ⊆X
hits a set C ∈C if Y ∩C ̸= ∅. A set Y ⊆X is a hitting set for C if Y hits every set
C ∈C. Show that the following problems are NP-hard:
(a) MINIMUM HITTING SET (MIN-HS): Given a collection C of subsets of a
set X, ﬁnd a minimum hitting set Y for C.
(b) Given a collection C of subsets of a set X, ﬁnd a subset Y of X of the
minimum size such that all sets Y ∩C for C ∈C are distinct.
(c) Given two collections C and D of subsets of X and a positive integer d, ﬁnd
a subset A ⊆X of size |A| ≤d that minimizes the total number of subsets
in C not hit by A and subsets in D hit by A.
1.16 Show that the following problems are NP-hard:
(a) Given a graph G = (V, E) and a positive integer m, ﬁnd the minimum
subset A ⊆V such that A covers at least m edges and the complement of
A has no isolated vertices.
(b) Given a 2-connected graph G = (V, E) and a set A ⊆V , ﬁnd the minimum
subset B ⊆V such that A ∪B induces a 2-connected subgraph.
1.17 Show that the following problem is NP-complete:
Given two disjoint sets X and Y , and a collection C of subsets of X∪Y ,
determine whether C can be partitioned into two disjoint subcollections
covering X and Y , respectively.
1.18 Let k > 0. A collection C of subsets of a set X is a k-set cover if C can be
partitioned into k disjoint subcollections each being a set cover for X.
(a) Consider the following problem:
k-SET COVER (k-SC): Given a collection C of subsets of a set
X, determine whether it is a k-set cover.
Show that the problem 2-SC is NP-complete.

32
Introduction
(b) Show that the following problem is not polynomial-time 2-approximable
unless P = NP:
Given a collection C of subsets of a set X, compute the minimum
k such that C is a k-set cover.
1.19 For each 3-CNF formula F , we deﬁne a graph G(F ) as follows: The vertex
set of G(F ) consists of all clauses and all literals in F . An edge exists in G(F )
between a clause C and a literal x if and only if x belongs to C, and an edge exists
between two literals x and y if and only if x = ¯y. A 3-CNF formula is called a
planar formula if G(F ) is a planar graph. Show that the following problems are
NP-complete:
(a) NOT-ALL-EQUAL 3-SAT: Given a 3-CNF F , determine whether F has an
assignment which assigns, for each clause C, value 1 to a literal in C and
value 0 to another literal in C.
(b) ONE-IN-THREE 3-SAT: Given a 3-CNF F , determine whether F has an
assignment which, for each clause C, assigns value 1 to exactly one literal
in C.
(c) PLANAR 3-SAT: Given a planar 3-CNF formula F , determine whether F is
satisﬁable.
1.20 A subset D of vertices in a graph G = (V, E) is called a dominating set if
every vertex v ∈V either is in D or is adjacent to a vertex in D.
(a) Show that the problem of computing the minimum dominating set for a
given graph is NP-hard.
(b) Show that the problem of determining whether there exist two disjoint dom-
inating sets for a given graph is polynomial-time solvable.
(c) Show that the problem of determining whether there exist three disjoint
dominating sets for a given graph is NP-complete. [Hint: Use NOT-ALL-
EQUAL 3-SAT.]
(d) Show that the problem of computing the maximum number of disjoint dom-
inating sets for a given graph is not (3/2)-approximable in polynomial time
unless P = NP.
1.21 A graph is said to be k-colorable if its vertices can be partitioned into k
disjoint independent sets.
(a) Show that the problem of deciding whether a given graph is 2-colorable or
not is polynomial-time solvable.
(b) Show that the problem of deciding whether a given graph is 3-colorable or
not is NP-complete.
(c) Show that the problem of computing, for a given graph G, the minimum k
such that G is k-colorable is not (3/2)-approximable unless P = NP.

Historical Notes
33
1.22 A subset C of vertices of a graph G = (V, E) is a clique if the subgraph
of G induced by C is a complete graph. Study the computational complexity of the
following problems:
(a) For a given graph, compute the maximum number of disjoint vertex covers.
(b) For a given graph, compute the minimum number of disjoint cliques such
that their union contains all vertices.
Historical Notes
Graham [1966] initiated the study of approximations using the performance ratio to
evaluate the approximation algorithms. However, the importance of this work was
not fully understood until Cook [1971] and Karp [1972] established the notion of
NP-completeness and its ubiquitous existence in combinatorial optimization. With
the theory of NP-completeness as its foundation, the study of approximation algo-
rithms took off quickly in the 1970s. Garey and Johnson [1979] gave an account of
the development in this early period.
The PTAS for KNAPSACK belongs to Sahni [1975]. The ﬁrst FPTAS for KNAP-
SACK was discovered by Ibarra and Kim [1975]. Since then, many different FP-
TASs, including Algorithm 1.D of Section 1.1, have been found for KNAPSACK.
Christoﬁdes [1976] found a polynomial-time (3/2)-approximation for TSP with the
triangleinequality. So far, nobody has found a better one in terms of the performance
ratio.

  

2
Greedy Strategy
Someone reminded me that I once said, “Greed is good.”
Now it seems that it’s legal.
— Gordon Gekko (in Wall Street: Money Never Sleeps)
I think greed is healthy. You can be greedy
and still feel good about yourself.
— Ivan Boesky
The greedy strategy is a simple and popular idea in the design of approximation
algorithms. In this chapter, we study two general theories, based on the notions
of independent systems and submodular potential functions, about the analysis of
greedy algorithms, and present a number of applications of these methods.
2.1
Independent Systems
The basic idea of a greedy algorithm can be summarized as follows:
(1) We deﬁne an appropriate potential function f(A) on potential solution sets
A.
(2) Starting with A = ∅, we grow the solution set A by adding to it, at each stage,
an element that maximizes (or, minimizes) the value of f(A ∪{x}), until
f(A) reaches the maximum (or, respectively, minimum) value.
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9_2, 
© Springer Science+Business Media, LLC 2012 
35

36
Greedy Strategy
We ﬁrst consider a simple setting, in which the potential function is the same as
the objective function. In the following, we write N+ to denote the set of positive
integers, and R+ the set of nonnegative real numbers.
Let E be a ﬁnite set and I a family of subsets of E. The pair (E, I) is called an
independent system if
(I1) I ∈I and I′ ⊆I ⇒I′ ∈I.
Each subset in I is called an independent subset. Let c : E →R+ be a nonneg-
ative function. For every subset F of E, deﬁne c(F) = 
e∈F c(e). Consider the
following problem:
MAXIMUM INDEPENDENT SUBSET (MAX-ISS): Given an indepen-
dent system (E, I) and a cost function c : E →R+,
maximize
c(I)
subject to
I ∈I.
We remark that the family I has, in general, an exponential size and cannot be
given explicitly (and, hence, an exhaustive search for the maximum c(I) is imprac-
tical). In most applications, however, the system (E, I) is given in such a way that
the condition of whether I ∈I can be determined in polynomial time. Under this
assumption, the following greedy algorithm, which uses the objective function c as
the potential function, works in polynomial time.
Algorithm 2.A (Greedy Algorithm for MAX-ISS)
Input: An independent system (E, I) and a cost function c : E →R+.
(1) Sort all elements in E = {e1, e2, . . ., en} in the decreasing order of c. Without
loss of generality, assume that c(e1) ≥c(e2) ≥· · · ≥c(en).
(2) Set I ←∅.
(3) For i ←1 to n do
if I ∪{ei} ∈I then I ←I ∪{ei}.
(4) Output IG ←I.
For any instance (E, I, c) of the problem MAX-ISS, let I∗be its optimal so-
lution and IG the independent set produced by Algorithm 2.A. We will see that
c(IG)/c(I∗) has a simple upper bound that is independent of the cost function c.
For any F ⊆E, a set I ⊆F is called a maximal independent subset of F if no
independent subset of F contains I as a proper subset. For any set I ⊆E, let |I|
denote the number of elements in I. Deﬁne
u(F) = min{|I| | I is a maximal independent subset of F},
v(F) = max{|I| | I is an independent subset of F }.
(2.1)

2.1 Independent Systems
37
Theorem 2.1 The following inequality holds for any independent system (E, I)
and any function c : E →R+:
1 ≤c(I∗)
c(IG) ≤max
F⊆E
v(F)
u(F).
Proof. Assume that E = {e1, e2, . . ., en}, and c(e1) ≥· · · ≥c(en). Denote Ei =
{e1, . . ., ei}. We claim that Ei ∩IG is a maximal independent subset of Ei. To
see this, we assume, by way of contradiction, that this is not the case; that is, there
exists an element ej ∈Ei \ IG such that (Ei ∩IG) ∪{ej} is independent. Now,
consider the jth iteration of the loop of step (3) of Algorithm 2.A. The set I at the
beginning of the jth iteration is a subset of IG, and so I ∪{ej} must be a subset of
(Ei ∩IG) ∪{ej} and, hence, is an independent set. Therefore, the algorithm should
have added ej to I in the jth iteration. This contradicts the assumption that ej ̸∈IG.
From the above claim, we see that
|Ei ∩IG| ≥u(Ei).
Moreover, since Ei ∩I∗is independent, we have
|Ei ∩I∗| ≤v(Ei).
Now, we express c(IG) and c(I∗) in terms of |Ei∩IG| and |Ei∩I∗|, respectively.
We note that for each i = 1, 2, . . ., n,
|Ei ∩IG| −|Ei−1 ∩IG| =
 1,
if ei ∈IG,
0,
otherwise.
Therefore,
c(IG) =

ei∈IG
c(ei) = c(e1) · |E1 ∩IG| +
n

i=2
c(ei) · (|Ei ∩IG| −|Ei−1 ∩IG|)
=
n−1

i=1
|Ei ∩IG| · (c(ei) −c(ei+1)) + |En ∩IG| · c(en).
Similarly,
c(I∗) =
n−1

i=1
|Ei ∩I∗| · (c(ei) −c(ei+1)) + |En ∩I∗| · c(en).
Denote ρ = maxF ⊆E v(F )/u(F ). Then we have
c(I∗) ≤
n−1

i=1
v(Ei) · (c(ei) −c(ei+1)) + v(En) · c(en)
≤
n−1

i=1
ρ · u(Ei) · (c(ei) −c(ei+1)) + ρ · u(En) · c(en) ≤ρ · c(IG).
□
www.allitebooks.com

38
Greedy Strategy
Figure 2.1:
Two maximal independent subsets I and J for the problem MAX-HC
(the thick lines indicate edges of I, the thin curves and dotted curves indicate the
edges of J, and the dotted curves indicate edges shared by I and J).
We note that the ratio ρ = maxF ⊆E v(F )/u(F ) depends only on the structure
of the family I and is independent of the cost function c. Thus, this upper bound
is often easy to calculate. We demonstrate the application of this property in two
examples.
First, consider the problem MAX-HC deﬁned in Section 1.5. Each instance of
this problem consists of n vertices and a distance table on these n vertices. The
problem is to ﬁnd a Hamiltonian circuit of the maximum total distance. Let E be
the edge set of the complete graph on the n vertices. Let I be the family of subsets
of E such that I ∈I if and only if I is either a Hamiltonian circuit or a union of
disjoint paths (i.e., paths that do not share any common vertex). Clearly, (E, I) is an
independent system and whether or not I is in I can be determined in polynomial
time. That is, the problem MAX-HC is a special case of the problem MAX-ISS, and
Algorithm 2.A runs on MAX-HC in polynomial time.
Lemma 2.2 Let (E, I) be the independent system deﬁned above, and F a subset of
E. Suppose that I and J are two maximal independent subsets of F . Then |J| ≤
2|I|.
Proof. For i = 1, 2, let Vi denote the set of vertices of degree i in I. That is, V1 is
the set of end vertices in I and V2 is the set of intermediate vertices in I. Clearly,
|I| = |V2| + |V1|/2. Since I is a maximal independent subset of F , every edge in
F either is incident on a vertex in V2 or connects two endpoints of a path in I. Let
J2 be the set of edges in J incident on a vertex in V2, and J1 = J \ J2. Since J is
an independent set, at most two edges in J2 could be incident on each vertex in V2.
That is, |J2| ≤2|V2|. Moreover, every edge in J1 must connect two endpoints in V1
in a path of I, and at most one edge in J1 could be incident on each vertex in V1.
Therefore, |J1| ≤|V1|/2. (Figure 2.1 shows an example of maximal independent
subsets I and J.) Together, we have
|J| = |J1| + |J2| ≤|V1|
2
+ 2|V2| ≤2|I|.
□
Theorem 2.3 When it is applied to the problem MAX-HC, Algorithm 2.A is a
polynomial-time 2-approximation.

2.1 Independent Systems
39
Figure 2.2:
Two maximal independent subsets I and J for the problem MAX-
DHP.
A similar application gives us a rather weaker performance ratio for the problem
MAX-DHP, also deﬁned in Section 1.5. An instance of this problem consists of n
vertices and a directed distance table on these n vertices. The problem is to ﬁnd a
directed Hamiltonian path of the maximum total distance. Let E be the set of edges
of the complete directed graph on the n vertices. Let I be the family of subsets of
E such that I ∈I if and only if I is a union of disjoint paths. Clearly, (E, I) is an
independent system, and whether or not I is in I can be determined in polynomial
time.
Lemma 2.4 Let (E, I) be the independent system deﬁned as above, and F a subset
of E. Suppose that I and J are two maximal independent subsets of F . Then |J| ≤
3|I|.
Proof. Since I is a maximal independent subset of F , every edge in F must have
one of the following properties:
(1) It shares a head with an edge in I;
(2) It shares a tail with an edge in I; or
(3) It connects from the head to the tail of a maximal path in I.
(Figure 2.2 shows an example of two maximal independent subsets I and J.)
Let J1, J2, and J3 be the subsets of edges in J that have properties (1), (2) and
(3), respectively. Since J is an independent subset, each edge in I can share its head
(or its tail) with at most one edge in J, and each maximal path in I can be connected
from the head to the tail by at most one edge in J. That is, |Ji| ≤|I|, for i = 1, 2, 3.
Thus,
|J| = |J1| + |J2| + |J3| ≤3|I|.
□
Theorem 2.5 When it is applied to the problem MAX-DHP, Algorithm 2.A is a
polynomial-time 3-approximation.
The following simple example shows that the performance ratio given by the
above theorem cannot be improved.
Example 2.6 Consider the following distance table on four vertices, in which the
parameter ε is a positive real number less than 1:

40
Greedy Strategy
a
b
c
d
a
0
1
ε
ε
b
ε
0
1
ε
c
ε
1 + ε
0
1
d
ε
ε
ε
0
It is clear that the longest Hamiltonian path has distance 3 and yet the greedy algo-
rithm selects the edge (c, b) ﬁrst and gets a path of total distance 1 + 3ε. The per-
formance ratio is, thus, equal to 3/(1 +3ε), which approaches 3 when ε approaches
zero.
□
2.2
Matroids
Let E be a ﬁnite set and I a family of subsets of E. The pair (E, I) is called a
matroid if
(I1) I ∈I and I′ ⊆I ⇒I′ ∈I; and
(I2) For any subset F of E, u(F) = v(F ),
where u(F ) and v(F ) are the two functions deﬁned in (2.1). Thus, an independent
system (E, I) is a matroid if and only if, for any subset F of E, all maximal inde-
pendent subsets of F have the same cardinality. From Theorem 2.1, we know that
Algorithm 2.A produces an optimal solution for the problem MAX-ISS if the input
instance (E, I) is a matroid. The next theorem shows that this property actually
characterizes the notion of matroids.
Theorem 2.7 An independent system (E, I) is a matroid if and only if for every
nonnegative function c : E →R+, the greedy Algorithm 2.A produces an optimal
solution for the instance (E, I, c) of MAX-ISS.
Proof. The “only if” part is just Theorem 2.1. Now, we prove the “if” part. Suppose
that (E, I) is not a matroid. Then we can ﬁnd a subset F of E such that F has two
maximal independent subsets I and I′ with |I| > |I′|. Deﬁne, for any e ∈E,
c(e) =
⎧
⎨
⎩
1 + ϵ,
if e ∈I′,
1,
if e ∈I \ I′,
0,
if e ∈E \ (I ∪I′),
where ϵ is a positive number less than 1/|I′| (so that c(I) > c(I′)). Clearly, for this
cost function c, Algorithm 2.A produces the solution set I′, which is not optimal. □
The following are some examples of matroids.
Example 2.8 Let E be a ﬁnite set of vectors and I the family of linearly indepen-
dent subsets of E. Then the size of the maximal independent subset of a subset
F ⊆E is the rank of F and is unique. Thus, (E, I) is a matroid.
□

2.2 Matroids
41
Example 2.9 Given a graph G = (V, E), let I be the family of edge sets of acyclic
subgraphs of G. Then it is clear that (E, I) is an independent system. We verify that
it is actually a matroid, which is usually called a graph matroid.
Consider a subset F of E. Suppose that the subgraph (V, F ) of G has m con-
nected components. We note that in each connected component C of (V, F ), a max-
imal acyclic subgraph is just a spanning tree of C, in which the number of edges
is exactly one less than the number of vertices in C. Thus, every maximal acyclic
subgraph of (V, F ) has exactly |V | −m edges. So, condition (I2) holds for the
independent system (E, I), and hence (E, I) is a matroid.
□
Example 2.10 Consider a directed graph G = (V, E) and a nonnegative integer
function f on V . Let I be the family of edge sets of subgraphs whose out-degree at
any vertex u is no more than f(u). It is clear that (E, I) is an independent system.
We verify that (E, I) is actually a matroid.
For any subset F ⊆E, let d+
F(u) be the number of out-edges at u which belong
to F . Then, all maximal independent sets in F have the same size,

u∈V
min{f(u), d+
F(u)}.
Therefore, (E, I) is a matroid.
□
In a matroid, all maximal independent subsets have the same cardinality. They
are called bases. For instance, in a graph matroid deﬁned by a connected graph
G = (V, E), every base is a spanning tree of G and they all have the same size
|V | −1.
There is an interesting relationship between the intersection of matroids and in-
dependent systems.
Theorem 2.11 For any independent system (E, I), there exist a ﬁnite number of
matroids (E, Ii), 1 ≤i ≤k, such that I = k
i=1 Ii.
Proof. Let C1, . . ., Ck be all minimal dependent sets of (E, I) (i.e, they are the
minimal sets among {F | F ⊆E, F ̸∈I}). For each i ∈{1, 2, . . ., k}, deﬁne
Ii = {F ⊆E | Ci ̸⊆F }.
Then it is not hard to verify that I = k
i=1 Ii. We next show that each (E, Ii) is a
matroid.
It is easy to see that (E, Ii) is an independent system. Thus, it sufﬁces to show
that condition (I2) holds for (E, Ii). Consider F ⊆E. If Ci ̸⊆F , then F contains
a unique maximal independent set, which is itself. If Ci ⊆F , then every maximal
independent subset of F is equal to F \ {u} for some u ∈Ci and hence has size
|F | −1.
□
Theorem 2.12 Suppose the independent system (E, I) is the intersection of k ma-
troids (E, Ii), 1 ≤i ≤k; that is, I = k
i=1 Ii. Then

42
Greedy Strategy
max
F⊆E
v(F)
u(F) ≤k,
where u(F ) and v(F) are the two functions deﬁned in (2.1).
Proof. Let F ⊆E. Consider two maximal independent subsets I and J of F with
respect to (E, I). For each 1 ≤i ≤k, let Ii be a maximal independent subset of
I ∪J with respect to (E, Ii) that contains I. [Note that I is an independent subset
of I ∪J with respect to (E, Ii), and so such a set Ii exists.] For any e ∈J \ I, if
e ∈k
i=1(Ii \ I), then I ∪{e} ∈k
i=1 Ii = I, contradicting the maximality of I.
Hence, e occurs in at most k −1 different subsets Ii \ I. It follows that
k

i=1
|Ii| −k|I| =
k

i=1
|Ii \ I| ≤(k −1)|J \ I| ≤(k −1)|J|,
or
k

i=1
|Ii| ≤k|I| + (k −1)|J|.
Now, for each 1 ≤i ≤k, let Ji be a maximal independent subset of I ∪J with
respect to (E, Ii) that contains J. Since, for each 1 ≤i ≤k, (E, Ii) is a matroid,
we must have |Ii| = |Ji|. In addition, for every 1 ≤i ≤k, |J| ≤|Ji|. Therefore,
we get
k|J| ≤
k

i=1
|Ji| =
k

i=1
|Ii| ≤k|I| + (k −1)|J|.
It follows that |J| ≤k|I|.
□
Example 2.13 Consider the independent system (E, I) for MAX-DHP deﬁned in
Section 2.1. Based on the analysis in the proof of Lemma 2.4 and Examples 2.9 and
2.10, we can see that I is actually the intersection of the following three matroids:
(1) The family I1 of all subgraphs with out-degree at most 1 at each vertex;
(2) The family I2 of all subgraphs with in-degree at most 1 at each vertex; and
(3) The family I3 of all subgraphs that do not contain a cycle when the edge
direction is ignored.
Thus, Theorem 2.5 can also be derived from Theorem 2.12.
On the other hand, for the independent system (E, I) for MAX-HC deﬁned in
Section 2.1, the analysis in the proof of Lemma 2.2 uses a more complicated count-
ing argument and does not yield the simple property that (E, I) is the intersection
of two matroids. In fact, it can be proved that (E, I) is not the intersection of two
matroids. We remark that, in general, the problem MAX-ISS for an independent
system that is the intersection of two matroids can often be solved in polynomial
time.
□

2.3 Quadrilateral Condition
43
Example 2.14 Let X, Y, Z be three sets. We say two elements (x1, y1, z1) and
(x2, y2, z2) in X × Y × Z are disjoint if x1 ̸= x2, y1 ̸= y2, and z1 ̸= z2. Consider
the following problem:
MAXIMUM 3-DIMENSIONAL MATCHING (MAX-3DM): Given three
disjoint sets X, Y , Z and a nonnegative weight function c on all triples
in X ×Y ×Z, ﬁnd a collection F of disjoint triples with the maximum
total weight.
For given sets X, Y , and Z, let E = X × Y × Z. Also, let IX (IY , IZ) be the
family of subsets A of E such that no two triples in any subset share an element in
X (Y , Z, respectively). Then (E, IX), (E, IY ), and (E, IZ) are three matroids and
MAX-3DM is just the problem of ﬁnding the maximum-weight intersection of these
three matroids. By Theorem 2.12, we see that Algorithm 2.A is a polynomial-time
3-approximation for MAX-3DM.
□
2.3
Quadrilateral Condition on Cost Functions
Theorem 2.7 gives us a tight relationship between matroids and the optimality of
greedy algorithms. It is interesting to point out that this tight relationship holds with
respect to arbitrary nonnegative objective functions c. That is, if (E, I) is a matroid,
then the greedy algorithm will ﬁnd optimal solutions for all objective functions c.
On the other hand, if (E, I) is not a matroid, then the greedy algorithm may still
produce an optimal solution, but the optimality must depend on some speciﬁc prop-
erties of the objective functions. In this section, we present such a property.
Consider a directed graph G = (V, E) and a cost function c : E →R. We say
(G, c) satisﬁes the quadrilateral condition if, for any four vertices u, v, u′, v′ in V ,
c(u, v) ≥max{c(u, v′), c(u′, v)}
=⇒c(u, v) + c(u′, v′) ≥c(u, v′) + c(u′, v).
The quadrilateral condition is quite useful in the analysis of greedy algorithms. The
following are some examples.
Let G = (V1, V2, E) be a complete bipartite graph with |V1| = |V2|. Let I be the
family of all matchings (recall that a matching of a graph is a set of edges that do
not share any common vertex). Clearly, (E, I) is an independent system. It is, how-
ever, not a matroid. In fact, for some subgraphs of G, maximal matchings may have
different cardinalities (although all maximal matchings for G always have the same
cardinality). A maximal matching in the bipartite graph is called an assignment.
MAXIMUM ASSIGNMENT (MAX-ASSIGN): Given a complete bipar-
tite graph G = (V1, V2, E) with |V1| = |V2|, and an edge weight func-
tion c : E →R+, ﬁnd a maximum-weight assignment.
Theorem 2.15 If the weight function c satisﬁes the quadrilateral condition for all
u, u′ ∈V1 and v, v′ ∈V2, then Algorithm 2.A produces an optimal solution for the
instance (G, c) of MAX-ASSIGN.

44
Greedy Strategy
Proof. Assume that V1 = {u1, u2, . . ., un} and V2 = {v1, v2, . . ., vn}. Also,
assume, without loss of generality, that M = {(ui, vi) | i = 1, 2, . . ., n} is the as-
signment found by Algorithm 2.A, in the order of (u1, v1), (u2, v2), . . ., (un, vn).
We claim that there must be an optimal assignment that contains the edge (u1, v1):
Let M ∗⊆E be an arbitrary optimal solution. If the edge (u1, v1) is not in
M ∗, then M ∗must have two edges (u1, v′) and (u′, v1), where v′ ̸= v1 and
u′ ̸= u1. From the greedy strategy of Algorithm 2.A, we know that c(u1, v1) ≥
max{c(u1, v′), c(u′, v1)}. Therefore, by the quadrilateral condition,
c(u1, v1) + c(u′, v′) ≥c(u1, v′) + c(u′, v1).
This means that replacing edges (u1, v′) and (u′, v1) in M ∗by (u1, v1) and (u′, v′)
does not decrease the total weight of the assignment. This completes the proof of
the claim.
Using the same argument, we can prove that for each i = 1, 2, . . ., n, there
exists an optimal assignment that contains all edges (u1, v1), . . ., (ui, vi). Thus, M
is actually an optimal solution.
□
Next, let us come back to the problem MAX-DHP.
Theorem 2.16 For the problem MAX-DHP restricted to the graphs with distance
functions satisfying the quadrilateral condition, the greedy Algorithm 2.A is a
polynomial-time 2-approximation.
Proof. Assume that G = (V, E) is a directed graph, and c : E →R+ is the distance
function. Let n = |V |. Let e1, e2, . . ., en−1 be the edges selected by Algorithm
2.A into the solution set H, in the order of their selection into H. They are, hence,
in nonincreasing order of their length. For each i = 1, 2, . . ., n −1, let Pi be a
longest simple path in G that contains edges e1, e2, . . ., ei, and let Qi = Pi −
{e1, e2, . . ., ei}. In particular, Q0 = P0 is an optimal solution, and Qn−1 = ∅. For
any set T of edges in G, we write c(T) to denote the total length of edges in T. We
claim that for i = 1, 2, . . ., n −1,
c(Qi−1) ≤c(Qi) + 2c(ei).
To prove the claim, let us consider the relationship between Pi−1 and Pi. If
Pi−1 = Pi, then Qi−1 = Qi ∪{ei}, and so
c(Qi−1) = c(Qi) + c(ei) ≤c(Qi) + 2c(ei).
If Pi−1 ̸= Pi, then we must have ei ̸∈Pi−1. Assume that ei = (u, v). To add ei to
Pi−1 to form a simple path Pi, we must remove up to three edges from Pi−1 (and
add ei and some new edges):
(1) The edge in Pi−1 that begins with u;
(2) The edge in Pi−1 that ends with v; and
(3) An edge in the path from v to u if Pi−1 contains such a subpath.

2.3 Quadrilateral Condition
45
P i -1
u
v
e i
New Path
 ’
 u ’
 v
v
u
Figure 2.3:
From path Pi−1 to a new path.
In addition, these edges are all in Qi−1 \ {ei}. Figure 2.3 shows an example of this
process.
From the greedy strategy of the algorithm, we know that c(ei) ≥c(e) for any
edge e ∈Qi−1. So, the total length of the edges removed is at most 3c(ei). We
consider two cases:
Case 1. We may form a new path passing through e1, . . ., ei from Pi−1 by re-
moving at most two edges, say, e′
j and e′
k. Then, c((Pi−1\{e′
j, e′
k})∪{ei}) ≤c(Pi).
Hence,
c(Qi−1) ≤c(Qi) + c({e′
j, e′
k}) ≤c(Qi) + 2c(ei).
Case 2. We must remove three edges from Pi−1 to form a new path passing
through e1, e2, . . ., ei. As discussed above, these three edges must be (u, v′), (u′, v),
for some u′, v′ ∈V , and an edge e in the subpath from v to u in Pi−1, and u, v,
u′, and v′ are all distinct. This means that Pi−1 has a subpath from u′ to v′, which
contains these three edges. Thus, after deleting (u, v′), (u′, v), and e, we can add
edge (u′, v′) to form a new path (cf. Figure 2.3). Therefore, we have
c(Qi) ≥c(Qi−1) −c({(u′, v), e, (u, v′)}) + c(u′, v′)
≥c(Qi−1) −c(e) −c(u, v)
≥c(Qi−1) −2c(ei),
where the second inequality follows from the quadrilateral condition on u, v, u′, and
v′ and the fact that c(u, v) ≥c(e′) for all e′ ∈Qi−1. This completes the proof of
the claim.
Now, we note that Qn−1 = ∅, and so c(Qn−1) = 0. Thus, we have
c(P0) = c(Q0) ≤c(Q1) + 2c(e1)
≤c(Q2) + 2c(e1) + 2c(e2)
≤· · · ≤c(Qn−1) + 2
n−1

i=1
c(ei) = 2c(H).
□

46
Greedy Strategy
The quadrilateral condition sometimes holds naturally. The following is an ex-
ample.
Recall that a (character) string is a sequence of characters from a ﬁnite alphabet
Σ. We say a string s is a superstring of t, or t is a substring of s, if there exist strings
u, v such that s = utv. If u is empty, we say t is a preﬁx of s, and if v is empty, then
we say t is a sufﬁx of s. The length of a string s is the number of characters in s, and
is denoted by |s|.
SHORTEST SUPERSTRING (SS): Given a set of strings S = {s1, s2,
. . ., sn} in which no string si is a substring of any other string sj, j ̸= i,
ﬁnd the shortest string s∗that contains all strings in S as substrings.
The problem SS has important applications in computational biology and data
compression.
A string v is called an overlap of string s with respect to string t if v is both
a sufﬁx of s and a preﬁx of t, that is, if s = uv and t = vw for some strings u
and w. We note that the overlap string may be an empty string. Also, the notion of
overlap strings is not symmetric. That is, an overlap of s with respect to t may not
be an overlap of t with respect to s. For any two strings s and t, we write ov(s, t) to
denote the longest overlap of s with respect to t.
To ﬁnd an approximation algorithm for SS, we can transform the problem SS
into the problem MAX-DHP: First, for any set S = {s1, s2, . . ., sn} of strings, we
deﬁne the overlap graph G(S) = (S, E) to be the complete directed graph on the
vertex set S, with all self-loops removed. For each edge (si, sj) in E, we let its
length be c(si, sj) = |ov(si, sj)|.
Suppose that s∗is a shortest superstring for S and that s1, s2, . . ., sn are the
strings in S in the order of occurrence from left to right in s∗. Then, for each
i = 1, . . ., n −1, si and si+1 must have the maximal overlap in s∗for, other-
wise, s∗could be shortened and would not be the shortest superstring. It is not hard
to verify that the sequence (s1, s2, . . ., sn) forms a directed Hamiltonian path H in
the overlap graph G(S), whose total edge length, denoted by c(H), is equal to the
sum of the total length of all overlap strings in s∗:
c(H) =
n−1

i=1
|ov(si, si+1)|.
Next, consider an arbitrary directed Hamiltonian path H = (sh(1), sh(2), . . .,
sh(n)) in G(S). We can construct a superstring for S from H as follows: For
each i = 1, 2, . . ., n −1, let zi be the preﬁx of sh(i) such that sh(i) = zi ·
ov(sh(i), sh(i+1)). Then, deﬁne p(H) = z1z2 · · ·zn−1sh(n). It is easy to check that
p(H) is a superstring of all sh(i), for i = 1, 2, . . ., n (cf. Figure 2.4). Clearly,
p(H)| =
n−1

i=1
|zi| + |sh(n)|
=
n−1

i=1
(|sh(i)| −|ov(sh(i), sh(i+1))|) + |sh(n)|

2.3 Quadrilateral Condition
47
.
.
.
s
s
s
s
h (1)
n
h
h (3)
 (2)
p(H)
−1
h
z
  z
z
 z 3
sh
−1
)
)
n
n
(
(
2
1
Figure 2.4:
A superstring obtained from a Hamiltonian path.
=
n

i=1
|sh(i)| −
n−1

i=1
|ov(sh(i), sh(i+1))| =
n

i=1
|si| −c(H).
That is, the length of p(H) equals the total length of the strings in S minus the total
edge length of the path H. It follows that the string p(H) generated from a longest
directed Hamiltonian path H is a shortest superstring of S, and vice versa.
Theorem 2.17 If H is a longest directed Hamiltonian path in the overlap graph
G(S), then the string p(H) is a shortest superstring for S. Conversely, if s∗is a
shortest superstring for S, then s∗= p(H) for some longest directed Hamiltonian
path H in G(S).
From this relationship, we can convert Algorithm 2.A into an approximation al-
gorithm for the problem SS.
Algorithm 2.B (Greedy Algorithm for SS)
Input: A set S = {s1, s2, . . ., sn} of strings.
(1) Set G ←{s1, s2, . . ., sn}.
(2) While |G| > 1 do
select si, sj in G with the maximum |ov(si, sj)|;
let si ←siu, where sj = ov(si, sj)u;
G ←G \ {sj}.
(3) Output the only string sG left in G.
Tarhio and Ukkonen [1988] and Turner [1989] noticed independently that the
overlap graph G(S) satisﬁes the quadrilateral condition.
www.allitebooks.com

48
Greedy Strategy
v
u
w
 v’
 u ’
(   ,   )
u v
(   ,   )
 u
v
 ’
 v’
(   ,    )
u
y
x
ov
ov
ov
Figure 2.5:
Overlaps among four strings.
Lemma 2.18 Let G(S) be the overlap graph of a set S of strings. Let u, v, u′, and
v′ be four distinct strings in S. If |ov(u, v)| ≥max{|ov(u, v′)|, |ov(u′, v)|}, then
|ov(u, v)| + |ov(u′, v′)| ≥|ov(u, v′)| + |ov(u′, v)|.
Proof. The proof is trivial when |ov(u, v)| ≥|ov(u, v′)| + |ov(u′, v)|. Thus, we
may assume that |ov(u, v)| < |ov(u, v′)| + |ov(u′, v)|.
Since both ov(u, v) and ov(u′, v) are preﬁxes of v, |ov(u′, v)| ≤|ov(u, v)|
implies that ov(u′, v) is a preﬁx of ov(u, v). Similarly, we get that ov(u, v′) is a
postﬁx of ov(u, v) (see Figure 2.5). Because |ov(u, v)| < |ov(u, v′)| + |ov(u′, v)|,
we know that the overlap of ov(u′, v) with respect to ov(u, v′) is not empty. Let
w = ov(ov(u′, v), ov(u, v′)). Then, we have ov(u, v) = xwy, ov(u′, v) = xw and
ov(u, v′) = wy for some strings x and y (cf. Figure 2.5). That is, w is an overlap of
u′ with respect to v′. It follows that
|ov(u′, v′)| ≥|w| = |ov(u, v′)| + |ov(u′, v)| −|ov(u, v)|.
□
Theorem 2.19 Let s∗be a shortest superstring for S. Let ∥S∥be the total length of
strings in S. Then
∥S∥−|s∗| ≤2(∥S∥−sG),
where sG is the superstring generated by Algorithm 2.B.
Proof. The theorem follows immediately from Lemma 2.18 and Theorem 2.16.
□
The following example shows that the bound on (∥S∥−s∗)/(∥S∥−sG) given
in Theorem 2.19 is the best possible.
Example 2.20 Let S = {abk, bk+1, bka}, where k ≥1. The shortest superstring for
S is abk+1a. However, Algorithm2.B may generate a superstring abkabk+1 (by ﬁrst
merging the string abk with bka). Thus, for this example, we have ∥S∥−|sG| = k
and ∥S∥−|s∗| = 2k.
□

2.4 Submodular Potential Functions
49
In the above example, we also have |sG|/|s∗| = (2k + 3)/(k + 3). This means
that the performance ratio of Algorithm 2.B cannot be better than 2. It has been
conjectured that the performance ratio of Algorithm 2.B is indeed equal to 2; that
is, |sG| ≤2|s∗|, while the best known result is |sG| ≤4|s∗| [Blum et al., 1991].
In the above, we have seen a nice relationship between the problem SS and the
problem MAX-DHP. This relationship can be extended to an interesting transfor-
mation from the problem SS to the traveling salesman problem TSP on directed
graphs (called DIRECTED TSP).
Let S = {s1, s2, . . ., sn} be an instance of the problem SS. Let sn+1 be the
empty string. Consider a complete directed graph with vertex set V = S ∪{sn+1},
and the distance function
d(si, sj) = |si| −|ov(si, sj)|,
for si, sj ∈V . [Note that ov(sn+1, si) = ov(si, sn+1) = sn+1 for all 1 ≤i ≤n.]
It is easy to see that the shortest superstring for set S corresponds to a minimum
Hamiltonian circuit with respect to the above distance function, and vice versa.
Thus, a good approximation for this special case of DIRECTED TSP would also
be a good approximation for the problem SS. It has also been proved that the above
distance function satisﬁes the triangle inequality; that is, for any si, sj, and sk, with
1 ≤i, j, k ≤n + 1, d(si, sk) ≤d(si, sj) + d(sj, sk) [Turner, 1989]. Based on this
relationship between the two problems DIRECTED TSP and SS, we will present,
in Chapter 6, a polynomial-time 3-approximation for SS, even though no constant-
ratio polynomial-time approximation for DIRECTED TSP is known.
2.4
Submodular Potential Functions
In the last three sections, we have applied the notion of independent systems to
study greedy algorithms. The readers may have noticed that most applications we
studied were about maximization problems. While minimization and maximization
look similar, the behaviors of approximation algorithms for them are quite different.
In this section, we introduce a different theory for the analysis of greedy algorithms
for minimization problems.
Consider a ﬁnite set E (called the ground set) and a function f : 2E →Z, where
2E denotes the power set of E (i.e., the family of all subsets of E). The function f
is said to be submodular if for any two sets A and B in 2E,
f(A) + f(B) ≥f(A ∩B) + f(A ∪B).
(2.2)
Example 2.21 (a) The function f(A) = |A| is submodular since
|A| + |B| = |A ∩B| + |A ∪B|.
Actually, in this case, the equality always holds, and we call f a modular function.
(b) Let (E, I) be a matroid. For any A ∈2E, deﬁne the rank of A as

50
Greedy Strategy
rank(A) =
max
I∈I,I⊆A |I|.
Then, the function rank is a submodular function.
To see this, consider two subsets A and B of E. Let IA∩B be a maximal inde-
pendent subset of A∩B. Let I′ be a maximal independent subset in A that contains
IA∩B as a subset. Since all maximal independent subsets in A have the same cardi-
nality, we know that |I′| = rank(A). Next, let I′′ be a maximal independent subset
in A ∪B that contains I′ as a subset. Similarly, we have |I′′| = rank(A ∪B). Let
J = I′′ \ I′. We note that J must be a subset of B since I′ is a maximal indepen-
dent subset in A. Thus, IA∩B ∪J ⊆I′′ ∩B is an independent subset in B. So,
|IA∩B ∪J| = |IA∩B| + |J| ≤rank(B). Or,
rank(A ∪B) +rank(A ∩B) −rank(A)
= |I′′| + |IA∩B| −|I′| = |J| + |IA∩B| ≤rank(B).
□
Assume that f is a submodular function on subsets of E. Deﬁne
ΔDf(C) = f(C ∪D) −f(C)
for any subsets C and D of E; that is, ΔDf(C) is the extra amount of f value we
gain by adding D to C. Then, the submodularity property (2.2) may be expressed as
ΔDf(A ∩B) ≥ΔDf(B),
(2.3)
where D = A \ B. When D = {x} is a singleton, we simply write Δxf(C) instead
of Δ{x}f(C).
To see the role of submodular functions in the analysis of greedy algorithms, let
us study a speciﬁc problem:
MINIMUM SET COVER (MIN-SC): Given a set S and a collection C of
subsets of S such that 
C∈C C = S, ﬁnd a subcollection A ⊆C with
the minimum cardinality such that 
C∈A C = S.
For any subcollection A ⊆C, let ∪A denote the union of sets in A; i.e., ∪A =

C∈A C, and deﬁne f(A) = | ∪A|. Then f is a submodular function. To see this,
we verify that, for any two subcollections A and B of C, f(A) + f(B) −f(A ∪B)
is equal to the number of elements in both ∪A and ∪B. Moreover, every element in
∪(A ∩B) must appear in both ∪A and ∪B. Therefore,
f(A) + f(B) −f(A ∪B) ≥f(A ∩B).
A function g on 2E is said to be monotone increasing if, for all A, B ⊆E,
A ⊆B =⇒g(A) ≤g(B).
It is easy to check that the above function f is monotone increasing. We can use this
function f as the potential function to design a greedy approximation for MIN-SC
as follows:

2.4 Submodular Potential Functions
51
Algorithm 2.C (Greedy Algorithm for MIN-SC)
Input: A set S and a collection C of subsets of S.
(1) A ←∅.
(2) While f(A) < |S| do
Select a set C ∈C to maximize f(A ∪{C});
Set A ←A ∪{C}.
(3) Output A.
This approximation algorithm can be analyzed as follows:
Theorem 2.22 Greedy Algorithm 2.C is a polynomial-time (1 + ln γ)-approxi-
mation for MIN-SC, where γ is the maximum cardinality of a subset in the input
collection C.
Proof. Let A1, . . ., Ag be the solution found by Algorithm 2.C, in the order of their
selection into the collection A. Denote Ai = {A1, . . ., Ai}, for i = 0, 1, . . ., g. Let
C1, C2, . . ., Cm be a minimum set cover (i.e., m = opt is the number of subsets
in a minimum set cover). By the greedy strategy, we know that Ai+1 covers the
maximum number of elements that are not yet covered by Ai. Let Ui denote the set
of elements in S that are not covered by Ai. Then the total number of elements in Ui
is |Ui| = |S| −f(Ai). The set Ui can be covered by the m subsets in the minimum
set cover {C1, . . ., Cm}. By the pigeonhole principle, there must be a subset Cj that
covers at least (|S| −f(Ai))/m elements in Ui. Therefore,
f(Ai+1) −f(Ai) ≥|S| −f(Ai)
m
.
(2.4)
Or, equivalently,
|S| −f(Ai+1) ≤(|S| −f(Ai)) ·

1 −1
m

.
By a simple induction, we get
|Ui| = |S| −f(Ai) ≤|S| ·

1 −1
m
i
≤|S| · e−i/m.
We note that the size of Ui decreases from |S| to 0, and so there must be an
integer i ∈{1, 2, . . ., g} such that |Ui+1| < m ≤|Ui|. That is, after i + 1 iterations
of the while-loop of step (2) of Algorithm 2.C, there are at most m −1 elements
left uncovered, and so the greedy Algorithm 2.C will halt after at most m −1 more
iterations. That is, g ≤i + m. In addition, we have m ≤|Ui| ≤|S|e−i/m, and so
i ≤m · ln
|S|
m

≤m · lnγ
and
g ≤i + m ≤m(1 + lnγ).
□

52
Greedy Strategy
In the above, we used the pigeonhole principle to prove inequality (2.4). It may
appear that the submodularity of the potential function f is not required in the proof.
It is important to point out that the above proof actually used the submodularity
property of f implicitly. To clarify this point, we present, in the following, an alter-
native proof that uses the submodularity property of f explicitly, and avoids the use
of the speciﬁc meaning of f about set coverings.
Alternative Proof for (2.4). Recall that {C1, . . ., Cm} is a minimum set cover. For
each j = 1, 2, . . ., m, let Cj = {C1, . . ., Cj}. By the greedy strategy, we have, for
each 1 ≤j ≤m,
f(Ai+1) −f(Ai) = ΔAi+1f(Ai) ≥ΔCjf(Ai),
and so
f(Ai+1) −f(Ai) ≥1
m ·
m

j=1
ΔCjf(Ai).
On the other hand, we note that
|S| −f(Ai) = f(Ai ∪Cm) −f(Ai) =
m

j=1
ΔCjf(Ai ∪Cj−1).
Therefore, to get (2.4), it sufﬁces to have
ΔCjf(Ai) ≥ΔCjf(Ai ∪Cj−1),
which follows from the submodularity and monotone increasing properties of the
function f.
□
The second proof above illustrates that the submodularity and monotone increas-
ing properties of the potential function are sufﬁcient conditions for inequality (2.4).
In particular, for m = 2, inequality (2.4) is equivalent to
ΔC2f(Ai) ≥ΔC2f(Ai ∪C1).
We will show, in the following, that this is equivalent to the condition that f is
submodular and monotone increasing.
Lemma 2.23 Let f be a submodular function on 2E. Then, for all sets A, C ⊆E,
ΔCf(A) ≤

x∈C
Δxf(A).
Proof. Note that if x ∈A, then Δxf(A) = 0. Thus, without loss of generality,
we may assume that A ∩C = ∅. For any x ∈C, set X = A ∪{x} and Y =
A ∪(C −{x}). Then, by the deﬁnition of submodular functions, we have

2.4 Submodular Potential Functions
53
f(C ∪A) + f(A) = f(X ∪Y ) + f(X ∩Y )
≤f(X) + f(Y ) = f(A ∪{x}) + f(A ∪(C −{x})).
It follows that
ΔCf(A) ≤Δxf(A) + ΔC−{x}f(A).
The lemma can now be derived easily from this inequality.
□
Lemma 2.24 Let f be a function on all subsets of a set E. Then f is submodular if
and only if, for any two subsets A ⊆B of E and any element x ̸∈B,
Δxf(A) ≥Δxf(B).
(2.5)
Proof. From A ⊆B and x ̸∈B, we know that (A ∪{x}) ∪B = B ∪{x} and
(A ∪{x}) ∩B = A. Therefore, if f is submodular, then
f(A ∪{x}) + f(B) ≥f(A) + f(B ∪{x}).
That is,
Δxf(A) ≥Δxf(B).
Conversely, suppose (2.5) holds for all subsets A ⊆B and all x ̸∈B. Consider
two arbitrary subsets A, B of E. Let D = A\B, and assume that D = {x1, . . ., xk}.
Then
ΔDf(A ∩B) =
k

i=1
Δxif((A ∩B) ∪{x1, . . ., xi−1})
≥
k

i=1
Δxif(B ∪{x1, . . ., xi−1}) = ΔDf(B).
(Note that D = A \ B, and so xi ̸∈B for all i = 1, 2, . . ., n.) That is, inequality
(2.3) holds and hence f is submodular.
□
Lemma 2.25 Let f be a function on all subsets of a set E. Then f is submodular
and monotone increasing if and only if, for any two subsets A ⊆B and any element
x ∈E,
Δxf(A) ≥Δxf(B).
Proof. We note that f is monotone increasing if and only if, for any subset A ⊆E
and any x ∈E, Δxf(A) ≥0. Now, assume that f is also submodular. Then, for
any subsets A ⊆B ⊆E and any x ∈E \ B, we have, by Lemma 2.24, Δxf(A) ≥
Δxf(B); and for x ∈B, we also have, by monotonicity of f, Δxf(A) ≥0 =
Δxf(B).
Conversely, assume that Δxf(A) ≥Δxf(B) for any subsets A ⊆B ⊆E and
any x ∈E. Then, by Lemma 2.24, we know that f is submodular. In addition, set

54
Greedy Strategy
B = E; we get Δxf(A) ≥Δxf(E) = 0 for all x ∈E, which implies that f is
monotone increasing.
□
A submodular function is normalized if f(∅) = 0. Every submodular function
f can be normalized by setting g(A) = f(A) −f(∅). We note that if f is a nor-
malized, monotone increasing submodular function, then f(A) ≥0 for every set
A ⊆E. A normalized, monotone increasing, submodular function f is also called
a polymatroid function. If f is deﬁned on 2E, then (E, f) is called a polymatroid.
There are close relationships among polymatroids, matroids, and independent sys-
tems; see Exercises 2.18–2.24.
Consider a submodular function f on 2E. Let Ωf = {C ⊆E | (∀x ∈E)
Δxf(C) = 0}. Intuitively, Ωf contains the maximal sets C under function f; that
is, f(C ∪B) = f(C) for all sets B.
Lemma 2.26 Let f be a monotone increasing, submodular potential function on
2E. Then, Ωf = {C | f(C) = f(E)}.
Proof. If C ∈Ωf, then
0 ≤f(E) −f(C) = ΔE−Cf(C) ≤

x∈E−C
Δxf(C) = 0.
Therefore, f(C) = f(E).
Conversely, if f(C) = f(E), then, for any x ∈E, f(C) ≤f(C ∪{x}) ≤f(E),
and so f(C) = f(C ∪{x}). That is, for any x ∈E, Δxf(C) = 0.
□
We are now ready to present a general result about greedy approximations which
use a monotone increasing, submodular function as the potential function. Consider
the following minimization problem.
MINIMUM SUBMODULAR COVER (MIN-SMC): Given a ﬁnite set E,
a normalized, monotone increasing, submodular function f on 2E, and
a nonnegative cost function c on E,
minimize
c(A) =

x∈A
c(x),
subject to
A ∈Ωf.
This minimization problem is a general form for many problems. In most ap-
plications, the submodular function f is not given explicitly in the form of the in-
put/output pairs, but its value at any set A ⊆E is computable in polynomial time.
Example 2.27 Consider the weighted version of the problem MIN-SC.
MINIMUM-WEIGHT SET COVER (MIN-WSC): Given a set S, a col-
lection C of subsets of S with ∪C = S, and a weight function w on all
sets C ∈C, ﬁnd a set cover with the minimum total weight.

2.4 Submodular Potential Functions
55
Following the discussion on MIN-SC, let the input collection C be the ground
set, and deﬁne, for any subcollection A of C, f(A) = | ∪A|. Then, f is a submod-
ular function. Moreover, f is apparently monotone increasing. With this function f,
ΔCf(A) = 0 if and only if C ⊆∪A. This means that a subcollection A belongs to
Ωf if and only if A is a set cover of S = ∪C. Thus, the problem MIN-WSC is just
the problem MIN-SMC with respect to this potential function f.
□
Example 2.28 A hypergraph H = (V, C) is a pair of sets V and C, where C is a
family of subsets of V . Each element in V is called a vertex and each subset in C
is called an edge (and sometimes, to emphasize that it is an edge of a hypergraph,
called a hyperedge). The degree of a vertex is the number of edges that contain the
vertex.
A subset A of vertices is called a hitting set of the hypergraph H = (V, C) if
every edge in C contains at least one vertex from A. The following problem is the
weighted version of MIN-HS deﬁned in Exercise 1.15:
MINIMUM-WEIGHT HITTING SET (MIN-WHS): Given a hypergraph
H = (V, C) and a nonnegative weight function c on vertices in V , ﬁnd
a hitting set A ⊆V of the minimum total weight.
Let V be the ground set, and deﬁne, for each A ⊆V , E(A) to be the collection
of sets C ∈C such that C ∩A ̸= ∅, and let f(A) = |E(A)|. Then it is easy to see
that E(A ∪B) = E(A) ∪E(B) and E(A ∩B) ⊆E(A) ∩E(B). Thus, we have
|E(A)| + |E(B)| = |E(A) ∪E(B)| + |E(A) ∩E(B)|
≥|E(A ∪B)| + |E(A ∩B)|.
That is, function f is a submodular function. Furthermore, it is easy to check that
E(∅) = ∅, and if A ⊆B, then E(A) ⊆E(B). Thus, f is a normalized, monotone
increasing, submodular function.
Now, what is Ωf? It is not hard to verify that A ∈Ωf if and only if A is a hitting
set. Thus, the problem MIN-WHS is just the problem MIN-SMC with respect to
this submodular potential function f.
□
The problem MIN-SMC has a natural greedy algorithm: In each iteration, we
add an element x to the solution set A to maximize the value Δxf(A), relative to
the cost c(x).
Algorithm 2.D (Greedy Algorithm for MIN-SMC)
Input: A ﬁnite set E, a submodular function f on 2E, and a function c : E →R+.
(1) Set A ←∅.
(2) While there exists an x ∈E such that Δxf(A) > 0 do
select a vertex x that maximizes Δxf(A)/c(x);
A ←A ∪{x}.
(3) Return AG ←A.

56
Greedy Strategy
The following theorem gives an estimation of the performance of this algorithm.
We write H(n) to denote the harmonic function H(n) = n
i=1 1/i. Note that
H(n) ≤1 + ln n (see Exercise 2.6).
Theorem 2.29 Let f be a normalized, monotone increasing, submodular function.
Then Algorithm 2.D produces an approximate solution within a factor of H(γ) from
the optimal solution to the input (E, f, c), where γ = maxx∈E f({x}).
Proof. Let A be the approximate solution obtained by Algorithm 2.D. Assume that
x1, x2, . . ., xk are the elements of A, in the order of their selection into the set.
Denote Ai = {x1, x2, . . ., xi}; in particular, A0 = ∅. Let A∗be an optimal solution
to the same instance.
For any set B ⊆E, we write c(B) to denote the total cost of B: c(B) =

x∈B c(x). We are going to prove that
c(A) ≤c(A∗) · H(γ)
by a weight-decomposition counting argument. That is, we decompose the total cost
c(A) of the approximate solution and distribute it to the elements of the optimal
solution A∗through a weight function w(y) on y ∈A∗. Then we calculate the
weight decomposition according to the optimal solution A∗and show that each
element y ∈A∗can pick up at most weight c(y) · H(γ). It follows, therefore, that
c(A∗) is at least c(A)/H(γ).
In other words, we need to assign weight w(y) to each element y of A∗so that it
satisﬁes the following properties:
(a) c(A) ≤
y∈A∗w(y); and
(b) w(y) ≤c(y) · H(γ).
Property (b) implies that 
y∈A∗w(y) ≤c(A∗)H(γ). Thus, properties (a) and (b)
together establish the desired result.
First, to simplify the notation, we let ri = Δxif(Ai−1) and zy,i = Δyf(Ai−1).
Now, we deﬁne, for each y ∈A∗,
w(y) =
k

i=1
(zy,i −zy,i+1)c(xi)
ri
.
Before we prove properties (a) and (b), we observe that
k

i=1
(zy,i −zy,i+1) = zy,1 −zy,k+1 = Δyf(A0) −Δyf(Ak) = f({y}).
[In the above, Δyf(A0) = f({y}) because f is normalized, and Δyf(Ak) = 0
because Ak = A ∈Ωf.] Therefore,

2.4 Submodular Potential Functions
57
k
y,
z k+
y,
1
x
k
x
ir
*
A
ir
y,
z
1
y,
z
z
2
y,i
y,i+
z
z
1
)
ix
weight = c(x  )
i
y
A
f(y
1
Figure 2.6:
The weight decomposition.

y∈A∗
k

i=1
(zy,i −zy,i+1) =

y∈A∗
f({y})
≥f(A∗) = f(A) =
k

i=1
Δxif(Ai−1) =
k

i=1
ri,
since both A∗and A are in Ωf. This relationship provides some intuition about how
the weight-decomposition function is deﬁned: As illustrated in Figure 2.6, we divide
each element xi into ri parts, each of weight c(xi)/ri, so that the total weight of all
parts, over all xi ∈A, is c(A). Then each y ∈A∗picks up zy,i −zy,i+1 parts from
the element xi. The total number of parts picked up by y, disregarding the different
weight, is f({y}). Our goal here is to distributepart of each xi ∈A to some y ∈A∗,
while each y ∈A∗does not take too much weight.
We now proceed to prove properties (a) and (b). For property (a), we can write
weight w(y) in the following form:
w(y) =
k

i=1
(zy,i −zy,i+1)c(xi)
ri
= c(x1)
r1
zy,1 +
k

i=2
c(xi)
ri
−c(xi−1)
ri−1

zy,i.
[Note that zy,k+1 = Δyf(Ak) = 0.] In addition, c(A) can also be expressed in a
similar form:
c(A) =
k

i=1
ri
ri
c(xi) =
k

i=1

k

j=i
rj −
k

j=i+1
rj
c(xi)
ri
= c(x1)
r1
k

j=1
rj +
k

i=2
c(xi)
ri
−c(xi−1)
ri−1

k

j=i
rj.
www.allitebooks.com

58
Greedy Strategy
Moreover, from the greedy strategy of Algorithm 2.D, we know that
r1
c(x1) ≥
r2
c(x2) ≥· · · ≥
rk
c(xk);
or, equivalently,
c(xi)
ri
−c(xi−1)
ri−1
≥0,
for all i = 1, . . ., k. Thus, to prove (a), it sufﬁces to prove that for any i = 1,
2, . . ., k,
k

j=i
rj ≤

y∈A∗
zy,i.
This inequality holds since, by Lemmas 2.23 and 2.26,
k

j=i
rj =
k

j=i
Δxjf(Aj−1) =
k

j=i
(f(Aj) −f(Aj−1))
= f(A) −f(Ai−1) = f(A∗) −f(Ai−1)
= f(A∗∪Ai−1) −f(Ai−1) = ΔA∗f(Ai−1)
≤

y∈A∗
Δyf(Ai−1) =

y∈A∗
zy,i.
Next, we prove property (b). Let y be a ﬁxed element in A∗. From the greedy
strategy of Algorithm 2.D, we know that if zy,i > 0, then
c(xi)
ri
≤c(y)
zy,i
,
for all i = 1, 2, . . ., k. In addition, we know from Lemma 2.25 that zy,i ≥zy,i+1.
Let ℓ= max{i | 1 ≤i ≤k, zy,i > 0}. We have
w(y) =
ℓ

i=1
(zy,i −zy,i+1)c(xi)
ri
≤
ℓ

i=1
(zy,i −zy,i+1)c(y)
zy,i
= c(y)
ℓ

i=1
zy,i −zy,i+1
zy,i
.
Note that for any integers p > q > 0, we have
p −q
p
=
p

j=q+1
1
p ≤
p

j=q+1
1
j = H(p) −H(q).
So, we have

2.5 Applications
59
w(y) ≤c(y)
ℓ−1

i=1

H(zy,i) −H(zy,i+1)

+ c(y) H(zy,ℓ) = c(y) H(zy,1).
Note that zy,1 = f({y}) ≤γ for all y ∈A∗. Therefore, we have proved property
(b) and, hence, the theorem.
□
2.5
Applications
Now we present some applications of the greedy Algorithm 2.D.
First, from Example 2.27, we get the upper bound for the performance ratio of the
greedy algorithm for MIN-WSC immediately. More speciﬁcally, the submodular
potential function f for the problem MIN-WSC is deﬁned to be f(A) = | ∪A|.
Therefore, when applied to MIN-WSC, the greedy strategy for Algorithm 2.D is to
select, at each stage, the set C ∈C with the highest value of
| ∪(A ∪{C})| −| ∪A|
c(C)
,
where c(C) is the weight of set C, and add C to the solution collection A. Also,
the parameter γ in the performance ratio H(γ) of Theorem 2.29 is equal to the
maximum value of f({C}) = |C| over all C ∈C. Therefore, we have the following
result:
Corollary 2.30 When it is applied to the problem MIN-WSC, Algorithm 2.D is
a polynomial-time H(m)-approximation, where m is the maximum cardinality of
subsets in the input collection C.
From Example 2.28, we know that the function f(A) = |E(A)| is monotone in-
creasing and submodular for the problem MIN-WHS. With respect to this potential
function f, Algorithm 2.D selects, at each stage, the element x ∈S with the highest
value of
|E(A ∪{x})| −|E(A)|
c(x)
,
and adds x to the solution set A. We note that in the setting of the problem MIN-
WHS, the parameter γ in the performance ratio H(γ) of Theorem 2.29 is just the
maximum degree over all vertices. So, we get the following result:
Corollary 2.31 When it is applied to the problem MIN-WHS, Algorithm 2.D is a
polynomial-time H(δ)-approximation, where δ is the maximum degree of a vertex
in the input hypergraph.
Note that if all edges in the input hypergraph H = (V, C) have exactly two
elements, then this subproblem of MIN-WHS is actually the weighted version of
the vertex cover problem MIN-VC (see Exercise 1.10).

60
Greedy Strategy
MINIMUM-WEIGHT VERTEX COVER (MIN-WVC): Given a graph
G = (V, E), with a nonnegative weight function c : V →R+, ﬁnd
a vertex cover of the minimum total weight.
We prove that the bound H(δ) of Corollary 2.31 is actually tight, even for the
nonweighted version of MIN-VC on bipartite graphs.
Theorem 2.32 For any n ≥1, there exists a bipartite graph G with degree at most
n and a minimum vertex cover of size n! such that Algorithm 2.D produces a vertex
cover of size H(n) · (n!) on graph G.
Proof. Let V1, V2,1, V2,2, . . ., V2,n be n+1 pairwisely disjoint sets of size |V1| = n!
and |V2,i| = n!/i, for each i = 1, 2, . . ., n. The bipartite graph G has the vertex sets
V1 and V2 = n
i=1 V2,i. To deﬁne the edges in G, we perform the following process
for each 1 ≤i ≤n: We partition V1 into n!/i disjoint subsets, each of size i, and
build a one-to-one correspondence between these n!/i subsets and n!/i vertices in
V2,i. Then, for each subset A of V1, we connect every vertex in A to the vertex in
V2,i that corresponds to subset A.
Thus, in the bipartite graph G, each vertex in V1 has degree n and each vertex
in V2,i has degree i ≤n. Clearly, V1 is a minimum hitting set, which has size n!.
However, the greedy Algorithm 2.D on graph G may produce V2 as the hitting set,
which has size n
i=1(n!)/i = H(n) · (n!).
□
The above result indicates that Algorithm 2.D is not a good approximation
for the nonweighted MIN-VC, as MIN-VC actually has a polynomial-time 2-
approximation, and MIN-VC in bipartite graphs can be solved in polynomial time
(see Exercise 1.10). On the other hand, Algorithm 2.D is probably the best approx-
imation for the nonweighted hitting set problem, unless certain complexity hierar-
chies collapse (see Historical Notes).
Our next example is the problem of subset interconnection design. Recall that for
any graph G = (V, E) and any set S ⊆V , G|S denotes the subgraph of G induced
by set S; i.e., G|S is the graph with vertex set S and edge set E|S = {{x, y} ∈E |
x, y ∈S}. For any subsets S1, S2, . . ., Sm of V , we say a subgraph H = (V, F ) of
G is a feasible graph for S1, S2, . . ., Sm if, for each i = 1, 2, . . ., m, the subgraph
H|Si induced by Si is connected.
WEIGHTED SUBSET INTERCONNECTION DESIGN (WSID): Given a
complete graph G = (V, E) with a nonnegative edge weight function
c : E →R+, and m vertex subsets S1, S2, . . ., Sm ⊆V , ﬁnd a feasible
subgraph H = (V, F ) for S1, S2, . . ., Sm, with the minimum total edge
weight.
Example 2.33 Let V = {v1, v2, . . ., v5}, and consider the ﬁve subsets S1 = {v1,
v2}, S2 = {v1, v2, v3}, S3 = {v3, v4, v5}, S4 = {v1, v2, v4}, and S5 = {v2,
v4, v5}. These subsets form a hypergraph on V , as shown in Figure 2.7, together
with a cost function c. Figure 2.8 shows two feasible graphs for these subsets. With
respect to the cost function c given in Figure 2.7, the graph in Figure 2.8(b) is a
minimum-cost feasible graph.
□

2.5 Applications
61
5
3
4
2
1
c(i, j) 1
2
3
4
5
1
5
6
7
8
2
5
6
7
3
5
6
4
5
5
Figure 2.7:
A hypergraph and its cost function.
6
5
6
5
5
5
6
5
5
5
(a)
(b)
Figure 2.8:
Feasible graphs for the input of Figure 2.7.
In the following, we deﬁne a submodular function r on subsets of the edge set E.
Consider the graph matroid of the induced subgraph G|Si = (V, Ei) (see Example
2.9), where Ei = E|Si. In this graph matroid, a set I ⊆Ei is an independent subset
if (Si, I) is an acyclic subgraph of G|Si. Let ri be the rank function of the graph
matroid of graph G|Si (see Example 2.21(b)). That is, for any A ⊆E, ri(A) = the
size of the largest edge set I ⊆A ∩Ei such that (Si, I) is an acyclic subgraph of
G|Si. Equivalently,
ri(A) = |Si| −the number of connected compo-
nents of the graph (Si, A ∩Ei).
By Example 2.21(b), ri is a submodular function.
Now, deﬁne r(A) = m
i=1 ri(A). Note that the sum of submodular functions is
submodular. Therefore, r is a submodular function. Furthermore, it is not hard to
check that r is monotone increasing and normalized.
For this submodular function r, the set Ωr is the collection of sets A ⊆E such
that r(A ∪{e}) = r(A) for all edges e in E. It is not hard to see that Ωr is just
the set of all feasible graphs. Thus, the problem WSID is actually the minimiza-
tion problem MIN-SMC with respect to the submodular potential function r. So,
Algorithm 2.D and Theorem 2.29 can be applied to it.
To be more precise, the greedy criterion of Algorithm 2.D for the problem WSID
is to select, at each stage, an edge {e} with the maximum ratio

62
Greedy Strategy
r(F ∪{e}) −r(F)
c(e)
and add it to the solution edge set F. What is the value r(F ∪{e}) −r(F)? It
is the number of indices i ∈{1, 2, . . ., m} such that edge e connects two distinct
connected components of the graph G|F∩Si.
Also, the parameter γ of Theorem 2.29 is equal to the maximum value of r({e}),
which is the maximum number of indices i ∈{1, 2, . . ., m} such that Si contains
the two endpoints of e.
Corollary 2.34 When it is applied to the problem WSID, Algorithm 2.D is a
polynomial-time H(K)-approximation, where K is the maximum number of in-
duced subgraphs G|Si that share a common edge.
It is known that for 0 < ρ < 1, the problem WSID has no polynomial-time
approximation within a factor of ρ ln n from the optimal solution unless every NP-
complete problem is solvable in deterministic time O(npolylogn)1 (this condition is
weaker than NP = P but is still considered not likely to be true).
For a connected graph G = (V, E), we say a subset C ⊆V is a connected vertex
cover if C is a vertex cover for G and the induced subgraph G|C is connected.
Consider the following problem:
MINIMUM-WEIGHT CONNECTED VERTEX COVER (MIN-WCVC):
Given a connected graph G = (V, E) and a nonnegative vertex weight
function c : V →R+, ﬁnd a connected vertex cover with the minimum
total weight.
For a graph G = (V, E) and a subset C ⊆V , let g(C) be the number of edges
in E that are not covered by C, and h(C) the number of connected components of
G|C. Deﬁne p(C) = |E| −g(C) −h(C). Clearly, p(∅) = |E| −g(∅) −h(∅) = 0.
We are going to prove that p is a monotone increasing, submodular function,
using a new characterization of submodular functions. In the following, we write
ΔxΔyf(A) to denote Δyf(A ∪{x}) −Δyf(A). For the proofs of the following
two lemmas, see Exercise 2.14.
Lemma 2.35 Let f be a function on 2E. Then f is submodular if and only if for any
A ⊆E and any two distinct elements x, y ̸∈A,
ΔxΔyf(A) ≤0.
Lemma 2.36 Let f be a function on 2E. Then f is monotone increasing and sub-
modular if and only if for any A ⊆E and x, y ∈E,
ΔxΔyf(A) ≤0.
1The notation polylog n denotes the class of functions (log n)k, for all k ≥1.

2.5 Applications
63
Now, we apply this characterization to show that p is a monotone increasing,
submodular function.
Lemma 2.37 p is monotone increasing and submodular.
Proof. Consider a vertex subset C and a vertex u ̸∈C. Then Δup(C) = −Δug(C)−
Δuh(C). We observe that −Δug(C) is just the number of edges incident on u in
graph G that are not covered by C. It follows that −Δug(C) = |N(u) \ C|, where
N(u) is the set of vertices in G that are adjacent to u. Moreover, −Δuh(C) is
equal to the number of connected components in G|C that are adjacent to u minus
1. Therefore, we always have −Δug(C) ≥0 and −Δuh(C) ≥−1.
By Lemma 2.36, it is sufﬁcient to prove that for any vertex subset C and two
vertices u and v,
ΔvΔup(C) ≤0.
Note that if u ∈C, then both Δup(C ∪{v}) and Δup(C) are equal to 0, and hence
ΔvΔup(C) = 0. Also, if v ∈C, then we have Δup(C ∪{v}) = Δup(C), and
hence ΔvΔup(C) = 0. Thus, we may assume that neither u nor v belongs to C.
We consider three cases.
Case 1: u = v. Since Δup(C ∪{v}) = 0, it sufﬁces to show Δup(C) ≥0. If
C ∩N(u) = ∅, then −Δug(C) = deg(u) and Δuh(C) = −1, which implies that
Δup(C) = deg(u) −1 ≥0, because G is connected and so deg(u) is at least 1. If
C ∩N(u) ̸= ∅, then u is adjacent to at least one connected component of G|C and
hence −Δuh(C) ≥0, which also implies that Δup(C) ≥0.
Case 2: u ̸= v and u is not adjacent to v. Then N(u) \ (C ∪{v}) = N(u) \ C,
and hence −Δug(C ∪{v}) = −Δug(C). Consider an arbitrary connected com-
ponent of G|C∪{v} that is adjacent to u. If it does not contain v, then it is also
a connected component of G|C adjacent to u. If it contains v, then it must con-
tain at least one connected component of G|C adjacent to u. Thus, the number of
connected components of G|C∪{v} adjacent to u is no more than the number of con-
nected components of G|C adjacent to u; that is, −Δuh(C ∪{v}) ≤−Δuh(C). So
Δup(C ∪{v}) ≤Δup(C).
Case 3: u ̸= v but u is adjacent to v. Then N(u)\(C ∪{v}) = (N(u)\C)\{v},
and hence −Δug(C ∪{v}) = −Δug(C) −1. Also, among all connected compo-
nents of G|C∪{v} that are adjacent to u, exactly one contains v and all others are con-
nected components of G|C adjacent to u. Hence, −Δuh(C ∪{v}) ≤−Δuh(C)+1.
Therefore, Δup(C ∪{v}) ≤Δup(C).
□
It can be veriﬁed that with respect to this submodular function p, the set Ωp is
exactly the collection of connected vertex covers of G.
Lemma 2.38 Let G = (V, E) be a connected graph with at least three vertices.
For any subset C ⊆V , C is a connected vertex cover if and only if, for any vertex
x ∈V , Δxp(C) = 0.
Proof. If C is a connected vertex cover, then it is clear that p(C) = |E| −g(C) −
h(C) = |E| −0 −1 = |E| −1, reaching the maximum value of p.

64
Greedy Strategy
Conversely, suppose that for any vertex x ∈V , Δxp(C) = 0. It is clear that
C ̸= ∅, for otherwise we can ﬁnd a vertex x ∈V of degree ≥2 and get Δxp(C) =
−Δxg(∅) −Δxh(∅) ≥2 −1 = 1. Now, assume, for the sake of contradiction, that
C is not a connected vertex cover. Let B = {x ∈V | x is adjacent to some v ∈C},
and A = V \ (B ∪C). Consider two cases.
Case 1: There exists an edge in E that is not covered by C. Then there must be
an edge e in E not covered by C such that one of its endpoints x is in B (otherwise,
A forms a nonempty connected component of G, contradicting the assumption that
G is connected). Now, we note that C ∪{x} covers at least one extra edge e than
C, and so −g(C ∪{x}) > −g(C). In addition, since x is in B and is adjacent to
at least one vertex in C, adding x to C does not increase the number of connected
components. Therefore, −h(C ∪{x}) ≥−h(C). Together, we get Δxp(C) > 0,
which is a contradiction.
Case 2: C covers every edge, but G|C is not connected. Since G is connected,
there must be a path in G connecting two connected components of G|C. Further-
more, such a shortest path must contain exactly two edges {u, x} and {x, v} with
u, v ∈C and x ∈B, for otherwise it would contain an edge whose two endpoints
are not in C. But then we have −h(C ∪{x}) > −h(C) but −g(C ∪{x}) =
−g(C) = 0, and hence Δxp(C) > 0, a contradiction again.
□
Corollary 2.39 When it is applied to the problem MIN-WCVC on connected
graphs of at least three vertices, with respect to the potential function p, Algorithm
2.D is a polynomial-time H(δ −1)-approximation, where δ is the maximum vertex
degree of the input graph G.
Proof. It follows from Theorem 2.29 and the facts that the maximum value of |E| −
g({x}) is equal to δ and that −h({x}) = −1 for all x ∈V .
□
The next example is a 0–1 integer programming problem.
GENERAL COVER (GC): Given nonnegative integers aij, bi, and cj,
for i = 1, 2, . . ., m and j = 1, 2, . . ., n,
minimize
n

j=1
cjxj
subject to
n

j=1
aijxj ≥bi,
i = 1, 2, . . ., m,
xj ∈{0, 1},
j = 1, 2, . . ., n.
We deﬁne a function f : 2{1,...,n} →N as follows: For any J ⊆{1, . . ., n},
f(J) =
m

i=1
min

bi,

ℓ∈J
aiℓ

.
Let I(J) = {i | 
ℓ∈J aiℓ< bi}. Then it is clear that for any j, k ∈{1, 2, . . ., n},

2.5 Applications
65
Δjf(J) =

i∈I(J)
min

aij, bi −

ℓ∈J
aiℓ

,
and
Δjf(J ∪{k}) =

i∈I(J∪{k})
min

aij, bi −

ℓ∈J
aiℓ−aik

.
Moreover, it is not hard to verify that for any 1 ≤k ≤n, I(J ∪{k}) ⊆I(J). Thus,
Δjf(J ∪{k}) ≤Δjf(J) for all sets J ⊆{1, 2, . . ., n} and all j, k ∈{1, 2, . . ., n}.
Thus, by Lemma 2.36, f is a monotone increasing, submodular function.
The collection Ωf consists of all sets J ⊆{1, 2, . . ., n} with the maximum value
f(J) = n
i=1 bi. So, Algorithm 2.D and Theorem 2.29 are applicable to problem
GC. In particular, the greedy criterion of Algorithm 2.D adds, at each stage, the
index j with the maximum value of
1
cj

i∈I(J)
min

aij, bi −

ℓ∈J
aiℓ

to the solution set J. Also, the parameter γ of the performance ratio H(γ) is no
more than the maximum value of m
i=1 aij, j = 1, 2 . . ., n.
Corollary 2.40 When it is applied to the problem GC, Algorithm 2.D produces an
H(γ)-approximation in polynomial time, where γ = max1≤j≤n
m
i=1 aij.
Finally, we consider a problem about matroids. Recall that a base of a matroid
(E, I) is just a maximal independent set. Consider the following problem:
MINIMUM-COST BASE (MIN-CB):
Given a matroid (E, I) and a nonnegative function c : E →R+,
minimize
c(I)
subject to
I ∈B,
where B is the family of all bases of the matroid (E, I).
Recall the function rank on a matroid (E, I) deﬁned in Example 2.21(b).
Then rank is a normalized, monotone increasing, submodular function, and it has
Ωrank = B. Therefore, MIN-CB is a special case of MIN-SMC with the poten-
tial function rank. Note that the corresponding parameter γ in Theorem 2.29 is
γ = maxx∈E rank({x}) = 1, and hence H(γ) = 1. In other words, the greedy
Algorithm 2.D for MIN-CB actually gives the optimal solutions.
Corollary 2.41 When it is applied to the problem MIN-CB, the greedy Algorithm
2.D produces a minimum solution in polynomial time.

66
Greedy Strategy
2.6
Nonsubmodular Potential Functions
When the associated potential function is not submodular, Theorem 2.29 for the
greedy algorithm no longer holds. In such circumstances, how do we analyze the
performance of the greedy algorithm? We study this problem in this section.
A dominating set of a graph G = (V, E) is a subset D ⊆V such that every
vertex is either in D or adjacent to a vertex in D. A connected dominating set C is
a dominating set with an additional property that it induces a connected subgraph.
The following problem has many applications in wireless communication.
MINIMUM CONNECTED DOMINATING SET (MIN-CDS): Given a
connected graph G = (V, E), ﬁnd a connected dominating set of G
with the minimum cardinality.
Consider a graph G and a subset C of vertices in G. Divide vertices in G into
three classes with respect to C, and assign different colors to them: Vertices that
belong to C are colored in black; vertices that are not in C but are adjacent to C are
colored in gray; and vertices that are neither in C nor adjacent to C are colored in
white.
Clearly, C is a connected dominating set if and only if there does not exist a white
vertex and the subgraph induced by black vertices is connected. This observation
suggests that we use the function g(C) = p(C) + h(C) as the potential function
in the greedy algorithm, where p(C) is the number of connected components of the
subgraph G|C induced by C, and h(C) is the number of white vertices. It is clear
that C is a connected dominating set if and only if g(C) = 1. However, the function
g is not really a good candidate for the potential function, because a set C may not
be a connected dominating set even if Δxg(C) = 0 for all vertices x. Figure 2.9
shows such an example, in which g(C) = p(C) + h(C) = 2 + 0 = 2 > 1, but
g(C ∪{x}) = g(C) for all vertices x. This means that if we apply Algorithm 2.D
to MIN-CDS with this potential function g, its output is not necessarily a connected
dominating set.
In general, we observe that the graph shown in Figure 2.9 is a typical case result-
ing from Algorithm 2.D with respect to the potential function g.
Lemma 2.42 Let G = (V, E) be a connected graph, and C ⊆V . If the subgraph
G|C induced by black vertices is not connected but Δxg(C) = 0 for all x ∈V , then
Figure 2.9:
Δxg(C) = 0 for all vertices x, but C is not a connected dominating
set.

2.6 Nonsubmodular Potential Functions
67
all black connected components of G|C can be connected together through chains
of gray vertices, with each chain having exactly two vertices.
Proof. We ﬁrst note that if Δxg(C) = 0 for all x ∈V , then G has no gray ver-
tex that is adjacent to two black components, since coloring such a gray vertex
in black would reduce the value of g(C). In addition, G also has no white verex,
for otherwise, by the connectivity of G, there must be a gray vertex adjacent to
some white vertex, and coloring this gray vertex in black would reduce the value
of g(C), too. Now, suppose, for the sake of contradiction, that some black com-
ponent cannot be connected to another black component through chains of two
adjacent gray vertices. Then, we can divide all black vertices into two parts such
that the distance between the two parts is more than 3. Consider a shortest path
π = (u, x1, x2, . . ., xk, v) between the two parts, with u and v belonging to the two
different parts and x1, x2, . . ., xk are gray vertices with k ≥3. Since x2 is gray, it
must be adjacent to a black vertex w. If w and u are in the same part, then the path
from w to v is a path between the two parts of black vertices shorter than π, which
is a contradiction. On the other hand, if w and v are in the same part, then the path
from u to w is a path between the two parts shorter than π, also a contradiction. So,
the lemma is proven.
□
From this lemma, a simple idea of an approximation algorithm works as follows:
First, apply the greedy algorithm with the potential function g until Δxg(C) = 0
for all x ∈V . Then, add extra vertices to connect components of G|C. A careful
analysis using the pigeonhole principle shows that this modiﬁed greedy algorithm
achieves the performance ratio H(δ)+3, where δ is the maximum degree of G (see
Section 6.2).
In the following, we take a different approach by choosing a different potential
function. Namely, we replace h(C) by q(C), the number of connected components
of the subgraph with vertex set V and edge set D(C), where D(C) is the set of all
edges incident on some vertices in C. Deﬁne f(C) = p(C) + q(C).
Lemma 2.43 Suppose G is a connected graph with at least three vertices. Then C
is a connected dominating set if and only if f(C ∪{x}) = f(C) for every x ∈V .
Proof. If C is a connected dominating set, then f(C) = 2, which reaches the mini-
mum value. Therefore, f(C ∪{x}) = f(C) for every x ∈V .
Conversely, suppose f(C ∪{x}) = f(C) for every x ∈V . First, C cannot be
the empty set. In fact, if C = ∅, then we can pick a vertex x of degree ≥2 and get
f(C ∪{x}) ≤|V | −1 < |V | = f(C).
So, we may assume C ̸= ∅. Consider a connected component of the subgraph
induced by C. Let B denote its vertex set, which is a subset of C, and A be the set
of vertices in V −B that are adjacent to a vertex in B. We claim that V = B ∪A
(and hence C = B is a connected dominating set for G).
To prove this claim, suppose, by way of contradiction, that V ̸= B ∪A. Then,
since G is connected, there must be a vertex x not in B ∪A that is adjacent to
a vertex y ∈B ∪A. Since all vertices adjacent to B are in A, we know that y
must be in A. Now, if x is white or gray, then we must have p(C ∪{y}) ≤p(C)

68
Greedy Strategy
x
B
A
x
Figure 2.10:
A counterexample showing f not supmodular.
and q(C ∪{y}) < q(C). If x is black, then we have p(C ∪{y}) < p(C) and
q(C ∪{y}) ≤q(C). In either case, we get f(C ∪{y}) < f(C), a contradiction to
our assumption. So, the claim, and hence the lemma, is proven.
□
This lemma shows that the greedy Algorithm 2.D for MIN-CDS with respect to
the potential function f will produce a connected dominating set.
A function f : 2E →R is supmodular if −f is submodular. Clearly, all results
about monotone increasing, submodular functions can be converted into the results
about the corresponding monotone decreasing, supmodular functions. It is easy to
see that f is monotone decreasing. Therefore, if f is a supmodular function, then
we could directly employ Theorem 2.29 to get the performance ratio of the greedy
Algorithm 2.D with respect to f. Unfortunately, as shown in the counterexample of
Figure 2.10, f is not supmodular. More speciﬁcally, in this example, A ⊆B but
Δxf(A) = −1 > −2 = Δxf(B), and so −f does not satisfy the condition of
Lemma 2.36 and is not submodular.
Actually, f is the sum of two functions p and q, where q is supmodular but p is
not.
Lemma 2.44 If A ⊆B, then Δyq(A) ≤Δyq(B).
Proof. Note that −Δyq(B) = the number of the connected components of the graph
(V, D(B)) that are adjacent to y but do not contain y. Since each connected com-
ponent of graph (V, D(B)) is constituted by one or more connected components of
graph (V, D(A)), the number of connected components of (V, D(B)) adjacent to y
is no more than the number of connected components of (V, D(A)) adjacent to y.
Thus, we get −Δyq(B) ≤−Δyq(A).
□
How do we analyze the performance of the greedy Algorithm 2.D with respect to
a nonsubmodular potential function? Let us look at the proof of Theorem 2.22 about
the greedy algorithm for MIN-SC again, and see where the submodularity property
of the potential function is used. It turns out that it was used only once, when we
proved the inequality
ΔCjf(Ai) ≥ΔCjf(Ai ∪Cj−1)
(2.6)
to get (2.4). An important observation about this inequality is that the incremental
variables Cj, 1 ≤j ≤m, are sets of the optimal solution, arranged in an arbitrary
order. Therefore, although for nonsubmodular functions f this inequality may not

2.6 Nonsubmodular Potential Functions
69
hold for an arbitrary ordering of sets in the optimal solution, a carefully arranged
ordering on these sets might still satisfy, or almost satisfy, this inequality. In the
following, we will implement this idea for the problem MIN-CDS.
Let the vertices x1, . . ., xg be the elements of the solution found by Algorithm
2.D with respect to the potential function f, in the order of their selection into the so-
lution set. Denote Ci = {x1, x2, . . ., xi} and consider f(Ci). Initially, f(C0) = n,
where n is the number of vertices in G. Let C∗be a minimum connected dominating
set for G. Assume that |C∗| = m.
Lemma 2.45 For i = 1, 2, . . ., g,
f(Ci) ≤f(Ci−1) −f(Ci−1) −2
m
+ 1.
(2.7)
Proof. First, consider the case of i ≥2. We note that
f(Ci) = f(Ci−1) + Δxif(Ci−1).
Since C∗is a connected dominating set, we can always arrange the elements of
C∗in an ordering y1, y2, . . ., ym such that y1 is adjacent to a vertex in Ci−1
and, for each j ≥2, yj is adjacent to a vertex in {y1, . . ., yj−1}. Denote C∗
j =
{y1, y2, . . ., yj}. Then
ΔC∗f(Ci−1) =
m

j=1
Δyjf(Ci−1 ∪C∗
j−1).
For each 1 ≤j ≤m, we note that yj can dominate at most one additional connected
component in the subgraph G|Ci−1∪C∗
j−1 than in G|Ci−1, which is the one that con-
tains C∗
j−1, since all vertices y1, . . ., yj−1 in C∗
j−1 are connected. Since −Δyp(C)
is equal to the number of connected components of G|C that are adjacent to y minus
1, it follows that
−Δyjp(Ci−1 ∪C∗
j−1) ≤−Δyjp(Ci−1) + 1.
Moreover, by Lemma 2.44,
−Δyjq(Ci−1 ∪C∗
j−1) ≤−Δyjq(Ci−1).
So we have
−Δyjf(Ci−1 ∪C∗
j−1) ≤−Δyjf(Ci−1) + 1.
[Note that this inequality is close to our desired inequality (2.6).] From this inequal-
ity, we get
f(Ci−1) −2 = −ΔC∗f(Ci−1)
=
m

j=1
(−Δyjf(Ci−1 ∪C∗
j−1)) ≤
m

j=1
(−Δyjf(Ci−1) + 1).

70
Greedy Strategy
By the pigeonhole principle, there exists an element yj ∈C∗such that
−Δyjf(Ci−1) + 1 ≥f(Ci−1) −2
m
.
By the greedy strategy of Algorithm 2.D,
−Δxif(Ci−1) ≥−Δyjf(Ci−1) ≥f(Ci−1) −2
m
−1.
Or, equivalently,
f(Ci) ≤f(Ci−1) −f(Ci−1) −2
m
+ 1.
For the case of i = 1, the proof is essentially identical, with the difference that
y1 could be an arbitrary vertex in C∗.
□
Theorem 2.46 When it is applied to the problem MIN-CDS with respect to the
potential function −f, the greedy Algorithm 2.D is a polynomial-time (2 + lnδ)-
approximation, where δ is the maximum degree of the input graph.
Proof. If g ≤2m, then the proof is already done. So we assume that g > 2m.
Rewrite the inequality (2.7) as
f(Ci) −2 ≤(f(Ci−1) −2)

1 −1
m

+ 1.
Solving this recurrence relation, we have
f(Ci) −2 ≤(f(C0) −2)

1 −1
m
i
+
i−1

k=0

1 −1
m
k
= (f(C0) −2)

1 −1
m
i
+ m

1 −

1 −1
m
i
= (f(C0) −2 −m)

1 −1
m
i
+ m.
From the greedy strategy of Algorithm 2.D, we reduce the value f(Ci−1) in each
stage i ≤g. Therefore, f(Ci) ≤f(Ci−1) −1. In addition, f(Cg) = 2. So we have
f(Cg−2m) ≥2m + 2. Set i = g −2m, and observe that
2m ≤f(Ci) −2 ≤(n −2 −m)

1 −1
m
i
+ m,
where n is the number of vertices in G. Since (1 −1/m)i ≤e−i/m, we obtain
i ≤m · ln n −2 −m
m
.
Note that each vertex has at most δ neighbors and so can dominate at most δ + 1
vertices. Hence, n/m ≤δ + 1. It follows that g = i + 2m ≤m(2 + lnδ).
□

2.6 Nonsubmodular Potential Functions
71
Now, let us consider another simple idea for designing greedy algorithms with
respect to a nonsubmodular potential function. In the greedy Algorithm 2.C for the
problem MIN-SC, we add, in each iteration, one subset C to the solution A. Sup-
pose we are allowed to add two or more subsets to A in each iteration. Does this
give us a better performance ratio? It is easy to see that the answer is no. In general,
does this idea work for the greedy Algorithm 2.D with respect to a submodular po-
tential function f? The answer is again no, since a submodular function satisﬁes the
property of Lemma 2.23. On the other hand, if the potential function f is not sub-
modular, then this idea may actually work. In the following,we show that the greedy
algorithm based on this idea actually gives a better performance ratio for MIN-CDS
than Algorithm 2.D. More precisely, the performance ratio of the following greedy
algorithm for MIN-CDS approaches 1 + ln δ, as k tends to ∞.
Algorithm 2.E (Greedy Algorithm for MIN-CDS)
Input: A connected graph G = (V, E) and an integer k ≥2.
(1) C ←∅.
(2) While f(C) > 2 do
Select a set X ⊆V of size |X| ≤2k −1 that maximizes −ΔXf(C)
|X|
;
Set C ←C ∪X.
(3) Output Cg ←C.
To analyze greedy Algorithm 2.E, we note the following property of the potential
function −f.
Lemma 2.47 Let A, B, and X be three vertex subsets. If both G|B and G|X are
connected, then
−ΔXf(A ∪B) + ΔXf(A) ≤1.
Proof. Since q is supmodular, we have ΔXq(A) ≤ΔXq(A ∪B).
For function p, we note that, since G|X is connected, −ΔXp(A) is equal to the
number of black components dominated by X in graph G|A minus 1. Since the
subgraph G|B is connected, the number of black components dominated by X in
G|A∪B is at most one more than the number of black components dominated by
X in G|A. Therefore, we have −ΔXp(A ∪B) ≤−ΔXp(A) + 1. It follows that
−ΔXf(A ∪B) ≤−ΔXf(A) + 1.
□
Let C∗be a minimum solution to MIN-CDS. We show two properties of C∗in
the following two lemmas.
Lemma 2.48 For any integer k ≥2, C∗can be decomposed into Y1, Y2, . . ., Yh,
for some h ≥1, such that
(a) C∗= Y1 ∪Y2 ∪· · · ∪Yh;
(b) For each 1 ≤i ≤h, both G|Y1∪Y2∪···∪Yi and G|Yi are connected;

72
Greedy Strategy
. . .
< k
< k
> k
y
y
y
1
2
< k
t
x
Figure 2.11:
Case 2 in proof of Lemma 2.48.
(c) For each 1 ≤i ≤h, 1 ≤|Yi| ≤2k −1; and for all but one 1 ≤i ≤h,
k + 1 ≤|Yh|; and
(d) |Y1| + |Y2| + · · · + |Yh| ≤|C∗| + h −1.
Proof. We can construct sets Y1, . . ., Yh recursively.
Let T be a subtree of G|C∗that contains all vertices in C∗. Choose an arbitrary
vertex r ∈C∗as the root of T. For any vertex x ∈C∗, let T(x) denote the subtree
of T rooted at x, and |T(x)| the number of vertices in T(x).
If |T| ≤2k −1, then let Y1 = C∗and the lemma holds with h = 1. If T
contains more than 2k −1 vertices, then there must exist a vertex x ∈C∗such that
|T(x)| ≥k + 1 and for every child y of x, |T(y)| ≤k. Now, consider two cases.
Case 1. There is a child y of x such that |T(y)| = k. Let Y1 consist of all vertices
of T(y) together with x and delete all vertices of T(y) from T.
Case 2. For every child y of x, |T(y)| ≤k−1. Suppose y1, . . ., yt are all children
of x (cf. Figure 2.11). There must exist an integer 1 ≤j ≤t −1 such that
|T(y1)| + · · · + |T(yj)| ≤k −1
and
|T(y1)| + · · · + |T(yj)| + |T(yj+1)| ≥k.
Since |T(yj+1)| ≤k −1, we have
|T(y1)| + · · · + |T(yj)| + |T(yj+1)| ≤2k −2.
Let Y1 consist of all vertices in T(y1) ∪· · · ∪T(yj+1) together with x and delete
Y1 −{x} from T.
Repeating the above process on the remaining T, and rearranging the order of
the sets Y1, . . ., Yh, we will obtain a required decomposition.
□
Lemma 2.49 Let δ be the maximum degree of G = (V, E). Then we have |V | ≤
(δ −1)|C∗| + 2.

2.6 Nonsubmodular Potential Functions
73
Proof. We prove by induction on |C| that a subset C of V with connected G|C can
dominate at most (δ−1)|C|+2 vertices. For |C| = 1, it is triviallytrue. For |C| ≥2,
choose a vertex x ∈C such that G|C−{x} is still connected. Since x has at most δ
neighbors, and at least one of them is in C −{x}, we see that C dominates at most
δ −1 more vertices than C −{x} does. By the induction hypothesis, C −{x} can
dominate at most (δ −1)(|C| −1) + 2 vertices. Therefore, C can dominate at most
(δ −1)|C| + 2 vertices.
□
Theorem 2.50 For any ε > 0, there exists a polynomial-time approximation with
performance ratio (1 + ε) ln(δ −1) for MIN-CDS, where δ is the maximum degree
of the input graph.
Proof. Let G = (V, E) be a connected graph with the maximum degree δ. We can
ﬁnd easily a minimum connected dominating set of G if δ ≤2: If δ = 1, then
G contains only one edge, and either vertex of the edge is a minimum connected
dominating set. If δ = 2, G is either a path or a cycle, and a minimum connected
dominating set of G can be obtained by deleting, respectively, either the two leaves
or any two adjacent vertices.
For graphs with δ ≥3, we consider Algorithm 2.E on G. Let X1, . . ., Xg be
the sets chosen by greedy Algorithm 2.E on graph G, in the order of their selection
into set C. Denote Ci = X1 ∪· · · ∪Xi, for 0 ≤i ≤g (in particular, Cg is the
output of Algorithm 2.E). Let C∗be a minimum connected dominating set for G,
and m = |C∗|. Decompose C∗into Y1, Y2, . . ., Yh, satisfying conditions given in
Lemma 2.48. Denote C∗
j = Y1 ∪· · · ∪Yj, for 0 ≤j ≤h.
From Lemma 2.48, we know that G|Yj and G|C∗
j are connected for each 1 ≤j ≤
h. Thus, we have, by Lemma 2.47,
−ΔYjf(Ci ∪C∗
j−1) ≤−ΔYjf(Ci) + 1,
for 0 ≤i ≤g and 1 ≤j ≤h. By the greedy rule of Algorithm 2.E, we get
−ΔXi+1f(Ci)
|Xi+1|
≥−ΔYjf(Ci)
|Yj|
,
for 0 ≤i ≤g and 1 ≤j ≤h. Note that f(C∗) = 2 and, hence, for 0 ≤i ≤g −1,
−ΔXi+1f(Ci)
|Xi+1|
≥
−h
j=1 ΔYjf(Ci)
h
j=1 |Yj|
≥
−(h −1) −h
j=1 ΔYjf(Ci ∪C∗
j−1)
h
j=1 |Yj|
≥−(h −1) −(f(Ci ∪C∗) −f(Ci))
m + h −1
= f(Ci) −(h + 1)
m + h −1
.

74
Greedy Strategy
Denote ai = f(Ci) −(h + 1). Then the above inequality can be rewritten as
ai −ai+1
|Xi+1|
≥
ai
m + h −1,
for 0 ≤i ≤g −1.
That is, for each 0 ≤i ≤g −1,
ai+1 ≤ai

1 −
|Xi+1|
m + h −1

≤ai · exp
 −|Xi+1|
m + h −1

≤a0 · exp
−(|Xi+1| + |Xi| + · · · + |X1|)
m + h −1

.
(2.8)
Fix the index i, 0 ≤i ≤g −1, such that
ai ≥m > ai+1,
and let b = ai −m and b′ = m −ai+1. Write |Xi+1| = d + d′ such that
b
d = b′
d′ = ai −ai+1
|Xi+1|
≥
ai
m + h −1.
(In case of b = 0, just let d′ = |Xi+1|.) We now divide the greedy solution |Cg| into
two parts, |X1| + · · · + |Xi| + d, and d′ + |Xi+2| + · · · + |Xg|, and bound them
separately.
For the ﬁrst part, we note that
ai −m
d
= b
d ≥
ai
m + h −1,
and so
m ≤ai

1 −
d
m + h −1

≤ai · e−d/(m+h−1).
Combining this with (2.8), we get
m ≤a0 · e−(d+|Xi|+···+|X1|)/(m+h−1).
Note that a0 = f(∅) −(h + 1) = |V | −(h + 1). Thus,
|X1| + · · · + |Xi| + d ≤(m + h −1) ln |V | −(h + 1)
m
.
For the second part, we note that −ΔXj+1f(Cj)/|Xj+1| ≥1 for all 0 ≤j ≤g−
1, since we can, by Lemma 2.43, always ﬁnd a vertex v to make −Δ{v}f(Cj) ≥1.
That is,
|Xj+1| ≤f(Cj) −f(Cj+1),
for 0 ≤j ≤g −1. Thus,
d′ + |Xi+2| + · · · + |Xg| ≤b′ + f(Ci+1) −f(Cg)
= m −ai+1 + f(Ci+1) −f(C∗) = m + h −1.

Exercises
75
Together, we have
|X1| + · · · + |Xg| ≤(m + h −1)

1 + ln |V | −(h + 1)
m

.
From conditions (c) and (d) of Lemma 2.48, we know that
(h −1)(k + 1) + 1 ≤|Y1| + |Y2| + · · · + |Yh| ≤m + h −1,
and hence
h −1 ≤m
k .
Moreover, by Lemma 2.49, |V | ≤(δ −1)m + 2. Since h ≥1, we have
|V | −(h + 1)
m
≤δ −1.
Therefore,
|X1| + · · · + |Xg| ≤m

1 + 1
k

1 + ln(δ −1)

.
Now, the theorem follows by choosing k such that 1/k < ε.
□
Exercises
2.1 Let (E, I) be an independent system. Suppose that all maximal independent
subsets of E have cardinality k. Deﬁne
p = max
F⊆E
v(F)
u(F ),
where u(F ) and v(F ) are the functions deﬁned in (2.1). Let c : E →R+ be a
nonnegative cost function on E. Also, let I∗be a maximal independent subset of E
with the minimum cost, and IG an independent subset obtained by greedy Algorithm
2.A on the problem MAX-ISS. Prove that
c(I∗) ≤c(IG) ≤1
p · c(I∗) + p −1
p
· k · M,
where M = maxe∈E c(e).
2.2 For a complete directed graph G = (V, E), let IG be the family of the edge
sets of all acyclic subgraphs of G. Show that for any integer k > 0, there exists a
complete directed graph G = (V, E) such that for the independent system (E, IG),
max
F⊆E
v(F)
u(F) ≥k.

76
Greedy Strategy
2.3 Show that for every integer k ≥1, there exists an independent system (E, I)
that is an intersection of k matroids but not an intersection of less than k matroids,
such that
max
F⊆E
v(F)
u(F) = k.
2.4 Prove that an independent system (E, I) is a matroid if and only if, for any
cost function c : E →N+, the greedy Algorithm 2.D produces a minimum solution
for MIN-CB.
2.5 Prove that the distance function deﬁned in the transformation from the prob-
lem SS to the problem TSP, as described at the end of Section 2.3, satisﬁes the
triangle inequality.
2.6 Prove that for every positive integer m, m
i=1 1/i ≤1 + ln m.
2.7 In terms of the notion of hypergraphs, the problem MIN-SC asks for a
minimum-size hyperedge set that is incident on each vertex of the input hypergraph.
A k-matching in a hypergraph H is a sub-hypergraph of degree at most k. Let mk
be the maximum number of edges in a k-matching. Prove that
(a) mk ≤k · |C∗|, where C∗is a minimum set cover of H, and
(b) |CG| ≤d
i=1 mi/(i(i + 1)) + md/d, where CG is the output of the greedy
Algorithm 2.C, and d is the maximum degree of H.
2.8 Use Exercise 2.7 to give another proof to Theorem 2.22.
2.9 Let G = (V, E) be a graph and c : E →2N a color-set function (i.e., c(e) is
a color set for edge e). A color-covering of the graph G is a color set C ⊆N such
that the set of edges e with c(e) ∩C ̸= ∅contains a spanning tree of G. Prove that
the following problem has a polynomial-time (1 + ln |V |)-approximation:
For a given graph G and a given color-set function c : E →2N, ﬁnd a
color-covering of the minimum cardinality.
2.10 Show that the following problem has a polynomial-time (2 + ln |V |)-
approximation:
Given a graph G = (V, E) and a color-set function c : E →2N, ﬁnd
the subset C ⊆V of the minimum cardinality such that all colors of
the edges incident upon the vertices in C form a color-covering of G.
2.11 A function g : N →R+ is a concave function if, for any m, r, n ∈N, with
m < r < n, g(r) ≥tg(m) + (1 −t)g(n), where t = (n −r)/(n −m). Let E be a
ﬁnite set, and let f be a real function deﬁned on 2E such that f(A) = g(|A|) for all
A ⊆E. Show that f is submodular if and only if g is concave.
2.12 Consider a graph G = (V, E). Let ¯δ(X) for X ⊆V denote the set of edges
between X and V −X. Show that |¯δ(X)| is a submodular function.

Exercises
77
2.13 Show that a function f on 2E is modular (both submodular and supmodu-
lar) if and only if f is linear.
2.14 Prove Lemmas 2.35 and 2.36.
2.15 Suppose f and c are two polymatroid functions on 2E, and f is an integer
function. Consider the problem MIN-SMC with a possibly nonlinear cost function
c; i.e., the problem of minimizing c(A) over {A ⊆E | f(A) = f(E)}. Show
that the greedy Algorithm 2.D for MIN-SMC is a (ρ · H(γ))-approximation, where
γ = max{f({x}) | x ∈E} and ρ is the curvature of c, deﬁned by
ρ = min

e∈S c(e)
c(S)
 f(S) = f(E)

.
2.16 Consider a digraph G = (V, E). For X ⊆V , let ¯δ+(X) (¯δ−(X)) denote
the set of edges going out from (coming into, respectively) X. Show that |¯δ+(X)|
and |¯δ−(X)| are submodular functions.
2.17 Let r be a function mapping 2E to N. Show that the following statements
are equivalent:
(a) I = {I ⊆E | r(I) = |I|} deﬁnes a matroid (E, I) and r is its rank
function.
(b) For all A, B ⊆E, r satisﬁes the following conditions:
(i) r(A) ≤|A|;
(ii) if A ⊆B, then r(A) ≤r(B); and
(iii) r is submodular.
2.18 Show that a polymatroid (E, r) is a matroid if and only if r({x}) = 1 for
every x ∈E.
2.19 Suppose (E, r1), (E, r2), ..., (E, rk) are matroids. Show that (E, k
i=1 ri)
is a polymatroid.
2.20 Let (E, I) be a matroid, and rank its rank function. Consider a collection
C of subsets of E. For A ⊆C, deﬁne
f(A) = rank
 
A∈A A

.
Show that (E, f) is a polymatroid.
2.21 Show that for any polymatroid (E, f), there exist a matroid (E, r) and a
one-to-one mapping φ : E →2E such that
f(A) = r
 
A∈φ(A) A

.

78
Greedy Strategy
2.22 For any polymatroid (E, f), deﬁne fd on 2E with
fd(S) =

j∈S
f({j}) −f(E) −f(E −S).
Show that (E, fd) is still a polymatroid. [It is called the dual polymatroid of (E, f).]
2.23 For any polymatroid (E, f), let I = {A | f(A) = |A|, A ⊆E}. Show that
(E, I) is an independent system.
2.24 Let (E, I) be an independent system. Deﬁne r(A) = max{|I| | I ∈I,
I ⊆A}. Give an example of (E, I) for which r is not a polymatroid function.
2.25 Let (E, f) be a polymatroid and c a nonnegative cost function on E. Show
that the problem of computing min{c(A) | f(A) ≥k, A ⊆E} has a greedy
approximation with performance ratio H(min{k, γ}), where γ = maxx∈E f({x}).
2.26 Consider the application of Algorithm 2.D to MIN-CDS with the potential
function f(C) = p(C) + q(C). Find a graph G on which the algorithm produces an
approximate solution of size g ≤2|C∗|.
2.27 Given a hypergraph H = (V, S) and a function f : S →N+, ﬁnd a
minimum vertex cover C such that for every hyperedge s ∈S, |C ∩s| ≥f(s).
Prove that this problem has a polynomial-time (1 + lnd)-approximation, where d is
the maximum vertex degree in H.
2.28 Let f : 2E →R be a normalized submodular function. We associate a
weight wi ≥0 with each i ∈E . Consider the following linear program:
maximize

i∈E
wixi
subject to

i∈A
xi ≤f(A),
A ⊆E.
Show that this problem can be solved by the following greedy algorithm:
(1) Sort elements of E and rename them so that w1 ≥w2 ≥· · · ≥wn.
(2) A0 ←∅;
for k ←1 to n do Ak ←{1, 2, . . ., k}.
(3) For k ←1 to n do xi ←f(Ai) −f(Ai−1).
2.29 Let E be a ﬁnite set and p : E →R+ a positive function on E. For every
subset A of E, deﬁne
g(A) =
 
i∈A
p(i)
2
+

i∈A
p(i)2.
Show that g is a supmodular function.

Exercises
79
2.30 Show that the following greedy algorithm for the problem MIN-CDS has
performance ratio 2(1 + H(δ)), where δ is the maximum vertex degree:
Grow a tree T starting from a vertex of the maximum degree. At each
iteration, add one or two adjacent vertices to maximize the increase in
the number of dominated vertices.
2.31 In the proof of Lemma 2.45, a simple argument has been suggested as fol-
lows:
Since m = |C∗| vertices are able to reduce the total number of con-
nected components in the two subgraphs from f(Ci−1) to 2, there must
exist a vertex that is able to reduce at least ⌈(f(Ci−1) −2)/m⌉−1
components (here, the term −1 comes from considering the increase
in the number of black components). Therefore, −Δxif(Ci−1) ≥
(f(Ci−1) −2)/m −1, and hence the lemma holds.
Find the error of this argument and explain why with a counterexample to the above
statement.
2.32 Give a counterexample to show that Lemma 2.47 does not hold if G|X is
not connected.
2.33 A dominating set A in a graph is said to be weakly connected if all edges
incident upon vertices in A induce a connected subgraph. Show that there exists a
greedy H(δ)-approximation for the problem of ﬁnding the minimum-size weakly
connected dominating set of a given graph, where δ is the maximum vertex degree
of the input graph.
2.34 Consider a hypergraph (V, E), where E is a collection of subsets of V . A
subcollection C of E is called a connected set cover if C is a set cover of V and
(V, C) is a connected sub-hypergraph. Show that the problem of ﬁnding a connected
set cover with the minimum cardinality has a greedy H(δ)-approximation, where δ
is the maximum vertex degree of the input hypergraph.
2.35 Consider a hypergraph (V, E), where E is a collection of subsets of V . A
subset A of V is called a dominating set, if every vertex is either in A or adjacent to
A. Furthermore, A is said to be connected if A induces a connected sub-hypergraph.
Design a greedy approximation for computing the minimum connected dominating
set in hypergraphs. Could you reach approximation ratio (1 −ε)(1 + lnδ) for any
ε > 0, where δ is the maximum vertex degree of the input hypergraph?
2.36 A set S of sensors is associated with a graph G = (S, E), and each sensor
s ∈S can monitor a set Ts of targets. Let T be the collection of all targets; i.e.,
T = 
s∈S Ts. Consider the following problem:
CONNECTED TARGET COVERAGE (CTC): Given a sensor graph G =
(S, E) and, for each sensor s ∈S, a target set Ts, ﬁnd a minimum-
cardinality subset A of S such that A can monitor all targets in T and
such that A also induces a connected subgraph of G.

80
Greedy Strategy
Design a greedy approximation for CTC and analyze the performance ratio of your
algorithm.
Historical Notes
The analysis of the greedy algorithm for independent systems was ﬁrst reported by
Jenkyns [1976] and Korte and Hausmann [1978]. Hausmann, Korte, and Jenkyns
[1980] further studied algorithms of this type. Submodular set functions play an im-
portant role in combinatorial optimization. Some of the results presented in Section
2.4 can be found in Wolsey [1982a].
Lund and Yannakakis [1994] proved that for any 0 < ρ < 1/4, there is no
polynomial-time approximation algorithm with performance ratio ρ ln n for MIN-
SC unless NP ⊆DTIME(npoly log n). Feige [1998] improved this result by relaxing
ρ to 0 < ρ < 1. This means that it is unlikely for MIN-SC to have a constant-
bounded polynomial-time approximation. Johnson [1974] and Lov´asz [1975] in-
dependently discovered a polynomial-time greedy H(δ)-approximation for MIN-
SC. Chv´atal [1979] extended the greedy approximation to the weighted case. The
greedy algorithm for MIN-SC can be analyzed in many ways. Slavik [1997] pre-
sented a tight one. The problem WSID was proposed by Du and Miller [1988].
Prisner [1992] presented a greedy approximation for it and claimed that it has per-
formance ratio 1 + ln K. Unfortunately, his proof contained an error. Du, Wu, and
Kelley [1998] ﬁxed this error. They also showed, based on a reduction from the prob-
lem MIN-SC, a lower bound on the performance ratio for WSID. It is known that
the problem MIN-CDS is NP-hard [Garey and Johnson, 1978]. Guha and Khuller
[1998a] presented a greedy algorithm for it with performance ratio 3 + lnδ. Ruan
et al. [2003] gave a new one with performance 2 + lnδ. The (1 + ε)(1 + lnδ)-
approximation can be found in Du et al. [2008].

3
Restriction
Success is restricted only from those who
restrict themselves from success.
— Gillis Triplett
When we design an approximate algorithm by the restriction method, we add some
constraints on an optimization problem to shrink the feasible domain so that the op-
timization problem on the resulting domain becomes easier to solve or approximate.
We may then use the optimal or approximate solutions for this restricted problem to
approximate the original problem.
When we analyze the performance of the algorithms designed with the restric-
tion method, we often reverse the process. Namely, for a minimization problem
minx∈Ω f(x), assume that we restrict the solutions to x ∈Γ ⊆Ω and ﬁnd the
optimal solution y∗∈Γ. For the analysis, we consider an optimal solution x∗to
the original problem, and modify it to a solution y that satisﬁes the restriction. The
difference f(y) −f(x∗) between the costs of these two solutions then can be used
to determine the performance ratio of this approximation. More precisely, as ex-
plained in Section 1.2 (see Figure 1.3), the performance ratio of the algorithm can
be estimated by
f(y∗)
f(x∗) ≤f(y)
f(x∗) = 1 + f(y) −f(x∗)
f(x∗)
.
For a maximization problem maxx∈Ω f(x), the approach is similar. Here, the per-
formance ratio is f(x∗)/f(y∗), and it can be bounded as follows:
f(x∗)
f(y∗) ≤f(x∗)
f(y) =

1 −f(x∗) −f(y)
f(x∗)
−1
.
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9_3, 
© Springer Science+Business Media, LLC 2012 
81

82
Restriction
In this and the next two chapters, we will apply the restriction method and this
analysis technique to a number of optimization problems.
3.1
Steiner Trees and Spanning Trees
The Steiner tree problem is a classical intractable problem with many applications
in the design of computer circuits, long-distance telephone lines, and mail routing,
etc. Given a set of points, called terminals, in a metric space, any minimal tree
interconnecting all terminals is called a Steiner tree (by “minimal,” we mean that
no edge can be deleted). The Steiner tree problem asks, for a given set of terminals,
ﬁnd a shortest Steiner tree, called a Steiner minimum tree (SMT), for them.
In a Steiner tree, the nonterminal vertices are called Steiner points or Steiner
vertices, and the terminals are also called regular points. If there is a terminal with
degree more than 1, then the tree can be decomposed at this terminal. In this way,
a Steiner tree can be decomposed into smaller subtrees such that every terminal in
a subtree is a leaf. These smaller subtrees are called full components. The size of a
full component is the number of terminals in it. Figure 3.1 shows an example of a
full component of size 5. A Steiner tree with only one full component is called a full
tree.
Figure 3.1:
A full component (
indicates a terminal, and ◦indicates a Steiner
point).
Depending on the speciﬁc metric spaces on which the trees are deﬁned, the
Steiner tree problem may assume different forms. The following are three classi-
cal Steiner tree problems.
EUCLIDEAN STEINER MINIMUM TREE (ESMT): Given a ﬁnite set P
of terminals in the Euclidean plane, ﬁnd a shortest network intercon-
necting all terminals in P .
RECTILINEAR STEINER MINIMUM TREE (RSMT): Given a ﬁnite set
P of terminals in the rectilinear plane, ﬁnd a shortest network intercon-
necting all terminals in P .1
1The rectilinear plane is the plane with the distance function d(⟨x1, x2⟩, ⟨y1, y2⟩) = |x1 −y1| +
|x2 −y2|.

3.1 Steiner Trees and Spanning Trees
83
NETWORK STEINER MINIMUM TREE (NSMT): Given an edge-
weighted graph (called a network) G = (V, E) and a subset P ⊆V
of terminals, ﬁnd a subgraph of G with the minimum total weight in-
terconnecting all vertices in P .
All of the above three versions of the Steiner tree problem are NP-hard, and we
need to look for approximations for them. A simple, natural idea is to restrict the
solutions to spanning trees, and use a minimum spanning tree (MST) to approximate
the Steiner minimum tree.
A spanning tree is a Steiner tree with the restriction that no Steiner points exist,
or, equivalently, a Steiner tree in which all full components are of size 2. In gen-
eral, an MST can be computed in time O(n2). In addition, in the Euclidean or the
rectilinear plane, an MST can be computed in time O(n logn).
For any set P of terminals, we let mst(P ) denote the length of the MST for
set P , and smt(P ) the length of the SMT for set P . When we use the MST as
the approximate solution to the Steiner tree problem, the performace ratio of this
algorithm is then the maximum of mst(P )/smt(P ) over all input instances P . In
the following, we show some results on the MST approximation to the Steiner tree
problem.
We ﬁrst consider the problem NSMT. In the problem NSMT, we usually assume
that the input graph is a complete graph, and that the edge weight satisﬁes the trian-
gle inequality. In fact, if an input graph is not complete, we can construct a complete
graph on the same set of vertices and let the weight of each edge {u, v} be the cost
of the shortest path connecting u and v. Thus, the network SMT in the new graph is
equivalent to the original one.
Theorem 3.1 For the problem NSMT, the performance ratio of the MST approxi-
mation is equal to 2.
Proof. Consider an SMT T interconnecting the terminals in P . Note that there exists
an Euler tour T1 of T, which uses each edge in T twice. Since we are working in a
metric space that satisﬁes the triangle inequality, the length of an Euler tour must be
greater than that of an MST. This means that mst(P ) ≤length(T1) ≤2 · smt(P ),
and so the performance ratio of the MST approximation is at most 2.
Next, to show that the performance ratio of the MST approximation cannot be
better than 2, consider a star graph G of n + 1 vertices, each edge of length 1. More
precisely, G is the complete graph with n+1 vertices {0, 1, . . ., n} and has distance
d(0, i) = 1 for all i = 1, 2, . . ., n, and d(i, j) = 2 for all i ̸= j ∈{1, 2, . . ., n}. For
the subset P = {1, 2, . . ., n}, it is clear that smt(P ) = n and mst(P ) = 2(n −1).
Therefore,
mst(P )
smt(P ) = 2(n −1)
n
= 2 −2
n .
As n approaches inﬁnity, this ratio approaches 2. This means that the performance
ratio of the MST approximation cannot be less than 2.
□

84
Restriction
  .C
E
D .
.
B
S
A
  .
.
.
Figure 3.2:
The angle between two edges of an SMT cannot be less than 120◦.
For the problem ESMT, the performance ratio of the MST approximation is
smaller than the general case of NSMT, due to some special properties of the SMTs
in the Euclidean plane.
Lemma 3.2 An SMT in the Euclidean plane has the following properties:
(a) Every angle formed by two adjacent edges is at least 120◦.
(b) Every vertex has degree at most 3.
(c) Every Steiner point has degree exactly 3 with an angle of 120◦between the
three edges.
Proof. Note that (a) implies (b) and (c). To show (a), we assume, for the sake of
contradiction, that there exist two edges forming an angle less than 120◦at point
B. Furthermore, assume that A and C are two points on the two edges of the angle,
respectively, such that |AB| = |BC|. Draw an equilateral triangle △ABD with
D on the opposite side of AB from C, and then draw a circle passing through
the three points A, B, and D. Since ∠ABC < 120◦, the line segment CD must
intersect the circle at a point S between A and B (see Figure 3.2). We claim that
|SA| + |SB| = |SD|. To see this, let E be a point on SD such that |DE| = |SB|.
Note that ∠ADE = ∠ABS and |AD| = |AB|. Therefore, △ADE ∼= △ABS.
This implies that ∠EAD = ∠SAB, and so ∠SAE = ∠BAD = 60◦. Since
∠DSA = ∠DBA = 60◦, we see that △ASE is an equilateral triangle. It follows
that |SE| = |SA|, and the claim is proven.
Now, if we replace the two edges AB and BC by the three edges SA, SB, and
SC, we can shorten the tree, because
|SA| + |SB| + |SC| = |SD| + |SC|
= |CD| < |CB| + |BD| = |AB| + |BC|.
This leads to a contradiction.
□

3.1 Steiner Trees and Spanning Trees
85
S
B
A
Figure 3.3:
Proof of Theorem 3.3. In the above, the solid lines denote SMT(F),
and the dotted lines denote SMT(V (F ) −{A}) ∪AB.
Theorem 3.3 For the problem ESMT, the performance ratio of the MST approxi-
mation is at most
√
3.
Proof. Following the approach outlined at the beginning of this chapter, we consider
a Euclidean SMT T on a set P of n terminals, and modify it into a spanning tree T ′
as follows:
While T contains a Steiner point do
ﬁnd a full component F of T with two terminals A and B
connected to a Steiner point S;
if |AS| ≥|BS|
then T ←(T \ F) ∪{AB} ∪SMT(V (F) −{A})
else T ←(T \ F ) ∪{AB} ∪SMT(V (F) −{B}).
[In the above, we write SMT(Q) to denote the Euclidean SMT of terminals in set
Q, V (F ) to denote the set of terminal points in a tree F, and AB to denote the edge
connecting points A and B.]
We show by induction on the number of Steiner points in T that the spanning
tree T ′ has length at most
√
3 · length(T). If T contains no Steiner point, then
this holds trivially. Assume that T contains a Steiner point. Then there must exist
a full component F of T with a Steiner point S adjacent to two terminals A and
B. Without loss of generality, assume that |AS| ≥|BS|. From Lemma 3.2(c), we
know that ∠ASB = 120◦. It follows that
|AB| =

|AS|2 + |BS|2 −2 cos 120◦· |AS| · |BS|
1/2
=

|AS|2 + |BS|2 + |AS| · |BS|
1/2 ≤

3 · |AS|21/2 =
√
3 · |AS|.
Note that (T \F)∪SMT(V (F)−{A}) contains two connected components, say,
T1 and T2. By the induction hypothesis, for i = 1, 2, the spanning tree T ′
i obtained
from Ti has length at most
√
3 · length(Ti). Therefore, the spanning tree T ′, which
is the union of T ′
1, T ′
2, and AB, has length

86
Restriction
length(T ′) ≤|AB| +
√
3 · length(T1) +
√
3 · length(T2)
≤
√
3 ·

|AS| + length((T \ F ) ∪SMT(V (F ) −{A}))

≤
√
3 · length(T),
since
length(SMT(V (F ) −{A})) ≤length(F −|AS|).
□
In each metric space, the Steiner ratio is the maximum ratio of the lengths be-
tween SMT and MST for the same set of input points. In other words, it is the
inverse of the performance ratio of the MST approximation for SMT. For instance,
Theorem 3.3 means that the Steiner ratio in the Euclidean plane is at least 1/
√
3.
Determining the Steiner ratio in various metric spaces is a classical mathematical
problem. The famous Gilbert and Pollak conjecture states that the Steiner ratio in
the Euclidean plane is equal to
√
3/2 [Gilbert and Pollak, 1968]. This conjecture
was resolved positively by Du and Hwang [1990]. That is, the performance ratio of
the MST approximation for ESMT is exactly 2/
√
3. For the problem RSMT, Hwang
[1972] proved that the Steiner ratio in the rectilinear plane is equal to 2/3.
3.2
k-Restricted Steiner Trees
We say a Steiner tree is a k-restricted Steiner tree if all of its full components have
size at most k. In particular, a spanning tree is a 2-restricted Steiner tree. A naive
idea of improving the MST approximation to the Steiner minimum-tree problems
is to consider k-restricted Steiner trees, for k ≥3, as approximations. Intuitively,
as k gets larger, the minimum tree among all k-restricted Steiner trees, called the
k-restricted Steiner minimum tree, gets closer to the Steiner minimum tree. In other
words, the larger the parameter k is, the better the performance ratio is. In the fol-
lowing, we present an estimation of the performance of the k-restricted Steiner min-
imum tree as an approximation to the Steiner minimum tree.
Following the general approach of the analysis of an approximation designed
with the restriction method, we consider an SMT and modify it into a k-restricted
Steiner tree. To do so, we work on a full component T of size more than k, and
perform the modiﬁcation in two steps: We ﬁrst express the full component T as a
regular binary tree.2 Then, we divide this tree into the union of smaller trees, each
of size k.
To express T as a regular binary tree, we ﬁrst modify it into a tree with the
property that every Steiner point has degree exactly 3. This can be done by adding
zero-length edges and new Steiner points to T. Next, we choose a root r in the
middle of an edge, and convert the tree into a regular binary weighted tree, which is
still called T (see Figure 3.4). In this regular binary tree, the weight of each edge is
its length in the metric space.
2A regular binary tree is a binary tree in which each internal vertex has exactly two child vertices.

3.2 k-Restricted Steiner Trees
87
3
4
5
s1
s2
2
b
c
d
e
f
g
r
0
0
0
s
s
s
s
s
1
0
0
0
s4
s5
s3
c
d
b
f
e
g
a
r
a
a
b
c
d
e
f
g
s1
2
s
Figure 3.4:
Constructing a regular binary tree from a Steiner tree.
Next, we modify this regular binary weighted tree T into a k-restricted Steiner
tree. To do so, we need a lemma about regular binary trees.
Lemma 3.4 For any regular binary tree T, there exists a one-to-one mapping f
from internal vertices to leaves, such that
(a) For any internal vertex u, f(u) is a descendant of u; and
(b) All tree paths p(u) from u to f(u) are edge-disjoint.
Proof. We will construct, by induction on the number of internal vertices in T, a
mapping f satisfying conditions (a) and (b).
If T has only one internal vertex, the lemma is obviously true. So, we assume that
T has more than one internal vertex. Consider an internal vertex x both of whose
two children are leaves. Let its children vertices be y1 and y2. Let T ′ be the tree T
with y1 and y2 deleted (and so x becomes a leaf of T ′). By the inductive hypothesis,
there is a one-to-one mapping g from the internal vertices of T ′ to the leaves of
T ′, satisfying conditions (a) and (b). Now, deﬁne f on the internal vertices of T as
follows:

88
Restriction
+
d
level
d
level
q
Figure 3.5:
Constructing a k-restricted Steiner tree.
f(u) =
⎧
⎪
⎨
⎪
⎩
g(u),
if u ̸= x and g(u) ̸= x,
y1,
if u ̸= x and g(u) = x,
y2,
if u = x.
It is not hard to check that f satisﬁes conditions (a) and (b).
□
Recall that the level of a vertex u in a rooted binary tree is the number of vertices
in the path from the root to u. Also, the level of a rooted binary tree is the maximum
level of a vertex.
We now divide the internal vertices in tree T into groups according to their levels.
Denote q = ⌊log2 k⌋. For each i ≥1, let Ii be the set of internal vertices at the ith
level of the tree T, and Ud = 
i≡d (mod q) Ii. It is clear that sets U1, U2, . . ., Uq
are pairwisely disjoint. Let f be the mapping found in Lemma 3.4 for the tree T.
Denote by ℓ(p(u)) the length of path p(u) from an internal vertex u to the leaf f(u).
Let ℓd = 
u∈Ud ℓ(p(u)). From Lemma 3.4(b), we know that ℓ1 + ℓ2 + · · · + ℓq ≤
smt(P ), where P is the set of terminals of T. By the pigeonhole principle, we can
choose an integer d, 1 ≤d ≤q, such that ℓd ≤smt(P )/q.
For each nonroot vertex u ∈T, let its parent vertex be π(u). We construct a new
tree Td as follows: For each nonroot vertex u ∈Ud, we replace edge (π(u), u) by
a new edge (π(u), f(u)) (see Figure 3.5). We will show that the new tree Td is a
k-restricted Steiner tree with length at most smt(P )(q + 1)/q.
First, we prove that Td is connected. To see this, we note that each replacement of
an edge (π(v), v) by the new edge (π(v), f(v)) keeps v and π(v) connected, since
f(v) is a descendant of v. Therefore, during each step of the construction of Td from
T, all vertices are connected together.
Next, we show that Td is k-restricted; that is, each full component of Td has size
at most k. To see this, we note that each full component of Td must contain either
the root r or a Steiner point u ∈Ud, because each of the other Steiner points in Td
must belong to the same full component as its parent vertex (in the binary tree T). In
addition, any two vertices in Ud ∪{r} must belong to two different full components
of Td, because the edge-replacement operations divide each vertex u ∈Ud and its

3.3 Greedy k-Restricted Steiner Trees
89
parent π(u) into two different full components. Now, consider a full component C
that contains a Steiner point u ∈Ud ∪{r}. Each terminal in C can be reached from
u through a path whose edges, other than the last one, are all in T. Therefore, such a
path contains at most q edges. It means that if we consider C as a binary tree rooted
at u, then it has at most q + 1 levels, and so the number of terminals in C is at most
2q ≤k.
Finally, we check that the total length of Td is bounded by smt(P )(q +1)/q. We
note that, during the construction of Td from T, each edge replacement increases the
edge length by at most ℓ(p(u)) for some u ∈Ud. Thus, the total increase is at most
ℓd, which is bounded by mst(P )/q from our choice of d. Thus, the total length of
Td is at most smt(P )(1 + 1/q).
Theorem 3.5 For k ≥2, the k-restricted SMT is a (1 + 1/⌊logk⌋)-approximation
to the Steiner minimum-tree problem.
Let ρk be the maixmum lower bound of the ratio of the lengths between the SMT
and the k-restricted SMT over the same set of terminals. That is,
ρk = min
P
smt(P )
smtk(P ),
where smtk(P ) is the length of the k-restricted Steiner minimum tree over terminal
points in the set P . This number ρk is called the k-Steiner ratio, which is the inverse
of the performance ratio of the k-restricted SMT as an approximation to the SMT
problem. For convenience and for historical reasons, we will use ρk, instead of its
inverse, in later sections. To summarize our results in terms of ρk, we have
Corollary 3.6 (a) For k ≥2, ρk ≥⌊log k⌋/(⌊log k⌋+ 1).
(b) limk→∞ρk = 1.
In the above, we only proved an upper bound for ρk. The precise value of ρk is
also known (see Borchers and Du [1995]): Write k = 2r +s, with 0 ≤s < 2r; then
we have
ρk =
r · 2r + s
(r + 1)2r + s.
Theorem 3.5 indicates that, for large k, the k-restricted SMT could be a good
approximation solution to SMT if it can be computed in polynomial time. Unfortu-
nately, for k ≥4, it is known that computing the k-restricted SMT is NP-hard, and
for k = 3, it is still open whether the 3-restricted SMT can be computed in polyno-
mial time or whether it is NP-hard. In the next section, we will study how to ﬁnd
good approximations to the k-restricted SMT itself, and use them to approximate
the SMT.
3.3
Greedy k-Restricted Steiner Trees
Since the minimum spanning trees (i.e., the 2-restricted SMTs) can be found by
greedy algorithms in polynomial time, it is natural to try to ﬁnd approximate k-
restricted SMTs by the greedy strategy. Before we present greedy approximations

90
Restriction
for the k-restricted SMTs, we ﬁrst develop a general result for greedy Algorithm
2.D with respect to noninteger potential functions.
Recall the setting of the greedy Algorithm 2.D: Assume that f is a polymatroid
on 2E, and Ωf = {C ⊆E | (∀x ∈E) Δxf(C) = 0}. The problem MIN-SMC
is to compute minA∈Ωf c(A), where c(A) = 
x∈A c(x). Algorithm 2.D ﬁnds an
approximate solution to MIN-SMC as follows:
(1) Set A ←∅.
(2) While there exists an x ∈E such that Δxf(A) > 0 do
select a vertex x that maximizes Δxf(A)/c(x);
A ←A ∪{x}.
(3) Return AG ←A.
Assume that A∗is an optimal solution to the problem MIN-SMC, and AG is
the approximate solution obtained by Algorithm 2.D with respect to the potential
function f and cost function c. Let x1, x2, . . ., xk be the elements in AG in the order
of their selection into the set AG, and denote A0 = ∅and Ai = {x1, x2, . . ., xi},
for i = 1, . . ., k.
Theorem 3.7 Assume that the approximate solution AG produced by Algorithm
2.D satisﬁes the condition Δxif(Ai−1)/c(xi) ≥1 for all i = 1, 2, . . ., k. Then
c(AG) ≤

1 + ln f(A∗)
c(A∗)

· c(A∗).
Proof. Let ai = f(A∗)−f(Ai) for i = 0, 1, . . ., k. Then Δxif(Ai−1) = ai−1 −ai,
and a0 = f(A∗).
Suppose A∗= {y1, y2, . . ., yh}. Then, for each j = 1, 2, . . ., k, we have, from
the greedy choice of xj and Lemma 2.23, that
aj−1 −aj
c(xj)
≥max
1≤i≤h
Δyif(Aj−1)
c(yi)
≥
h
i=1 Δyif(Aj−1)
c(A∗)
≥ΔA∗f(Aj−1)
c(A∗)
= f(A∗) −f(Aj−1)
c(A∗)
=
aj−1
c(A∗).
(3.1)
Hence, for each j = 1, 2, . . ., k,
aj ≤aj−1 ·

1 −c(xj)
c(A∗)

.
(3.2)
Note that
a0 = f(A∗) = f(AG) =
k

i=1
Δxif(Ai−1) ≥
k

i=1
c(xi) = c(AG) ≥c(A∗),

3.3 Greedy k-Restricted Steiner Trees
91
and ak = f(A∗) −f(AG) = 0. Moreover, for each i = 1, 2, . . ., k, ai ≤ai−1
since f is monotone increasing. Thus, there exists an integer r, 0 ≤r ≤k, such that
ar+1 < c(A∗) ≤ar. From (3.1), we know that
ar −ar+1
c(xr+1)
≥
ar
c(A∗).
We divide the numerator of the left-hand side of the above inequality into two parts:
a′ = c(A∗)−ar+1, a′′ = ar −c(A∗) (so that a′ +a′′ = ar −ar+1), and also divide
the denominator into two parts proportionally: c(xr+1) = c′ + c′′, with c′ and c′′
satisfying
a′
c′ = a′′
c′′ = ar −ar+1
c(xr+1) .
Then
a′′
c′′ = ar −ar+1 −a′
c′′
≥
ar
c(A∗).
Hence, by repeatedly applying (3.2), we get
c(A∗) = ar+1 + a′ ≤ar

1 −
c′′
c(A∗)

≤a0

1 −c(x1)
c(A∗)

· · ·

1 −c(xr)
c(A∗)

1 −
c′′
c(A∗)

≤a0 · exp

−c′′ + r
i=1 c(xi)
c(A∗)

,
since 1 + x ≤ex. It follows that
c′′ +
r

i=1
c(xi) ≤c(A∗) · ln
a0
c(A∗).
Note that
k

i=r+2
c(xi) ≤
k

i=r+2
Δxif(Ai−1) = f(A) −f(Ar+1) = ar+1.
Also, a′/c′ ≥ar/c(A∗) ≥1. Therefore,
c(A) ≤c(A∗) · ln
a0
c(A∗) + c′ + ar+1
≤c(A∗) · ln
a0
c(A∗) + a′ + ar+1 = c(A∗)

1 + ln f(A∗)
c(A∗)

.
□
In many cases, the potential function f is closely related to the cost function c
and satisﬁes the condition Δxif(Ai−1)/c(xi) ≥1 of Theorem 3.7, as the cost c(xi)
is usually no more than the savings from Δxif(Ai−1).

92
Restriction
Indeed, we can verify that this condition is satisﬁed by the potential function f
of the following natural greedy algorithm for the k-restricted SMT problem. For a
given set P of terminals, let Qk be the set of all full components of size at most
k (over all possible Steiner trees) on P . For any A ⊆Qk, let MST(P : A) be the
minimum spanning tree on P after every edge in every component of A is contracted
into a single point, and let mst(P : A) denote its length. Then the greedy algorithm
for the k-restricted SMT problem can be described as follows:
(1) A ←∅; T ←MST(P ).
(2) While A does not connect all terminals in P do
ﬁnd K ∈Qk that miximizes (mst(P : A) −mst(P : A ∪K))/c(K);
A ←A ∪K;
T ←MST(P : A).
(3) Output A.
In other words, this is the greedy Algorithm 2.D with respect to the potential func-
tion
f(A) = mst(P ) −mst(P : A).
Lemma 3.8 f(A) = mst(P ) if and only if A forms a connected graph intercon-
necting all terminals.
Proof. Trivial.
□
To prove that f is submodular, we will reduce the general case of k ≥2 to
the special case of k = 2. Since this reduction technique may be applied to other
potential functions, we state it as a separate lemma.
Lemma 3.9 Suppose that g : 2E →R is a monotone increasing, submodular func-
tion,and that C is a collection of subsets of E. Then the function h : 2C →R induced
from g by h(A) = g(
S∈A S) is also monotone increasing and submodular.
Proof. It is clear that h is monotone increasing. To see that h is submodular, let
A ⊆B ⊆C and X ∈C. We need to show that ΔXh(A) ≥ΔXh(B). Since g is
monotone increasing and submodular, we have
Δyg
 
S∈A S

≥Δyg
 
S∈B S

,
for any y ∈E, because A ⊆B implies 
S∈A S ⊆
S∈B S. This inequality can be
extended so that, for any X ⊆E,
ΔXg
 
S∈A S

≥ΔXg
 
S∈B S

.
It follows that
ΔXh(A) = ΔXg
 
S∈A S

≥ΔXg
 
S∈B S

= ΔXh(B).
□

3.3 Greedy k-Restricted Steiner Trees
93
Lemma 3.10 The function f is a polymatroid function on 2Qk.
Proof. Clearly, f is normalized and monotone increasing. To see that it is submodu-
lar, we reduce the general case to the case k = 2. For a given set P of terminals, let
E be the set of all edges connecting terminals in P , and g : 2E →R be the function
deﬁned by g(S) = mst(P ) −mst(P : S) (that is, g is the function f in the case
k = 2). Now, for any T ∈Qk, let e(T) be the set of edges in a spanning tree on the
terminals in T. Then it is easy to see that f(A) = g(
T ∈A e(T)). Thus, by Lemma
3.9, we only need to prove that g is submodular.
Note that
g is submodular and monotone increasing
⇐⇒(∀A ⊆B ⊆E) (∀y ∈E) Δyg(B) ≤Δyg(A)
⇐⇒(∀A ⊆E) (∀x, y ∈E) Δyg(A ∪{x}) ≤Δyg(A)
⇐⇒(∀A ⊆E) (∀x, y ∈E) Δ{x,y}g(A) ≤Δxg(A) + Δyg(A).
From the deﬁnition of g, we have
Δxg(A) = g(A ∪{x}) −g(A) = mst(P : A) −mst(P : A ∪{x}).
So, it sufﬁces to prove, for any A ⊆E and any x, y ∈E,
mst(P : A) −mst(P : A ∪{x, y})
≤(mst(P : A) −mst(P : A ∪{x})) + (mst(P : A) −mst(P : A ∪{y})).
Let T = MST(P : A). This tree T contains a path πx connecting two endpoints
of x and a path πy connecting two endpoints of y. Let ex (and ey) be a longest edge
in πx (in πy, respectively). Then we have
mst(P : A) −mst(P : A ∪{x}) = length(ex),
mst(P : A) −mst(P : A ∪{y}) = length(ey).
In addition, the value of mst(P : A) −mst(P : A ∪{x, y}) can be computed as
follows: Choose a longest edge e′ from πx ∪πy. Notice that T ∪{x, y} −{e′}
contains a unique cycle C. Choose a longest edge e′′ from (πx ∪πy) ∩C. Then we
have
mst(P : A) −mst(P : A ∪{x, y}) = length(e′) + length(e′′).
Now, to show the submodularity of g, it sufﬁces to prove
length(ex) + length(ey) ≥length(e′) + length(e′′).
(3.3)
Case 1. Neither ex nor ey is in πx ∩πy. Without loss of generality, assume
length(ex) ≥length(ey). Then we have length(e′) = length(ex). So, if we

94
Restriction
choose e′ = ex, then (πx ∪πy) ∩C = πy. Hence, we have length(e′′) =
length(ey). It follows that the two sides of (3.3) are equal.
Case 2. ex ̸∈πx ∩πy and ey ∈πx ∩πy. Clearly, length(ex) ≥length(ey).
Hence, we may choose e′ = ex so that (πx ∪πy) ∩C = πy, and length(e′′) =
length(ey). Again, the two sides of (3.3) are equal.
Case 3. ex ∈πx ∩πy and ey ̸∈πx ∩πy. Similar to Case 2.
Case 4. Both ex and ey are in πx ∩πy. In this case, length(ex) = length(ey) =
length(e′) ≥length(e′′). Hence, inequality (3.3) holds.
□
Lemma 3.11 Each element xi, 1 ≤i ≤k, selected by Algorithm 2.D, with respect
to the potential function f, must satisfy the condition Δxif(Ai−1)/c(xi) ≥1.
Proof. It is clear that Δef(Ai−1)/c(e) = 1 for any edge e of MST(P : Ai−1). It
follows that the value Δxif(Ai−1)/c(xi) of the best choice xi, which is greater
than or equal to this value, must be at least 1.
□
Let c(T) denote the length of tree T. The following theorem follows from Theo-
rem 3.7.
Theorem 3.12 Suppose A is the approximate solution produced by Algorithm 2.D
with respect to the potential function f deﬁned above. Then
c(A)
smtk(P ) ≤1 + ln mst(P )
smtk(P ).
Corollary 3.13 Suppose A is the approximate solution produced by Algorithm 2.D.
Then
c(A)
smt(P ) ≤ρ−1
k

1 + ln ρk
ρ2

.
Proof. By Theorem 3.12,
c(A)
smt(P ) ≤smtk(P )
smt(P )

1 + ln smt(P )/smtk(P )
smt(P )/mst(P )

.
Note that
smt(P )
smtk(P ) ≥ρk
and
smt(P )
mst(P ) ≥ρ2.
Now, the corollary follows from the observation that the function (1 + ln(x/a))/x
is monotone decreasing when x ≥a.
□
Note that limk→∞ρk = 1. Thus, when k goes to ∞, the greedy Algorithm 2.D
produces approximate solutions with performance ratio close to 1 −lnρ2.
In the above analysis, the condition in Theorem 3.7 that the selected element
x always satisﬁes Δxf(Ai−1)/c(x) ≥1 is critical. Suppose this condition does
not hold; can we still get a good estimate of the performance ratio of the greedy
Algorithm2.D? The answer is yes, but we may need to modify the potential function

3.3 Greedy k-Restricted Steiner Trees
95
B
s
s
A
s
A
+U
K
K
’s
E
E
D
D
C
B
F
F
A
C
’
K
’
K
’’s
’s
Figure 3.6:
Operation K ⊎K′.
f and/or the cost function c so that a property similar to the condition of Theorem
3.7 still holds. In the following, we present such an example, which gives a better
approximation for NSMT.
The idea of this greedy algorithm is as follows: It again begins with T =
MST(P ). At each iteration, it selects a full component K in Qk, replaces T by
the union of T and K, and then eliminates edges from the union until it does not
have a cycle. The greedy strategy suggested by Algorithm 2.D would select K to
maximize the saving of this process relative to the cost c(K). However, since the
saving here is not necessarily greater than or equal to c(K), Theorem 3.7 cannot be
applied directly, and so we need to modify this strategy.
Before we describe how to modify this algorithm, we ﬁrst deﬁne the notion of
the union of two Steiner trees. For A, B ∈Qk, we let the union A ⊎B be the graph
obtained from A and B by identifying the same terminals in A and B, but keeping
separate copies of the same Steiner vertices (see Figure 3.6). More precisely, sup-
pose A has terminals TA, Steiner vertices SA, and edges EA; and B has terminals
TB, Steiner vertices SB, and edges EB. Then A⊎B has terminals TA ∪TB, Steiner
vertices SA⊎B = {sA | s ∈SA} ∪{sB | s ∈B}, and edges EA⊎B = EA ∪EB.3
This deﬁnition of operation ⊎can also be extended to two subgraphs A and B.
Now we can deﬁne the potential function g for this greedy algorithm. For con-
venience, we deﬁne ΔKg(T) directly and denote it by gT (K): For A ⊆Qk and a
Steiner tree T on P , let
gT(A) = c(T) −mst

T ⊎
 
K∈A K

.
Lemma 3.14 Let T be a Steiner tree on terminal set P . Then, for K, K′ ∈Qk,
gT (K ⊎K′) ≤gT(K) + gT (K′).
Proof. It sufﬁces to show that
mst(T ⊎K) −mst(T ⊎K ⊎K′) ≤gT(K′).
(3.4)
3Note that if |TA| > 2, then all edges in EA must have a Steiner vertex as an endpoint. This implies
that EA ∩EB = ∅unless TA = TB has size 2.

96
Restriction
We ﬁrst study how to get the MST of T ⊎K′. Suppose T ⊎K′ has a cycle base of
size h.4 Then, MST(T ⊎K′) can be found as follows:
For i ←1 to h do
ﬁnd a cycle Qi in (T ⊎K′) \ {e1, . . ., ei−1};
remove a longest edge ei from cycle Qi.
We can express gT (K′) in terms of the edges ei as follows:
gT(K′) =
h

i=1
c(ei) −c(K′).
Next, we consider the MST of graph H = MST(T ⊎K) ⊎K′. Again, H has a
cycle base of size h, and we can ﬁnd MST(H) by ﬁnding h cycles Q′
i, 1 ≤i ≤h,
in H and removing a longest edge e′
i from each cycle Q′
i. In order to prove (3.4), we
need to show that the total cost of the removed edges is no more than the total cost
of c(ei), 1 ≤i ≤h. This property can be proved by modifying, at each stage, cycle
Qi to form a new cycle Q′
i so that each edge in Q′
i is no longer than c(ei). More
precisely, we can ﬁnd MST(H) as follows:
For i ←1 to h do
ﬁnd, from Qi, a cycle Q′
i in H \ {e′
1, . . ., e′
i−1} with the property
that all edges in Q′
i are no longer than ei;
delete a longest edge e′
i from Q′
i.
To see how to ﬁnd Q′
i from Qi with the desired property, let H1 = MST(T ⊎K).
If Qi is a cycle in H, then let Q′
i = Qi. On the other hand, if Qi is not a cycle in H,
that is, if there is an edge {u, v} in Qi \ H, then this edge must be in T and hence in
(T ⊎K) \ H1. Thus, H1 must contain a path πu,v from u to v, which, together with
{u, v}, forms a cycle in T ⊎K. In addition, since H1 is a minimum spanning tree
of T ⊎K, {u, v} must be a longest edge in this cycle. (Note that this cycle cannot
be identical to Qi, since Qi must contain at least one edge in K′.) Thus, for each
edge {u, v} in Qi that is not in H \ {e′
1, . . ., e′
i−1}, we can replace it by a path πu,v
in H in which each edge is no longer than {u, v}. (This is also true for edges in
Qi ∩{e′
1, . . ., e′
i−1}, since each e′
j, with j < i, was deleted from a cycle Q′
j in H.)
Repeating this on all edges in Qi \ H, we obtain a cycle Q′
i in H with the required
property.
This implies that
gMST(T ⊎K)(K′) = mst(T ⊎K) −mst(T ⊎K ⊎K′)
= h
i=1 c(e′
i) −c(K′) ≤h
i=1 c(ei) −c(K′) = gT(K′),
and the lemma is proven.
□
4A cycle base in a graph is a minimal set of cycles from which all cycles in the graph can be generated.

3.3 Greedy k-Restricted Steiner Trees
97
Loss 
T
ζ
(
)
T
(
)
T
Figure 3.7:
An example of Loss(T) and ζ(T).
Note that we write gT(K) to denote ΔKg(T). So the potential function of the
following greedy algorithm is submodular.
Algorithm 3.A (Greedy Algorithm for NSMT)
Input: A complete graph G = (V, E) with edge cost c, and P ⊆V .
(1) Set T ←MST(P ).
(2) While there exists a K ∈Qk such that gT(K) > 0 do
select K ∈Qk that maximizes gT(K)/c(K);
T ←MST(T ⊎K).
(3) Output TG ←T.
As we pointed out earlier, the function gT, unfortunately, does not necessarily
satisfy the condition of Theorem 3.7, and so the performance of the above algo-
rithm is hard to estimate. To resolve this problem, Robin and Zelikovsky [2000]
introduced a new technique based on the notion of loss of a Steiner tree. The loss
of a Steiner tree T, denoted by Loss(T), is the shortest forest connecting all Steiner
points to terminals. We write loss(T) to denote its length. In addition, we deﬁne
ζ(T) to be the tree obtained from T by contracting every edge in Loss(T) into a
point. We show Loss(T) and ζ(T) in Figure 3.7. Note that although ζ(T), as shown
in the ﬁgure, looks like a spanning tree of T, the length of its edges may be shorter
than the original edge length.
Proposition 3.15 For any Steiner tree T, loss(T) ≤length(T)/2.
Proof. We can construct recursively a forest L connecting each Steiner point in T to
a terminal as follows:
While there is a Steiner point do
ﬁnd a Steiner point S adjacent to two terminals A and B;
add to L the shorter of the two edges SA and SB;
reset S as a terminal point.
It is clear that this forest L has length at most one half of length(T).
□

98
Restriction
The following is a key lemma relating the cost c(T) of a k-restricted Steiner tree
T with loss(T).
Lemma 3.16 Let T be a k-restricted Steiner tree. If, for all K ∈Qk, gT(K) ≤0,
then
c(T) ≤smtk(P ) + loss(T).
Proof. Suppose SMTk(P ) is the union of full components K1, . . ., Kp, each of size
at most k. Then, by Lemma 3.14, we have
gT (K1 ⊎· · · ⊎Kp) ≤
p

i=1
gT (Ki) ≤0.
That is,
c(T) ≤mst(T ⊎K1 ⊎· · · ⊎Kp).
Note that MST(T ⊎K1 ⊎· · · ⊎Kp) is a shortest tree connecting all vertices in
T ⊎K1 ⊎· · · ⊎Kp, including terminals and all Steiner vertices in T, K1, . . ., Kp,
using the edges in T, K1, . . ., Kp. But SMTk(P ) ∪Loss(T) is just such a tree. It
follows that c(T) ≤smtk(P ) + loss(T).
□
This lemma suggests that we can use loss(K) instead of c(K) as the cost of
K in Algorithm 3.A. In addition, since we changed the cost to loss(K), the sav-
ing gT(K) needs to be adjusted accordingly. That is, at each iteration, we only add
ζ(K), instead of K, to the current Steiner tree T, to calculate gT (K) (in the follow-
ing algorithm, we call this new tree H).
Algorithm 3.B (Robin–Zelikovsky Algorithm for NSMT)
Input: A complete graph G = (V, E) with edge cost c, and P ⊆V .
(1) Set E∗←{K ∈Qk | loss(K) > 0}; T ←MST(P ); H ←MST(P ).
(2) While there exists a K ∈E∗such that gH(K) > 0 do
select a smallest K ∈E∗that maximizes gH(K)/loss(K);
T ←MST(T ⊎K);
H ←MST(H ⊎ζ(K)).
(3) Output TG ←T.
To analyze the performance of Algorithm 3.B, we observe the following proper-
ties of the tree H. In the following,for i ≥1, we let Ki denote the full component K
selected at the ith iteration, and Hi the Steiner tree H at the end of the ith iteration.
Lemma 3.17 For each i ≥1, MST(Hi−1 ⊎Ki) must contain all edges of Ki.
Proof. For the sake of contradiction, suppose e is an edge in Ki that is not in
MST(Hi−1 ⊎Ki). Then we claim that there must be a Loss(Ki) that does not
contain e.

3.3 Greedy k-Restricted Steiner Trees
99
To see this, let us consider how to ﬁnd Loss(Ki). In general, for A ∈Qk, we can
ﬁnd Loss(A) as follows: Let Z(A) be the complete graph on the terminals in A, with
edge cost equal to zero for all edges. Let B = MST(Z(A) ⊎A). Then we observe
that the edges in A ∩B must be a Loss(A), since all terminals are connected in B
by edges in Z(A). Now, consider the speciﬁc case of Loss(Ki) here. We can add
Z(Ki) to Hi−1⊎Ki, and consider B = MST(Hi−1⊎Ki⊎Z(Ki)). From the above
observation, we see that the edges in Ki∩B form a Loss(Ki). Now, since e is not in
MST(Hi−1⊎Ki), there is a minimum spanning tree B = MST(Hi−1⊎Ki⊎Z(Ki))
that does not contain e. (We can ﬁnd such a tree B by adding, one by one, an
edge e′ ∈Z(Ki) to MST(Hi−1 ⊎Ki) and then removing a longest edge from the
cycle that resulted from the addition of e′.) It follows that the corresponding forest
Loss(Ki) does not contain e. This completes the proof of the claim.
Now, we note that e divides Ki into two parts C and D. Since e ̸∈MST(Hi−1 ⊎
Ki), we have gHi−1(Ki) = gHi−1(C ⊎D). By Lemma 3.14,
gHi−1(Ki) ≤gHi−1(C) + gHi−1(D).
If e connects a terminal to a Steiner vertex, then either C or D is a single terminal
point, and the other is K′
i = Ki \ {e} ∈Qk; and we have gHi−1(Ki) = gHi−1(K′
i).
Moreover, loss(Ki) = loss(K′
i). Hence,
gHi−1(Ki)
loss(Ki) = gHi−1(K′
i)
loss(K′
i) .
However, K′
i is smaller than Ki, and this contradicts the greedy choice of Ki in
Algorithm 3.B.
On the other hand, if both endpoints of e are Steiner vertices, then we have
loss(Ki) = loss(C) + loss(D), and so
gHi−1(Ki)
loss(Ki) ≤gHi−1(C) + gHi−1(D)
loss(C) + loss(D)
≤max
gHi−1(C)
loss(C) , gHi−1(D)
loss(D)

.
Again, this is a contradiction to the greedy choice of Ki. So, the lemma is proven.
□
Lemma 3.18 For each i ≥1,
gHi−1(Ki) + loss(Ki) = c(Hi−1) −c(Hi).
Proof. From Lemma 3.17, we know that Ai = MST(Hi−1 ⊎Ki) contains Ki. In
addition, if we change the cost of each edge in Loss(Ki) to zero, we obtain the
tree ζ(Ki), and since the edge cost of ζ(Ki) is no more than that of Ki, Hi =
MST(Hi−1⊎ζ(Ki)) must also contain ζ(Ki). Therefore, the edges in trees Ai \Ki
and Hi \ ζ(Ki) are identical. Thus, the difference between the costs of the two trees
Ai and Hi is just c(Ki) −c(ζ(Ki)) = loss(Ki). That is,
c(Hi) = mst(Hi−1 ⊎ζ(Ki)) = mst(Hi−1 ⊎Ki) −loss(Ki).

100
Restriction
In addition, by the deﬁnition of gHi−1, we know that
gHi−1(Ki) = c(Hi−1) −mst(Hi−1 ⊎Ki).
It follows that
gHi−1(Ki) + loss(Ki) = c(Hi−1) −c(Hi).
□
Now, we are ready to estimate the performance ratio of the greedy Algorithm
3.B. The analysis is similar to that of Theorem 3.7.
Theorem 3.19 The greedy Algorithm 3.B produces an approximate solution for
NSMT with cost at most
smtk(P ) + lossk · ln

1 + mst(P ) −smtk(P )
lossk

,
where lossk = loss(SMTk(P )).
Proof. Assume that greedy Algorithm 3.B halts after m iterations. For 1 ≤i ≤m,
let Ki denote the full component K selected at the ith iteration in Algorithm 3.B,
and Hi the tree H at the end of the ith iteration. For convenience, we also let li =
loss(Ki) and gi = gHi−1(Ki). By Lemma 3.18,
c(Hi−1) −c(Hi) = gi + li.
Let Y1, . . ., Yh be all full components of SMTk(P ). Then, by the greedy strategy
and Lemma 3.14,
gi
li
≥max
1≤j≤h
gHi−1(Yj)
loss(Yj) ≥
h
j=1 gHi−1(Yj)
h
j=1 loss(Yj)
≥
gHi−1
 h
j=1 Yj

lossk
= c(Hi−1) −smtk(P )
lossk
.
Hence,
c(Hi−1) −c(Hi)
li
= gi + li
li
≥1 + c(Hi−1) −smtk(P )
lossk
.
Denote ai = c(Hi) + lossk −smtk(P ). Then we can rewrite the above inequality
as
ai−1 −ai
li
≥ai−1
lossk
;
that is,
ai ≤ai−1

1 −
li
lossk

≤ai−1 · exp

−
li
lossk

.
(3.5)
We note that by Lemma 3.16, c(Hm) ≤smtk(P ) and, hence, am = c(Hm) +
lossk −smtk(P ) ≤lossk. Moreover, a0 = mst(P ) + lossk −smtk(P ) ≥lossk.
Therefore, we can ﬁnd an integer i such that ai+1 < lossk ≤ai. (If am = lossk,

3.3 Greedy k-Restricted Steiner Trees
101
then set i = m.) Divide ai −ai+1 into a′ and a′′ by a′ = ai −lossk and a′′ =
lossk −ai+1. Also, divide li+1 into c′ and c′′ proportionally so that c′ + c′′ = li+1
and
a′
c′ = a′′
c′′ = ai −ai+1
li+1
.
Note that
ai −lossk
c′
= a′
c′ = ai −ai+1
li+1
≥
ai
lossk
.
Thus,
lossk ≤ai

1 −
c′
lossk

≤ai · exp

−
c′
lossk

.
Applying (3.5) recursively to the above inequality, we get
lossk ≤a0 · exp

−c′ + li + · · · + l1
lossk

,
or
l1 + · · · + li + c′ ≤lossk · ln
a0
lossk
= lossk · ln

1 + mst(P ) −smtk(P )
lossk

.
Now let us estimate the cost of the output approximation TG of Algorithm 3.B.
Since the cost of the approximate Steiner tree T in each iteration is decreasing,
c(TG) is at most mst(H0⊎K1⊎· · ·⊎Ki+1). To estimate this value, we can construct
a spanning tree S for H0⊎K1 ⊎· · ·⊎Ki+1 as follows: We ﬁrst put L = Loss(K1)∪
· · · ∪Loss(Ki+1) into S; then we contract each edge of L into a single point, ﬁnd
an MST of the resulting graph, and add it to S. It follows that
c(TG) ≤mst(H0 ⊎K1 ⊎· · · ⊎Ki+1)
≤c(S) = mst(H0 ⊎ζ(K1) ⊎· · · ⊎ζ(Ki+1)) + l1 + · · · + li+1
= c(Hi+1) + l1 + · · · + li+1.
Furthermore, we know that
c(Hi+1) = c(Hi) −(ai −ai+1) = c(Hi) −a′ −a′′,
and that
a′′
c′′ = ai −ai+1
li+1
≥
ai
lossk
≥1.
So, we have
c(TG) ≤c(Hi+1) + l1 + · · · + li+1
= c(Hi) −a′ −a′′ + l1 + · · · + li + c′ + c′′
= (c(Hi) −a′) + (l1 + · · · + li + c′) + (c′′ −a′′)
≤smtk(P ) + lossk · ln

1 + mst(P ) −smtk(P )
lossk

.
□

102
Restriction
Since the value of
lossk · ln

1 + mst(P ) −smtk(P )
lossk

is increasing with respect to lossk, we get, from Proposition 3.15,
lossk · ln

1 + mst(P ) −smtk(P )
lossk

≤smtk(P )
2
ln

1 + mst(P ) −smtk(P )
smtk(P )/2

.
Therefore, the performance ratio of Algorithm 3.B is bounded by
smtk(P)
smt(P )

1 + 1
2 ln

1 + 2 · mst(P )/smt(P ) −smtk(P )/smt(P )
smtk(P )/smt(P )

≤ρ−1
k

1 + 1
2 ln

1 + 2 · 2 −ρ−1
k
ρ−1
k

= ρ−1
k

1 + ln(4ρk −1)
2

.
When k →∞, we have ρk →1, and hence ρ−1
k (1 + ln(4ρk −1)/2) tends to
1 + (ln 3)/2 < 1.55.
Corollary 3.20 The greedy Algorithm 3.B produces a (1.55)-approximation for
NSMT.
3.4
The Power of Minimum Spanning Trees
Minimum spanning trees play an important role in the design of approximation al-
gorithms for network optimization problems. They are a natural candidate for ap-
proximation when the objective function is a function of the total edge length. In
some cases, they might be a good approximation even if the objective function is
not a function of edge length. This is due to many special properties of minimum
spanning trees. The analysis of such approximation algorithms often depends on
these special properties. We present three examples in this section.
First, consider the following problem:
STEINER TREES WITH MINIMUM STEINER POINTS (ST-MSP): Giv-
en n terminals in the Euclidean plane and a number r > 0, ﬁnd a Steiner
tree interconnecting all terminals with the minimum number of Steiner
points such that the length of each edge is at most r.
The problem ST-MSP arises from the design of networks in which there are limits
on the edge length. For instance, in a wavelength-division multiplexing (WDM)
optical network, each node has a limited transmission power, and signals can only
travel a limited distance r. Then, ﬁnding the optimal networks under this restriction
is just the problem ST-MSP.
A Steiner tree as a feasible solution for ST-MSP may contain a Steiner point
of degree 2. We can obtain a Steiner tree T ′ with only Steiner points of degree 2

3.4 Power of Minimum Spanning Trees
103
by adding Steiner points on the edges of a spanning tree T. We call such a tree a
Steinerized spanning tree (induced from the spanning tree T). In the following, we
will reserve the term “minimum spanning tree” for a spanning tree with the mini-
mum length, and use the term “minimum Steinerized spanning tree” for a Steiner-
ized spanning tree with the minimum number of Steiner points.
A simple heuristic for the problem ST-MSP is to use a minimum Steinerized
spanning tree as an approximate solution. The following lemma shows that the
Steinerized spanning tree induced from a minimum spanning tree is, in fact, a min-
imum Steinerized spanning tree.
Lemma 3.21 Let T be a minimum spanning tree on a set P of terminals, and r a
positive real number. Suppose, for each edge e in T, we break it into shorter edges
of length at most r by adding the minimum number of Steiner points on e. Then the
resulting tree is a minimum Steinerized spanning tree.
Proof. Let T ∗be an MST on P and T ′ an arbitrary spanning tree on P . Let E(T ∗)
and E(T ′) be their corresponding edge sets. Then there is a one-to-one, onto map-
ping f from E(T ∗) to E(T ′) such that
length(e) ≤length(f(e)),
for all e ∈E(T ∗) (see Exercise 3.16). The lemma follows immediately from this
fact.
□
Theorem 3.22 Suppose that, for any set of terminals as an input to the problem
ST-MSP, there always exists a minimum spanning tree with vertex degree at most
d. Then the minimum Steinerized spanning tree is a (d −1)-approximation for ST-
MSP.
Proof. Let P be a set of terminals and r > 0 a given real number. Let S∗be an
optimal tree on input P for ST-MSP with respect to the edge-length limit r. Sup-
pose S∗contains k Steiner points s1, s2, . . ., sk, in the order of their occurrence
in a breadth-ﬁrst search starting from a terminal point of S∗. Let N(Q) denote the
number of Steiner points in a minimum Steinerized spanning tree on Q. We claim
that, for 0 ≤i ≤k −1,
N(P ∪{s1, . . ., si}) ≤N(P ∪{s1, . . ., si, si+1}) + d −1.
(3.6)
In other words, we claim that we can eliminate Steiner points sk, sk−1, . . ., s1, one
by one, and convert S∗into a Steinerized spanning tree, adding at most d −1 new
Steiner points in each step.
To prove this claim, consider a minimum spanning tree T for P ∪{s1, . . ., si,
si+1}, with degree at most d. Suppose si+1 is adjacent to vertices v1, . . ., vj, where
j ≤d, in T. Write d(x, y) to denote the Euclidean distance between two points
x and y. Then we must have d(vℓ, si+1) ≤r for some 1 ≤ℓ≤j, because, by
the ordering of Steiner points s1, . . ., sk, we know that one of the vertices in P ∪

104
Restriction
is
+1
is
+1
v
v
v
v
v
v
v
v
v
v
 5
 4
2
 3
1
 4
2
 3
 5
1
Figure 3.8:
Proof of Theorem 3.22.
{s1, . . ., si} has distance at most r from si+1. Without loss of generality, assume
that d(v1, si+1) ≤r.
Now, we can get a spanning tree T ′ on P ∪{s1, . . ., si} by deleting j edges
{si+1, v1}, . . ., {si+1, vj}, and adding j−1 edges {v1, v2}, . . ., {v1, vj} (see Figure
3.8). Note that, for each 2 ≤ℓ≤j,
d(v1, vℓ) ≤d(v1, si+1) + d(si+1, vℓ) ≤r + d(si+1, vℓ).
Thus, we only need one more degree-2 Steiner point to break the edge {v1, vℓ} into
shorter edges of length ≤r than to break the edge {si+1, vℓ}. This means that the
minimum Steinerized spanning tree induced from T ′ contains at most j −1 more
Steiner points than that induced from T. Now, (3.6) follows from Lemma 3.21.
Finally, by applying (3.6) repeatedly, we get
N(P ) ≤N(P ∪{s1, . . ., sk}) + k(d −1) = k(d −1).
□
Note that for any set P of terminals in the Euclidean plane, there is a minimum
spanning tree of P with degree at most 5 (see Exercise 3.19). Therefore, we have
the following result:
Corollary 3.23 The minimum Steinerized spanning tree is a 4-approximation for
ST-MSP in the Euclidean plane.
Next, we consider a problem closely related to ST-MSP.
BOTTLENECK STEINER TREE (BNST): Given a set P of terminals
in the Euclidean plane and a positive integer k, ﬁnd a Steiner tree on
P with at most k Steiner vertices which minimizes the length of the
longest edge.
A simple approach to this problem is to use Steinerized spanning trees to ap-
proximate it. The following algorithm, called the Optimal Cut, applies the greedy
strategy to obtain a Steinerized spanning tree from a given spanning tree T.
Algorithm 3.C (Optimal Cut for the Steinerized spanning tree)
Input: A spanning tree T on a set P of terminals in the Euclidean plane and an
integer k > 0.

3.4 Power of Minimum Spanning Trees
105
(1) For each edge e ∈T do n(e) ←0.
(2) For i ←1 to k do
select an edge e ∈T with the maximum length(e)
n(e) + 1 ;
set n(e) ←n(e) + 1.
(3) For each edge e ∈T do
cut e evenly with n(e) Steiner points.
The following two lemmas show that Algorithm 3.C gives the best Steinerized
spanning tree if we start with an MST T.
Lemma 3.24 Among the Steinerized spanning trees induced by T with at most k
Steiner points, the optimal cut tree produced by Algorithm 3.C has the minimum
value of the longest edge length.
Proof. Let e1, e2, . . ., et be all edges of T. Let T be the collection of trees that
can be obtained from T by adding k Steiner points on edges e1, e2, . . ., et, and let
opt(k; e1, . . ., et) be the minimum value of the longest edge length of T ′, among all
possible trees T ′ in T . We will prove the lemma by induction on k.
For k = 0, it is trivial. For the general case, we assume that, after adding k
Steiner points to T according to Algorithm 3.C,
opt(k; e1, . . ., et) = max
1≤i≤t
length(ei)
n(ei) + 1 .
Without loss of generality, assume that
length(e1)
n(e1) + 1 = max
1≤i≤t
length(ei)
n(ei) + 1 .
From Algorithm 3.C, we need to prove
opt(k + 1; e1, . . ., et) = max

max
2≤i≤t
length(ei)
n(ei) + 1 , length(e1)
n(e1) + 2

.
(3.7)
We ﬁrst observe that in Algorithm 3.C, on input e1, e2, . . ., et, if we ignore the
steps of adding points on e1, then the remaining steps are exactly those steps in the
algorithm on input e2, . . ., et. Therefore, by the induction hypothesis, we have
opt(k −n(e1); e2, . . ., et) = max
2≤i≤t
length(ei)
n(ei) + 1 .
(3.8)
Furthermore, as the right-hand side of Equation (3.7) is derived from a speciﬁc way
of putting k + 1 Steiner points on tree T, we see that it is greater than or equal to
opt(k + 1; e1, . . ., et). Thus, it sufﬁces to prove
opt(k + 1; e1, . . ., et) ≥max

opt(k −n(e1); e2, . . ., et), length(e1)
n(e1) + 2

.

106
Restriction
Suppose, for the sake of contradiction,
opt(k + 1; e1, . . ., et) < max

opt(k −n(e1); e2, . . ., et), length(e1)
n(e1) + 2

.
(3.9)
Let n∗(e1) denote the number of Steiner points on e1 in an optimal solution for
opt(k + 1; e1, . . ., et). Thus,
opt(k + 1; e1, . . ., et) = max

opt(k + 1 −n∗(e1); e2, . . ., et), length(e1)
n∗(e1) + 1

.
Consider three cases:
Case 1. n∗(e1) ≤n(e1). Note that
opt(k + 1; e1, . . ., et) ≥length(e1)
n∗(e1) + 1 ≥length(e1)
n(e1) + 1 = opt(k; e1, . . ., et).
However, from (3.8), we know that the right-hand side of (3.9) is no greater than
opt(k; e1, . . ., et). This is a contradiction.
Case 2. n∗(e1) = n(e1) + 1. Then,
opt(k + 1 −n∗(e1); e2, . . ., et) = opt(k −n(e1); e2, . . ., et),
and
length(e1)
n∗(e1) + 1 = length(e1)
n(e1) + 2 .
So, the two sides of (3.9) are equal. This is also a contradiction.
Case 3. n∗(e1) > n(e1) + 1. From the induction hypothesis and (3.8), we know
that the right-hand side of (3.9) is no greater than opt(k; e1, . . ., et). So, we have
opt(k + 1 −n∗(e1); e2, . . ., et) ≤opt(k + 1; e1, . . ., et) < opt(k; e1, . . ., et).
Also, from n∗(e1) > n(e1) + 1, we get
length(e1)
n∗(e1)
< length(e1)
n(e1) + 1 = opt(k; e1, . . ., et).
Hence,
max

opt(k + 1 −n∗(e1); e2, . . ., et), length(e1)
n∗(e1)

< opt(k; e1, . . ., et).
In other words, there is a Steinerized spanning tree T ′ induced by T with n∗(e1) −
1 Steiner points on e1, and k −(n∗(e1) −1) Steiner points on other edges such
that the longest edge length of T ′ is less than opt(k; e1, . . ., et). This is again a
contradiction.
□
Lemma 3.25 Among the optimal cut Steinerized spanning trees, the one induced by
a minimum spanning tree has the minimum value of the longest edge length.

3.4 Power of Minimum Spanning Trees
107
Proof. Let T be a spanning tree and T ∗a minimum spanning tree. By Exercise 3.16,
there is a one-to-one, onto mapping f from edges in T to edges in T ∗such that
length(e) ≥length(f(e)),
for all e in T. Suppose, in the optimal cut for tree T, there are n(e) Steiner points
on each edge e of T. Then, by putting n(e) Steiner points on each edge f(e) of T ∗,
we get a Steinerized spanning tree induced from T ∗whose longest edge length is
no longer than that of the optimal cut for T. By Lemma 3.24, we see that the longest
edge length of the optimal cut for T ∗is no longer than that of the optimal cut for T.
□
Theorem 3.26 The optimal cut Steinerized spanning tree induced by a minimum
spanning tree is a 2-approximation for BNST.
Proof. The optimal cut tree is the optimal solution to BNST with the restriction
on Steinerized spanning trees. Following the general approach on the analysis of
algorithms based on the restriction method, we will convert an optimal solution T
to BNST to a Steinerized spanning tree with the longest edge length at most twice
that of T.
Without loss of generality, it sufﬁces to consider the case that T is a full Steiner
tree with k Steiner points. Assume that the length of the longest edge length in T
is R. We arbitrarily select a Steiner point s as the root. Call a path from the root to
a leaf a root-leaf path. The length of a root-leaf path is the number of edges on the
path or, equivalently, the number of Steiner points on the path. Let h be the length
of a shortest root-leaf path in T, and d the length of a longest root-leaf path in T
(called the depth of T). We will show by induction on the depth d of T that there
exists a Steinerized spanning tree for all terminals in T with at most k −h Steiner
points such that each edge has length at most 2R.
For d = 0, T contains only one terminal so it is trivial. For d = 1, T contains
only one Steiner point. We directly connect the terminals without any Steiner points.
By the triangle inequality, the distance between two terminals is at most 2R. Thus,
the induction statement holds for d = 1.
Next, we consider the general case of d ≥2. Suppose s has m children
s1, . . ., sm. For each si, 1 ≤i ≤m, there is a subtree Ti rooted at si with depth
≤d −1. Let ki be the number of Steiner points in Ti and hi the length of a shortest
root-leaf path in Ti, from si to a leaf vi (see Figure 3.9). By the induction hypothe-
sis, there exists, for each 1 ≤i ≤m, a Steinerized spanning tree Si for the terminals
in Ti with at most ki −hi Steiner points such that each edge has length at most 2R.
Without loss of generality, assume that h1 ≥h2 ≥· · · ≥hm = h −1. Now, we
connect all trees Si, for 1 ≤i ≤m, into a Steinerized spanning tree S with edges
{v1, v2}, {v2, v3}, . . ., {vm−1, vm}, and add, for each i = 1, . . ., m −1, hi Steiner
points on the edge {vi, vi+1}. Note that S contains
m

i=1
(ki −hi) +
m−1

i=1
hi =
m

i=1
ki −hm = k −1 −hm = k −h

108
Restriction
s
v
s
v
s
v
s
v
s
1
2
3
4
4
1
2
3
Figure 3.9:
Proof of Theorem 3.26. Here, a dark square
denotes a terminal, a
circle ◦denotes a Steiner point in the optimal solution, a dashed line denotes an
edge of the approximate solution, and a shaded circle denotes a Steiner point in the
approximate solution.
Steiner points. Moreover, we note that for each 1 ≤i ≤m −1, the path between vi
and vi+1 in T contains hi + hi+1 + 2 edges. By the triangle inequality, the distance
between vi and vi+1 is at most (hi + hi+1 + 2)R ≤2(hi + 1)R. Therefore, the
hi Steiner points on the edge {vi, vi+1} break it into hi + 1 shorter edges each of
length at most 2R. Thus, all edges in S have length ≤2R, and the induction proof
is complete.
□
Our third example is about a broadcasting problem in a wireless network. We
represent a wireless network by a directed graph in the Euclidean plane. In a wireless
network, a broadcasting routing from a source node s is an out-arborescence T
rooted at s (i.e., a directed, rooted tree T with root s and with edge directions going
from parents to children). Assume that a node u in T has k out-edges, (u, vi), i =
1, . . ., k. Then the energy consumption of u in the routing is
max
1≤i≤k c · d(u, vi)α,
where d is the Euclidean distance function, and c and α are two positive constants
with α ≥2. The energy consumption of a broadcasting routing T is the sum of
energy consumptions over all nodes in T.
MINIMUM-ENERGY BROADCASTING (MIN-EB): Given a set S of
points in the Euclidean plane and a source node s ∈S, ﬁnd a broad-
casting routing from s with the minimum total energy consumption.
A simple idea for an approximation to MIN-EB is to turn a minimum spanning
tree T into a broadcasting routing. Its total energy consumption is at most
c

e∈T
∥e∥α,

3.4 Power of Minimum Spanning Trees
109
where ∥e∥denotes the Euclidean length of the edge e. To establish the performance
ratio of this MST-approximation, we ﬁrst prove the following.
Lemma 3.27 Let C be a disk with center x and radius R, and P a set of points
inside C, including the center x. Let T be a minimum spanning tree on P . Then, for
α ≥2,

e∈T
∥e∥α ≤8Rα.
Proof. Since x ∈P , the edge length of T cannot exceed R. For any 0 ≤r < R, let
Tr be the subgraph of T with vertex set P and all edges in T of length at most r.
Let n(T, r) denote the number of connected components in Tr.
We can rewrite 
e∈T ∥e∥α as

e∈T
∥e∥α =

e∈T
 ∥e∥
0
drα =

e∈T
 R
0
χe(r)drα =
 R
0

e∈T
χe(r)drα,
where
χe(r) =
1,
if 0 ≤r < ∥e∥,
0,
if ∥e∥≤r.
Note that, for ﬁxed r, 
e∈T χe(r) is equal to the number of edges in T that are
longer than r, or, equivalently, n(T, r) −1. Therefore, we have

e∈T
∥e∥α =
 R
0

e∈T
χe(r)drα = α
 R
0
(n(T, r) −1)rα−1dr.
For any r ≤R, let us associate each node u ∈P with a disk D(u; r/2) with
center u and radius r/2. Then these disks have the following properties: For each
connected component C of Tr, the corresponding disks form a connected region.
In addition, since T is a minimum spanning tree, two regions formed by disks cor-
responding to two different connected components of T are disjoint. Furthermore,
since each of these regions contains at least one disk with radius r/2, its area is at
least π(r/2)2. Hence, the boundary of each region has length at least πr, because,
among all connected regions of the same area, circles have the shortest boundary.
For any r ≤R, deﬁne a(P, r) to be the total area covered by disks D(u; r/2),
for all u ∈P. Then we have
a(P, R) =
 R
0
d(a(P, r)) ≥
 R
0
n(T, r)πr d
r
2

= π
2
 R
0
(n(T, r) −1)rdr + πR2
4
= π
4

e∈T
∥e∥2 + πR2
4 .
Note that a(P, R) is contained in a disk centered at x with radius 3R/2. Therefore,

110
Restriction
π
4

e∈T
∥e∥2 + πR2
4
≤a(P, R) ≤π
3R
2
2
,
and so

e∈T
∥e∥2 ≤8R2.
Finally, we note that for every e ∈T, ∥e∥≤R. Thus, for α ≥2,

e∈T
∥e∥
R
α
≤

e∈T
∥e∥
R
2
≤8,
and the lemma holds for all α ≥2.
□
Theorem 3.28 The minimum spanning tree provides an 8-approximation for the
problem MIN-EB.
Proof. Let T ∗be a minimum-energy broadcasting routing. For each node u of T ∗,
we draw a smallest disk to cover all out-edges from u. Let R(D) be the radius of
disk D, and D the set of all such disks. Then disks in D cover all points in the input
set S, and the total energy consumption of T ∗is

D∈D
c(R(D))α.
For each disk D, construct an MST TD connecting all points in D. These MSTs
form an MST T connecting all points in S. By Lemma 3.27, the energy consumption
of T is at most

e∈T
c∥e∥α ≤8

D∈D
c(R(D))α.
Now, from Exercise 3.16, we see that the MST routing is an 8-approximation to
MIN-EB.
□
We remark that the bound 8Rα of Lemma 3.27 can be improved to 6Rα
[Amb¨uhl, 2005]. Thus, the minimum spanning tree is actually a 6-approximation
to MIN-EB.
3.5
Phylogenetic Tree Alignment
In this section, we study a simple application of the restriction method to a problem
in bioinformatics. We ﬁrst give some deﬁnitions.
Let Σ be a set of ﬁnite symbols and “−” a special blank symbol not in Σ. Assume
that there is a metric distance σ : (Σ ∪{−})2 →N between these symbols that
satisﬁes the triangle inequality. For any two strings s = s1s2 · · ·sn, s′ = s′
1s′
2 · · ·s′
n
in (Σ ∪{−})∗that are of the same length, where each si or s′
j denotes a symbol in
Σ ∪{−}, the score between them is

3.5 Phylogenetic Tree Alignment
111
score(s, s′) =
n

i=1
σ(si, s′
i).
For k strings s1, . . ., sk ∈Σ∗, we can align them by inserting the blank sym-
bols into them to make them of the same length. More precisely, an alignment
of s1, s2, . . ., sk ∈Σ∗is a mapping from (s1, . . ., sk) to (s′
1, . . ., s′
k), where
s′
i ∈(Σ ∪{−})∗for 1 ≤i ≤k, such that
(1) |s′
1| = |s′
2| = · · · = |s′
k|,
(2) Each string s′
i, 1 ≤i ≤k, is generated from si with insertion of blanks, and
(3) At any position j, 1 ≤j ≤|s′
1|, at least one string of s′
1, . . ., s′
k has a non-
blank symbol.
Often, we use images (s′
1, . . ., s′
k) or a matrix with rows s′
1, . . ., s′
k to represent
this alignment. For instance, the following matrix represents an alignment of strings
AGGTC, GTTCG, and TGAAC:
⎛
⎜
⎝
A G G T −C −
−G −T T C G
T G −A A C −
⎞
⎟
⎠.
The score of an alignment (s′
1, . . ., s′
k) is deﬁned to be

1≤i<j≤k
score(s′
i, s′
j).
The function score induces a metric distance D between strings in Σ∗:
D(s, s′) = the minimum score of an alignment of (s, s′).
It is not hard to see that the distance function D and the corresponding minimum
score alignment can be computed by dynamic programming.
Lemma 3.29 The minimum score alignment of two strings s and s′ in Σ∗can be
computed by dynamic programming in time O(|s| · |s′|).
Proof. Assume that s = s1s2 · · ·sn and s′ = s′
1s′
2 · · ·s′
m, where each si or s′
j
denotes a symbol in Σ. Denote V (i, j) = D(s1 · · ·si, s′
1 · · · s′
j). Then it is easy to
see that V (0, 0) = 0, V (1, 0) = σ(s1, −), V (0, 1) = σ(−, s′
1); and, for i, j ≥0,
V (i + 1, j + 1) = min
&
V (i, j) + σ(si+1, s′
j+1),
V (i, j + 1) + σ(si+1, −), V (i + 1, j) + σ(−, s′
j+1)
'
.
There are O(nm) entries of V (i, j)’s, and each entry V (i+1, j+1) can be computed
in time O(1) from V (i, j), V (i + 1, j), and V (i, j + 1). Therefore, V (n, m) can be
computed in time O(nm).
□

112
Restriction
AACTG
w:
TCACG
t:
CCTG
s:
ATAG
:
v
ACTG
u:
Figure 3.10:
A tree with labels.
Consider a tree T = (V, E) in which each vertex v is assigned a label sv ∈Σ∗.
An alignment of T is a tree T ′ with the same vertex set and edge set, and pos-
sibly different labels s′
v for v ∈V such that the set {s′
v | v ∈V } is an align-
ment of the set {sv | v ∈V }. The score of the alignment tree T ′ is deﬁned to be

{u,v}∈E score(s′
u, s′
v).
The following lemma shows that the minimum-score alignment tree can be found
in polynomial time.
Lemma 3.30 The minimum-score alignment of tree T has the score value

{u,v}∈E
D(su, sv)
and can be found in time O(nm(n + m)), where n is the number of edges in T and
m is the length of the longest label in T.
Proof. First, we note that the score of an alignment of T cannot be smaller than

{u,v}∈E D(su, sv). Moreover, from alignments for each edge, we can induce an
alignment for the whole tree, preserving score values for every edge. Thus, the
minimum-score alignment of T can reach the lower bound 
{u,v}∈E D(su, sv).
More precisely, we can grow the tree T ′ and adjust the labels iteratively. Let
T ′ = (V ′, E′). Initially, V ′ contains a single vertex v, with a label s′
v = sv, and
E = ∅. At each iteration, we select an edge {u, w} ∈E, with u ∈V ′ and w ̸∈V ′,
and add w to V ′ and {u, w}to E′. We followLemma 3.29 to ﬁnd the minimum score
alignment (s′′
u, s′
w) of (su, sw). Let tu be the alignment of su such that the number
of blanks between any two nonblank symbols in tu is equal to the maximum number
of blanks between them in s′
u and s′′
u. String tu may have more blanks than s′
u or s′′
u.
For each extra blank in tu that is not in s′′
u, we insert a blank, at the corresponding
position, into s′
w. For each extra blank in tu that is not in s′
u, we insert a blank, at
the corresponding position, into each s′
v in T ′ (including s′
u, so that s′
u now is equal
to tu).
To make this process clear, let us look at a simple example. Consider the tree T
in Figure 3.10. Assume that the minimum pairwise alignments of labels are

3.5 Phylogenetic Tree Alignment
113
u:
C−CT−G
v
TCAC−−G
C−−CT−G
A−ACT−G
A−−−TAG
A−−CT−G
AACT−G
A−−TAG
A−CT−G
A−TAG
ACT−G
:
AACT−G
u:
:
v
w:
:
v
u:
:
A−−TAG
A−CT−G
s:
w:
:
v
u:
t:
s:
w
Figure 3.11:
Constructing the minimum score alignment of a tree.
(u, v):
(u, w):
(w, s):
(w, t):
A C T −G
A −C T G
A A C T G
A −A C T G
A −T A G
A A C T G
C −C T G
T C A C −G.
Then the minimum-score alignment T ′ of T can be found as in Figure 3.11.
We note that at each iteration we added blanks to both labels of an edge at the
same positions, and so did not increase its score. Thus, the total score of T ′ remains
equal to 
{u,v}∈E D(su, sv). It is also easy to see that each iteration takes time
O(m2 + nm), and so the total running time is O(nm(n + m)).
□
Now we consider the following problem.
PHYLOGENETIC TREE ALIGNMENT (PTA): Given a rooted tree T
with k leaves labeled with k distinct strings s1, . . ., sk ∈Σ∗, respec-
tively, ﬁnd string labels for internal vertices which minimize the total
alignment score of the tree.
The problem PTA is known to be NP-hard. To ﬁnd an approximation to this
problem, we study a restricted version of PTA, which requires that an internal ver-
tex must have the same label as one of its children. A tree alignment satisfying this
restriction is called a lifted alignment. The following lemma shows that the opti-
mal lifted alignment can be found in polynomial time; thus, it can be used as an
approximation to the problem PTA.
Lemma 3.31 The optimal lifted alignment of a tree T can be computed by dynamic
programming in time O(m2 + k3), where k is the number of leaves in T and m is
the total length of leaf labels in T.

114
Restriction
Proof. Let S = {s1, . . ., sk} be the set of leaf labels in T. For each vertex v in
tree T, let Tv denote the subtree of T rooted at v. Denote by c(v, s) the score of
the best lifted alignment for Tv in which vertex v is labeled by s from S. Suppose
the label for v is ﬁxed to be s. Then one of its children x must also have label s.
Since all labels of the leaves are distinct, this child x is unique. For each other child
y of v, the best label for y is the leaf label s′ in Ty that minimizes the total score of
D(s, s′) + c(y, s′). Thus, we have the following recursive formula for c(v, s):
c(v, s) = c(x, s) +

y ∈child(v)
y ̸= x
min
s′∈leaf(Ty)[D(s, s′) + c(y, s′)].
A dynamic programming algorithm can be designed with this formula running in
time O(m2 + k3). Indeed, we can ﬁrst compute, by Lemma 3.29, all k(k −1)/2
pairwise distances D(si, sj) in time O(m2). Then each c(v, s) can be computed, in
the bottom-up order, from the recurisve formula in time O(k). There are altogether
O(k2) entries of c(v, s)’s. Therefore, the total running time is O(m2 + k3).
□
Next, we need to estimate the performance ratio of the optimal lifted alignment
as an approximation to PTA. By Lemma 3.30, the objective function of the problem
PTA is 
{u,v}∈E(T ) D(su, sv), where su is the label of vertex u.
Following the general approach for the analysis of approximations based on the
restriction method, we consider a tree T ∗with the optimal assignment of labels s∗
v
for internal vertices and modify it into a lifted alignment tree TL. The modiﬁcation
is a bottom-up process according to the following formula:
sv = argminsx
x∈child(v)
D(s∗
v, sx).
That is, initially, we let sv = s∗
v for all leaves v ∈TL. Then, in each iteration, we
select a vertex v in TL with all labels of its children already deﬁned, and choose a
child vertex x of v with the minimum D(s∗
v, sx) and set label sv = sx.
For each edge {v, w}, where w is a child of v, if sv ̸= sw, then we have, by the
triangle inequality,
D(sv, sw) ≤D(sv, s∗
v) + D(s∗
v, sw) ≤2D(s∗
v, sw).
Note that there is a lifted path πw from w to a leaf z in which all vertices have the
same label sw. In particular, the leaf z of πw in the optimal tree T ∗has label s∗
z =
sz = sw (see Figure 3.12). Applying the triangle inequality to the path {v, w} ∪πw
in T ∗, we get
D(s∗
v, sw) = D(s∗
v, s∗
z) ≤D(s∗
v, s∗
w) + the score of πw in T ∗.
That is, we can charge the score D(sv, sw) of TL to the edges in the path {v, w}∪πw
in T ∗, with each edge {x, y} in this path charged with the score 2 · D(s∗
x, s∗
y). Note
that every lifted path πw is uniquely determined by its lowest vertex w. Moreover,
all lifted paths are disjoint, and all edges {x, y} in the lifted paths have score zero

Exercises
115
s
w
vs
vs
vs
vs
ws
ws
w
π
Figure 3.12:
A lifted alignment tree.
in TL. Therefore, each edge {v, w} in T ∗can be charged at most once: If sv ̸= sw,
then it can only be charged by D(sv, sw), since it is not in a lifted path; otherwise, it
is in a lifted path πt, and it can only be charged by D(su, st), where u is the parent
of t. It follows that

{v,w}∈E
D(sv, sw) ≤

{v,w}∈E
sv̸=sw
2D(s∗
v, sw) ≤2

{x,y}∈E
D(s∗
x, s∗
y).
That is, the performance ratio of the optimal lifted alignment is bounded by 2.
Theorem 3.32 The optimal lifted alignment is a polynomial-time 2-approximation
for the problem PTA.
Exercises
3.1 Prove the followingproperties of Steiner minimum trees in the d-dimensional
Euclidean space, for d ≥3:
(a) Every Steiner point is on the two-dimensional plane determined by the three
adjacent vertices.
(b) An angle between any two adjacent edges at a vertex is at least 120◦.
(c) Every Steiner point has degree 3 and the three angles at a Steiner point are
all equal to 120◦.
3.2 Prove the following properties about rectilinear SMTs:
(a) For any set P of terminal points, there exists a rectilinear SMT in which
every maximal vertical or horizontal segment contains a terminal.
(b) For any set P of terminal points, there exists a rectilinear SMT in which
every full component is in one of the following forms:

116
Restriction
(c) The Steiner ratio in the rectilinear plane is 2/3.
3.3 Show that for any rooted tree T, there is a mapping f from the leaves to
the internal vertices such that the paths from leaves v to f(v) form an edge-disjoint
decomposition of tree T.
3.4 Show that for k = 2r +s, where 0 ≤s < 2r, the k-Steiner ratio for network
Steiner trees is
ρk =
r2r + s
(r + 1)2r + s.
3.5 Determine whether or not the following argument is correct: Assume that f
is a potential function in greedy Algorithm 2.D. Set g(A) = f(A) + c(A). Then
Δxg(A)/c(x) = 1 + Δxf(A)/c(x). This means that, using g(A) as a potential
function, greedy Algorithm 2.D would generate the same solution as using f(A).
However, with the potential function g(A), we always have Δxg(A)/c(x) ≥1. By
Theorem 3.7, we conclude that greedy Algorithm 2.D generates a solution within a
factor of 1 + ln(1 + f(A∗)/c(A∗)) from the optimal solution A∗.
3.6 Consider the following greedy algorithm for the problem NSMT: Grow a
tree T starting with the empty set. At each iteration, choose a Steiner point v ̸∈T
that maximizes the number of terminals in G \ T adjacent to v, relative to the edge-
weight. In other words, let E consist of all stars in G that contain a Steiner vertex at
the center and terminals as leaves. For each T ⊆E, deﬁne f(T) = r −1, where r
is the number of leaves in T. Show that the greedy Algorithm 2.D with the potential
function f is a 2-approximation for NSMT and, in addition, the performance ratio 2
is tight for this approximation.
3.7 Consider the problem NSMT. Let T be a minimum spanning tree on terminal
set P . Show that if, for any full component K of size at most k, gT(K) ≤0, then
T is a k-restricted Steiner minimum tree.
3.8 Consider the problem NSMT. For a Steiner tree T on the terminal set P and
a full component K in Qk, deﬁne
gainT (K) = mst(T) −mst(T ∪ζ(K)) −c(K),
and for a subset A of full components,

Exercises
117
gainT (A) = mst(T) −mstT ∪(
K∈A ζ(K)) −
K∈A c(K).
Show the following:
(a) For any two full components K, K′ of tree T, gainT ({K, K′}) ≤
gainT(K) + gainT (K′).
(b) If gainT (K) ≤0 for every full component K of size at most k, then T is a
k-restricted Steiner minimum tree.
(c) If we replace gH(K) with gainH(K) in Algorithm 3.B, it will also give
us a (1.55)-approximation for NSMT. Furthermore, when there are more
than one K ∈E∗having the maximum value of gainH(K)/loss(K), the
choice of K can be arbitrary; in other words, the condition “smallest” for
K in step (2) of greedy Algorithm 3.B can be deleted.
3.9 Show that gT(K) = c(T) −mst(T ⊎K) is a submodular function, but is
not a polymatroid function.
3.10 Suppose f and c are polymatroid functions on 2E in the problem MIN-
SMC. Suppose it is hard to compute the values maxy∈E Δyf(A)/c(y). Therefore,
in greedy Algorithm 2.D, instead of choosing an element x ∈E to maximize
Δxf(A)/c(x), we choose an x such that
α · Δxf(A)
c(x)
≥max
y∈E
Δyf(A)
c(y)
,
for some constant α ≥1. Show that if the element x selected in step (2) always
satisﬁes Δxf(A)/c(x) ≥1, then this modiﬁed greedy algorithm produces a solu-
tion within a factor of 1 + α · ln(f(A∗)/c(A∗)) from the optimal solution c(A∗) of
MIN-SMC.
3.11 Consider a rooted tree T = (V, E) of n leaves, with edge cost c : E →R+,
and any integer k > 0. Let s(v) be the number of leaves in the subtree rooted at v,
and for i = 0, . . ., k,
Vi = {v ∈V | s(v) ≥n(k−i)/k and s(v′) < n(k−i)/k for any child v′ of v}.
Construct a new k-level tree T k with vertex set V , and edge set {(u, v)|u ∈Vi, v ∈
Vi+1, for some i = 0, 1, . . ., k −1; and v is a descendant of u in T}, with the cost
cost(u, v) equal to the total cost of the path from u to v in T. Show that
cost(T k) ≤n1/k · cost(T).
3.12 Consider the following problem:
ACYCLIC DIRECTED STEINER TREE (ADST): For a given acyclic di-
graph G = (V, E) satisfying the transitive relation, i.e., (u, v), (v, w) ∈
E implying (u, w) ∈E, with an edge cost function c : E →R+ sat-
isfying the triangle inequality, a given set P ⊆V , and a point r ∈V ,
ﬁnd a minimum-cost outward-directed tree from r to all vertices in P .

118
Restriction
(a) Let Ak be the set of full Steiner components of at most k levels. For a
subset A ⊆Ak, let f(A) = mst(P ∪{r}) −mst(P ∪{r} : A), where
mst(P ∪{r} : A) is the length of the minimum spanning tree for P ∪{r}
after contracting every component in A into a terminal point. Show that for
k = 1, 2, and any A ⊆Ak, maxT ∈Ak ΔTf(A)/c(T) is polynomial-time
computable.
(b) For any set S ⊆V , let US(s) = {v ∈V | s = argmins∈Sc(s, v)}. For any
A ⊆Ak, and any T ∈Ak, deﬁne gA(T) = ΔTf(A)/c(T). For u ∈V and
k ≥3, compute k-level trees T k(u) recursively as follows.
(1) Let s0 ←argmins∈P∪{r}c(s, u), and T k(u) ←(s0, u).
(2) Set S ←P ∪{u}.
(3) While (∃v ∈US(u)) gT k(u)(T k−1(v)) ≥0 do
v∗←argmaxv∈US(u)gT k(u)(T k−1(v));
T k(u) ←T k(u) ∪T k−1(v∗).
Let T ∗= argmaxT ⊆Akf(T) and u the unique child of the root of T ∗. Show
that
f(T k(u)) · (2 + logn)k−2 ≥f(T ∗).
(c) Show that there is a polynomial-time approximation for ADST with perfor-
mance ratio n1/k(1 + log n)k−1 for any k ≥1.
3.13 Let V be n stations (points) in the Euclidean plane. Each station v ∈V has
a communication range with radius rv, which depends on its energy consumption
Ev according to the formula Ev = crα
v for some constant α ≥2. These commu-
nication ranges induce a digraph G = (V, E) such that (u, v) ∈E if and only
if ru > dist(u, v). They also induce an undirected graph G′ = (V, E′), where
{u, v} ∈E′ if and only if both ru and rv are greater than dist(u, v).
(a) Show that the minimum spanning tree is a 2-approximation for the problem
of minimizing the total energy 
v Ev subject to the condition that the com-
munication ranges induce a connected undirected graph over all stations.
(b) Show that the minimum spanning tree is a 2-approximation for the prob-
lem of minimizing the total energy 
v Ev subject to the condition that the
communication ranges induce a strongly connected directed graph over all
stations.
(c) Find an approximation of a constant performance ratio for the problem of
minimizing the total energy 
v Ev subject to the condition that the commu-
nication ranges induce a weakly connected directed graph over all stations.
3.14 Consider the following problem:
TERMINAL STEINER TREE (TST): Given a complete graph G =
(V, E) with an edge-weight function w : E →R+, which satisﬁes

Exercises
119
v
v
sk
−1
ks
−1
ks
s
s
s
sk
s
s
s
 3
 
 2
1
1
2
3
Figure 3.13:
Step (3) of the algorithm in Exercise 3.15.
the triangle inequality, and a subset P ⊆V of terminals, ﬁnd a short-
est Steiner tree interconnecting all terminals such that all terminals are
leaves.
Let opt denote the length of a minimum solution to this problem. Show the following
results:
(a) For each terminal v, denote by c(v) the closest nonterminal vertex to v.
Then the total length of edges {v, c(v)}, for v ∈V , is at most opt.
(b) The length of the network SMT on all c(v)’s is at most 2 · opt.
(c) All edges {v, c(v)} together with a ρ-approximation of the problem NSMT
on all c(v)’s form a (1 + 2ρ)-approximation for TST.
3.15 Consider the problem TST again. Assume that the problem NSMT is ρ-
approximable. Show that the following algorithm is a (2ρ)-approximation for TST:
(1) G′ ←G \ {{u, v} | u, v ∈P }.
(2) In graph G′, ﬁnd a ρ-approximation T for NSMT on terminals P .
(3) For each v ∈P with deg(v) > 1 do
assume v’s neighbors are s1, . . ., sk, and d(v, s1) = min1≤i≤k d(v, si);
for i ←2 to k do T ←T ∪{s1, si} \ {v, si} (see Figure 3.13).
3.16 Show that for a minimum spanning tree T ∗and any spanning tree T of a
graph G, there exists a one-to-one, onto mapping f between their edge sets E(T ∗)
and E(T) such that length(e) ≤length(f(e)) for each e ∈E(T ∗).
3.17 Consider the following problem:
SELECTED-INTERNAL STEINER TREE (SIST): Given a complete
graph G = (V, E) with an edge-cost function c : E →R+ and two
vertex subsets P and P ′ with P ′ ⊂
̸= P ⊆V , ﬁnd a shortest tree in-
terconnecting all vertices (terminals) in P under the constraint that no
vertex in P ′ can be a leaf.
Any tree satisfying the constraint given above is called a selected-internal Steiner
tree.

120
Restriction
(a) Show that every selected-internal Steiner tree can be modiﬁed into a span-
ning tree with no vertex in P ′ being a leaf such that the total length is at
most twice that of the original tree.
(b) Determine whether or not the minimum spanning tree under the above con-
straint can be computed in polynomial time.
3.18 Consider the problem SIST again. Assume that the problem NSMT is ρ-
approximable. Show that the following algorithm gives a (2ρ)-approximation for
SIST.
(1) Compute a ρ-approximation T for NSMT on subset P .
(2) For each leaf v of T that is in P ′ do
ﬁnd the closest internal vertex mv to v such that either
mv ̸∈P ′ or deg(mv) ≥3;
choose a vertex tv adjacent to mv, but not in the path from v to mv;
replace edge {mv, tv} by edge {v, tv}.
3.19 Show that for any ﬁnite set of points in the Euclidean plane, there exists a
minimum spanning tree with degree at most 5.
3.20 Show that for ST-MSP in the rectilinear plane, the minimum Steinerized
spanning tree is a 3-approximation to it.
3.21 Consider the following problem:
MULTIPLE SEQUENCE ALIGNMENT (MSA): Given k strings s1, . . . ,
sk, ﬁnd their minimum score alignment.
(a) Show that the optimal solution to MSA can be computed by dynamic pro-
gramming in time O(k2kmk), where m is the total length of the given
strings.
(b) Choose si to minimize 
j̸=i D(si, sj). Show that if (s′
1, . . ., s′
k) is an
alignment of (s1, . . ., sk) such that score(s′
i, s′
j) = D(si, sj) for all j ̸= i,
then 
1≤j<h≤k score(s′
j, s′
h) ≤2 · opt, where opt is the score of the opti-
mal solution to MSA.
(c) Use fact (b) to design a 2-approximation for MSA.
3.22 Show that the optimal lifted alignment can actually be computed in time
O(m2 + k2) by dynamic programming, where k is the number of leaves and m is
the total length of leaf labels.
3.23 Consider a binary tree T in which each leaf is labeled with a string. An
alignment of T is uniformly lifted if, at each level j, either every internal vertex is
assigned by the label of its left child or every internal vertex is assigned by the label
of its right child.

Historical Notes
121
(a) Show that the best uniformly lifted alignment can be computed faster than
the best lifted alignment.
(b) Show that the best uniformly lifted alignment is a 2-approximation for PTA.
3.24 Show that, for a binary tree, at least 1/2d−1 of all lifted alignments have
cost less than twice that of the optimal solution to PTA, where d is the depth of the
tree T.
3.25 Show that the average cost of all lifted alignments for a binary tree is less
than twice that of the optimal solution to PTA.
Historical Notes
The Steiner tree problem for three terminal points, that is, the problem of ﬁnding
a point connecting three given points on the Euclidean plane with the shortest total
distance, was ﬁrst proposed by Fermat (see, e.g., Wesolowsky [1993]). This problem
has two generalizations to the cases with more than three terminal points. The ﬁrst
one is to ﬁnd a single point connecting all given terminals with the shortest total
distance. This is commonly called the Fermat problem. The second one is to ﬁnd a
shortest network interconnecting all given terminals. This was called, for unknown
reasons, the Steiner tree problem by Courant and Robbins [1941], although Gauss
in 1836 had already studied this problem.
In a letter to Gauss dated on March 19, 1836, Schumacher mentioned a paradox
about the Fermat problem: For four vertices of a convex quadrilateral, the solution to
the Fermat problem is the intersection point of the two diagonals. When two of the
neighboring vertices of the quadrilateral move toward a same point, the intersection
point of the two diagonals would also move to this point. However, this point is not
the solution to the Fermat problem when the quadrilaterals converge to a triangle.
Two days later, Gauss wrote back to Schumacher and explained the paradox. He
suggested another generalization of the Fermat problem, which aims at the network
structure instead of a single point position. Gauss also discussed in the letter all
possible topologies of the Steiner minimum trees (SMTs) for four terminal points.
(See Schreiber [1986].)
It is well known that the Steiner tree problems in many different topologies are
NP-hard [Karp, 1972; Garey and Johnson, 1977; Garey, Graham, and Johnson,
1977; Foulds and Graham, 1982]. Much effort has been devoted to ﬁnd good ap-
proximate solutions. For the minimum spanning tree (MST) approximation, Hwang
[1972] determined its performance ratio in the rectilinear plane. For the case in the
Euclidean plane, Gilbert and Pollak [1968] conjectured that the performance ra-
tio is exactly 2/
√
3. This conjecture remained open for more than 20 years, and
was ﬁnally proved by Du and Hwang [1990], who adopted many ideas from pre-
vious works in their proof, including Chung and Gilbert [1976], Chung and Gra-
ham [1985], Chung and Hwang [1978], Graham and Hwang [1976], and Rubinstein
and Thomas [1991]. The ﬁrst approximation with the performance ratio better than
that of the MST approximation was found by Zelikovsky [1993] for NSMT. Later,

122
Restriction
Du, Zhang and Feng [1991] showed that such approximations exist in all metric
spaces as long as SMTs for a ﬁxed number of points are computable in polynomial
time. Recently, a (1.55)-approximation has been found for NSMT [Robin and Ze-
likovsky, 2000], and various PTAS algorithms have been designed for ESMT and
RSMT (see Chapter 5). The performance ratios of those approximations for NSMT
are determined through the estimate of the k-Steiner ratio, which was established
by Borchers and Du [1995].
Steiner trees have many variations arising from various applications, such as ter-
minal Steiner trees [Lin and Xue, 2002; Drake and Hougardy, 2004], Steiner trees
with the minimum number of Steiner points [Lin and Xue, 1999, Mandoiu and Ze-
likovsky, 2000], acyclic directed Steiner trees [Zelikovsky, 1997], bottleneck Steiner
trees [Wang and Du, 2002], and selected-internal Steiner trees [Hsieh and Yang,
2007]. In a way, the phylogenetic tree alignment can also be considered as a Steiner
tree problem with a given topology in a special metric space [Ravi and Kececioglu,
1995; Wang and Gusﬁeld, 1996].

4
Partition
But it’s important that we all pull together
to reduce the strain on the grid.
— Gray Davis
The basic idea of partition is to divide the input object into smaller parts so that
each part has a simple solution, and a feasible solution to the input instance can be
constructed by combining the solutions of the smaller parts. The method of partition
can be seen as a special form of restriction; that is, we restrict our attention to the
feasible solutions that can be constructed through partitions.
The partition technique may be divided into two types: nonadaptive partition and
adaptive partition. In nonadaptive partition, the input object is divided into smaller
parts in one round, and the solutions to the smaller parts can be found independently
from each other. In adaptive partition, the input object is divided into smaller parts
by a sequence of subdivision operations recursively, and the solution to each part is
also to be found recursively from the solutions of its own subproblems. We study,
in this chapter, applications of nonadaptive partition to a number of geometric op-
timization problems. The technique of adaptive partition will be studied in the next
chapter.
4.1
Partition and Shifting
We begin with a simple example to demonstrate the basic techniques of partition. In
the following, by a unit disk we mean a disk of diameter 1.
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9_4, 
© Springer Science+Business Media, LLC 2012 
123

124
Partition
Figure 4.1:
Partitioning a square into cells.
UNIT DISK COVERING (UDC): Given n points in the Euclidean plane,
ﬁnd a minimum number of unit disks to cover all given points.
Let P be the set of n given points in the Euclidean plane. Assume that Q is
a square that covers all points in P . The idea of the partition technique for the
problem UDC is as follows: First, we divide the square Q into a grid of squares,
called cells, each of size m × m for some constant m (see Figure 4.1). Then, we
solve the problem UDC for each cell. Finally, we take the union of the solutions of
all cells as the solution to the original input.
Algorithm 4.A (Partition Algorithm for UDC)
Input: A set of points, all lying in square Q; an integer m > 1.
(1) Divide Q into cells, each of size m × m;
Let cell(Q) ←the set of all nonempty cells in Q.1
(2) For each e ∈cell(Q) do
ﬁnd a minimum unit disk cover A(e) for all points in e.
(3) Output A ←
e∈cell(Q) A(e).
To see that Algorithm 4.A runs in polynomial time, we claim that the problem
UDC restricted to a single cell e can be solved in time nO(m2) by an exhaustive
search algorithm. Note that a unit disk can cover a
1
√
2 ×
1
√
2 square. Since a cell of
size m × m can be partitioned into at most ⌈
√
2m⌉2 such squares, at most O(m2)
unit disks are needed to cover all points in a cell.
Assume that a cell e contains ne input points. If there is a point in cell e having
distance greater than 1 from any other point, then we need to use an isolated disk
to cover it. If a point has a distance at most 1 from some other points, then we
1A cell e is nonempty if it contains at least one input point.

4.1 Partition and Shifting
125
Figure 4.2:
There are at most two possible positions for a unit disk with two given
points on its boundary.
can use a disk D to cover it with some other points. In this case, we can move
the disk to a canonical position so that at least two points covered by D lie on
the boundary of D. For any two given points within distance 1, there are at most
two possible canonical positions (see Figure 4.2). Therefore, for ne given points in
cell e, we need to consider at most 2
ne
2

canonical positions. Together with the
earlier observation that we need at most O(m2) unit disks to cover all points in a
cell, we see that, in the exhaustive search algorithm, we need to inspect at most
(ne(ne −1))O(m2) = nO(m2)
e
possible solutions to ﬁnd a minimum disk cover for
cell e.
Thus, over all nonempty cells, the total time for step (2) of Algorithm 4.A is

e∈cell(Q)
nO(m2)
e
≤


e∈cell(Q)
ne
O(m2)
= nO(m2).
Next, we consider the performance of Algorithm 4.A as an approximation to
UDC. Following the general approach of the analysis of approximation algorithms
designed by the restriction method, we consider an optimal solution D∗to UDC
and modify it to a feasible approximate solution. [Here, by a feasible approximate
solution, we mean a solution that can be represented as
(
e∈cell(Q)
S(e),
where each S(e) is a unit disk cover of points in cell e.] The modiﬁcation is simple:
For each disk in D∗that intersects more than one cell, we make additional copies of
the disk and use them to cover points in different cells. If a disk intersects k cells,
2 ≤k ≤4, then we make k −1 additional copies. Each copy is used to cover points
in a different cell.
Note that if there are d disks in D∗that intersect more than one cell, then the
above modiﬁcation adds at most 3d unit disks to D∗. It follows that the solution A
obtained by Algorithm 4.A satisﬁes
|A| ≤|D∗|

1 + 3d
|D∗|

,
since A is the optimal one among all feasible approximate solutions. In the worst
case, it could happen that every disk in D∗intersects four cells (i.e., d = |D∗|),

126
Partition
and we would have |A| ≤4|D∗|. Thus, Algorithm 4.A is a polynomial-time 4-
approximation for UDC (independent of the value of the constant m, as long as
m > 1).
The performance ratio 4 obtained above is rather high, and we would like to im-
prove it by reducing the number d of disks that intersect more than one cell. Before
we do that, let us ﬁrst look at a simple probabilistic analysis of the average-case per-
formance of Algorithm 4.A, which might suggest some ideas for the improvement.
Suppose that the positions of the given points are evenly distributed in square Q,
so that the center of each disk in D∗is evenly distributed in square Q. Note that a
disk intersects more than one cell if and only if its center is within distance 1/2 from
a grid line. It follows that the probability that a disk in D∗does not intersect any grid
line is equal to (m −1)2/m2, and the number of disks that intersect a grid line is
binomially distributed with the success probability p = 1−(m−1)2/m2 = (2m−
1)/m2. Therefore, among |D∗| disks, the expected number of disks intersecting a
grid line is
μ = |D∗| · 2m −1
m2
< |D∗| · 2
m.
Note that the median of a binomially distributed variable is equal to ⌈μ⌉or ⌊μ⌋.
That is, with probability 1/2, the number of disks intersecting a grid line is at most
2|D∗|/m. From the above analysis, we get a much better performance ratio in the
average case.
Theorem 4.1 Assume that the given input points for UDC are evenly distributed
in a square Q. Then, with probability 1/2, the solution A of Algorithm 4.A is a
(1 + 6/m)-approximation for UDC.
This probabilistic analysis suggests a randomized partition algorithm in which
we choose the grid lines randomly so that the expected number d of unit disks in-
tersecting more than one cell is close to 2|D∗|/m. In the following, we show that
this idea can actually be further improved to a deterministic partition algorithm. The
basic technique here is the shifting strategy. That is, we shift the grid lines to ﬁnd a
partition with a small number of disks intersecting with grid lines.
To do this, let us examine the partition more carefully. Recall that Q is the initial
square containing all n input points. Without loss of generality, assume that Q is of
size q × q, and that
Q = {(x, y) | 0 ≤x ≤q, 0 ≤y ≤q},
where q is a positive integer. Let p = ⌊q/m⌋+ 1. Consider the square
Q = {(x, y) | −m ≤x ≤mp, −m ≤y ≤mp}.
Partition Q into (p+1)2 cells, with each cell a square of size m×m. We denote this
partition of Q by P (0, 0). Note that the lower-left corner of the partition P (0, 0) is
(−m, m). In general, for any integers 0 ≤a < m and 0 ≤b < m, we can create a
new partition P (a, b) for square Q by shifting the lower-left corner from (−m, −m)
to (−m + a, −m + b) (see Figure 4.3).

4.1 Partition and Shifting
127
−m
(
)
−m+b
,
−m+a
)
(
,−m
Figure 4.3:
Square Q (the shaded area), partition P (0, 0) (the solid grid), and
partition P (a, b) (the dashed grid).
Consider a partition P (a, a), for some integer a ∈{0, 1, . . ., m −1}. Note that
the cells of P (a, a) cover the square Q. Let Aa denote the output of Algorithm 4.A
using partition P (a, a) [instead of P (0, 0)]; that is, Aa is the union of the minimum
unit disk covers for all cells in P (a, a). As shown earlier, Aa can be computed by
an exhaustive search algorithm in time nO(m2). In the following, we show that, for
at least one value of a ∈{0, 1, . . ., m −1}, Aa is a (1 + 3/m)-approximation for
the problem UDC.
Theorem 4.2 For at least one value of a ∈{0, 1, . . ., m −1}, |Aa| ≤(1 +
3/m)|D∗|, where D∗is a minimum disk cover of UDC for all n input points.
Proof. For simplicity, let us formally deﬁne that a cell is an m×m square, excluding
the top and right boundaries. For each cell e in a partition P (a, a), let D∗(e) denote
the set of unit disks in D∗that intersect cell e. Then we have
|Aa| ≤

e∈cell(P(a,a))
|D∗(e)|,
where cell(P (a, a)) denotes the set of all cells in partition P (a, a).
We call the collection of all cells in the partition P (a, a) that lie along a horizon-
tal or vertical line a strip of P (a, a). Let Ha (or Va) be the set of all disks in D∗that
intersect two horizontal (or, respectively, vertical) strips in P (a, a). Note that a unit
disk can intersect at most four cells in P (a, a), and if it intersects more than two
cells in P (a, a), it must belong to both Ha and Va. It follows that
|Aa| ≤

e∈cell(P(a,a))
|D∗(e)| ≤|D∗| + |Ha| + 2|Va|.

128
Partition
Now, note that, by our formal deﬁnition of cells, a unit disk cannot be in both
Ha and Hb for a ̸= b; that is, all sets Ha, for a = 0, 1, . . ., m −1, are pairwisely
disjoint. Thus,
m−1

a=0
|Ha| ≤|D∗|.
Similarly,
m−1

a=0
|Va| ≤|D∗|.
Therefore,
m−1

a=0
|Aa| ≤
m−1

a=0
(|D∗| + |Ha| + 2|Va|) ≤(m + 3)|D∗|.
Hence,
1
m
m−1

a=0
|Aa| ≤

1 + 3
m

|D∗|.
That is, the average value of |Aa| is bounded by (1 + 3/m)|D∗|. This implies that,
for at least one value of a ∈{0, 1, . . ., m −1}, |Aa| ≤(1 + 3/m)|D∗|.
□
Corollary 4.3 For any ε > 0, there is a (1 + ε)-approximation for UDC that runs
in time nO(1/ε2).
Proof. Choose m = ⌈3/ε⌉. Note that computing each Aa needs at most nO(m2) =
nO(1/ε2) time. By Theorem 4.2, a (1 + ε)-approximation can be obtained by com-
puting all m solutions Aa and choosing the best one. The total running time is
mnO(1/ε2) = nO(1/ε2).
□
In the problem UDC, we are allowed to use any unit disk in our solution. Suppose
we add some restrictions on the location of the unit disks; the partition technique
might still work. For instance, if we require that the center of each unit disk in the
solution must be located at an input point, it is not hard to check that a similar
argument for Theorem 4.2 works. In other words, the following variation of the
problem UDC has, for any ﬁxed ε > 0, a (1 + ε)-approximation that runs in time
nO(1/ε2).
UNIT DISK COVERING WITH RESTRICTED LOCATIONS (UDC1):
Given a set of n points in the Euclidean plane, ﬁnd a minimum num-
ber of unit disks to cover all input points, with the center of each disk
located at an input point.

4.2 Boundary Area
129
4.2
Boundary Area
A key step in the partition technique is to combine the solutions of the smaller
parts of the partition into a feasible global solution to the original input. In the
approximation Algorithm 4.A for UDC, this is straightforward, as the union of the
local solutions for smaller cells is naturally a feasible global solution. In general,
when the relationship between local solutions and global solutions is not so simple,
we may have to modify the local solutions around the boundary area of the partition
to get the global solution to the original input. In this section, we study this issue
through the example of the connected dominating set problem in unit disk graphs.
Recall that a dominating set in a graph G = (V, E) is a subset D of vertices
V such that every vertex is either in the set D or adjacent to some vertex in D.
If, in addition, the subgraph induced by a dominating set is connected, then such a
dominating set is called a connected dominating set. A unit disk graph is a graph in
which each vertex is a point in the Euclidean plane and there is an edge between two
points u and v if and only if the two unit disks centered at u and v have a nonempty
intersection.
CONNECTED DOMINATING SET IN A UNIT DISK GRAPH (CDS-
UDG): Given a connected unit disk graph G, ﬁnd a connected dom-
inating set of G with the minimum cardinality.
First, we notice that the minimum dominating set problem in a unit disk graph
can be easily converted to the problem UDC1 and, hence, has a PTAS.
Theorem 4.4 For any ε > 0, there is a (1 + ε)-approximation for the minimum
dominating set problem in unit disk graphs that runs in time nO(1/ε2).
Proof. Let G = (V, E) be a unit disk graph. Then there is an edge between two
vertices u and v if and only if the distance between u and v is less than or equal to
1. Equivalently, a vertex u dominates a vertex v ∈V if and only if the disk D(u, 2)
centered at u and having diameter 2 covers the vertex v. It follows that the minimum
dominating set in G has size k if and only if the set V of points can be covered by
k disks, each of which is centered at an input point and has diameter 2. That is, the
minimum dominating set problem in unit disk graphs is equivalent to the variation of
UDC1 in which all disks have diameter 2. The theorem now follows from Corollary
4.3.
□
The following result follows easily from the simple relationship between domi-
nating sets and connected dominating sets.
Corollary 4.5 There is a polynomial-time 4-approximation for CDS-UDG.
Proof. Let G = (V, E) be a unit disk graph. Suppose that D ⊆V is a (4/3)-
approximation for the minimum dominating set of G and D∗is a minimum con-
nected dominating set. Then we must have |D| ≤4|D∗|/3, since the size of a
minimum dominating set cannot exceed the size of a minimum connected dominat-
ing set. Now we claim that if D is not connected, then we can reduce the number

130
Partition
of connected components in D by one by adding one or two vertices into D. To see
this, recall that the input graph G of the problem CDS-UDG is always connected.
Consider a shortest path (v1, v2, . . ., vm) between any two connected components
of D. First, the vertex v2 must not be in D, for otherwise the path can be shortened
to (v2, . . ., vm). If v3 ∈D, then we can add v2 to D to reduce the number of con-
nected components in D. If v3 ̸∈D, then v3 must be dominated by a vertex u in
D. We note that u and v1 cannot be in the same connected component in D, for
otherwise (u, v3, v4, . . ., vm) would be a shorter path between two connected com-
ponents of D. Therefore, we must have m = 4 and u = vm, and adding v2 and v3
to D reduces the number of connected components by one. So, the claim is proven.
From the above claim, we need to add at most 2(c −1) vertices to D to get a
connected dominating set, where c is the number of connected components in D.
It is clear that c ≤|D|. That is, we can ﬁnd a connected dominating set C of size
|C| ≤3|D| −2 ≤3 · (4|D∗|/3) = 4|D∗|.
□
Next, we describe how to apply the above 4-approximation algorithm to get a
PTAS for CDS-UDG.
Let G = (V, E) be a given connected unit disk graph. We deﬁne partitions
P (a, b) as in Section 4.1. That is, assume that Q is a square containing all vertices
in V . Without loss of generality, assume that Q = {(x, y) | 0 ≤x ≤q, 0 ≤y ≤q}.
Let m be an integer whose value will be determined later. Let p = ⌊q/m⌋+ 1.
Consider the square Q = {(x, y) | −m ≤x ≤mp, −m ≤y ≤mp}. Partition
Q into (p + 1) × (p + 1) cells so that each cell is an m × m square, excluding the
top and right boundary edges. This partition of Q is denoted by P (0, 0). In general,
the partition P (a, b) is obtained from P (0, 0) by shifting the lower-left corner of Q
from (−m, −m) to (−m + a, −m + b).
Let h be an integer such that 2h + 2 < m. For each cell e of size m × m, we
deﬁne its central area to be the set of points in e that have distance at least h from
the boundary of e; that is, it is the (m −h) × (m −h) square that shares the same
center with cell e. In addition, we deﬁne the boundary area of a cell e to be the set
of points in e that are within distance < h + 1 from the boundary of e. Note that
for each cell, its boundary area and central area have an overlapping area of width 1
(see Figure 4.4). Finally, we deﬁne the boundary area of a partition P (a, a) to be the
union of the boundary areas of all cells in P (a, a). The idea of our algorithm is to
solve the problem CDS-UDG on the central area of each cell e, and take the union of
these local solutions, plus a 4-approximation global solution on the boundary area,
as the solution to the input graph G.
For each cell e of a partition P (a, a), let Gc[e] be the subgraph of G induced
by all vertices in the central area of e. This graph Gc[e] may have more than one
connected component. Let C[e] be a minimum subset of vertices in e satisfying the
following condition:
(C1) For each connected component H of Gc[e], the subgraph of G induced by
C[e] has a connected component dominating H.
Lemma 4.6 For each cell e in a partition P (a, a), the set C[e] can be computed in
time nO(m2)
e
, where ne is the number of vertices in e.

4.2 Boundary Area
131
h
+1
h
central area
boundary area
Figure 4.4:
Central area and boundary area overlapping with width 1.
Proof. We note that for any square of size
1
√
2 ×
1
√
2, the set of vertices lying inside
the square induces a complete subgraph in which any vertex dominates all other
vertices. It follows that the minimum dominating set S for Gc[e] has size at most
⌈
√
2m⌉2. To create a connected dominating set from S, we need to add at most two
vertices to reduce the number of connected components of S by 1, and so C[e] has
size at most 3⌈
√
2m⌉2. Thus, the number of candidates for C[e] is at most
3⌈
√
2m⌉2

k=0
ne
k

= nO(m2)
e
.
It is clear that for any set C′ of vertices in cell e, we can check in linear time whether
it holds that, for each connected component H of Gc[e], the subgraph of G induced
by C′ has a connected component dominating H. Therefore, an exhaustive search
for C[e] only takes time ne · nO(m2)
e
= nO(m2)
e
.
□
Now, we are ready to describe a (1+ε)-approximation algorithmfor CDS-UDG.
Algorithm 4.B (PTAS for CDS-UDG)
Input: A unit disk graph G = (V, E), with all vertices lying in square Q.
(1) Let h ←3 and m ←⌈160/ε⌉.
(2) Let D ⊆V be a 4-approximation to the minimum connected dominating set
for G (obtained by the algorithm of Corollary 4.5).
(3) For a ←0 to m −1 do
(3.1) Let Da ←{v ∈D | v lies in the boundary area of P (a, a)};
(3.2) For each cell e of P (a, a) do
compute set C[e] (by exhaustive search of Lemma 4.6);
(3.3) Let Aa ←Da ∪


e∈P(a,a)
C[e]

.
(4) Let a∗←argmin
0≤a<m
|Aa|.

132
Partition
(5) Output A ←Aa∗.
The following shows the correctness of this approximation.
Lemma 4.7 For each a ∈{0, 1, . . ., m−1}, set Aa computed by Algorithm 4.B in
step (3) is a connected dominating set for input graph G.
Proof. Note that every vertex lying within distance ≤h from the grid lines of
P (a, a) must be dominated by a vertex in Da. Also, every vertex lying with distance
> h from the grid lines of P (a, a) lies in the central area of a cell e in P (a, a), and
hence is dominated by C[e]. Thus, Aa is a dominating set of G.
Next, we show that Aa is connected. Consider two connected components E1, E2
of Da that are connected through a path π in D passing through the central area
of a cell e of P (a, a). Note that the central area and the boundary area of cell e
have an overlapping area of width 1. Thus, this path π must begin with a vertex
x1 in E1 that lies in the overlapping area and end with a vertex x2 in E2 that also
lies in the overlapping area. Obviously, π is a subgraph of a connected component
H of the graph Gc[e] and hence, by the requirement on C[e], is dominated by a
connected component C′ of the subgraph of G induced by C[e]. In particular, C′
must dominate both x1 and x2. It follows that E1 and E2 are connected through C′.
This proves that all connected components of Da are connected through C[e]’s over
all cells e of partition P (a, a).
Moreover, a similar argument shows that every connected component C′ of C[e]
for any cell e is connected to some vertex in Da, and hence Aa is connected. To see
this, assume, by way of contradiction, that a connected component C′ of C[e] is not
connected to any vertex in Da. Then every vertex of C′ lies in the central area of
e, for otherwise it would be dominated by a vertex of Da. Let H be the connected
component of Gc[e] that contains C′. By the minimality of C[e], C′ dominates H.
Let x be a vertex in C′. Then x is dominated by a vertex y ∈D \ Da. Since D
is connected, there must be a path π in D from y to a vertex z ∈Da with every
vertex in π lying in the central area of e. Clearly, the path π is a subgraph of H and
so is dominated by C′. In particular, z ∈Da is adjacent to a vertex in C′, which is
a contradiction. This completes the proof of the claim and, hence, the proof of the
lemma.
□
Remark. In the above proof, the minimality of set C[e] is not required. In fact, the
proof is correct as long as, for every cell e of P (a, a), C[e] satisﬁes condition (C1),
and every connected component C′ of C[e] dominates some connected component
H of Gc[e].
To verify that Algorithm 4.B runs in polynomial time, we note that, from Lemma
4.6, each C[e] can be computed in time nO(m2)
e
, and so the total time for step (3.2)
is at most

e∈P(a,a)
nO(m2)
e
≤


e∈P(a,a)
ne
O(m2)
= nO(m2).

4.2 Boundary Area
133
It follows that each Aa is computable in time nO(m2), and so the output A = Aa∗
can be found in time O(mn) + m · nO(m2) = nO(m2).
Finally, we show that Algorithm 4.B is a PTAS.
Theorem 4.8 Output Aa∗of Algorithm 4.B is a (1 + ε)-approximation for CDS-
UDG with computation time nO(1/ε2).
Proof. Let D∗denote a minimum connected dominating set for G. Following the
general approach for analyzing the performance of an approximation constructed
by the restriction method, we will modify D∗into a feasible approximate solu-
tion D′. Here, by a feasible approximate solution, we mean that, for some a ∈
{0, 1, . . ., m −1}, D′ contains Da, and, for each cell e of P (a, a), set D′[e] of
all vertices in D′ lying in cell e satisﬁes condition (C1) (that is, for every con-
nected component H of Gc[e], the subgraph of G induced by vertices in D′[e] has a
connected component dominating H). Note that Aa is such a feasible approximate
solution with the minimum C[e]’s, and so if D′ is feasible with respect to partition
P (a, a), then |Aa| ≤|D′|.
The modiﬁcation of D∗is divided into two steps. We ﬁrst ﬁnd a suitable shifting
parameter b ∈{0, 1, . . ., m−1}. Then we modify D∗on each cell e of the partition
P (b, b) to get the required D′[e].
Recall that for a ∈{0, 1, . . ., m −1}, Da denotes the set of vertices in D that
lie in the boundary area of partition P (a, a); in addition, let D∗
a denote the set of
vertices in D∗that lie in the boundary area of P (a, a). We claim that there exists an
integer b ∈{0, 1, . . ., m −1} such that
6 ·
D∗
b
 +
Db
 ≤ε ·
D∗.
To prove this, let us study how the shifting of the partition from P (0, 0) to P (1, 1),
P (2, 2), . . ., P(m −1, m −1) affects sets D∗
a and Da. When the partition shifts
(toward northeast), the location of the graph G relative to the grid of the partition
changes. We may imagine that the grid of the partition is ﬁxed, but actually the
graph G is moving (toward southwest). For each vertex v of G, the moving of the
graph leaves a trace in the grid. The trace of v consists of m points in a straight line
of slope 1, with distance
√
2 between any two consecutive points (see Figure 4.5).
Thus, the trace of v contains at most 4(h + 1) points in the boundary area of the
(ﬁxed) partition.
In other words, for any vertex v in D∗, it belongs to at most 4(h + 1) of the sets
D∗
0, D∗
1, . . ., D∗
m−1. Therefore, by the pigeonhole principle, we have
m−1

a=0
D∗
a
 ≤4(h + 1)
D∗.
Similarly, for any vertex v in D, it belongs to at most 4(h + 1) of the sets D0,
D1, . . ., Dm−1, and so
m−1

a=0
Da
 ≤4(h + 1)
D
 ≤16(h + 1)
D∗.

134
Partition
Figure 4.5:
The trace of a vertex has at most 4(h+1) points lying in the boundary
area.
It follows that
m−1

a=0

6 ·
D∗
a
 +
Da

≤40(h + 1)
D∗.
Therefore, there must exist an integer b ∈{0, 1, . . ., m −1} such that
6 ·
D∗
b
 +
Db
 ≤40(h + 1)
m
D∗ ≤ε ·
D∗.
(4.1)
Now, we ﬁx this shifting parameter b. For each cell e of the partition P (b, b), let
D∗[e] denote the set of all vertices in D∗that lie in cell e. We will modify D∗[e]
into a set D′[e] that satisﬁes condition (C1).
For a cell e of P (b, b), consider a connected component H of Gc[e]. Clearly,
D∗[e] dominates H. Assume that H is dominated by k connected components
D1, . . ., Dk of D∗[e]. Since H is connected, these connected components can be
connected together into a single component by adding at most 2(k −1) vertices
of H. We deﬁne D′[e] to be the set D∗[e] plus the collection of all these connect-
ing vertices for all connected components H of Gc(e). Clearly, each D′[e] satisﬁes
condition (C1). Therefore,
D′ = Db ∪

(
e∈P(b,b)
D′[e]

is a feasible solution (see the remark after Lemma 4.7), and |Ab| ≤|D′|.
To estimate the size of D′, let D∗
b [e] be the set of all vertices in D∗
b that lie in
the boundary area of cell e. Again, assume that a connected component H of Gc[e]
is dominated by k connected components D1, D2, . . ., Dk of D∗[e]. Since D∗is
connected, each of its connected components Di, i = 1, 2, . . ., k, is connected to

4.2 Boundary Area
135
w
v1
v2
central
area
Figure 4.6:
Charging v1 and v2 to the vertex w outside the central area. In the
above, • denotes a vertex in D∗, and ◦denotes a vertex in Gc[e] that is not in D∗.
some vertices of D∗lying outside the cell e through an edge crossing a boundary
edge of e (unless there is no vertex of D∗outside cell e, in which case k = 1).
That is, each Di must contain a vertex that lies outside the central area of cell e and,
hence, belongs to D∗
b [e]. Now, we describe a charging method that charges the cost
of vertices in D′[e]\D∗[e] to different vertices in D∗
b [e], so that each vertex in D∗
b [e]
is charged at most six times.
Note that to connect the connected components D1, D2, . . ., Dk of D∗[e] into
a single component, we need to add at most 2(k −1) vertices in H to D′[e]. We
charge these vertices evenly to k −1 of the components in such a way that
(i) When two vertices (or, one vertex) are added to connect two components Di
and Dj, they are both charged to Di or both charged to Dj, and
(ii) Each component Di is charged at most twice (by two vertices).
Thus, for each connected component H of Gc[e], a connected component Di of
D∗[e] can be charged at most twice. However, a component Di of D∗[e] may be
used to dominate more than one component H of Gc[e]. Therefore, we need to
further distribute the charges to different vertices in Di. To be more speciﬁc, when
a vertex v1 is charged (maybe together with another vertex v2) to a component Di,
we charge it to vertex w of Di lying outside the central area of e that is the closest
to vertex v1 through a path of Di ∪{v1, v2} (see Figure 4.6).
Note that if we charge v1 to w of Di according to the above criteria, all vertices,
except w, in the shortest path in Di ∪{v1, v2} between w and v1 must lie in the
central area of e. Thus, a vertex w can be charged at most 2ℓtimes if it has ℓin-
dependent neighbors inside the central area of e. (Two neighbors of w that are not
independent to each other must belong to the same connected component H of the
subgraph Gc[e].) It is easy to see that in a unit disk graph, a vertex lying outside
the central area of e can have at most three independent neighbors lying inside the
central area (cf. Figure 4.6). Thus, each vertex w in Di can be charged at most six
times. Furthermore, a vertex is charged only if it lies outside the central area of e

136
Partition
Figure 4.7:
An edge exists if and only if two associated disks overlap.
and, hence, only if it is in D∗
b[e]. It follows that
D′[e]
 ≤
D∗[e]
 + 6 ·
D∗
b[e]
.
Now, we get, from inequality (4.1),
D′ ≤
Db
 +

e∈P(b,b)
D′[e]
 ≤
Db
 +

e∈P(b,b)
D∗[e]
 + 6 ·
D∗
b[e]


=
Db
 +
D∗ + 6 ·
D∗
b
 ≤(1 + ε)
D∗.
Since the output Aa∗of Algorithm 4.B has the minimum size among all sets Aa,
and since |Ab| ≤|D′|, we conclude that |Aa∗| is at most (1 + ε)|D∗|.
□
4.3
Multilayer Partition
In a unit disk graph, each vertex v is a point in the Euclidean plane and is associated
with a unit disk centered at v. An edge exists between two vertices if and only if the
two associated disks have a nonempty intersection. This notion can be generalized to
an intersection disk graph in which disks may be of different sizes. More precisely,
in an intersection disk graph, each vertex v is a point in the Euclidean plane and
is associated with a disk centered at v, but different points may be associated with
disks of different diameters; and, an edge exists between two vertices if and only if
the two associated disks have a nonempty intersection (see Figure 4.7).
When we apply the partition technique to intersection disk graphs with disks of
different sizes, a simple partition of a ﬁxed size does not work well. Instead, we
need to use partitions of different grid sizes to deal with disks of different sizes. We
call this the multilayer partition. Let us look at the following example:
MAXIMUM INDEPENDENT SET IN AN INTERSECTION DISK GRAPH
(MIS-IDG): Given an intersection disk graph G, ﬁnd an independent
set of G with the maximum cardinality.
Clearly, a subset of vertices is independent if and only if their associated disks do
not overlap. For convenience, we will identify the vertices with their associated disks
and work on the disks directly. In particular, we say a set of disks is independent if
these disks are mutually disjoint (see Figure 4.8).

4.3 Multilayer Partition
137
Figure 4.8:
Independent disks.
In the following, we will apply the multilayer partition to the problem MIS-
IDG. First, assume that all given disks are contained in the interior of a square Q.
Fix an integer k > 0, and rescale all disks so that the largest disk has diameter
1 −1/k. Let dmin be the diameter of the smallest disk in the new scale, and let m =
⌊logk+1(1/dmin)⌋. We now divide all disks into m+1 layers: For 0 ≤j ≤m, layer
j consists of all disks with diameters d in the range (k +1)−(j+1) < d ≤(k +1)−j.
So, the largest disk is in layer 0, and the smallest disk is in layer m.
Next, corresponding to each layer j of disks, with 0 ≤j ≤m, we deﬁne a
partition of square Q. Without loss of generality, assume that
Q = {(x, y) | 0 ≤x ≤q, 0 ≤y ≤q}.
Let p = ⌊q/k⌋+ 1. We extend square Q to the square
Q = {(x, y) | −k ≤x ≤kp, −k ≤y ≤kp}.
For each 0 ≤j ≤m, partition Q into (p+1)(k +1)j ×(p+1)(k +1)j cells so that
each cell is a k(k +1)−j ×k(k +1)−j square (excluding the top and right boundary
edges). We call this the layer-j partition of Q, and denote it by Pj(0, 0) (see Figure
4.9). Note that the diameter of a disk in layer j is at least 1/k of the grid size of a
layer-(j + 1) partition, and is at most 1/k of the grid size of a layer-j partition.
Next, we describe how to apply the shifting technique to the partitions Pj(0, 0).
The critical idea here in multilayer partition is to shift partitions in different layers
with different distances. In general, for 0 ≤a < k and 0 ≤b < k, the layer-j
partition Pj(a, b) can be obtained from Pj(0, 0) by shifting the lower-left corner of
Q from (−k, −k) to (−k + a(k + 1)−j, −k + b(k + 1)−j). Note that, for the same
shifting parameters a, b, partition Pj(a, b) and partition Pj+1(a, b) have different
lower-left corners. However, since we have extended the original square Q to a
bigger square Q, the outer square of every Pj(a, b) contains the original square Q.
Furthermore, inside the original square Q, the grid lines of Pj(a, b) are also the grid
lines of Pj+1(a, b).
Lemma 4.9 For any a, b ∈{0, 1, . . ., k−1} and any j ∈{0, 1, . . ., m−1}, a grid
line of the layer-j partition Pj(a, b) inside the square Q is also a grid line of the
layer-(j + 1) partition Pj+1(a, b).

138
Partition
k
k  k( +1)
−j
Figure 4.9:
Layer-j partition Pj(0, 0).
Proof. From the setting of Pj(a, b), the x-coordinateof a vertical grid line in Pj(a, b)
is of the form −k+(a+ik)(k+1)−j for some integer i ≥0. Note that (a+ik)(k+
1)−j = (a + (ik + a + i)k)(k + 1)−(j+1). Thus, every vertical grid line in Pj(a, b)
within Q is also a vertical grid line in Pj+1(a, b). Similarly, every horizontal grid
line in Pj(a, b) within Q is also a horizontal grid line in Pj+1(a, b).
□
Let D be the set of the input disks, and a, b ∈{0, 1, . . ., k −1}. For each j,
0 ≤j ≤m, delete from D all disks in layer j that hit a grid line in the corresponding
partition Pj(a, b).2 Let D(a, b) denote the collection of all remaining disks (in all
layers).
Lemma 4.10 The maximum independent set of disks in D(a, b) can be computed in
time nO(k4).
Proof. For a set E of disks, let opt(E) denote the maximum independent set of disks
in E. In the following, we present a dynamic programming algorithm computing
opt(D(a, b)) in time nO(k4).
First, for convenience, let us call a cell in a layer-j partition a j-cell. A j-cell is
said to be relevant if it contains a disk in layer j. For cells in different layers, we
deﬁne a parent–child relation. For j′ > j, we say a relevant j′-cell e′ is a child of a
relevant j-cell e if e contains e′ and no other relevant j′′-cell e′′, with j < j′′ < j′,
satisfying e′ ⊂
̸= e′′ ⊂
̸= e. A relevant cell without a relevant parent is called a maximal
relevant cell. Let E be the set of all maximal relevant cells. Note that for any two
2By hitting a grid line, we mean that the disk intersects the grid line or touches the grid line.

4.3 Multilayer Partition
139
e
cell
’
disks in layers < j
e
cell
disks in layer j
I
J
that belong to
that belong to
Figure 4.10:
The relationships among cells e, e′ and disk sets I, J in the recursive
relation.
relevant cells e and e′, we can determine, in time O(n), whether e′ is a child of e
and whether e is a maximal relevant cell.
In the dynamic programming algorithm for opt(D(a, b)), we will build a table T
of the following form: Let e be a relevant j-cell and I a set of independent disks in
layers < j that hit cell e. Then T(e, I) contains the maximum independent set of
disks in layers ≥j that are in cell e and are disjoint from all disks in I. Clearly,
opt(D(a, b)) =
(
e∈E
T(e, ∅).
To build table T, let INDj(e, I) denote the collection of all sets J of independent
disks in layer j that are in cell e and are disjoint from all disks in I. Also, let Ie
denote the set of disks in I that intersect cell e, and child(e) the set of children of
cell e. Then the recursive relation of the dynamic programming can be described as
follows (cf. Figure 4.10):
(1) For each J ∈INDj(e, I), let
AJ =
(
e′∈child(e)
T(e′, (I ∪J)e′);
that is, AJ is the maximum independent set of disks in layers ≥j +1 that are
in cell e and are disjoint from I ∪J.
(2) Let J∗= argmaxJ∈INDj(e,I)|J ∪AJ|. Then we have T(e, I) = J∗∪AJ∗.

140
Partition
k
smallest disk in
layer 
(k+1)
−j
(k+1)
−j
(k+1)
−j
+2)
k
(
j−1
Figure 4.11:
Square S.
The above shows that each entry T(e, I) of the table T can be computed recur-
sively from entries T(e′, I′), over all children e′ of e. To complete the proof, we
need to verify that
(a) The computation of each entry T(e, I) can be done in time nO(k4), and
(b) The table size of T is bounded by nO(k2).
To prove (a), we ﬁrst note that disks in a set J ∈INDj(e, I) must be in layer
j and contained in e. The cell e has size (k(k + 1)−j)2, and each disk in J has
diameter ≥(k + 1)−(j+1) and, hence, has area ≥π((k + 1)−(j+1)/2)2. It means
the set J contains at most
(k(k + 1)−j)2
π((k + 1)−(j+1)/2)2 = 4
π k2(k + 1)2 = O(k4)
disks. Thus, the collection INDj(e, I) has at most nO(k4) sets J.
In addition, we note that there are at most n relevant cells, and, as we pointed
out earlier, the parent–child relation between cells can be determined in time O(n).
Thus, the computation of the entry T(e, I) can be done in time nO(k4).
Next, we calculate the size of table T. We ﬁrst count the size of I, i.e., the max-
imum number of independent cells in layers < j that can intersect a j-cell e. To
do this, we draw, as shown in Figure 4.11, a square S of size (k + 2)(k + 1)−j ×
(k + 2)(k + 1)−j, that contains e in the center. We note that every disk in layer < j
has size at least π((k + 1)−j/2)2. Thus, if it intersects cell e, then it must occupy a
region of size at least π((k + 1)−j/2)2 in S. Therefore, the size of set I is at most
((k + 2)(k + 1)−j)2
π((k + 1)−j/2)2
= 4(k + 2)2
π
= O(k2).

4.3 Multilayer Partition
141
Therefore, for any cell e, there are at most nO(k2) possible sets I to be considered,
and the size of table T is bounded by n · nO(k2) = nO(k2). This completes the proof
of (b) and, hence, the proof of the theorem.
□
Now, we are ready to describe a (1 + ε)-approximation for MIS-IDG.
Algorithm 4.C (PTAS for MIS-IDG)
Input: A set D of disks.
(1) Let k ←2⌈1 + 1/ε⌉.
(2) For a ←0 to k −1 do compute opt(D(a, a)).
(3) Let a′ ←argmax
0≤a<k
|opt(D(a, a))|.
(4) Output A ←opt(D(a′, a′)).
From Lemma 4.10, Algorithm 4.C runs in time nO(1/ε4). Next, we show that it
is a PTAS.
Theorem 4.11 The output A of Algorithm 4.C is a (1 + ε)-approximation to the
optimal solution opt(D).
Proof. Let A∗be a maximum independent set of disks in D. For each a ∈
{0, 1, . . ., k −1}, let Hj(a) denote the set of layer-j disks in A∗that hit a grid line
in the layer-j partition Pj(a, a), and let H(a) = m
j=0 Hj(a). Note that, for each
a ∈{0, 1, . . ., k −1}, A∗−H(a) is a feasible solution to the problem MIS-IDG
with respect to the set D(a, a) of disks, and hence |A| ≥|A∗−H(a)|.
Note that a disk in layer j has a diameter d ≤(k + 1)−j, and so it can appear in
at most two of the sets H(0), H(1), . . ., H(k −1). Therefore,
k−1

a=0
|H(a)| ≤2|A∗|.
It follows that there must exist an integer a ∈{0, 1, . . ., k −1} such that
|H(a)| ≤2
k |A∗| ≤
ε
1 + ε|A∗|.
Now, we have
|A| ≥|A∗−H(a)| = |A∗| −|H(a)| ≥
1
1 + ε|A∗|;
or, equivalently,
|A∗| ≤(1 + ε)|A|.
□

142
Partition
4.4
Double Partition
In the previous sections, we have used the partition technique to design PTASs
for some geometric problems. In these algorithms, the tradeoff between the per-
formance ratio and the running time is straightforward. That is, in order to get a
smaller performance ratio, we simply increase the cell size and spend extra time to
solve the subproblems on larger cells. We note that in order for this approach to
work, the running time for solving the subproblems on larger cells must remain a
polynomial function in the input size—even though the degree of the polynomial
function may increase along with the cell size.
For some size-sensitive problems, however, this approach may not work. That is,
a problem may be easy to solve on a cell of a certain small size, but it becomes more
difﬁcult to solve (or approximate) on larger cells. For such a problem, a PTAS is
difﬁcult to get, but some kind of tradeoff between the performance ratio and running
time can still be achieved. In this section, we introduce a new technique, called
double partition, to deal with such problems. Namely, we ﬁrst partition the input
data into cells of a small size on which the subproblems are easy to solve; we then
apply the second partition on this partitioned problem to reduce the performance
ratio. To demonstrate how this technique works, we study a speciﬁc problem about
unit disk graphs.
WEIGHTED DOMINATING SET IN A UNIT DISK GRAPH (WDS-
UDG): Given a unit disk graph G = (V, E) with a vertex-weight func-
tion c : V →R+, ﬁnd a dominating set of G with the minimum total
vertex weight.
We will present a polynomial-time (6+ε)-approximation for this problem. Since
the proof of this result is quite involved, we will establish it in three steps:
(a) We ﬁnd a 2-approximation for a subproblem of WDS-UDG restricted to a cell
of size μ × μ, where μ = 1/
√
2.
(b) We extend result (a) to the subproblem of WDS-UDG restricted to a cell of
arbitrarily large constant size, and get a 6-approximation to this subproblem.
(c) We partition the input data of the unrestricted WDS-UDG into cells of size
mμ × mμ for constant m. We apply the 6-approximation algorithm of result
(b) above to each cell (which requires a second partition), and then apply the
shifting technique to get a (6 + ε)-approximation to the original problem.
We will present the proof of part (a) in Sections 4.4.1 and 4.4.2, that of part (b) in
Section 4.4.3, and that of part (c) in Section 4.4.4.
4.4.1
A Weighted Covering Problem
To prepare for the ﬁrst result (a) above, we ﬁrst study a weighted unit disk covering
problem (see Figure 4.12).

4.4 Double Partition
143
μ
Figure 4.12:
A weighted unit disk covering problem.
WEIGHTED UNIT DISK COVERING (WUDC): Given a set P of points
lying inside a horizontal strip of width μ, a set D of disks with radius 1
and centers lying outside the strip, and a weight function c : D →R+,
ﬁnd a minimum-weight subset C ⊆D of disks that cover all points in
P .
The problem WUDC can be solved in polynomial time by dynamic program-
ming.
Theorem 4.12 The minimum-weight covering C for the problem WUDC can be
computed in time O(m4n), where n = |P | and m = |D|.
Proof. Let p1, p2, . . ., pn be all the points in P , ordered from left to right. For each
i = 1, 2, . . ., n, let Li be the vertical line that passes through point pi. We call a disk
D in D an upper disk (or, a lower disk) if the center of D lies above (or, respectively,
below) the strip. For the simplicity of the description, we add two dummy disks to
D; that is, the two boundary lines of the strip are considered as disks of weight zero,
with the upper boundary an upper disk, and the lower boundary a lower disk. Note
that these two dummy disks do not cover any point in P , but they always intersect
line Li for any i = 1, 2, . . ., n. For any disk D ∈D having a nonempty intersection
with Li, let int(Li, D) denote the lowest (or, highest) point in Li ∩D if D is an
upper (or, respectively, lower) disk.
We will use dynamic programming to ﬁnd the minimum-weight covering C. This
algorithm uses a table T with three parameters. To be more precise, for an integer
i ∈{1, 2, . . ., n}, an upper disk D and a lower disk D′, with D ∪D′ covering point
pi, we deﬁne Ti(D, D′) to be the set of disks with the minimum weight satisfying
the following conditions:
(1) Disks in T(D, D′) cover points p1, . . ., pi.
(2) D and D′ are used to cover some points in {p1, . . ., pi} unless D or D′ is a
dummy disk.
(3) The intersection point int(Li, D) is the lowest one among all intersec-
tion points of Li with upper disks in Ti(D, D′); and the intersection point
int(Li, D′) is the highest one among all intersection points of Li with lower
disks in Ti(D, D′).

144
Partition
2
C
1
1
i−
C
L
j
i
j
i
p
p
p
i−1
L
L
Figure 4.13:
If C2 is no lower than C2 on line Li and is no higher than C1 on line
Li−1, then it cannot be higher than C1 on Lj.
Let c(Ti(D, D′)) be the total weight of disks in Ti(D, D′), and Ai(D, D′) =
{(D1, D2) | D1 is an upper disk in D, D2 is a lower disk in D, int(Li, D1) is
no lower than int(Li, D), and int(Li, D2) is no higher than int(Li, D′)}. In the
following, we write, for any predicate Q, [Q] to denote the truth value of Q; that is,
[Q] = 1 if Q is true, and [Q] = 0 if Q is false. We claim that c(Ti(D, D′)) satisﬁes
the following recurrence relation:
c(Ti(D,D′)) =
min
(D1,D2)
∈Ai(D,D′)
&
c(Ti−1(D1, D2))
+ [D1 ̸= D] c(D) + [D2 ̸= D′] c(D′)'.
(4.2)
Before we show the claim, we ﬁrst observe a simple property between two upper
(or, lower) disks (see Figure 4.13):
Property 4.13 For any two upper disks C1, C2, of which C1 is not a dummy disk,
and for 1 ≤j < i −1, it is not possible that
(i) int(Lj, C1) is lower than int(Lj, C2),
(ii) int(Li−1, C1) is no lower than int(Li−1, C2), and
(iii) int(Li, C1) is no higher than int(Li, C2).
A similar property holds for lower disks C1, C2 if C1 is not a dummy disk.
Now we prove the claim. Let D1 be the upper disk in Ti(D, D′) with the lowest
intersection point int(Li−1, D1) among upper disks in Ti(D, D′), and D2 the lower
disk in Ti(D, D′) with the highest intersection point int(Li−1, D2) among lower
disks in Ti(D, D′). Clearly, D1 ∪D2 covers pi−1. Moreover, if D covers a point pj
for some j < i−1, then, by Property 4.13, D1 must also cover pj (note that D covers
pj and so is not a dummy disk). Similarly, if D′ covers a point pj for j < i−1, then
D2 must also cover pj. Therefore, (Ti(D, D′)\ {D, D′})∪{D1, D2} covers points
p1, . . ., pi−1, and so is a candidate for Ti−1(D1, D2) (i.e., they satisfy conditions
(1), (2), and (3) with respect to i −1). It follows that

4.4 Double Partition
145
c(Ti(D, D′)) −[D1 ̸= D]c(D) −[D2 ̸= D′] c(D′) ≥c(Ti−1(D1, D2)),
and so
c(Ti(D, D′)) ≥
min
(D1,D2)
∈Ai(D,D′)
&
c(Ti−1(D1, D2)) + [D1 ̸= D] c(D) + [D2 ̸= D′] c(D′)
'
.
Next, to show the “≤” part of the recurrence relation (4.2), assume that the min-
imum value of the right-hand side of (4.2) is achieved at (D∗
1, D∗
2) ∈Ai(D, D′);
that is,
c(Ti−1(D∗
1, D∗
2)) + [D∗
1 ̸= D] c(D) + [D∗
2 ̸= D′] c(D′)
=
min
(D1,D2)
∈Ai(D,D′)
&
c(Ti−1(D1, D2)) + [D1 ̸= D]c(D) + [D2 ̸= D′] c(D′)
'
.
Further assume that Ti−1(D∗
1, D∗
2) contains the smallest number of disks among
these minimum pairs (D∗
1, D∗
2). Then, it must be true that, for every upper disk
C in Ti−1(D∗
1, D∗
2), the intersection point int(Li, C) with Li is no lower than
int(Li, D). To see this, suppose, by way of contradiction, that there exists an upper
disk C ∈Ti−1(D∗
1, D∗
2) having a lower intersection point int(Li, C) with Li than
int(Li, D). Since (D∗
1, D∗
2) ∈Ai(D, D′), int(Li, D∗
1) is no lower than int(Li, D).
So, C ̸= D∗
1, and C must cover a point pj for some j < i −1 that is not covered
by D∗
1 (otherwise, C can be deleted and it violates the minimality assumption about
Ti−1(D∗
1, D∗
2)). However, it means that the pair (C, D∗
1) of upper disks satisﬁes
the three conditions of Property 4.13, which is a contradiction (note that C covers
pj and so is not a dummy disk). Similarly, we can see that the intersection point
int(Li, C′) of every lower disk C′ in Ti−1(D∗
1, D∗
2) is no higher than int(Li, D′).
The above shows that the set Ti−1(D∗
1, D∗
2)∪{D, D′} satisﬁes conditions (1)—(3),
and so is a candidate for Ti(D, D′).
In addition, we note that if D ∈Ti−1(D∗
1, D∗
2), then D must be identical to D∗
1,
for otherwise D would cover a point pj for some j < i−1 that is not covered by D∗
1,
and the pair (D, D∗
1) would satisfy the three conditions of Property 4.13. Similarly,
if D′ ∈Ti−1(D∗
1, D∗
2), then D′ must be identical to D∗
2. Together, we get
c(Ti(D, D′)) ≤c(Ti−1(D∗
1, D∗
2) ∪{D, D′})
= c(Ti−1(D∗
1, D∗
2)) + [D∗
1 ̸= D] c(D) + [D∗
2 ̸= D′] c(D′)
=
min
(D1,D2)
∈Ai(D,D′)
&
c(Ti−1(D1, D2)) + [D1 ̸= D]c(D) + [D2 ̸= D′] c(D′)
'
,
and the proof of (4.2) is complete.
The recursive formula (4.2) induces a dynamic programming algorithm that com-
putes all c(Ti(D, D′)) in time O(nm4), since the table size is O(nm2) and each
entry c(Ti(D, D′)) can be computed from formula (4.2) in time O(m2). Finally, the
minimum-weight disk cover C for p1, . . ., pn can be computed from c(Tn(D, D′)),
over all possible D, D′ ∈D, in time O(m2).
□

146
Partition
Note that, in a unit disk graph, a vertex v dominates a vertex w if and only if the
distance between v and w is at most 1. Thus, the dominating set problem in a unit
disk graph can be transformed into the covering problem with disks of radius 1. In
particular, Theorem 4.12 gives us the following result about a special subproblem
of WDS-UDG, which will be used in the next subsection.
Corollary 4.14 The subproblem of WDS-UDG that, for a given unit disk graph
G = (V, E) and a given strip of width μ = 1/
√
2, asks for a minimum-weight set
D of vertices satisfying properties (i) and (ii) below can be solved in time O(n5):
(i) D dominates all vertices lying in the strip; and
(ii) All vertices in D lie outside the strip.
4.4.2
A 2-Approximation for WDS-UDG on a Small Cell
Now we consider the problem WDS-UDG restricted to a single cell. Let μ = 1/
√
2,
and consider a cell e of size μ × μ. Let V (e) denote the set of vertices in V lying in
e, and V +(e) the set of vertices v in V such that v dominates some vertex in e; i.e.,
V +(e) = {v ∈V | v lies in e or is adjacent to some w ∈V (e)}. The subproblem
of WDS-UDG on a single cell e can be stated as follows:
WDS-UDG1: Given a unit disk graph G = (V, E) with weight c :
V →R+, and a cell e of size μ × μ, ﬁnd a minimum-weight subset of
V +(e) that dominates V (e).
In this subsection, we show the following result:
Theorem 4.15 There is a polynomial-time 2-approximation for WDS-UDG1.
To prove Theorem 4.15, let D∗(e) be a minimum-weight dominating set for V (e)
and, for any set U ⊆V , let c(U) denote the total weight of set U. We consider two
cases.
Case 1. D∗(e) contains a vertex in V (e). Since the cell e has size μ × μ, any
single vertex in V (e) dominates all vertices in V (e). Thus, D∗(e) contains a single
vertex v, which is of the minimum weight among all vertices in V (e). It is easy to
ﬁnd this vertex in linear time.
Case 2. D∗(e) ⊆V +(e) \ V (e). In this case, we will apply the algorithm of
Corollary 4.14 to get a 2-approximation of D∗(e) in polynomial time.
Although we do not know whether D∗(e) belongs to Case 1 or Case 2 above, we
can simply choose, from the two solutions obtained in the above two cases, the one
with the smaller weight, and it must be a 2-approximation to D∗(e).
In the following, we focus on Case 2. Let A, B, C, D be the four corners of cell
e, and divide the area outside e into eight subareas, as shown in Figure 4.14. Also,
let
N = NW ∪CN ∪NE,
S = SW ∪CS ∪SE,
W = NW ∪CW ∪SW ,
E = NE ∪CE ∪SE.

4.4 Double Partition
147
B
A
e
SW
CW
NW
CN
NE
CE
SE
CS
D
C
Figure 4.14:
The area outside e is divided into eight subareas.
We say V1 and V2 form a feasible partition of set V (e) if V (e) = V1 ∪V2, V1 ∩V2 =
∅, every vertex in V1 is dominated by some vertex in D∗(e) that lies in the area
N ∪S, and every vertex in V2 is dominated by some vertex in D∗(e) that lies in the
area W ∪E.
Suppose we are given a feasible partition (V1, V2) of V (e); then we can ap-
ply the algorithm of Corollary 4.14 to ﬁnd the minimum-weight subsets D1 ⊆
V +(e) ∩(N ∪S) and D2 ⊆V +(e) ∩(W ∪E) that dominate vertices in V1 and
V2, respectively. Then, c(D1) ≤c(D∗(e)) and c(D2) ≤c(D∗(e)). It follows that
D1 ∪D2 is a 2-approximation to WDS-UDG1.
Following this idea, we will develop, in the following,an algorithmthat generates
up to |V (e)|4 different partitions of set V (e) such that one of these partitions is a
feasible partition. From these partitions, we can ﬁnd a 2-approximation to WDS-
UDG1 in Case 2 by computing the optimal solutions D1 ∪D2 for each partition
(V1, V2) of V (e) and then taking the solution with the minimum weight.
For any vertex p ∈V (e), draw two straight lines L1(p) and L−1(p) passing
through point p and having slopes 1 and −1, respectively. These two lines meet the
boundary of the square □ABCD at an angle of 45◦and divide the square □ABCD
into four parts. We call them ΔN(p), ΔS(p), ΔW(p), and ΔE(p), according to their
location relative to point p (see Figure 4.15).
Lemma 4.16 If p is dominated by a vertex u in the area CS (CW, CN, or CE),
then every point in the area ΔS(p) (ΔW (p), ΔN(p), or ΔE(p), respectively) is
dominated by u.
Proof. Since ΔS(p) is a convex polygon, it sufﬁces to show that the distance from
u to every corner vertex of ΔS(p) is at most 1.
Suppose v is a corner vertex of ΔS(p) on line BC (cf. Figure 4.16). Draw a
line L′ that is perpendicular to line pv and divides pv evenly. Let d(x, y) denote
the distance between two points x and y. If u and v lie on the same side of line
L′ or if u lies on L′, then we have d(u, v) ≤d(u, p) ≤1. Otherwise, if u and p
lie on the same side of line L′, then we have ∠uvp < π/2 and, hence, ∠uvB >

148
Partition
)
p
Δ N
p
L1
L−1
( )
p
(
( )
p
( )
p
ΔW
( )
p
ΔE
( )
p
Δ S
Figure 4.15:
L1(p) and L−1(p) divide e into four parts.
B
C
D
A
B
C
D
L’
L’
p
v
u
u
v
p
A
Figure 4.16:
ΔS(p) is dominated by u.
π/4 because ∠pvC = π/4. It follows that d(u, v) ≤length(AB)/ sin(∠uvB) <
μ/ sin(π/4) = 1.
For the cases where the vertex v of ΔS(p) lies on line AB or AD, the proofs are
similar.
□
Next, consider two vertices p, p′ ∈V (e). Suppose p lies to the left of p′ or on
the same vertical line as p′. We deﬁne ΔS(p, p′) as follows: If ΔS(p′) ⊆ΔS(p),
then ΔS(p, p′) = ΔS(p), and if ΔS(p) ⊆ΔS(p′), then ΔS(p, p′) = ΔS(p′).
Otherwise, let p′′ be the intersection point of lines L1(p) and L−1(p′), and deﬁne
ΔS(p, p′) = ΔS(p′′) (see Figure 4.17). The area ΔN(p, p′) is deﬁned in a similar
way.

4.4 Double Partition
149
"
p
p’
(
)
L−1
L (
1 p)
p’
p
Figure 4.17:
ΔS(p, p′).
Lemma 4.17 Let K be a subset of V +(e) \ V (e) that dominates all vertices in
V (e). Assume that p, p′ ∈V (e) and p lies to the left of p′ or on the same vertical
line as p′. If both p and p′ are dominated by some vertices in K ∩CS (or, if both
are dominated by some vertices in K ∩CN), but neither p nor p′ is dominated by
any vertex in K ∩(CW ∪CE), then every vertex in ΔS(p, p′) (or, respectively,
ΔN(p, p′)) is dominated by some vertex in K ∩(N ∪S).
Proof. By Lemma 4.16, it sufﬁces to consider a vertex v lying in ΔS(p, p′) \
(ΔS(p) ∪ΔS(p′)). For the sake of contradiction, suppose v is dominated by a ver-
tex u in K ∩(CW ∪CE). If u ∈CW, then p lies in ΔW (v) and so, by Lemma
4.16, is dominated by u, which is a contradiction. If u ∈CE, we can get a similar
contradiction.
□
In general, for a set T ⊆V (e) with |T| ≤2, we deﬁne ΔS(T) = ∅if T = ∅,
ΔS(T) = ΔS(p) if T = {p}, and ΔS(T) = ΔS(p, p′) if T = {p, p′} and p lies to
the left of p′ or on the same vertical line as p′. We deﬁne ΔN(T) in a similar way
for a subset T ⊆V (e) with |T| ≤2.
Let UCS be the set of all vertices v in V (e) such that v is dominated by some
vertex in D∗(e) ∩CS, but not dominated by any vertex in D∗(e) ∩(CW ∪CE).
Choose pℓto be the point in UCS with the leftmost L1(pℓ), and pr to be the point in
UCS with the rightmost L−1(pr). Similarly, let UCN be the set of all vertices v in
V (e) such that v is dominated by some vertex in D∗(e) ∩CN, but not dominated
by any vertex in D∗(e) ∩(CW ∪CE). Choose qℓto be the point in UCN with the
leftmost L−1(qℓ), and qr to be the point in UCN with the rightmost L1(qr).
Deﬁne TS = {pℓ, pr} if UCS ̸= ∅, and TS = ∅otherwise; and TN = {qℓ, qr}
if UCN ̸= ∅, and TN = ∅otherwise. By Lemma 4.17, every vertex in V1(e) =
ΔS(TS)∪ΔN(TN) is dominated by D∗(e)∩(N ∪S), and every vertex in V2(e) =
V (e) \ V1(e) is dominated by D∗(e) ∩(W ∪E). In other words, the partition
(V1(e), V2(e)) is a feasible partition of V (e). This observation suggests that we

150
Partition
search for feasible partitions of V (e) by searching over partitions corresponding to
all possible sets TS and TN.
In the following, we let
VCS(e) = {v ∈V (e) | v is dominated by some vertex in CS},
VCN(e) = {v ∈V (e) | v is dominated by some vertex in CN},
V +
1 (e) = V +(e) ∩(N ∪S), and
V +
2 (e) = V +(e) ∩(W ∪E).
In addition, let
T (e) = {(TS, TN) | TS ⊆VCS(e), |TS| ≤2, TN ⊆VCN(e), |TN| ≤2}.
Algorithm 4.D (2-Approximation for WDS-UDG1)
Input: A cell e, sets V (e), V +(e), a weight function c : V +(e) →R+.
(1) u ←argminv∈V (e)c(v); D ←{u}.
(2) For each pair (TS, TN) ∈T (e) do
(2.1) V1(e) ←ΔS(TS) ∪ΔN(TN);
(2,2) V2(e) ←V (e) \ V1(e);
(2.3) D1 ←the minimum-weight subset of V +
1 (e) that dominates
V1(e) (by Corollary 4.14);
(2.4) D2 ←the minimum weight subset of V +
2 (e) that dominates
V2(e) (by Corollary 4.14);
(2.5) if c(D) > c(D1 ∪D2) then D ←D1 ∪D2.
(3) Output D.
It is clear that step (2) is executed O(|V (e)|4) times and so, by Corollary 4.14,
Algorithm 4.D runs in time O(n9).
Next, to estimate the performance of Algorithm 4.D, we note that if D∗(e) ∩
V (e) ̸= ∅, then c(D) = c(D∗(e)). On the other hand, if D∗(e) ∩V (e) = ∅, then
for the sets TS and TN deﬁned by points pℓ, pr and qℓ, qr that are chosen based on
D∗(e), we have
c(D1) ≤c(D∗(e) ∩(N ∪S))
and
c(D2) ≤c(D∗(e) ∩(W ∪E)).
Therefore,
c(D) ≤c(D1 ∪D2) ≤2c(D∗(e)).
This completes the proof of Theorem 4.15.

4.4 Double Partition
151
4.4.3
A 6-Approximation for WDS-UDG on a Large Cell
We ﬁrst note that the 2-approximation to WDS-UDG1 gives us immediately a 28-
approximation to WDS-UDG (see Exercise 4.13). The performance ratio 28 is,
however, too large, and we now proceed to improve this approximation algorithm.
The main idea is to combine sets V1(e) of the cells along a horizontal strip and
combine sets V2(e) of the cells along a vertical strip, and work on them together.
This approach unfortunately only works for the graphs lying in a square of a ﬁxed
size. More precisely, we will develop the following result in this subsection.
Theorem 4.18 For any constant m > 0, the subproblem of WDS-UDG restricted
to input graphs that lie in a square of size mμ × mμ has a 6-approximation that
runs in time nO(m2).
In the following, we assume that the input unit disk graph G = (V, E) lies in
the interior of a square Q of size mμ × mμ, for some constant m > 0. Divide the
square Q into m2 cells with each cell of size μ × μ. Let C be the set of the cells
in Q. We collect cells in C whose lower edges lie on the same horizontal line as a
horizontal strip, and collect the cells in C whose left edge lie on the same vertical
line as a vertical strip. We let H1, H2, . . ., Hm denote all horizontal strips, and
Y1, Y2, . . ., Ym all vertical strips.
Intuitively, our approximate solution consists of three parts:
(1) For some cells e, we use a single vertex in e to dominate all vertices in e (like
in case 1 of the Section 4.4.2).
(2) For other cells e, we get a feasible partition (V1(e), V2(e)) of V (e). Then we
combine sets V1(e) over all cells in a horizontal strip Hi and apply the algo-
rithm of Corollary 4.14 to get a minimum-weight dominating set for vertices
in 
e∈Hi V1(e).
(3) We combine sets V2(e) over all cells in a vertical strip Yi and get a minimum-
weight dominating set for vertices in 
e∈Yi V2(e).
To see how this works, let us ﬁrst analyze how an optimal solution can be con-
verted to such a feasible approximate solution. Let Δ∗be a minimum-weight dom-
inating set for G, and opt be its total weight; that is, opt = c(Δ∗). For each cell e,
let Δ+(e) denote the set of vertices u ∈Δ∗that dominates some vertices in V (e).
Recall some notations used in the last subsection: For each cell e, we let N be
the area above the upper edge of e, S the area below the lower edge of e, W the
area to the left of the left edge of e, and E the area to the right of the right edge
of e. Let V +
1 (e) = V +(e) ∩(N ∪S) and V +
2 (e) = V +(e) ∩(W ∪E). Also, let
Δ+
1 (e) = Δ+(e) ∩(N ∪S) and Δ+
2 (e) = Δ+(e) ∩(W ∪E).
Now we convert Δ∗into a feasible approximate solution. For part (1), let C1 =
{e ∈C | e ∩Δ∗̸= ∅}, and, for any e ∈C1, let ve be the vertex in Δ∗∩e of the
lowest weight. Then ve dominates all vertices in V (e).
For part (2), we ﬁrst let U = {ve | e ∈C1} and ZU = {v ∈V | v is dom-
inated by some ve ∈U}. Then for each e ∈C −C1, we ﬁnd a feasible partition

152
Partition
(V1(e), V2(e)) of V (e) \ ZU by ﬁnding points pℓ, pr, qℓ, qr and sets TS and TN
according to Δ+(e), as described in the last subsection. Now, we deﬁne, for each
i = 1, 2, . . ., m, a subset VHi = 
e∈(C−C1)∩Hi V1(e). Note that the set
Δ+
1 (Hi) =

(
e∈(C−C1)∩Hi
Δ+
1 (e)

\ U
dominates VHi.
Similarly, for part (3), we deﬁne, for each i = 1, 2, . . ., m, a set VYi =

e∈(C−C1)∩Yi V2(e), and observe that set
Δ+
2 (Yi) =

(
e∈(C−C1)∩Yi
Δ+
2 (e)

\ U
dominates VYi.
In summary, we divide, in the above, set V into the following (mutually disjoint)
parts:
(i) V (e), for e ∈C1, and ZU;
(ii) VHi, for i = 1, 2, . . ., m; and
(iii) VYi, for i = 1, 2, . . ., m.
For each part, a subset of Δ∗has been identiﬁed that dominates that part; namely,
U dominates 
e∈C1 V (e) and ZU, Δ+
1 (Hi) dominates VHi, and Δ+
2 (Yi) dominates
VYi.
The above analysis suggests that we can divide the original problem into the
following three types of subproblems and use the union of the solutions to these
subproblems as the approximate solution to the original problem:
A1: For each e ∈C1, ﬁx a vertex v ∈V (e) to dominate V (e).
A2: For each i = 1, 2, . . ., m, ﬁnd a minimum-weight subset of V +
1 (Hi) =

e∈(C−C1)∩Hi V +
1 (e) that dominates VHi.
A3: For each i = 1, 2, . . ., m, ﬁnd a minimum-weight subset of V +
2 (Yi) =

e∈(C−C1)∩Yi V +
2 (e) that dominates VYi.
We note that all these subproblems can be solved in polynomial time. In partic-
ular, each subproblem of A2 and A3 can be solved by the algorithm of Corollary
4.14. The only problem we have now is that all the above subproblems are deﬁned
assuming we know what sets C1 and U are. Since C1 and U are deﬁned from the
optimal solution Δ∗, this assumption is too strong. Instead, we will work on all pos-
sible subsets C1 of C and all possible sets U; that is, we will solve the subproblems
for all C1 and U and use the solution of the minimum weight as the approximate
solution.

4.4 Double Partition
153
To prepare for the presentation of the complete algorithm, we need some more
notations. First, let C′ be the set of all nonempty cells; that is, C′ = {e ∈C |
V (e) ̸= ∅}. For any C1 ⊆C′, let UC1 be the collection of all sets U that contain
exactly one vertex in each cell e ∈C1.
Next, let T (e) be the set of all possible choices of pairs (TS, TN), where TS ⊆
VCS(e)\ZU , |TS| ≤2, and TN ⊆VCN(e)\ZU , |TN| ≤2. Now, for each C1 ⊆C,
let TC1 be the Cartesian product of T (e) over all e ∈C −C1; that is, if cells in
C −C1 are e1, e2, . . ., ek, then TC1 = {(TS,1, TN,1, TS,2, TN,2, . . ., TS,k, TN,k) |
(TS,j, TN,j) ∈T (ej), j = 1, 2, . . ., k}.
Algorithm 4.E (6-Approximation for WDS-UDG on a Large Cell)
Input: A unit disk graph G = (V, E) on a square Q of size mμ × mμ.
(1) For each C1 ⊆C′ do
(1.1) for each U ∈UC1 do ﬁnd A(C1, U);
(1.2) let U ∗(C1) ←argmin
U∈UC1
c(A(C1, U)).
(2) Let C∗←argmin
C1⊆C′ c(A(C1, U ∗(C1))).
(3) Output A ←A(C∗, U ∗(C∗
1)).
In the above, each set A(C1, U) is computed by the following procedure:
Algorithm for Function A(C1, U):
(1) Let ZU ←{u ∈V | u is dominated by some v ∈U}.
(2) For each T ∈TC1 do
(2.1) For each cell e ∈C −C1 do
let (TS, TN) be the pair in T corresponding to cell e;
V1(e) ←ΔS(TS) ∪ΔN(TN);
V2(e) ←(V (e) \ ZU) \ V1(e);
(2.2) For i ←1 to m do
D∗(Hi) ←the minimum-weight subset of V +
1 (Hi) that
dominates VHi (by Corollary 4.14);
D∗(Yi) ←the minimum-weight subset of V +
2 (Yi) that
dominates VYi (by Corollary 4.14);
(2.3) D(T) ←m
i=1(D∗(Hi) ∪D∗(Yi)).
(3) Let T ∗←argmin
T ∈TC1
c(D(T)).
(4) A(C1, U) ←D(T ∗) ∪U.

154
Partition
To prove that Algorithm4.E is a 6-approximation,we claim that the set A(C1, U)
found by Algorithm 4.E, with C1 = {e ∈C | Δ∗∩V (e) ̸= ∅} and U = {ve | e ∈
C1}, where ve is the vertex in Δ∗∩V (e) with the lowest weight, must have
c(A(C1, U)) ≤c(U) +
m

i=1
c(Δ+
1 (Hi)) +
m

i=1
c(Δ+
2 (Yi)).
(4.3)
To see this, recall that when we convert the optimal solution Δ∗to a feasible ap-
proximate solution, we constructed, for each cell e ∈C −C1, a pair (TS, TN) of
subsets of V (e), and used them to partition V (e) \ ZU into V1(e) and V2(e). We
observe that in step (2) of the algorithm for function A(C1, U), we run through all
possible T ∈TC1, including the one that contains, corresponding to each e, the pair
(TS, TN) we obtained in the above conversion. Thus, for this set T, the partitions
(V1(e), V2(e)) of V (e) \ ZU for each e ∈C −C1, and hence the sets VHi and VYi
for each i = 1, 2, . . ., m, that we found in the algorithm for A(C1, U) are iden-
tical to those we obtained in the conversion. Thus, c(D∗(Hi)) ≤c(Δ+
1 (Hi)) and
c(D∗(Yi)) ≤c(Δ+
2 (Yi)), for i = 1, 2, . . ., m, and
c(A(C1, U)) ≤c(U) + c(D(T)) ≤c(U) +
m

i=1
c(D∗(Hi)) +
m

i=1
c(D∗(Yi))
≤c(U) +
m

i=1
c(Δ+
1 (Hi)) +
m

i=1
c(Δ+
2 (Yi)).
Next, we notice that Δ+
1 (Hi)∩U = Δ+
2 (Yi)∩U = ∅, and so each vertex ve ∈U
is counted at most once on the right-hand side of (4.3). For any other vertex u ∈Δ∗,
we note that the dominating range of u is a disk of radius 1, and so it can overlap
with at most four horizontal strips. Since a vertex u lying in strip Hi cannot appear
in Δ+
1 (Hi), it can appear in at most three different Δ+
1 (Hj)’s. Similarly, a vertex
u can appear in at most three different Δ+
2 (Yj)’s. That is, a vertex u ∈Δ∗can be
counted at most six times on the right-hand side of (4.3). It follows that the output
A of Algorithm 4.E satisﬁes
c(A) ≤c(A(C1, U)) ≤c(U) +
m

i=1
c(Δ+
1 (Hi)) +
m

i=1
c(Δ+
2 (Yi)) ≤6 · opt.
Finally, let us estimate the time complexity of Algorithm 4.E. First, for any C1 ⊆
C′ and any U ∈UC1, there are at most O(n4m2) sets T ∈TC1, and each D∗(Hi)
and each D∗(Yi) in step (2.2) of the algorithm for A(C1, U) can be found in time
O(n5). Therefore, set A(C1, U) can be found in time nO(m2). Now, we observe that
there are O(2m2) subsets C1 of C′ and, for each C1 ⊆C′, UC1 contains at most
nO(m2) sets U. Therefore, the total running time of Algorithm 4.E is nO(m2). This
completes the proof of Theorem 4.18
Corollary 4.19 The subproblem of WDS-UDG that asks, for a given graph G =
(V, E), a constant m > 0, and an mμ×mμ square S, for a minimum-weight subset

4.4 Double Partition
155
m
μ
μ
Figure 4.18:
Double partition.
of vertices in G that dominate all vertices in S has a 6-approximation with running
time nO(m2), where n = |V |.
Proof. The algorithm for this problem is almost identical to Algorithm 4.E, except
that we include vertices outside square S in sets V +
1 (Hi) and V +
2 (Yi) in step (2.2)
of the algorithm for A(C1, U).
□
4.4.4
A (6 + ε)-Approximation for WDS-UDG
Now, we apply the double partition and shifting techniques to obtain a (6 + ε)-
approximation to WDS-UDG.
Theorem 4.20 For any ε > 0, there exists a (6+ε)-approximation for WDS-UDG
with computation time nO(1/ε2).
Proof. Assume that ε < 1, and let m = ⌈72/ε⌉. Without loss of generality, assume
that G lies in a square
Q = {(x, y) | mμ ≤x < kmμ, mμ ≤y ≤kmμ}
for some integer k > 1. Let
Q = {(x, y) | 0 ≤x < kmμ, 0 ≤y ≤kmμ}.
We partition Q into k2 cells, each of size mμ × mμ. We call this partition P (0, 0)
(see Figure 4.18). For each a ∈{0, 1, . . ., m −1}, partition P (a, a) is the partition
P (0, 0) with its lower-left corner shifted to (aμ, aμ).
For each partition P (a, a), we solve the problem WDS-UDG for each cell e in
P (a, a) by Corollary 4.19. Let this solution be Aa(e). Then Aa = 
e∈P(a,a) Aa(e)

156
Partition
is an approximate solution to G. Let Aa∗be the one with the minimum weight
c(Aa∗) among all these solutions over a = 0, 1, . . ., m −1. We output A = Aa∗as
the approximate solution to G.
Let Opt be an optimal solution to G, and opt = c(Opt). We claim that c(A) ≤
(6 + ε)opt.
The proof of the claim is similar to the proof of Theorem 4.2. For any partition
P (a, a) and any cell e ∈P (a, a), let Δ∗(e) be the optimal solution to the sub-
problem deﬁned in Corollary 4.19 with respect to cell e. From Corollary 4.19, we
know that c(Aa(e)) ≤6 · c(Δ∗(e)). Let Opt(e) = {u ∈Opt | u dominates some
v ∈V (e)}. Since Δ∗(e) is the optimal solution to the subproblem on cell e, we
have c(Δ∗(e)) ≤c(Opt(e)) and, hence, c(Aa(e)) ≤6 · c(Opt(e)).
A vertex u in Opt may belong to Opt(e) for more than one cell e. For any parti-
tion P (a, a), let Ha = {u ∈Opt | u belongs to two cells of two different horizontal
strips}, and Ya = {u ∈Opt | u belongs to two cells of two different vertical strips}.
Note that if u belongs to Opt(e), then the disk Du with center u and radius 1 has a
nonempty intersection with cell e. Therefore, we have
c(Aa) ≤

e∈P(a,a)
c(Aa(e)) ≤6

e∈P(a,a)
c(Opt(e)) ≤6(opt + c(Ha) + 2 · c(Ya)).
Next, we observe that a vertex u belongs to Ha only if Du intersects a horizontal
grid line of P (a, a). Since the shifting distance is 1/
√
2, a disk of radius 1 can
intersect horizontal grid lines of at most four partitions. That is, a vertex can belong
to at most four different sets Ha. Therefore,
m−1

a=0
c(Ha) ≤4 · opt.
Similarly,
m−1

a=0
c(Ya) ≤4 · opt.
It follows that
m−1

a=0
c(Aa) ≤6

m · opt +
m−1

a=0
c(Ha) + 2
m−1

a=0
c(Ya)

≤6(m + 12)opt.
So, the minimum solution Aa∗has weight
c(Aa∗) ≤1
m
m−1

a=0
c(Aa) ≤

6 + 72
m

opt ≤(6 + ε)opt.
Finally, we verify that the total computation time of this algorithm is, from Corol-
lary 4.19, m · nO(m2) = nO(1/ε2).
□
This result can be extended to the problem of ﬁnding the minimum-weight con-
nected dominating set in a unit disk graph.

4.5 Tree Partition
157
WEIGHTED CONNECTED DOMINATING SET IN A UNIT DISK GRAPH
(WCDS-UDG): Given a unit disk graph G = (V, E) with a weight
function c : V →R+, ﬁnd a connected dominating set with the mini-
mum total weight.
Theorem 4.21 For any ε > 0, there exists a (7 + ε)-approximation for WCDS-
UDG that runs in time nO(1/ε2).
Proof. We ﬁrst ﬁnd a dominating set D for G of total weight
c(D) ≤

6 + ε
2

c(Δ∗),
where Δ∗is a minimum-weight dominating set for G. Thus, we have reduced the
problem to the following subproblem:
WCDS-UDG1: Given a unit disk graph G = (V, E) with a weight
function c : V →R+, and a dominating set D ⊆V , ﬁnd a minimum-
weight subset C ⊆V −D that connects D.
It can be shown that WCDS-UDG1 has a PTAS (see Exercise 4.17). So, we can
ﬁnd a (1 + ε/2)-approximate solution C ⊆V −D for WCDS-UDG1 and use
C ∪D as the solution to graph G for problem WCDS-UDG.
Let C∗be a minimum-weight connected dominating set of G and C′ a minimum-
weight subset of V that connects D. We verify that c(C ∪D) is a (7 + ε)-
approximation to C∗:
First, it is obvious that c(Δ∗) ≤c(C∗), and so c(D) ≤(6 + ε/2) · c(C∗). Next,
we observe that C∗∪D is a connected dominating set of G, and so C∗\ D is a
feasible solution to WCDS-UDG1 for input (G, D). Therefore,
c(C) ≤

1 + ε
2

c(C′) ≤

1 + ε
2

c(C∗\ D) ≤

1 + ε
2

c(C∗).
Together, we get c(C ∪D) ≤(7 + ε) · c(C∗).
□
4.5
Tree Partition
Recall the problem PHYLOGENETIC TREE ALIGNMENT (PTA) introduced in Sec-
tion 3.5, where we showed that the optimal lifted alignment is a 2-approximation to
PTA. In this section, we use a tree partition to construct a PTAS for PTA. We assume
that in the input tree T to the problem PTA, every internal vertex has at least 2, and
at most d, children, where d is a constant greater than 2. First, we note that Lemma
3.4 about regular binary trees can be extended to such trees.
Lemma 4.22 For any rooted tree in which each internal vertex has at least two
sons, there exists a mapping f from all internal vertices to leaves such that
(a) For every internal vertex u, f(u) is a descendant of u; and

158
Partition
(b) All tree paths from u to f(u) are edge-disjoint.
Let T be a tree and t > 1 a constant. For each i = 0, 1, . . ., t −1, let Vi be the
set of vertices of T in any level j, with j ≡i (mod t). We may consider each set
Vi as a partition of T into a collection of small trees of at most t + 1 levels. To be
more precise, each small tree in the partition is either rooted at the root r of T and
containing all vertices in levels j ≤i, or is rooted at a vertex v ∈Vi and containing
all descendants of v in levels j, with level(v) ≤j ≤level(v) + t. Thus, each small
tree in the partition has at most t + 1 levels and at most (dt+1 −1)/(d −1) vertices.
For such a small tree with t + 1 levels, its root and leaves of level t + 1 all belong to
Vi.
Suppose T has k leaves with labels s1, s2, . . ., sk, respectively. The problem PTA
asks for the labeling of the internal vertices of T with the minimum total alignment
scores. We say a tree T ′ is a phylogenetic alignment of T if
(i) T ′ has the same vertex set and edge set as T,
(ii) Each vertex of T ′ is labeled with a string, and
(iii) The leaves of T ′ are labeled with the same strings as those of T.
A phylogenetic alignment tree T ′ is called t-restricted if there is an integer i ∈
{0, 1, . . ., t −1} such that the label of every vertex v in Vi is the same as the label
of a descendant leaf of v.
Lemma 4.23 Let T be a tree whose leaves have been labeled with strings s1, s2,
. . ., sk. For any t > 0, there exists a t-restricted, phylogenetic alignment T ′ of tree
T such that
cost(T ′) ≤

1 + 3
t

opt,
where cost(T ′) is the total alignment score of the tree T ′, and opt is the minimum
cost of a phylogenetic alignment of T.
Proof. Let T ∗be an optimal phylogenetic alignment of T. Assume that each internal
vertex v of T ∗is assigned with string s∗
v. Now, for each vertex v ∈V , let sv be the
label of a descendant leaf of v such that D(sv, s∗
v) is minimized. Deﬁne, for each
i ∈{0, 1, . . ., t}, a phylogenetic alignment Ti of T as follows: For each v ∈Vi,
label it with string sv, and for any other internal vertex u, label it with s∗
u. Let us
estimate the total alignment score cost(Ti) of tree Ti.
For an internal vertex v ∈Vi, let π(v) denote the parent of v and Γ(v) denote the
set of the children of v. First, we observe that, for any w ∈Γ(v), sw is the label of a
descendant leaf of w, and so also the label of a descendant leaf of v. It follows that
D(s∗
v, sv) ≤D(s∗
v, sw). Therefore, by the triangle inequality, we have

4.5 Tree Partition
159
D(s∗
π(v), sv) +

w∈Γ(v)
D(sv, s∗
w)
≤D(s∗
π(v), s∗
v) +

w∈Γ(v)
D(s∗
v, s∗
w) + (|Γ(v)| + 1) D(s∗
v, sv)
≤D(s∗
π(v), s∗
v) +

w∈Γ(v)
D(s∗
v, s∗
w) + D(s∗
v, sv) +

w∈Γ(v)
D(s∗
v, sw)
≤D(s∗
π(v), s∗
v) + 2

w∈Γ(v)
D(s∗
v, s∗
w) + D(s∗
v, sv) +

w∈Γ(v)
D(s∗
w, sw).
Thus,
cost(Ti) −cost(T ∗)
≤

v∈Vi
)
D(s∗
π(v), sv) +

w∈Γ(v)
D(sv, s∗
w) −D(s∗
π(v), s∗
v) −

w∈Γ(v)
D(s∗
v, s∗
w)
*
≤

v∈Vi
) 
w∈Γ(v)
D(s∗
v, s∗
w) + D(s∗
v, sv) +

w∈Γ(v)
D(s∗
w, sw)
*
=

v∈Vi∪Vi+1
D(s∗
v, sv) +

v∈Vi

w∈Γ(v)
D(s∗
v, s∗
w).
It is clear that
t−1

i=0

v∈Vi

w∈Γ(v)
D(s∗
v, s∗
w) = cost(T ∗).
Furthermore, we note that D(s∗
v, sv) is the minimum of D(s∗
v, s∗
z) over all descen-
dant leaves z of v. Therefore, by the triangle inequality, for any descendant leaf z
of v, the total cost of the path in T ∗from v to z is at least as large as D(s∗
v, sv). By
Lemma 4.22, there is a function f mapping each internal vertex v to a descendant
leaf f(v) of v such that all paths from v to f(v) are edge-disjoint. Let Π(v) denote
the path in T ∗from v to f(v). Then
t−1

i=0

v∈Vi
D(s∗
v, sv) ≤

v∈V
D(s∗
v, s∗
f(v)) ≤

v∈V
cost

Π(v)

≤cost(T ∗) = opt.
Together, we have
t−1

i=0
cost(Ti) ≤(t + 3)opt.
Therefore, there exists an integer i ∈{0, 1, . . ., t −1} such that
cost(Ti) ≤

1 + 3
t

opt.
□

160
Partition
For any ﬁxed integer t > 0, the optimal t-restricted phylogenetic alignment of a
given tree T can be computed by dynamic programming in time
O

kdt−1+2 ndt−1+1
,
where k is the number of leaves in T and n is the total length of the leave labels (see
Exercise 4.20). Therefore, we have a PTAS for the problem PTA.
Theorem 4.24 For any t ≥3, there exists a polynomial-time (1 + 3/t)-approxima-
tion for the problem PTA.
There are a number of ways to partition trees to get approximate solutions. The
reader may ﬁnd more examples in the exercises.
Exercises
4.1 Find a necessary and sufﬁcient condition for two points in the plane to have
exactly one unit disk with its boundary passing through them.
4.2 Consider the problem of ﬁnding the minimum number of d-dimensional
balls that cover a given set of n points in the d-dimensional Euclidean space. Show
that this problem has a (1+1/m)-approximation with running time O(md·n2md+1).
4.3 Show that the problem of ﬁnding the minimum vertex cover in a unit disk
graph has a PTAS.
4.4 A vertex cover C in a graph G is connected if the subgraph induced by C is
connected.
(a) Show that the problem of ﬁnding the minimum connected vertex cover in a
given graph has a polynomial-time 3-approximation.
(b) Show that the problem of ﬁnding the minimum connected vertex cover in a
unit disk graph has a PTAS.
4.5 Show that for any connected graph G, its minimum dominating set D and
minimum connected dominating set C have the following relationship: |C| ≤
3|D| −2.
4.6 Can you ﬁnd a polynomial-time constant approximation for the problem of
ﬁnding a minimum-weight connected dominating set in a vertex-weighted unit disk
graph?
4.7 An independent set of a graph G = (V, E) is a subset I ⊆V with no edge
in E connecting any two vertices in I. An independent set I is maximal if there is
no other independent set properly contains I. Note that any maximal independent
set in a graph G is a dominating set of G.

Exercises
161
(a) Design a polynomial-time algorithm to compute a maximal independent set
I for a given graph G such that |I| ≥(|C| + 1)/2, where C is a minimum
connected dominating set of G.
(b) Show that, in a unit disk graph G, any maximal independent set I and the
minimum connected dominating set C have the relationship |C| ≤4|I|+1.
(c) Use fact (b) above to design a polynomial-time 8-approximation for the
problem of ﬁnding the minimum connected dominating set in a unit disk
graph.
4.8 Consider the subproblem of ESMT with the following restriction:
(R1) The ratio of the length of the longest edge to the length of the shortest edge
in the minimum spanning tree of the terminal points is bounded above by a
constant.
Show that there is a PTAS for this subproblem of ESMT.
4.9 Consider the following problem:
RECTILINEAR STEINER MINIMUM TREE WITH RECTILINEAR OB-
STRUCTION (RSMTRO): Given a set T of terminal points and a set
R of rectilinear rectangles in the rectilinear plane, ﬁnd the Steiner min-
imum tree that connects terminals in T and avoids the rectilinear ob-
structions in R.
Show that the subproblem of RSMTRO with the restriction (R1) deﬁned in Exercise
4.8 has a PTAS.
4.10 Show that the problem of ﬁnding the maximum-weight independent set in
a vertex-weighted intersection disk graph has a PTAS.
4.11 Show that the problem of ﬁnding the minimum-weight vertex cover in a
vertex-weighted intersection disk graph has a PTAS.
4.12 In the proof of Lemma 4.10, consider a different approach in which we do
not introduce the notion of relevant cells but use the following more straightforward
recursive relation: For a j-cell e and a set I of independent disks in layers < j that
intersect e, let J∗= argminJ∈INDj(e,I)|J ∪AJ| and T(e, I) = J∗∪AJ∗, where
AJ =
(
e′∈Cj+1(e)
T(e′, (I ∪J)e′),
and Cj+1(e) is the set of all cells e′ in layer j + 1 that are contained in e. With this
recursive formula, does the corresponding dynamic programming algorithm still run
in polynomial time? Justify your answer.
4.13 Show that if we divide the a square into cells of size (1/
√
2) × (1/
√
2),
then a unit disk can intersect at most 14 cells. Use this fact, together with the 2-
approximation to the problem WDS-UDG1, to get a 28-approximation to WDS-
UDG.

162
Partition
4.14 Consider the following modiﬁcation on Algorithm 4.E: We ﬁx, for any
C1 ⊆C′, UC1 to consist of a single set U = {ve | e ∈C1}, where ve is the
minimum-weight vertex in cell e. Is Algorithm 4.E still a 6-approximation to WDS-
UDG on a square of size mμ × mμ?
4.15 Suppose that in the partition for problem WDS-UDG, we use hexagonal
2
1
Figure 4.19:
Hexagonal cells.
small cells of edge length 1/2 (see Figure 4.19) instead of square cells of edge
length μ. Can you get a polynomial-time approximation with performance ratio
smaller than 2 for WDS-UDG1? Can you get an approximation with performance
ratio better than (6 + ε)?
4.16 Prove the following results about vertex-weighted unit disk graphs.
(a) Let G = (V, E) be a vertex-weighted unit disk graph. For any vertex subset
U ⊆V , if the subgraph of G induced by U is connected, then there is a
spanning tree on U with each vertex having degree at most 5.
(b) There exists a 4-approximation to the following problem: Given a weighted
unit disk graph G = (V, E) and a set of terminals P ⊆V , ﬁnd a Steiner
tree on P with the minimum total vertex-weight.
4.17 Show that the following problem has a PTAS:
Given a vertex-weighted unit disk graph G = (V, E) and a dominating
set D ⊆V , ﬁnd the minimum-weight subset C ⊆V interconnecting
D.
4.18 Consider the following problem:
MAXIMUM INDEPENDENT RECTANGLES (MAX-IR): Given a set of n
rectangles in the rectilinear Euclidean plane, ﬁnd the maximum subset
of mutually disjoint rectangles.
(a) Show that the subproblem of MAX-IR with the following restriction has a
PTAS:

Exercises
163
(R2) The ratio of the height to the width of every input rectangle is in the
range [a, b] for some constants 0 < a < b.
∗(b) Is there a constant approximation for the problem MAX-IR on rectlinear
rectangles without the condition (R2)?
4.19 Let r and s be two integers with 0 ≤s < 2r, and let k = 2r + s. For each
balanced binary tree, consider the following labeling, which assigns each vertex
with a set of exactly 2r integers chosen from L = {1, 2, . . ., r2r + s}:
(1) For each vertex v at the ith level, 0 ≤i ≤r −1, assign v with label
set {i2r + 1, i2r + 2, . . ., (i + 1)2r}. In particular, the root has label set
{1, 2, . . ., 2r}, and its two sons have label sets {2r + 1, 2r + 2, . . ., 2r+1}.
(2) For each i ≥r, assume that vertices at levels j, 0 ≤j ≤i −1, have been
labeled. Let u be a vertex at level i. The label set for u is deﬁned as follows:
(i) First, let v be the ancestor of u at level i −r, and let its label set be
Sv = {ℓ1, ℓ2, . . ., ℓ2r}. Suppose that u is the jth level-i descendant of
v; then, let S′
u = {ℓt mod 2r | j ≤t ≤j + 2r −s −1}.
(ii) Assume that the label sets of the lowest r ancestors of u are L1, L2,
. . ., Lr. Let the label set of u be
Su = S′
u ∪(L −(L1 ∪· · · ∪Lr)).
Show that the above labeling induces r2r + s partitions of a balanced binary tree T
such that each partition breaks tree T into smaller trees each of at most k leaves and
that each vertex in T appears as a break point in at most 2r partitions.
4.20 Let T be a tree whose leaves are labelled with strings. Let t > 0 be a
constant integer and, for each i ∈{0, 1, . . ., t −1}, let Vi be the set of vertices in
T at levels j ≡i (mod t). Design, by dynamic programming, a polynomial-time
algorithm to ﬁnd an optimal t-restricted phylogenetic alignment tree Ti of T with
each vertex v in Vi labeled with the same label as one of its descendant leaves.
4.21 Consider the following problem:
VERTEX-WEIGHTED ST: Given a graph G = (V, E) with vertex-
weight c : V →R+, and a subset P of V , ﬁnd a Steiner tree inter-
connecting vertices in P with the minimum total vertex-weight.
For a given graph G = (V, E) with vertex-weight c : V →R+, and a set P ⊆V ,
let π(u, v) be the path between vertices u and v with the minimum total weight.
Construct a complete graph K on P and assign every edge {u, v} with weight equal
to c(π(u, v)) −c(u) −c(v). Show that the minimum spanning tree of K induces a
4-approximation for VERTEX-WEIGHTED ST in a unit disk graph.
⋆4.22 Is there a constant approximation for the problem of ﬁnding the minimum
dominating set in an intersection disk graph?

164
Partition
Historical Notes
Partition is a simple idea that has been used in the design of approximation or heuris-
tic algorithms for a long time. Karp [1977] gave the ﬁrst probabilistic analysis for
partition with applications to Euclidean TSP. Komolos and Shing [1985] applied
this approach to RSMT. Baker [1983, 1994] and Hochbaum and Maass [1985] in-
troduced the shifting technique to design deterministic PTASs for a family of prob-
lems in covering and packing. This technique is used extensively to design PTASs
for many problems [Min et al., 2003; Cheng et al., 2003; Zhang, Gao, Wu, and Du,
2009; Hunt et al., 1998, Vavasis, 1991; Wang and Jiang, 1996]. Cheng et al. [2003]
gave the ﬁrst PTAS for the minimum connected dominating set in a unit disk graph.
Zhang, Gao, Wu, and Du [2009] provided a simple one that runs faster and can be
extended to unit ball graphs in higher-dimensional space.
Erlebach et al. [2001] ﬁrst introduced the multilayer partition technique to deal
with disks with different sizes and with arbitrary squares. The maximum indepen-
dent set problem in rectangle intersection graphs has interesting applications in map
labeling and data mining [Agarwal et al., 1998; Berman et al., 2001; Chan, 2003;
Erlebach et al., 2001]. Various partition techniques yield PTASs for this problem
under the restriction (R2) (see Exercise 4.18). For arbitrary rectangles, no constant
approximation has been found. The best-known approximation has a performance
ratio O(log n) [Agarwal et al., 1998; Chan, 2004; Khanna et al., 1998; Nielsen,
2000].
Amb¨uhl et al. [2006] used the partition technique to get a polynomial-time 72-
approximation for the minimum-weight dominating set and a polynomial-time 84-
approximation for the minimum-weight connected dominating set in a unit disk
graph. Gao et al. [2008] introduced the double-partition technique and obtained a
(6 + ε)-approximation for the minimum-weight dominating set and a (10 + ε)-
approximation for the minimum-weight connected dominating set in a unit disk
graph. Dai and Yu [2009] improved the ﬁrst result to a (5 + ε)-approximation. Zou
et al. [2008a] improved the second result to a (9.85 + ε)-approximation, and Zou et
al. [2008b] further lowered the performance ratio to (6 + ε).
Du, Zhang, and Feng [1991] proved a useful lemma for the shifting technique
in tree partition when they proved a lower bound for the k-Steiner ratio. With this
lemma, Jiang et al. [1994] and Wang et al. [1996] designed a PTAS for the tree
alignment problem. Wang et al. [1997] introduced a new partition of balanced binary
trees that results in a more efﬁcient PTAS (see Exercise 4.19).

5
Guillotine Cut
It will be as ﬂeeting as a cool breeze
upon the back of one’s neck.
— Joseph I. Guillotine
Guillotine cut is a technique of adaptive partition that has found interesting appli-
cations in many geometric problems. Roughly speaking, a guillotine cut is a subdi-
vision by a straight line that partitions a given area into at least two subareas. By
a sequence of guillotine cut operations, we can partition the input area into smaller
areas, solve the subproblems in these smaller areas, and combine these solutions to
obtain a feasible solution to the original input.
In some applications of guillotine cut, there may be an exponential number of
ways to form a feasible solution from the solutions of the subproblems. A few meth-
ods have been developed to reduce the number of ways of combining the solutions
of the subproblems and yet still preserve good approximation. In this chapter, we
study the technique of guillotine cut and the related methods for combining the so-
lutions of subproblems.
5.1
Rectangular Partition
We start with a geometric problem MIN-RP, which has a number of applications
in engineering design, such as process control, layout for integrated circuits, and
architectural design.
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9_5, 
© Springer Science+Business Media, LLC 2012 
165

166
Figure 5.1:
A rectilinear polygon with holes.
MINIMUM EDGE-LENGTH RECTANGULAR PARTITION (MIN-RP):
Given a rectilinear polygon, possibly with some rectilinear holes, par-
tition it into rectangles with the minimum total edge length.
In the above deﬁnition, by a hole in the input polygon, we mean a rectilinear
polygon that may be completely or partially degenerated into a line segment or a
point (see Figure 5.1). The existence of holes in an input polygon makes a difference
in the polynomial-time solvability of the problem: While the problem MIN-RP,
in the general case, is NP-hard, the problem MIN-RP for hole-free inputs can be
solved in time O(n4), where n is the number of vertices in the input rectilinear
polygon.The polynomial-timealgorithm for the hole-free MIN-RP is an application
of dynamic programming, based on the following fact.
Lemma 5.1 Suppose that the input R to MIN-RP is hole-free. Then there exists an
optimal rectangular partition P for R in which each maximal line segment contains
a vertex of the boundary.1
Proof. Consider a rectangular partition P of R with the minimum total length. Sup-
pose P has a maximal line segment AB that does not contain any vertex of the
boundary. Without loss of generality, let us assume it is a vertical line segment.
Then the two endpoints A and B of this line segment must lie on the interior of
two horizontal line segments that are in P or in the boundary. Suppose there are r
horizontal line segments touching the interior of AB from the right, and ℓhorizontal
line segments touching the interior of AB from the left. We claim that r must be
equal to ℓ. Indeed, if r > ℓ(or r < ℓ), then we can move the line segment AB
to the right (or to the left, respectively) to reduce the total length of the rectangular
partition (see Figure 5.2(a)). This contradicts the optimality of P .
Since r = ℓ, moving AB to either the right or left does not increase the total
length of P. Let us keep moving AB to the left until it is not movable. Then the
1A maximal line segment in a partition is one that cannot be extended farther in either direction.
Guillotine Cut

5.1 Rectangular Partition
167
(a)
B
D
C
A
F
E
B
A
(b)
Figure 5.2:
(a) Moving AB toward CD would reduce the total length of P . (b)
Moving AB to the left and merging it with EF would reduce the total length of P .
line segment AB, in its ﬁnal position, must meet either a vertex of the boundary or
another vertical line segment in P . The latter case is, however, not possible: If AB
meets another line segment EF in P , then they merge into one, and the total length
of the rectangular partition is reduced, contradicting the optimality of P again (see
Figure 5.2(b)).
This proves that the line segment AB can be moved to meet a vertex of the
boundary. We perform such movements for all line segments in P that do not contain
a boundary vertex, and the resulting partition has the required property and has the
same length as P .
□
From the above lemma, we can see that there are only O(n2) candidates for a
line segment in an optimal rectangular partition P satisfying the property of Lemma
5.1: Let us deﬁne a grid point to be the intersection point of any two lines that pass
through a boundary vertex. Then one of the endpoints of a maximal line segment in
P must be a boundary vertex, and the other one must be either a boundary vertex
or a grid point (see Figure 5.3). Therefore, there are O(n2) such line segments.
This observation allows us to design a dynamic programming algorithm to solve the
problem MIN-RP without holes in polynomial time (see Exercise 5.1).
When the input rectilinear polygon has holes, the problem MIN-RP becomes
NP-hard. In the following, we apply the technique of guillotine cut to approximate
this problem.
A guillotine cut is a straight line that cuts through a connected region and breaks
it into at least two subregions. A rectangular partition is called a guillotine rectan-
gular partition if it can be constructed by a sequence of guillotine cuts, each cutting
through a connected subregion. It is not hard to see that the minimum-length guil-
lotine rectangular partition of a given rectilinear polygon (possibly with holes) can
be found in polynomial time. First, it can be proved by the argument similar to that
of Lemma 5.1 that there exists a minimum-length guillotine rectangular partition
in which every maximal line segment contains a vertex of the boundary. Moreover,
the restriction of using only guillotine cuts in each step allows us to apply dynamic

168
Figure 5.3:
The vertices of the boundary (dark circles) and the grid points (white
circles) of a rectilinear polygon.
programming to this problem. That is, we can partition a rectilinear polygon R by
ﬁrst using a guillotine cut to break it into two or more smaller rectilinear polygons,
and then recursively partition these new rectilinear polygons. In this recursive al-
gorithm, there are, in each iteration, O(n) possible choices for the next guillotine
cut. In addition, there are altogether O(n4) possible subproblems, because each sub-
problem’s boundary is composed of pieces of the input boundary plus at most four
guillotine cut edges. Therefore, the minimum-length guillotine rectangular partition
can be computed by dynamic programming in time O(n5).
Since the minimum-length guillotine rectangular partition can be computed in
polynomial time, it is natural to try to use it to approximate the problem MIN-
RP. What is the performance ratio of this method? Unfortunately, no good bounds
of the performance ratio have been found for the general case of MIN-RP. In the
following, we present a special case for which this method has a nice performance
ratio.
MIN-RP1: Given a rectangle R with a ﬁnite number of points inside the
rectangle, ﬁnd a minimum-length rectangular partition of R, treating
the given points as degenerate holes.
It has been proven that the restricted version MIN-RP1 is still NP-hard.
Theorem 5.2 The minimum-length guillotine rectangular partition is a 2-approxi-
mation to MIN-RP1.
Proof. We follow the general approach of the analysis of approximation algorithms
designed by the restriction method; that is, we will convert an optimal rectangular
partition to a guillotine rectangular partition. To be more precise, let R be an input
rectangle to MIN-RP1, and P ∗a minimum-length rectangular partition of R. We
are going to construct a guillotine rectangular partition PG from P ∗such that the
Guillotine Cut

5.1 Rectangular Partition
169
total edge length of PG is at most twice the total edge length of P ∗. Therefore, the
total length of an optimal guillotine rectangular partition cannot exceed twice the
total edge length of P ∗.
In the construction, we will use guillotine cuts to divide R into smaller rectangles
and recursively partition these rectangles. We will call each intermediate rectangle
created by guillotine cuts a window (so that it will not be confused with the ﬁnal
rectangles created by partition PG). For a window W, we write int(W) to denote
the interior area of W.
The guillotine cuts will add new edges to PG \ P ∗. In order to estimate the cost
of these new edges, we will use a charging method to charge the cost of each new
edge in PG \ P ∗to the edges in the original partition P ∗. The charging policy will
be explained using the notion of dark points in a window W. We say a point z in
W is a vertical (or, horizontal) 1-dark point with respect to the partition P ∗and
window W if each vertical (or, respectively, horizontal) half-line starting from z,
but not including point z, going in either direction meets at least one horizontal (or,
respectively, vertical) line segment in P ∗∩int(W). (In particular, a point z on the
boundary of W is not 1-dark.) In the construction, a new horizontal edge t is added
to PG \ P ∗only if all of its points are vertical 1-dark points, and so its cost can be
charged to edges in P ∗that lie parallel to t.
To be more precise, the guillotine rectangular partition PG can be constructed
from P ∗by applying the following rules on each window W, starting with the initial
window W = R:
(1) If int(W) does not contain any edge in P ∗, then do nothing.
(2) If there exists a horizontal line segment s in P ∗that cuts through the whole
window W, then we apply a guillotine cut to W along the line segment s.
(3) If there exists a vertical line segment s in P ∗whose length in W ≥h/2,
where h is the height of the window W, then we apply a guillotine cut to W
along s. The cut extends s to a line segment at most twice as long as s.
(4) If W contains at least one edge in P ∗and yet neither case (2) nor case (3)
holds, then we apply to W a horizontal guillotine cut t that partitions W into
two equal parts.
We note that in case (2), we did not introduce any new edge in PG \ P ∗, and so
there is no extra cost. In case (3), the new edge added to PG \ P ∗has total length
≤length(s). We charge the cost of this new edge to the line segment s.
For case (4), we ﬁrst claim that every point in the line segment t is a vertical
1-dark point. To see this, we assume, by way of contradiction, that there is a point
z in t that is not vertical 1-dark. Then the rectangle deﬁned by the partition P ∗
that contains z must have height at least h/2. The boundary of this rectangle must
contain either a horizontal segment in P that cuts through the whole window W or
a vertical segment in P of length ≥h/2, both cases contradicting the assumption
of case (4). This proves the claim. Since each point in t is vertical 1-dark, we can
charge the cost of each new edge t1 in t ∩(PG \ P ∗) as follows: We charge 1/2

170
Case (3)
Case (4)
Figure 5.4:
Constructing a guillotine partition from a given partition. (The arrows
indicate how to charge its cost.)
of the cost of t1 to the horizontal line segments in P ∗that lie immediately above
the line segment t1, and the other 1/2 to the horizontal line segments in P ∗that lie
immediately below the line segment t1 (see Figure 5.4).
In the above charging policy, each vertical line segment in P ∗is charged at most
once with cost less than or equal to its own length. In addition, each horizontal line
segment in P ∗is charged at most twice, each time with cost less than or equal to 1/2
of its length. To see this, we note that if a horizontal line segment s has been charged
once by a new line segment t1 in PG below it, then t1 becomes the boundary of the
new windows, and all points between t1 and s are non-1-dark in the new window
containing s. Thus, the line segment s cannot be charged again by any cut below it.
The above analysis shows that the total charge and, hence, the total length of
added line segments in PG \ P ∗cannot exceed the total length of P ∗.
Finally, we observe that each time we perform a guillotine cut, each new subwin-
dow must contain fewer line segments in P ∗than the current window. Thus, after
a ﬁnite number of guillotine cuts, int(W) no longer contains any line segment of
P ∗. This means that P ∗⊆PG. In particular, PG covers every given point in R.
Thus, PG is a guillotine rectangular partition of R. This completes the proof of the
theorem.
□
Since the optimal guillotine rectangular partition Q can be computed in time
O(n5), and since its total length does not exceed that of PG, we get the following
conclusion:
Corollary 5.3 The problem MIN-RP1 has a polynomial-time 2-approximation.
5.2
1-Guillotine Cut
The main idea of the proof of Theorem 5.2 is to choose, in case (4), a cut line that
consists of vertical 1-dark points. This idea works for the special case of MIN-RP1,
Guillotine Cut

171
 
 
Figure 5.5:
A 1-guillotine cut partitions a window into two parts with closed and
open boundary segments.
but does not work directly for the general case of MIN-RP. Indeed, if a window W
contains nondegenerate holes, such a line may not exist. In this section, we modify
this idea to make it work for the general case. The new idea is to allow partial cuts
that do not use the whole line segment of the cut in the partition. With the help of
a technical lemma about 1-dark points, it is shown that a suitable partial cut always
exists so that its length can be charged evenly to its two sides. Thus, the proof of
Theorem 5.2 can be extended to the general case.
We ﬁrst introduce the concept of 1-guillotine cuts, which is the simplest type of
partial cuts. Consider an input rectilinear polygon R to the problem MIN-RP. We
may assume that R is a rectangle, for, otherwise, we can ﬁnd a rectangle that covers
R, and treat the areas between R and the rectangle as holes. We let H0 denote the
holes in R, and R0 = R \ H0. In a guillotine rectangular partition P of R, we use
a straight line L to cut a window W into two smaller windows, and add the line
segments in (L ∩W) ∩R0 to the partition.2 In a 1-guillotine rectangular partition,
we still use L to cut a window W into two smaller windows, but we do not use
the whole line segment L ∩W for the partition P . Instead, we select a subsegment
s of L ∩W and add segments in s ∩R0 into the partition P . We call such a cut
a 1-guillotine cut (more precisely, the line segment s is called a 1-guillotine cut).
Figure 5.5 shows a 1-guillotine cut.
Note that, after a 1-guillotine cut s, the window W becomes two smaller win-
dows, and the line segment L ∩W becomes a common boundary edge of the new
windows. This boundary edge contains a 1-guillotine cut segment s and two (pos-
sibly degenerate) line segments to the two sides of s. The 1-guillotine cut segment
is called a closed boundary segment of the new windows and the other two line
segments are called open boundary segments (see Figure 5.5, in which a solid line
2That is, the line segment L ∩W may be broken into smaller segments by holes in H0. We only add
those segments in R0 to the partition.
5.2 1-Guillotine Cut

172
indicates the closed segment, and the dashed lines indicate the open segments).3 In
the construction of the rectangular partition, the open boundary segments are only
temporary boundaries allowing recursive partitions and are not included in the ﬁnal
partition.
Thus, in each iteration, a 1-guillotine cut generates two new subproblems of the
following form:
Given a window W with holes and with possible open boundaries on
each of its four sides, use 1-guillotine cuts to construct a partition PW
with the following boundary conditions:
(1) The partition PW does not include any interior point of the open
boundary segments.
(2) The partition PW must contain the endpoints of the closed bound-
ary segments, unless the endpoint is a corner of the window W.
With these boundary conditions, can we still use dynamic programming to ﬁnd
the minimum-length 1-guillotine rectangular partition? The answer is yes. First, it
can be shown, similar to Lemma 5.1, that there exists a minimum-length 1-guillotine
rectangular partition Q such that every maximal line segment in Q contains a vertex
of the boundary. Therefore, if we consider only these types of 1-guillotinepartitions,
there are at most O(n) choices for a cut line at each iteration, and at most O(n4)
windows W to be considered.
In addition, we observe that each side of a window W created by a 1-guillotine
cut has O(n2) choices of a line segment as the closed boundary. Therefore, there
are O(n8) choices of the boundary conditions for a window W. Altogether, the
total number of possible subproblems to be examined in the dynamic programming
algorithm is O(n12). For each subproblem, there are O(n) choices of the cut line
and, for each cut line, O(n2) choices of the closed boundary segment. So, there
are O(n3) possible 1-guillotine cuts to be considered. It follows that the minimum-
length 1-guillotine rectangular partition can be computed by dynamic programming
in time O(n15).
Now, we estimate the performance ratio of the minimum 1-guillotine rectangular
partition as an approximation to MIN-RP. Similar to the proof of Theorem 5.2,
we are going to construct, from an optimal partition P ∗, a 1-guillotine partition P1
whose total length is no more than twice the total length of P ∗. To do this, we need
the following interesting observation about the relationship between vertical and
horizontal 1-dark points. (Here, 1-dark points only include those points in R0.)
Lemma 5.4 (Mitchell’s Lemma) Assume that P is a rectangular partition of an
instance R of MIN-RP, and W is a window in R. Let H (and V ) be the set of all
horizontal (and, respectively, vertical) 1-dark points with respect to partition P and
3Note that the closed boundary segment and the open boundary segments may include points in the
holes H0.
Guillotine Cut

173
window W. Then there exists either a horizontal cut line LH that does not contain
a line segment of P such that
length(LH ∩H) ≤length(LH ∩V ),
or a vertical cut line LV that does not contain a line segment of P such that
length(LV ∩V ) ≤length(LV ∩H).
Proof. Assume that the four corners of the window W are (a, a′), (a, b′), (b, b′), and
(b, a′). First, consider the case that the area of H is at least as large as the area of
V . Let Lu denote the vertical line {(x, y) | x = u}. Then the areas of H and V can
be represented by + b
a length(Lu ∩H)du and + b
a length(Lu ∩V )du, respectively.
Since
 b
a
length(Lu ∩H)du ≥
 b
a
length(Lu ∩V )du,
and since P has only ﬁnitely many line segments, there must exist a vertical line
Lu, with u ∈(a, b), that does not contain a line segment of P such that
length(Lu ∩H) ≥length(Lu ∩V ).
The line LV = Lu is what we need.
Similarly, for the case that the area of H is smaller than the area of V , we can
show that there exists a horizontal cut line LH that does not contain a line segment
of P such that
length(LH ∩H) ≤length(LH ∩V ).
□
This lemma suggests the following strategy to construct P1: At each iteration, we
make a 1-guillotine cut through a horizontal cut line LH (or a vertical cut line LV )
satisfying the property of Mitchell’s lemma, and let the 1-guillotine cut segment s
be the maximal line segment in LH ∩W (or, LV ∩W) whose two endpoints are
in vertical (or, respectively, horizontal) line segments in P ∗∩int(W). Note that
all points in s ∩R0, other than the two endpoints of s, are horizontal (or, vertical)
1-dark. Actually, s is the maximal segment in line LH (or, in LV ) with this property.
Suppose that we select a horizontal 1-guillotinecut s according to the above rule.
Then this cut adds some new edges s ∩R0 to P1 \ P ∗whose total length is at most
length(LH ∩H) and hence, by Mitchell’s lemma, at most length(LH ∩V ). This
means that the total length of the horizontal edges in P ∗∩int(W) that lie on each
side of LH is no less than the total length of s ∩R0. Therefore, we can charge the
cost of the new edges in s ∩R0 to these horizontal line segments in P ∗∩int(W)
that lie on the two sides of LH, with each line segment charged with at most one
half of its own length. The same property holds for a vertical 1-guillotine cut.
With this analysis, the performance ratio 2 can be established for the general case
of MIN-RP.
Theorem 5.5 The minimum-length 1-guillotine rectangular partition is a 2-appro-
ximation for MIN-RP.
5.2 1-Guillotine Cut

174
Proof. Assume that P ∗is a minimum-length rectangular partition of the input R (a
rectangle with holes). We will construct from P ∗a 1-guillotine rectangular partition
P1 by a sequence of 1-guillotine cuts such that the total length of edges in P1 \ P ∗
does not exceed the total length of P ∗. At each iteration, we are given a window W
with boundary conditions, and we need to ﬁnd a 1-guillotinecut to divide it into two
smaller windows. We select the 1-guillotine cut by the following rules.
(1) If P ∗∩int(W) = ∅, then do nothing.
(2) If P ∗∩int(W) contains a line segment s that is actually a 1-guillotine cut
with respect to P ∗(i.e., s ∩R0 = P ∗∩L ∩int(W), where L is the cut line
along s), then we perform the 1-guillotinecut s. [So the line L∩W becomes a
boundary of two new windows, with the segment s being the close boundary,
and the segments in (L ∩W) \ s being the open boundaries.] We did not add
any new edge to P1.
(3) If P ∗∩int(W) ̸= ∅but it does not contain a 1-guillotine cut with respect
to P ∗, then the area of the set H (or set V ) of horizontal (or, respectively,
vertical) 1-dark points in W with respect to P ∗must be greater than zero.
Thus, as discussed earlier, we can select the 1-guillotine cut s by Mitchell’s
lemma. More precisely, we select the cut line LH (or, LV ) with the property
of Mitchell’s lemma and let s be the maximal line segment in LH ∩int(W)
(or, LV ∩int(W)) whose two endpoints are in vertical (or, respectively, hor-
izontal) line segments of P ∗∩int(W). We perform a 1-guillotine cut s, and
add all segments in s ∩R0 to P1.
We observe that in the above procedure, each new subwindow created by a 1-
guillotine cut contains fewer line segments of P ∗than the current window. So, after
a ﬁnite number of steps, the subwindows W have no more line segments of P ∗in
int(W). This means that P ∗⊆P1. Furthermore, we note that since the endpoints of
a 1-guillotine cut must be in P ∗, the endpoints of each new edge in P1 \ P ∗must be
either in P ∗or on the boundary of R (including the boundary of holes). Therefore,
each edge in P1 \ P ∗must divide a rectangle created by the optimal partition P ∗
into two smaller rectangles. It follows that P1 is a 1-guillotine rectangular partition
of R0.
Now, we estimate the cost of the new edges in P1 \ P ∗. In case (2), we did not
add new edges to P1. For case (3), assume that we perform a vertical 1-guillotinecut
s along line LV on window W. From the earlier analysis, the total length of edges
in s ∩R0 is bounded by length(LV ∩H). We charge one half of the cost of these
new edges to the vertical line segments in P ∗∩int(W) lying immediately to the
right of LV , and the other half to the vertical line segments in P ∗∩int(W) lying
immediately to the left of LV (cf. Figure 5.6). Similar to the proof of Theorem 5.2,
we see that each edge in P ∗can be charged at most twice, and each time with cost
at most one half its own length. So the total length of the new edges in P1 \ P ∗is
no more than the total length of P ∗. This completes the proof of the theorem.
□
Corollary 5.6 The problem MIN-RP has a polynomial-time 2-approximation.
Guillotine Cut

175
B
D
C
A
 V
L
Figure 5.6:
The cost of 1-guillotine cut AC, excluding the points in the holes, is
charged to the vertical edges lying on the two sides of BD, excluding the points in
the hole.
5.3
m
The concept of 1-guillotine cut can be extended to m-guillotine cut for any m >
1. For any window W, an m-guillotine cut along a line L is a line segment s in
L ∩int(W), plus at most 2(m −1) points in L ∩int(W), with at most m −1
points in each side of s. (Note that this includes the case when the line segment
s is degenerated to a single point. Thus, any cut with at most 2m −1 points is
considered an m-guillotine cut.) After an m-guillotine cut, a window is divided into
two smaller windows. The common boundary of the two new windows contains a
line segment of closed boundary and up to m open boundary segments in each side
of the closed boundary segment, separated by the points of the m-guillotine cut.
Each new window, like that in the case of 1-guillotine cut, deﬁnes a subproblem of
m-guillotine rectangular partition:
Given a window W with holes and with up to m open boundary seg-
ments on each end of each of the four sides of W, use m-guillotine cuts
to construct a partition PW with the following boundary conditions:
(1) The partition PW does not include any interior point of the open
boundary segments.
(2) The partition PW must contain the endpoints of the open and
closed boundary segments, unless the endpoint is a corner of the
window W.
-Guillotine          Cut
5.3 m-Guillotine Cut
 

176
Figure 5.7:
An m-guillotine cut results in 2m open segments on each subprob-
lem’s boundary.
Figure 5.7 shows an m-guillotine cut. (In Figure 5.7, the short horizontal lines indi-
cate that the partition PW must contain line segments touching the endpoints of the
open and closed boundary segments.)
A rectangular partition is called an m-guillotine rectangular partition if it can be
realized by a sequence of m-guillotine cuts. Similar to the problem of 1-guillotine
rectangular partition, the problem of the minimum-length m-guillotine rectangular
partition can be computed by dynamic programming in time O(n10m+5). To see
this, we observe that, at each iteration of the dynamic programming algorithm, an
m-guillotine cut has at most O(n2m+1) choices: It has O(n) choices of the cut line
and, at each cut line, O(n2m) choices for the 2m endpoints of the open and closed
boundary segments. In addition, there are O(n8m+4) possible subproblems: There
are O(n4) possible windows, each having O(n8m) possible boundary conditions.
So the total running time is O(n10m+5).
To analyze the approximation to MIN-RP by m-guillotinecut, we need to extend
the notion of 1-dark points to m-dark points, for m > 1. Let R be an instance of
MIN-RP (i.e., a rectangle with holes), P a partition of R, and W a window of R. We
say a point z in W is a horizontal (or, vertical) m-dark point with respect to window
W and partition P if each horizontal (or, respectively, vertical) half-line starting
from z, but not including z, going in either direction meets at least m vertical (or,
respectively, horizontal) line segments in P ∩int(W).
By an argument similar to Mitchell’s lemma about 1-dark points, we can easily
establish the following property about m-dark points.
Guillotine Cut

177
Lemma 5.7 Assume that P is a rectangular partition of an instance R of MIN-RP,
and W is a window of R. Let m > 1, and Hm (and Vm) be the set of all horizontal
(and, respectively, vertical) m-dark points with respect to partition P and window
W. Then there exists either a horizontal cut line LH that does not contain a line
segment of P such that
length(LH ∩Hm) ≤length(LH ∩Vm),
or a vertical cut line LV that does not contain a line segment of P such that
length(LV ∩Vm) ≤length(LV ∩Hm).
Theorem 5.8 For any m > 1, the minimum-length m-guillotine rectangular parti-
tion is a (1 + 1/m)-approximation to MIN-RP.
Proof. Let P be a rectangular partition of an instance R of MIN-RP. Also, let
H0 be the set of the points in the holes in R and R0 = R \ H0. We will con-
struct an m-guillotine rectangular partition Pm with total length bounded by (1 +
1/m)length(P ).
The construction is similar to that of Theorem 5.5: At each iteration with a win-
dow W, if P ∩int(W) has an m-guillotinecut, then we make such a cut. Otherwise,
we choose a cut line according to Lemma 5.7. Without loss of generality, assume
that by Lemma 5.7, there is a vertical cut line L that does not contain a line segment
in P such that length(L ∩Vm) ≤length(L ∩Hm). Let s be the maximal segment
of L whose interior points are all vertical m-dark. Since all vertical m-dark points
in L are in s, the cut line L contains exactly m −1 points in P ∩int(W) on each
side of s. We select line segment s plus these points as the m-guillotine cut.
For such an m-guillotine cut along the cut line L, we note that the total length of
s ∩R0 is at most length(L ∩Vm) ≤length(L ∩Hm). This means that there are
line segments in L, of total length at least length(s∩R0), which have the following
property: There are at least m layers of vertical edges in P lying on each side of
these segments. So we can charge the length of s ∩R0 to the edges in the m layers
that are closest to line L, with each edge charged with at most 1/(2m) of its own
length. Furthermore, an edge in P can be charged at most twice: After an edge t
is charged by a cut s through line L, the line segment L ∩W becomes a boundary
edge of the two new windows. In the new window containing t, there are at most
m −1 layers of vertical edges of P between t and L, and so all points between L
and t are not horizontal m-dark in the new window, and t can no longer be charged
by any cut between t and L. It follows that the total length of m-guillotine cuts is
bounded by (1 + 1/m)length(P ).
□
Corollary 5.9 For any constant ε > 0, MIN-RP has a polynomial-time (1 + ε)-
approximation with running time nO(1/ε).
The signiﬁcance of the technique of guillotine cut stems not only from the PTAS
for the problem MIN-RP, but also from wide applications to other geometric opti-
mization problems. As another example, let us apply the technique of m-guillotine
5.3 m-Guillotine Cut
 

178
cut line locations
Figure 5.8:
Hanan grid (solid lines) and cut lines (dashed lines) in L.
cut to ﬁnd approximation algorithms for RSMT (RECTILINEAR STEINER MINI-
MUM TREE) introduced in Section 3.1. Let Q be the minimal rectangle covering all
n given points in the rectilinear plane. We will deﬁne the concept of m-guillotine
rectilinear Steiner trees, and show that the shortest m-guillotine rectilinear Steiner
tree provides a (1 + 1/m)-approximation to the rectilinear SMT.
First, we deﬁne the location of the cut lines. For a given set A of terminal points,
let the Hanan grid be the set of all horizontal and vertical lines each passing through
a point in A. The Hanan theorem states that for any given set A of points, there is
a rectilinear SMT T ∗lying on the Hanan grid. Thus, we can avoid the case of the
guillotine cut lines overlapping with the edges of tree T ∗by choosing the cut lines
off the lines of the Hanan grid. Moreover, in order to limit the number of possible
cut lines, we let L be the set of lines lying at the middle between two adjacent lines
of the Hanan grid (see Figure 5.8), and require that each m-guillotine cut must use
a line L ∈L as a cut line.
Let W be a window of Q. Then an m-guillotine cut of W is a cut along a line
L ∈L that consists of a line segment s and up to 2m−2 points, with at most m−1
of them on each side of s. In addition, it is required that all the cut points and the
endpoints of the cut segment lie on the Hanan grid. After each cut, W is divided into
two subwindows, the cut segment s is included as part of the Steiner tree, and L∩W
becomes a common boundary of the two new subwindows. In the problem MIN-RP,
the boundary conditionsof the new subwindows are conditionsabout at most 2m−2
cut points and the cut segment s. Here, our boundary conditions are those about at
most 2m −2 cut points plus one of the endpoints of s. (These points are called
crosspoints.) Furthermore, since the given points in A can be connected by edges
Guillotine Cut

179
passing through other windows, the boundary conditions here are more complicated
than those in the problem MIN-RP: Some of the crosspoints are required to be
connected to each other and some are not. Also, the cut segment s is not part of the
boundary condition; instead, we choose one of its endpoints as a crosspoint and add
a boundary condition on that point.
More precisely, each m-guillotine cut generates two subwindows and, for each
subwindow, we need to solve a subproblem of the following form:
(1) A window W is given, together with at most four closed boundary segments
and a set S of at most 8m −4 crosspoints on the boundary of W (with each
edge of W having at most one closed segment s, m −1 crosspoints on each
side of s, and an endpoint of s as a crosspoint). All input points of A in
window W lie in the interior of W and all crosspoints lie on the Hanan grid.
(2) A partition of the set S is given.
(3) The problem is to ﬁnd a rectilinear Steiner forest F that includes the closed
boundary segments of W and satisﬁes the following properties:
(a) All crosspoints in each part of S are connected by F;
(b) Two crosspoints in different parts of S are not connected by F;
(c) No two line segments of F cross each other (other than at a Steiner
point);
(d) The Steiner forest F does not contain any point on the boundary of W
other than those in set S; and
(e) If S is not empty, then each input point in A is connected by F to at
least one crosspoint; otherwise, all input points are connected by F.
Note that if there is no line L ∈L passing through the interior of a window W,
then an m-guillotine cut of W is not possible. We call such a window a minimal
window. Each minimal window W contains at most one input point in A, and each
side of W contains at most one crosspoint, all lying on the Hanan grid. For a minimal
window, a shortest rectilinear Steiner forest satisfying the boundary conditions is
easy to ﬁnd.
We say a rectilinear Steiner tree T is an m-guillotine rectilinear Steiner tree if it
can be obtained by m-guillotinecuts so that each edge of T is either an m-guillotine
cut segment or an edge in a minimal window.
Now, we show that the minimum m-guillotine rectilinear Steiner tree is a (1 +
1/m)-approximation to the rectilinear SMT. To do this, we will construct, from a
given rectilinear Steiner tree T lying on the Hanan grid, an m-guillotine rectilinear
Steiner tree Tm whose total edge length is at most (1+1/m) of the total edge length
of T.
We ﬁrst deﬁne the notion of horizontal and vertical m-dark points with respect
to T and a window W, similar to that for the problem MIN-RP. That is, a point z in
W is horizontal (or, vertical) m-dark if each of the two horizontal (or, respectively,
vertical) half-lines starting from z, but not including z, meets at least m vertical (or,
5.3 m-Guillotine Cut
 

180
respectively, horizontal) edges of T in int(W). Using a similar argument as that of
Mitchell’s lemma, we have the following property.
Lemma 5.10 Let A be a given set of points and T a rectilinear Steiner tree of A.
Also, let Q be the minimal rectangle covering points in A, and let W be a window
in Q. Then there exists either a horizontal cut line LH ∈L such that
length(LH ∩Hm) ≤length(LH ∩Vm)
or a vertical cut line LV ∈L such that
length(LV ∩Vm) ≤length(LV ∩Hm),
where Hm (Vm) is the set of all horizontal (vertical, respectively) m-dark points
with respect to T and W.
Proof. Let (a, a′), (a, b′), (b, b′), and (b, a′) be the four vertices of the window W.
First, assume that the area of Hm is greater than or equal to the area of Vm. Denote
Lu = {(x, y) | x = u}. Then the areas of Hm and Vm can be represented by
+ b
a length(Lu ∩Hm)du and
+ b
a length(Lu ∩Vm)du, respectively. Since
 b
a
length(Lu ∩Hm)du ≥
 b
a
length(Lu ∩Vm)du,
and since there are only ﬁnitely many u’s such that Lu passes through a point in A,
there must exist u ∈(a, b) such that Lu does not pass through any point in A and
length(Lu ∩Hm) ≥length(Lu ∩Vm).
That is, line Lu must lie between two lines L1 and L2 of the Hanan grid. Let Lv be
the line in L that lies between L1 and L2. Since T lies on the Hanan grid, the m-
dark segments on Lu and Lv have the same length: A point (u, w) in Lu is vertical
(or, horizontal) m-dark if and only if the point (v, w) is vertical (or, respectively,
horizontal) m-dark. So length(Lu ∩Hm) = length(Lv ∩Hm), and length(Lu ∩
Vm) = length(Lv ∩Vm). Therefore, Lv satisﬁes the required property.
The case when the area of Hm is smaller than the area of Vm is similar.
□
We now construct tree Tm by performing a sequence of m-guillotine cuts. At
each iteration of the construction, we are given a window W with boundary condi-
tions (i.e., a set S of at most 2m−1 crosspoints on each side of W, all on the Hanan
grid, and a partition of S) and a partially constructed Tm ∩W (which consists of up
to four closed boundary segments) satisfying the following conditions:
(i) All crosspoints in the same part of S are connected by (T ∪Tm) ∩W (note
that this includes the closed boundary segments of W);
(ii) Two crosspoints in different parts of S are not connected by (T ∪Tm) ∩W;
(iii) All input points in W are connected by (T ∪Tm) ∩W and are connected to
at least one crosspoint if S is nonempty.
Guillotine Cut

181
L
z
y
x
v
u
Figure 5.9:
The partition of crosspoints changes.
Initially, we let Tm = ∅. Since the window Q does not have boundary conditions,
Tm apparently satisﬁes conditions (i)–(iii) above. For a given window W, we ﬁnd
an m-guillotine cut by the following rules:
(1) If W is a minimal window, then do nothing.
(2) If W is not a minimal window and if there exists a cut line L ∈L that inter-
sects T at no more than 2m −1 points, then we cut W along line L, and put
these intersection points as the crosspoints. (In this case, we do not add any
line segment to Tm.) We partition the crosspoints of each new subwindow W1
according to (T ∪Tm)∩W1; that is, two crosspoints are in the same part if they
are connected by edges of (T ∪Tm)∩W1. Note that we need to repartition the
old crosspoints on the three other boundaries of the subwindow W1, since the
connection by T ∪Tm may change within W1. This is demonstrated in Fig-
ure 5.9: In the original window W, crosspoints x and y are in the same part.
After the m-guillotine cut along line L, they belong to two different parts in
the right subwindow (the new partition of the right subwindow is now {x, u},
{y, v}). Also note that u and v are in different parts in the right subwindow,
but they are in the same part when we consider the left subwindow.
(3) Otherwise, the area of set Hm (or, set Vm) of horizontal (or, respectively,
vertical) m-dark points in W must be greater than zero. We choose a cut
line L from L that satisﬁes the property of Lemma 5.10. Without loss of
generality, assume that L is a vertical line. We make an m-guillotine cut of
the window W along line L. This cut contains a segment s of all vertical m-
dark points and 2m −2 points in T, with m −1 points in each side of s.
We add the cut segment s to Tm. For each subwindow created by this cut, we
add the 2m −2 points of the cut as the crosspoints, and choose one endpoint
of s as an additional crosspoint (see Figure 5.10). Again, we partition the
crosspoints of each new subwindow W1 according to (T ∪Tm) ∩W1 so that
5.3 m-Guillotine Cut
 

182
Figure 5.10:
The new crosspoints.
two crosspoints are in the same part if and only if they are connected by edges
of (T ∪Tm) ∩W1, including the new edge s we just added to Tm.
By the above rule of partition, we see that T ∪Tm satisﬁes boundary conditions
(i)–(iii)in the new subwindows, because the boundary conditionsare simply deﬁned
by (T∪Tm)∩W. Therefore, the ﬁnal tree Tm = T∪Tm is an m-guillotinerectilinear
Steiner tree since the initial window Q has no boundary conditions.4
Finally, we verify that the length of Tm is at most (1 + 1/m) of the length of
T. We observe that in case (3), each cut line L is chosen to satisfy the property
of Lemma 5.10. Assume, without loss of generality, that L is a vertical line. Then
the total length of the segments of horizontal m-dark points in L ∩W is at least
length(s). So the cost of the cut segment s can be evenly charged to the 2m closest
layers of vertical line segments in T ∩W that lie on the two sides of L. By the same
argument as in Theorem 5.8, we can see that each edge in T can be charged at most
twice, each time at most 1/(2m) of its own length. So the total length of Tm \ T is
at most 1/m of the length of T. We just proved the following result:
Theorem 5.11 For any m > 0, the minimum m-guillotine rectilinear Steiner tree
is a (1 + 1/m)-approximation to RSMT.
Next, we verify that, similar to the case of the m-guillotine rectangular partition,
the minimum m-guillotinerectilinear Steiner tree can be computed by dynamic pro-
gramming in polynomial time as follows:
First, at each iteration, there are O(n) possible positions to choose the cut line,
since the cut line must belong to L. For each cut line, there are at most 2m −1
crosspoints on each cut plus a line segment s, with all crosspoints and the endpoints
of s lying on the Hanan grid. Therefore, there are totally O(n2m) possible cuts to
consider in each iteration.
4Strictly speaking, the ﬁnal set Tm = T ∪Tm is not necessarily a tree since it might contain loops.
This problem can be easily resolved by removing some redundant edges from the ﬁnal Tm.
Guillotine Cut

183
Next, we estimate how many subproblems may occur in the computation of the
dynamic programming algorithm. Each subproblem, as deﬁned earlier, is given by
a window with four boundary lines, up to 8m −4 crosspoints on the boundaries,
and a partition of these crosspoints. Because all boundaries of a window must be
on lines in L, and all crosspoints must lie on the Hanan grid, we see that there are
O(n4) possible windows, and for each window there are O(n8m−4) possible sets S
of crosspoints. For each set S of 8m −4 crosspoints, there are 2O(m log m) different
partitions of S. Note, however, that not every partition of S has a feasible solution
satisfying boundary conditions (a)–(d) (for instance, in the right subwindow of Fig-
ure 5.9, the partition {{x, v}, {u, y}} is not feasible). We will prove in Lemma 5.12
that the number of partitions of S that have a feasible solution satisfying boundary
conditions (a)–(d) is 2O(m). Therefore, the total number of subproblems that may
occur in the dynamic programming algorithm is O(n4 · n8m−4 · 2O(m)) = nO(m).
Moreover, Lemma 5.12 also shows that we can actually generate all feasible parti-
tions of S in time 2O(m). It follows that the dynamic programming algorithm runs
in time nO(m) · 2O(m) = nO(m).
Lemma 5.12 Let W be a window and S the set of crosspoints on the boundary of
W. Then the number of partitions of S that have a feasible solution satisfying the
boundary conditions (a)–(d) is 2O(m).
Proof. Break the boundary of window W at a point and spread it out into a straight
line. Then the problem is reduced to counting the number Nk of partitions of a set
S of k (k ≤8m −4) points on a horizontal line such that there exists a forest
above the line satisfying conditions (a)–(c). Let us denote the k points on the line by
p1, p2, . . ., pk, from left to right. When point p1 is connected to no other point, the
number of required partitions is Nk−1. When p1 is connected to at least one other
point and pi is the leftmost point other than p1 that is connected to p1, the number
of required partitions is Ni−2Nk−i+1. (We deﬁne N0 = 1.) Therefore,
Nk = Nk−1 +
k

i=2
Ni−2Nk−i+1 =
k−1

i=0
NiNk−1−i.
(5.1)
Let f(x) be the generating function of Nk; that is, f(x) = ∞
k=0 Nkxk. Then we
have
f(x)2 =
∞

k=0
k

i=0
NiNk−ixk =
∞

k=0
Nk+1xk.
Hence, xf(x)2 = f(x) −1. Thus,
f(x) = 1 ± √1 −4x
2x
.
Since limx→0 f(x) = 1 and limx→0(1 + √1 −4x)/(2x) = ∞, we get
f(x) = 1 −√1 −4x
2x
= −
∞

k=1
1/2
k
(−4x)k
2x
.
5.3 m-Guillotine Cut
 

184
That is, f(x) is the generating function of the well-known Catalan numbers, and
Nk = −
 1/2
k + 1
(−4)k+1
2
= (1/2)(1 −1/2)(2 −1/2) · · ·(k −1/2)
(k + 1)!
· 22k+1 = 2O(k).
In addition, we remark that we obtained the recurrence (5.1) by a simple case
analysis, which may be used as a recursive algorithm to generate all feasible parti-
tions of S.
□
Corollary 5.13 For any ε > 0, there exists a (1+ε)-approximation for the problem
RSMT with running time nO(1/ε).
5.4
Portals
In the last two sections, we have studied the technique of m-guillotine cut. We note
that in the design of an approximation algorithm by guillotine cut, we often face
two conﬂicting requirements: On the one hand, after a guillotine cut, we need to
allow the two subproblems resulting from the cut to communicate through the com-
mon boundary so that we can combine the solutions of the two subproblems into a
good approximate solution to the current window. On the other hand, the commu-
nication points (i.e., the crosspoints) must be limited so that the number of possible
boundary conditions is polynomially bounded and, hence, an algorithm of dynamic
programming can ﬁnd the optimal solution (of the guillotine-cut restricted problem)
in polynomial time. In the m-guillotine cut technique, this problem is resolved by
allowing up to 2m −2 crosspoints on the cut line for the communication between
the two subproblems. Note, however, that the running time of the dynamic program-
ming algorithm, though polynomially bounded, is very high even with reasonably
small values of m.
In this section, we introduce a different technique to deal with these conﬂicting
requirements. In this technique, we allow up to O(log n) crosspoints on each cut
line, but the locations of the crosspoints are predetermined. That is, we deﬁne a set
of p = O(log n) points on a cut line that evenly divide the cut segment (called p-
portals) and require that the connections between the two new windows resulting
from the cut can only go through these portals. Since the number of portals on a
cut line is bounded by O(logn), the number of possible boundary conditions is still
polynomially bounded and so the optimal guillotine-cut restricted solution can be
found by dynamic programming in polynomial time.
To be more speciﬁc, let us consider the problem RSMT again. Let P be the set
of n input terminals. Initially, we use a minimal square R to cover all points in P ,
and divide the square R, by a grid of lines, into g × g cells of equal size, where
g = ⌈(4.5)n/ε⌉for a given 0 < ε < 1. Assume the length of each side of R is L.
Then each cell is a square of size (L/g) × (L/g).
Guillotine Cut

5.4 Portals
185
Figure 5.11:
Moving each terminal to a center.
For each terminal u ∈P , let u′ be the center of the cell containing point u.
Denote P ′ = {u′ | u ∈P }. We ﬁrst show that in order to get a PTAS for P , it
sufﬁces to construct a PTAS for P ′ (see Figure 5.11).
Lemma 5.14 For any ε > 0, if there is a polynomial-time (1 + ε)-approximation
for RSMT on P ′, then there exists a polynomial-time (1 + 2ε)-approximation for
RSMT on P .
Proof. Let smt(P ) denote the length of the rectilinear SMT on P , and mst(P )
denote the length of the rectilinear minimum spanning tree on P . Recall that the
Steiner ratio (i.e., the maximum ratio of smt(Q) to mst(Q) on the same input points
Q) in the rectilinear plane is equal to 2/3. Since R is the minimal square covering
the input points, the length L of each side of R is no greater than mst(P ), and hence
no greater than (3/2)smt(P ). In addition, we note that to move each point u ∈P
to u′ ∈P ′, we increase the length of the rectilinear Steiner tree by a value at most
L/g. Thus, we have |smt(P ) −smt(P ′)| ≤nL/g.
Let Tε(P ′) be a polynomial-time (1 + ε)-approximation for the rectilinear SMT
on P ′. That is,
length(Tε(P ′)) ≤(1 + ε)smt(P ′).
We can construct a tree T interconnecting points in P from Tε(P ′) by connecting
each point u′ in P ′ to its corresponding point u in P . Then we have
length(T) ≤length(Tε(P ′)) + nL
g
≤length(Tε(P ′)) + nL
g
≤(1 + ε)smt(P ′) + nL
g

186
1
3
1
3
cut line
Figure 5.12:
A (1/3, 2/3)-restricted guillotine cut.
≤(1 + ε)

smt(P ) + nL
g

+ nL
g
= (1 + ε)smt(P ) + (2 + ε)nL
g
≤

1 + ε + 3n
2g (2 + ε)

smt(P ) ≤(1 + 2ε)smt(P ),
since g ≥(4.5)n/ε.
□
Based on this lemma, we will work on set P ′ instead of P . That is, we will
assume that all terminals lie at the centers of the cells (and we still use the name P
for the set of these terminals).
Next, we apply guillotine cuts to partition the rectangle R, step by step, into
smaller rectangles (called windows) until each rectangle contains at most one termi-
nal (called a minimal window). In order to limit the depth of the cutting process, we
will only choose cut lines that lie close to the middle of the window. That is, at each
iteration, for a given window W, we choose a grid line parallel to the shorter edge
of W which cuts through the middle 1/3 of the longer edge of W. We call such a
cut a (1/3, 2/3)-restricted guillotine cut (see Figure 5.12), and a partition made by
such cuts a (1/3, 2/3)-partition.
A (1/3, 2/3)-partition has a natural binary tree structure. The root of the tree is
the initial window R. For each window W, a guillotine cut divides W into two new
windows, which are the two children of W in the tree (see Figure 5.13). This binary
tree has an important property: it has depth O(logn).
Lemma 5.15 The binary tree structure of a (1/3, 2/3)-partition of a window of n
terminal points has O(log n) levels.
Guillotine Cut

5.4 Portals
187
Figure 5.13:
Binary tree structure of a (1/3, 2/3)-partition.
Figure 5.14:
Portals.
Proof. At each node of the binary tree, a window W is divided into two smaller
windows, each having area at most 2/3 of W. Thus, a window at the ith level, for
i ≥0, has area at most L2(2/3)i. Since each cut runs along a grid line, the window
of a leaf node has area at least (L/g)2. Therefore, the level s of a leaf node satisﬁes
L2(2/3)s ≥(L/g)2. That is, s = O(logg) = O(logn).
□
To limit the number of crosspoints at each cut line, we ﬁx the number and loca-
tions of portals on the line where the edges of the Steiner tree can cross the cut line.
For an integer p > 0, we deﬁne p-portals on a cut line to be the p points on the line
that evenly divide the cut line into p+1 segments. We have selected the locations of
the portals independently of the input terminals. Thus, a portal only serves as a po-
tential crosspoint, and may not actually be used in the approximate solution. Thus,
in the computation of the approximation algorithm, we need to identify some portals
as active portals, which, in the new windows resulting from a cut, must connect to
the output Steiner tree; that is, active portals are the real crosspoints.
With p-portals, a subproblem in the guillotine cut algorithm for RSMT has the
following form:

188
(1) A window W is given, with all terminal points lying in the interior of W.
(2) A set S of portals is given on the boundaries of W. A subset of portals is
identiﬁed as active portals, and a partition of active portals on the boundary is
given.
(3) The problem is to ﬁnd a rectilinear Steiner forest F in W with the following
properties:
(a) All active portals in each part of the partition are connected by F;
(b) Two active portals in different parts are not connected by F;
(c) All other points on the boundary, including inactive portals, are not con-
nected by F to any other portals or terminals;
(d) No two line segments of F cross each other except at a Steiner point;
and
(e) Each terminal is connected to at least one active portal unless no active
portals exist on the boundary, in which case, all terminals are connected
to each other.
We say a rectilinear Steiner tree T is a (1/3, 2/3)-guillotine(p-portal) rectilinear
Steiner tree if there exists a (1/3, 2/3)-partition of the initial rectangle such that
each edge of T intersects a cut line only through a p-portal.
Lemma 5.16 The minimum-length(1/3, 2/3)-guillotinep-portal rectilinear Steiner
tree of a given set P of terminal points can be computed in time n112O(p).
Proof. Based on the binary tree structure of the (1/3, 2/3)-partition, we can employ
dynamic programming to ﬁnd the minimum (1/3, 2/3)-guillotinerectilinear Steiner
tree. To estimate the running time of this dynamic programming algorithm, we ﬁrst
note that each boundary of a window must be a grid line, and so there are O(n4)
possible windows. Each window W has four sides, and one of them is the cutting
line of the parent window of W and contains p portals. However, each of the three
other boundary sides may contain fewer than p portals, as it may be a subsegment
of a longer cutting segment from a cut on a nonparent ancestor window (see Figure
5.15). Note that there are O(n2) potential ancestor windows that may have made a
cut along a side line of W. The locations of the portals on this side line resulting
from a cut by different ancestor windows are different. Therefore, the number of
possible sets of portals for each of these three sides is O(n2). In addition, we do not
know which of the four sides is the cut segment of the parent window of W. Thus,
the total number of sets of portal locations on the boundary of W is 4 · O(n6) =
O(n6). After the locations of the portals are ﬁxed, we need to choose a subset of
active portals and a partition of this subset. There are 2O(p) choices for the subset
of active portals and, for each subset, there are, as proved in Lemma 5.12, 2O(p)
possible choices of partitions that satisfy boundary conditions (a)–(d). Therefore,
the total number of possible subproblems is n102O(p).
Guillotine Cut

5.4 Portals
189
Figure 5.15:
Portals deﬁned from different cuts.
Moreover, in each iteration of the dynamic programming algorithm, the number
of possible cuts is O(n), since the cut must be made along a grid line. For each
cut, we need to choose a set of active portals on the cutting segment and, for each
subwindow created by this cut, a partition of the active portals of the subwindow.
Therefore, each iteration takes time n2O(p) and the overall running time of the dy-
namic programming algorithm is n112O(p).
□
Now, let us estimate the performance ratio of the minimum (1/3, 2/3)-guillotine
rectilinear Steiner tree as an approximation to the rectilinear SMT. To do so, con-
sider a rectilinear SMT T ∗, and we are going to modify T ∗to meet our restriction.
That is, we will construct a (1/3, 2/3)-partition and will move all crosspoints at cut
segments to portals.
More precisely, the (1/3, 2/3)-partition is constructed in the following way: At
each step, among all possible cut lines that cut through the window W in the mid-
dle third of the longer side of W, choose the one with the minimum number of
intersection points with T ∗. Then, set up the p-portals on the cut segment. For each
crosspoint of T ∗on the cut line, move it to the nearest portal by adding a detour
path (see Figure 5.16), and deﬁne these cross portals as the active portals. At last,
as in the case of the construction of the m-guillotine rectilinear Steiner trees (de-
scribed in the proof of Theorem 5.11), repartition the set of active portals depending
on whether two active portals are connected by T ∗.
The following lemma shows that moving all crosspoints to the portals does not
cost much.
Lemma 5.17 Let i ≥0. The length increase that resulted from moving all cross-
points to portals in all windows at level i of a (1/3, 2/3)-partition is at most
(6/p) · length(T ∗).
Proof. Let W be a window at level i. Suppose a longer edge of W has length a and
a shorter edge has length b (with 0 < b ≤a). Without loss of generality, assume that

190
Figure 5.16:
Moving crosspoints to portals.
the longer edges of W are horizontal line segments. Then the guillotine cut on W
is a vertical (1/3, 2/3)-restricted cut of W; that is, it is a vertical line that intersects
each longer edge of W in the middle third of that edge. Furthermore, this line is
chosen to have, among all such vertical (1/3, 2/3)-restricted cuts, the minimum
number of intersections (i.e., crosspoints) with tree T ∗. Suppose that the chosen
cut has c crosspoints with T ∗. Then for every vertical line that lies in the middle
third of W, it has at least c crosspoints with T ∗. This means that the total length of
horizontal line segments in TW = T ∗∩W is at least ca/3. It follows that the total
length of TW is at least ca/3. Moving each crosspoint to its nearest portal requires
adding two edges to T ∗, each of length at most b/(p + 1). [For the middle p −2
portals, each additional edge is only of length at most b/(2(p + 1)).] So moving all
c crosspoints to their respective nearest portals increases the length of the tree by at
most
2cb
(p + 1) ≤
2ca
(p + 1) ≤6
p · ca
3
≤6
p · length(TW ).
We note that the union of TW over all windows at level i of the (1/3, 2/3)-partition
is just T ∗, and so

W∈level i
length(TW ) = length(T ∗).
Thus, the total length increase resulting from moving crosspoints to portals on all
windows at level i is at most (6/p) · length(T ∗).
□
Theorem 5.18 The minimum (1/3, 2/3)-guillotine rectilinear Steiner tree using p-
portals, for some p = O((log n)/ε), is a (1 + ε)-approximation for RSMT. More-
over, this tree can be computed in time nO(1/ε).
Proof. Suppose that the binary tree structure of a (1/3, 2/3)-partition has d logn
levels for some constant d > 0. Then the total length increase that resulted from
moving crosspoints to portals on all windows of the partition is at most
Guillotine Cut

5.5 Quadtree Partition and Patching
191
d logn · 6
p · length(T ∗) ≤ε · length(T ∗)
if we choose p = ⌈6d log n/ε⌉. So the (1/3, 2/3)-guillotine rectilinear Steiner tree
obtained from T ∗as described above has length at most (1 + ε)length(T ∗).
Also, note that for p = ⌈6d log n/ε⌉, the running time of the dynamic program-
ming algorithm is n112O(p) = n11+O(⌈6d/ε⌉) = nO(1/ε).
□
5.5
Quadtree Partition and Patching
We have introduced two techniques of limiting the number of crosspoints on the
guillotine cut lines, namely, the m-guillotine cut technique and the portal technique.
In this section, we show how to combine the two techniques to further improve the
guillotine cut approximation algorithms. Let us ﬁrst compare the two techniques in
different applications.
First, consider geometric problems in the three- or higher-dimensional space.
When we perform guillotine cuts on such a problem, a cut line needs to be replaced
by a cut plane or a cut hyperplane. As a consequence, the number of portals on the
cut plane or hyperplane would increase from O(log n/ε) to O((log n/ε)2) or even
higher. With so many possible crosspoints, the dynamic programming algorithm for
ﬁnding the optimal guillotine cut–restricted solution may no longer run in polyno-
mial time. On the other hand, the m-guillotine cut allows at most 2m crosspoints
in each dimension. Since m is a constant with respect to n, the polynomial-time
bound for the corresponding dynamic programming algorithm is preserved in the
higher-dimensional spaces.
For some other problems, moving crosspoints to predetermined portals is difﬁcult
or even impossible. For such problems, the portal techniques cannot be applied at
all. This includes the problem MIN-RP and the following problems, for which the
m-guillotine cut technique works well:
RECTILINEAR STEINER ARBORESCENCE: Given n terminals in the
ﬁrst quadrant of the rectilinear plane, ﬁnd the minimum-length directed
tree rooted at the origin, connecting to all terminals and consisting of
only horizontal arcs oriented from left to right and vertical arcs oriented
from bottom to top.
SYMMETRIC RECTILINEAR STEINER ARBORESCENCE: Given n ter-
minals in the ﬁrst and second quadrants of the rectilinear plane, ﬁnd
the minimum-length directed tree rooted at the origin, connecting to all
terminals and consisting of only horizontal arcs (in either orientation)
and vertical arcs oriented from bottom to top.
MINIMUM CONVEX PARTITION: Given a polygon with polygonal
holes, partition it into convex areas with the minimum total length of
cut lines.

192
On the other hand, the m-guillotine cut may be difﬁcult to apply to some prob-
lems, while the portal technique works well on them. This includes the following
problems:
EUCLIDEAN k-MEDIANS: Given a set P of n points in the Euclidean
plane, ﬁnd k medians in the plane such that the sum of the distances
from each terminal to the nearest median is minimized.
EUCLIDEAN FACILITY LOCATION: Given n points x1, . . ., xn in the
Euclidean plane and, for each i = 1, . . ., n, a cost ci for opening a
facility at xi, ﬁnd a subset F of {1, 2, . . ., n} that minimizes

i∈F
ci +
n

i=1
min
j∈F d(xi, xj),
where d(xi, xj) is the Euclidean distance between xi and xj.
EUCLIDEAN GRADE STEINER TREE: Given a sequence of point sets
P1 ⊂P2 ⊂· · · ⊂Pm in the Euclidean plane and weights c1 > c2 >
· · · > cm, ﬁnd a network G = (V, E) of the minimum total weight such
that G contains a Steiner tree Ti for every Pi, where the total weight of
G equals 
e∈E length(e) · maxe∈Ti ci.
For the problems to which both techniques can be applied, such as RSMT, it is
natural to ask whether the two techniques can be combined to yield a better approx-
imation algorithm. As both techniques already produce PTASs, we mainly look for
a combined method that can reduce the running time for the dynamic programming
algorithm. A general idea is as follows: We may ﬁrst use the portal technique to
reduce the number of possible locations of crosspoints to O((log n)/ε) and then
choose 2m portals to form a m-guillotine cut (with m = O(1/ε)). In this way, the
dynamic programming algorithm for ﬁnding the best such partition would run in
time nc(log n)O(1/ε), where c is a constant independent of ε. However, when we
try to implement this idea, we might encounter troubles in the analysis of the per-
formance ratio. More precisely, when we modify the optimal solution to meet our
restriction, we ﬁrst need to construct a relevant partition and, in particular, need to
know how to select the cut lines for the construction. With the portal technique, we
want to select the cut lines in a way that minimizes the number of crosspoints on the
cut lines. On the other hand, in the m-guillotine cut technique, we need to select cut
lines that satisfy the inequality in Mitchell’s lemma. In general, these two selection
criteria are often incompatible and would prevent us from ﬁnding a good combined
partition.
How do we overcome this problem? An idea is to move our attention away from
ﬁnding the local optimal guillotine cut at each step, but instead to work on the entire
adaptive partition directly. To illustrate this point, let us deﬁne a family of adaptive
partitions called quadtree partitions: Initially, we are given a square window that
covers all the input points. In each subsequent step, if a square window contains
Guillotine Cut

5.5 Quadtree Partition and Patching
193
Figure 5.17:
Idea of m-guillotine cut with portals.
Figure 5.18:
Quadtree partition and P (a, b) covering Q.
more than one input point, then partition it into four smaller square windows of
equal size (see Figure 5.18). This quadtree partition has a correspoinding quaternary
tree structure, in which each node v is associated with a square window W(v), and
each internal node v has four children each associated with a smaller subsquare of
W(v) (see Figure 5.19).
With quadtree partitions, the cut lines are predetermined, and so there might be
a large number of crosspoints on the cut segments. However, we can reduce the
number of crosspoints by performing m-guillotine cuts on these cut segments. Fur-
thermore, by the shifting technique introduced in Chapter 4, we can limit the extra
cost of the m-guillotine cuts without employing Mitchell’s lemma. In the following,
we illustrate how this technique works on the problem RSMT.
Let Q be a square that covers all input terminals. By Lemma 5.14, we may divide
Q into a 2q × 2q grid, where (4.5)n/ε < 2q = O(n/ε), and assume that every
terminal point lies at the center of a cell. We may further rescale the grid and assume

194
Figure 5.19:
The tree structure of a quadtree partition.
that Q = {(x, y) | 0 ≤x ≤2q, 0 ≤y ≤2q}. With this assumption, a rectilinear
Steiner tree of the input points has the following nice property:
Lemma 5.19 Assume that the input terminals lie at the center of the cells of a grid
of size 2q × 2q as described above, and that T is a rectilinear Steiner tree over
these points with the property that every Steiner point also lies at the center of a
cell. Then the total number of crosspoints of T over the vertical grid lines equals
the total length of the horizontal segments in T, and the total number of crosspoints
of T over the horizontal grid lines equals the total length of the vertical segments in
T.
Suppose an RSMT T ∗lies on the Hanan grid. Then every Steiner point of T ∗
must lie at a grid point of the Hanan grid and hence is at the center of a cell. So
Lemma 5.19 holds for T ∗.
For the quadtree partition, we need to modify the deﬁnition of p-portals as fol-
lows: In addition to the p-portals deﬁned before, we include the endpoints of a cut
segment as two new p-portals. Note that the endpoints of a cut segment s may also
be an endpoint of a neighboring cut segment. We assume that they are different
copies of the same point. In particular, when we cut a square window W into four
square subwindows, we create four portals at the center of W, to be used as portals
for the four cut segments (see Figure 5.20). We call these new portals the endpoint
portals and the original portals the interior portals. Similarly, if a crosspoint on a
cut segment locates at one of the endpoints of the cut segment, then we call it an
endpoint crosspoint; otherwise, we call it an interior crosspoint.
Now, let p = O((log n)/ε) and m = O(1/ε) be two ﬁxed parameters. For each
(a, b), with 0 ≤a, b < 2q, we deﬁne a quadtree partition P (a, b) as follows: Use
(a, b) as the center to draw an initial square Q with edge length twice of that of Q
(i.e., 2q+1). It is obvious that Q covers Q. From this initial square Q, construct a
quadtree partition as described earlier, and place p + 2 portals on each cut segment
(see Figures 5.18 and 5.20).
Guillotine Cut

5.5 Quadtree Partition and Patching
195
Figure 5.20:
Quadtree cuts and portals on them, where a ◦indicates an endpoint
crosspoint.
We say a rectilinear Steiner tree T is a P (a, b)-restricted rectilinear Steiner tree
(with parameters (p, m)) if, for the quadtree partition P (a, b),
(1) Every edge of T crosses a cut segment at a portal, and
(2) There exist at most m interior crosspoints on every cut segment, plus possibly
one or two crosspoints at the endpoints of the cut segment.
Remark. In condition (1) above, we allow an edge of T to cross a cut segment at its
endpoint portal. This edge may lie on the boundary of the window, but it is treated
as an edge in the interior of the window, and it can only be connected to edges in
other windows through portals.
Lemma 5.20 The minimum P (a, b)-restricted rectilinear Steiner tree with param-
eters (p, m) can be computed in time npO(m)2O(m).
Proof. Based on the tree structure of P (a, b), we employ the method of dynamic pro-
gramming to compute the minimum P (a, b)-restricted rectilinear Steiner tree. Each
subproblem of this dynamic programming algorithm can be described as follows:
(1) A square window W is given, with all points lying in the interior of W.
(2) A set S of portals is given on the boundary of W. A subset of portals, at most
m of them on the interior of each boundary, is identiﬁed as active portals, and
a partition of the active portals is given.

196
(3) The problem is to ﬁnd a P (a, b)-restricted rectilinear Steiner forest F of the
minimum total length that has the following properties:
(a) All active portals in each part of the partition are connected by F;
(b) Two active portals in different parts are not connected by F;
(c) All other points on the boundary, including inactive portals, are not con-
nected to each other or to terminals;
(d) No two line segments of F cross each other except at a Steiner point;
and
(e) Each terminal in W is connected by F to at least one active portal unless
no active portal exists in the boundary, in which case all terminals are
connected by F.
Note that in the tree structure of a quadtree partition, there are exactly n leaves
associated with a nonempty square. Since each internal node associated with a
nonempty square must have at least two children with nonempty squares, there are at
most n−1 internal nodes that are associated with nonempty squares. Therefore, the
total number of nonempty squares is at most 2n −1. For each nonempty square, the
number of possible sets of active portals is O(p4m). For each set of active portals,
the number of possible partitions of the active portals is, by Lemma 5.12, 2O(m).
Therefore, the total number of subproblems is O(np4m)2O(m).
Moreover, each iteration in the dynamic programming algorithm can be com-
puted in time pO(m)2O(m) since, for each cut segment, we need to consider all
possible choices of the set of active portals and, for each set of active portals, all
possible choices of the partition of this set. Putting them together, the dynamic pro-
gramming algorithm runs in time npO(m)2O(m).
□
Choose p = O(q/ε) = O((log n)/ε) and m = O(1/ε), and let Ta denote the
minimum P (a, a)-restricted rectilinear Steiner tree with parameters (p, m). Then,
by Lemma 5.20, Ta can be computed in time n((log n)/ε)O(1/ε). As a result, the
shortest tree among T0, T1, . . ., T2q−1, denoted by Ta∗, can be computed in time
n2(log n)O(1/ε).
Next, let us estimate the performance ratio of Ta∗as an approximation to the
rectilinear SMT. To do so, consider a rectilinear SMT T ∗lying on the Hanan grid
so that the conclusion of Lemma 5.19 holds for T ∗. For each quadtree partition
P (a, a), we will modify T ∗to satisfy the P (a, a) restriction and estimate the cost
of the modiﬁcation.
The modiﬁcation consists of two parts. In the ﬁrst part, we move each crosspoint
to the nearest portal in the boundary. In the second part, we perform a patching
procedure on cut segments to reduce the number of crosspoints such that each cut
segment contains at most m interior crosspoints.
Let P be the family of partitions P (a, a), for a = 0, 1, . . ., 2q −1. We ﬁrst
estimate the total cost of the modiﬁcation in the ﬁrst part over all partitions in P,
instead of a single partition P (a, a). That is, we calculate the total length increase
resulting from moving all crosspoints to their corresponding nearest p-portals over
all partitions in P.
Guillotine Cut

5.5 Quadtree Partition and Patching
197
Lemma 5.21 Let c1(P, T) denote the total length increase resulting from moving
each crosspoint of a rectilinear Steiner tree T to the nearest p-portal in a partition
P . Then, for the rectilinear SMT T ∗,
1
2q

0≤a<2q
c1(P (a, a), T ∗) ≤
q + 1
2(p + 1) · length(T ∗).
Proof. Consider the tree structure of the partition P (a, a). As usual, we say a vertex
v in this tree (or its associated square W(v)) is at level i, for some i ≥0, if the path
from the root to v has length i. In particular, the root is at level 0. A cut segment of
P (a, a) is also said to be at level i if it is one of the four cut segments of a level-i
square W that cuts W into four squares at level i + 1. Note that all cut segments on
a grid line must be at the same level. Thus, we may say that a grid line is at level i
if all cut segments on it are at level i.
In the following, we let H (and V) denote the set of all horizontal (and, re-
spectively, vertical) grid lines in partitions in P. Consider an arbitrary vertical
grid line ℓ∈V. When we shift the partition from P (0, 0) to P (1, 1), . . ., and to
P (2q −1, 2q −1), the level of line ℓchanges along. In particular, in the family of
2q quadtree partitions in P = {P (a, a) | 0 ≤a < 2q}, ℓis a level-0 cut line for
exactly one partition in P: the partition whose center vertex lies on ℓ. In addition,
ℓis also a level-1 cut line for one partition: the partition whose center vertex has
distance 2q−1 from ℓ. In general, for each 1 ≤i ≤q −1, ℓis a level-i cut line for
a partition if the center of the partition has distance (2j + 1)2q−i from ℓfor some
j ≥0. It is easy to see that there are exactly 2i−1 such partitions, and hence ℓis a
level-i cut line for 2i−1 partitions.
Let T ∗
H (and T ∗
V ) denote the set of all horizontal (and, respectively, vertical) line
segments in tree T ∗. Also, for any ℓ∈V, let n(ℓ, T ∗) denote the number of cross-
points of T ∗on line ℓ(note that this value is independent of the partitions). Note
that a level-i cut segment in a partition P (a, a) has edge length 2q−i. Thus, for a
partition P (a, a) relative to which ℓis at level i, moving a crosspoint on ℓto a near-
est p-portal on P (a, a) increases the length of T ∗by at most 2q−i/(p + 1) [note
that any point on ℓhas distance ≤1/(2(p + 1)) to the nearest portal]. Therefore,
by Lemma 5.19, the total length increase for moving crosspoints at vertical cuts to
portals, over all partitions in P, is at most

ℓ∈V
n(ℓ, T ∗) ·

2q +
q−1

i=1
2i−1 · 2q−i

1
p + 1 = length(T ∗
H) · 2q−1(q + 1)
p + 1
.
Similarly, the total length increase resulting from moving crosspoints at all horizon-
tal cut segments to portals, over all partitions in P, is at most
length(T ∗
V ) · 2q−1(q + 1)
p + 1
.
Putting them together, we get

198
Figure 5.21:
Patching.
1
2q

0≤a<2q
c1(P (a, a), T ∗) ≤
q + 1
2(p + 1) · length(T ∗).
□
Next, we study how to reduce the number of crosspoints so that each cut segment
contains at most m interior crosspoints. An idea motivated from the m-guillotinecut
is to add a guillotine cut segment to the Steiner tree and leave out at most m interior
crosspoints not covered by this new segment. To simplify the operation, we will
add the whole cut segment to the Steiner tree and keep only one crosspoint. More
precisely, the patching operation on a cut segment s is as follows: If s contains
more than m interior crosspoints, then we add two copies of the cut segment s to
the Steiner tree (one for each subwindow resulting from the cut by s), and keep one
single (interior or endpoint) crosspoint on s (see Figure 5.21).
As in the analysis of the m-guillotine cut approximation, we need to keep the
total cost of patching bounded by ε · length(T ∗). This bound is, however, difﬁcult
to get. For instance, for a cut line s at level 0, the patching operation on s would in-
crease the length of T ∗by 2q−1, which by itself might be greater than ε·length(T ∗)
already. Thus, we need to modify the patching operation to avoid such expensive
patchings. Intuitively, for a cut segment s having more than m interior crosspoints,
we may choose a subsegment of s with a high density of crosspoints and only patch
this subsegment so that the extra length added is proportional to the number of
crosspoints reduced. The following procedure is an implementation of this idea.
First, let us introduce a new notation for line segments: Write [x, y] to denote the
line segment with endpoints x and y. For a line segment [x, y], denote by x(h) the
point in [x, y] with distance h from x. For instance, if [x, y] is a line segment at level
i (and hence of length 2q−i), then x = x(0), y = x(2q−i), and the middle point on
[x, y] is x(2q−i−1).
Iterated Patching Procedure (on a cut segment [x, y] at level i):
For k ←0 to q −i do
for j ←0 to 2q−i−k −1 do
if [x(j2k), x((j + 1)2k)] has more than m interior crosspoints
then patch the line segment [x(j2k), x((j + 1)2k)].
Guillotine Cut

5.5 Quadtree Partition and Patching
199
It is worth pointing out that although the patching operation is similar to the
m-guillotine cut, it is not used in the dynamic programming algorithm to ﬁnd the
minimum P (a, b)-restricted rectilinear Steiner tree. Instead, we only use it as a tool
for the analysis of the minimum P (a, b)-restricted rectilinear Steiner tree as an ap-
proximation to the rectilinear SMT. Since we did not use patching in the dynamic
programming algorithm, we do not need to include the running time of the Iterated
Patching Procedure in the construction of Ta∗. On the other hand, to keep the new
tree T a P (a, a)-restricted rectilinear Steiner tree, we need to make two copies of
the patching edge, one for each subwindow, so that T does not violate condition (c)
given in the proof of Lemma 5.20.
Now, to reduce the number of crosspoints on a cut segment, we execute, for
each partition P (a, a) in P, the Iterated Patching Procedure on every cut segment
[x, y] at every level in P (a, a), in the order of the cut segments being generated
by the quadtree partition, starting at level 0 and then moving to higher-level cut
segments. The next lemma estimates the total cost of this reduction procedure over
all partitions in P.
Lemma 5.22 Let c2(P, T) denote the total length increase resulting from executing
the Iterated Patching Procedure on all cut segments in partition P , with respect to
the crosspoints of a rectilinear Steiner tree T. Then, for the rectilinear SMT T ∗,
1
2q

1≤a<2q
c2(P (a, a), T ∗) ≤2
m · length(T ∗).
Proof. First, we deﬁne sets H, V, T ∗
H, and T ∗
V as in the proof of Lemma 5.21.
Let ℓbe a vertical cut line in V. Consider the procedure of Iterated Patching
applied to ℓ, as if ℓis a level-0 cut line (and hence consists of two level-0 cut seg-
ments). This procedure patches subsegments of ℓfrom shorter segments to longer
segments. For any k, with 0 ≤k ≤q, let g(k, ℓ) be the number of length-2k sub-
segments patched in this procedure, or, equivalently, the number of patches done
by the procedure in the kth iteration. Note that this number g(k, ℓ) depends only
on the crosspoints of T ∗with line ℓand is independent of the quadtree partitions.
Indeed, for any partition P (a, a) relative to which ℓis at level i ≤q −k, the total
number of patches done in the kth iteration of the Iterated Patching Procedure, on
all cut segments in ℓ, is equal to g(k, ℓ). (Note that when we patch a line segment,
the new patch segment may intersect a cut segment s at a higher level and generate
new crosspoints on s. However, all these new crosspoints locate at the endpoints of
the cut segment s, and so they do not affect later patching procedures, as it only
considers the interior crosspoints and ignores the endpoint crosspoints.)
Thus, if ℓis at level i relative to a partition P (a, a), then the total length in-
crease resulting from executing the Iterated Patching Procedure on cut segments of
partition P (a, a) that lie in ℓis at most
q−i

k=0
g(k, ℓ) · 2k+1.

200
(Note that each cut segment is doubled for patching.)
Now, consider the Iterated Patching Procedure applied to grid line ℓover all
quadtree partitions P (a, a) in P. Recall from the proof of Lemma 5.21 that ℓis at
level 0 for one partition in P, and, for each i ≥1, ℓis at level i for 2i−1 partitions in
P. Therefore, the total length increase resulting from executing the Iterated Patching
Procedure on all segments in ℓ, over all partitions in P, is at most
q

k=0
g(k, ℓ) · 2k+1 +
q

i=1
2i−1
q−i

k=0
g(k, ℓ) · 2k+1
=
q

k=0
g(k, ℓ)

2k+1 +
q−k

i=1
2k+i

=
q

k=0
g(k, ℓ) · 2q+1.
Note that each patching of a cut segment reduces at least m crosspoints of T ∗with
ℓ. Thus,
q

k=0
g(k, ℓ) ≤n(ℓ, T ∗)
m
,
where n(ℓ, T ∗) is the number of crosspoints of T ∗on ℓ. It follows that the total
length increase that resulted from Iterated Patching on ℓ, over all partitions in P, is
at most
2q+1 · n(ℓ, T ∗)
m
.
Therefore, by Lemma 5.19,
1
2q

1≤a<2q
c2(P (a, a), T ∗) ≤

ℓ∈H∪V
2 · n(ℓ, T ∗)
m
= 2
m · length(T ∗).
□
In summary, for each quadtree partition P (a, a) in P, we modify the rectilinear
SMT T ∗as follows: We ﬁrst perform the Iterated Patching procedure on all cut
segments of P (a, a) to reduce the number of crosspoints of T ∗to no more than m
on each cut segment. Call the resulting tree T ′
a. Then we move all crosspoints of T ′
a
on the grid lines of P (a, a) to their corresponding nearest p-portals. Let T ′′
a denote
the resulting Steiner tree. It is clear that T ′′
a is a P (a, a)-restricted rectilinear Steiner
tree.
Lemma 5.23 With parameters p ≥2(q + 1)/ε and m ≥8/ε, at least one half of
the trees T ′′
a , for 0 ≤a < 2q, have
length(T ′′
a ) ≤(1 + ε)length(T ∗).
Proof. It is clear that
length(T ′′
a ) ≤length(T ∗) + c2(P (a, a), T ∗) + c1(P (a, a), T ′
a)).
Guillotine Cut

5.6 Two-Stage Portals
201
Furthermore, we note that each crosspoint coming from an edge in T ′
a \ T ∗is
an endpoint portal, and need not be moved. Therefore, when we move the cross-
points of T ′
a on grid lines of P (a, a) to their corresponding nearest portals, only
original crosspoints of T ∗on a grid line of P (a, a) need to be moved. Thus,
c1(P (a, a), T ′
a) ≤c1(P (a, a), T ∗). It follows that
length(T ′′
a ) ≤length(T ∗) + c2(P (a, a), T ∗) + c1(P (a, a), T ∗).
For ﬁxed parameters p and m such that p ≥2(q + 1)/ε and m ≥8/ε, we have,
by Lemmas 5.22 and 5.21,
1
2q

1≤a<2q
length(T ′′
a ) ≤

1 + 2
m +
q + 1
2(p + 1)

· length(T ∗)
≤

1 + ε
2

· length(T ∗).
Therefore, it holds, for at least one half of a ∈{0, 1, . . ., 2q−1}, that length(T ′′
a ) ≤
(1 + ε) · length(T ∗).
□
Theorem 5.24 There exists a (1 + ε)-approximation to the problem RSMT that
can be computed in time n2(log n)O(1/ε). Moreover, with probability1/2, a (1+ε)-
approximation for RSMT can be computed in time n(log n)O(1/ε).
Proof. The ﬁrst half of the corollary is a direct consequence of Lemma 5.23. For the
second half, we can choose a random quadtree partition P (a, a) and compute the
minimum P (a, a)-restricted rectilinear Steiner tree Ta.
□
5.6
Two-Stage Portals
In the last section, we combined the portal and m-guillotine cut techniques to get a
PTAS for the problem RSMT in time n2(log n)O(1/ε). We now introduce yet another
idea, called two-stage portals, to further improve the running time of the PTAS.
Let [x, y] be a cut segment in a quadtree partition.For two integers p1, p2 > 0, we
can set up two-stage (p1, p2)-portals on [x, y] as follows: We ﬁrst set up a set {z0 =
x, z1, . . ., zp1, zp1+1 = y} of p1-portals on [x, y]. Next, we choose two points x′, y′
from {z0, z1, . . ., zp1+1}, and set up a set {w0 = x′, w1, . . ., wp2, wp2+1 = y′}
of p2-portals on [x′, y′]. We call {w0, w1, . . ., wp2+1} a set of two-stage (p1, p2)-
portals on segment [x, y] (see Figure 5.22). We note that for each segment [x, y],
there are O(p2
1) sets of (p1, p2)-portals on [x, y].
To apply two-stage portals to the approximation of RSMT, we ﬁrst modify the
notion of P (a, b)-restricted rectilinear Steiner trees accordingly. That is, a rectilin-
ear Steiner tree T is a P (a, b)-restricted rectilinear Steiner tree (with parameters
(p1, p2, m)) if it satisﬁes the following conditions:
(1) Each crosspoint of T on a cut segment s of P (a, b) belongs to a set of (p1, p2)-
portals on s or is an endpoint of s, and

202
p
−portals
)
,
( 1p
−portals
1
p2
Figure 5.22:
Two-stage portals.
(2) There exist at most m interior crosspoints of T on each cut segment of
P (a, b),
Lemma 5.25 With parameters p1, p2, m, the minimum P (a, b)-restricted rectilin-
ear Steiner tree can be computed in time np10
1 p5m
2 2O(m). In particular, if we choose
2q = O(n/ε), p1 = O(q/ε) = O((log n)/ε), m = O(1/ε), and p2 = O(m2), then
the minimum P (a, b)-restricted rectilinear Steiner tree can be computed in time
n(logn)10(1/ε)O(1/ε).
Proof. Let T(a, b) be the minimum P (a, b)-restricted rectilinear Steiner tree. We
construct T(a, b) by dynamic programming based on the tree structure of the
quadtree partition P (a, b) as in Lemma 5.20. In particular, a subproblem in the
dynamic programming algorithm is the same as that in Lemma 5.20, except that the
active portals are m points in a (p1, p2)-portal, plus possibly one or two endpoints of
the cut segments. We note that for a cut segment s of the partition P (a, b), there are
O(p2
1pm
2 ) possible choices of active portals: First, there are O(p2
1) sets of (p1, p2)-
portals on s; and, second, there are O(pm
2 ) ways to choose m active interior portals
out of p2 + 2 locations. Thus, following the argument in the proof of Lemma 5.20,
there are O(np8
1p4m
2 ) · 2O(m) possible subproblems in the dynamic programming
algorithm. Namely, there are at most 2n nonempty squares in the tree structure of
the quadtree partition; each square has four sides, with each side having O(p2
1pm
2 )
possible choices of active portals; and, ﬁnally, there are 2O(m) ways to partition
active portals into connected parts.
Moreover, using the same argument, we can see that each iteration takes time
O(p2
1pm
2 )·2O(m). So, the total running time of the dynamic programming algorithm
is np10
1 p5m
2 2O(m).
□
To estimate the average performance of Ta = T(a, a) for a ∈{0, 1, . . ., 2q −1},
we consider a rectilinear SMT T ∗lying in the Hanan grid and modify T ∗to satisfy
Guillotine Cut

5.6 Two-Stage Portals
203
the P (a, a)-restriction. We ﬁrst perform the Iterated Patching Procedure on each
cut segment of the partition P (a, a) so that each cut segment contains at most m
interior crosspoints. We call the resulting tree T ′
a. Then, for each cut segment s
of the partition, we choose a subsegment [x, y] of s with the minimum length that
satisﬁes the following properties:
(i) Each of x, y is a p1-portal on s;
(ii) All interior crosspoints of T ′
a on s lie in [x, y].
We set up the two-stage (p1, p2)-portals on the segment [x, y], and move all inte-
rior crosspoints of T ′
a to their corresponding nearest portals on this set of (p1, p2)-
portals. We call the resulting tree T ′′
a . The following lemma estimates the length
increase resulted from this modiﬁcation.
Lemma 5.26 With parameters m ≥8/ε, p2 = 2m2, and p1 ≥2(q + 1), at least
one half of the trees T ′′
a , for 0 ≤a < 2q −1, satisfy
length(T ′′
a ) ≤(1 + ε)length(T ∗).
Proof. First, we follow the notation of the proof of Lemma 5.22 and denote the total
length increase that resulted from Iterated Patching of T ∗on partition P (a, a) by
c2(P (a, a), T ∗). Also, let c3(P (a, a), T ′
a) be the total length of the subsegments
[x, y] of cut segments s of P (a, a) that we chose to form the (p1, p2)-portals on s.
Then the length increase from moving the crosspoints of T ′
a to their corresponding
nearest two-stage (p1, p2)-portals is bounded by (m/(p2 + 1))c3(P (a, a), T ′
a).
Now, we claim that
1
2q

0≤a<2q
c3(P (a, a), T ′
a) ≤4 · length(T ∗).
With this claim, the total length increase that resulted from modifying T ∗into T ′′
a ,
over all partitions P (a, a) in P, is
1
2q

0≤a<2q

c2 (P (a, a), T ∗) +
m
p2 + 1 · c3(P (a, a), T ′
a)

≤
 2
m +
4m
p2 + 1

· length(T ∗) ≤ε
2 · length(T ∗).
Therefore, for at least one half of a ∈{0, 1, . . ., 2q −1}, we must have length(T ′′
a )
≤(1 + ε)length(T ∗).
It remains to prove our claim.
For each cut segment s = [x, y] of P (a, a) of length 2q−i, let
subseg(s) = {[x(j2k), x((j + 1)2k)] | 0 ≤k ≤q −i, 0 ≤j < 2q−i−k}.
For each cut segment s of P (a, a) that contains at least one interior crosspoint of
T ′
a, let [u(s), v(s)] be the shortest segment in subseg(s) that contains all interior
crosspoints of T ′
a on s.

204
We note that if a cut segment s contains only one interior crosspoint, then the
length of [u(s), v(s)] must be 1. Thus, the total length L1 of [u(s), v(s)] over all cut
segments s containing exactly one crosspoint is bounded by the number of interior
crosspoints of T ′
a on P (a, a). Since each interior crosspoint of T ′
a on P (a, a) is
also a crosspoint of T ∗on P (a, a), the length L1 is, by Lemma 5.19, bounded by
length(T ∗).
Next, consider a cut segment s of P (a, a) that contains at least two interior cross-
points of T ′
a. Suppose we apply the Iterated Patching Procedure to segment s, with
parameter m = 1 and crosspoints of T ′
a; then [u(s), v(s)] would be the segment
where we apply the (last) patching operation. Moreover, we note that the interior
crosspoints of T ′
a are obtained from the crosspoints of T ∗through an Iterated Patch-
ing Procedure. Therefore, if we apply the Iterated Patching Procedure to segment s,
with parameter m = 1 and crosspoints of T ∗, the last patching operation must be
on the same segment [u(s), v(s)]. Thus, the total length of [u(s), v(s)] over all cut
segments s that contain at least two crosspoints is bounded by c2(P (a, a), T ∗) with
respect to the parameter m = 1. [We write c(1)
2 (P (a, a), T ∗) to emphasize that the
parameter m used in this bound is m = 1.]
Finally, let us move u(s) and v(s) to two p1-portals u′(s), v′(s) of s sat-
isfying [u(s), v(s)] ⊆[u′(s), v′(s)]. This will increase the total length by at
most 4c1(P(a, a), T ′
a). (Note that the distance between u′(s) and u(s) is at most
1/(p1 +1), but might be greater than 1/(2(p1 +1)).) Again, since all interior cross-
points of T ′
a are also crosspoints of T ∗, this length increase is actually bounded by
4c1(P (a, a), T ∗).
Now, from Lemmas 5.21 and 5.22, we know that the total length increase
c3(P (a, a), T ′
a), over all partitions P (a, a) in P, can be bounded as follows:
1
2q

0≤a<2q
c3(P (a, a), T ′
a)
≤length(T ∗) + 1
2q

0≤a<2q
c(1)
2 (P (a, a), T ∗) + 1
2q

0≤a<2q
4c1(P (a, a), T ∗)
≤length(T ∗) + 2

1 + q + 1
p1 + 1

· length(T ∗) ≤4 · length(T ∗).
This completes the proof of the claim and, hence, the lemma.
□
The following theorem follows immediately from Lemmas 5.25 and 5.26.
Theorem 5.27 For any ε > 0, there exists a (1 + ε)-approximation for the problem
RSMT that runs in time n2(logn)10(1/ε)O(1/ε). Moreover, with probability 1/2, a
(1 + ε)-approximation for RSMT can be computed in time n(log n)10(1/ε)O(1/ε).
So far, we have used portals together with patching to reduce the running time of
the PTASs for the problem RSMT substantially. We note, however, that the cost
of moving crosspoints to portals depends on the depth of the adaptive partition
(cf. Theorem 5.18 and Lemma 5.21), and so it is hard to further reduce the run-
ning time of the PTAS using the portal technique. Thus, for further improvement
Guillotine Cut

Exercises
205
over the running time of the PTAS, we must give up on portals and look for other
techniques. One promising direction is to combine the patching technique with the
graph-theoretic notions of spanners and banyans. For the problem RSMT, it has
been shown, using spanners, banyans, and patching, that, for any ε > 0, there exist
a randomized (1 + ε)-approximation running in time O(n logn) and a determinis-
tic (1 + ε)-approximation running in time O(n2 logn). The proof is, unfortunately,
too involved to be included here. The interested reader is referred to Rao and Smith
[1998] for the details.
Exercises
5.1 Show that, for any given rectilinear polygon without holes, the minimum-
length rectangular partition can be found by dynamic programming in time O(n4).
5.2 A stair is a rectilinear polygon of the shape as shown in Figure 5.23.
(a) Show that the minimum-length rectangular partition for stairs can be com-
puted by dynamic programming in time O(n2).
(b) Can you improve the running time of the above algorithm to O(n log n)?
Figure 5.23:
A stair.
5.3 Consider the problem MIN-RP1. Prove, by constructing a counterexample,
that the upper bound for the ratio of the minimum-length guillotine rectangular par-
tition to the minimum-length rectangular partition cannot be smaller than 3/2.
5.4 Consider a rectangular partition P of a rectilinear polygon, possibly with
rectilinear holes. Let projx(P ) denote the total length of segments on a horizontal
line covered by vertical projection of the partition P . Let guil(P ) be the set of
guillotine rectangular partitions obtained from adding some segments to P . Show,
by induction on the number k of segments in P , that there exists a partition PG ∈
guil(P ) such that
length(PG) ≤2 · length(P ) −projx(P ).

206
5.5 Show that for any rectilinear SMT of n points in a rectangle, and any m ≥1,
there exists a constant c > 0 such that either (i) there exists a horizontal line L, not
passing through any input point, such that
length(L ∩Hm) + c · length(L ∩H1) ≤length(L ∩Vm) + c · length(L ∩V1),
or (ii) there exists a vertical line L, not passing any input point, such that
length(L ∩Hm) + c · length(L ∩H1) ≥length(L ∩Vm) + c · length(L ∩V1),
where Hm (Vm) is the set of all horizontal (vertical, respectively) m-dark points.
5.6 For each of the following problems, use both techniques of m-guillotine cut
and quadtree partition with patching to design PTASs for it:
(a) RECTILINEAR STEINER ARBORESCENCE;
(b) SYMMETRIC RECTILINEAR STEINER ARBORESCENCE;
(c) MINIMUM-LENGTH CONVEX PARTITION.
In general, are the two techniques equivalent? If not, show a counterexample.
5.7 For each of the following problems, use both techniques of (1/3, 2/3)-
partition with portals and quadtree partition with portals to design PTASs for it:
(a) EUCLIDEAN k-MEDIANS;
(b) EUCLIDEAN FACILITY LOCATION;
(c) EUCLIDEAN GRADE STEINER TREE.
In general, are the two techniques equivalent? If not, show a counterexample.
5.8 For each of the following problems, use the technique of quadtree partition
with patching and portals to design a PTAS for it:
(a) ESMT;
(b) EUCLIDEAN-TSP: Given n points in the Euclidean plane, ﬁnd a minimum-
length tour passing through all n points;
(c) EUCLIDEAN k-SMT: Given n terminals in the Euclidean plane and an integer
1 ≤k ≤n, ﬁnd a shortest tree interconnecting at least k terminals.
5.9 Consider the following idea of combining the techniques of m-guillotine
cut and portals: We ﬁrst make a 1-guillotine cut, and put portals on the cut segment.
Next, perform an m-guillotinecut with these portals. Apply this idea to design a (1+
ε)-approximation for the problems RSMT, ESMT and EUCLIDEAN-TSP. Show
that these approximation algorithms can be made to run in time nc(logn)O(1/ε) for
some constant c > 0.
Guillotine Cut

Exercises
207
5.10 Design a PTAS for the following problem:
3-DIMENSIONAL RSMT: Given a set of n terminals in the three-
dimensional rectilinear space, ﬁnd a minimum-length tree interconnect-
ing all terminals.
5.11 Show that in any rooted tree with each internal vertex having at least two
children, the number of internal vertices is less than the number of leaves.
5.12 Consider the following variation of quadtree partition, called binary-tree
partition: On an input square, at each step 2i −1, partition each square into two
rectangles of equal size; and at each step 2i, partitioneach rectangle into two squares
of equal size. Show that we can use the binary-tree partition to replace the quadtree
partition to get results in Sections 5.5 and 5.6.
5.13 Consider a grid on a square in the Euclidean plane with each cell a unit
square. Show that for any line segment AB with the two endpoints located at the
centers of two grid cells, the number of crosspoints of AB on the grid lines is
bounded by
√
2 · length(AB).
5.14 For each of the following problems, apply the technique of two-stage por-
tals to design a PTAS for it such that a (1 + ε)-approximation can be found in time
n(logn)10(1/ε)O(1/ε):
(a) ESMT;
(b) EUCLIDEAN-TSP.
5.15 Design a PTAS for the following problem:
INTERCONNECTING HIGHWAYS: Given a set of disjoint line segments
in the Euclidean plane, ﬁnd a shortest tree interconnecting them.
5.16 Consider the following problem:
RSMT WITH OBSTRUCTIONS: Given a set of terminals in the rectilin-
ear plane with the presence of rectilinear obstructions, ﬁnd a shortest
tree interconnecting all terminals without passing through the obstruc-
tions.
Explain why neither the m-guillotine cut nor portal technique works for this prob-
lem. Could you ﬁnd a new technique to construct a PTAS for it?
5.17 Consider the following variation of EUCLIDEAN-TSP:
Given n disjoint regions in the Euclidean plane, ﬁnd a shortest tour
visiting each region at least once.
Could you ﬁnd a PTAS for this problem? If you cannot, what are the difﬁculties
when you try to apply the techniques of m-guillotinecut and portals to this problem?

208
5.18 Consider the following problem:
Given a set of n sites in the Euclidean plane, a special site r, and a
positive number L > 0, ﬁnd a tour starting from r and returning to r
with total length at most L that maximizes the number of visited sites.
Show that there is a 2-approximation for this problem.
5.19 Extend the m-guillotine partition technique to the approximation to polyg-
onal partition problems, in which a partition segment is not necessarily rectilinear.
In particular, show that for any polygonal partition with edge set E of total length
L, there exists an m-guillotine partition of length at most
L +
√
2
m

L −ζ(m)(E)
2

,
whose edge set contains E, where ζ(m)(E) is the sum of the lengths of the four sets
of one-sided m-dark points on the subsegments of E. (We say a point z is one-sided
m-dark with respect to set E, in the direction D ∈{left, right, above, below}, if
the half-line starting from z, not including z, going in the direction D meets at least
m line segments of E. In a set of one-sided m-dark points, all points in this set are
one-sided m-dark in the same direction D.)
Historical Notes
Using the technique of adaptive partition to design approximation algorithms was
ﬁrst introduced by Du, Pan, and Shing [1986] in the study of MIN-RP. The problem
MIN-RP was ﬁrst proposed by Lingas et al. [1982], who showed that the general
case of MIN-RP is NP-hard, but its hole-free subproblem can be solved in time
O(n4). A na¨ıve idea of designing approximation algorithms for the general case of
MIN-RP is to use a forest connecting all holes to the boundary and then solve the re-
sulting hole-free case. With this idea, Lingas [1983] gave the ﬁrst constant-bounded
approximation to MIN-RP, with the performance ratio 41. Du [1986] improved the
algorithm and obtained an approximation with performance ratio 9. Meanwhile,
Levcopoulos [1986] presented a faster approximation based on the greedy strategy,
but with a larger performance ratio.
Motivated by the work of Du et al. [1988] on the application of dynamic pro-
gramming to ﬁnding optimal routing trees, Du, Pan, and Shing [1986] initiated the
idea of guillotine cut for the problem MIN-RP. They showed that the minimum-
length guillotine rectangular partition can be computed in time O(n5) with dynamic
programming and, as an approximation to MIN-RP, it has a performance ratio at
most 2 for the special case of MIN-RP1. Du, Hsu, and Xu [1987] gave a different
proof for this result. They also extended the idea of guillotine cuts to the problem
of MINIMUM CONVEX PARTITION. The special case of MIN-RP1 was shown to
be NP-hard by Gonzalez and Zheng [1985]. Gonzalez and Zheng [1989] improved
Guillotine Cut

Historical Notes
209
the performance ratio 2 proved in Theorem 5.2 to 1.75 with an ad hoc case-by-case
analysis.
Arora [1996] is a milestone in the study of adaptive partition. He used this
technique to design PTASs for many geometric optimization problems, including
the problems EUCLIDEAN-TSP, ESMT, RSMT, DEGREE-RESTRICTED SMT, k-
TSP, and k-SMT. His approximation algorithms typically run in time nO(1/ε) for
the performance ratio 1 + ε. In the meantime, an independent line of study on m-
guillotine cuts had been made by Mitchell. Inspired by the work of Du, Pan, and
Shing [1986], Mitchell [1996a] introduced the notion of 1-guillotine cut. Mitchell
[1996b] (later published in a journal version by Mitchell [1999]) pointed out that
results similar to those of Arora [1996] could be obtained by a minor modiﬁcation
of his work in Mitchell [1996a] (a journal version was later published as Mitchell et
al. [1999]). A year later, Arora [1997] used quadtree partition and the technique of
patching, which was inspired by the idea of m-guillotine cut, to reduce the running
time of the PTASs from nO(1/ε) to n3(logn)O(1/ε). Soon later, Mitchell [1997]
also improved his algorithms with the idea of two-stage portals. In Arora [1997],
a family of O((n/ε)2) quadtree partitions was employed to establish the average
performance of the algorithm. Du [2001] improved it to use only O(n/ε) quadtree
partitions, and reduced the running time of derandomization.
Interesting applications of the above techniques have been found in STEINER
ARBORESCENCE by Lu and Ruan [2000], SYMMETRIC STEINER ARBORESCENCE
by Cheng, DasGupta, and Lu [2001], INTERCONNECTING HIGHWAYS by Cheng,
Kim, and Lu [2001], EUCLIDEAN k-MEDIANS and EUCLIDEAN FACILITY LOCA-
TION by Arora, Raghavan, and Rao [1998], and Arkin et al. [1998]. Rao and Smith
[1998] applied spanners and banyans to geometric approximation problems. Arora,
Grigni et al. [1998] extended these ideas to problems in planar graphs. The applica-
tion of adaptive partition to graph problems is a rich area with potential for further
research.
It is an open problem whether there are (1 + ε)-approximations that run in time
nc(log n)O(1/ε) for the problems MIN-RP, RECTILINEAR STEINER ARBORES-
CENCE, SYMMETRIC RECTILINEAR STEINER ARBORESCENCE, EUCLIDEAN k-
MEDIANS, and EUCLIDEAN FACILITY LOCATION.

  

6
Relaxation
Your mind will answer most questions
if you learn to relax and wait for the answer.
—William S. Burroughs
An optimization problem asks for a solution from a given feasible domain that pro-
vides the optimal value of a given objective function. The technique of relaxation
is, contrary to the technique of restriction, to relax some constraints on the feasible
solutions and, hence, enlarge the feasible domain so that an optimal or a good ap-
proximate solution to the relaxed version of the problem can be found in polynomial
time. This optimal or approximate solution to the relaxed version is not necessarily
feasible for the original problem, and we may need to modify it to get a feasible so-
lution to the original input. This modiﬁcation step often requires special tricks and
is an important part of the relaxation technique.
In this chapter, we introduce various ideas about relaxation. Then, in Chapters 7,
8, and 9, we will study how to relax combinatorial optimization problems into linear
programs or semideﬁnite programs, and how to modify their solutions to feasible
solutions of the original problems.
6.1
Directed Hamiltonian Cycles and Superstrings
Depending on the nature of the problem, there are many ways of relaxing the con-
straints of an optimization problem. Let us ﬁrst look at some simple examples about
ﬁnding Hamiltonian circuits.
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9_6, 
© Springer Science+Business Media, LLC 2012 
211

212
Relaxation
Example 6.1 Recall the problem TSP (TRAVELING SALESMAN PROBLEM) stud-
ied in Section 1.6. The feasible domain of an instance G of TSP consists of all
Hamiltonian circuits of the input graph G. Note that a Hamiltonian circuit of G
must be a spanning graph of G. As the minimum spanning tree of a graph is well
known to be computable in polynomial time, we may relax the feasible domain of
TSP to contain all spanning graphs, and try to use the minimum spanning tree as an
approximation to the minimum Hamiltonian circuit. Note, however, that the mini-
mum spanning tree is not a Hamiltonian circuit. Thus, we need to modify it to get a
feasible solution for the original problem.
This approach was taken in Algorithms 1.G and 1.H. In Algorithm 1.G, the mod-
iﬁcation consists of three steps: We ﬁrst double the edges of the minimum spanning
tree so that every vertex of the tree has an even degree. Then we convert this tree
into an Euler tour. Finally, we take a shortcut through the Euler tour to get a Hamil-
tonian circuit and use it as the approximate solution. When the input graph satisﬁes
the triangle inequality, this algorithm gives us a 2-approximation to TSP.
In Algorithm 1.H (Christoﬁdes’s approximation), an additional idea is used for
the ﬁrst step: Instead of doubling every edge of the minimum spanning tree, we only
add a perfect matching on vertices of odd degrees to get a subgraph in which each
vertex has an even degree. This new idea improves the performance ratio to 3/2
when the input satisﬁes the triangle inequality.
For the problem of DIRECTED TSP (or, MINIMUM DIRECTED HAMILTONIAN
CIRCUIT), it seems rather difﬁcult to modify a directed spanning tree (also called
an arborescence spanning tree) to a Hamiltonian circuit. So this approach does not
work well for DIRECTED TSP.
□
Example 6.2 Consider the problems MAX-HC (MAXIMUM HAMILTONIAN CIR-
CUIT) and MAX-DHC (MAXIMUM DIRECTED HAMILTONIAN CIRCUIT). The
feasible domain of either problem is, again, the set of all Hamiltonian circuits of
the input graph. Note that the objective function of these two problems is the length
of a Hamiltonian circuit, which can be written as the sum of the lengths of two
matchings if the number of vertices of the input graph is even, or the sum of the
lengths of three matchings if the number of vertices is odd. So we may relax the
feasible domain to include all pairs or triples of (independent) matchings, and in
turn further relax it to simply the set of all matchings. That is, we ﬁnd a maximum
matching, and then modify it to a Hamiltonian circuit and use it as an approximation
to MAX-HC or MAX-DHC. Note that the total length of the maximum matching
of a graph G is at least one third that of the maximum Hamiltonian circuit of G.
Therefore, connecting the maximum matching into a Hamiltonian circuit results in
an approximation to MAX-HC or MAX-DHC with performance ratio 3. For the
problem MAX-DHC, this approximation is as good as the greedy algorithm.
The above idea of relaxation can also be applied to the problems MAX-HP
(MAXIMUM HAMILTONIAN PATH) and MAX-DHP (MAXIMUM DIRECTED
HAMILTONIAN PATH). Since a Hamiltonian path can always be written as the sum
of two matchings, the maximum matching provides an approximation to these two
problems with performance ratio 2.

6.1 DHC and Superstrings
213
Theorem 6.3 For each of the problems MAX-HP and MAX-DHP, there exists a
polynomial-time 2-approximation.
For the directed case (i.e., MAX-DHP), this result is better than that of the greedy
algorithm (cf. Theorems 2.5 and 2.16).
□
Example 6.4 For Hamiltonian circuits, there is another possible relaxation. Recall
that an assignment is a maximal matching in a bipartite graph. For a complete di-
rected graph G = (V, E), we may call a collection of disjoint cycles that cover all
vertices of G an assignment. To see this, deﬁne a bipartite graph H = (V, V ′, E′),
where V ′ = {x′ | x ∈V } and E′ = {{x, y′} | {x, y} ∈E}. Then a maximal
matching of H is a maximal set M of disjoint edges in H. Since M is maximal and
edges in M are disjoint, this matching deﬁnes a one-to-one function from V to V ′.
When we identify V ′ with V , this matching becomes a collection of disjoint cycles
that cover every vertex in G.
It is clear that a Hamiltonian circuit in a directed graph is an assignment. Since
maximum matching is polynomial-timecomputable, the above observation suggests
that we relax the problem of ﬁnding directed Hamiltonian circuits to ﬁnding assign-
ments.
For the problems MAX-DHC, this idea leads to the following approximation
algorithm:
Algorithm 6.A (Approximation Algorithm for MAX-DHC)
Input: A complete directed graph G = (V, E) without self-loops, and a weight
function w : E →N.
(1) Find a maximum assignment A = C1 ∪C2 ∪· · · ∪Ct for the graph G with
edge weight w, where each Ci, for i = 1, 2, . . ., t, is a cycle.
(2) For i ←1 to t do
let (ui, vi) be an edge in Ci with the lowest weight;
let C′
i be the path in Ci that begins at vi and ends at ui.
(3) Let H be the cycle formed by connecting the paths C′
i, i = 1, 2, . . ., t, with
edges (u1, v2), (u2, v3), . . ., (ut−1, vt), and (ut, v1) (cf. Figure 6.1);
return H.
Note that the maximum Hamiltonian circuit H∗of G is an assignment, and so
w(A) ≥w(H∗). Furthermore, since G has no self-loops, each cycle Ci in A has at
least two edges. Therefore, each edge (ui, vi) has weight w((ui, vi)) ≤w(Ci)/2;
or, equivalently, each path C′
i has weight w(C′
i) ≥w(Ci)/2. It follows that
w(H) ≥
t

i=1
w(C′
i) ≥1
2
t

i=1
w(Ci) = w(A)
2
≥w(H∗)
2
.
Therefore, Algorithm 6.A is a 2-approximation to MAX-DHC.
□

214
Relaxation
u1
u2
ut
vt
v2
v1
C1
C2
Ct
C
’
’
’
Figure 6.1:
Construct a directed HC from an assignment.
We have pointed out in Example 6.1 that the relaxation of Hamiltonian circuits
to spanning trees does not work well for DIRECTED TSP. Can we apply the idea of
Example 6.4 to DIRECTED TSP? Unfortunately, it still looks hard. Let us see why.
First, we may assume that the input graph satisﬁes the triangle inequality, since it
is well known that ﬁnding a constant-ratio approximation to the general case of
DIRECTED TSP is NP-hard. Next, we can modify Algorithm 6.A to the following
algorithm:
Algorithm 6.B (Approximation Algorithm for DIRECTED TSP)
Input: A complete directed graph G = (V, E) without self-loops, and a weight
function w : E →N.
(1) Find a minimum assignment A = C1 ∪C2 ∪· · · ∪Ct of G, where each Ci,
1 ≤i ≤t, is a cycle.
(2) For i ←1 to t do
Select an edge (ui, vi) from cycle Ci;
Let C′
i be the path in Ci that begins at vi and ends at ui.
(3) Form a directed cycle C of V ′ = {v1, v2, . . ., vt}. Without loss of generality,
assume that the cycle is C = (v1, v2, . . ., vt, v1).
(4) Let H be the cycle formed by connecting the paths C′
i, i = 1, 2, . . ., t, with
edges (u1, v2), (u2, v3), . . ., (ut−1, vt), (ut, v1) (see Figure 6.1);
output H.
It is clear that H is a directed Hamiltonian circuit, whose total length, by the tri-
angle inequality, is no more than w(A) + w(C). Since A is a minimum assignment,
we have w(A) ≤w(H∗), where H∗is a minimum Hamiltonian circuit. Therefore,
to get a constant-ratio approximation for the minimum directed Hamiltonian circuit,
we only need to construct, in polynomial time, a suitable cycle C over vertices in V ′
with the total length bounded by O(w(H∗)). Unfortunately, this problem of ﬁnding

6.1 DHC and Superstrings
215
the minimum cycle C is, in the general case, just DIRECTED TSP itself, and we are
back to square one.
Nevertheless, for some special cases of the problem DIRECTED TSP, this relax-
ation approach could still produce nice approximations. In the following,we present
an application of this idea to the problem SHORTEST SUPERSTRING (SS), which
was ﬁrst studied in Section 2.3.
First, we assume that no two strings of the input to the problem SS have the
superstring–substring relationship, as we can always ignore all strings that are sub-
strings of some other input strings. At the end of Section 2.3, we showed a natural
reduction from SS to DIRECTED TSP. Recall that the overlap ov(s, t) of a string s
with respect to another string t is the longest string v that is a sufﬁx of s as well as a
preﬁx of t. Also deﬁne, for two strings s and t, pref (s, t) to be the preﬁx r of s such
that r · ov(s, t) = s.1 In the following, we reserve the name s0 for the empty string.
The overlap graph of a set S = {s1, . . ., sn} of nonempty strings is the complete
directed graph F(S) = (V, E) with vertex set V = S ∪{s0} and the following
distance function:
d(si, sj) = |si| −|ov(si, sj)| = |pref (si, sj)|.
Then strings in S and graph F(S) have the following interesting relationship: The
strings in S appear in a shortest superstring s∗of S in the order of si1, si2, . . ., sin
if and only if the cycle H∗= (s0, si1, si2, . . ., sin, s0) is a minimum TSP tour of
the directed graph F (S) (note that it means we attach the empty string s0 at the end
of s∗). Furthermore, the total length of H∗is d(H∗) = |s∗|. From this relation, we
may convert an approximation algorithm for DIRECTED TSP to an approximation
algorithm for SS. In particular, we will apply the idea of relaxation of Hamiltonian
circuits to assignments to the construction of an approximation algorithm for SS.
Let s be a superstring for S = {s1, s2, . . ., sn}. Assume that strings of S appear
as substrings of s in the order of si1, si2, . . ., sin. Then we say that s is a minimal
superstring of S with respect to the order si1, si2, . . ., sin if each pair of adjacent
strings sij and sij+1, for j = 1, 2, . . ., n−1, has the maximal overlap between them
in s. We write ⟨si1, si2, . . ., sin⟩to denote the minimal superstring of S with respect
to the order si1, si2, . . ., sin. We note that, for any ordering of strings in S, there is
a unique minimal superstring of S with respect to this order. Also note that both
the optimum superstring Opt(S) = s∗and the superstring Greedy(S) obtained by
greedy Algorithm 2.B are minimal superstrings.
Let A = C1 ∪C2 ∪· · · ∪Ct+1 be a minimum assignment in the directed graph
F (S), where each Ci, 1 ≤i ≤t + 1, is a cycle. Without loss of generality, as-
sume that Ct+1 contains the vertex s0. Let (u1, v1), (u2, v2), . . ., (ut+1, vt+1) be
edges selected from cycles C1, C2, . . ., Ct+1, respectively, with vt+1 = s0. As
discussed in step (3) of Algorithm 6.B, we need to ﬁnd a cycle C over V ′ =
{v1, v2, . . ., vt+1}. Assume that C = (vi1, vi2, . . ., vit, vt+1, vi1). Let C′ be the
cycle C with vertex vt+1 removed. Then the path C′ = (vi1, vi2, . . ., vit) corre-
1For two strings x and y, we write x · y or xy to denote the concatenation of x and y.

216
Relaxation
sponds to the minimal superstring s′ = ⟨vi1, vi2, . . ., vit⟩. Furthermore, the length
of this minimal superstring s′ is equal to the length of the total distance d(C) of the
cycle C:
|⟨vi1, vi2, . . ., vit⟩|
= |pref (vi1, vi2)| + |pref (vi2, vi3)| + · · · + |pref (vit−1, vit)| + |vit|
= d(vi1, vi2) + d(vi2, vi3) + · · · + d(vit−1, vit) + d(vit, s0) + d(s0, vi1)
= d(C).
We pointed out earlier that ﬁnding the minimum cycle C′ covering all vertices
in V ′ is, in general, as difﬁcult as the problem DIRECTED TSP. However, in this
case, we can prove that the greedy Algorithm 2.B for SS will actually produce a
superstring of v1, v2, . . ., vt with length at most 2 · |Opt(S)|.
To show this result, we need some simple properties about strings. For any
nonempty string x, we write ρ(x) to denote the root of x; that is, ρ(x) is the shortest
string y satisfying x = yk for some k > 0.
Lemma 6.5 If y is the root of a nonempty string u [i.e., y = ρ(u)], then y = ρ(y).
Proof. Since y = ρ(u), we know that yk = u for some k > 0. If x = ρ(y) ̸= y,
then xℓ= y for some ℓ> 1. It follows that xℓk = u, and x is shorter than y,
contradicting the assumption that y is the root of u.
□
Lemma 6.6 Suppose that u and v are two nonempty strings satisfying uv = vu.
Then ρ(u) = ρ(v).
Proof. Without loss of generality, assume that |u| ≥|v|. We prove the lemma by
induction on |u|. For |u| = 1, it is obvious that ρ(v) = v = u = ρ(u). Now, assume
|u| > 1. If |u| = |v|, then uv = vu implies that u = v and, hence, ρ(u) = ρ(v).
Suppose that |u| > |v|. Then uv = vu implies u = vu1 for some nonempty string
u1. Now, (vu1)v = uv = vu = v(vu1) implies u1v = vu1. By the induction
hypothesis, ρ(u1) = ρ(v).
We now claim that y = ρ(u1) = ρ(v) is also the root of u. Suppose this is not
true. Then the root x = ρ(u) of u must be shorter than y, since yi = u1 and yj = v
for some i, j > 0 implies that yi+j = u. It follows that xk = u = yi+j for some
k > i + j. Now, from the relationship xk = yi+j, we see that x is a preﬁx of y, as
well as a sufﬁx of y. Let ℓ= ⌊|y|/|x|⌋, z be the sufﬁx of y such that y = xℓz, and w
be the preﬁx of y such that y = wxℓ. Note that z is also a preﬁx of x, since xℓ+1 is a
preﬁx of y2. Thus, both z and w are preﬁxes of x of the same length, and it follows
that z = w. This means that y = xℓz = zxℓ. If z is the empty string, then xℓ= y,
and this contradicts the fact that ρ(y) = y. On the other hand, if z is nonempty,
then we have xz = zx. By the induction hypothesis, ρ(x) = ρ(z). However, this
implies ρ(x)p = x and ρ(x)q = z for some p, q > 0, and so ρ(x)pℓ+q = y, again
contradicting the fact that y = ρ(y). This completes the proof of the claim and,
hence, the lemma.
□

6.1 DHC and Superstrings
217
We now consider a cycle C = (x1, x2, . . ., xk, x1) over some vertices in F(S).
For each i = 1, 2, . . ., k, we may attach the string pi to the edge (xi, xi+1),
where pi = pref (xi, xi+1) (identifying xk+1 with x1). Let s(C) = p1p2 · · · pk;
then we have d(C) = |s(C)|. A string w is called a period of C if w =
pipi+1 · · ·pkp1 · · ·pi−1 for some 1 ≤i ≤k, that is, if w is a cyclic shift of s(C)
beginning at some vertex xi ∈C.2 We say that the cycle C embeds a string x′ if
x′ is a substring of s(C)ℓfor some sufﬁciently large integer ℓ. Clearly, C embeds
every string xj in the cycle C.
Lemma 6.7 If a cycle C = (x1, x2, . . ., xk, x1) embeds, in addition to strings in
C, strings xk+1, xk+2, . . ., xm in S, with m > k, then there is another cycle C′
over all vertices x1, x2, . . ., xm with distance d(C′) = d(C).
Proof. Let w be a period of C. Then all strings x1, x2, . . ., xm occur as substrings of
wℓfor some ℓ. We may assume, without loss of generality, that every xi begins, as a
substring of wℓ, in the ﬁrst copy of w in wℓ. We can rearrange them according to the
order of their occurrences in wℓ(since no string xi is a substring of xj, for i ̸= j,
this order is well deﬁned). Now, deﬁne a cycle C′ over all strings x1, x2, . . ., xm,
according to this order. Apparently, w is still a period of C′, and d(C′) = d(C). □
Lemma 6.8 Assume that w is a period of a cycle C = (x1, x2, . . ., xk, x1) and
ρ(w) ̸= w. Then there exists a cycle C′ over the same set of vertices in C having
ρ(w) as a period of C′ and d(C′) = |ρ(w)|.
Proof. Assume that ρ(w)m = w for some m > 1. Then, every string xi, for 1 ≤
i ≤k, occurs as a substring of ρ(w)ℓfor sufﬁciently large ℓ. Note that the ﬁrst
occurrence of each xi in ρ(w)ℓmust begin within the ﬁrst copy of ρ(w). We can re-
arrange strings in C in the order of their ﬁrst occurrence in ρ(w)ℓ, and form a cycle
C′ over the vertices in C in this order. Since all strings occur in ρ(w)ℓbeginning in
the ﬁrst copy of ρ(w), we have s(C′) = ρ(w). It follows that C′ embeds all strings
in C and has d(C′) = |ρ(w)|.
□
The above lemma means that the period w of a cycle C in a minimum assignment
must have ρ(w) = w. In addition, together with Lemma 6.7, it implies that two
periods w1 and w2 of two different cycles C1 and C2, respectively, in a minimum
assignment must have w1 = ρ(w1) ̸= ρ(w2) = w2.
Lemma 6.9 Suppose C1 and C2 are two cycles in a minimum assignment of F(S).
Let x1 and x2 be two vertices in cycles C1 and C2, respectively. Then
|ov(x1, x2)| < d(C1) + d(C2).
Proof. For contradiction, suppose |ov(x1, x2)| ≥d(C1) + d(C2). Let u and v be
the preﬁxes of ov(x1, x2) of lengths |u| = d(C1), and |v| = d(C2), respectively.
2A string u is a cyclic shift of string v if there exist strings s, t such that u = st and v = ts.

218
Relaxation
ov x , x 
(
)2
1
w
2x
v
v
v
u
1x
u1
u
1
v
Figure 6.2:
Relationships between u and v.
Then u and v are periods of C1 and C2, respectively. Moreover, for sufﬁciently large
k and ℓ, uk and vℓhave a preﬁx of length |ov(x1, x2)| in common. We claim that
uv = vu, and hence, by Lemma 6.6, ρ(u) = ρ(v), a contradiction.
It remains to prove the claim. First, if |u| = |v|, then we have u = v, and so
uv = vu. So, we may assume that |u| > |v|. Let w be the preﬁx of ov(x1, x2) of
length |w| = |u| + |v|. Since both u and v are preﬁxes of ov(x1, x2), we know that
v is a preﬁx of u. From |ov(x1, x2)| ≥|u| + |v|, we know that w = uu1, where
u1 is the preﬁx of u of length |v|, and hence is equal to v (see Figure 6.2). That is,
w = uv. On the other hand, w = vv1, where v1 is of length |u|. Furthermore, v1 is
the preﬁx of vℓ−1 and, hence, the preﬁx of ov(x1, x2). Therefore, v1 = u, w = vu,
and the claim is proven.
□
Now, we come back to step (3) of Algorithm 6.B, and consider how to ﬁnd a
cycle over vertices in V ′ = {v1, v2, . . ., vt, vt+1}. Let Opt(V ′) denote the shortest
supersting of strings in V ′ \ {vt+1}, and Greedy(V ′) the superstring found by the
greedy Algorithm 2.B. Assume that Opt(V ′) = ⟨vi1, vi2, . . ., vit⟩. By Theorem
2.19, we have
∥V ′∥−|Opt(V ′)| ≤2(∥V ′∥−|Greedy(V ′)|),
where ∥V ′∥denotes the total length of strings in V ′. It is clear that
∥V ′∥=
t−1

j=1
|pref (vij, vij+1)| + |ov(vij, vij+1)| + |vit|
and
|Opt(V ′)| =
t−1

j=1
|pref (vij, vij+1)| + |vit|.
Therefore,
|Greedy(V ′)| ≤|Opt(V ′)| + 1
2
t−1

j=1
|ov(vij, vij+1)|.
By Lemma 6.9,

6.2 Two-Stage Greedy Approximations
219
t−1

j=1
|ov(vij, vij+1)| ≤d(Ci1) + 2
t−1

j=2
d(Cij) + d(Cit) ≤2 · |Opt(S)|.
Moreover, |Opt(V ′)| ≤|Opt(S)|. Therefore,
|Greedy(V ′)| ≤2|Opt(S)|.
We have just proved the following theorem:
Theorem 6.10 Let A = C1 ∪C2 ∪· · · ∪Ct+1 be a minimum assignment of
the directed graph F (S). Suppose that Ct+1 contains the empty string s0, and
v1, v2, . . ., vt are vertices chosen from cycles C1, . . ., Ct, respectively. Let s′ be
the superstring of vertices v1, v2, . . ., vt found by greedy Algorithm 2.B. Then
|s′| ≤2 · |Opt(S)|.
Corollary 6.11 The problem SS has a polynomial-time approximation with perfor-
mance ratio 3.
Proof. Apply Algorithm 6.B to F (S), using greedy Algorithm 2.B to ﬁnd a cycle C
in step (3).
□
We remark that the performance ratio 3 of Corollary 6.11 can be further improved
to a value close to 2.5 (see Historical Notes). Nevertheless, all the improvements are
based on the fundamental idea we studied in this section.
6.2
Two-Stage Greedy Approximations
The algorithm used in Corollary 6.11 can be considered a two-stage approximation
algorithm, in which we combine the relaxation technique with the greedy strategy
to solve the problem SS. To be more precise, we relax, in the ﬁrst stage, the problem
SS to the minimum assignment problem, and ﬁnd the minimum assignment in poly-
nomial time. Then, in the second stage, we apply the greedy algorithm to modify the
minimum assignment to an approximate superstring. In some two-stage approxima-
tions, we may also apply, in the ﬁrst stage, the greedy strategy directly to the relaxed
problem to ﬁnd an optimal or approximate solution for the relaxed problem. Then,
in the second stage, we modify the solution into the feasible region of the original
problem. In the following, we study two examples in this approach.
Recall the problem MIN-CDS (MINIMUM CONNECTED DOMINATING SET) In
Section 2.5, we proposed a potential function for this problem as follows: Given a
graph G and a vertex subset C, we ﬁrst color all vertices in three colors: a vertex in
C is colored in black, a vertex adjacent to some black vertex is colored in gray, and
all remaining vertices are colored in white. Let p(C) be the number of connected
components of the induced subgraph G|C, and h(C) the number of white vertices.
Let g(C) = p(C)+h(C). It is clear that C is a connected dominating set if and only
if g(C) = 1. Therefore, we might use function g as a potential function. However,
we showed, in Section 2.5, by a counterexample, that a vertex subset C may not

220
Relaxation
be a connected dominating set even though Δxg(C) = 0 for all vertices x. As a
consequence, the output of a greedy algorithm using g as the potential function may
not be a connected dominating set, and we did not take g as the potential function in
our greedy approximation for MIN-CDS. On the other hand, if we examine this idea
closely, we would ﬁnd that the output of the greedy algorithm using the potential
function g can be easily modiﬁed into a connected dominating set (cf. Lemma 2.42).
This observation suggests the following two-stage greedy approximation.
Algorithm 6.C (Two-Stage Greedy Algorithm for MIN-CDS)
Input: A connected graph G.
Stage 1: Set C ←∅;
While there exists a vertex x such that Δxg(C) < 0 do
Choose a vertex x to maximize −Δxg(C);
Set C ←C ∪{x}.
Stage 2: While there exists more than one black component do
Find a chain of two gray vertices x and y connecting
at least two black components;
Set C ←C ∪{x, y};
Output C.
In this two-stage greedy approximation, Stage 1 is a greedy algorithm computing
a dominating set and Stage 2 connects this dominating set into a connected set. As
the value h(C) is included in the potential function g(C), the greedy choice based
on g(C) makes sure that the output of Stage 1 is a dominating set.
Lemma 6.12 At the end of Stage 1 of Algorithm 6.C, the graph G contains no white
vertex.
Proof. Let x be a white vertex with respect to some vertex subset C. Suppose x has
a white neighbor; then coloring x in black eliminates at least two white vertices,
and it introduces at most one new black connected component. Therefore, we have
Δxg(C) < 0. On the other hand, if x has no white neighbor, then x must have a
gray neighbor y. Then, coloring y in black does not increase the number of black
connected components, but it eliminates at least one white vertex. Again, we have
Δyg(C) < 0. In either case, Stage 1 does not end at this point.
□
In addition, we included value p(C) in g(C), and so the number of black con-
nected components in the output of Stage 1 is kept small. As a result, we do not
need to add too many vertices in Stage 2.
Theorem 6.13 Suppose the input graph G is not a star. Then Algorithm 6.C is a
polynomial-time (3 + ln δ)-approximation for MIN-CDS, where δ is the maximum
vertex degree of the input graph.

6.2 Two-Stage Greedy Approximations
221
Proof. By a piece (with respect to a set C of black vertices), we mean a white vertex
or a connected component of the subgraph induced by black vertices. A piece is said
to be touched by a vertex x if x is either in the piece or adjacent to the piece. It is
clear that, for any vertex subset C, the number of pieces with respect to C is exactly
g(C). Suppose x1, x2, . . ., xt are the vertices selected, in this order, in Stage 1 of
Algorithm 6.C. Denote Ci = {x1, x2, . . ., xi}, for 1 ≤i ≤t, and C0 = ∅.
Consider set Ci−1 for some 1 ≤i ≤t. Suppose a nonblack vertex x touches
m pieces with respect to Ci−1. If x is white, then all pieces touched by x are
white vertices. Therefore, coloring x in black would eliminate m white vertices
(including x itself), and introduce one new black connected component. That is,
−Δxg(Ci−1) = m −1. On the other hand, if x is gray, then it may touch k white
neighbors and m−k black connected components. Coloring x in black would elim-
inate k white pieces and connect m −k black connected components into one.
Again, −Δxg(Ci−1) = m −1. In other words, for any vertex x, it touches exactly
1 −Δxg(Ci−1) pieces with respect to Ci−1. Among all vertices, xi is the vertex
that touches the maximum number of pieces.
Since a piece must be touched by a vertex in the minimum connected dominating
set, xi must have touched at least ⌈g(Ci−1)/opt⌉pieces, where opt is the number
of vertices in a minimum connected dominating set D∗. It follows that
1 −Δxig(Ci−1) ≥g(Ci−1)
opt
;
or, equivalently,
g(Ci) ≤g(Ci−1)

1 −1
opt

+ 1.
Set ai = g(Ci) −opt. Then we have
ai ≤ai−1

1 −1
opt

.
Note that if ai−1 > 0, then g(Ci−1)/opt > 1, and so −Δxg(Ci−1) > 0 for some x
in D∗, and hence ai < ai−1. It follows that at ≤0. Choose index j ≤t such that
aj ≤0 < aj−1. Then we must have at ≤j −t, since the value of ai must decrease
by at least one in each iteration. This implies that there are at most opt−t+j pieces
left when Stage 1 ends. From Lemma 6.12, we know that all these pieces are black
connected components. Since we only need to add two black vertices to reduce the
number of black connected components by one, at most 2(opt −t + j) vertices
would be added in Stage 2.
Choose i < j such that ai+1 < opt ≤ai [if no such i exists, then we have
a0 = g(∅) = n < opt, and coloring every vertex black is a 2-approximation]. Then
j −i ≤opt, and
opt ≤ai ≤ai−1

1 −1
opt

≤a0

1 −1
opt
i
≤n · e−i/opt,
where n is the number of vertices in the input graph. Thus,

222
Relaxation
i ≤opt · ln
 n
opt

.
Note that for a nonstar graph of the maximum vertex degree δ, the size of a con-
nected dominating set is at least n/δ. Therefore, the total number of vertices selected
by Algorithm 6.C is at most
t + 2(opt −t + j) ≤2 · opt + j
≤3 · opt + i ≤opt

3 + ln
 n
opt

≤opt(3 + lnδ).
□
Next, we study the minimum power broadcasting problem. Recall the notion of a
broadcasting tree in a network introduced in Section 3.4. Let G be a network, that is,
a connected, bi-directed graph with nonnegative edge weight, with the property that
w(u, v) = w(v, u) when both (u, v) and (v, u) are edges in G. A broadcasting tree
T of G from a node s is an arborescence rooted at s over all nodes of S. The power
of a nonsink node u in a broadcasting tree T is the maximum weight of out-edges
in T from u, and the power of the tree T is the sum of the powers over all nonsink
nodes in T.
BROADCASTING TREE WITH MINIMUM POWER (BT-MP): Given a
connected, bi-directed graph G with nonnegative edge weight and a
node s, ﬁnd a broadcasting tree from s with the minimum power.
A directed graph G is weakly connected if it is connected when direction on each
edge is removed (and so G becomes a connected undirected graph). A broadcast-
ing tree is clearly a weakly connected subgraph. This observation suggests that we
may relax BT-MP to the problem of ﬁnding a weakly connected subgraph with
the minimum power. Along this idea, a two-stage greedy approximation can be de-
signed as follows: At Stage 1, use a greedy algorithm to ﬁnd an approximation for
the minimum-power weakly connected spanning tree; and at Stage 2, modify the
weakly connected spanning tree obtained in Stage 1 to a broadcasting tree.
To design a greedy algorithm for Stage 1, we need to deﬁne a potential function.
Let G = (V, E) be a directed graph and w an edge-weight function on E. A star A
(centered at a vertex v) in G is a subset of out-edges from v in G. The weight of a
star A, denoted by w(A), is the maximum weight of an edge in the star. Let F be
the set of all stars A satisfying the following condition: If A contains an out-edge
from v with weight w, then every out-edge from v, with weight not exceeding w,
is also in A. For a directed graph G, a weakly connected component is a connected
component of the undirected graph G′ obtained from G by removing the directions
of all edges in G. For every subset S of F, deﬁne f(S) to be the number of weakly
connected components of the subgraph GS = (V, ∪S), whose edge set consists of
all edges in all stars in S. The following lemma shows that we can use −f(S) as the
potential function to design the greedy algorithm.
Lemma 6.14 f(S) is a monotone decreasing, supmodular function on E.

6.3 Connected Dominating Sets in Unit Disk Graphs
223
Proof. Obvious.
□
In Stage 2, we need to convert a weakly connected spanning tree into a broad-
casting tree. The following lemma suggests that we can simply reverse the direction
of an edge whenever it is needed.
Lemma 6.15 Suppose B is a weakly connected spanning subgraph of G. If B does
not contain a broadcasting tree from s, then there is an edge (u, v) in B such that s
can reach v but cannot reach u.
Proof. Let V1 be the set of nodes reachable from s and V0 the set of nodes not
reachable from s. Since G is weakly connected. There must exist an edge (u, v)
from V0 to V1.
□
Algorithm 6.D (Two-Stage Greedy Approximation for BT-MP)
Input: A connected bi-directed graph G with a nonnegative edge-weight function
w, and a node s.
Stage 1: Set S ←∅;
While f(S) > 1 do
Choose a star A ∈F to maximize −ΔAf(S)/w(A);
Set S ←S ∪{A};
B ←
A∈S A.
Stage 2: While B does not contain a broadcasting tree from s do
Find an edge (u, v) in B such that s can reach v but not u;
Set B ←B ∪{(v, u)};
Output a broadcasting tree T from the graph (V, B).
Theorem 6.16 Algorithm 6.D is a 2H(δ)-approximation for BT-MP, where δ is
the maximum node degree of the input graph, and H is the harmonic function.
Proof. Let opt be the minimum power of a broadcasting tree. Then the minimum
power of a weakly connected subgraph is at most opt. Let B1 be the set B at the
end of stage 1. Then, by Theorem 2.29, the power of the graph G1 = (V, B1) is at
most H(γ) · opt, where γ = maxA∈F (−f(A) + f(∅)) ≤δ [note that the function
g(A) = −f(A) + f(∅) is the potential function used in Theorem 2.29]. In Stage 2,
we note that for each edge (v, u) added to B, the weight of v increases to at most
w(v, u) = w(u, v), which does not exceed the weight of u. Therefore, the total
power increase does not exceed the power of the digraph G1 = (V, B1) at the end
of Stage 1. Therefore, the power of T is at most 2H(δ) · opt.
□
6.3
Connected Dominating Sets in Unit Disk Graphs
Next, let us review the problem CDS-UDG (CONNECTED DOMINATING SET IN
A UNIT DISK GRAPH). We found in Section 4.2 a PTAS for this problem. Its run-
ning time nO(1/ε2) is, however, too high to be implemented for moderately large

224
Relaxation
x1
x2
A
B
C
θ
Figure 6.3:
Two adjacent vertices have a common covering area.
input graphs. Therefore, one would still like to ﬁnd good approximations to it with a
lower running time, allowing implementation in applications in, for instance, wire-
less sensor networks. In this section, we follow the idea of the two-stage greedy
approximations to design an approximation to this problem. Namely, we ﬁrst con-
struct, by a greedy algorithm, a dominating set of the input graph, and then connect
it together.
For the construction of a dominating set, a popular way is to relax it to the maxi-
mal independent set problem.3 We note that a maximal independent set of a graph G
must be a dominating set of G. In addition, in a unit graph, the size of any maximal
independent set is within a constant factor from the size of the minimum connected
dominating set.
Lemma 6.17 In a unit disk graph G, the size of a maximal independent set is upper-
bounded by (3.74)opt + 5.26, where opt is the size of a minimum connected domi-
nating set of G.
Proof. We will bound the number of independent vertices by counting the areas of
unit disks centered at these vertices. First, deﬁne the covering area of a vertex x
in G to be the disk with center x and radius 3/2. Two adjacent vertices in G have
distance at most 1. Therefore, the covering areas of two adjacent vertices must share
a common area of size at least 9
2 arccos 1
3 −
√
2. To see this, we draw two circles
of radius 3/2, with centers x1 and x2 of distance 1 apart (see Figure 6.3). Then the
angle θ = ∠Ax1C is equal to arccos(1/3) and so the shaded area
x1AC has
size s = (3/2)2θ/2 = (9/8) arccos(1/3). The area of the intersection of the two
circles is equal to 4s minus the area of ♦x1Ax2B, that is, 4s −
√
2.
Thus, the total covering area of a minimum connected dominating set of n ver-
tices is at most
3A maximal independent set of a graph G = (V, E) is an independent set S ⊆V such that no
superset T ⊇S is independent.

6.3 Connected Dominating Sets in Unit Disk Graphs
225
(n −1)
, 3
2
2
π −
 9
2 arccos 1
3 −
√
2
-
+
 3
2
2
π
≤2.93(n −1) + 2.25π.
Now, for every vertex y in a maximal independent set, draw a disk centered at y
with radius 1/2. Then all these disks are mutually disjoint and lie in the covering
area of the minimum connected dominating set. Therefore, the size of a maximal
independent set is at most
2.93(n −1) + 2.25π
0.25π
≤3.74(n −1) + 9 = 3.74n + 5.26.
□
We remark that the above estimate of the upper bound for the size of a maximal
independent set is not very tight and can be further improved. The best result known
so far is that every maximal independent set has size at most 3.478 · opt+4.874[Li,
Gao, and Wu, 2008]. It is conjectured in the literature that every maximal indepen-
dent set has size at most 3 · opt + 2. This would be the best possible upper bound
[Wan et al., 2008].
In order to have a simple connecting strategy in Stage 2, we will construct a
maximal independent set D with the following property:
Π1: For any proper subset S ⊂D, there is a vertex x such that x is adjacent to
both S and its complement D \ S.
Such a maximal independent set is easy to construct based on the white–gray–black
coloring. Namely, when we add a vertex to the independent set, we always select a
white vertex that has a gray neighbor.
Now, consider the connecting stage. If we consider the maximal independent
set D constructed in the ﬁrst stage as a set of terminals, then the problem for the
connecting stage is a variation of the problem ST-MSP of Section 3.4. Since ST-
MSN is NP-hard, we need to design an approximation for it. Here, with the maximal
independent set D having the special property Π1, a greedy approximation to this
problem is easy to design. For any vertex subset C, let p(C) denote the number
of connected components of the subgraph of G induced by C. We can use p(C)
as the potential function to design a greedy algorithm to connect the connected
components of D into a connected dominating set.
Algorithm 6.E (Two-Stage Approximation Algorithm for CDS-UDG)
Input: A connected unit disk graph G.
Stage 1: Select a vertex x; Set D ←{x};
Color x in black, all its neighbors in gray, and all other
vertices in white;
While there is a white vertex do
Choose a white vertex x with a gray neighbor;

226
Relaxation
D ←D ∪{x};
Color x in black and its white neighbors in gray;
Return D.
Stage 2: Set C ←D;
While p(C) ≥2 do
Choose a vertex x to maximize −Δxp(C);
C ←C ∪{x};
Return C.
It is clear that the set D constructed by the end of Stage 1 has property Π1, and
hence the output C of Stage 2 is a connected dominating set. The following theorem
gives us an upper bound for the performance of the second stage of Algorithm 6.E.
Theorem 6.18 Assume that the maximal independence set D found in Stage 1 of
Algorithm 6.E has size |D| ≤α · opt + β for some α ≥1 and β > 0, where opt
is the size of a minimum connected dominating set. Then the connected dominating
set C found by Algorithm 6.E has size at most
(α + 2 + ln(α −1))opt + β + ⌊β⌋.
Proof. We follow the standard approach for the analysis of greedy algorithms. Let
x1, . . ., xg be the vertices selected in Stage 2 of Algorithm 6.E, in the order of
their selection into the set C. Also, let {y1, . . ., yopt} be a minimum connected
dominating set for G with the property that, for each i = 1, 2, . . ., opt, the set
{y1, . . ., yi} induces a connected subgraph. Denote C0 = D and, for 0 ≤i ≤g −1,
Ci+1 = Ci ∪{xi+1}. In addition, for each j = 1, 2, . . ., opt, we write C∗
j denote
set {y1, . . ., yj}.
By the greedy strategy, we know that for each i = 0, . . ., g −1,
−Δxi+1p(Ci) ≥−Δyjp(Ci),
for all j = 1, . . ., opt. In addition, since the induced subgraph G|C∗
j is connected,
we have
−Δyjp(Ci ∪C∗
j−1) + Δyjp(Ci) ≤1.
Thus, for opt ≥2 and any i = 0, 1, . . ., g −1,
−Δxi+1p(Ci) ≥
−opt
j=1 Δyjp(Ci)
opt
≥
−opt + 1 −opt
j=1 Δyjp(Ci ∪C∗
j−1)
opt
= −opt + 1 −p(Ci ∪C∗
opt) + p(Ci)
opt
= −opt + p(Ci)
opt
.
That is,

6.3 Connected Dominating Sets in Unit Disk Graphs
227
−p(Ci+1) ≥−p(Ci) + −opt + p(Ci)
opt
,
for all i = 0, 1, . . ., g −1. Denote ai = −opt −β + p(Ci). Then, for each i =
0, 1, . . ., g −1,
ai+1 ≤ai

1 −1
opt

,
and so
ai ≤a0

1 −1
opt
i
≤a0e−i/opt.
First, consider the case a0 ≥opt. Note that ag = −opt −β + p(Cg) = −opt −
β + 1 < opt. Thus, there exists an integer j, 0 ≤j < g, such that
aj+1 < opt ≤aj.
Since the values of the ai’s decrease in each iteration, we must have
g −(j + 1) ≤aj+1 −ag < opt −(−opt −β + 1) = 2 · opt + β −1;
or, equivalently, g ≤j + 2 · opt + ⌊β⌋. Now, from
opt ≤aj ≤a0e−j/opt,
we get
j ≤opt · ln
 a0
opt

= opt · ln
−opt −β + |D|
opt

≤opt · ln(α −1).
Therefore,
|D| + g ≤(α + 2 + ln(α −1))opt + β + ⌊β⌋.
Next, consider the case of a0 < opt. This implies that p(C0) < 2 · opt + β, and
so g ≤2 · opt −1 + ⌊β⌋. Thus,
|D| + g ≤(α + 2)opt + β + ⌊β⌋−1.
□
Corollary 6.19 The connected dominating set found by Algorithm 6.E has size at
most (6.7)opt + 10.26, where opt is the size of the minimum connected dominating
set of the input graph G.
Finally, we remark that the simple greedy strategy of Stage 2 of Algorithm 6.E
works because the maximal independence set D found in Stage 1 satisﬁes property
Π1. However, we did not take full advantage of property Π1 in our analysis of the
algorithm. A more careful analysis using this property actually shows that the output
of Algorithm 6.E has size at most 6 7
18 · opt [Wan et al., 2008].

228
Relaxation
Figure 6.4:
A strongly connected dominating set (dark nodes indicating the dom-
inating set).
6.4
Strongly Connected Dominating Sets in Digraphs
Consider a digraph (i.e., a directed graph) G = (V, E). A node subset C ⊆V is a
dominating set of G if, for every node x not in C, there is an edge going from x to
C and an edge coming from C to x, i.e., if there are edges (x, y), (z, x) in E with
y, z ∈C. Furthermore, set C is called a strongly connected dominating set of G
if C is a dominating set and, in addition, the subgraph G|C of G induced by C is
strongly connected (see Figure 6.4). In this section, we study the following problem:
STRONGLY CONNECTED DOMINATING SET (SCDS): Given a di-
graph G, ﬁnd a strongly connected dominating set of the minimum
cardinality.
No good direct approximations are known for this problem at this time. Here we
relax it to the problem of ﬁnding broadcasting trees with the minimum number of
internal nodes.
Recall that a broadcasting tree of a directed graph G is a spanning arborescence
of G, i.e., a rooted directed tree in which every node is reachable from the root. For
any digraph G = (V, E), let GR be the graph obtained from G by reversing the
direction of each edge in E; that is, GR = (V, ER), where ER = {(y, x) | (x, y) ∈
E}. In a broadcasting tree, a nonleaf node is called an internal node. We observe
that a strongly connected dominating set S of G can be viewed as the collection
of internal nodes of two broadcasting trees T1, T2 of G and GR, respectively, that
share the same source node. Conversely, if we have two broadcasting trees T1 and
T2 of G and GR, respectively, sharing the same source node, then the collection of
all internal nodes of T1 and T2 is a strongly connected dominating set. Thus, we can
relax the problem SCDS to the following problem:
BROADCASTING TREE WITH MINIMUM INTERNAL NODES (BT-
MIN): Given a digraph G and a source node s, ﬁnd a broadcasting tree

6.4 Strongly Connected Dominating Sets in Digraphs
229
of G with source node s with the minimum number of internal nodes
other than s.
In the following, we write optB(G, r) to denote the number of internal nodes
in the minimum solution to BT-MIN on digraph G and source node r. Also, let
optS(G) denote the size of the minimum strongly connected dominating set of G.
Lemma 6.20 For any digraph G and any node r in G, we have
optB(G, r) ≤optS(G).
Moreover, if r belongs to an optimum solution of SCDS on input G, then
optB(G, r) ≤optS(G) −1.
Proof. Let G = (V, E) be a digraph and C∗⊆V a minimum strongly connected
dominating set of G. For any node x ∈V , there is a path from r to x that passes
through only nodes in C∗. To see this, we note that C∗is a dominating set, and
so there must be nodes y, z ∈C∗such that (r, y), (z, x) ∈E. In addition, C∗is
strongly connected and, hence, there is a path π from y to z using only nodes in C∗.
So, the path (r, y) ∪π ∪(z, x) is the desired path. This means that we can construct
a broadcasting tree at source node r using only nodes in C∗as internal nodes.
When r ∈C∗, the number of internal nodes of this broadcasting tree is at most
|C∗|, and the value optB(G, r) is at most |C∗| −1.
□
Lemma 6.21 Assume that there is a polynomial-timeα-approximationfor the prob-
lem BT-MIN, for some α > 1. Then there is a polynomial-time(2α)-approximation
for the problem SCDS.
Proof. Let G = (V, E) be a digraph and C∗a minimum strongly connected dom-
inating set of G. For any node s ∈V , apply the α-approximation algorithm for
BT-MIN to ﬁnd a broadcasting tree T1 in G and a broadcasting tree T2 in GR, with
the common source s. For i = 1, 2, let I(Ti) denote the internal nodes of Ti. We
claim that Cs = I(T1) ∪I(T2) is a strongly connected dominating set of G.
To see this, we note that for every node x ∈V , there is a path from s to x in
T1, and so an edge from some node y ∈I(T1) to x. In addition, there is a path
π from s to x in T2. Therefore, there is an edge from some node z ∈I(T2) to x
in GR, which means that there is an edge from x to z ∈I(T2) in G. This shows
that Cs = I(T1) ∪I(T2) is a dominating set of G. In addition, for any two nodes
x, y ∈Cs, there exists a path in G from x to s with all internal nodes in I(T2), as
well as a path in G from s to y with all internal nodes in I(T1). Together, the union
is a path from x to y in Cs, and so Cs is strongly connected, and the claim is proven.
Clearly,
|Cs| = |I(T1) ∪I(T2)| ≤|I(T1) −{s}| + |I(T2) −{s}| + |{s}|
≤α

optB(G, s) + optB(GR, s)

+ 1.

230
Relaxation
body node
feet
Figure 6.5:
A spider.
Moreover, when s belongs to a minimum strongly connected dominating set,
|Cs| ≤α

optB(G, s) + optB(GR, s)

+ 1
≤αoptS(G) −1 + optS(GR) −1) + 1 ≤2α · optS(G),
since a minimum strongly connected dominating set for G is also a minimum
strongly connected dominating set for GR. Thus, to ensure that Cs is of size at
most 2α · optS(G), we only need to ﬁnd a node s in C∗. Choose an arbitrary node
u and let N(u) = {u} ∪{x ∈V | (x, u) ∈E}. It is clear that N(u) ∩C∗̸= ∅.
Therefore, we can just ﬁnd, for each s ∈N(u), a connected dominating set Cs and
use the smallest one among them as the approximation to C∗.
□
Next, we describe a polynomial-time approximation for BT-MIN. First, we in-
troduce some new terminologies. Assume that each node of the input digraph G has
been assigned a unique ID. Let H = (V1, E1) be a subgraph of G and s a source
node. An orphan of H is a strongly connected component of H satisfying the fol-
lowing properties: (i) It does not contain s, and (ii) there is no edge in E that starts
at a vertex in V1 \ C and ends at a vertex in C. For each orphan C of H, the node in
C with the smallest ID is called the head of C. Note that if a subgraph H contains
all nodes of G and has no orphans, then it must contain a broadcasting tree.
We call a subgraph S of G a spider if S consists of a body node and several
disjoint directed paths from the body node to its foot nodes (see Figure 6.5). The
general idea of our algorithm is to use a greedy strategy to build a broadcasting tree
T by adding spiders to it one by one. More precisely, we start with the subgraph H
that consists of all nodes in G and no edge (so that every node in H other than s is
an orphan head). Then, we select, at each iteration, a spider S in G and add it to H
until H has no more orphan heads. The selection of the spider is based on the greedy
strategy that minimizes the number of internal nodes in S relative to the number of
orphan heads (with respect to the current H) in S.
One of the problems with the above idea is that, at each iteration, there may be
an exponential number of spiders to consider. To make the algorithm running in
polynomial time, we need to limit our choices to some special spiders. We say a
spider S is legal (with respect to H) if it satisﬁes the following three conditions:

6.4 Strongly Connected Dominating Sets in Digraphs
231
x
Figure 6.6:
A new orphan is introduced when we add a spider.
(a) All feet of S are heads of some orphans of H,
(b) An orphan head can only occur in S at a foot or at the body node, and
(c) S contains at least two orphan heads of H, unless the body node of S is the
source s.
In the above, conditions (a) and (b) allow us to decompose a broadcasting tree
into the union of legal spiders at orphan heads. Condition (c) is required to make
sure that the number of orphan heads in H decreases after each iteration. Note that
when we add a legal spider S to H, the orphan heads in S at the feet of S are no
longer orphan heads. In the meantime, a new orphan may emerge, which contains
the body node of S. Thus, by condition (c), we reduce the number of orphan heads
by at least one. Figure 6.6 shows such a case, where a dark square denotes an orphan
head in H and the dashed edges denote a spider S. After spider S is added to H, an
orphan is introduced that includes the body node x of S.
For a subgraph H and a legal spider S with respect to H, let hH(S) be the
number of orphan heads in S and costH(S) the number of internal nodes in S
other than the internal nodes in H and the source s. When the subgraph H is clear,
we write h(S) for hH(S) and cost(S) for costH(S). Deﬁne quotH(S), or simply
quot(S), to be the ratio
quot(S) = cost(S)
h(S) .
Our intention is to use quot(S) as the potential function and to add, in each
iteration of our algorithm, the spider S with the minimum quot(S) to H. However,
even with the restriction to legal spiders, the number of possible choices of spiders
is still too large and the spider with the minimum quot(S) is still hard to ﬁnd. To
resolve this problem, we generalize the notion of spiders to pseudospiders.
Let u be a node in G. Suppose p1, p2, . . ., pk, for some k ≥2 (or k = 1 and
u = s), are k shortest paths from u to k different orphan heads such that none of
the internal nodes of the paths p1, . . ., pk are orphan heads. Then we say the subtree
S = p1 ∪p2 ∪· · · ∪pk is a legal pseudospider (note that the paths p1, . . ., pk may

232
Relaxation
share some common internal nodes). For a pseudospider S = p1 ∪· · · ∪pk, we
deﬁne h(S) and cost(S) as if the paths p1, . . ., pk are disjoint; that is, cost(S) =
length(p1)+· · ·+length(pk)−k+1. Note that for any legal spider S, there is a legal
pseudospider S′ with the same body node and same feet as S and having cost(S′) ≤
cost(S). Thus, when we consider legal spiders with the minimum quot(S), we need
only consider legal pseudospiders.
Moreover, we can compute the minimum quot(S) over all legal pseudospiders
S rooted at node u [called quot(u)] as follows: Suppose H has k orphan heads
and p1, . . ., pk are shortest paths from node u to them, without passing through any
orphan heads. Order the paths according to the cost: cost(p1) ≤cost(p2) ≤· · · ≤
cost(pk). Then, it is easy to see that, for u ̸= s,
quot(u) = min
2≤i≤k quot(p1 ∪· · · ∪pi);
and for u = s,
quot(u) = min
1≤i≤k quot(p1 ∪· · · ∪pi).
Now, we are ready to describe the algorithm.
Algorithm 6.F (Greedy Approximation Algorithm for BT-MIN)
Input: A strongly connected digraph G = (V, E), a source node s ∈V , and a
unique ID for each node in V .
(1) H ←V ; A ←V \ {s}.
(2) For each v ∈V do calculate quot(v).
(3) While A ̸= ∅do
(3.1) Choose a node u ∈V with the minimum quot(u);
(3.2) Let S(u) be the legal pseudospider at u with quot(S) = quot(u);
(3.3) A ←A \ {v | v is a head in S(u)};
(3.4) H ←H ∪S(u);
(3.5) If the strongly connected component Cu of H that contains u
becomes an orphan then A ←A ∪{head of Cu};
(3.6) For each v ∈V do recalculate quot(v).
(4) Let T be a broadcasting tree of H; Return T.
We now analyze the performance of this algorithm.
Lemma 6.22 For any subgraph H of G with q orphans, there exists a node u with
quot(u) ≤optB(G, s)
q
.

6.4 Strongly Connected Dominating Sets in Digraphs
233
u
w
x
s
Figure 6.7:
Spider decomposition.
Proof. Let T ∗be an optimal broadcasting tree. We ﬁrst prune T ∗to obtain a subtree
T such that every leaf of T is an orphan head of H. That is, we repeatedly remove
the leaves in the tree that are not orphan heads of H until there are no more such
leaves in H.
Next, we show that tree T can be decomposed to a sequence of legal spiders. For
any leaf x of T, let anc(x) be the lowest ancestor of x that is either a head of H
or has out-degree greater than 1. Let Anc(T) = {anc(x) | x is a leaf of T}. We
remove legal spiders from T as follows:
Case 1. There exists a leaf x of T whose anc(x) is a head and has out-degree 1.
Note that the path from anc(x) to x in T has no other branches and is a legal spider
with respect to H. Therefore, we can remove this spider from T. The remaining part
of T is still a tree, and we prune it to make all its leaves orphan heads.
Case 2. Not Case 1. Let y be a lowest node in Anc(T). Assume that y = anc(x).
Then the subtree Ty rooted at y must have at least two leaves, since anc(x) has out-
degree at least 2. Furthermore, we know that all leaves w of Ty have anc(w) = y,
for otherwise anc(w) would be a proper descendant of y and so is a lower ancestor
node in Anc(T). Thus, all internal nodes other than y in Ty are not heads, and so Ty
is a legal spider with respect to H. We can remove Ty from T, and again we prune
T if necessary so that all its leaves are heads.
We perform the above procedure until T is a single node s. Then we get a se-
quence of legal spiders S1, S2, . . ., Sℓsuch that
(i) Each spider Si, 1 ≤i ≤ℓ, is a subtree of T;
(ii) The spiders S1, . . ., Sℓare mutually disjoint; and
(iii) Each orphan head of H is in one of the spiders S1, . . ., Sℓ.
For instance, the tree T in Figure 6.7 can be decomposed into spiders Sw, Sx, Su
and Ss, where St denotes the spider with body node t. In the ﬁgure, the nodes with
labels are the body nodes of the spiders, the dark squares indicate orphan heads, and
the dashed edges are the edges pruned in this process.

234
Relaxation
We have, from (i) and (ii),
cost(S1) + · · · + cost(Sℓ) ≤optB(G, s).
(Note that each Si, 1 ≤i ≤ℓ, is a real spider, with all its legs disjoint.) We also
have, from (iii),
h(S1) + · · · + h(Sℓ) = q.
Thus,
min
1≤i≤ℓquot(Si) ≤optB(G, s)
q
.
This means that one of the heads u of the spiders S1, . . ., Sℓmeets our requirement.
□
Theorem 6.23 The problem BT-MIN has a polynomial-time approximation with
the performance ratio (1 + 2 ln(n −1)).
Proof. Suppose Algorithm 6.F runs on an input digraph G and a source node s and
halts in k iterations. For each i = 0, 1, . . ., k −1, let ni denote the number of
orphans in H right after the ith iteration. Also, let Si, for i = 1, . . ., k, be the legal
pseudospider chosen at the ith iteration, and hi be the number of heads in Si. Note
that we initially have n0 = n −1 orphans and, at the last iteration, nk−1 = hk
orphans. In the following, we write opt to denote optB(G, s).
In each iteration i, for i = 1, 2, . . ., k, we reduce at least hi −1 heads from H.
Therefore, we get
ni ≤ni−1 −hi
2 ,
for each i = 1, 2, . . ., k (when hi = 1, we reduce exactly one head in the ith
iteration). Moreover, by Lemma 6.22, for each i = 1, . . ., k,
cost(Si)
hi
≤opt
ni−1
.
Together, for each i = 1, . . ., k,
ni
ni−1
≤1 −cost(Si)
2 · opt .
Repeatedly applying the above inequality, we get
nk−1
n0
≤
k−1
.
i=1

1 −cost(Si)
2 · opt

.
Hence,
ln
nk−1
n0

≤−
k−1
i=1 cost(Si)
2 · opt
.

6.5 Multicast Routing in Optical Networks
235
Or, equivalently,
k−1

i=1
cost(Si) ≤2 · opt · ln
 n0
nk−1

≤2 · opt · ln(n −1).
Since cost(Sk)/hk ≤opt/nk−1 and hk = nk−1, we have cost(Sk) ≤opt. There-
fore,
k

i=1
cost(Si) ≤(1 + 2 ln(n −1))opt.
□
As a consequence, we have
Corollary 6.24 The problem SCDS has a polynomial-time (2 + 4 ln(n −1))-
approximation.
6.5
Multicast Routing in Optical Networks
In this section, we study the multicast routing problem in optical networks with
both splitting and nonsplittingnodes. An optical network is usually formulated as an
edge-weighted graph, with the switches represented as vertices. In the graph, there
are two types of vertices, nonsplitting and splitting. A splitting vertex can send an
input signal to several output vertices, while a nonsplitting vertex can only send the
input signal to one output. A multicast route in a graph G is a subtree of G in which
each edge is assigned a direction so that only a splitting vertex can have a higher
out-degree than its in-degree.
MINIMUM-WEIGHT MULTICAST ROUTING (MIN-MR): Given a
graph G = (V, E) with an edge-weight function w : E →R+ that
satisﬁes the triangle inequality, a subset A ⊆V of splitting vertices,
a source s ∈V , and a subset M ⊆V of multicast members, ﬁnd a
multicast route that spans all members in M with the minimum total
edge-weight.
We notice that if all vertices are nonsplitting and M = V , then MIN-MR can be
reduced to the minimum-weight Hamiltonian path problem, which is as hard as the
traveling salesman problem (TSP). If all vertices are splitting, then MIN-MR is just
the network Steiner minimum tree problem (NSMT). Thus, when both nonsplitting
and splitting vertices are allowed, the problem MIN-MR is at least as hard as TSP
and NSMT.
A simple idea for this problem is to ﬁrst relax the problem to NSMT, and then
modify the solution to get a multicast route. In the following, we assume that we
have a polynomial-time ρ-approximation algorithm ANSMT for NSMT.
Algorithm 6.G (Relaxation Algorithm for MIN-MR)
Input: An edge-weighted graph G = (V, E) with A ⊆V identiﬁed as splitting
vertices, a source vertex s ∈V , and a subset M ⊆V of multicast members.

236
Relaxation
Stage 1: Relax the input graph G to a new graph G′ that has the same vertex and
edge sets as G but every vertex in G′ is a splitting vertex;
Apply algorithm ANSMT on G′ to get a Steiner tree T.
Stage 2: Starting from the source vertex s, perform a depth-ﬁrst search on T, treat-
ing every vertex as a nonsplitting vertex;
Output the resulting route R.
Let opt be the weight of the minimum-weight multicast route of G. It is easy to
see that the weight w(T ∗) of the SMT T ∗of graph G′ is no greater than opt. There-
fore, weight w(T) of the tree obtained in Stage 1 is at most ρ · opt. In addition, the
weight w(R) of the output is at most twice as large as the weight w(T). Therefore,
the above algorithm is a (2ρ)-approximation for MIN-MR.
We note that the second stage of Algorithm 6.G is a straightforward modiﬁcation
of T. Can we improve it with some more sophisticated modiﬁcation? The answer
is yes. The following algorithm, similar to Christoﬁdes’s algorithm for TSP, uses
minimum matching in the second stage to get a better approximation.
Algorithm 6.H (Improved Relaxation Algorithm for MIN-MR)
Input: An edge-weighted graph G = (V, E) with A ⊆V identiﬁed as splitting
vertices, a source vertex s ∈V , and a subset M ⊆V of multicast members.
Stage 1:
(1.1) Let G′ be the complete graph on vertices in {s} ∪M ∪A;
(1.2) For each edge {u, v} of G′ do
w({u, v}) ←the total weight of the shortest path between u
and v in the input graph G;
(1.3) Apply ANSMT to G′ with weight w to get a Steiner tree T, treating all
vertices in M ∪{s} as terminals and all other vertices as Steiner vertices.
Stage 2:
(2.1) Let F be the subgraph of T that consists of all edges in T that are incident
on some Steiner node;
(2.2) For each connected component C of F do
treat C as a rooted tree, with root being the node closest to s in T,
and let p(C) be a path from the root to a leaf in C;
(2.3) Let K be the subgraph of T consisting of all edges in T \F plus all edges
in p(C) for all connected components C of F;
(2.4) Let D be the set of vertices with an odd degree in K, and
let M be a minimum-weight perfect matching for D;
(2.5) Find a multicast route R in T ∪M, and
output R.

6.5 Multicast Routing in Optical Networks
237
We ﬁrst show that the above algorithm is well deﬁned. In Stage 1, we note that
the weight function w deﬁned on G′ satisﬁes the triangle inequality. Therefore, the
algorithm ANSMT works on G′ with weight w. We also note that in the forest K
constructed in Stage 2, every Steiner vertex has an even degree. Therefore, the num-
ber of multicast members with odd degrees in K must be even, and the minimum-
weight perfect matching M for D exists. Finally, the following lemma shows that
the last step (2.5) is well deﬁned.
Lemma 6.25 In Algorithm 6.H, the set T ∪M contains a multicast route using
each edge at most once.
Proof. We note that since K is a forest, K ∪M is a disjoint union of cycles; each
cycle is a connected component of K ∪M. One of these cycles contains the source
node s. We can construct a multicast route R in T ∪M as follows:
(1) Initially, R contains a single vertex s.
(2) While there is a cycle Q of K ∪M such that R contains a vertex x in Q but
not all vertices in Q do
(2.1) Traverse the cycle Q, starting from x, until all vertices in Q are visited;
add these edges to R.
(2.2) While there exists a Steiner vertex y in R whose neighbors in T are not
all in R do
Split R at y to include edges from y to all its neighbors that are not
in R yet.
It is clear that this multicast route R uses each edge at most once. To see that
the route R covers every multicast member, we note that the connected components
of K ∪M are connected by the Steiner vertices in T. So we can see, by a simple
induction, that every cycle Q of K ∪M will be visited by route R.
□
We next estimate the total weight of T ∪M. To this end, it sufﬁces to study the
weight of M since the total weight of T is within a factor of ρ from the weight of a
Steiner minimum tree, and hence is at most ρ·opt, where opt is the minimum-weight
of a multicast route.
Lemma 6.26 The total weight of matching M found in Algorithm 6.H is at most
opt.
Proof. Let T ∗be a minimum multicast tree in the input optical network. Starting
from the source s, perform a depth-ﬁrst search of tree T ∗. Then we obtain a tour Q
of the graph G whose total weight is at most 2 · opt. Note that the source node and
all multicast members are in the cycle Q.
Recall that the set D consists of all vertices in K with odd degrees. We connect
vertices in D along the cycle Q to get a cycle Q′ over D. The total weight of cycle
Q′ is at most 2 · opt, since the edge-weight in G satisﬁes the triangle inequality.
Since D contains an even number of vertices, the cycle Q′ can be decomposed into

238
Relaxation
two disjoint perfect matchings for D. One of them must have the total weight ≤ρ.
Therefore, the total weight of M is at most opt.
□
Theorem 6.27 Assume that NSMT has a polynomial-time ρ-approximation. Then
there is a polynomial-time (1 + ρ)-approximation for MIN-MR.
6.6
A Remark on Relaxation versus Restriction
In this and previous chapters, we have studied the restriction and relaxation tech-
niques for approximation. It is useful, however, to point out that these techniques
are only general ideas. When they are applied to speciﬁc problems, we often need
to combine them with other techniques, such as greedy strategy and two-stage ap-
proximation to make them work. Moreover, these two techniques are not mutually
exclusive. Indeed, an approximation can actually be derived from both techniques
of relaxation and restriction. Let us look at a simple example.
MULTIWAY CUT (MWC): Given a graph G = (V, E) with a nonnega-
tive edge-weight function w : E →N, and k terminals x1, . . ., xk ∈V ,
ﬁnd a minimum total-weight subset of edges that, when removed, sep-
arate all k terminals from each other.
To get some idea of the approximation for this problem, let us examine an optimal
solution C∗for MWC. Without loss of generality, we may assume that C∗has the
minimum number of edges among all optimal solutions. Then removal of edges
from C∗leaves the graph G with exactly k connected components G1, . . ., Gk,
containing k terminals x1, . . ., xk, respectively. Moreover, each edge e ∈C∗is
between two different components Gi and Gj. For each i = 1, 2, . . ., k, let
Ci = {{u, v} ∈C∗| u ∈Gi, v ∈Gj for some j ̸= i}.
Then each Ci, with 1 ≤i ≤k, is a cut separating xi from other terminals, and each
edge {u, v} ∈C∗appears in exactly two Ci’s.
Motivated by the above fact, we can design the following approximation algo-
rithm for MWC:
Algorithm 6.I (Approximation Algorithm for MWC)
Input: A graph G = (V, E) with an edge-weight function w : E →Z+ and k
terminals x1, . . ., xk ∈V .
(1) For i ←1 to k do
compute a minimum weight cut Di separating xi from other
terminals
(2) Output C ←k
i=1 Di.
It is well known that the minimum cut separating a terminal from some other
terminals can be found in polynomial time. In addition, it is easy to see that this
algorithm has a performance ratio 2.

6.6 Relaxation versus Restriction
239
original
relaxation
relaxation
relaxation
1
2
k
restriction
Figure 6.8:
Relaxation and restriction.
Theorem 6.28 Algorithm6.I is a polynomial-time2-approximationfor the problem
MWC.
Proof. Since each Di, for 1 ≤i ≤k, is the minimum cut separating xi from other
terminals, we have w(Di) ≤w(Ci) for every i = 1, 2, . . ., k. It follows that
w(C) ≤
k

i=1
w(Di) ≤
k

i=1
w(Ci) = 2w(C∗).
□
Now, let us examine the techniques used in the design of the above 2-approxima-
tion. First, we may view it as a two-stage relaxation algorithm. That is, we ﬁrst relax
the requirement of a multiway cut for all k terminals to a simpler requirement of
cutting one terminal from the other k −1 terminals. This relaxation generates k new
relaxed problems. Then, in the second stage, we combine the k optimal solutions for
the k relaxed problems to an approximate solution for the original problem. Indeed,
this type of two-stage approximation by relaxation is quite popular.
On the other hand, we may also consider the design of Algorithm 6.I as a restric-
tion method. Namely, we restrict the feasible solutions to be the union of k solutions
Di each a minimum solution for separating one terminal from all other k −1 ter-
minals. As illustrated in Chapters 3 and 4, this type of restriction of the feasible
solutions to the unions of solutions of subproblems is also very popular.
The ideas behind these two viewpoints can be seen more clearly in Figure 6.8. In
particular, when we relax the original problem into k new relaxed subproblems, the
combined solution of the solutions from these relaxed subproblems is a restricted
solution to the original problem. In addition, the restriction we imposed on the prob-
lem requires us to ﬁrst solve k relaxed problems.

240
Relaxation
From this example, we see that although the relaxation and restriction techniques
are based on different ideas, they can be applied together in a single approximation
algorithm. Indeed, the two techniques are complementary and cannot be strictly
separated. In some cases, mixing the two techniques together might produce better
approximations that cannot be achieved by a single technique.
Exercises
6.1 Let S be an input instance of the problem SS. A minimum assignment in
F (S) is canonical if every string s in S belongs to a cycle whose weight is the
smallest among all cycles that embed s. Prove the following:
(a) Every minimum assignment can be transformed into one in the canonical
form in time O(nL), where L is the total length of the strings in S.
(b) Let C1 and C2 be two cycles in a canonical minimum assignment and s1,
s2 two strings belonging to C1, C2, respectively. Then
|ov(s1, s2)| + |ov(s2, s1)| < max{|s1|, |s2|} + min{d(C1), d(C2)}.
6.2 Show that two disks with radius 1 and center distance at most 1 can cover at
most eight points that are apart from each other with distance bigger than 1. Use this
fact to show that, in any unit disk graph G, the size of a maximal independent set is
upper-bounded by (3.8)opt + 1.2, where opt is the size of the minimum connected
dominating set of G.
6.3 Give an example to show that two disks with radius 1 and center distance at
most 1 can cover nine points that are apart from each other with distance at least 1.
6.4 Consider a unit disk graph G. For any vertex subset C of G, deﬁne f(C) to
be the number of connected components of the subgraph of G induced by C. The
following is a greedy algorithm to connect a maximal independent set D of G into
a connected dominating set C.
(1) C ←D.
(2) While f(C) ≥2 do
choose a vertex x to maximize −Δxf(C);
C ←C ∪{x}.
(3) Return C.
Show that this algorithm returns a connected dominating set of G of size at most
(6 + ln4)opt if |D| ≤4 · opt + 1, where opt is the size of a minimum connected
dominating set of G.
6.5 Given four unit disks with one of them containing the centers of the other
three, how many points can be covered by these four disks such that the distance
between any two of the points is greater than 1/2?

Exercises
241
6.6 A unit ball is a ball in the three-dimensional space with radius 1/2. Prove the
following:
(a) A unit ball can touch at most 12 unit balls without incurring any interior
intersection point between any two balls.
(b) A unit ball can contain up to 12 points that are apart from each other with
distance greater than 1/2.
6.7 A graph is called a unit ball graph if each vertex is associated with a unit
ball in the three-dimensional Euclidean space such that an edge {u, v} exists if and
only if the two unit balls associated with u and v have a nonempty intersection.
Show that, in a unit ball graph, every maximal independent set has size at most
11 · opt + 1, where opt is the size of a minimum connected dominating set.
6.8 Let D be a maximal independent set in a unit disk graph G, of which every
proper subset D′ ⊆D is within distance 2 from D \ D′. Show that the following
algorithm uses at most 3 · opt vertices to connect D into a connected dominating
set, where opt is the size of a minimum connected dominating set of G.
(1) Color all vertices in D in black and all other vertices in gray.
(2) While there exists a gray vertex x adjacent to at least three black
components do
Change the color of x to black.
(3) While there exists a gray vertex x adjacent to at least two black
components do
Change the color of x to black.
(4) Return all black vertices.
6.9 Show that Algorithm 6.E is a (6 7
18)-approximation for the problem CDS-
UDG.
6.10 Design a polynomial-time greedy approximation for the minimum con-
nected dominating set in a unit ball graph that produces a solution of size at most
(13 + ln 10)opt + 1, where opt is the size of the minimum connected dominating
set.
6.11 Show that there exists a polynomial-time algorithm that, on a given con-
nected graph G, ﬁnds a connected dominating set of G with the minimum diameter.
6.12 Show that there exists a polynomial-time algorithm that, on a given con-
nected graph G, ﬁnds a connected dominating set C of G with the properties of
|C| ≤α · optS
and

242
Relaxation
diameter(C) ≤β · optD,
for some constants α, β > 1, where optS is the size of a minimum connected domi-
nating set of G, and optD is the diameter of a minimum-diameter connected domi-
nating set of G.
6.13 Let D be a maximal independent set in a unit disk graph G. Consider the
following algorithm to connect set D into a connected dominating set:
While there exist u, v ∈D with distance 3 do
connect u and v by adding all vertices on the shortest path between
u and v to D.
Return D.
Show that the size of the connected dominating set obtained by this algorithm is at
most 192 · opt + 48, where opt is the size of a minimum connected dominating set
of G.
6.14 A wireless network with different transmission ranges can be formulated
as the following disk graph: Each vertex u is associated with a disk centered at u
having radius equal to its transmission range. An edge exists between two vertices
u and v if and only if the disk u covers the vertex v and the disk v covers the vertex
u. Let G be such a disk graph. Prove the following:
(a) Every maximal independent set of G has size at most K · opt, where
K =
/ 5,
if rmax/rmin = 1,
10
ln(rmax/rmin)
ln(2 cos(π/5))

,
otherwise,
rmax (and rmin) is the maximum (and, respectively, minimum) radius of
disks in G, and opt is the size of a minimum connected dominating set of
G.
(b) There is a polynomial-time(2+ln K)-approximation for the minimum con-
nected dominating set of G.
6.15 Consider the following problem:
Given a vertex-weighted graph G = (V, E) and a vertex subset A ⊆V ,
ﬁnd a Steiner tree interconnecting the vertices in A with the minimum
total vertex weight.
Show that this problem has a polynomial-time (2 ln n)-approximation, where n =
|V |.

Historical Notes
243
6.16 Consider the following problem:
Given a vertex-weighted strongly connected digraph G = (V, E), ﬁnd a
strongly connected dominating set of G with the minimum total vertex
weight.
Show that this problem has a polynomial-time (2 ln n)-approximation, where n =
|V |.
6.17 Consider the following problem:
Given a vertex-weighted connected graph G = (V, E), ﬁnd a connected
dominating set of G with the minimum total vertex weight.
Show that this problem has a polynomial-time ( 3
2 ln n)-approximation, where n =
|V |.
6.18 Show that the problem SCDS has a polynomial-time (3 lnn)-approxima-
tion, where n = |V |.
6.19 Show that the following algorithm is a 3-approximation for MIN-MR:
(1) Construct a graph G′ and edge-weight w from the input network as de-
scribed in Stage 1 of Algorithm 6.H.
(2) Construct a traveling salesman tour Q in G′ with Christoﬁdes’s approxima-
tion (Algorithm 1.H).
(3) Traverse along the tour Q, starting from the source vertex, to all multicast
members. Convert this path in G′ into a route in the original optical network.
Historical Notes
The 3-approximation for SS was given by Blum et al. [1991]. The performance ratio
3 has been improved subsequently to 2.889 by Teng and Yao [1997], to 2.833 by
Czumaj et al. [1994], to 2.793 by Kosaraju et al. [1994], and to 2 2
3 by Armen and
Stein [1996].
Connected dominating sets have important applications in multicast routing in
wireless sensor networks (called virtual backbone in the literature of wireless net-
works). Much effort has been made to ﬁnd approximations for the minimum con-
nected dominating sets; see Das and Bhaghavan [1997], Sivakumar et al. [1998],
Stojmenovic et al. [2002], Wu and Li [1999], Wan et al. [2002], Chen and Liestman
[2002], and Alzoubi et al. [2002].
Guha and Khuller [1998a] showed a two-stage greedy (lnΔ + 3)-approximation
for the minimum connected dominating sets in general graphs where Δ is the
maximum degree in the graph. They also gave a lower bound (ln Δ + 1) for
any polynomial-time approximation for the minimum connected dominating set,
provided NP ̸⊆DTIME(nlog log n). Ruan et al. [2004] found a one-stage greedy
(lnΔ + 2)-approximation.

244
Relaxation
Cheng et al. [2003] showed the existence of a PTAS for the minimum connected
dominating sets in unit disk graphs. However, its high running time makes it hard to
implement in practice. The two-stage approximation is a popular idea to construct
connected dominating sets in unit disk graphs; see Wan et al. [2002], Alzoubi et al.
[2002], Wan et al. [2008], Li et al. [2005], Cadei et al. [2002], Funke et al. [2006],
and Min et al. [2006]. Among these approximations, the best performance ratio is
6 7
18 of Wan et al. [2008].
The (4 lnn)-approximation for SCDS of Section 6.4 was given by Li, Du et al.
[2008]. It has been improved to a (3 ln n)-approximation by Li et al. [2009]. The
spider decomposition technique was ﬁrst used by Klein and Ravi [1995] in their
analysis of an algorithm for vertex-weighted Steiner trees (Exercise 6.15). Guha
and Khuller [1998b] applied this technique to get an improvement to the weighted
dominating set problem.
For the problem MIN-MR, Yan et al. [2003] gave the ﬁrst polynomial-time
approximation (Algorithm 6.G). Suppose ρ is the performance ratio of the best
polynomial-timeapproximation for the Steiner minimum tree known today, then the
performance ratio of Algorithm 6.G is 2ρ ≈3.1. Du et al. [2005] improved it to a 3-
approximation (see Exercise 6.19). Guo et al. [2005] further improved it by giving a
(1 + ρ)-approximation (Algorithm 6.H). The polynomial-time 2-approximation for
MWC (Algorithm 6.I) is from Dahlhaus et al. [1994].

7
Linear Programming
People take the longest possible paths, digress to
numerous dead ends, and make all kinds of mistakes.
Then historians come along and write summaries of this messy,
nonlinear process and make it appear like a simple, straight line.
— Dean Kamen
A widely used relaxation technique for approximation algorithms is to convert an
optimization problem into an integer linear program and then relax the constraints
on the solutions allowing them to assume real, noninteger values. As the optimal
solution to a (real-valued) linear program can be found in polynomial time, we can
then solve the linear program and round the solutions to integers as the solutions for
the original problem. In this chapter, we give a brief introduction to the theory of
linear programming and discuss various rounding techniques.
7.1
Basic Properties of Linear Programming
Recall that an optimization problem is usually of the following form:
minimize (or, maximize)
c(x1, x2, . . ., xn)
subject to
(x1, x2, . . ., xn) ∈Ω,
where c is a real-valued objective function and Ω ⊆Rn is the feasible region of the
problem. An optimization problem is called a linear program (LP) if its objective
function c is a linear function and its feasible region is constrained by linear equa-
tions and/or linear inequalities. Moreover, if its variables are required to be integers,
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9_7, 
© Springer Science+Business Media, LLC 2012 
245

246
Linear Programming
−
x
x
= 1
x
x
+ x
= 0
x
x
+ x
= 1/2
2
2
 1
2
  1
 1
 1
2
Figure 7.1:
Feasible region of a linear program.
then it is called an integer linear program (ILP). For instance, the following is a
linear program:
minimize
x1 + x2
subject to
x1 −x2 ≤1,
x1 ≥0, x2 ≥0.
(7.1)
In this example, the feasible region is constrained by three linear inequalities and
can be easily seen to be a two-dimensional polyhedron as shown in Figure 7.1. Its
objective function x1 +x2 reaches the minimum at point (0, 0), which is a vertex of
the polyhedron.
As another example, consider the problem KNAPSACK studied in Section 1.1. If
we replace the constraints “xi ∈{0, 1}” by “0 ≤xi ≤1,” for all i = 1, 2, . . ., n,
then we obtain the following linear program:
maximize
c1x1 + c2x2 + · · · + cnxn
subject to
s1x1 + s2x2 + · · · + snxn ≤S,
0 ≤xi ≤1,
for i = 1, 2, . . ., n,
where c1, . . ., cn, s1, . . ., sn, S are nonnegative real numbers. If S ≥n
i=1 si, then
(x1, x2, . . ., xn) = (1, 1, . . ., 1) is a trivial optimal solution. Otherwise, the optimal
solution can be computed as follows: First, sort all ci/si in nonincreasing order.
Without loss of generality, assume c1/s1 ≥c2/s2 ≥· · · ≥cn/sn. Let k satisfy
k

i=1
si ≤S <
k+1

i=1
si.
Then the following is the optimal solution:

7.1 Basic Properties
247
xi =
⎧
⎨
⎩
1,
if 1 ≤i ≤k,
(S −k
i=1si)/sk+1,
if i = k + 1,
0,
if k + 2 ≤i ≤n.
In fact, replacing the constraints “xi ∈{0, 1}” by “0 ≤xi ≤1,” for 1 ≤i ≤n,
is equivalent to allowing each item to be cut into smaller pieces of arbitrary size.
Therefore, the best strategy for Ali Baba in this situation is to ﬁll the knapsack with
items in the decreasing order of the density ci/si. We note that this optimal solution
has at most one nonintegral component; that is, at most one item is to be cut into
smaller pieces. Thus, if we give up this item, then we get an approximate solution
to the original KNAPSACK problem within the difference of max{ci | 1 ≤i ≤n}
from the optimal solution. This is the essential idea of the greedy algorithm for
KNAPSACK in Section 1.1.
Now, let us extend this idea to study a resource management problem with more
than one type of resources:
maximize
c1x1
+ c2x2
+ · · · + cnxn
subject to
a11x1 + a12x2 + · · · + a1nxn ≤b1,
a21x1 + a22x2 + · · · + a2nxn ≤b2,
· · ·
am1x1 + am2x2 + · · · + amnxn ≤bm,
xi ∈{0, 1},
for i = 1, 2, . . ., n,
(7.2)
where aij, bi, cj ∈R for all i = 1, . . ., m and j = 1, . . ., n. Following the example
of KNAPSACK, we may wish to convert this integer program into a linear program
by relaxing the constraints “xi ∈{0, 1}” to “0 ≤xi ≤1,” for 1 ≤i ≤n. Because
of the complexity of this problem, however, we need to explore the theory of linear
programming a little more before we can attack this problem.
Linear programs have a standard form as follows:
minimize
cx
subject to
Ax = b,
x ≥0,
(7.3)
where A is an m × n matrix over reals, with m ≤n, x is an n-dimensional col-
umn vector over reals, c is an n-dimensional row vector over reals, and b is an
m-dimensional column vector over reals. (For a vector x = (x1, . . ., xn) ∈Rn, we
write x ≥0 to denote that xi ≥0 for all i = 1, 2, . . ., n.) Every linear program can
be transformed into an equivalent one in the standard form. In fact, if a variable xi is
not nonnegative, then we can use two nonnegative variables to replace it; that is, set
xi = ui −vi, ui ≥0, vi ≥0. Furthermore, an inequality can also be transformed to
an equivalent equality by introducing a new nonnegative variable. For example, the
linear program (7.1) can be transformed into an equivalent one in the standard form
as follows:

248
Linear Programming
minimize
x1 + x2
subject to
x1 −x2 + w = 1,
x1 ≥0, x2 ≥0, w ≥0.
(7.4)
In the standard form of linear programming, we usually assume that rank(A) =
m. In fact, if the feasible domain is not empty, then the property rank(A) < m
means that there exist some useless constraints, and these useless constraints can be
deleted to make the rank of the coefﬁcient matrix equal to the number of rows.
It can be seen easily in Figure 7.1 that the optimal solution to (7.1) occurs at a
vertex of the feasible region. Indeed, this is a very important general property of
linear programs.
What is a vertex of the feasible region? Note that the feasible region of every
linear program is a polyhedron. A point x in a polyhedron Ω is called a vertex or an
extreme point if it has the following property:
If x = (y + z)/2 for some y, z ∈Ω, then x = y = z.
With this deﬁnition, let us ﬁrst give a formal proof for our observation.
Lemma 7.1 Let Ω = {x | Ax = b, x ≥0}. If minx∈Ω(cx) has an optimal
solution, then it can be found at one of its vertices.
Proof. Consider an optimal solution x∗with the maximum number of zero compo-
nents among all optimal solutions. We will show that x∗is a vertex of Ω. By way of
contradiction, suppose x∗is not a vertex; that is, suppose there exist y, z ∈Ω such
that x∗= (y+z)/2 but x∗, y, and z are distinct (note that if two of them are equal,
then they are all equal). Since cx∗≤cy, cx∗≤cz, and cx∗= (cy + cz)/2, we
must have cx∗= cy = cz. This means that y and z are also optimal solutions. It
follows that all feasible points on the line x∗+ α(y −x∗), α ∈R, are optimal so-
lutions. However, by the constraints xi ≥0 for i = 1, 2, . . ., n, Ω does not contain
a whole line. Thus, the line x∗+ α(y −x∗) must have a point x′ not in Ω; that is,
x′ violates at least one constraint.
Note that for any α, A(x∗+ α(y −x∗)) = b. Thus, x′ cannot violate constraint
Ax = b. Moreover, suppose that x∗
i = 0 for some i, 1 ≤i ≤n. Since x∗
i =
(yi + zi)/2 and yi, zi ≥0, we must have zi = yi = x∗
i = 0. Therefore, the ith
component of x∗+ α(y −x∗) is equal to 0 for any α. This means that x′ cannot
violate any constraint xi ≥0 with x∗
i = 0. Hence, x′ must violate a constraint
xj ≥0 for some j with x∗
j > 0. We claim that there must exist some β, 0 < β < 1,
such that x′′ = βx∗+ (1 −β)x′ is an optimal solution in Ω but has one more zero
component than x∗, contradicting the assumption that x∗has the maximum number
of zero components among optimal solutions.
To prove the claim, let J = {j | 1 ≤j ≤n, x′
j < 0} and, for each j ∈J, deﬁne
βj =
−x′
j
x∗
j −x′
j
.

7.1 Basic Properties
249
Note that 0 < βj < 1, for all j ∈J. Choose j0 ∈J such that βj0 is the maximum
among all βj’s. Then we can see that x′′ = βj0x∗+ (1 −βj0)x′ has the properties
x′′
j0 = 0 and x′′
j ≥0 for all j ∈{1, 2, . . ., n} −{j0}. So, x′′ is an optimal solution
in Ω. In addition, when x∗
j = 0, we must have x′
j = 0 and, hence, x′′
j = 0. Also,
since j0 ∈J, x∗
j0 > 0. Thus, x′′ has at least one more zero component than x∗. □
Since the optimal solutions occur at the vertices of the feasible region, it is useful
to give a necessary and sufﬁcient condition for a feasible point to be a vertex.
Lemma 7.2 Consider the linear program (7.3) in the standard form. Let aj, for
1 ≤j ≤n, denote the jth column of A. Then a feasible point x ∈Ω is a vertex if
and only if the vectors in {aj | 1 ≤j ≤n, xj ̸= 0} are linearly independent.
Proof. Assume {j | 1 ≤j ≤n, xj ̸= 0} = {j1, j2, . . ., jk}.
For the “if” part, suppose x = (y +z)/2 and y, z ∈Ω. Note that xj = 0 implies
yj = zj = 0. Thus, (xj1, xj2, . . ., xjk), (yj1, yj2, . . ., yjk), and (zj1, zj2, . . ., zjk)
are all solutions to the following system of linear equations (over variables uj1,
uj2, . . ., ujk):
aj1uj1 + aj2uj2 + · · · + ajkujk = b.
(7.5)
Since aj1, aj2, . . ., ajk are linearly independent, this system of linear equations has
a unique solution. Thus, (xj1, xj2, . . ., xjk) = (yj1, yj2, . . ., yjk) = (zj1, zj2, . . .,
zjk). Hence, x = y = z. This means that x is a vertex.
For the “only if” part, suppose that x is a vertex. We claim that the sys-
tem of linear equations (7.5) has a unique solution. Suppose otherwise that (7.5)
has a second solution (x′
j1, x′
j2, . . ., x′
jk) ̸= (xj1, xj2, . . ., xjk). Set x′
j = 0 for
j ∈{1, . . ., n} \ {j1, j2, . . ., jk}. Then Ax′ = b. In addition, for sufﬁciently small
α > 0, we have x + α(x′ −x) ≥0 and x −α(x′ −x) ≥0. Fix such an α and set
y = x + α(x′ −x) and z = x −α(x′ −x). Then y, z ∈Ω, x = (y + z)/2, and
x ̸= y, contradicting the fact that x is a vertex. Thus, the claim is proven. It follows
that aj1, aj2, . . ., ajk are linearly independent.
□
Recall that we may assume rank(A) = m. Thus, by Lemma 7.2, a vertex x
has at most m nonzero components. In the case of x having fewer than m nonzero
components, we can add more columns to form a maximum independent subset of
columns of A. This means that a feasible point x is a vertex if and only if there
exists a set J = {j1, . . ., jm} of m integers between 1 and n such that columns
aj1, aj2, . . ., ajm of A are linearly independent and xj = 0 for j ̸∈J.
A vertex is also called a basic feasible solution. The index subset J = {j1,
j2, . . ., jm} associated with a basic feasible solution as described above is called
a feasible basis. For any index subset J
= {j1, j2, . . ., jm}, denote AJ
=
(aj1, aj2, . . ., ajm) and xJ = (xj1, xj2, . . ., xjm)T . Then an index subset J is a
feasible basis if and only if rank(AJ) = m = |J| and A−1
J b ≥0. Given a feasible
basis J, we can determine the vertex x associated with J as follows:
xJ = A−1
J b,
x ¯
J = 0,

250
Linear Programming
where ¯J = {1, 2, . . ., n} −J. Note that if the number of nonzero components of x
is smaller than m, then x may correspond to more than one feasible basis.
A linear program is said to satisfy the nondegeneracy assumptionif the number of
nonzero components of every basic feasible solution is exactly m, or, equivalently,
for every feasible basis J, A−1
J b > 0. For a nondegenerate linear program, the
above relationship between basic feasible solutions and feasible bases is a one-to-
one correspondence.
Now, let us go back to the resource management problem (7.2). After relaxation,
we obtain the following linear program:
maximize
c1x1
+ c2x2
+ · · · + cnxn
subject to
a11x1 + a12x2 + · · · + a1nxn ≤b1,
a21x1 + a22x2 + · · · + a2nxn ≤b2,
· · ·
am1x1+ am2x2 + · · · + amnxn ≤bm,
0 ≤x1, x2, . . ., xn ≤1.
That is,
maximize
cx
subject to
Ax ≤b,
0 ≤x ≤1.
(7.6)
This linear program can be transformed into the following one in the standard form:
maximize
cx
subject to
Ax + y = b,
x + z = 1,
x ≥0, y ≥0, z ≥0.
(7.7)
It is easy to show that every vertex of the feasible region of (7.6) is transformed into
a vertex of the feasible region of (7.7) and vice versa (see Exercise 7.2).
Now, we are going to study the basic feasible solutions of (7.7). We can write
(7.7) in the matrix form as
 A Im 0
In 0 In
⎛
⎝
x
y
z
⎞
⎠=
 b
1n

,
where In is the identity matrix of order n, and 1n = (1, 1, . . ., 1



n
)T. Note that
rank
 A Im 0
In 0 In

= m + n.
Thus, every feasible basis contains m + n column indices.

7.1 Basic Properties
251
Lemma 7.3 Every basic feasible solution to (7.7) [or (7.6)] contains at most m
nonintegral components in x.
Proof. Consider a basic feasible solution (x, y, z) determined by a feasible basis J.
Observe the following facts:
(a) If J contains an index j, 1 ≤j ≤n, but not n + m + j, then we must have
zj = 0 and hence xj = 1.
(b) If J does not contain an index j, 1 ≤j ≤n, but contains n + m + j, then we
must have xj = 0.
Thus, if 0 < xj < 1, then J must contain both indices j and n+m+j. Subtracting
the (m + n + j)th column from the jth column, we obtain a column vector of the
form (aT
j , 0)T , where aj is the jth column of A. We note that all these columns
are still linearly independent. Since rank(A) ≤m, we can have at most m such
linearly independent columns. It follows that there exist at most m indices in J ∩
{1, 2, . . ., n} such that 0 < xj < 1.
□
This lemma suggests that if m is a ﬁxed integer, then we can generalize the
greedy algorithms for KNAPSACK (Algorithms 1.B and 1.C) to the resource man-
agement problem with arbitrarily small errors. (See Exercises 7.3 and 7.5.) Of
course, these generalized algorithms must contain a subroutine for solving linear
programming problems.
There are three important families of algorithms for linear programming: the
simplex method, the ellipsoid method, and the interior-point method. The simplex
method searches for the optimal solutions from a vertex to another vertex. It re-
quires, in the worst case, more than polynomial time, but it runs in polynomial time
in the average case and has been used widely in practice. The ellipsoid method is
the ﬁrst polynomial-time algorithm found for linear programming, but it is not ef-
ﬁcient in practice. The best-known running time for the ellipsoid method is O(n6).
The interior-point method runs efﬁciently both theoretically and practically. In the
interior-point method, a nonlinear potential function is introduced, and it searches
for the optimal solutions from points in the interior of the feasible region. Since it
uses a nonlinear potential function, nonlinear programming techniques can be ap-
plied in this method. The best-known running time for an interior-point algorithm is
O(n3).
Since the interior-point method involves nonlinear programming techniques, we
will not present it in this book. We include a concise presentation of the simplex
method in the next section, and a very brief discussion of the application of the
ellipsoid method in Section 7.5.
From the (worst-case) polynomial running time of the ellipsoid and interior-point
methods for linear programming, we see that, similar to the knapsack problem, it is
not hard to design a linear programming-based PTAS for the resource management
problem, when the number m of resources is ﬁxed.
Theorem 7.4 When the number m of resources is ﬁxed, the resource management
problem (7.2) has a PTAS.

252
Linear Programming
7.2
Simplex Method
The simplex method is motivated by the important observation made in Lemma 7.1:
If an optimal solution exists for a linear program, then it can be found from a vertex
of the feasible region.
Based on this observation, the simplex method starts from a vertex and, at each
iteration, moves from one vertex to another, at which the value cx of the objective
function decreases.
To describe it in detail, suppose x is a basic feasible solution associated with the
feasible basis J. Let us explain how to determine whether x is an optimal solution
and, if x is not optimal, how to ﬁnd another feasible basic solution x+ with feasible
basis J+ such that cx+ < cx. Let y be a feasible solution in Ω and yJ the vector
composed of components yj for j ∈J. From Ay = b, we know that AJyJ +
A ¯
Jy ¯
J = b; or, equivalently, yJ = A−1
J (b −A ¯
Jy ¯
J). Thus,
cy = cJyJ + c ¯
Jy ¯
J = cJA−1
J b + (c ¯
J −cJA−1
J A ¯J)y ¯
J.
(7.8)
If c ¯
J −cJA−1
J A ¯
J ≥0, then cy ≥cJA−1
J b for all feasible solutions y. In par-
ticular, if y = x, then we have y ¯
J = 0, and so cy reaches the minimum value
cJA−1
J b. It follows that x is an optimal solution and we cannot improve over it. On
the other hand, if c′
¯
J = c ¯
J −cJA−1
J A ¯
J has a negative component, say c′
ℓ< 0 for
some ℓ∈¯J, then increasing the value of xℓmay decrease the value of cx. In other
words, using ℓto replace an index in J may result in a better feasible basis. How do
we change the feasible basis? We next study this problem.
Denote (a′
ij) = A′ = A−1
J A and b′ = A−1
J b. We note that to ﬁnd the basic
feasible solution x associated with the feasible basis J = {j1, j2, . . ., jm}, we
transform the equation Ax = b into A′x = b′, and let xJ = b′
J. Now, suppose we
want to replace the ith index ji in J by ℓto get a new basis J+ = (J −{ji}) ∪{ℓ}.
We need to perform a linear transformation to change the equation
A′x = (A−1
J A)x = A−1
J b = b′
to
A′′x = (A−1
J+A)x = A−1
J+b = b′′.
Note that the ℓth column of A−1
J+A is a unit vector with value 1 in the ith component
and value 0 in other components. It means that we need to perform the following
operations to obtain (A′′, b′′) from (A′, b′):
(1) Divide the ith row of (A′, b′) by a′
iℓ.
(2) For each k, 1 ≤k ≤m, k ̸= i, subtract a′
kℓtimes the ith row from the kth
row of (A′, b′).
In particular,
b′′
i = b′
i
a′
iℓ
,
b′′
k = b′
k −a′
kℓ
b′
i
a′
iℓ
if k ̸= i.
(7.9)

7.2 Simplex Method
253
In order to make J+ = (J −{ji})∪{ℓ} a feasible basis, we must have b′′
j ≥0 for
all j = 1, 2, . . ., m. First, we must have a′
iℓ> 0 in order to have b′′
i = b′
i/a′
iℓ≥0.
Next, in order to have b′′
k ≥0 for indices k ̸= i, we must have b′
k ≥a′
kℓb′
i/a′
iℓ.
For indices k with a′
kℓ≤0, this is clearly true. For indices k with a′
kℓ> 0, this
amounts to a new requirement: We must have b′
i/a′
iℓ≤b′
k/a′
kℓ. That is, we must
choose index i by the condition
b′
i
a′
iℓ
= min
 b′
k
a′
kℓ
 1 ≤k ≤m, a′
kℓ> 0

.
(7.10)
Note that if there is an index i with a positive a′
iℓ, then an index i satisfying
(7.10) always exists. On the other hand, if a′
iℓ≤0 for all i = 1, . . ., m, then
we can see that the linear program (7.3) has no optimal solution: Suppose that we
set xj = 0 for all j ∈¯J −{ℓ}, xℓto be an arbitrary large real number, and xJ =
b′−(a′
1ℓxℓ, . . ., a′
mℓxℓ)T . Then this is a feasible solution,and the objective function
value on this feasible solution is equal to cJA−1
J b + c′
ℓxℓ. Since c′
ℓ< 0, this value
tends to −∞as xℓgoes to ∞.
In summary, for any feasible basis J, we have the following possibilities:
(1) If c′
¯
J ≥0, then the associated basic feasible solution x is an optimal solution
to the linear program.
(2) If c′¯
J has a negative component c′
ℓ< 0, but a′
iℓ≤0 for all i = 1, 2, . . ., m,
then the linear program has no optimal solution.
(3) If c′
¯
J has a negative component c′
ℓ< 0, and a′
iℓ> 0 for some i = 1, 2, . . ., m,
then we can choose index i by (7.10) and move our attention to a new feasible
basis J+ = (J −{ji}) ∪{ℓ}.
The transformation from basis J to basis J+ is called a pivot. The simplex
method begins with an initial feasible basis, and then perform a sequence of piv-
ots until it ﬁnds an optimal solution or determines that the linear program has no
optimal solution.
Algorithm 7.A (Simplex Method for LINEAR PROGRAMMING)
Input: A linear program in the standard form (7.3).
(1) Find an initial feasible basis J.
(2) Repeat the following:
(2.1) Let c′ ←c −cJA−1
J A; A′ ←A−1
J A; b′ ←A−1
J b.
(2.2) If c′ ≥0, then stop and output the current basic feasible solution
((b′
J)T, 0)T associated with J.
(2.3) If c′ has a component c′
ℓ< 0 then do
if a′
iℓ≤0 for all 1 ≤i ≤m
then stop and output “no optimal solution”
else ﬁnd an index i satisfying (7.10), and perform a
pivot at a′
iℓto get a new feasible basis J+;
let J ←J+.

254
Linear Programming
Let us ﬁrst demonstrate by a numerical example how the simplex method works.
Example 7.5 Consider the following linear program:
minimize
z = x6 + x7
subject to
x1 + 2x2 + x5 = 8,
x1 + x2 −x3 + x6 = 3,
−x1 + x2 −x4 + x7 = 1,
x1 , x2 , . . . , x7 ≥0.
To implement the simplex method, we introduce a simplex table to store all infor-
mation of the program with respect to a feasible basis J:
−z
c −cJA−1
J A
A−1
J b
A−1
J A
xJ
Before we compute the initial feasible basis, the simplex table is as follows:
0
0
0
0
0
0
1
1
8
1
2
0
0
1
0
0
3
1
1 −1
0
0
1
0
1
−1
1
0 −1
0
0
1
Assume that we select J = {5, 6, 7} as the initial feasible basis. Then the associated
basic feasible solution is (0, 0, 0, 0, 8, 3, 1)T, cJ = (c5, c6, c7) = (0, 1, 1), and
AJ = I3. From cJ and AJ, we obtain the following simplex table:
−4
0
−2
1
1
0
0
0
8
1
2
0
0
1
0
0
x5
3
1
1
−1
0
0
1
0
x6
1
−1
1
0 −1
0
0
1
x7
In the above, the vector c′ = c −cJA−1
J A has only one negative component c′
2 =
−2. In addition, b′
3/a′
3,2 is the minimum among three values of b′
k/a′
k2. So we select
a′
3,2 as the pivot element. (The pivot element is shown with a square around it.) That
is, our new feasible basis is J = {5, 6, 2}, and cJ = (c5, c6, c2) = (0, 1, 0). Let a′
3
denote the bottom row of the above simplex table. To perform the pivot at a′
3,2, we
subtract c′
2a′
3 from the top row (or, the 0th row), and subtract a′
i,2a′
3 from the ith
row, for i = 1, 2, and we obtain

7.2 Simplex Method
255
−2
−2
0
1 −1
0
0
2
6
3
0
0
2
1
0 −2
x5
2
2
0 −1
1
0
1 −1
x6
1
−1
1
0 −1
0
0
1
x2
The new c′ has two negative components: c′
1 = −2 and c′
4 = −1. We arbitrarily
let ℓ= 1 and, from (7.10), select a′
2,1 as the new pivot element. After the second
pivot, we obtain
0
0
0
0
0
0
1
1
3
0
0
3
2
1
2
1 −3
2 −1
2
x5
1
1
0 −1
2
1
2
0
1
2 −1
2
x1
2
0
1 −1
2 −1
2
0
1
2
1
2
x2
Now, the components in the top row c −cJA−1
J A are all nonnegative. It means
that J∗= {5, 1, 2} is an optimal feasible basis. Its corresponding optimal solution
is (1, 2, 0, 0, 3, 0, 0)T.
□
In the above example, the value of the objective function decreases after each
pivot. Is this true for all linear programs? In other words, does the simplex method
always halt after a ﬁnite number of pivots? From (7.8) and (7.9), we can see that
when we change the feasible basis from J to J+ = (J −{ji}) ∪{ℓ}, the value of
the objective function on the new basic feasible solution x+ becomes cJA−1
J b +
c′
ℓb′
i/a′
iℓ. This value is less than the previous value cJA−1
J b as long as b′
i > 0.
Therefore, if the linear program (7.3) satisﬁes the nondegeneracy assumption (and
so A−1
J b > 0 for all feasible bases J), then the value of the objective function
decreases after each pivot. It follows that the algorithm will terminate after a ﬁnite
number of pivots, since it must reach a new feasible basis after each pivot and the
number of the feasible basis is ﬁnite.
Theorem 7.6 Under the nondegeneracy assumption, the simplex method halts after
a ﬁnite number of pivots. It either ﬁnds an optimal solution to the linear program
(7.3) or outputs the fact that the linear program (7.3) has no optimal solution.
What will happen if the given linear program does not satisfy the nondegeneracy
assumption? In this case, the simplex method may fall into a cycle. We demonstrate
this situation in the following example.
Example 7.7 Consider the following linear program:

256
Linear Programming
minimize
z = −3
4x4 + 20x5 −1
2x6 + 6x7
subject to
x1 + 1
4x4 −8x5 −
x6 + 9x7
= 0,
x2 + 1
2x4 −12x5 −1
2x6 + 3x7 = 0,
x3 + x6 = 1,
x1 , x2 , . . . , x7 ≥0.
The following are seven simplex tables that form a cycle.
0
0
0
0
−3
4
20 −1
2
6
0
1
0
0
1
4
−8 −1
9
x1
0
0
1
0
1
2
−12 −1
2
3
x2
1
0
0
1
0
0
1
0
x3
0
3
0
0
0 −4 −7
2
33
0
4
0
0
1
−32 −4
36
x4
0
−2
1
0
0
4
3
2
−15
x2
1
0
0
1
0
0
1
0
x3
0
1
1
0
0
0
−2
18
0
−12
8
0
1
0
8
−84
x4
0
−1
2
1
4
0
0
1
3
8 −15
4
x5
1
0
0
1
0
0
1
0
x3
0
−2
3
0
1
4
0
0
−3
0
−3
2
1
0
1
8
0
1 −21
2
x6
0
1
16 −1
8
0 −3
64
1
0
3
16
x5
1
3
2
−1
1 −1
8
0
0
21
2
x3
0
−1
1
0 −1
2
16
0
0
0
2
−6
0 −5
2
56
1
0
x6
0
1
3
−2
3
0 −1
4
16
3
0
1
x7
1
−2
6
1
5
2 −56
0
0
x3

7.2 Simplex Method
257
0
0 −2
0 −7
4
44
1
2
0
0
1 −3
0 −5
4
28
1
2
0
x1
0
0
1
3
0
1
6
−4 −1
6
1
x7
1
0
0
1
0
0
1
0
x3
0
0
0
0 −3
4
20 −1
2
6
0
1
0
0
1
4
−8 −1
9
x1
0
0
1
0
1
2
−12 −1
2
3
x2
1
0
0
1
0
0
1
0
x3
□
In order to prevent the algorithm from falling into a cycle, we need to employ
additional rules for the choice of the pivot element a′
iℓ. One such rule is the lexico-
graphical ordering method. In the following, we discuss how this rule works.
First, let us explain what the lexicographical ordering <L is. Consider two vec-
tors x = (x1, x2, . . ., xn) and y = (y1, y2, . . ., yn). The vector x is said to be
lexicographically less than the vector y, written as x <L y, if x1 = y1, . . ., xi−1 =
yi−1, and xi < yi, for some 1 ≤i ≤n. A vector x is said to be lexicographically
positive if x >L 0.
The lexicographical ordering method makes the following modiﬁcations on the
simplex method:
(1) In step (1) of Algorithm 7.A, after the initial feasible basis J is found, re-
arrange the ordering of n columns such that the initial feasible basis J is placed at
the ﬁrst m columns. This ensures that every row in the initial simplex table, except
the top row (i.e., c −cJA−1
J A), is lexicographically positive.
(2) In the “else” clause of step (2.2), instead of using (7.10) to choose the index
i, we choose i by the following new rule:
 b′
i
a′
iℓ
, a′
i1
a′
iℓ
, . . ., a′
in
a′
iℓ

= min
L
 b′
k
a′
kℓ
, a′
k1
a′
kℓ
, . . ., a′
kn
a′
kℓ
  1 ≤k ≤m, a′
kℓ> 0

,
where min
L
denotes the minimum element under the lexicographical ordering (i.e.,
for every row k with a′
kℓ> 0, divide it by a′
kℓ> 0, and then choose the lexico-
graphically smallest row i among these rows).
The above new rule (2) guarantees that the lexicographical positiveness of all
rows other than the top row is preserved under pivoting. For instance, suppose that
we select a′
iℓ, with i > 1, as the pivot element under the new rule. Also, suppose that,
for some k ̸= i, a′
kℓ, a′
k1, and a′
i1 are all positive, and b′
k/a′
kℓ= b′
i/a′
iℓ, a′
k1/a′
kℓ>
a′
i1/a′
iℓ. Then after the pivoting, we get b′′
k = b′
k −a′
kℓb′
i/a′
iℓ= 0, and a′′
k1 =
a′
k1 −a′
kℓa′
i1/a′
iℓ> 0, and row k is still lexicographically positive.
Now, we note that all rows other than the top row are lexicographically positive.
In addition, since c′
ℓ< 0 and a′
iℓ> 0, each pivot adds to the top row with a positive
constant times one of the nontop rows. As a result, each pivot makes the top row

258
Linear Programming
increase strictly in the lexicographical ordering. Therefore, the modiﬁed simplex
algorithm visits each feasible basis at most once and the objective function value is
nonincreasing. It follows that it must halt after a ﬁnite number of pivots.
Theorem 7.8 The simplex method with the additionallexicographical ordering rule
always halts in a ﬁnite number of pivots, and it either ﬁnds an optimal solution to
the linear program (7.3) or outputs the fact that the linear program (7.3) has no
optimal solutions.
Example 7.9 We observe that in the initial simplex table of Example 7.7, there are
two choices of the pivot element: a1,4 = 1/4 and a2,4 = 1/2, because b1/a1,4 =
b2/a2,4 = 0. In Example 7.7, we arbitrarily chose the element a1,4 as the pivot
point, and ended up in a cycle. If we apply the lexicographical ordering rule to this
table, we can break the tie between b1/a1,4 and b2/a2,4 by comparing
a1,1
a1,4
= 4 > 0 = a2,1
a2,4
,
and choosing instead a2,4 as the pivot element. With respect to this pivot element,
our new simplex table is as follows:
0
0
3
2
0
0
2 −5
4
21
2
0
1 −1
2
0
0
−2
−3
4
15
2
x1
0
0
2
0
1 −24
−1
6
x4
1
0
0
1
0
0
1
0
x3
From this table, a′
3,6 is the unique choice as the new pivot element, and the new
simplex table becomes
5
4
0
3
2
5
4
0
2
0
21
2
3
4
1 −1
2
3
4
0
−2
0
15
2
x1
1
0
2
1
1 −24
0
6
x4
1
0
0
1
0
0
1
0
x6
Since the top row is all nonnegative, we see that x = (3/4, 0, 0, 1, 0, 1, 0)T is an
optimal solution, with the objective function value equal to −5/4.
□
We summarize the relationship between the feasible basis and the optimal solu-
tion of a linear program as follows:
Theorem 7.10 If a linear program (7.3) has an optimal solution, then it has an
optimal basic feasible solution that is associated with a feasible basis J satisfying
c −cJA−1
J A ≥0. Moreover, if a feasible basis J satisﬁes c −cJA−1
J A ≥0, then
the basic feasible solution associated with J is optimal.

7.3 Combinatorial Rounding
259
7.3
Combinatorial Rounding
Many combinatorial optimization problems can be transformed into integer linear
programs. By extending the feasible domain to allow real, noninteger numbers, we
can relax an integer linear program to a linear program. Rounding the optimal so-
lution of the resulting linear program to a feasible solution of the original combi-
natorial problem produces an approximation. This is a general approach to ﬁnding
an approximation for a wide range of combinatorial optimization problems. In this
section, we study some simple examples using this approach.
In Section 2.4, we studied the weighted vertex cover problem MIN-WVC and
showed that the greedy algorithm for MIN-WVC has an H(δ)-approximation,
where δ is the maximum degree of the input graph. On the other hand, the un-
weighted version MIN-VC of this problem has a simple 2-approximation based on
matching (see Exercise 1.10). It is therefore natural to ask whether this algorithm
can be extended to the weighted version MIN-WVC with a better performance ra-
tio than that of the greedy algorithm. To answer this question, we show, in the fol-
lowing, how to apply the linear programming approach to this problem to get a
2-approximation.
First, we transform the problem MIN-WVC into a 0–1 integer linear program.
Suppose V
= {v1, v2, . . ., vn}. We represent a subset C ⊆V by n variables
x1, x2, . . ., xn, with xi = 1 if vi ∈C and xi = 0 otherwise, for i = 1, 2, . . ., n. Let
wi be the weight of vertex vi. Then every vertex cover C corresponds to a feasible
solution x in the following integer program, and the minimum-weight vertex cover
corresponds to the optimal solution of this integer program:
minimize
w1x1 + w2x2 + · · ·+ wnxn
subject to
xi + xj ≥1,
{vi, vj} ∈E,
xi = 0 or 1,
i = 1, 2, . . ., n.
(7.11)
By relaxing the constraints of xi = 0 or 1, for 1 ≤i ≤n, to the constraints of
0 ≤xi ≤1 on real numbers xi, for 1 ≤i ≤n, this integer program is turned into a
linear program:
minimize
w1x1 + w2x2 + · · ·+ wnxn
subject to
xi + xj ≥1,
{vi, vj} ∈E,
0 ≤xi ≤1,
i = 1, 2, . . ., n.
(7.12)
By solving this linear program (7.12) and rounding its optimal solution to the
nearest integers, we obtain an approximation for MIN-WVC:
Algorithm 7.B (Linear Programming Approximation for MIN-WVC)
Input: A graph G = (V, E) and a function w : V →N.
(1) Convert the input into a 0–1 integer program (7.11), and construct the corre-
sponding linear program (7.12).

260
Linear Programming
(2) Find an optimal solution x∗to the linear program (7.12).
(3) For i ←1, 2, . . ., n do
set xA
i =
1,
if x∗
i ≥1/2,
0,
otherwise.
(4) Output xA.
For each {vi, vj} ∈E, since x∗
i + x∗
j ≥1, at least one of x∗
i or x∗
j must be
greater than or equal to 1/2. Therefore, at least one of xA
i or xA
j is equal to 1. This
guarantees that xA is a feasible solution to (7.11). In addition, it is clear that
n

i=1
wixA
i ≤2
n

i=1
wix∗
i
and that the optimal value of the objective function of (7.11) is no smaller than
n
i=1 wix∗
i . Therefore, the following theorem is proven.
Theorem 7.11 Algorithm 7.B is a polynomial-time 2-approximation for MIN-
WVC.
In the above algorithm, the method we used to construct the approximate solu-
tion xA from an optimal solution x∗to the linear program (7.12) is called threshold
rounding. Next, we present another example of using the threshold rounding tech-
nique. Recall that a Boolean formula F is in conjunctive normal form (CNF) if it is
a product of a ﬁnite number of clauses. If, in addition, each clause in a CNF formula
F contains exactly two literals, then we say F is in 2-CNF.
MINIMUM 2-SATISFIABILITY (MIN-2SAT): Given a Boolean formula
in 2-CNF, determine whether it is satisﬁable and, if it is, ﬁnd a satisfy-
ing assignment that contains a minimum number of true variables.
MIN-2SAT can be seen as a generalization of the vertex cover problem MIN-VC.
In fact, for each graph G = (V, E), we can construct a 2-CNF F(G) as follows: For
each vertex vi ∈V , deﬁne a Boolean variable xi, and for each edge {vi, vj} ∈E,
deﬁne a clause (xi ∨xj). Then each vertex cover of G corresponds to a satisfying
assignment of F(G). Furthermore, the graph G has a vertex cover of size k if and
only if F (G) has a satisfying assignment with k true variables.
Similar to the problem MIN-VC, the problem MIN-2SAT can be transformed
into a 0–1 integer program. Consider a 2-CNF formula F. Suppose that F has n
Boolean variables x1, x2, . . ., xn. We will use the same symbols x1, . . ., xn to de-
note the corresponding 0–1 integer variables. Then the problem MIN-2SAT is equiv-
alent to the following integer program:

7.3 Combinatorial Rounding
261
minimize
x1 + x2 + · · · + xn
subject to
xi + xj ≥1,
for each clause (xi ∨xj) in F ,
(1 −xi) + xj ≥1,
for each clause (¯xi ∨xj) in F ,
(1 −xi) + (1 −xj) ≥1,
for each clause (¯xi ∨¯xj) in F ,
xi = 0 or 1,
i = 1, 2, . . ., n.
(7.13)
Relaxing the constraints of xi = 0 or 1 for i = 1, 2, . . ., n, to the constraints of
0 ≤xi ≤1 for i = 1, 2, . . ., n, we obtain the following linear program:
minimize
x1 + x2 + · · · + xn
subject to
xi + xj ≥1,
for each clause (xi ∨xj) in F ,
(1 −xi) + xj ≥1,
for each clause (¯xi ∨xj) in F ,
(1 −xi) + (1 −xj) ≥1,
for each clause (¯xi ∨¯xj) in F ,
0 ≤xi ≤1,
i = 1, 2, . . ., n.
(7.14)
Suppose x∗is an optimal solution to (7.14). We may try to apply threshold round-
ing to x∗to get an approximate solution xA to (7.13). For instance, we can set
xA
i = 1 if x∗
i > 1/2 and xA
i
= 0 if x∗
i < 1/2. This will satisfy all inequalities
in which at least one variable xi has x∗
i ̸= 1/2. However, it is not clear how to
determine the value of xA
i when x∗
i = 1/2. For instance, if F contains both clauses
(xi ∨xj) and (¯xi ∨¯xj) and if x∗
i = x∗
j = 1/2, then neither xi = xj = 0 nor
xi = xj = 1 can satisfy both clauses.
What should we do in this case? We ﬁrst note that since this problem is a gener-
alization of MIN-WVC, we expect that the approximation algorithm based on the
linear program (7.14) has a performance ratio at least 2. Now, let F1 be the set of all
clauses in F both of whose two variables have x∗value equal to 1/2. We observe
that for variables in F1, the rounding of their values to either 1 or 0 keeps the per-
formance ratio within constant 2. Thus, all we have to do is to ﬁnd any satisfying
assignment for F1, without having to minimize the number of true variables in F1.
Based on this idea, we have the following approximation algorithm for MIN-2SAT.
Algorithm 7.C (Linear Programming Approximation for MIN-2SAT)
Input: A 2-CNF formula F over variables x1, x2, . . ., xn.
(1) Convert formula F into a linear program (7.14) and ﬁnd an optimal solution
x∗for it.
(2) For i ←1 to n do
if x∗
i > 1/2 then xA
i ←1
else if x∗
i < 1/2 then xA
i ←0.
(3) Let F1 be the collection of all clauses both of whose two variables have x∗
value equal to 1/2, and let J ←{j | 1 ≤j ≤n, xj is in F1}.
(4) For i ←1 to n do
if x∗
i = 1/2 and i ̸∈J then xA
i ←0.

262
Linear Programming
(5) If F1 is satisﬁable
then let xA
J be a satisfying assignment for F1 and output xA
else output “F is not satisﬁable.”
It is easy to see that if F is satisﬁable, then the solution xA generated by Al-
gorithm 7.C is a feasible solution to (7.13). First, by step (5), we know that every
clause in F1 is satisﬁed by xA. For a clause (xi ∨xj) not in F1, we must have either
x∗
i > 1/2 or x∗
j > 1/2 since x∗
i + x∗
j ≥1. Thus, by step (2), either xA
i
= 1 or
xA
j = 1, and so xA satisﬁes the clause (xi ∨xj). A similar argument applies to
other types of clauses, such as (xi ∨¯xj) or (¯xi ∨¯xj).
In addition, we note that xA
i ≤2x∗
i for each i = 1, 2, . . ., n. Therefore, xA is an
approximation of performance ratio ≤2.
It remains to prove that Algorithm 7.C runs in polynomial time. To see this, we
only need to demonstrate a polynomial-time algorithm for the following simpler
problem:
2-SAT: For a given 2-CNF formula F1, determine whether F1 is sat-
isﬁable or not, and if F1 is satisﬁable, ﬁnd a satisfying assignment for
F1.
In the following, we present an algorithm that converts the problem 2-SAT into a
graph problem and solve it in polynomial time.
Algorithm 7.D (Polynomial-Time Algorithm for 2-SAT)
Input: A 2-CNF formula F1 over variables x1, x2, . . ., xn.
(1) Construct a digraph G(F1) = (V, E) as follows:
V ←{xi, ¯xi | 1 ≤i ≤n},
E ←{(¯yi, yj), (¯yj, yi) | (yi ∨yj) is a clause in F1},
where yi denotes a literal xi or ¯xi.
(2) For i ←1 to n do
if vertices xi and ¯xi are strongly connected
then output “F1 is not satisﬁable” and halt.
(3) For i ←1 to n do
if there is a path from xi to ¯xi
then for each literal yj that is reachable from ¯xi, set τ(yj) ←1;1
if there is a path from ¯xi to xi
then for each literal yj that is reachable from xi, set τ(yj) ←1.
(4) For i ←1 to n do
if τ(xi) is undeﬁned
then for each literal yj that is reachable from xi, set τ(yj) ←1.
(5) Output τ.
1This means that if yj = xk for some variable xk, then we set τ(xk) ←1, and if yj = ¯xk, then we
set τ(xk) ←0.

7.3 Combinatorial Rounding
263
 x
 x
 x
 x
 x
 x
 x
 x
 x
1
1
2
2
4
4
5
5
3
 x3
Figure 7.2:
Digraph G(F1).
Example 7.12 We consider the formula
F1 = (¯x1 ∨x2) ∧(¯x2 ∨¯x3) ∧(¯x1 ∨x3) ∧(x3 ∨¯x4) ∧(x4 ∨x5) ∧(x1 ∨¯x4).
The corresponding graph G(F1) is shown in Figure 7.2.
Since there is a path from x1 to ¯x1, we set x1 = 0 and consequently assign 1 to ¯x4
and x5. Now, for the remaining variables x2 and x3, we arbitrarily set x2 = 1, and
consequently ¯x3 = 1. This gives us a satisfying assignment: τ(x1) = 0, τ(x2) = 1,
τ(x3) = 0, τ(x4) = 0, τ(x5) = 1.
□
Theorem 7.13 Algorithm 7.D solves the problem 2-SAT correctly in polynomial
time.
Proof. To see that Algorithm 7.D works correctly, we ﬁrst observe that the edge
(y, z) in E indicates that, for any satisfying assignment τ for F1, we must have
[τ(y) = 1 ⇒τ(z) = 1]. This property also extends to all pairs y and z for which
there is a path from y to z. Thus, if some variable xi and its negation ¯xi are strongly
connected, then F1 is unsatisﬁable. This means that step (2) of Algorithm 7.E is
correct.
Next, we consider step (3) of Algorithm7.E. We observe another important prop-
erty of the digraph G(F1): If there is a path from a vertex y to a vertex z, then there
is a path from ¯z to ¯y. From this property, we can prove that the assignment τ in step
(3) is consistent; that is, it is not possible to assign, in step (3), both values 0 and 1
to a variable xi.
To see this, suppose that a variable w is assigned with both values 0 and 1. Then,
from the assignment τ(w) = 1, we know that there must be a path from a literal ¯u to
u and then from u to w. From the assignment τ( ¯w) = 1, we know that there must be
a path from a literal ¯v to v and then from v to ¯w. However, from the above property,
we must also have a path from ¯w to ¯u, and a path from w to ¯v. Together, they form
a cycle that passes through both vertices w and ¯w (see Figure 7.3), and Algorithm
7.E must have declared that F1 is unsatisﬁable and terminated in step (2).
The above property also extends to step (4). That is, in step (4), if xi is unassigned
and if yj is reachable from xi, then yj either is unassigned or is assigned with value
1, for, otherwise, τ(¯yj) must have the value 1 and, hence, ¯xi, which is reachable

264
Linear Programming
u
u
w
w
v
v
Figure 7.3:
A cycle passing through both u and ¯u.
from ¯yj, would have also been assigned value 1. Furthermore, we can see that if xi
is unassigned and if yj is reachable from xi, then ¯yj is not reachable from xi, for
otherwise there would be a path from yj to ¯xi, and hence a path from xi to ¯xi, which
means ¯xi should have been assigned in step (3). Therefore, the assignment of τ in
step (4) is also consistent.
Finally, we check that each clause (yi ∨yj) in F1 generates two edges ( ¯yi, yj)
and ( ¯yj, yi) in E. From steps (3) and (4), we see that it is not possible to assign
τ(yi) = τ(yj) = 0, and τ must be a satisfying assignment.
□
From the above analysis, we conclude:
Theorem 7.14 Algorithm7.C is a polynomial-time2-approximation to MIN-2SAT.
In the above example, we used a polynomial-time algorithm for 2-SAT to ﬁnd a
rounding strategy. In the next example, we use the polynomial-time algorithm for
matching to ﬁnd a rounding strategy for a scheduling problem on unrelated parallel
machines.
SCHEDULING ON UNRELATED PARALLEL MACHINES (SCHEDULE-
UPM): Given n jobs, m machines and, for each 1 ≤i ≤m and each
1 ≤j ≤n, the amount of time tij required for the ith machine to
process the jth job, ﬁnd the schedule for all n jobs on these m machines
that minimizes the makespan, i.e., the maximum processing time over
all machines.
For each pair (i, j), with 1 ≤i ≤m and 1 ≤j ≤m, let xij be the indicator for
the ith machine to process the jth job; that is, xij = 1 if the jth job is processed on
the ith machine, and xij = 0 otherwise. Then the problem SCHEDULE-UPM can
be formulated as the following ILP:
minimize
t
subject to
m

i=1
xij
= 1,
1 ≤j ≤n,
n

j=1
xijtij ≤t,
1 ≤i ≤m,
xij ∈{0, 1},
1 ≤i ≤m, 1 ≤j ≤n.

7.3 Combinatorial Rounding
265
A natural relaxation of this ILP to an LP is as follows:
minimize
t
subject to
m

i=1
xij
= 1,
1 ≤j ≤n,
n

j=1
xijtij ≤t,
1 ≤i ≤m,
0 ≤xij ≤1,
1 ≤i ≤m, 1 ≤j ≤n.
(7.15)
Consider an optimal extreme point x∗to this LP. In order to devise a feasible
rounding strategy, let us study the combinatorial properties of x∗. Let J = {j |
(∃i) 0 < x∗
ij < 1} and M = {1, . . ., m}. Deﬁne a bipartite graph H = (M, J, E)
with E = {(i, j) | 0 < x∗
ij < 1}; that is, there is an edge (i, j) connecting j to i if
and only if the jth job is partially assigned to the ith machine.
Lemma 7.15 The bipartite graph H contains a matching covering J.
Proof. It sufﬁces to show that each connected component of H contains a matching
covering all jobs in the connected component. Consider a connected component
H′ = (M ′, J′, E′) of H. For each variable xij with i ̸∈M ′ or j ̸∈J′, let us ﬁx
its value in LP (7.15) by xij = x∗
ij. Then we get a new LP over variables xij, for
i ∈M ′ and j ∈J′. It is easy to verify that x′ = (x∗
ij)i∈M′, j∈J′ is an extreme
point of this new LP. In fact, suppose x′ = (y′ + z′)/2 for some points y′, z′ in the
feasible region of the new LP. Deﬁne y to have yij = x∗
ij for i ̸∈M ′ or j ̸∈J′, and
have yij = y′
ij for i ∈M ′ and j ∈J′; also, deﬁne z to have zij = x∗
ij for i ̸∈M ′ or
j ̸∈J′, and have zij = z′
ij for i ∈M ′ and j ∈J′. Then we have x∗= (y + z)/2.
It follows that y = z = x∗and, hence, y′ = z′ = x′.
Let ak be the kth row of the constraint matrix of the LP (7.15). We say an in-
equality constraint akx ≥bk is active at a point x∗if akx∗= bk. Note that an
extreme point x′ of the new LP has |M ′| · |J′| components, and hence must be de-
termined by |M ′| · |J′| active constraints. However, for each active constraint of the
form xij ≥0 or xij ≤1, the corresponding component x′
ij must be an integer. Note
that there are only |M ′| + |J′| constraints not of such a form. Thus, x′ can have
at most |M ′| + |J′| nonintegral components. In other words, graph H′ has at most
|M ′| + |J′| edges. Since H′ is connected, H′ is either a tree or a tree plus an edge.
Case 1. H′ is a tree. Fix any vertex r ∈J′ as the root. Then H becomes a rooted
tree. Note an important fact of this tree: A vertex j ∈J ′ cannot be a leaf. To see
this, we note that for integer j ∈J′, the constraint 
i∈M′ xij = 1 on x′ implies
that 0 < x′
ij < 1 for at least two different i ∈M ′. This means that there are at
least two edges incident upon j, and so j is not a leaf. From this property, we have
a simple way to ﬁnd a matching covering J′: For each j ∈J′, match it to a child of
j in the tree.
Case 2. H′ is a tree plus an edge. This edge introduces a cycle, and H′ is a
cycle plus some trees growing out from the cycle (see Figure 7.4, in which a circle

266
Linear Programming
Figure 7.4:
H′ contains only one cycle.
◦denotes a job and a dark square
denotes a machine). Since H′ is bipartite,
the cycle has an even number of vertices and thus contains a matching covering all
vertices on the cycle. Contracting the cycle of H′ into a root point results in a rooted
tree over other vertices. Again, a nonroot vertex j ∈J′ in this tree cannot be a leaf.
Thus, for each internal, nonroot vertex j ∈J′ in the tree, we can match it to one of
its children. Together with the matching of the cycle, we obtain a matching of H′
covering every vertex in J′.
□
Lemma 7.15 means that the partially assigned jobs in J can be assigned to ma-
chines in such a way that each machine receives at most one such job. It suggests
a simple rounding strategy: First, for each job j with x∗
ij = 1 for some i ∈M,
we assign it to the ith machine. For the remaining jobs in J, we deﬁne the bipartite
graph H and ﬁnd a matching A of H that covers J, and for each j ∈J, assign it
to the ith machine if (i, j) ∈A. This rounding strategy gives us an approximation
with the makespan at most
opt +
max
1≤i≤m,1≤j≤ntij,
where opt is the minimum makespan. Thus, if we can bound the maximum tij by
c · opt for some constant c > 0, then the above rounding strategy yields a constant-
ratio approximation to SCHEDULE-UPM. Is such a bound possible? Unfortunately,
the answer is no, as tij could be, in general, much greater than opt.
On the other hand, we observe that if a value tij is greater than opt, then the
optimal solution must not assign the jth job to the ith machine. Therefore, we can
prune the variable xij from the LP (7.15), and expect to get the same solution. This
observation suggests that we set an upper bound T on tij and prune the variable xij
if tij > T. How do we ﬁnd the best value of the bound T? Since we do not know the
value of opt, we cannot just set it to opt. Instead, we can search for the minimum T
for which the following LP has a feasible solution:

7.4 Pipage Rounding
267
minimize
t
subject to

1≤i≤m, tij≤T
xij
= 1,
1 ≤j ≤n,

1≤j≤n, tij≤T
xijtij ≤t,
1 ≤i ≤m,
0 ≤xij ≤1,
1 ≤i ≤m, 1 ≤j ≤n.
(7.16)
Since the above LP (7.16) can be solved in polynomial time, we can use bisecting
to ﬁnd the minimum T for which (7.16) has a feasible solution. Denote this T as
T ∗and let x∗be an optimal extreme point of (7.16). Then T ∗≤opt and tij ≤opt
for all x∗
ij > 0. Therefore, by the rounding based on Lemma 7.15, we obtain a
polynomial-time 2-approximation to SCHEDULE-UPM.
Theorem 7.16 The problem SCHEDULE-UPM has a polynomial-time approxima-
tion with performance ratio 2.
7.4
Pipage Rounding
In this section, we introduce the idea of pipage rounding. Let us ﬁrst look at an
example.
MAXIMUM-WEIGHT HITTING (MAX-WH): Given a collection C of
subsets of a ﬁnite set E with a nonnegative weight function w on C and
a positive integer p, ﬁnd a subset A of E with |A| = p that maximizes
the total weight of subsets in C hit by A.
Assume E = {1, 2, . . ., n} and C = {S1, S2, . . ., Sm}. Denote wi = w(Si) for
1 ≤i ≤m. Let xi be a 0–1 variable that indicates whether element i is in subset A.
Then the problem MAX-WH can be formulated into the following integer program,
which, as we will see later, can be relaxed to a linear program:
maximize
L(x) =
m

j=1
wj · min

1,

i∈Sj
xi

subject to
n

i=1
xi = p,
xi ∈{0, 1},
i = 1, 2, . . ., n.
(7.17)
The following equivalent formulation of MAX-WH (as a nonlinear program) will
be useful in the rounding algorithm:

268
Linear Programming
maximize
F(x) =
m

j=1
wj ·

1 −
.
i∈Sj
(1 −xi)

subject to
n

i=1
xi = p,
xi ∈{0, 1},
i = 1, 2, . . ., n.
(7.18)
The functions L(x) and F (x) have the same value when each xi takes value
0 or 1. However, when the constraints xi ∈{0, 1} are relaxed to the constraints
0 ≤xi ≤1, they may have different values. Nevertheless, they satisfy the following
relationship.
Lemma 7.17 For the relaxed versions of (7.17) and (7.18), we must have F(x) ≥
(1 −1/e)L(x).
Proof. Consider a ﬁxed set Sj for some j = 1, 2, . . ., m. Assume that |Sj| = k.
Then, by the arithmetic mean–geometric mean inequality, we have
1 −
.
i∈Sj
(1 −xi) ≥1 −

i∈Sj (1 −xi)
k
k
= 1 −

1 −

i∈Sj xi
k
k
.
Let f(z) = 1−(1 −z/k)k. Then, for 0 ≤z ≤k, we have f′(z) = (1−z/k)k−1 ≥
0 and f′′(z) = −((k −1)/k)(1 −z/k)k−2 ≤0. Therefore, f(z) is monotone
increasing and concave in the interval [0, k]. Moreover, f(0) = 0. It follows that
f(z) ≥z · f(1) for z ∈[0, 1], and so f(z) ≥f(1) · min{1, z} for z ∈[0, k]. Note
that
f(1) = 1 −

1 −1
k
k
≥1 −1
e .
Thus,
1 −
.
i∈Sj
(1 −xi) ≥

1 −1
e

· min

1,

i∈Sj
xi

,
and the lemma is proven.
□
The relaxation of the integer program (7.17) is as follows:
maximize
L(x) =
m

j=1
wj · min

1,

i∈Sj
xi

subject to
n

i=1
xi = p,
0 ≤xi ≤1,
i = 1, 2, . . ., n.
(7.19)
We can introduce m new variables to get an equivalent LP as follows:

7.4 Pipage Rounding
269
maximize
m

j=1
wjzj
subject to

i∈Sj
xi ≥zj,
j = 1, . . ., m,
n

i=1
xi = p,
0 ≤xi ≤1,
i = 1, 2, . . ., n,
0 ≤zj ≤1,
j = 1, 2, . . ., m.
The optimal solution to this LP can be found in polynomial time. We will use
function F (x) to round the optimal solution x∗of (7.19) to get an integer solution
xA for (7.17). More precisely, we round, at each step, one or two nonintegral com-
ponents of x∗to integers, with the criterion that the rounding does not decrease the
value of F (x).
Algorithm 7.E (Pipage Rounding Algorithm for MAX-WH)
Input: A set E = {1, 2, . . ., n}, a collection C of subsets of E, a nonnegative weight
function w : C →N, and an integer p > 0.
(1) Construct the linear program (7.19) from the input, and ﬁnd an optimal solu-
tion x∗to it.
(2) x ←x∗.
(3) While x has an nonintegral component do
(3.1) Choose 0 < xk < 1 and 0 < xj < 1 (with k ̸= j);
(3.2) Deﬁne x(ε) by
xi(ε) ←
⎧
⎨
⎩
xi,
if i ̸= k, j,
xj + ε,
if i = j,
xk −ε,
if i = k;
(3.3) Let ε1 ←min{xj, 1 −xk};
ε2 ←min{1 −xj, xk};
(3.4) If F(x(−ε1)) ≥F(x(ε2)) then x ←x(−ε1) else x ←x(ε2).
(4) Output xA ←x.
We remark that, at step (3.4), we replace x by either x(−ε1) or x(ε2). In either
case, the sum n
i=1 xi remains an integer. Therefore, at step (3.1) of the next iter-
ation, x must have at least two distinct nonintegral components. Thus, Algorithm
7.E is well deﬁned.
The following is an important property of F(x(ε)).
Lemma 7.18 F(x(ε)) is convex with respect to ε.
Proof. We consider

270
Linear Programming
F(x(ε)) =
m

ℓ=1
wℓ·

1 −
.
i∈Sℓ
1 −xi(ε)
as a function of ε, with respect to a ﬁxed x and ﬁxed elements j, k ∈{1, 2, . . ., n}.
Then, for each ℓ= 1, 2, . . ., m, we consider three cases.
Case 1. Sℓcontains neither j nor k. Then the ℓth term of F(x(ε)), wℓ· (1 −

i∈Sℓ(1 −xi(ε))), is a constant with respect to ε, and so is convex.
Case 2. Sℓcontains one of j or k. Then the ℓth term of F(x(ε)) is linear with
respect to ε, and so is convex.
Case 3. Sℓcontains both k and j. Then the ℓth term of F (x(ε)) is of the form
g(ε) = wℓ−a(b + ε)(c −ε)
for some nonnegative constants a, b, and c. If a = 0, then this term is a constant wℓ
and hence convex. If a > 0, then g′′(ε) = 2a > 0, and so g(ε) is convex.
Thus, each term of F(x(ε)) is a convex function. Now, the lemma follows from
the fact that the sum of a ﬁnite number of convex functions is still convex.
□
By Lemma 7.18, max{F(x(−ε1)), F (x(ϵ2))} ≥F(x), since ε1, ε2 > 0. Thus,
the value of F(x) is nondecreasing during step (3) (called the Pipage Rounding
process) of Algorithm 7.E. Therefore, F (xA) ≥F(x∗).
Theorem 7.19 Algorithm 7.E is a polynomial-time approximation to MAX-WH
with performance ratio(e/(e −1)).
Proof. First, we note that xA has only integer components, and so F(xA) = L(xA).
It follows that
L(xA) = F(xA) ≥F(x∗) ≥

1 −1
e

L(x∗),
where x∗is the optimal solution to (7.19).
□
The above example is a typical application of the pipage rounding technique.
We can extend it to the following general setting: Consider a bipartite graph G =
(U, V, E) and an integer program with 0–1 variables xe, each associated with an
edge e ∈E, and with constraints in the form

e∈δ(v)
xe ≤pv,
or

e∈δ(v)
xe = pv,
or

e∈δ(v)
xe ≥pv,
for some v ∈U ∪V , where δ(v) is the set of all edges incident to v ∈U ∪V and
pv is a nonnegative integer. For instance, consider the following integer program:
maximize
L(x)
subject to

e∈δ(v)
xe ≤pv,
v ∈U ∪V,
xe ∈{0, 1},
e ∈E.
(7.20)

7.4 Pipage Rounding
271
(Intuitively, the above integer program asks for a subgraph G1 = (U, V, E1) of G,
with each vertex v having degree at most pv, that maximizes L(E1).)
Suppose L(x) has a companion function F(x) such that
(A1) L(x) = F(x) when xe ∈{0, 1} for all e ∈E, and
(A2) L(x) ≤c · F (x), for some constant c > 0, when 0 ≤xe ≤1 for all
e ∈E.
Further assume that
(A3) The relaxation of the integer program (7.20) is equivalent to an LP:
maximize
L(x)
subject to

e∈δ(v)
xe ≤pv,
v ∈U ∪V,
0 ≤xe ≤1,
e ∈E.
(7.21)
Then we can apply the pipage rounding technique to the optimal solution x∗of
(7.21) to obtain an integer solution xA as follows:
Pipage Rounding
(1) Initially, set x ←x∗.
(2) While x is not an integer solution do
(2.1) Let Hx be the subgraph of G induced by all edges e ∈E with 0 < xe <
1. Let R be a cycle or a maximal path of Hx. Then R can be decomposed
into two matchings M1 and M2.
(2.2) Deﬁne x(ε) by
xe(ε) =
⎧
⎨
⎩
xe,
if e ̸∈R,
xe + ε,
if e ∈M1,
xe −ε,
if e ∈M2.
(2.3) Let ε1 ←min

min
e∈M1 xe, min
e∈M2(1 −xe)

,
ε2 ←min

min
e∈M1(1 −xe), min
e∈M2 xe

.
(2.4) If F (x(−ε1)) ≥F (x(ε2)) then x ←x(−ε1) else x ←x(ε2).
Lemma 7.20 For ε ∈[−ε1, ε2], x(ε) is a feasible solution for (7.21).
Proof. First, suppose R is a cycle. Then, for each vertex v in R, there is an edge
in δ(v) ∩M1 and an edge in δ(v) ∩M2, and so 
e∈δ(v) xe(ε) = 
e∈δ(v) xe.
Therefore, x(ε) is feasible.
Next, suppose R is a maximal path. Then, by a similar argument, we know
that for each intermediate vertex v of R, 
e∈δ(v) xe(ε) = 
e∈δ(v) xe. That is,

272
Linear Programming

e∈δ(v) xe(ε) ̸= 
e∈δ(v) xe only if v is an endpoint of R. Let v be an end-
point of R and e′ ∈δ(v) ∩R. By the deﬁnitions of x(ε), ε1, and ε2, we know
that, for ε ∈[−ε1, ε2], 0 ≤xe′(ε) ≤1. In addition, we observe that for each
e ∈δ(v) \ {e′}, xe is an integer, since R is a maximal path in Hx. Therefore, we
have pv −
e∈δ(v)\{e′} xe ≥1. It follows that
pv −

e∈δ(v)
xe(ε) = pv −

e∈δ(v)\{e′}
xe −xe′(ε) ≥1 −xe′(ε) ≥0.
Again, x(ε) is feasible.
□
Finally, assume
(A4) For any R, F (x(ε)) is convex with respect to ε.
Then, the above Pipage Rounding procedure results in an integer solution xA
such that F (xA) ≥F (x∗). Therefore,
L(xA) = F (xA) ≥F (x∗) ≥c · L(x∗) ≥c · opt.
For the problem MAX-WH, we can formulate it into a star bipartite graph
G = (U, V, E), with U = {u}, V
= {v1, v2, . . ., vn}, and E = {(u, v1),
(u, v2), . . ., (u, v2)}. Each variable xi corresponds to an edge (u, vi), and the con-
straint n
i=1 xi = p becomes 
e∈δ(u) xe = p. Under this setting, the set R in step
(2.1) of the Pipage Rounding procedure is always a maximal path consisting of two
edges, which correspond to two nonintegral components xj and xk in step (3.1) of
Algorithm 7.E.
7.5
Iterated Rounding
Recall the threshold rounding technique introduced in Section 7.3. We observe that
it worked for the problem MIN-WVC, because the optimal fractional solution al-
ways has at least one variable in each clause taking value greater than or equal to
1/2. Therefore, rounding these values to 1 yields a feasible solution that is a 2-
approximation to the optimal integer solution. Suppose, however, that we are given
some additional constraints of the form 
i∈A xi ≥k. Then it is possible that there
are not enough variables taking values at least 1/2 in the optimal fractional solution
to satisfy these constraints. Thus, the solution obtained by rounding these variables
to 1 may not be feasible. What should we do in this situation? An idea is to perform
a partial rounding, that is, to round those values greater than or equal to 1/2 to 1,
and then deal with the residual linear program. In the case that the fractional opti-
mal solution of the residual linear program always contains a component of value
greater than or equal to 1/2, we can continue this rounding process and eventually
obtain a feasible integer solution that is still a 2-approximation. This is the basic
idea of iterated rounding. Now, let us apply this idea to a speciﬁc problem.
GENERALIZED SPANNING NETWORK (GSN): Given a graph G =
(V, E) with a nonnegative cost function c : E →R+ on edges, and

7.5 Iterated Rounding
273
an integer k > 0, ﬁnd a k-edge-connected subgraph with the minimum
total edge cost.
The fact that a subgraph F is k-edge-connected may be veriﬁed as follows: For
each partition (S, V −S) of the vertex set V of G, there are at least k edges in F
between S and V −S. Based on this concept, the problem GSN can be formulated
as the following ILP:
minimize

e∈E
cexe
subject to

e∈δG(S)
xe ≥k,
∅̸= S ⊂V,
xe ∈{0, 1},
e ∈E,
where δG(S) denotes the set of edges with exactly one endpoint in S. Its LP relax-
ation is as follows:
minimize

e∈E
cexe
subject to

e∈δG(S)
xe ≥k,
∅̸= S ⊂V,
0 ≤xe ≤1,
e ∈E.
(7.22)
First, we need to point out that this LP, though having more than 2|V | constraints,
can be solved in polynomial time in |V |. This fact is somewhat surprising because it
would take time 2|V | even to write down all constraints explicitly. In the following,
we present a brief description of how an algorithm based on the ellipsoid method
can solve this LP in polynomial time in |V |.
The critical idea here is that we do not need to write down all constraints explic-
itly when we employ the ellipsoid method to solve the LP (7.22). What we need is,
instead, an algorithm to ﬁnd, for any infeasible solution x, an unsatisﬁed constraint
in polynomial time in |V |. This is called a separation oracle.
More precisely, solving a linear program can be reduced to solving a system of
linear inequalities. For a system of linear inequalities, the algorithm based on the
ellipsoid method maintains an ellipsoid (initially, a ball) that contains a feasible
region of a certain volume if the system of linear inequalities has a solution. In
each iteration, it checks whether or not the center of the ellipsoid is a solution of
the system of linear inequalities. If not, it ﬁnds an unsatisﬁed constraint to cut the
ellipsoid into two halves and uses a new ellipsoid to cover the half that satisﬁes
the constraint. Moreover, the volume of the ellipsoid shrinks, in each iteration, by
a ﬁxed ratio r < 1 (which may depend on the input size n). Thus, if none of the
centers of the ellipsoids is a solution, then the volume of the ellipsoid becomes, after
a polynomial number of iterations, smaller than the volume of the possible feasible
region and the algorithm terminates, reporting that the system of linear inequalities
has no solutions.

274
Linear Programming
The solution obtained by the ellipsoid method may not be a basic feasible solu-
tion. However, from the proof of Lemma 7.1, we can easily construct a polynomial-
time algorithm to compute an optimal basic feasible solution from an optimal solu-
tion.
Thus, for a linear program with an exponential number of constraints, we can
still solve it in polynomial time as long as we can construct separation oracles in
polynomial time. In our case here, the separation oracles for the LP (7.22) can be
constructed based on the maximum-ﬂow minimum-cut theorem as follows: We ﬁrst
convert the constraints into a network ﬂow problem. That is, for a potential solution
x to (7.22), we assign, to each edge e, a capacity xe. Then x is feasible if and only
if, for every two nodes s, t of graph G, the maximum ﬂow from s to t is at least k.
Next, we compute the maximum ﬂow for each pair (s, t) of nodes of graph G. When
a pair (s, t) is found with the maximum ﬂow from s to t less than k, we know that
x is infeasible. In addition, by the maximum-ﬂow minimum-cut theorem, there is a
cut (S, V −S) with the total capacity less than k. The constraint corresponding to
this cut S is an unsatisﬁed constraint we are looking for; that is, 
e∈δG(S) xe < k.
Note that here the minimum cut (S, V −S) can be found in polynomial time in |V |,
since the input to the minimum-cut problem is just the graph G.
Next, we note that if we make a partial assignment to the variables of the LP
(7.22), the residual LP is still polynomial-time solvable with respect to |V |. In-
deed, suppose for e ∈F ⊂E, xe is already assigned value ue. Now, suppose
an assignment (xe)e∈E−F is not feasible for the residual LP. Then this assignment
(xe)e∈E−F , together with the partial assignment (ue)e∈F , forms an infeasible as-
signment for the original LP. Therefore, in polynomial time with respect to |V |, we
can ﬁnd an unsatisﬁed constraint

e∈δG(S)
xe ≥k
in the original LP. The corresponding constraint

e∈δG(S)\F
xe ≥k −

e∈F
ue
of the residual LP is then an unsatisﬁed constraint for (xe)e∈E−F . In other words,
the separation oracles for the residual LP can also be constructed in polynomial time
in |V |.
Now, let us study how iterated rounding works. First, we extend the notion of
supmodular functions, which has been studied in Chapter 2, to weakly supmodular
functions. A function f : 2V →Z is weakly supmodular if
(a) f(V ) = 0, and
(b) For any two subsets A, B ⊆V , either
f(A) + f(B) ≤f(A \ B) + f(B \ A)
or
f(A) + f(B) ≤f(A ∩B) + f(A ∪B).

7.5 Iterated Rounding
275
The following is a key lemma in the application of iterated rounding to the prob-
lem GSN. Its proof is quite involved and is postponed to the end of this section.
Lemma 7.21 Suppose f : 2V →Z is a weakly supmodular function. Then, for the
following LP,
minimize

e∈E
cexe
subject to

e∈δG(S)
xe ≥f(S),
S ⊆V,
0 ≤xe ≤1,
e ∈E,
(7.23)
every basic feasible solution x contains at least one component xe ≥1/3.
Note that the function
f(S) =
0,
if S = ∅or V,
k,
otherwise
(7.24)
is weakly supmodular. By Lemma 7.21, every basic feasible solution of (7.12) con-
tains at least one component xe ≥1/3. We round such variables xe to 1 and study
the residual LP. After setting xe = 1 for edges e ∈F for some subset F ⊆E, the
residual LP can be represented as follows:
minimize

e∈E−F
cexe
subject to

e∈δG−F (S)
xe ≥f(S) −|δF(S)|,
S ⊆V,
0 ≤xe ≤1,
e ∈E,
(7.25)
where F also represents the subgraph of G with edge set F and vertex set V . It is not
hard to verify that f(S) −|δF (S)| is still weakly supmodular (see Exercise 7.15).
By Lemma 7.21, every basic feasible solution of (7.25) must contain a component
xe ≥1/3, which can be rounded to 1. From the above analysis, we can now present
the iterated rounding algorithm for GSN as follows.
Algorithm 7.F (Iterated Rounding Algorithm for GSN)
Input: A graph G = (V, E) with an edge-cost function c : E →Q+, and an integer
k > 0.
(1) Construct an LP (7.25) with f(S) of (7.24) and F = ∅.
(2) While F is not k-edge-connected do
(2.1) Find an optimal basic feasible solution x∗of (7.25);

276
Linear Programming
(2.2) F ←F ∪{e | x∗
e ≥1/3}.
(3) Output F.
Theorem 7.22 Algorithm 7.F produces a 3-approximation for the problem GSN.
Proof. Suppose F is the output obtained from Algorithm 7.F through t iterations.
For i = 1, 2, . . ., t, let Fi be the set of edges added to F in the ﬁrst i iterations; thus,
F = Ft. Also, denote, for i = 1, 2, . . ., t, F i = E −Fi. Let xi denote the optimal
fractional solution of (7.25) with respect to F = Fi. Thus, under the condition that
xe = 1 for e ∈Fi, xi is a better solution to (7.25) than any other solution, including
xi−1. It follows that

e∈F
ce ≤

e∈Ft−1
ce + 3

e∈F t−1
cext−1
e
≤

e∈Ft−1
ce + 3

e∈F t−1
cext−2
e
≤

e∈Ft−2
ce + 3

e∈F t−2
cext−2
e
≤
· · ·
≤3

e∈E
cex0
e ≤3 · opt,
where opt is the value of optimal integer solution of (7.25) for F = ∅.
□
The rest of this section is devoted to the proof of Lemma 7.21.
We ﬁrst prove an important property of the basic feasible solutions of (7.23).
Let aS denote the row of the constraint matrix of (7.23) corresponding to a set
S ⊆V ; that is, each nonzero component of aS has value 1 and corresponds to
an edge in δG(S). So, we have aSx = 
e∈δG(S) xe. Recall that an inequality
constraint aSx ≥f(S) is active for a basic feasible solution x if the constraint
holds as an equality; that is, aSx = f(S). We say a set S ⊆V is active for x
if its corresponding constraint aS is active for x. We note that for a basic feasible
solution x with k fractional components (i.e., 0 < xe < 1 for k edges e), there are
at least k active constraints. Furthermore, the corresponding rows aS of these active
constraints have rank equal to k.
We say a set A ⊆V crosses another set B ⊆V if A \ B ̸= ∅, B \ A ̸= ∅,
and A ∩B ̸= ∅. A family F of sets is called a laminar family if no member of
F crosses another member. In the following lemma, we will show that each basic
feasible solution x of (7.23) is determined by a laminar family of active sets for x.
In the following, we assume, without loss of generality, that 0 < xe < 1 for all
e ∈E. Indeed, if xe = 0 for some edges e, we may delete these edges from G
and the proof works for the resulting graph; and if xe = 1, then Lemma 7.21 holds
trivially.
Lemma 7.23 Let x be a basic feasible solution of (7.23), with 0 < xe < 1 for all
e ∈E. Then there is a laminar family F of active sets in G such that

7.5 Iterated Rounding
277
(a) |F| = |E|,
(b) The set of vectors aS, over all S ∈F, is linearly independent, and
(c) f(S) ≥1, for all S ∈F.
Proof. It sufﬁces to show that for every maximal laminar family L of active sets,
{aS | S ∈L} has rank |E|. In fact, if this is true, then we can simply choose a
subfamily F of a maximal laminar family L such that {aS | S ∈F} forms a basis
of {aS | S ∈L}. It is clear that this laminar family F satisﬁes conditions (a) and
(b). For condition (c), we note that for an active set S, f(S) must be nonnegative.
In addition, if f(S) = 0, then aS would be equal to 0 because xe > 0 for all edges
e ∈E, contradicting condition (b). Thus, condition (c) also holds.
For the sake of contradiction, suppose that L is a maximal laminar family of
active sets such that the rank of {aS | S ∈L} is less than |E|. Let Span(L)
denote the set of all linear combinations of all aS with S ∈L. Since the set of
all active constraints has rank equal to |E|, there exists an active set A such that
aA ̸∈Span(L). Since L is maximal, A must cross a set in L. We choose A to be
the active set that crosses the minimum number of sets in L, among all active sets S
whose corresponding constraint aS is not in Span(L).
Let B ⊆V be a set in L that crosses A. Note that f is weakly supmodular. Thus,
we have either
f(A) + f(B) ≤f(A \ B) + f(B \ A)
or
f(A) + f(B) ≤f(A ∪B) + f(A ∩B).
First, we assume that
f(A) + f(B) ≤f(A \ B) + f(B \ A).
(7.26)
For two disjoint sets C, D ⊆V , let E(C, D) denote the set of all edges in E with
one endpoint in C and the other in D. Also, denote S1 = A \ B, S2 = A ∩B,
S3 = B \ A, and S4 = V −(A ∪B). For 1 ≤i, j ≤4, let
mi,j =

e∈E(Si,Sj)
xe.
Since A and B are both active, we have
f(A) = m1,3 + m1,4 + m2,3 + m2,4,
f(B) = m1,2 + m1,3 + m2,4 + m3,4.
Moreover, for constraints S1 and S3, we have
f(S1) ≤m1,2 + m1,3 + m1,4,
f(S3) ≤m1,3 + m2,3 + m3,4.

278
Linear Programming
Thus,
f(S1) + f(S3) + 2m2,4 ≤f(A) + f(B).
However, by (7.26), we know that f(A)+ f(B) ≤f(S1)+f(S3). Therefore, m2,4
must be equal to 0, and
f(A) + f(B) = f(S1) + f(S3).
It means that S1 and S3 are active. In addition, m2,4 = 0 implies E(S2, S4) = ∅,
since xe > 0 for all e ∈E. It follows that
aA + aB = aS1 + aS3.
Since aA ̸∈Span(L) and B ∈L, either aS1 or aS3 is not in Span(L).
Case 1. aS1 ̸∈Span(L). We claim that every set C ∈L crossing set S1 must
also cross set A. To see this, suppose that C ∈L crosses S1. Note that A is a
superset of S1. Therefore, S1 ∩C ̸= ∅implies A ∩C ̸= ∅, and S1 \ C ̸= ∅implies
A \ C ̸= ∅.
Furthermore, S1 ∩C ̸= ∅also implies that C \B ̸= ∅. Since B and C are both in
L, we have either B ⊂C or B ∩C = ∅. In either case, we must have C \ A ̸= ∅: If
B ⊂C, then (C\A) ⊇(B\A) ̸= ∅, and if B∩C = ∅, then (C\A) = (C\S1) ̸= ∅.
It follows that C crosses A, and the claim is proven.
Now we observe that set B crosses A but does not cross S1. Together with the
above claim, we see that the number of sets in L crossing S1 is strictly less than the
number of sets in L crossing A. This is a contradiction to our choice of A.
Case 2. aS3 ̸∈Span(L). Then, similar to Case 1, we claim that every set C in L
crossing S3 must also cross A. To prove this claim, suppose that C ∈L crosses S3.
Then S3 \ C ̸= ∅implies B \ C ̸= ∅, and S3 ∩C ̸= ∅implies B ∩C ̸= ∅. Since B
and C are both in L, we must have C ⊂B. It follows that ∅̸= (C \ S3) ⊆(A∩C).
Moreover, (A \ C) ⊃(A \ B) ̸= ∅and (C \ A) = (C ∩S3) ̸= ∅. Therefore, C
crosses A.
Now, we observe that set B crosses A, but not S3, and this, together with the
claim, leads to a contradiction to our choice of A.
Finally, we note that for the case
f(A) + f(B) ≤f(A ∪B) + f(A ∩B),
a contradiction can be derived by a similar argument.
□
Next, we use a counting argument to show a nice property of the laminar family
F given by Lemma 7.23.
Lemma 7.24 Suppose xe is fractional for every e ∈E. Then the laminar family F
of Lemma 7.23 contains a set S with |δG(S)| ≤3.
Proof. Suppose to the contrary that for every S ∈F, |δG(S)| ≥4. We construct a
forest T over set F such that (A, B) is an edge in T if and only if A ⊃B and there
is no other set C such that A ⊃C ⊃B. (Note that if A ⊂B and A ⊂C, then

7.5 Iterated Rounding
279
B ∩C ̸= ∅, and hence either B ⊂C or C ⊂B, since F is a laminar family. Thus,
T is a forest.) Next, we will count the number of endpoints in T. For each vertex
u ∈V , we count it as an endpoint for each edge incident on u. To be more precise,
let E′ = {(u, e) | u is an endpoint of e}, and we call each (u, e) ∈E′ an endpoint.
We assign an endpoint (u, e) ∈E′ to a set S ∈F, and write (u, e) ∈P (S), if
u ∈S and u ̸∈S′ for any proper subset S′ of S in F. For a subtree T ′ of T, we
deﬁne P (T ′) to be the set of endpoints (u, e) that are in P (S) for some node S of
T ′. Note that each leaf S of T has |δG(S)| ≥4, and hence P (S) has at least four
endpoints. We claim that for any subtree T ′ of T, |P (T ′)| ≥2|V (T ′)| + 2, where
V (T ′) is the set of nodes in T ′.
If T ′ contains only a single leaf S, then the claim holds trivially, as, by the above
observation, |P (S)| ≥4 = 2|V (T ′)| + 2. In general, suppose T ′ contains at least
two nodes.
Assume that R is the root of T ′. Suppose R has k ≥2 children which are the
roots of k subtrees T1, T2, . . ., Tk. By the induction hypothesis, the number of end-
points in P (T ′) is at least
2|V (T1)| + 2 + 2|V (T2)| + 2 + · · · + 2|V (Tk)| + 2 ≥2|V (T ′)| + 2.
Suppose R has only one child S. Let T1 be the subtree rooted at S. By the in-
duction hypothesis, the number of endpoints in P (T1) is at least 2|V (T1)| + 2. If
there are at least two endpoints in P (R), then the number of endpoints in P (T ′) is
at least 2|V (T1)|+2+2 = 2|V (T ′)|+2. Otherwise, if there is at most one endpoint
in P (R), then δG(R) and δG(S) must differ in exactly one edge. Indeed, since aR
and aS are linearly independent, δG(R) and δG(S) must be different. If there is an
edge e = {u, v} ∈δG(R) \ δG(S), with u ∈R and v ̸∈R, then (u, e) ∈P (R).
In addition, if e = {u, v} ∈δG(S) \ δG(R), with u ∈S and v ̸∈S, then v must
be in R and so (v, e) ∈P (R). Therefore, δG(S) and δG(R) can differ in at most
one edge. Let e be the edge in δG(R)ΔδG(S). Then xe = |f(R) −f(S)| must be
an integer, contradicting the assumption that all components xe are fractional. This
completes the proof of our claim.
The above claim implies that there are totally at least 2|F| + 2 = 2|E| + 2
endpoints. However, since each edge can generate only two endpoints, there are
only 2|E| endpoints in E′, and we have reached a contradiction.
□
To ﬁnish the proof of Lemma 7.21, we note that if xe = 1 for some edge e, then
Lemma 7.21 holds. Otherwise, let S be an active set in the laminar family F of
Lemma 7.24 with |δG(S)| ≤3. Then, by condition (c) of Lemma 7.23, we have

e∈δG(S)
xe = f(S) ≥1,
and at least one of the edges e ∈δG(S) has xe ≥1/3.
By exploring more properties of the laminar families, people have found ways
to further improve the result of Lemma 7.21. The reader is referred to Jain [2001],
Gabow and Gallagher [2008], and Gabow et al. [2009] for these results.

280
Linear Programming
7.6
Random Rounding
A general idea in rounding is to round a fractional optimal solution point randomly
to an integer point. With a natural probability distribution, such a random round-
ing scheme often gets a reasonably good expected performance ratio. Moreover, for
some types of simple random rounding schemes, derandomization techniques may
be applied to get a deterministic approximation algorithm with the same perfor-
mance ratio. The following is a simple example.
MAXIMUM SATISFIABILITY (MAX-SAT): Given a CNF Boolean for-
mula F, ﬁnd a Boolean assignment to maximize the number of satisﬁed
clauses.
Suppose F contains m clauses C1, . . ., Cm over n variables x1, . . ., xn. Then
the problem MAX SAT on input F can be formulated as the following integer linear
program:
maximize
z1 + z2 + · · · + zm
subject to

xi∈Cj
yi +

xi∈Cj
(1 −yi) ≥zj,
j = 1, 2, . . ., m,
yi ∈{0, 1},
i = 1, 2, . . ., n,
zj ∈{0, 1},
j = 1, 2, . . ., m,
in which the value of the integer variable yi, 1 ≤i ≤n, corresponds to the value
assigned to the Boolean variable xi.
After relaxing the integer variables yi’s and zj’s to real number variables, we get
the following linear program:
maximize
z1 + z2 + · · · + zm
subject to

xi∈Cj
yi +

xi∈Cj
(1 −yi) ≥zj,
j = 1, 2, . . ., m,
0 ≤yi ≤1,
i = 1, 2, . . ., n,
0 ≤zj ≤1,
j = 1, 2, . . ., m.
(7.27)
Let (y∗, z∗) be an optimal solution of the above LP, and let optLP be its corre-
sponding optimal objective function value; that is, optLP = z∗
1 +z∗
2 +· · ·+z∗
m. Now,
to get an integer solution to F, we randomly round each y∗
i to 1 or 0 independently
as follows:
Algorithm 7.G (Independent Random Rounding Algorithm for MAX-SAT)
Input: A CNF Boolean formula F of clauses C1, C2, . . ., Cm over variables x1,
x2, . . ., xn.
(1) Construct LP (7.27) and ﬁnd an optimal solution (y∗, z∗).

7.6 Random Rounding
281
(2) For i ←1 to n do
Set xi ←1 with probability y∗
i .
To analyze the performance of this independent random rounding, let Zj be the
indicator random variable for the event that clause Cj is satisﬁed.
Lemma 7.25 For any clause Cj, 1 ≤j ≤m, E[Zj] ≥z∗
j (1 −1/e).
Proof. We note that Zj is an indicator random variable, and so
E[Zj] = Pr[Zj = 1] = 1 −Pr[Zj = 0]
= 1 −
.
xi∈Cj
(1 −y∗
i ) ·
.
xi∈Cj
y∗
i .
(7.28)
By an argument similar to that of Lemma 7.17, we can prove that E[Zj] ≥z∗
j (1 −
1/e). We omit the detail.
□
Denote ZF = Z1 + Z2 + · · ·+ Zm, and let opt be the optimal objective function
value of MAX-SAT. By Lemma 7.25, we have
E[ZF] = E[Z1] + E[Z2] + · · · + E[Zm]
≥

1 −1
e

(z∗
1 + z∗
2 + · · · + z∗
m)
≥optLP ·

1 −1
e

≥opt ·

1 −1
e

,
and we get a performance ratio e/(e −1) for Algorithm 7.G:
Theorem 7.26 The expected output value of Algorithm 7.G is an (e/(e −1))-
approximation to MAX-SAT.
The random rounding of Algorithm 7.G rounds each variable xi independently.
For such a simple random rounding, we can derandomize it by the method of con-
ditional probability. Namely, we note that
E[ZF] = E[ZF | x1 = 1] · y∗
1 + E[ZF | x1 = 0] · (1 −y∗
1).
Therefore, we have either
E0ZF|x1 =1
1 = E0ZF
 x1 = 11 ≥optLP ·

1 −1
e

or
E
0
ZF|x1=0
1
= E
0
ZF
 x1 = 0
1
≥optLP ·

1 −1
e

,
where F |x1=b, b ∈{0, 1}, denotes the Boolean formula obtained from F with the
partial assignment x1 = b. Moreover, as shown in (7.28), each E[Zj], and hence
E[ZF], can be computed in polynomial time. This also applies to E[ZF | x1 =

282
Linear Programming
0] and E[ZF | x1 = 1]. Therefore, we can ﬁnd out, in polynomial time, which of
the two assignments x1 = 0 or xi = 1 has a better expected output value. This
observation suggests the following derandomization of Algorithm 7.G.
Algorithm 7.H (Derandomization of Algorithm 7.G for MAX-SAT)
Input: A CNF Boolean formula F of clauses C1, C2, . . ., Cm over variables x1,
x2, . . ., xn.
(1) Construct LP (7.27) and ﬁnd an optimal solution (y∗, z∗).
(2) For i ←1 to n do
if E
0
ZF
 xi = 1
1
≥E
0
ZF
xi = 0
1
then xi ←1; F ←F|xi=1
else xi ←0; F ←F|xi=0.
Theorem 7.27 MAX SAT has a polynomial-time e/(e −1)-approximation.
Proof. We observe that, at each iteration,
max
&
E
0
ZF
 xi = 0
1
, E
0
ZF
xi = 1
1'
≥E[ZF].
Thus, we can prove, by a simple induction, that the formula F at the end of each
iteration must satisfy E[ZF ] ≥(1 −1/e) optLP. Note that, at the end of the nth
iteration, F contains no variable, and so
ZF = E[ZF] ≥

1 −1
e

optLP ≥

1 −1
e

opt.
□
In the above example, each variable is rounded to an integer independently. Next,
we introduce some general random rounding techniques in which the roundings for
different variables are not independent.
Recall the pipage rounding technique introduced in Section 7.4, where the round-
ing at each stage is determined by a companion function which is closely related to
the objective function. Within the setting of pipage rounding, we can apply random
rounding to avoid the use of the companion function. This technique of combining
random rounding with pipage rounding has many applications.
We ﬁrst study the general framework of random pipage rounding. Consider a
bipartite graph G = (U, V, E) and variables xe, for e ∈E. Let x∗be an optimal
solution to an LP of the form (7.21).
Random Pipage Rounding
(1) Initially, set x ←x∗.
(2) While x is not an integer solution do
(2.1) Let Hx be the subgraph of G induced by all edges e ∈E with 0 < xe <
1. Let R be a cycle or a maximal path of Hx. Then R can be decomposed
into two matchings M1 and M2.

7.6 Random Rounding
283
(2.2) Deﬁne x(ε) by
xe(ε) =
⎧
⎨
⎩
xe
if e ̸∈R,
xe + ε if e ∈M1,
xe −ε if e ∈M2.
(2.3) Let ε1 ←min

min
e∈M1 xe, min
e∈M2(1 −xe)

,
ε2 ←min

min
e∈M1(1 −xe), min
e∈M2 xe

.
(2.4) Set
x ←
x(ε2),
with probability ε1/(ε1 + ε2),
x(−ε1), with probability ε2/(ε1 + ε2).
Lemma 7.28 For each edge e, let Xe be the random variable denoting the value of
xe output by the Random Pipage Rounding procedure. Then the followingproperties
hold for Xe:
(P1) (Marginal Distribution) For every edge e, Pr[Xe = 1] = x∗
e.
(P2) (Degree Preservation) For any vertex v ∈U ∪V ,
Pr
0
Dv ∈{⌊dv⌋, ⌈dv⌉}
1
= 1,
where Dv = 
e∈δ(v) Xe and dv = 
e∈δ(v) x∗
e.
(P3) (Negative Correlation) For any v ∈U ∪V , S ⊆δ(v), and b ∈{0, 1},
Pr
, 2
e∈S
(Xe = b)
-
≤
.
e∈S
Pr 0Xe = b1.
Proof. For property (P1), we prove it by induction on the number k of edges e with
nonintegral x∗
e. For k = 0, it is trivial. Now, we consider the case k ≥1. Let
x′
e be the random variable for the value of xe at the end of the ﬁrst iteration, and
write x′ = (x′
e)e∈E. Note that within steps (2.1)–(2.3) of the ﬁrst iteration, we have
x′ = x∗. Then, after step (2.4), the number of nonintegral components of x′ is at
most k −1. Therefore, by the induction hypothesis, we have
Pr
0
Xe = 1
 x′ = x(−ε1)
1
= x′
e = xe(−ε1) = x∗
e(−ε1),
Pr
0
Xe = 1
x′ = x(ε2)
1
= x′
e = xe(ε2) = x∗
e(ε2).
It follows that
Pr[Xe = 1] = Pr
0
Xe = 1
 x′ = x(−ε1)
1
· Pr[x′ = x(−ε1)]
+ Pr
0
Xe = 1
x′ = x(ε2)] · Pr[x′ = x(ε2)]
= x∗
e(−ε1) ·
ε2
ε1 + ε2
+ x∗
e(ε2) ·
ε1
ε1 + ε2
.

284
Linear Programming
Now, if e ̸∈R, then x∗
e(−ε1) = x∗
e(ε2) = x∗
e and, hence, Pr[Xe = 1] = x∗
e. If
e ∈M1, then x∗
e(−ε1) = x∗
e −ε1 and x∗
e(ε2) = x∗
e + ε2. Hence,
Pr[Xe = 1] = (x∗
e −ε1) ·
ε2
ε1 + ε2
+ (x∗
e + ε2) ·
ε1
ε1 + ε2
= x∗
e.
If e ∈M2, then x∗
e(−ε1) = x∗
e + ε1 and x∗
e(ε2) = x∗
e −ε2. Hence,
Pr[Xe = 1] = (x∗
e + ε1) ·
ε2
ε1 −ε2
+ (x∗
e −ε2) ·
ε1
ε1 + ε2
= x∗
e.
For property (P2), we consider three cases.
Case 1. For all edges e ∈δ(v), x∗
e is an integer. Then Xe = x∗
e for all e ∈δ(v)
and so Dv = dv.
Case 2. There exists exactly one edge e ∈δ(v) such that x∗
e is nonintegral. Then,
Dv = ⌊dv⌋if Xe = 0 and Dv = ⌈dv⌉if Xe = 1. So, (P2) holds in this case.
Case 3. There exists more than one edge e ∈δ(v) such that x∗
e is nonintegral.
Then, at the beginning of an iteration, if there is more than one edge e ∈δ(v)
with nonintegral xe, then, by the argument in the proof of Lemma 7.20, the value

e∈δ(v) xe does not change after this iteration and so is still equal to dv. If, at the
end of an iteration, the number of nonintegral components xe, for e ∈δ(v), drops
below two, then either case 1 or case 2 applies, and so Dv = ⌊dv⌋or ⌈dv⌉. This
shows that (P2) also holds for this case.
For property (P3), we will also prove it by induction on the number k of edges e
with nonintegral x∗
e. For k = 0, (P3) holds trivially with equality. Now, we consider
the case of k ≥1. Let x′
e be the random variable for the value of xe at the end of
the ﬁrst iteration and let x′ = (x′
e)e∈δ(v). So, by the induction hypothesis
Pr
, 2
e∈S
(Xe = b)
 x′ = x(−ε1)
-
≤
.
e∈S
Pr
0
Xe = b
 x′ = x(−ε1)
1
and
Pr
, 2
e∈S
(Xe = b)
 x′ = x(ε2)
-
≤
.
e∈S
Pr
0
Xe = b
 x′ = x(ε2)
1
.
Note that S ⊆δ(v) may have at most two edges in R. We consider the following
three cases.
Case 1. No edge in S belongs to R. Then, by property (P1), for any e ∈S,
Pr
0
Xe = 1
 x′ = x(−ε1)
1
= Pr
0
Xe = 1
 x′ = x(ε2)
1
= x′
e = x∗
e = Pr[Xe = 1]
and
Pr
0
Xe = 0
 x′ = x(−ε1)
1
= Pr
0
Xe = 0
 x′ = x(ε2)
1
= 1 −x∗
e = Pr[Xe = 0].

7.6 Random Rounding
285
Therefore, we have
Pr
, 2
e∈S
(Xe = b)
-
= Pr
, 2
e∈S
(Xe = b)
 x′ = x(−ε1)
-
· Pr[x′ = x(−ε1)]
+ Pr
, 2
e∈S
(Xe = b)
x′ = x(ε2)
-
· Pr[x′ = x(ε2)]
≤
.
e∈S
Pr[Xe = b] ·
ε2
ε1 + ε2
+
.
e∈S
Pr[Xe = b] ·
ε1
ε1 + ε2
=
.
e∈S
Pr[Xe = b],
and so (P3) holds for case 1.
Case 2. S contains only one edge e′ in R. Without loss of generality, assume
that e′ ∈M1. Then, at the end of the ﬁrst iteration, xe′(−ε1) = x∗
e′ −ε1 and
xe′(ε2) = x∗
e′ + ε2. So, by (P1),
Pr 0Xe′ = 1
x′ = x(−ε1)1 = xe′(−ε1) = x∗
e′ −ε1,
Pr
0
Xe′ = 1
x′ = x(ε2)
1
= xe′(ε2) = x∗
e′ + ε2.
Therefore,
Pr
, 2
e∈S
(Xe = 1)
-
≤
)
(x∗
e′ −ε1) ·
ε2
ε1 + ε2
+ (x∗
e′ + ε2) ·
ε1
ε1 + ε2
*
·
.
e∈S−{e′}
Pr[Xe = 1]
= x∗
e′
.
e∈S−{e′}
Pr[Xe = 1] =
.
e∈S
Pr[Xe = 1].
Similarly,
Pr
, 2
e∈S
(Xe = 0)
-
≤
)
(1 −x∗
e′ + ε1) ·
ε2
ε1 + ε2
+ (1 −x∗
e′ −ε2) ·
ε1
ε1 + ε2
*
·
.
e∈S−{e′}
Pr[Xe = 0]
= (1 −x∗
e′)
.
e∈S−{e′}
Pr[Xe = 0] =
.
e∈S
Pr[Xe = 0].
This shows that (P3) holds for case 2.
Case 3. S contains two edges e′ and e′′ in R. Then we must have e′ ∈M1 and
e′′ ∈M2. By (P1), we know that

286
Linear Programming
Pr
0
Xe′ = 1
 x′ = x(−ε1)
1
= x∗
e′ −ε1,
Pr
0
Xe′ = 1
 x′ = x(ε2)
1
= x∗
e′ + ε2,
Pr 0Xe′′ = 1
x′ = x(−ε1) 1 = x∗
e′′ + ε1,
Pr
0
Xe′′ = 1
x′ = x(ε2)
1
= x∗
e′′ −ε2.
Therefore,
Pr
, 2
e∈S
(Xe = 1)
-
≤
)
(x∗
e′ −ε1)(x∗
e′′ + ε1) ·
ε2
ε1 + ε2
+ (x∗
e′ + ε2)(x∗
e′′ −ε2) ·
ε1
ε1 + ε2
*
·
.
e∈S−{e′,e′′}
Pr[Xe = 1]
= (x∗
e′x∗
e′′ −ε1ε2) ·
.
e∈S−{e′,e′′}
Pr[Xe = 1]
≤
.
e∈S
Pr[Xe = 1].
For the case of b = 0, the proof is similar.
□
For a simple application of the above properties of the Random Pipage Round-
ing procedure, consider the problem MAX-WH again. Let x∗= (x∗
i )1≤i≤n be
an optimal (fractional) solution for the LP-relaxation (7.19) of MAX-WH. Apply-
ing the Random Pipage Rounding procedure to x∗, we round each variable xi,
1 ≤i ≤n, to a random variable Xi ∈{0, 1}. Let Lj(X) = min{1, 
i∈Sj Xi}
and L(X) = m
j=1 wjLj(X); that is, L(X) is the objective function value of
the random pipage rounding. The following theorem shows that the expected value
of L(X) is as good as the approximate solution produced by the deterministic pi-
page rounding of Algorithm 7.E. In the following, opt denotes the optimal objective
function value of the problem MAX-WH.
Theorem 7.29
E
0
L(X)
1
≥

1 −1
e

opt.
Proof. Note that for each j = 1, 2, . . ., n,
Pr 0Lj(X) = 11 = 1 −Pr 0Lj(X) = 01
= 1 −Pr
, 2
i∈Sj
(Xi = 0)
-
≥1 −
.
i∈Sj
Pr
0
Xi = 0
1
(by negative correlation)
= 1 −
.
i∈Sj

1 −x∗
i

(by marginal distribution)
≥

1 −1
e

· min

1,

i∈Sj
x∗
i

,

7.6 Random Rounding
287
where the last inequality follows from the proof of Lemma 7.17. Thus, we have
E
0
L(X)
1
=
m

j=1
wj · E
0
Lj(X)
1
=
m

j=1
wj · Pr
0
Lj(X) = 1
1
≥

1 −1
e
 m

j=1
wj · min

1,

i∈Sj
x∗
i

≥

1 −1
e

opt.
□
Next, we study a random rounding technique based on the geometric structure
of the feasible region. Consider an n-dimensional polytope P with integer vertices.
Then, every point x in P can be expressed as a convex combination of at most n+1
vertices. The following is a simple rounding scheme based on this property.
Vector Rounding
Input: An n-dimensional polytope P with integer vertices, and a noninteger solu-
tion x in P .
(1) Write x = n+1
i=1 αivi, where v1, . . ., vn+1 are vertices of P , αi ≥0, and
n+1
i=1 αi = 1.
(2) Round x to vertex vi with probability αi, 1 ≤i ≤n + 1.
The above vector rounding can be extended to the following more general geo-
metric rounding scheme. In the following, we write ⟨v1, . . ., vn, vn+1⟩to denote
the n-dimensional simplex generated by points v1, . . ., vn+1.
Geometric Rounding
Input: A simplex P = ⟨v1, . . ., vn, vn+1⟩and a point x in the simplex.
(1) For i ←1 to n + 1 do
Select a random number βi from (0, 1].
(2) Let u ←
n+1
i=1 βi vi
n+1
i=1 βi
.
(3) Round x to vi if u lies in the simplex ⟨v1, . . ., vi−1, x, vi+1, . . ., vn+1⟩
(see Figure 7.5).
Indeed, if each βi, 1 ≤i ≤n + 1, is chosen randomly based on the unit-
exponential distribution, then the corresponding geometric rounding is equivalent
to vector rounding. This relationship can be seen from the following two lemmas.
Let P = ⟨v1, . . ., vn+1⟩be a nondegenerate simplex, and x = n+1
i=1 αivi, where
αi ≥0 for each 1 ≤i ≤n + 1, and n+1
i=1 αi = 1. Also, let u be the point deﬁned
in the Geometric Rounding procedure about the simplex P and point x.
Lemma 7.30 The point u lies in the simplex ⟨v1, . . ., vi−1, x, vi+1, . . ., vn+1⟩if
and only if
βi
αi
=
min
1≤k≤n+1
βk
αk
.

288
Linear Programming
v
v
v
x
1
2
3
u
Figure 7.5:
Geometric rounding: rounding x to v2.
Proof. It sufﬁces to consider the case i = 1. Suppose u ∈⟨x, v2, . . ., vn+1⟩. Then
u can be written as a convex combination of x, v2, . . ., vn+1; that is, u = λ1x +
λ2v2 + · · · + λn+1vn+1, with λi ≥0, for each 1 ≤i ≤n + 1, and n+1
i=1 λi = 1.
Substituting n+1
i=1 αivi for x, we obtain
u = λ1α1v1 +
n+1

i=2
(λ1αi + λi)vi.
Since ⟨v1, . . ., vn+1⟩is a nondegenerate simplex, the convex combination for u, in
terms of vi, 1 ≤i ≤n + 1, is unique. Hence,
β1
β
= λ1α1,
βi
β
= λ1αi + λi,
2 ≤i ≤n + 1,
where β = n+1
i=1 βi. Thus, for 2 ≤i ≤n + 1,
β1
α1
= βλ1 = βi −λi
αi
≤βi
αi
.
Conversely, assume that βk/αk = min1≤i≤n+1 βi/αi and yet u ̸∈⟨v1, . . .,
vk−1, x, vk+1, . . ., vn+1⟩. Without loss of generality, assume that k ̸= 1 and u ∈
⟨x, v2, . . ., vn+1⟩. So we can write u = λ1x+n+1
i=2 λivi. Then, as shown above,
we must have
β1
α1
=
min
1≤i≤n+1
βi
αi
= βk
αk
.
Furthermore, from the above proof, we know that λk = 0. In other words, u can be
written as a convex combination of x, v2, . . ., vk−1, vk+1, . . ., vn+1. However, this
means that u ∈⟨v1, . . ., vk−1, x, vk+1, . . ., vn+1⟩, which leads to a contradiction.
□
In the following, we write X ∼exp(λ) to denote that the random variable X
has the exponential distribution with rate parameter λ; that is, its probability density
function is f(x) = λe−λx.

Exercises
289
Lemma 7.31 In the Geometric Rounding procedure, if β1, . . ., βn+1 are chosen
independently with the distribution βi ∼exp(1) for 1 ≤i ≤n + 1, then
Pr[x is rounded to vi] = αi.
Proof. It sufﬁces to prove the case that x is rounded to v1. Since, for each 1 ≤i ≤
n+1, βi ∼exp(1), we have βi/αi ∼exp(αi). By Lemma 7.30, x is rounded to v1
by the Geometric Rounding procedure if and only if β1/α1 = min1≤i≤n+1 βi/αi.
Note that the cumulative distribution of X ∼exp(λ) is F(x) = 1 −e−λx, and
 ∞
a
λe−λxdx = 1 −e−λx
∞
x=a = e−aλ.
Therefore, we have
Pr[x is rounded to v1] = Pr
) β1
α1
=
min
1≤i≤n+1
βi
αi
*
=
 ∞
0
α1e−α1x1  ∞
x1
α2e−α2x2dx2 · · ·
 ∞
x1
αn+1e−αn+1xn+1dxn+1

dx1
=
 ∞
0
α1e−α1x1 e−α2x1 · · ·e−αn+1x1 dx1
=
 ∞
0
α1e−x1dx1 = α1.
□
Applications of geometric rounding techniques can be found in Ge, Ye, and
Zhang [2010] and Ge, He et al. [2010].
Exercises
7.1 Give an example to show that a linear program not satisfying the nonde-
generacy assumption may still have a one-to-one and onto correspondence between
basic feasible solutions and feasible bases.
7.2 Show that vertices of the feasible region are invariant under the transforma-
tion from a linear program to its standard form.
7.3
(a) Generalize the greedy algorithm for KNAPSACK in Section 1.1 to obtain
a polynomial-time (m + 1)-approximation for the resource management
problem (7.2).
(b) Generalize the generalized greedy algorithm for KNAPSACK in Section 1.1
to obtain a PTAS for the resource management problem (7.2).
7.4 Recall that a subset A of the vertex set V of a graph G = (V, E) is an inde-
pendent set if no two vertices in A form an edge in E. The maximum independent

290
Linear Programming
set problem (MAX-IS) asks one, for a given graph G, to ﬁnd an independent set
of G of the maximum cardinality. Formulate the problem MAX-IS as a resource
management problem with an unlimited number of resources.
7.5 Discuss whether the simplex method can avoid cycles for a linear program
if it is known that this linear program has a one-to-one and onto correspondence
between its basic feasible solutions and feasible bases.
7.6 Use the optimal feasible basis obtained in Example 7.5 to solve the following
linear program with the simplex method:
minimize
z =
−2x1 −x2
subject to
x1 + 2x2 + x5 = 8,
x1 +
x2 −x3 + x6 = 3,
−x1 +
x2 −x4 + x7 = 1,
x1 , x2 , . . . , x7 ≥0.
7.7 Design an (e/(e −1))-approximation for MAX-SAT with the pipage round-
ing technique.
7.8 Consider the following problem.
MAXIMUM k-CUT IN A HYPERGRAPH (MAX-k-CUT-HYPER): Given
a hypergraph H = (V, E) with nonnegative edge weight w : E →N,
and k positive integers p1, . . ., pk, with p1+· · ·+pk = |V |, partition V
into k parts X1, . . ., Xk such that |Xt| = pt, 1 ≤t ≤k, to maximize
the total weight of edges not lying entirely in any part.
This problem can be formulated as the following ILP:
maximize

S∈E
wS zS
subject to
zS ≤|S| −

i∈S
xit,
S ∈E, t = 1, 2, . . ., k,
k

t=1
xit = 1,
i ∈V,
n

i=1
xit = pt,
t = 1, 2, . . ., k,
xit, zS ∈{0, 1},
S ∈E, i ∈V, t = 1, 2, . . ., k.
Deﬁne
F (x) =

S∈E
wS

1 −
k

t=1
.
i∈S
xit


Exercises
291
and
L(x) =

S∈E
wS · min

1, min
1≤t≤k

|S| −

i∈S
xit

.
Show that
F (x) ≥ρ · L(x),
where ρ = min{λ|S| | S ∈E} and λr = 1 −(1 −1/r)r −(1/r)r. Use the pipage
rounding scheme to design a (1/ρ)-approximation for MAX-k-CUT-HYPER.
7.9 Show that for r ≥3, λr = 1 −(1 −1/r)r −(1/r)r ≥1 −e−1.
7.10 Consider the following problem:
MAXIMUM COVERAGE WITH KNAPSACK CONSTRAINTS (MAX-
COVER-KC): Given a set I = {1, 2, . . ., n} with weights ci ≥0 for
i ∈I, a family F = {S1, S2, . . ., Sm} of subsets of I with weights
wj ≥0 for j ∈{1, . . ., m}, and a positive integer B, ﬁnd a subset
X ⊆I with 
i∈X ci ≤B that maximizes the total weight of the sets
in F having nonempty intersections with X.
This problem can be formulated as the following ILP:
maximize
m

j=1
wjzj
subject to

i∈Sj
xi ≥zj,
1 ≤j ≤m,
n

i=1
cixi ≤B,
xi, zj ∈{0, 1},
1 ≤j ≤m, 1 ≤i ≤n.
Deﬁne
F (x) =
m

j=1
wj

1 −
.
i∈Sj
(1 −xi)

.
Use the pipage rounding scheme with function F (x) to design a ρ-approximation
for MAX-COVER-KC, where ρ = 1/(1 −(1 −1/k)k) and k = max{|Sj| | 1 ≤
j ≤m}.
7.11 Let X be a ﬁnite set. We say a function f : [0, 1]X →R+ is submodular if
f(x ∨y) + f(x ∧y) ≤f(x) + f(y),
where, for x, y ∈[0, 1]X, x ∨y and x ∧y are members of [0, 1]X deﬁned by
(x ∨y)i = max{xi, yi} and (x ∧y)i = min{xi, yi}. We say f is monotone
increasing if f(x) ≤f(y) as long as x ≤y coordinatewise. Show that if f is
smooth, that is, if its second partial derivatives exist everywhere in [0, 1]X, then

292
Linear Programming
(a) f is monotone increasing if and only if ∂f/∂yj ≥0 for each j ∈X;
(b) f is submodular if and only if ∂2f/(∂yi∂yj) ≤0 for any i, j ∈X.
7.12 Let f : 2X →R+ be a monotone increasing, submodular function.
(a) Show that f can be extended to a smooth, monotone increasing, submodular
function f : [0, 1]X →R+ as follows:
f(y) =

R⊆X
f(R)
.
i∈R
yi
.
i̸∈R
(1 −yi).
(b) Show that for any y ∈[0, 1]X and i, j ∈X, f(y + t(ei −ej)) is convex
with respect to t, where ei is the unit vector with the ith component equal
to 1.
(c) Design a pipage rounding algorithm with the potential function f over the
feasible domain

y ∈[0, 1]X  y ≥0,

j∈S
yj ≤r(S), for all S ⊆X

,
where r : 2X →N is a polymatroid function with r({i}) = 1 for all i ∈X.
7.13 Consider a graph G = (V, E) and a function f : 2V →N. Show that f is
weakly supmodular if f satisﬁes the following conditions:
(i) f(V ) = 0;
(ii) For every subset S of V , f(S) = f(V −S); and
(iii) For any two disjoint subsets A and B of V , f(A∪B) ≤max{f(A), f(B)}.
7.14 We say a function f : 2V →Z is strongly submodular if f(V ) = 0 and for
every two subsets A and B of V ,
f(A) + f(B) ≥f(A \ B) + f(B \ A)
and
f(A) + f(B) ≥f(A ∪B) + f(A ∩B).
For any subset S ⊆V , let δG(S) denote the set of edges with exactly one endpoint
in S. Show that for any graph G = (V, E), the function f(S) = |δG(S)| is strongly
submodular.
7.15 Consider a graph G = (V, E) with a weakly supmodular function f :
2V →Z. Show that for any subgraph F of G, f(S) −|δF(S)| is still weakly
supmodular.

Exercises
293
7.16 Consider a graph G = (V, E) and the following LP:
minimize

e∈E
cexe
subject to

e∈δG(S)
xe ≥k,
S ⊆V,
0 ≤xe ≤1,
e ∈E.
Show that the constraint matrix of this LP has rank |E|.
7.17 Consider the following problem:
GENERALIZED STEINER NETWORK: Given a graph G = (V, E) with
a positive edge cost function c : E →Z+, and a subset P of V , ﬁnd a
minimum-cost k-edge-connected subgraph containing P .
Use the iterated rounding technique to construct a 3-approximation for this problem.
7.18 Improve Lemma 7.21 by showing that every basic feasible solution of the
linear program (7.23) has a component whose value is at least 1/2.
7.19 Show that the following algorithm is a 2-approximation for MAX-SAT:
Input: A CNF formula F over variables x1, . . ., xn.
(1) For i ←1 to n do
assign xi ←1 with probability 1/2.
{Let ZF be the number of clauses satisﬁed by this assignment.}
(2) For i ←1 to n do
if E[ZF | xi = 1] ≥E[ZF | xi = 0] then xi ←1; F ←F |xi=1
else xi ←0; F ←F |xi=0.
7.20 Show that the following algorithm is a (4/3)-approximationfor MAX-SAT:
Input: A CNF formula F over variables x1, . . ., xn.
(1) Construct the LP-relaxation of F , and ﬁnd its optimal solution y∗.
{Let optLP denote the optimal objective function value of this LP.}
(2) For i ←1 to n do
assign xi ←1 with probability 1/4 + y∗/2.
{Let ZF be the number of clauses satisﬁed by this assignment.}
(3) For i ←1 to n do
if E[ZF | xi = 1] ≥optLP · (1 −1/e) then xi ←1; F ←F |xi=1
else xi ←0; F ←F |xi=0.
7.21 Extend Algorithm 7.H to get an (e/(e −1))-approximation for the follow-
ing problem:

294
Linear Programming
MAXIMUM-WEIGHT SATISFIABILITY (MAX-WSAT): Given a CNF
formula with nonnegative weight on clauses, ﬁnd a Boolean assignment
to its variables that maximizes the total weight of true clauses.
7.22 Show that if two random variables X ∼exp(μ) and Y ∼exp(λ) are
independent, then for 0 ≤α ≤β,
Pr[αX < Y < βX] = μ

1
μ + λα −
1
μ + λβ

.
7.23 Suppose that in the Geometric Rounding scheme, we choose u uniformly
from the simplex P = ⟨v1, . . ., vn+1⟩, and round x to 3x and y to 3y by this method.
Show that
E[d(3x, 3y)] ≤2 · d(x, y),
where d(x, y) is the Euclidean distance between x and y.
7.24 Consider the following problem:
MINIMUM FEASIBLE CUT: Given a graph G = (V, E) with edge
weight c : E →R+, a vertex s ∈V , and a set M of pairs of ver-
tices in G, ﬁnd a subset of V , with the minimum-weight edge cut, that
contains s but does not contain any pair in M.
This problem can be formulated as the following ILP:
minimize

e∈E
cexe
subject to
xe ≥yu −yv,
e = {u, v} ∈E,
xe ≥yv −yu,
e = {u, v} ∈E,
yu + yv ≤1,
{u, v} ∈M,
ys = 1,
yu, xe ∈{0, 1},
u ∈V, e ∈E.
Let (x, y) be an optimal solution of the LP relaxation of the above ILP, and optLP
the corresponding optimal objective function value. Choose a number U uniformly
from [0, 1], and round each yu to 1 if yu ≥U, and to 0 if yu < U. Let 3y denote the
resulting y, and set 3xe = |3yu −3yv| if e = {u, v}. Show that
optLP ≤E
) 
e∈E
ce3xe
*
≤2 · optLP.
7.25 Consider the following problem:
MIN-SAT: Given a CNF Boolean formula F with weighted clauses
C1, . . ., Cm over variables x1, . . ., xn, ﬁnd an assignment to the vari-
ables that minimizes the total weight of satisﬁed clauses.

Historical Notes
295
This problem can be formulated as the following ILP:
minimize
m

j=1
wjzj
subject to
zj ≥yi,
xi ∈Cj,
zj ≥1 −yi,
¯xi ∈Cj,
yi, zj ∈{0, 1},
1 ≤i ≤n, 1 ≤j ≤m.
Let (y, z) be an optimal solution of the LP-relaxation of the above ILP, and optLP
the corresponding optimal objective function value. Split set {1, 2, . . ., n} into two
sets A and B randomly, with probability 1/2 of assigning each i, 1 ≤i ≤n, to set
A. Choose U uniformly from [0, 1]. For each i ∈A, set 3xi = 1 if yi > U, and 0
otherwise, and for each yi ∈B, set 3xi = 1 if yi > 1 −U, and 0 otherwise. For each
j = 1, 2, . . ., m, set
3zj = max

max
xi∈Cj 3yi, max
¯xi∈Cj(1 −3yi)

.
Show that
E
) m

j=1
wj3zj
*
≤2

1 −1
2k

· optLP.
Historical Notes
The simplex method for linear programming was ﬁrst proposed by Dantzig in 1947
[Dantzig, 1951, 1963]. Charnes [1952] gave the ﬁrst method, called the perturbation
method, which is equivalent to the lexicographical ordering method, to deal with de-
generacy in linear programming. Bland [1977] found another rule to overcome the
degeneracy problem. Klee and Minty [1972] presented an example showing that
the simplex method does not run in polynomial time in the worst case. Khachiyan
[1979] found the ﬁrst polynomial-timesolution, the ellipsoid method, for linear pro-
gramming, with the worst-case running time O(n6). Karmarkar [1984] discovered
the interior-point method for linear programming, which runs in time O(n3).
The application of linear programming in combinatorial optimization began in
the early 1950s. However, its application to approximation algorithms came only
after 1970. The works of Lov´asz [1975], Chv´atal [1979], and Wolsey [1980] were
pioneering in this direction.
Bellare et al. [1995] showed that MIN-VC does not have a polynomial-time
ρ-approximation for ρ < 16/15 unless P = NP. So far, no polynomial-time ρ-
approximation, with ρ < 2, has been found for MIN-VC. A survey on MIN-VC
and GC can be found in Hochbaum [1997a]. The 2-approximation for MIN-2SAT of
Section 7.3 is due to Gusﬁeld and Pitt [1992]. The 2-approximation for SCHEDULE-
UPM of the same section was given by Lenstra et al. [1990].

296
Linear Programming
The pipage rounding technique was proposed by Ageev and Sviridenko [2004].
Gandhi et al. [2006] applied this technique to dependent rounding. With pipage
rounding,Calinescu et al. [2007] studied the maximization of monotone submodular
functions subject to matroid constraints. Exercises 7.8–7.10 are from Ageev and
Sviridenko [2004]. The iterated rounding scheme was proposed by Jain [2001] and
was later improved by Gabow and Gallagher [2008] and Gabow et al. [2009]. It has
found a lot of applications [Fleischer et al., 2001; Cheriyan et al., 2006; Chen, 2007;
Melkonian and Tardos, 2004]. Exercise 7.11 is from Wolsey [1982b], Exercise 7.12
is from Calinescu et al. [2007], and Exercises 7.13 and 7.15 are from Goemans,
Goldberg et al. [1994]. For the improvement over Lemma 7.21 (Exercise 7.18), see
Jain [2001]. It is known that MAX-SAT has no PTAS unless P = NP (see Chapter
10). Its approximation has been studied extensively [Johnson, 1974; Yannakakis,
1994; Goemans and Williamson, 1994; Karloff and Zwick, 1997]. Exercise 7.19 is
from Johnson [1974], and Exercise 7.20 is from Goemans and Williamson [1994].
The techniques of dependent randomized rounding were initiated by Bertsimas et
al. [1999]. They also proposed the vector rounding scheme in an earlier version of
the paper. Its generalization, the geometric rounding scheme, can be found in Ge,
Ye, and Zhang [2010] and Ge, He et al. [2010]. Exercise 7.23 is from Ge, Ye, and
Zhang [2010]. Exercises 7.24 and 7.25 are from Bertsimas et al. [1999].

8
Primal-Dual Schema
and Local Ratio
We believe, in fact, that the one act of respect has little force
unless matched by the other—in balance with it.
The acting out of that dual respect I would
name as precisely the source of our power.
— Barbara Deming
Based on the duality theory of linear programming, a new approximation technique,
called the primal-dual schema, has been developed. With this technique, we do not
need to compute the optimal solution of the relaxed linear program in order to get
an approximate solution of the integer program. Thus, we can reduce the running
time of many linear programming–based approximation algorithms from O(n3) to
at most O(n2). Moreover, this method can actually be formulated in an equivalent
form, called the local ratio method, which does not require the knowledge of the
theory of linear programming. In this chapter, we study these two techniques and
their relationship.
8.1
Duality Theory and Primal-Dual Schema
One of the most important and intriguing elements of linear programming is the
duality theory. Consider a linear program of the standard form
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9_8, 
© Springer Science+Business Media, LLC 2012 
297

298
Primal-Dual Schema
minimize
cx
subject to
Ax = b,
x ≥0,
(8.1)
where A is an m × n matrix over reals, c an n-dimensional row vector, x an n-
dimensional column vector, and b an m-dimensional column vector. We can deﬁne
a new linear program
maximize
yb,
subject to
yA ≤c,
(8.2)
where y is an m-dimensional row vector.1 This linear program (8.2) is called the
dual linear program of the primal linear program (8.1). These two linear programs
have a very interesting relationship.
Theorem 8.1 Suppose x and y are feasible solutions of (8.1) and (8.2), respec-
tively. Then cx ≥yb.
Proof. Since x and y satisfy the constraints of (8.1) and (8.2), respectively, we have
cx ≥(yA)x = yb.
□
Corollary 8.2 The linear programs (8.1) and (8.2) satisfy one of the following con-
ditions:
(1) Neither (8.1) nor (8.2) has a feasible solution.
(2) The linear program (8.1) has a feasible solution but has no optimal solutions,
and the dual linear program (8.2) has no feasible solutions.
(3) The linear program (8.1) has no feasible solutions, and its dual linear pro-
gram (8.2) has a feasible solution but has no optimal solutions.
(4) Both the linear program (8.1) and its dual linear program (8.2) have an opti-
mal solution.
Proof. From Theorem 8.1, if either (8.1) or (8.2) has unbounded solutions, then the
other linear program cannot have a feasible solution. Thus, if none of cases (1), (2),
or (3) is satisﬁed, then both (8.1) and (8.2) have bounded solutions and, hence, have
optimal solutions.
□
From the proof of Theorem 8.1, it is easy to see that, for two feasible solutions x
and y of linear programs (8.1) and (8.2), respectively, cx = yb if and only if
(c −yA)x = 0.
The above equation is called the complementary slackness condition. This condition
can be used to verify whether x and y are optimal solutions.
1Note that we write, for convenience, b and x as column vectors, while c and y are row vectors.

8.1 Duality Theory
299
Theorem 8.3 (a) Suppose x and y are feasible solutions of the primal and dual
linear programs (8.1) and (8.2), respectively. If (c −yA)x = 0, then x and y are
optimal solutions of (8.1) and (8.2), respectively.
(b) Suppose x∗and y∗are optimal solutions of the primal and dual linear pro-
grams (8.1) and (8.2), respectively. Then cx∗= y∗b.
Proof. Part (a) follows immediately from Theorem 8.1.
For part (b), it sufﬁces to show that if (8.1) and (8.2) have optimal solutions,
then there exist feasible solutions x and y for (8.1) and (8.2), respectively, such that
(c−yA)x = 0. From Theorem 7.10, we know that if (8.1) has an optimal solution,
then it has a feasible basis J such that c −cJA−1
J A ≥0. Suppose x is the basic
feasible solution of (8.1) associated with basis J and y = cJA−1
J . Then c ≥yA,
and so y is a feasible solution of (8.2). In addition, we have
(c −yA)x = 0
since cJ −yAJ = cJ −cJA−1
J AJ = 0, and x ¯
J = 0.
□
We notice that the primal linear program (8.1) and its dual (8.2) are of different
forms. In general, the primal linear program does not have to be in standard form.
The following is such a pair of primal and dual linear programs of the symmetric
form:
(primal LP)
(dual LP)
minimize
cx
maximize
yb
subject to
Ax ≥b,
subject to
yA ≤c,
x ≥0,
y ≥0.
(8.3)
For this pair of linear programs, Theorem 8.1 still holds, but the complementary
slackness condition is changed to
(c −yA)x + y(Ax −b) = 0;
or, equivalently,
(c −yA)x = 0 = y(Ax −b).
In the above, (c −yA)x = 0 is called the primal complementary slackness condi-
tion, while y(Ax −b) = 0 is called the dual complementary slackness condition.
The duality theory of linear programming provides us with a new tool to ap-
proach some approximation problems from a different direction. For instance, we
mentioned, in Section 7.3, that the 2-approximation for MIN-VC, which is based on
maximum matching, cannot be extended immediately to the weighted version MIN-
WVC. Nevertheless, with the duality theory, we can look at this approximation from
a different angle and get an extension.
Consider the unweighted case of the vertex cover problem MIN-VC. Assume
that the input to MIN-VC is a graph G = (V, E), where V = {v1, v2, . . ., vn}. We
may formulate the problem as the following integer program:

300
Primal-Dual Schema
minimize
x1 + x2 + · · · + xn
subject to
xi + xj ≥1,
{vi, vj} ∈E,
xi ∈{0, 1},
i = 1, 2, . . ., n.
(8.4)
A natural relaxation of the above integer linear program is as follows:
minimize
x1 + x2 + · · · + xn
subject to
xi + xj ≥1,
{vi, vj} ∈E,
xi ≥0,
i = 1, 2, . . ., n.
Its dual linear program is as follows:
maximize

{vi,vj}∈E
yij
subject to

j:{vi,vj}∈E
yij ≤1,
i = 1, 2, . . ., n,
yij ≥0,
{vi, vj} ∈E.
(8.5)
Now, consider any 0–1 dual feasible solution y (i.e., a feasible solution to the
dual linear program (8.5)). Note that for each vertex vi, the constraint

j:{vi,vj}∈E
yij ≤1
requires that, among all edges incident upon vi, there is at most one edge {vi, vj} ∈
E having yij = 1. This means that the set Y = &{vi, vj} ∈E | yij = 1' forms a
matching of the graph G. When Y is a maximal matching, the following assignment
is then a primal feasible solution for (8.4):
xi =
⎧
⎨
⎩
1,
if

j:(vi,vj)∈E
yij = 1,
0,
otherwise.
Indeed, this is exactly the 2-approximation for MIN-VC based on maximum match-
ing. Next, we show how to follow this approach to extend this 2-approximation
algorithm to the weighted case.
We ﬁrst formulate the weighted version MIN-WVC into an integer program:
minimize
c1x1 + c2x2 + · · · + cnxn
subject to
xi + xj ≥1,
{vi, vj} ∈E,
xi ∈{0, 1},
i = 1, 2, . . ., n.

8.1 Duality Theory
301
Then we relax it to the following linear program:
minimize
c1x1 + c2x2 + · · · + cnxn
subject to
xi + xj ≥1,
{vi, vj} ∈E,
xi ≥0,
i = 1, 2, . . ., n.
Its dual linear program is
maximize

{vi,vj}∈E
yij
subject to

j:{vi,vj}∈E
yij ≤ci,
i = 1, 2, . . ., n,
yij ≥0,
{vi, vj} ∈E.
(8.6)
In terms of the graph G, this dual linear program may be viewed as a general-
ized maximum matching problem: Maximize the total value of yij over all edges
{vi, vj}, under the constraint that the total value of all edges incident on a vertex vi
is bounded by ci. A simple idea of the algorithm, thus, is to repeatedly select an edge
{vi, vj} into the generalized matching, with the value yij of the edge maximized
within the bound max{ci, cj}. This idea leads to the following 2-approximation
algorithm for MIN-WVC.
Algorithm 8.A (Primal-Dual Approximation Algorithm for MIN-WVC)
Input: Graph G = ({v1, . . ., vn}, E), and vertex weights c = (c1, . . ., cn).
(1) Construct the dual linear program (8.6) from G and c.
(2) For each {vi, vj} ∈E do set yij ←0.
(3) While there exists some {vi, vk} ∈E such that

j:{vi,vj}∈E
yij < ci and

j:{vk,vj}∈E
ykj < ck do
yik ←yik + min

ci −

j:{vi,vj}∈E
yij, ck −

j:{vk,vj}∈E
ykj

.
(4) For i ←1 to n do
xi ←
/ 1,
if

j:{vi,vj}∈E
yij = ci,
0,
otherwise.
Theorem 8.4 Let xA be the output of Algorithm 8.A. Then set C = {vi | xA
i = 1}
is a 2-approximation for MIN-WVC.

302
Primal-Dual Schema
Proof. Let opt denote the optimal objective value of the input (G, c). For each edge
{vi, vj} ∈E, let 3yij denote the ﬁnal value of yij in Algorithm 8.A. From step
(3), we see that for every edge {vi, vk} ∈E, at least one of the endpoints vi has

j:(vi,vj)∈E 3yij = ci. Hence, every edge {vi, vk} in E is covered by set C.
To show n
i=1 cixA
i ≤2 · opt, we note that 3y = (3yij){vi,vj}∈E is a dual feasible
solution to (8.6), and hence

{vi,vj}∈E
3yij ≤opt.
(8.7)
Note that for each i = 1, 2, . . ., n, xA
i = 1 if and only if 
j:{vi,vj}∈E 3yij = ci.
Thus,
n

i=1
cixA
i =

xA
i =1
ci =

xA
i =1

j:{vi,vj}∈E
3yij ≤2

{vi,vj}∈E
3yij ≤2 · opt.
□
Now, let us examine more carefully the relationship between xA and 3y obtained
from Algorithm 8.A. From step (4) we see that for each i = 1, 2, . . ., n,
xA
i

ci −

j:{vi,vj}∈E
3yij

= 0.
That is, the primal complementary slackness condition holds. On the other hand, we
can see that the dual complementary slackness condition does not necessarily hold.
More precisely, for some edges {vi, vj} ∈E, we may not have the relationship
3yij(xA
i + xA
j −1) = 0.
Instead, we only have the following relaxed relationship:
3yij > 0
=⇒
1 ≤xA
i + xA
j ≤2,
which allows us to establish the performance ratio 2 for Algorithm 8.A. In other
words, we do not actually need the full power of the dual complementary slackness
condition to prove that the solution xA is a good approximation to the original
problem MIN-WVC. All we need is that 3y = (3yij){vi,vj}∈E is a dual feasible
solution of (8.6). This property alone is, by the duality theory, sufﬁcient to imply the
bound (8.7), which in turn gives us the constant bound 2 for the performance ratio of
Algorithm8.A. This observation suggests a general idea of designing approximation
algorithms based on the duality theory of linear programming. We elaborate in the
following.
In the LP-based approximations, we ﬁrst relax a minimization (or, a maximiza-
tion) problem Π to a linear program ΠLP. We then solve the linear program ΠLP,
and round its optimal solution optLP to a feasible solution for Π. Note that optLP is
a lower bound (or, respectively, an upper bound) for the optimal solution opt of Π,
and we often use the difference between optLP and opt to estimate the performance

8.2 General Cover
303
ratio of this approximation. Now, from the duality theory, we know that every dual
feasible solution provides us a lower bound (or, respectively, an upper bound) for
optLP of ΠLP and, hence, also a lower bound (or, respectively, an upper bound) for
the optimal solution opt of Π. This means that a “reasonably good” dual feasible
solution can also be used to establish the performance ratio of approximation. Thus,
we do not need to compute the exact value optLP of the optimal primal solution
of ΠLP. Instead, we may simply compute a reasonably good dual feasible solution
and convert it to a feasible solution of problem Π, and then use the difference be-
tween them to estimate the performance ratio. This method is called the primal-dual
schema.
The advantage of the primal-dual schema is that by avoiding the step of ﬁnding
the optimal primal solution, we can speed up the computation a lot, as the running
time of the software implementations for linear programming tends to be high. In
particular, the best-known implementation of the interior point method for linear
programming runs in time O(n3.5) (even though the theoretical time bound for it is
O(n3)). Indeed, for applications to certain types of online problems, computing the
optimal solution for the primal LP is impractical, and this speedup is necessary.
The following lemma gives a more precise mathematical interpretation of the
above idea.
Lemma 8.5 Let Π be a minimization integer program and ΠLP its LP-relaxation.
Suppose a primal (integer) feasible solution x of Π and a dual feasible solution y
of ΠLP satisfy the following conditions:
(i) (Relaxed primal condition) cx
r1
≤yAx ≤cx; and
(ii) (Relaxed dual condition) yb ≤yAx ≤r2yb.
Then cx ≤(r1r2)yb; that is, x is an (r1r2)-approximation.
Proof. cx ≤r1yAx ≤(r1r2)yb.
□
For instance, for the problem MIN-WVC, the primal complementary slackness
condition implies r1 = 1, and the relation (8.7) gives us the bound r2 = 2, and so
Algorithm 8.A is a 2-approximation for MIN-WVC. In the next two sections, we
study the application of the primal-dual schema to two speciﬁc problems.
8.2
General Cover
Recall the problem GENERAL COVER (GC) deﬁned in Chapter 2, which can be
formulated as the following integer linear program:
minimize
cx
subject to
Ax ≥b,
x ∈{0, 1}n,
(8.8)

304
Primal-Dual Schema
where A is an m × n matrix over N, c is an n-dimensional row vector over N,
and b is an m-dimensional column vector over N. In this section, we consider a
subproblem of GC in which all the components of b are equal to 1:
GC1:
minimize
cx
subject to
Ax ≥1m,
x ∈{0, 1}n,
(8.9)
where A is an m × n nonnegative integer matrix, c is a positive integer
n-dimensional row vector, and 1m is the m-dimensional column vector
with all of its components having value 1.2
Suppose A = (aij)1≤i≤m,1≤j≤n. Let f be the maximum of the row sum of
matrix A; that is, f = max1≤i≤m
n
j=1 aij. We are going to apply the primal-dual
schema to get an f-approximation algorithm for GC1 that runs in time O(n2).
The following are the primal and dual linear programs of a natural LP-relaxation
of the problem GC1:
(primal LP)
(dual LP)
minimize
cx
maximize
y1m
subject to
Ax ≥1m,
subject to
yA ≤c,
x ≥0,
y ≥0,
(8.10)
where x is an n-dimensional column vector, and y is an m-dimensional row vector.
An idea of approximation for GC1 based on the dual LP above is, similar to that
of Algorithm 8.A, to increase the values of yi as much as possible, without violating
the constraint yA ≤c. However, as this constraint yA ≤c is more complicated
than that in (8.6), it is not clear how we should increase the values of variables yi
in each stage. Let us study this issue more carefully through the complementary
slackness condition.
The complementary slackness condition between the two linear programs for
GC1 is
(c −yA)x = 0 = y(Ax −1m).
Suppose that x is a primal feasible solution and y is a dual feasible solution. Then,
by the constraints yA ≤c and Ax ≥1m, the above complementary slackness
condition can be divided into the following subconditions:
(CP) For each j = 1, 2, . . ., n, if m
i=1 aijyi < cj, then xj = 0; and
(CD) For each i = 1, 2, . . ., m, if n
j=1 aijxj > 1, then yi = 0.
2Note that the requirement of c being a positive vector is not too restrictive: If a component of c, say
cj, is equal to 0, then we may set xj = 1 and remove, for each i, the ith row of A if aij ≥1 to get an
equivalent LP with c ≥1n.

8.2 General Cover
305
Our goal is to keep the difference cx −y1m between the objective function values
of the two linear programs as small as possible. Note that
cx −y1m = (c −yA)x + y(Ax −1m).
(8.11)
Thus, the more conditions in (CP) and (CD) above are satisﬁed, the closer the val-
ues cx and y1m are. On the other hand, we cannot expect all subconditions to be
satisﬁed when we round x and y to integer solutions.
For instance, we may, following the approach of Algorithm 8.A for problem
MIN-WVC, try to satisfy all the primal subconditions in (CP), and simply deﬁne
the approximate solution xA from y as follows:
xA
j =
1,
if m
i=1 aijyi = cj,
0,
if m
i=1 aijyi < cj.
(8.12)
The problem with this approach is that, while this assignment for xA would satisfy
the primal complementary slackness condition, it may not be primal feasible itself.
In this case, we need to go back to modify y to make the corresponding xA primal
feasible. Thus, it suggests the following general structure of the algorithm: We start
with an initial dual feasible y and iteratively modify it until the corresponding xA
(as deﬁned by (8.12)) becomes primal feasible.
Now, under this framework, how do we proceed in each iteration? We observe
that, in each iteration, we want to make xA closer to a feasible solution for the
primal problem. To do so, we need to increase the number of components of xA
that have value 1 (since A is nonnegative); or, equivalently, from (8.12), we need
to modify y to increase the number of j’s satisfying m
i=1 aijyi = cj. This in turn
amounts to increasing some values of yi. For which indices i and for what amount
should we increase the values of yi? Let us examine the complementary slackness
condition (8.11) again.
First, we note that if xA does not satisfy AxA ≥1m, then the set I = {i |
1 ≤i ≤m, n
j=1 aijxA
j = 0} is nonempty. For an index i ̸∈I, increasing yi could
increase the second term y(Ax−1m) of (8.11), and hence increase the gap between
cx and y1m. This means that we should not increase these yi’s. On the other hand,
for an index i ∈I, increasing yi will actually decrease the gap between cx and
y1m. So we should try to increase yi’s only for those i ∈I. In addition, we note
that we need to keep the new y dual feasible. That is, the new values of yi must still
satisfy m
i=1 aijyi ≤cj for all j. This condition suggests that we should increase
the values of yi, for all i ∈I, simultaneously, until one of the sum m
i=1 aijyi
reaches the value cj.
The above analysis yields the following algorithm.
Algorithm 8.B (Primal-Dual Schema for GC1)
Input: An m × n nonnegative integer matrix A and c ∈(Z+)n.
(1) Set x0 ←0; y0 ←0; k ←0.

306
Primal-Dual Schema
(2) While xk is not primal feasible do
Jk ←{j | 1 ≤j ≤n, xk
j = 0};
Ik ←{i | 1 ≤i ≤m, n
j=1 aijxk
j ≤0};
Choose r ∈Jk such that
cr −m
i=1 airyk
i

i∈Ik air
= α = min
j∈Jk
cj −m
i=1 aijyk
i

i∈Ik aij
;
For j ←1 to n do
if j = r then xk+1
j
←1 else xk+1
j
←xk
j ;
For i ←1 to m do
if i ∈Ik then yk+1
i
←yk
i + α else yk+1
i
←yk
i ;
k ←k + 1.
(3) Output xA = xk.
Algorithm 8.B runs in time O(n(m+n)) because the algorithm runs at most n it-
erations and each iteration takes time O(m+n) (note that the value cj −m
i=1 aijyk
i
can be updated from that of the (k −1)st iteration in time O(1)).
Next, we show that it has the performance ratio f.
Lemma 8.6 During the execution of Algorithm 8.B, the following properties hold
for all k ≥0:
(a) yk is dual feasible.
(b) (c −ykA)xk = 0.
(c) ykAxk ≤fyk1m, where f = max1≤i≤m
n
j=1 aij.
Proof. We prove properties (a) and (b) by induction on k. It is clear that conditions
(a) and (b) are true with respect to the initial values x0 = y0 = 0. Next, suppose
they hold true for some k ≥0 and consider the case of k + 1.
For condition (a), we note that, from condition (a) of the induction hypothesis,
yk is dual feasible, and so α must be nonnegative, and so yk+1
i
≥yk
i ≥0 for all i =
1, 2, . . ., m. First, consider the case of j ̸∈Jk. From condition (b) of the induction
hypothesis, we know that if j ̸∈Jk, then cj −m
i=1 aijyk
i = 0. Furthermore, for
each i ∈Ik, we have n
j=1 aijxk
j = 0, and so aij = 0 for each j ̸∈Jk. It follows
that
cj −
m

i=1
aijyk+1
i
= cj −

i̸∈Ik
aijyk+1
i
= cj −

i̸∈Ik
aijyk
i = 0.
Next, for the case j ∈Jk, we know, by the choice of α, that
α

i∈Ik
aij ≤cj −
m

i=1
aijyk
i .

8.2 General Cover
307
Thus, for j ∈Jk,
cj −
m

i=1
aijyk+1
i
= cj −
m

i=1
aijyk
i −α

i∈Ik
aij ≥0.
So, yk+1 is dual feasible.
For condition (b), consider an index j ∈{1, 2, . . ., n} with m
i=1 aijyk+1
i
< cj.
Since yk ≤yk+1, we know that m
i=1 aijyk
i < cj. By the induction hypothesis,
xk
j = 0. In addition, we have, from the choice of r,
m

i=1
airyk+1
i
=
m

i=1
airyk
i +

i∈Ik
airα =
m

i=1
airyk
i + cr −
m

i=1
airyk
i = cr.
Therefore, j ̸= r, and we must have xk+1
j
= xk
j = 0.
Finally, for condition (c), we note that
ykAxk =
m

i=1
yk
i

n

j=1
aijxk
j

≤
m

i=1
yk
i

n

j=1
aij

≤f
m

i=1
yk
i = fyk1m,
and the lemma is proven.
□
Theorem 8.7 Let opt be the optimal value of the problem GC1. The solution xA
produced by Algorithm 8.B satisﬁes
cxA ≤f · opt,
where f = max1≤i≤m
n
j=1 aij.
Proof. By Lemma 8.6 and Theorem 8.1, we have
cxA = ykAxk ≤f · yk1m ≤f · opt,
where k is the ﬁnal value of the variable k in Algorithm 8.B.
□
From the proof of Lemma 8.6, we see that property (c) of Lemma 8.6 holds for
every dual feasible solution yk. Therefore, we have cx ≤f ·opt, as long as a primal
feasible solution x and a dual feasible solution y satisfy the primal complementary
slackness condition (c −yA)x = 0. This observation shows that the following
variation of Algorithm 8.B has the same performance ratio f as Algorithm 8.B.
Algorithm 8.C (Second Primal-Dual Schema for GC1)
Input: An m × n nonnegative integer matrix A and c ∈(Z+)n.
(1) Set x0 ←0; y0 ←0; k ←0.

308
Primal-Dual Schema
(2) While xk is not primal feasible do
Select an index i′ such that n
j=1 ai′jxk
j = 0;
Jk ←{j | xk
j = 0 and ai′j > 0};
Choose r ∈Jk such that
cr −m
i=1 airyk
i
ai′r
= α = min
j∈Jk
cj −m
i=1 aijyk
i
ai′j
;
For j ←1 to n do
if j = r then xk+1
j
←1 else xk+1
j
←xk
j ;
For i ←1 to m do
if i ∈Ik then yk+1
i
←yk
i + α else yk+1
i
←yk
i ;
k ←k + 1.
(3) Output xA = xk.
It is interesting to point out that neither Algorithm8.B nor Algorithm8.C requires
solving a linear program. The theory of linear programming is used as an inspiration
and as an analysis tool only. It is therefore natural to ask whether we can design
such algorithms without the knowledge of linear programming at all. The answer is
afﬁrmative. We will introduce an equivalent local ratio method in later sections.
Finally, we remark that, for a single integer program, there are often more than
one way to relax it to linear programs. For instance, in Algorithms 8.B and 8.C,
we used the primal and dual linear programs obtained from GC1 by relaxing the
condition “xj ∈{0, 1}” to “xj ≥0.” One might ask why we did not relax it to
a stronger condition “0 ≤xj ≤1.” As to be seen below, the reason is that the
primal-dual algorithm obtained from the stronger relaxation is actually weaker than
Algorithms 8.B and 8.C. To see this, let us consider this relaxation:
minimize
cx
subject to
Ax ≥1m,
0 ≤x ≤1n.
To ﬁnd a primal-dual algorithm based on this relaxation, we ﬁrst write this linear
program and its dual linear program in the symmetric form of (8.3):
(primal LP)
(dual LP)
minimize
cx
maximize
y1m −z1n
subject to
Ax ≥1m,
subject to
yA −z ≤c,
−x ≥−1n,
y ≥0, z ≥0,
x ≥0,
where y ∈Rm, z ∈Rn are row vectors.

8.2 General Cover
309
Following the analysis of the primal and dual linear programs of (8.10), we can
express the difference between the two objective functions as
cx −y1m + z1n = y(Ax −1m) + z(1n −x) + (c −yA + z)x.
Correspondingly, the complementary slackness condition of the new pair of primal
and dual linear programs above is
y(Ax −1m) + z(1n −x) + (c −yA + z)x = 0.
Now, we can follow the approach of Algorithm 8.B to approximate GC1.
Namely, we want to increase the number of components of x to have value 1 and,
in the meantime, keep (y, z) dual feasible. Notice that when we increase the value
of xj from 0 to 1, we need to change the values of the yi’s and zj’s to satisfy
m
i=1 aijyi −zj = cj. Since increasing the values of the zj’s only means we need
to increase more to the values of the yi’s, we can just focus on increasing the yi’s.
Thus, the criteria for selecting the components of y to increase are the same as those
for Algorithm 8.B. The only difference here is that we need to, if necessary, adjust
the values of other zk’s to make sure that n
i=1 aikyk −zk is no greater than ck.
These observations lead to the following primal-dual algorithm for GC1:
Algorithm 8.D (Third Primal-Dual Schema for GC1)
Input: An m × n nonnegative integer matrix A and c ∈(Z+)n.
(1) Set x0 ←0; y0 ←0;, z0 ←0; k ←0.
(2) While xk is not prime feasible do
Jk ←{j | xk
j = 0};
Ik ←{i | n
j=1 aijxk
j = 0};
Choose r ∈Jk such that
cr −m
i=1 airyk
i

i∈Ik air
= α = min
j∈Jk
cj −m
i=1 aijyk
i

i∈Ik aij
;
For i ←1 to m do
if i ∈Ik then yk+1
i
←yk
i + α else yk+1
i
←yk
i ;
For j ←1 to n do
if j = r then xk+1
j
←1 else xk+1
j
←xk
j ;
zk+1
j
←max
& m
i=1 aijyk+1
i
−cj, 0
'
;
k ←k + 1.
(3) Output xA = xk.
Comparing Algorithm 8.D with Algorithm 8.B, we ﬁnd that z is redundant. In-
deed, in Algorithm 8.D, we did not use zk in the computation of xk+1 and yk+1.
So we may as well remove the variables in z from the relaxed LP. Note that z was

310
Primal-Dual Schema
introduced by the extra constraints x ≤1n, and so removing the variables in z is
equivalent to removing the constraints x ≤1n.
Another interesting observation about the removal of z is that after z is removed,
the lower bound for the optimal solution of the original integer linear program is
actually improved from yb −z1n to yb.
8.3
Network Design
For many subproblems of GENERAL COVER (called covering-type problems), we
can often use the primal-dual method to obtain approximations with performance
ratios better than f as shown in Theorem 8.7. For instance, consider the following
subclass of covering-type problems:
NETWORK DESIGN: Given a graph G = (V, E) with nonnegative edge
costs ce, for e ∈E, solve the integer program
minimize

e∈E
cexe
subject to

e∈δ(S)
xe ≥f(S),
∅̸= S ⊂V,
xe ∈{0, 1},
e ∈E,
(8.13)
where δ(S) is the set of edges between S and V −S (i.e., the cut
between S and V −S), and f(S) is a 0–1 function over 2V .
The following are two speciﬁc instances of the network design problem:
TREE PARTITION: Given a graph G = (V, E) with nonnegative edge
costs ce, for e ∈E, and a positive integer k, ﬁnd the minimum-cost
subset of edges that partitionsall vertices into trees of at least k vertices.
STEINER FOREST: Given a graph G = (V, E) with edge costs ce,
for e ∈E, and m disjoint subsets P1, P2, . . ., Pm of vertices, ﬁnd a
minimum-cost forest F of G such that every set Pi is contained in a
connected component of F .
The problem TREE PARTITION can be formulated as the integer program (8.13)
with the following f(S):
f(S) =
1,
if 0 < |S| < k,
0,
otherwise.
STEINER FOREST can be formulated as the integer program (8.13) with the follow-
ing f(S):

8.3 Network Design
311
f(S) =
 1,
if (∃Pi) [S ∩Pi ̸= ∅̸= (V −S) ∩Pi],
0,
otherwise.
In both instances above, the function f(S) satisﬁes the following maximality
property: For any two disjoint sets A, B ⊆V ,
f(A ∪B) ≤max{f(A), f(B)}.
In the network design problem, if a vector x = (xe)e∈E is not a feasible solution,
then there must be a nonempty vertex subset S ⊆V such that

e∈δ(S)
xe < f(S).
We call such a set S ⊆V a violated set (with respect to x). If, furthermore, no
proper nonempty subset T of S satisﬁes

e∈δ(T )
xe < f(T),
then we call S a minimal violated set. We denote by Violate(x) the collection of all
minimal violated sets with respect to x. When a network design problem has the
maximality property, the minimal violated sets have a nice characterization.
Lemma 8.8 Suppose f(S) is a 0–1 function over 2V with the maximality property.
Then, for any x, every minimal violated set S is a connected component of graph
Gx = (V, {e | xe = 1}).
Proof. Note that if S is a violated set, then we must have
0 =

e∈δ(S)
xe < f(S) = 1.
This means that for any edge e ∈δ(S), xe = 0. Thus, S is a union of connected
components of the graph Gx.
If S contains more than one connected component, then, by the maximality prop-
erty, f(T) = 1 for some connected component T in S. Thus,

e∈δ(T )
xe = 0 < f(T) = 1,
and T is a violated set. It follows that S is not a minimal violated set.
□
The above lemma indicates that for each x, the set of all minimal violated sets is
easy to compute, and hence suggests the followingsimpliﬁed primal-dual algorithm.
Algorithm 8.E (Primal-Dual Schema for NETWORK DESIGN)
Input: A graph G = (V, E) with edge costs ce, for e ∈E, and a function f : 2V →
{0, 1} (given implicitly).

312
Primal-Dual Schema
(1) x ←0; For every S ⊆V do yS ←0.
(2) While Violate(x) ̸= ∅do
{Increase the values of yS simultaneously for all minimal
violated sets S until some edge e becomes tight.}
Let e∗be the edge that reaches the minimum
α =
min
e∈E,xe=0
ce −
S:e∈δ(S) yS
|Violate(x) ∩{S | e ∈δ(S)}|;
For each S ∈Violate(x) do yS ←yS + α;
xe∗←1.
(3) For each e ∈E do
let x′ be the vector x modiﬁed with x′
e ←0;
if x′ is primal feasible then x ←x′.
(4) Output x.
Let us analyze the running time of Algorithm 8.E ﬁrst. We note that, in general,
the network design problem has an exponential number of constraints (with respect
to the size of the input graph G). Thus, a straightforward implementation of Algo-
rithm 8.E would take superpolynomial time. However, when the function f(S) has
the maximality property, Algorithm 8.E can be implemented to run in polynomial
time. To see this, we note that if f(S) has the maximality property, then, by Lemma
8.8, each set S ∈Violate(x) is a connected component of Gx. So, in each itera-
tion of Algorithm 8.E, there are only polynomially many minimal violated sets and
we can compute them in polynomial time. Moreover, the value of yS may become
nonzero only if S is a minimal violated set. Therefore, in each iteration, there are
only polynomially many nonzero terms in the sum te = 
S:e∈δ(S) yS. From this
observation, we can implement steps (1) and (2) of Algorithm 8.E as follows to
make it run in polynomial time:
(1) x ←0; For every e ∈E do te ←0.
(2) While Violate(x) ̸= ∅do
Let e∗be the edge that reaches the minimum
α =
min
e∈E,xe=0
ce −te
|Violate(x) ∩{S | e ∈δ(S)}|;
For each e ∈E do
for each S ∈Violate(x) do
if e ∈δ(S) then te ←te + α;
xe∗←1.
Next, we consider the performance ratio of Algorithm 8.E. A function f is down-
ward monotone if
∅̸= T ⊂S ⇒f(S) ≤f(T).
Clearly, downward monotonicity implies maximality. We note that the function f
deﬁning TREE PARTITION is downward monotone, while that for STEINER FOREST
is not.

8.3 Network Design
313
Theorem 8.9 Suppose the input function f(S) in Algorithm 8.E is downward
monotone. Then Algorithm 8.E is a 2-approximation for the associated network
design problem.
Proof. For any primal value x, let F (x) = {e ∈E | xe = 1}, and let F ∗denote
the set F (x) corresponding to the output x of Algorithm 8.E. Note that for each
e ∈F ∗, 
S:e∈δ(S) yS = ce. Therefore, we have

e∈F ∗
ce =

e∈F ∗

S:e∈δ(S)
yS
=

S⊆V

e∈δ(S)∩F ∗
yS =

S⊆V
degF ∗(S) · yS,
where degF ∗(S) = |δ(S) ∩F ∗|. Now, from Lemma 8.5, it sufﬁces to prove

S⊆V
degF ∗(S) · yS ≤2

S⊆V
yS.
(8.14)
To get (8.14), we note that it is sufﬁcient to show that at each iteration,

S∈Violate(x)
degF ∗(S) ≤2 · |Violate(x)|.
(8.15)
To see this, let xk denote the value of x at the beginning of the kth iteration, and let
αk be the minimum value α found in the kth iteration. Thus, in the kth iteration, we
added αk to yS for each S ∈Violate(xk). So the right-hand side of (8.14) can be
decomposed into
2

S⊆V
yS = 2
K

k=1
αk · |Violate(xk)|,
assuming Algorithm 8.E halts after K iterations. Moreover, the sum on the left-hand
side of (8.14) can also be decomposed into

S⊆V
degF ∗(S) · yS =
K

k=1

S∈Violate(xk)
degF ∗(S) · αk
=
K

k=1
αk

S∈Violate(xk)
degF ∗(S).
Thus, to get (8.14), it sufﬁces to show that for each k,

S∈Violate(xk)
degF ∗(S) ≤2 · |Violate(xk)|.
Now, in order to prove (8.15), construct a graph H with the vertex set V (H)
containing all connected components of the graph Gx = (V, F (x)) and the edge

314
Primal-Dual Schema
set E(H) = F ∗−F (x). From step (3) of Algorithm 8.E and the fact that f(S) ∈
{0, 1}, we know that H is acyclic. Therefore, the number of edges in H equals the
number of vertices minus the number of connected components in H. It follows that

S∈Violate(x)
degF ∗(S) = 2|F ∗−F (x)| ≤2(|V (H)| −c),
where c is the number of connected components in H.
To prove (8.15), we show that each connected component of H contains at most
one vertex S such that f(S) = 0. For the sake of contradiction, suppose there
exist two vertices S1 and S2 in a connected component C of H such that f(S1) =
f(S2) = 0. Let e be an edge of H in the path between S1 and S2. Then e ∈F ∗
and, by step (3) of Algorithm 8.E, F ∗−{e} is not feasible. Thus, there exists a
set S ⊂V such that e ∈δ(S), f(S) = 1, and (F ∗−{e}) ∩δ(S) = ∅. Since H
is acyclic, the removal of e splits the connected component C into two connected
components A and B. Since (F ∗−{e}) ∩δ(S) = ∅, we must have either A ⊆S
or B ⊆S and, consequently, either S1 ⊆S or S2 ⊆S. However, by the downward
monotone property of f, we would have either f(S1) = 1 or f(S2) = 1, which
leads to a contradiction.
Since each connected component of H contains at most one vertex S with
f(S) = 0, all but c many vertices S of H are in Violate(x). We conclude that
|V (H)| −c ≤|Violate(x)|, and (8.15) is proven.
□
Corollary 8.10 Algorithm 8.E is a 2-approximation for TREE PARTITION.
A function f over 2V is said to be symmetric if f(S) = f(V −S) for all S ⊂
V . The function f deﬁning the problem STEINER FOREST is symmetric with the
maximality property.
Lemma 8.11 Let f be a 0–1 symmetric function on 2V with the maximality prop-
erty. Then f(A) = f(B) = 0 implies f(A \ B) = 0.
Proof. By the symmetry property of f, f(V −A) = 0. So, by the maximality
property, f((V −A) ∪B) = 0. Now the lemma follows from the fact of V −(A \
B) = (V −A) ∪B.
□
Theorem 8.12 Assume that f is a 0–1 symmetric function on 2V with the maximal-
ity property. Then Algorithm 8.E is a 2-approximation for the associated network
design problem.
Proof. Following the proof of Theorem 8.9, we see that it is sufﬁcient to show (8.15).
Also, consider the graph H constructed in the same proof. We claim that for every
leaf vertex S of H, f(S) = 1. For the sake of contradiction, suppose that S is a leaf
of H with f(S) = 0. Let e be the unique edge in E(H) = F ∗−F (x) incident
upon S, and let C be the connected component of graph (V, F ∗) that contains S.
Since F ∗is feasible, we must have f(C) = 0 and so, by Lemma 8.11, f(C −S)

8.4 Local Ratio
315
is also equal to 0. However, we note that F ∗−{e} is not feasible, which implies
either f(S) = 1 or f(C −S) = 1 and gives us a contradiction.
The above claim implies that every vertex S of H that is not in Violate(x) has
degree at least 2. Therefore,

S∈Violate(x)
degF ∗(S) =

S∈V (H)
degF ∗(S) −

S̸∈Violate(x)
degF ∗(S)
≤2(|V (H)| −1) −2(|V (H)| −|Violate(x)|)
= 2|Violate(x)| −2.
□
Corollary 8.13 Algorithm 8.E is a 2-approximation for STEINER FOREST.
8.4
Local Ratio
Local ratio is a simple, yet powerful, technique for designing approximation algo-
rithms with broad applications. It also has a close relationship with the primal-dual
schemas in linear programming. In this section, we study some examples.
The main idea of the local ratio method comes from the following observation:
Theorem 8.14 (Local Ratio Theorem) Assume that in a minimization problem
min{c(x) | x ∈Ω},
we can decompose the cost function c into c = c1 + c2. If x ∈Ω is an r-
approximation with respect to both cost functions c1 and c2, then x is also an r-
approximation with respect to the cost function c.
Proof. Suppose x∗
1, x∗
2, and x∗are optimal solutions with respect to cost functions
c1, c2, and c, respectively. Then we have
c1(x) ≤rc1(x∗
1) ≤rc1(x∗),
c2(x) ≤rc2(x∗
2) ≤rc2(x∗).
Therefore,
c(x) = c1(x) + c2(x) ≤rc1(x∗) + rc2(x∗) = rc(x∗).
□
To see the applications of the local ratio theorem, let us ﬁrst review the weighted
vertex cover problem, MIN-WVC. Given a graph G = (V, E) with nonnegative
vertex weight c, we choose an edge {u, v} with c(u) > 0 and c(v) > 0. (If such
an edge does not exist, then all vertices with weight zero form an optimal solution.)
Suppose c(u) ≤c(v). Deﬁne c1(u) = c1(v) = c(u), and c1(x) = 0 for x ∈
V −{u, v}. Then, any feasible solution is a 2-approximation with respect to c1. So
the problem is reduced to ﬁnding a 2-approximation for the problem with respect to

316
Primal-Dual Schema
the cost function c2 = c −c1. If all vertices x with c2(x) = 0 form a vertex cover,
then it is optimal with respect to c2 and clearly also a 2-approximation solution
with respect to c. Otherwise, we can continue the above process to decompose the
weight function c2 and to generate a new subproblem with more vertices having
weight zero. This algorithm is summarized as follows.
Algorithm 8.F (Local Ratio Algorithm for MIN-WVC)
Input: A graph G = (V, E) with a nonnegative vertex weight function c : V →N.
(1) While ∃{u, v} ∈E with c(u) > 0 and c(v) > 0 do
c1 ←min{c(u), c(v)};
c(u) ←c(u) −c1;
c(v) ←c(v) −c1.
(2) Output {v | c(v) = 0}.
It is inspiring to compare this algorithm with the Second Primal-Dual Schema
(Algorithm 8.C). We rewrite Algorithm 8.C in the following for the problem MIN-
WVC, in which we write, for a vertex v ∈V , E(v) to denote the set of all edges
incident on v.
Algorithm 8.C (Revisited, for MIN-WVC)
Input: A graph G = (V, E) with a nonnegative vertex weight function c : V →N.
(1) x0 ←0; y0 ←0; k ←0;
(2) While xk is not primal feasible (i.e., {j | xk
j = 1} is not a vertex cover) do
(2.1) Choose an uncovered edge i′ = {u, v};
(2.2) Choose r ∈{u, v} such that
α = cr −

i∈E(r)
yk
i =
min
j∈{u,v}

cj −

i∈E(j)
yk
i

;
(2.3) For j ←1 to n do
if j = r then xk+1
j
←1 else xk+1
j
←xk
j ;
(2.4) For i ←1 to m do
if i = i′ then yk+1
i
←yk
i + α else yk+1
i
←yk
i ;
(2.5) k ←k + 1.
(3) Output xk.
Note that if we update the cost function by setting
cj ←cj −

i∈E(j)
yk+1
i

8.4 Local Ratio
317
after line (2.4), and replace the deﬁnition of α of line (2.2) by
α = cr =
min
j∈{u,v}cj,
then Algorithm 8.C is reduced to exactly Algorithm 8.F. In other words, these two
algorithms are actually equivalent.
In general, it is easy to see that Algorithm 8.C is equivalent to the following local
ratio algorithm for GC1.
Algorithm 8.G (Local Ratio Algorithm for GC1)
Input: An m × n nonnegative integer matrix A and c ∈(Z+)n.
(1) Set x ←0.
(2) While x is not feasible do
Select an index i′ such that n
j=1 ai′jxj = 0;
Set J ←{j | xj = 0 and ai′j > 0};
Choose j′ such that α = cj′
ai′j′ = min
j∈J
cj
ai′j
;
xj′ ←1;
For j ←1 to n do cj ←cj −ai′jα.
(3) Output x.
What about the First Primal-Dual Schema? Is there a local ratio algorithm equiv-
alent to Algorithm 8.B? The answer is yes. The following is such an algorithm for
the problem MIN-WVC. We leave the general local ratio algorithm for GC1 as an
exercise.
Algorithm 8.H (Second Local Ratio Algorithm for MIN-WVC)
Input: A graph G = (V, E) with a nonnegative vertex weight function c : V →N.
(1) C ←∅.
(2) While G ̸= ∅do
Choose u ∈V such that
c(u)
degG(u) = min
v∈V
c(v)
degG(v);
For every {u, v} ∈G do c(v) ←c(v) −
c(u)
degG(u);
C ←C ∪{u};
V ←V −{u};
G ←G|V .
(3) Output C.
In each iteration of the above algorithm, the cost function c is decomposed into
two parts c = c1 + c2, where c1(u) = c(u) and c1(v) = c(u)/ degG(u) for each

318
Primal-Dual Schema
v ∈G that is adjacent to u, and c1(v) = 0 otherwise. Thus, any vertex cover for G is
a 2-approximationwith respect to c1. So it provides us with another 2-approximation
for MIN-WVC.
In general, a local ratio algorithm can be divided into the following two steps:
Step 1. Find a type of weight function c1 with which an r-approximation can be
constructed.
Step 2. Reduce the general weight c by a weight function c1 of the above special
type iteratively until a feasible solution can be found trivially.
In all of Algorithms 8.F, 8.G, and 8.H, step 1 is somewhat trivial, in the sense
that the cost function c1 found has the property that any feasible solution for the
problem is a 2-approximation for c1. In general, can we expect to always ﬁnd such
a trivial function c1? The answer is no, as demonstrated by the following example.
PARTIAL VERTEX COVER (PVC): Given a graph G = (V, E) with
nonnegative vertex weight c : V →N, and an integer k > 0, ﬁnd a
minimum-weight subset of vertices that covers at least k edges.
We note that in the general cases of this problem, no single vertex subset must
contribute to all feasible solutions. Thus, it is hard to ﬁnd a function c1 with respect
to which any feasible solution is trivially a 2-approximation. In such situations, we
focus instead on minimal feasible solutions.
A feasible solution is said to be minimal if none of its proper subsets is feasible.
The idea here is to ﬁnd a cost function c1 with respect to which every minimal
feasible solution is a 2-approximation. To do so, we consider the minimum cost
needed to cover a single edge in graph G. Suppose a feasible solution includes a
vertex v, which has degree deg(v) ≤k. Then, vertex v covers deg(v) edges with
cost c(v), and so each edge incident on v incurs cost c(v)/ deg(v). If deg(v) >
k, then each edge incurs cost c(v)/k since we only need to cover k edges. This
observation suggests we assign c1(v) as follows:
First, let α be the minimum cost to cover a single edge; that is,
α = min
v∈V
c(v)
min{k, deg(v)}.
Next, for every u ∈V , deﬁne c1(u) to be the cost of covering all edges (up to k
many) incident on u:
c1(u) = α · min{k, deg(u)}.
Lemma 8.15 Every minimal feasible solution for G is a 2-approximation with re-
spect to cost function c1.
Proof. From the deﬁnition of α, we know that covering any edge in G costs at least
α. Therefore, kα is a lower bound for the optimal solution opt.
Now, consider a minimal feasible solution C for graph G. If C contains a vertex
v such that deg(v) ≥k, then C = {v} with cost kα. Therefore, we may assume

8.4 Local Ratio
319
that C contains at least two vertices, all with degree < k. In this case, the total cost
of C is equal to α · 
v∈C deg(v). Since kα is a lower bound of the optimal cost, it
sufﬁces to show 
v∈C deg(v) ≤2k.
For each vertex v ∈C and i ∈{1, 2}, let di(v) denote the number of edges
incident on v that have i endpoints in C. Then deg(v) = d1(v)+d2(v). Choose v∗∈
C with d1(v∗) = minv∈C d1(v). Then, d1(v∗) ≤
v∈C−{v∗} d1(v). Next, observe
that the total number of edges covered by C is equal to 
v∈C(d1(v) + d2(v)/2).
Since C is minimal, we must have
1
2

v∈C
d2(v) +

v∈C−{v∗}
d1(v) < k,
for otherwise C −{v∗} would be feasible, violating the minimality assumption
about C. Therefore,

v∈C
deg(v) = d1(v∗) +

v∈C−{v∗}
d1(v) +

v∈C
d2(v)
≤2
 1
2

v∈C
d2(v) +

v∈C−{v∗}
d1(v)

< 2k.
□
Corollary 8.16 The problem PARTIAL VERTEX COVER has a polynomial-time 2-
approximation.
Next, we consider the following problem. We say a subset F of vertices of a
graph G = (V, E) is a feedback vertex set if the removal of F results in an acyclic
graph, that is, if G|V −F is acyclic.
FEEDBACK VERTEX SET (FVS): Given a graph G = (V, E) with non-
negative vertex weight w : V →N, ﬁnd a minimum-weight feedback
vertex set of G.
A feedback vertex set F is said to be minimal if no proper subset of F is a
feedback vertex set. To design a local ratio algorithm for this problem, we follow
the same idea in the design of function c1 for the problem PVC and deﬁne the
following special weight function:
w1(u) = ε · deg(u),
where ε is a positive constant.
Lemma 8.17 Let G be a graph and w1 a weight function deﬁned above. Suppose
each vertex in G has degree at least 2. Then every minimal feedback vertex set F is
a 2-approximation for FVS with respect to weight w1.
Proof. Since F is minimal, for each u ∈F, there exists a cycle Cu such that u is the
only vertex in F contained in Cu. For each u ∈F, ﬁx the cycle Cu and let Pu be the

320
Primal-Dual Schema
path obtained from Cu by deleting u. Denote by G1 the subgraph of G consisting
of all connected components of G|V −F that contain such a path Pu. Let V1 be the
vertex set of G1, and V2 = V −F −V1. For i = 1, 2, deﬁne ni = |Vi|, and deﬁne
mi to be the number of edges in G incident on vertices in Vi. In addition, deﬁne mF
to be the number of edges in G between vertices in F and, for i = 1, 2, deﬁne m′
i
to be the number of edges in G between a vertex in Vi and a vertex in F . Now, we
observe the following relationships between these parameters:
(a) The total degree of vertices in F can be expressed as

u∈F
deg(u) = m′
1 + m′
2 + 2mF .
(b) The total degree of vertices in V2 is

u∈V2
deg(u) = 2(m2 −m′
2) + m′
2 = 2m2 −m′
2.
Since each vertex in G has degree at least 2, we have 2n2 ≤
u∈V2 deg(u) and,
hence,
m′
2 ≤2(m2 −n2).
(c) Let F ∗be a minimum feedback vertex set with respect to weight w1. We
claim that
m′
1 ≤m1 −n1 + |F ∗|.
To see this, we ﬁrst note that each connected component of G1 is a tree, and so
m′
1 = m1 −n1 + k, where k is the number of connected components of G1. Next,
we note that each connected component of G1 contains a Pu, and each Cu must
contain a vertex in F ∗. Thus, either u ∈F ∗or Pu contains a vertex in F ∗\ F . It
follows that each connected component of G1 contains either a vertex in F ∗\ F or
a Pu with u ∈F ∗∩F . This means that k ≤|F ∗\ F | + |F ∗∩F | = |F ∗|, and the
claim is proven.
(d) Since each vertex in F has at least two edges going to vertices in V1, we have
2|F | ≤m′
1.
From the above relationships, we get

u∈F
deg(u) ≤m1 −n1 + |F ∗| + 2m2 −2n2 + 2mF
= 2(m1 + m2 + mF ) −2(n1 + n2) −(m1 −n1 + |F ∗|) + 2|F ∗|
≤2|E| −2|V | + 2|F | −m′
1 + 2|F ∗|
≤2(|E| −|V | + |F ∗|) ≤2

u∈F ∗
deg(u).
The last inequality above is derived as follows: After removing F ∗, the graph G has
no cycles and, hence, has at most |V | −|F ∗| −1 edges left. This means that at least

8.4 Local Ratio
321
|E| −|V | + |F ∗| + 1 edges have been removed, a number that cannot exceed the
total degree 
u∈F ∗deg(u) of vertices in F ∗.
□
The above lemma suggests the following local ratio algorithm.
Algorithm 8.I (Local Ratio Algorithm for FVS)
Function FVS(G, w)
(1) If G = ∅then return ∅.
(2) If ∃u ∈V (G) with deg(u) ≤1 then return FVS(G −{u}, w).
(3) If ∃u ∈V (G) with w(u) = 0
then F ←FVS(G −{u}, w);
if F is a feedback set for G then return F
else return F ∪{u}
else set ε ←
min
u∈V (G)
w(u)
deg(u);
for all u ∈V (G) do w1(u) ←ε · deg(u);
return FVS(G, w −w1).
Theorem 8.18 Algorithm 8.I is a 2-approximation for FVS.
Proof. Let F ∗(G, w) denote an optimal solution for FVS on input (G, w). Also,
let F be the set returned by FVS(G, w). We show by induction that F is a mini-
mal feedback vertex set of G and is a 2-approximation to F ∗(G, w). For G = ∅,
this is trivially true. For general G, suppose u is the ﬁrst vertex deleted from G in
Algorithm 8.I. There are two cases.
Case 1. deg(u) ≤1. In this case, a vertex subset is a feedback vertex set of G if
and only if it is a feedback vertex set of G−{u}. By the induction hypothesis, F is a
minimal feedback vertex set of G−{u} and is a 2-approximation to F ∗(G−{u}, w).
It follows that F is also a minimal feedback vertex set of G and is a 2-approximation
to F ∗(G, w) = F ∗(G −{u}, w).
Case 2. w(u) = 0. In this case, every vertex v of G has deg(v) ≥2. Now
consider two subcases:
Subcase 2.1. u ̸∈F . From line 3 of step (3), we know that F is a feedback
vertex set of G. By the induction hypothesis, F is a minimal feedback vertex set of
G −{u} and hence is also minimal for G. In addition, F is a 2-approximation to
F ∗(G −{u}, w), and so it is also a 2-approximation to F ∗(G, w).
Subcase 2.2. u ∈F. By the induction hypothesis, F −{u} is a minimal feedback
vertex set of G −{u} but not a feedback vertex set of G. Therefore, F must be
a feedback vertex set of G and must also be minimal. Since w(u) = 0, F and
F −{u} have the same weight. Therefore, the induction hypothesis that F −{u}
is a 2-approximation to F ∗(G −{u}, w) implies that F is a 2-approximation to
F ∗(G, w).
Finally, we notice that, before a vertex u with w(u) = 0 is deleted from G, the
algorithm may have reduced the weight w to w −w1. In such a case, the above

322
Primal-Dual Schema
argument in case 2 showed that F is a minimal feedback vertex set of G and is a
2-approximation to F ∗(G, w −w1). By Lemma 8.17, F is also a 2-approximation
to F ∗(G, w1). Hence, by the local ratio theorem, F is also a 2-approximation to
F ∗(G, w).
□
Next, we study a maximization problem. Recall that a vertex subset S ⊆V of a
graph G = (V, E) is an independent set if no two vertices in S are connected by an
edge in E.
MAXIMUM-WEIGHT INDEPENDENT SET (MAX-WIS): Given a graph
G = (V, E) with a nonnegative vertex weight function w : V →N,
ﬁnd an independent set with the maximum total weight.
In the analysis of the local ratio algorithm for PVC (Lemma 8.15), we introduced
a new analysis technique. Instead of comparing the approximate solution with the
optimal solution opt, we compare it with a lower bound kα of opt. Here we will
apply this technique again, in a more sophisticated way, by comparing the approxi-
mate solution of MAX-WIS with an upper bound of the optimal solution (as this is
a maximization problem while PVC is a minimization problem).
To ﬁnd an upper bound of the optimal solution, we can ﬁrst formulate the prob-
lem as an integer linear program:
maximize

u∈V
w(u)xu
subject to
xu + xv ≤1,
{u, v} ∈E,
xu ∈{0, 1},
u ∈V.
Then we relax this ILP to the following LP by replacing the constraints xu ∈{0, 1}
with 0 ≤xu ≤1:
maximize

u∈V
w(u)xu
subject to
xu + xv ≤1,
{u, v} ∈E,
0 ≤xu ≤1,
u ∈V.
(8.16)
Let x∗be an optimal solution of this LP. Then, 
u∈V w(u)x∗
u is an upper bound
for the optimal solution opt of the ILP. Now, instead of deﬁning a weight function
w1 for which an r-approximation is easy to ﬁnd, we only need to deﬁne a weight
function w1 for which a feasible solution x satisfying

u∈V
w1(u)xu ≥1
r

u∈V
w1(u)x∗
u
is easy to ﬁnd.

8.4 Local Ratio
323
Let V+ = {u ∈V | w(u) > 0}. For each u ∈V , let N(u) denote the set
consisting of vertex u and its neighbors in G. Choose a vertex v ∈V+ to minimize

u∈N(v)∩V+ x∗
u. Let ε = w(v), and deﬁne
w1(u) =
 ε,
if u ∈N(v) ∩V+,
0,
otherwise.
Lemma 8.19 For any independent subset I of V+ with I ∩N(v) ̸= ∅, we have

u∈V
w1(u)x∗
u ≤δ + 1
2
· w1(I),
where δ is the maximum vertex degree of the input graph G.
Proof. From the deﬁnition of w1, we see that

u∈V
w1(u)x∗
u = ε ·

u∈N(v)∩V+
x∗
u.
Since I ∩(N(v) ∩V+) ̸= ∅, we have w1(I) ≥ε. This means that we only need to
show

u∈N(v)∩V+
x∗
u ≤δ + 1
2
.
By the choice of v, it sufﬁces to show the existence of a vertex s ∈V+ with

u∈N(s)∩V+
x∗
u ≤δ + 1
2
.
Choose s = arg maxu∈V+ x∗
u. Without loss of generality, we assume |N(s)| ≥2.
Now, if x∗
s ≤1/2, then x∗
u ≤1/2 for all u ∈N(s), and so

u∈N(s)∩V+
x∗
u ≤deg(s) + 1
2
≤δ + 1
2
.
On the other hand, if x∗
s > 1/2, then, by the constraint xs + xu ≤1, we know
that x∗
u < 1/2 for all u ∈N(s) −{s}. Pick a neighbor t of s, and let N ′(s) =
N(s) −{s, t}; then we get

u∈N(s)∩V+
x∗
u ≤(x∗
s + x∗
t ) +

u∈N′(s)∩V+
x∗
u ≤1 + deg(s) −1
2
≤δ + 1
2
.
□
The following is the local ratio algorithm for MAX-WIS, which decomposes the
input weight recursively to simpler weights of the form w1.
Algorithm 8.J (Local Ratio Algorithm for MAX-WIS)
Input: A graph G = (V, E), with a nonnegative vertex weight function w : V →N.

324
Primal-Dual Schema
(1) Solve LP (8.16); let x∗be an optimal solution.
(2) Output WIS(G, w, x∗).
The function WIS(G, w, x∗) is deﬁned as follows:
Function WIS(G, w, x∗).
(1) V+ ←{u | w(u) > 0}.
(2) If V+ is independent in G then return V+.
(3) Choose v ∈V+ to minimize 
u∈N(v)∩V+ x∗
u.
(4) ε ←w(v).
(5) For all u ∈V do w1(u) ←
ε, if u ∈N(v) ∩V+,
0, otherwise.
(6) S ←WIS(G, w −w1, x∗).
(7) If S ∪{v} is independent in G then return S ∪{v}
else return S.
Theorem 8.20 Algorithm 8.J is a ((δ+1)/2)-approximation for MAX-WIS, where
δ is the maximum degree of the input graph.
Proof. Let I denote the set returned by the function WIS(G, w, x∗). We claim that
I is an independent subset of V+ and that

u∈V
w(u)x∗
u ≤δ + 1
2
· w(I).
We prove this claim by induction on the number of recursive calls made to get
the output I. In the case that no recursive call is made, V+ is independent. Clearly,
our claim is true since I = V+.
In general, we consider the ﬁrst recursive call of the form WIS(G, w −w1, x∗).
Suppose this call returns set S. Denote w2 = w −w1. By the induction hypothesis,
we have

u∈V
w2(u)x∗
u ≤δ + 1
2
· w2(S)
(8.17)
and S is an independent subset of V ′
+ = {u | w2(u) > 0}. Note that V+ = V ′
+ ∪
(N(v) ∩V+). If S ∪{v} is independent, then I = S ∪{v}, which is clearly an
independent subset of V+. If S ∪{v} is not independent, then I = S, and it must
contain a vertex in N(v). Thus, in either case, I is an independent subset of V+,
with I ∩N(v) ̸= ∅. We have, by Lemma 8.19,

u∈V
w1(u)x∗
u ≤δ + 1
2
· w1(I).

8.5 Equivalence
325
In addition, we note that w2(v) = 0. Therefore, by (8.17), we have

u∈V
w2(u)x∗
u ≤δ + 1
2
· w2(S) = δ + 1
2
· w2(I).
Together, we get

u∈V
w(u)x∗
u ≤δ + 1
2
· w(I),
and the claim is proven.
□
We remark that the recursive Algorithm 8.J for MAX-WIS may be further im-
proved. In each recursive call, we may compute a new point x∗∗corresponding to
the weight w2 = w −w1, and call function WIS with parameters (G, w2, x∗∗) in-
stead of (G, w2, x∗). Then we can use the total weight at x∗∗as an upper bound for
the optimal solution for MAX-WIS of G with respect to weight w2. This way, we
might get a better performance ratio. Indeed, the idea of this extension is exactly that
of iterated rounding introduced in Section 7.5. In other words, the iterated rounding
technique can also be seen as an application of the local ratio technique in LP-based
approximations.
8.5
More on Equivalence
In the last section, we demonstrated the equivalence between the primal-dual
schema and the local ratio method for the problems MIN-WVC and GC1. In this
section, we further discuss the relationship between these two techniques.
We ﬁrst make two observations on the problems studied in this chapter with the
primal-dual schema. The ﬁrst observation is that all problems studied so far in this
chapter are of the covering type; that is, they are the following special cases of the
problem GENERAL COVER:
Consider a base set X, a collection C of subsets of X, and a nonneg-
ative cost function c on X. For each subset C of X, denote c(C) =

x∈C c(x). A minimization problem
min{c(C) | C ∈C}
is said to be of the covering type if A ⊂B and A ∈C imply B ∈C.
The second observation is that every primal-dual schema studied so far preserves
the primal complementary slackness condition and relaxes the dual complementary
slackness condition. To be more speciﬁc, let us consider the problem GC1 and its
dual:
(primal LP)
(dual LP)
minimize
cx
maximize
y1m
subject to
Ax ≥1m,
subject to
yA ≤c,
x ≥0,
y ≥0.

326
Primal-Dual Schema
The primal complementary slackness condition is
(c −yA)x = 0.
To keep this condition holding, we set x in the following way:
xj = 1
⇐⇒
m

i=1
aijyi = cj.
The condition m
i=1 aijyi = cj provides us with a decomposition of the cost func-
tion. Note that in a local ratio algorithm, we usually set xj ←1 when the weight cj
is reduced to 0. Therefore, there is a simple correspondence between the condition
m
i=1 aijyi = cj in the primal-dual schema and the assignment cj ←0 in the local
ratio algorithm. Suppose yk
i is the value of yi after the kth iteration in a primal-dual
schema. Then
c′
j =
m

i=1
aij(yk+1
i
−yk
i )
is the cost reduction in the (k + 1)st iteration of the local ratio algorithm that corre-
sponds to the primal-dual schema, and a translation between the primal-dual schema
and the local ratio algorithm can be built upon this relationship. As an example, let
us consider the problem NETWORK DESIGN. Its primal-dual schema, Algorithm
8.E, can be translated into the following equivalent local ratio algorithm:
Algorithm 8.K (Local Ratio Algorithm for NETWORK DESIGN)
Input: A graph G = (V, E) with edge costs ce, for e ∈E, and a function f : 2V →
{0, 1} (given implicitly).
(1) x ←0.
(2) While x is not primal feasible do
Set α ←min
e∈E
ce
|Violate(x) ∩{S | e ∈δ(S)}|;
For each e ∈E do
c′
e ←α · |Violate(x) ∩{S | e ∈δ(S)}|;
ce ←ce −c′
e;
if ce = 0 then xe ←1.
(3) For each e ∈F do
Let x′ be the vector x modiﬁed with x′
e ←0;
If x′ is primal feasible then x ←x′.
(4) Output x.
Now, let us look at how we analyze this local ratio algorithm.
Let x∗be the output of Algorithm 8.K, and let F ∗= {e | x∗
e = 1}. Also, let
xk be the value of x at the beginning of the kth iteration, αk the minimum value

8.5 Equivalence
327
α found in the kth iteration, and c′
e(k) the value of c′
e at the kth iteration. That
is, in the kth iteration, we decompose the cost function ce into the sum of c′
e(k)
and ce −c′
e(k). By the local ratio theorem, all we need to prove is that solution
x∗, as a local solution to the problem with respect to the cost function c′
e(k), is a
2-approximation. That is, we need to show

e∈E
c′
e(k)x∗
e ≤2 · optk,
(8.18)
where optk is the cost value of the optimal solution with respect to the cost function
c′
e(k). Note that

e∈E
c′
e(k)x∗
e =

e∈F ∗
c′
e(k) =

e∈F ∗

S∈Violate(xk )
e∈δ(S)
αk
=

S∈Violate(xk)
degF ∗(S) · αk
and
optk ≥|Violate(xk)| · αk.
The second inequality follows from the fact that for every S ∈Violate(xk), there
must be an edge e ∈F ∗∩δ(S). So, to show (8.18), it sufﬁces to prove

S∈Violate(x)
degF ∗(S) ≤2 · |Violate(x)|.
This is exactly the inequality (8.15) that we encountered in the analysis of the
primal-dual schema (see Theorem 8.9). Thus, not only does the cost decomposi-
tion in Algorithm 8.K follow from the primal-dual schema of Algorithm 8.E, but
the analysis can also be done in a similar way.
From the above observations, we see that the equivalence between the primal-
dual schema and the local ratio method is built on the covering-type problems and
the preservation of the primal complementary slackness condition. A natural ques-
tion arises: For a noncovering-type problem and a primal-dual schema that does not
preserve the primal complementary slackness condition, can we still ﬁnd an equiv-
alent local ratio algorithm? This question is difﬁcult to answer, because there are
very few primal-dual schemas known that relax the primal complementary slack-
ness condition. One of the proposed primal-dual schema of this type is about the
following facility location problem.
Consider a set C of m cities and a set F of n possible locations for facilities,
with two cost functions cij, for i ∈F and j ∈C, and fi, for i ∈F . Intuitively, cij
is the cost for city j to use facility at location i, and fi is the cost of installing the
facility at location i. We say the costs cij satisfy the extended triangle inequality if
cij ≤ci′j + ci′j′ + cij′, for any i, i′ ∈F and j, j′ ∈C.
FACILITY LOCATION: Given sets C and F , costs cij, fi, for i ∈F and
j ∈C, with cij satisfying the extended triangle inequality, ﬁnd a subset

328
Primal-Dual Schema
S ⊆F to install facilities such that the total cost of installingfacilities
and the use of these facilities is minimized, under the condition that
each city is assigned to exactly one facility.
This problem can be formulated into the following integer linear program, in
which we use xij = 1 to indicate that city j is assigned to use facility at location i,
and yi = 1 to indicate a facility is installed at location i:
minimize

i∈F,j∈C
cijxij +

i∈F
fiyi
subject to

i∈F
xij ≥1,
j ∈C,
yi −xij ≥0,
i ∈F, j ∈C,
xij, yi ∈{0, 1},
i ∈F, j ∈C.
The following are a relaxation of this ILP and its corresponding dual LP:
(primal LP)
minimize

i∈F,j∈C
cijxij +

i∈F
fiyi
subject to

i∈F
xij ≥1,
j ∈C,
yi −xij ≥0,
i ∈F, j ∈C,
xij ≥0, yi ≥0,
i ∈F, j ∈C;
(dual LP)
maximize

j∈C
αj
subject to
αj −βij ≤cij,
i ∈F, j ∈C,

j∈C
βij ≤fi,
i ∈F,
αj ≥0, βij ≥0,
i ∈F, j ∈C.
The intuitive meaning of the variables αj and βij of the above dual LP is as follows:
For each i ∈F , city j pays βij toward the installation of the facility i. Also, each
city j pays altogether αj for the installation and the use of these facilities. The
primal complementary slackness conditions of the above primal and dual LPs are
xij(cij −(αj −βij)) = 0,
for i ∈F, j ∈C,
yi

fi −

j∈C
βij

= 0,
for i ∈F,
and the dual complementary slackness conditions are

8.5 Equivalence
329
αj
 
i∈F
xij −1

= 0,
for j ∈C,
βij(yi −xij) = 0,
for i ∈F, j ∈C.
As this is not a covering-type problem, and the objective function of the primal
LP is complicated, there does not seem to be a simple primal-dual schema for it that
preserves the primal complementary slackness condition. Instead, Jain and Vazi-
rani [2001] proposed the following idea to get a primal-dual schema that preserves
the dual complementary slackness condition but relaxes the primal complementary
slackness condition.
(1) Keep the primal solutions xij and yi, for i ∈F and j ∈C, integral. Also,
each city j ∈C is to be assigned to a unique facility φ(j).
(2) Cities in C are partitioned into two sets D and C −D. Only cities in D pay
for the installation cost of the facilities; that is, βij = 0 if j ̸∈D or if i ̸= φ(j).
(3) For j ∈C −D, the ﬁrst primary complementary slackness condition is
relaxed to
1
3 cφ(j)j ≤αj ≤cφ(j)j.
(4) All other dual and primary complementary slackness conditions are to be
satisﬁed. In particular, for j ∈D,
αj −βφ(j)j = cφ(j)j,
and, for each i with yi = 1,
fi =

j:φ(j)=i
βij.
The above proposed method appears interesting. It is not clear, however, whether
it can be implemented in such a way that the algorithm always outputs a feasible
solution, as the details of the implementation were not presented in the paper (see
Exercise 8.10). It is also not known whether there is an equivalent local ratio algo-
rithm for FACILITY LOCATION, even if the above ideas can indeed be implemented
in a polynomial-time approximation with a constant performance ratio.
Finally, we point out that weight decomposition is a well-known proof technique
in discrete mathematics. Essentially, the local ratio method may be viewed as the ex-
tension of this old proof technique to the design of algorithms. In particular, we note
that this proof technique has been used in the analysis of the greedy approximation
for the problem MIN-SMC (see Theorem 2.29). As the local ratio algorithms we
studied in this chapter can be converted to equivalent primal-dual schemas, we may
ask whether the weight decomposition analysis can also be proved by certain primal-
dual relationships. The answer is afﬁrmative for some problems. For instance, for
the analysis of the greedy approximation for MIN-SMC, we can employ the duality
theory of linear programming as follows.
First, let us recall the problem MIN-SMC. Let E = {1, 2, . . ., n}, f : 2E →R
a polymatroid function, and c : E →R+ a nonnegative cost function. The problem

330
Primal-Dual Schema
MIN-SMC asks us to minimize c(A) = 
a∈A c(a) for A ∈Ωf = {A | f(A) =
f(E)}.
This problem can be formulated as an integer linear program as follows:3
minimize

i∈E
c(i)vi
subject to

i∈E−S
Δif(S) vi ≥ΔE−Sf(S),
S ∈2E,
vi ∈{0, 1},
i ∈E.
(8.19)
To see this, let A ∈Ωf; that is, f(A) = f(E). We claim that
vi =
 1,
if i ∈A,
0,
otherwise,
is a feasible solution of LP (8.19). Indeed, for any S ∈2E,

i∈E−S
Δif(S) vi =

i∈A\S
Δif(S) ≥ΔA\Sf(S)
= f(A) −f(S) = f(E) −f(S) = ΔE−Sf(S).
Conversely, if v is a feasible solution of LP (8.19), then we can see that A = {i |
vi = 1} satisﬁes f(A) = f(E). In fact, considering the inequality constraint for
S = A, we have

i∈E−A
Δif(A) vi ≥ΔE−Af(A);
that is,
0 ≥f(E) −f(A).
Since f is monotone increasing, we must have f(E) = f(A). The above shows that
the ILP (8.19) is equivalent to the problem MIN-SMC.
Now, we can relax this ILP to an LP and get its dual LP as follows:
maximize

S∈2E
ΔE−Sf(S) yS
subject to

S:i̸∈S
Δif(S) yS ≤c(i),
i ∈E,
yS ≥0,
S ∈2E.
Next, we review the analysis of the greedy Algorithm 2.D on the functions f and
c. Suppose x1, x2, . . ., xk are the elements selected by the greedy Algorithm 2.D in
3We use vi, instead of xi, to denote a variable corresponding to element i ∈E, to avoid confusion
with the name xi used in the analysis in Theorem 2.29.

8.5 Equivalence
331
the order of their selection into the approximate solution A. Denote A0 = ∅and, for
i = 1, . . ., k, Ai = {x1, . . ., xi}. In the proof of Theorem 2.29, we decomposed the
total weight c(A) to k
i=1 w(xi), where, for each a ∈E,
w(a) =
k

j=1
(za,j −za,j+1)c(xj)
rj
,
za,j = Δaf(Aj−1), and rj = Δxjf(Aj−1). Also, recall that in the proof of Theo-
rem 2.29, we established property (b), which states that for any a ∈E,
w(a) = c(x1)
r1
za,1 +
k

j=2
c(xj)
rj
−c(xj−1)
rj−1

za,j ≤c(a) · H(γ),
(8.20)
where γ = maxx∈E f({x}).
Now, set
yS =
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎩
1
H(γ) · c(x1)
r1
,
if S = A0,
1
H(γ)
c(xi+1)
ri+1
−c(xi)
ri

,
if S = Ai, 1 ≤i ≤k −1,
0
otherwise.
Then, from (8.20), we see that for any a ∈E,

S:a̸∈S
Δaf(S) yS =
k−1

j=0
Δaf(Aj)yAj
=
1
H(γ)
c(x1)
r1
za,1 +
k

j=2
c(xj)
rj
−c(xj−1)
rj−1

za,j

=
1
H(γ) · w(a) ≤c(a),
and, hence, yS is feasible for the dual LP of MIN-SMC. In addition, we observe
that

S∈2E
ΔE−Sf(S) yS =
1
H(γ)
 c(x1)
r1

f(E) −f(A0)

+
k

j=2
c(xj)
rj
−c(xj−1)
rj−1

·

f(E) −f(Aj−1)

.
Thus, from f(Ak) = f(E), we have

332
Primal-Dual Schema
c(Ak) =
k

i=1
c(xi) =
k

i=1
c(xi)
ri
(f(Ai) −f(Ai−1))
= c(x1)
r1
(f(E) −f(A0)) +
k

j=2
c(xj)
rj
−c(xj−1)
rj−1

(f(E) −f(Aj−1))
= H(γ)

S∈2E
ΔE−Sf(S) yS ≤H(γ) · opt,
where opt is the minimum value of the objective function of LP (8.19). So, we have
obtained a new proof for Theorem 2.29 using the duality theory of linear program-
ming.
Exercises
8.1 Consider the dual linear program (8.6) of the relaxation of MIN-WVC.
A dual feasible solution y is maximal if no y′ exists such that y′ ≥y and

{vi,vj}∈E y′
ij > 
{vi,vj}∈E yij. Deﬁne
xi =
⎧
⎨
⎩
1,
if

j:{vi,vj}∈E
yij = ci,
0,
otherwise.
Show that if y is a maximal dual feasible solution, then {vi | xi = 1} is a 2-
approximation for the optimal weighted vertex cover.
8.2 Consider the following approximation algorithm for MIN-WVC:
(1) Set C ←∅.
(2) For each vi ∈V do w′
i ←ci.
(3) While E ̸= ∅do {E denotes the set of uncovered edges}
Choose an edge {vi, vj} ∈E;
If w′
i ≤w′
j
then C ←C ∪{vi};
E ←E −{{vi, vk} | {vi, vk} ∈E};
w′
j ←w′
j −w′
i
else C ←C ∪{vj};
E ←E −{{vj, vk} | {vj, vk} ∈E};
w′
i ←w′
i −w′
j.
(4) Output C.
Now, compute a dual feasible solution y along with the above algorithm as fol-
lows:

Exercises
333
(i) Initially, in step (1), set y ←0.
(ii) In step (3), when an edge {vi, vj} is chosen from E, set yij ←min{w′
i, w′
j}.
Show that y is a maximal dual feasible solution (see Exercise 8.1 for deﬁnition)
and vi ∈C implies 
j:{vi,vj}∈E yij = ci. Furthermore, show that C is a 2-
approximation for MIN-WVC, running in time O(n).
8.3 Consider the following approximation algorithm for MIN-WVC:
(1) Set C ←∅.
(2) For each vi ∈V do w′
i ←ci.
(3) While E ̸= ∅do
Choose vi ∈V satisfying
w′
i
dE(vi) =
min
k∈V −C
w′
k
dE(vk);
{dE(vi) is the number of edges in E with endpoint vi.}
For each vk ∈V with {vi, vk} ∈E do w′
k ←w′
k −
w′
i
dE(vi);
C ←C ∪{vi};
E ←E −{{vi, vk} | {vi, vk} ∈E}.
(4) Output C.
Compute a dual feasible solution y along with the above algorithm as follows:
(i) Initially, in step (1), set y ←0.
(ii) In step (3), when a vertex vi is chosen, set yik ←w′
i/dE(vi) for each vk ∈V
such that {vi, vk} ∈E.
Show that y is a maximal dual feasible solution (see Exercise 8.1 for deﬁnition),
and vi ∈C implies 
j:{vi,vj}∈E yij = ci. Furthermore, show that C is a 2-
approximation for MIN-WVC.
8.4 Consider the problem GC as deﬁned in (8.8). The followingis a modiﬁcation
of Algorithm 8.B for the general case of GC. Explain why this algorithm is not an
approximation algorithm for GC.
(1) Set x0 ←0; y0 ←0; k ←0.
(2) While xk is not primal feasible do
Jk ←{j | 1 ≤j ≤n, xk
j = 0};
Ik ←{i | 1 ≤i ≤m, n
j=1 aijxk
j ≤bi −1};
Choose r ∈Jk such that
cr −m
i=1 airyk
i

i∈Ik air
= α = min
j∈Jk
cj −m
i=1 aijyk
i

i∈Ik aij
;
For j ←1 to n do

334
Primal-Dual Schema
if j = r then xk+1
j
←1 else xk+1
j
←xk
j ;
For i ←1 to m do
if i ∈Ik then yk+1
i
←yk
i + α else yk+1
i
←yk
i ;
k ←k + 1.
(3) Output xA = xk.
8.5 Recall the weighted version of the set cover problem MIN-WSC deﬁned in
Section 2.4. The following is an LP-relaxation of MIN-WSC:
minimize
n

j=1
wjxj
subject to
n

j=1
|Sj ∩T|xj ≥|T|,
T ⊆S
xj ≥0,
j = 1, 2, . . ., n,
where S is the given set and C = {Sj | j = 1, 2, . . ., n} is the given family. Based
on this formulation, design an approximation algorithm for MIN-WSC. Discuss the
relationships between your algorithm and that of Exercise 8.3 for MIN-VC.
8.6 Design a primal-dual approximation algorithm for the problem MIN-WSC.
8.7 Consider the following problem:
PRIZE COLLECTING VERTEX COVER: Given a graph G = (V, E)
with vertex weight and edge weight w : V ∪E →N, ﬁnd a vertex
subset C to minimize

u∈C
w(u) +

{u,v}∈E, u̸∈C, v̸∈C
w({u, v}).
(a) Show that the following local ratio algorithm is a 2-approximation for this
problem:
While ∃{u, v} ∈E with min{w(u), w(v), w({u, v})} > 0 do
Set ε ←min{w(u), w(v), w({u, v})};
w(u) ←w(u) −ε;
w(v) ←w(v) −ε;
w({u, v}) ←w({u, v}) −ε.
Return C = {u | w(u) = 0}.
(b) Design a primal-dual algorithm for this problem that is equivalent to the
above algorithm.
8.8 Consider the network design problem given in Section 8.3. Prove the follow-
ing properties to get an improvement over Theorem 8.9.

Exercises
335
(a) Suppose f is a 0–1 downward monotone function. Then, for any x, by
Lemma 8.8, every minimal violated set S is a connected component of
graph Gx. However, not every connected component is a minimal violated
set. Suppose x∗is a minimal primal feasible solution and F ∗= {e | x∗
e =
1}. Let H∗be the graph obtained from Gx by adding edges in F ∗to it.
Show that each connected component of H∗contains at most one connected
component of Gx which is not a minimal violated set.
(b) Show that if f is a 0–1 downward monotone function, then Algorithm 8.E
is a 2-approximation for NETWORK DESIGN.
8.9 Consider the problem NETWORK DESIGN given in Section 8.3. Suppose
f is a 0–1 downwards monotone function. Show that the following algorithm is a
2-approximation for it.
(1) T ←MST(G). {MST(G) is the minimum spanning tree of G.}
(2) Sort edges of T in the nonincreasing order of cost.
{Without loss of generality, assume c(e1) ≥c(e2) ≥· · · ≥c(en).}
(3) For j = 1 to n do
if T −{ej} is feasible then T ←T −{ej}.
8.10 Consider the problem FACILITY LOCATION.
(a) Design a primal-dual schema for FACILITY LOCATION based on the ideas
presented in Section 8.5, and prove that if this algorithm outputs a primal
feasible solution, then the solution is a 3-approximation to the optimal so-
lution.
(b) Can you prove that the algorithm you designed above always produces a
feasible solution?
8.11 Design a primal-dual approximation algorithm for the problem PVC with
performance ratio 2.
8.12 A tournament is a directed graph G = (V, E) without self-loops such that
for any two vertices u and v, either (u, v) ∈E or (v, u) ∈E, but not both.
(a) Show that a tournament contains a cycle if and only if it contains a triangle
(a cycle of size 3).
(b) Use part (a) above to design a local ratio approximation for the problem
FVS on tournaments with performance ratio 3.
(c) Design a primal-dual approximation for the problem FVS on tournaments
with performance ratio 3.
8.13 A t-interval system is a collection {I1, I2, . . ., In} of nonempty sets each
of at most t disjoint real intervals. A t-interval graph G = (V, E) is the intersection
of a t-interval system {I1, I2, . . ., In}; i.e., V = {I1, I2, . . ., In} and {Ii, Ij} ∈

336
Primal-Dual Schema
E if and only if A ∩B ̸= ∅for some intervals A ∈Ii and B ∈Ij. Let R be the set
of right endpoints of intervals in the system. Given a t-interval graph G = (V, E)
with nonnegative node weight w : V →N, we consider the problem MAX-WIS,
i.e., the problem of ﬁnding a maximum-weight independent set in G. Let x∗be an
optimal solution of the following linear program:
maximize

u∈V
w(u)xu
subject to

u:p∈∈u
xu ≤1,
p ∈R,
0 ≤xu ≤1,
u ∈V,
where p ∈∈u means p belongs to an interval A ∈u.
(a) Recall that V+ = {u ∈V | w(u) > 0} and, for each v ∈V , N(v) is
the set consisting of v and all its neighbors. Choose v ∈V+ to minimize

u∈N(v)∩V+ x∗
u. Show that 
u∈N(v)∩V+ x∗
u ≤2t.
(b) Design a local ratio algorithm that is a (2t)-approximation for MAX-WIS
on t-interval graphs.
8.14 For a vertex v in a graph G = (V, E), let deg(v) denote the degree of the
vertex v and δ(v) the set of neighbors of v in V . Consider the following problem:
Given a simple graph G = (V, E) and an integer t ≥0, ﬁnd the mini-
mum subset D ⊆V such that D0 ∪D1 ∪· · ·∪Dt = V , where D0 = D
and Di+1 = {v | |(D0 ∪· · · ∪Di) ∩δ(v)| ≥deg(v)/2}.
(a) Find an integer linear programming formulation for this problem.
(b) Construct a greedy approximation for this problem with performance ratio
O(log(tδ)), where δ is the maximum vertex degree of the input graph G.
Historical Notes
The primal-dual method for linear programming was proposed by Dantzig, Ford,
and Fulkerson [1956]. The primal-dual approximation as a modiﬁed version of this
method was ﬁrst used by Bar-Yehuda and Even [1981] for the weighted set cover
problem. Since then, the primal-dual schema has become a major technique for
the design of approximations for covering-type problems, including many network
design problems [Agrawal et al., 1995; Goemans and Williamson, 1995a, 1997;
Ravi and Klein, 1993; Williamson et al., 1995; Bertsimas and Teo, 1998]. Exercises
8.8 and 8.9 are from Goemans and Williamson [1997].
The initial idea of primal-dual approximation is to enforce the primal comple-
mentary slackness condition and relax the dual complementary slackness condi-
tions. Jain and Vazirani [2001] presented ideas of primal-dual schemas to enforce
the dual complementary slackness condition and relax the primal complementary

Historical Notes
337
slackness condition for the noncovering-type problems FACILITY LOCATION and
k-MEDIAN. It is, however, not clear how to implement the ideas. For the special
case of METRIC FACILITY LOCATION, the currently best-known lower bound for
the approximation ratio is 1.463 [Guha and Khuller, 1998c], and the best-known
upper bound is 1.5 [Mahdian et al., 2002; Byrka, 2007].
The primal complementary slackness condition is the root of the equivalence of
the primal-dual schema and local ratio method. The local ratio method was ﬁrst
proposed by Bar-Yehuda and Even [1985]. Later, this method has been used to de-
sign approximation algorithms for the feedback vertex set problem [Bafna et al.,
1999], the node deletion problem [Fujito, 1998], resource allocation and schedul-
ing problems [Bar-Noy et al., 2001], the minimum s-t cut problem, the assignment
problems [Bar-Yehuda and Rawitz, 2004], and MAX-WIS on t-interval graphs (Ex-
ercise 8.13) [Bar-Yehuda et al. 2004]. Bar-Yehuda and Rawitz [2005a] gave a frame-
work for describing the equivalence between the primal-dual schema and local ratio
method for the covering-type problems. Other interesting issues on the primal-dual
schema and the local ratio method can be found in Bar-Yehuda and Rawitz [2004,
2005b], Freund and Rawitz [2003], and Jain et al. [2003].
Wolsey [1982] was the ﬁrst to analyze the greedy approximation for MIN-SMC
with the primal-dual method. This method has been extended to more general prob-
lems [Fujito, 1999; Fujito and Yabuta, 2004; Chv´atal, 1979]. Exercise 8.14 is from
Wang et al. [2009].

  

9
Semideﬁnite Programming
A set deﬁnite objective must be established
if we are to accomplish anything in a big way.
— John McDonald
Semideﬁnite programming studies optimization problems with a linear objective
function over semideﬁnite constraints. It shares many interesting properties with lin-
ear programming. In particular, a semideﬁnite program can be solved in polynomial
time. Moreover, an integer quadratic program can be transformed into a semideﬁnite
program through relaxation. Therefore, if a combinatorial optimization problem can
be formulated as an integer quadratic program, then we can approximate it using
the semideﬁnite programming relaxation and other related techniques such as the
primal-dual schema. As the semideﬁnite programming relaxation is a higher-order
relaxation, it often produces better results than the linear programming relaxation,
even if the underlying problem can be formulated as an integer linear program. In
this chapter, we introduce the fundamental concepts of semideﬁnite programming,
and demonstrate its application to the approximation of NP-hard combinatorial op-
timization problems, with various rounding techniques.
9.1
Spectrahedra
Let Sn be the family of symmetric matrices of order n over real numbers. Recall
that if a square matrix A over real numbers is symmetric, then all of its eigenvalues
are real. If, in addition, all the eigenvalues of A are nonnegative, then A is called a
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9_9, 
© Springer Science+Business Media, LLC 2012 
339

340
Semideﬁnite Programming
positive semideﬁnite matrix. Also, if all eigenvalues are positive, then it is called a
positive deﬁnite matrix. Consider any two matrices A = (aij)n×n, B = (bij)n×n
in Sn. The Frobenius inner product of A and B is deﬁned to be
A • B = Tr(ATB) =
n

i=1
n

j=1
aijbij.
That is, if we treat each of A and B as an n2-dimensional vector, then the Frobe-
nius inner product is just the inner product of two vectors. If A −B is positive
semideﬁnite, then we write A ⪰B. If A −B is positive deﬁnite, then we write
A ≻B.
Positive semideﬁnite matrices have a number of useful characterizations. We list
some of them below.
Proposition 9.1 Let A be a matrix in Sn. Then the following are equivalent:
(i) A is positive semideﬁnite.
(ii) For any x ∈Rn, xT Ax ≥0.
(iii) A = V T V for some matrix V .
It is useful to consider the geometric meaning of a semideﬁnite inequality. For
given matrices Q0, Q1, . . ., Qm, the solution set of a semideﬁnite inequality
S =

x

n

i=1
xiQi ⪯Q0

is a closed convex set and is called a spectrahedron. This spectrahedron may be
viewed as a generalization of the polyhedron deﬁned by a system of linear inequal-
ities:
P = {x | Ax ≤b},
where A is an m × n matrix and b is an m-dimensional vector. In fact, suppose
A = (a1, a2, . . ., an), where each ai is an m-dimensional vector. Then P may be
represented as the spectrahedron of the following form:

x

n

i=1
xi · Diag(ai) ⪯Diag(b)

,
where
Diag(b) =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
b1
0
· · ·
0
0
b2
· · ·
0
...
...
...
...
0
0
· · ·
bm
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Spectrahedra share many properties with polyhedra. The following is an exam-
ple.

9.2 Semideﬁnite Programming
341
Proposition 9.2 The intersection of two spectrahedra is still a spectrahedron.
Proof. Consider two spectrahedra
G =

x

m

i=1
xiGi ⪯G0

,
H =

x

m

i=1
xiHi ⪯H0

.
Deﬁne
Qi =
⎛
⎝Gi
Hi
⎞
⎠.
Note that two symmetric matrices A and B are both positive semideﬁnite if and
only if the matrix
⎛
⎝A
B
⎞
⎠
is positive semideﬁnite. Now, we observe that
G ∩H =

x

m

i=1
xiQi ⪯Q0

,
and so it is a spectrahedron.
□
An immediate consequence of this proposition is that, for any matrices Q1,
Q2, . . ., Qm and real numbers c1, c2, . . ., cm, the set
Ω = {U | Qi • U = ci, i = 1, 2, . . ., m; U ⪰0}
is a spectrahedron because Ω is the intersection of a polyhedron
{U | U • Qi = ci, 1 ≤i ≤m}
with a spectrahedron
{U | U ⪰0}.
9.2
Semideﬁnite Programming
A semideﬁnite program is a maximization or minimization problem with a linear ob-
jective function whose feasible domain is a spectrahedron. It shares many properties
with a linear program.
A standard form of the semideﬁnite program is as follows:
minimize
U • Q0
subject to
U • Qi = ci,
i = 1, 2, . . ., m,
U ⪰0,
(9.1)

342
Semideﬁnite Programming
where Q0, Q1, . . ., Qm are given linearly independent symmetric matrices of order
n, and c1, . . ., cm are given constants. As we pointed out in the last section, its
feasible domain
Ω = {U | U • Qi = ci, 1 ≤i ≤m; U ⪰0}
is a spectrahedron.
The semideﬁnite program (9.1) has a dual program
maximize
cTx
subject to
m

i=1
xiQi ⪯Q0,
(9.2)
where c = (c1, c2, . . ., cm)T .
The primal program (9.1) and the dual program (9.2) have the following rela-
tions:
Lemma 9.3 Suppose U is a primal feasible solution of (9.1) and x a dual feasible
solution of (9.2). Then cTx ≤U • Q0. In addition, if cT x = U • Q0, then U and
x are, respectively, the optimal primal and dual solutions.
Proof. We observe that
cT x =
m

i=1
cixi =
m

i=1
(U • Qi)xi = U •
 m

i=1
xiQi

.
Now, we note that the trace of the product of two positive semideﬁnite matrices must
be nonnegative [see Exercise 9.1(b)]. Thus, we have
U • Q0 −cTx = U •

Q0 −
m

i=1
xiQi

= Tr

U

Q0 −
m

i=1
xiQi

≥0.
Clearly, if U • Q0 = cT x, then U must be an optimal primal solution to (9.1)
and x an optimal dual solution to (9.2).
□
Semideﬁnite programs have an equivalent form called vector programs. A vec-
tor program is an optimization problem on vector variables, with a linear objective
function and linear constraints with respect to inner products between the vector
variables. The following is an example of a vector program on n vector variables
v1, v2, . . . , vn:
maximize
1
4

1≤i,j≤n
wij(1 −vi · vj)
subject to
n

i=1
n

j=1
vi · vj = 0,
vi · vi = 1,
i = 1, 2, . . ., n.
(9.3)

9.2 Semideﬁnite Programming
343
To see the relations between semideﬁnite programs and vector programs, we
note, from Proposition 9.1, that every positive semideﬁnite matrix U can be ex-
pressed as U = V TV for some matrix V . Thus, we can convert a semideﬁnite
program (9.1) into a vector program as follows: Let V = (v1, v2, . . ., vn). Sub-
stituting U = V T V into the semideﬁnite program (9.1), we obtain the following
equivalent vector program:
minimize
Q0 • V TV
subject to
Qi • V TV = ci,
for i = 1, 2, . . ., m.
Conversely, for each vector program, we can obtain an equivalent semideﬁnite
program by replacing vi · vj with variable uij. For instance, the above vector pro-
gram (9.3) can be converted into the following equivalent semideﬁnite program:
maximize
1
4 W • (J −U)
subject to
J • U = 0,
uii = 1,
i = 1, 2, . . ., n,
U ⪰0.
(9.4)
where W = (wij), U = (uij), and J is the n × n matrix with all entries having
value 1.
Thus, for a given vector program such as (9.3), we can solve it as follows:
We ﬁrst convert it into a semideﬁnite program (9.4). Then we solve (9.4) to get
a positive semideﬁnite matrix solution U. Finally, we compute matrix V such that
U = V T V . The computation of the last step is called the Cholesky factorization.
In the following, we show that it can be done in time O(n3).
We ﬁrst show a simple lemma about submatrices of a positive semideﬁnite ma-
trix.
Lemma 9.4 Let U be a positive semideﬁnite matrix of order n. Assume that
U =
⎛
⎝a
bT
b
N
⎞
⎠,
where a ∈R and b ∈Rn−1.
(a) If a > 0, then N −1
a bbT ⪰0.
(b) If a = 0, then b = 0.
Proof. (a) We prove this result by the characterization (ii) of Proposition 9.1. For
any x ∈Rn−1,
xT
N −1
a bbT 
x =

−1
a bT x, xT 
U
⎛
⎝−1
a bTx
x
⎞
⎠≥0.

344
Semideﬁnite Programming
Hence, N −1
a bbT ⪰0.
(b) For the sake of contradiction, suppose b ̸= 0. Note that N is also positive
semideﬁnite. Choose c > bT Nb/(2∥b∥2). Then
(−c, bT)
⎛
⎝0
bT
b
N
⎞
⎠
⎛
⎝−c
b
⎞
⎠= −2c∥b∥2 + bT Nb < 0,
contradicting the assumption that U is positive semideﬁnite.
□
Now, we are ready to present the O(n3)-time algorithm for Cholesky factoriza-
tion.
Theorem 9.5 Given a positive semideﬁnite matrix U, we can compute a matrix V
satisfying U = V T V in O(n3) time.
Proof. We prove the theorem by induction on n. For n = 1, suppose U = (a). Then
V = (√a).
For n ≥2, suppose
U =
⎛
⎝a
bT
b
N
⎞
⎠⪰0,
where a ∈R and b ∈Rn−1. Then a is nonnegative.
If a > 0, then we can express U as
U =
⎛
⎜
⎝
√a
0T
1
√a b
In−1
⎞
⎟
⎠
⎛
⎜
⎝
1
0T
0
N −1
a bbT
⎞
⎟
⎠
⎛
⎜
⎝
√a
1
√a bT
0
In−1
⎞
⎟
⎠.
By Lemma 9.4(a), N −1
a bbT ⪰0. Thus, we can compute its Cholesky factoriza-
tion
N −1
a bbT = M TM
recursively, and get
U =
⎛
⎜
⎝
√a
1
√a bT
0
M
⎞
⎟
⎠
T ⎛
⎜
⎝
√a
1
√a bT
0
M
⎞
⎟
⎠.
If a = 0, then by Lemma 9.4(b),
U =
⎛
⎝0
0T
0
N
⎞
⎠
and N ⪰0. Compute the Cholesky factorization

9.3 Hyperplane Rounding
345
N = MT M,
and we obtain
U =
⎛
⎝0
0T
0
M
⎞
⎠
T ⎛
⎝0
0T
0
M
⎞
⎠.
Since there are only O(n) recursive steps, and since each step needs at most time
O(n2) to compute N −1
a bbT, the total computation time is O(n3).
□
The most important property of semideﬁnite programs is their polynomial-time
solvability.
Theorem 9.6 Semideﬁnite programs can be solved within a factor 1 + ε from the
optimal solutions in time polynomial in n + 1/ε, where n is the input size of the
semideﬁnite program and ε is an arbitrary positive number.
As the emphasis of this book is on the application, rather than the theory, of
semideﬁnite programming, we omit the proof of the polynomial-time algorithm for
semideﬁnite programming. The reader is referred to Alizadeh [1991] and de Klerk
[2002] for details.
9.3
Hyperplane Rounding
In the remainder of this chapter, we present some applications of semideﬁnite pro-
gramming in the design of approximation algorithms, together with various round-
ing techniques.
We ﬁrst consider the following problem.
MAX-CUT: Given a graph G = (V, E), where V = {1, 2, . . ., n}, and
a nonnegative edge weight wij for each edge {i, j} ∈E, ﬁnd a cut
(S, V −S) of G that maximizes the total weight of the cut {wij |
{i, j} ∈E, i ∈S, j ∈V −S}.
First, let us extend the weight wij to arbitrary pairs (i, j) ∈V ×V , with wij = 0
if {i, j} ̸∈E. Then the problem MAX-CUT can be formulated as an integer linear
program as follows:
maximize

1≤i<j≤n
wijxij
subject to
xij ≤1 −yi + yj
2
,
1 ≤i < j ≤n,
xij ≤1 + yi + yj
2
,
1 ≤i < j ≤n,
yi ∈{−1, 1},
1 ≤i ≤n,
xij ∈{0, 1},
1 ≤i < j ≤n.

346
Semideﬁnite Programming
If we relax the constraints yi ∈{−1, 1} to −1 ≤yi ≤1, then it is easy to see
that the optimal solution would be reached at yi = 0, for all i = 1, 2, . . ., n, and
xij = 1, for all edges {i, j} ∈E. This optimal solution nevertheless does not offer
any help in ﬁnding an approximation for the original problem of MAX-CUT, as the
feasible domain of the relaxed linear program is too big. In such a case, a general
idea is to add some additional constraints to get a relaxed linear program with a
smaller feasible domain. With this approach, we can obtain a linear programming-
based 2-approximation for MAX-CUT.
On the other hand, as we will see below, the semideﬁnite programming relaxation
on the following quadratic programming formulation will give us a better approxi-
mation:
maximize

1≤i<j≤n
wij · 1
2 (1 −xixj)
subject to
x2
i = 1,
i = 1, 2, . . ., n.
First, we change this quadratic program to a vector program by substituting an n-
dimensional vector vi for the variable xi. The constraint x2
i = 1 is thus replaced by
the constraint vi ∈S1, where S1 = {(1, 0, . . ., 0)T, (−1, 0, . . ., 0)T}.
maximize

1≤i<j≤n
1
2 wij(1 −vi · vj)
subject to
vi ∈S1,
i = 1, 2, . . ., n.
Next, we further relax the constraint v ∈S1 to v ∈Sn, where Sn is the n-
dimensional unit sphere Sn = {y | ∥y∥= 1}, and arrive at the following vector
program:
maximize

1≤i<j≤n
1
2 wij(1 −vi · vj)
subject to
vi · vi = 1,
i = 1, 2, . . ., n.
(9.5)
Finally, following the idea explained in the last section, we can convert this vector
program into an equivalent semideﬁnite program as follows:
maximize
1
4 W • (J −U)
subject to
uii = 1,
i = 1, 2, . . ., n,
U ⪰0,
(9.6)
where W = (wij), U = (uij), and J is the n × n matrix with all entries having
value 1.
Now, we can solve this semideﬁnite program and obtain, through the Cholesky
factorization, the optimum solution (v1, v2, . . ., vn) for the vector program (9.5).

9.3 Hyperplane Rounding
347
Note that the endpoints of these n vectors are all located on the unit sphere Sn.
These points correspond to n vertices in the graph G. That is, the solution to the
above semideﬁnite programming relaxation is an embedding of the graph G on the
unit sphere Sn.
To obtain an approximation to the original instance of MAX-CUT, we need to
partition these vertices into two parts and maintain as much weight between the two
parts as possible. In other words, we need to round the solution and move each ver-
tex to either (1, 0, . . ., 0)T or (−1, 0, . . ., 0)T. A simple idea of this rounding is to
select a hyperplane that passes through the origin to cut the unit sphere into two
parts and move the vertices in one part to (1, 0, . . ., 0)T and the vertices in the other
part to (−1, 0, . . ., 0)T . As it appears hard to ﬁnd such a hyperplane by a determin-
istic method that maintains near-optimal weight between the two parts, we resort to
a simple random method. That is, we simply select a random hyperplane uniformly
and show that the expected weight between the two parts is high. Algorithmically,
this is equivalent to ﬁrst selecting a random normal vector a of a hyperplane uni-
formly on the unit sphere, and then setting xi = 1 or xi = −1 depending on whether
aT vi ≥0 or aTvi < 0, respectively. This method is called hyperplane rounding.
We summarize it as follows.
Algorithm 9.A (Semideﬁnite Programming Approximation for MAX-CUT)
Input: A graph G = (V, E) and nonnegative edge weights wij, for i, j ∈V .
(1) Construct the semideﬁnite program (9.6).
(2) Solve the semideﬁnite program (9.6);
Compute v1, v2, . . ., vn by Cholesky factorization.
(3) Choose a random vector a uniformly from Sn;
For i ←1 to n do
if aT vi ≥0 then xi ←1 else xi ←−1.
(4) Output the cut (S, V −S), where S = {i | xi = 1}.
To evaluate the performance of this approximation, we ﬁrst show the following
two lemmas.
Lemma 9.7 Assume that xi and xj are deﬁned from vectors vi and vj as in step
(3) of Algorithm 9.A. Then we have
Pr[xixj = −1] = arccos vT
i vj
π
.
Proof. Let P be the two-dimensional plane spanned by vectors vi and vj. The hy-
perplane with normal vector a separates vi and vj if and only if the projection of
a onto plane P lies in the two dark regions shown in Figure 9.1. Each region is a
fan-shaped area with the angle equal to the angle formed by the two vectors vi and
vj, whose size is arccos vT
i vj. The lemma follows from this observation.
□

348
Semideﬁnite Programming
vi
jv
Figure 9.1:
The area of the normal vectors a that separate vi from vj.
Lemma 9.8 For 0 ≤θ ≤π,
θ
π ≥α · 1 −cos θ
2
,
where α = 0.878567.
Proof. First, we note that
θ
π = 1 −cos θ
2
,
for θ = 0, π/2, or π. Moreover,
1 −cos θ
2
′′
= cos θ
2
≥0,
for 0 ≤θ ≤π/2; that is, (1 −cos θ)/2 is convex on [0, π/2]. Therefore, we have
θ
π ≥1 −cos θ
2
,
for θ ∈[0, π/2] (cf. Figure 9.2).
Next, we consider the case of θ ∈[π/2, π]. Deﬁne
f(θ) = θ
π −α · 1 −cos θ
2
,
where α = 0.878567. Then,
f′(θ) = 1
π −α · sin θ
2
and
f′′(θ) = −α
2 · cos θ.
Note that f′′(θ) ≥0 for θ ∈[π/2, π]. Hence, f(θ) is convex on [π/2, π]. Also, note
that f′(π/2) = 1/π −α/2 < 0 and f′(π) = 1/π > 0. Thus, f(θ) is not monotone
on [π/2, π], and it reaches its minimum at the point θ∗= π −arcsin(2/(πα)),
where f′(θ∗) = 0. The proof of the lemma is completed by verifying that

9.3 Hyperplane Rounding
349
/2
π
=
y
(1−cosθ)/2
θ /π
π
y =
0
Figure 9.2:
Function (1 −cos θ)/2 versus θ/π.
f(θ∗) = π −arcsin
2
πα
π
−α ·
1 +
4
1 −
 2
πα
2
2
≥0.
□
From these two lemmas, we get the following performance ratio for Algorithm
9.A.
Theorem 9.9 Let optCUT denote the objective function value of the optimum solu-
tion to MAX-CUT. We have
E
)

1≤i<j≤n
wij · 1
2 (1 −xixj)
*
≥α · optCUT,
where α = 0.878567.1
Proof. The inequality can be derived as follows:
E
)

1≤i<j≤n
wij · 1
2 (1 −xixj)
*
=

1≤i<j≤n
wij · E
,1
2(1 −xixj)
-
=

1≤i<j≤n
wij · arccos vT
i vj
π
≥α

1≤i<j≤n
wij · 1 −vT
i vj
2
≥α · optCUT.
□
Finally, we remark that the above random rounding can be derandomized by a
standard, but nontrivial derandomization technique. The reader is referred to Maha-
jan and Ramesh [1999] for details.
Next, we apply the hyperplane rounding technique to the following problem.
1In this chapter, we follow the literature in semideﬁnite programming–based approximation using
infI A(I)/opt(I), where I ranges over all input instances, to measure the performance of an approxi-
mation algorithm A on a maximization problem. For deterministic algorithms A, this is the reciprocal of
the performance ratio deﬁned in Section 1.6.

350
Semideﬁnite Programming
MAX-2SAT: Given m clauses C1, C2, . . ., Cm over n Boolean vari-
ables x1, x2, . . ., xn, with each clause Cj having at most two literals,
and a nonnegative weight wj for each clause Cj, ﬁnd an assignment to
variables that maximizes the total weight of satisﬁed clauses.
We ﬁrst formulate this problem into an integer quadratic program. To do so, we
introduce n + 1 variables y0, y1, . . ., yn, which take values either −1 or 1, and
associate these variables with the input Boolean variables under the following inter-
pretation: For 1 ≤i ≤n,
xi =

TRUE,
if yi ̸= y0,
FALSE,
if yi = y0.
(9.7)
For convenience, we deﬁne n + 1 additional variables yn+1, . . ., y2n+1, and use the
quadratic constraints y0y2n+1 = 1 and yiyn+i = −1, for i = 1, 2, . . ., n, to make
y2n+1 = y0 and yn+i = −yi, for i = 1, 2, . . ., n.
Under this setting, we can now encode each clause Cj, 1 ≤j ≤m, by some
quadratic inequalities over these integer variables. We ﬁrst deﬁne, for each j, 1 ≤
j ≤m, two integers j1 and j2 as follows:
(1) If Cj contains only one literal xi (or, ¯xi), then let j1 = i (or, respectively,
j1 = n + i) and j2 = 2n + 1.
(2) If Cj = xi ∨xi′, then let j1 = i and j2 = i′.
(3) If Cj = xi ∨¯xi′, then let j1 = i and j2 = n + i′.
(4) If Cj = ¯xi ∨¯xi′, then let j1 = n + i and j2 = n + i′.
With these choices of j1 and j2 and the interpretation (9.7), we get the following
relationship between clause Cj and the three variables y0, yj1, and yj2:
Cj = FALSE
⇐⇒
y0 = yj1 = yj2.
Or, equivalently,
Cj = TRUE
⇐⇒
3 −y0yj1 −y0yj2 −yj1yj2
4
= 1,
Cj = FALSE ⇐⇒
3 −y0yj1 −y0yj2 −yj1yj2
4
= 0.
From this, we obtain the following integer quadratic program for MAX-2SAT:
maximize
m

j=1
wj · 3 −y0yj1 −y0yj2 −yj1yj2
4
subject to
y0y2n+1 = 1,
yiyn+i = −1,
1 ≤i ≤n,
y2
i = 1,
0 ≤i ≤2n + 1.
(9.8)

9.3 Hyperplane Rounding
351
By a semideﬁnite programming relaxation similar to the one used for the problem
MAX-CUT, we get the following semideﬁnite program:
maximize
m

j=1
wj · 3 −u0,j1 −u0,j2 −uj1,j2
4
subject to
u0,2n+1 = 1,
ui,n+i = −1,
1 ≤i ≤n,
uii = 1,
0 ≤i ≤2n + 1,
U ⪰0,
(9.9)
where U = (uij)0≤i,j≤2n+1.
We can now solve this semideﬁnite program and apply hyperplane rounding to
get an approximation for MAX-2SAT.
Algorithm 9.B (Semideﬁnite Programming Approximation for MAX-2SAT)
Input: A CNF formula with clauses C1, . . ., Cm, each with at most two literals, and
weights w1, . . ., wm.
(1) Formulate the semideﬁnite program (9.9) as above.
(2) Solve the semideﬁnite program (9.9) to obtain U ∗;
Compute v0, v1, . . ., v2n+1 by the Cholesky factorization.
(3) Choose a random vector a uniformly from S2n+2;
For i ←0 to n do
if aTvi ≥0 then yi ←1 else yi ←−1.
(4) For i ←1 to n do
if yi ̸= y0 then xi ←TRUE else xi ←FALSE.
(5) Output x.
The following analysis shows that this algorithm has the same performance ratio
as Algorithm 9.A.
Theorem 9.10 Let opt2SAT denote the objective function value of the optimal solu-
tion to the problem MAX-2SAT. Then we have
E
) m

j=1
wj · 3 −y0yj1 −y0yj2 −yj1yj2
4
*
≥α · opt2SAT,
where α = 0.878567.
Proof. Denote θij = arccos vT
i vj. Then we have, from Lemmas 9.7 and 9.8,

352
Semideﬁnite Programming
E
)
m

j=1
wj · 3 −y0yj1 −y0yj2 −yj1yj2
4
*
=
m

j=1
wj ·

E
,1 −y0yj1
4
-
+ E
,1 −y0yj2
4
-
+ E
,1 −yj1yj2
4
-
=
m

j=1
wj ·
θ0,j1
2π + θ0,j2
2π + θj1,j2
2π

≥α ·
m

j=1
wj ·
1 −u0,j1
4
+ 1 −u0,j2
4
+ 1 −uj1,j2
4

= α ·
m

j=1
wj · 3 −u0,j1 −u0,j2 −uj1,j2
4
≥α · opt2SAT.
□
9.4
Rotation of Vectors
The hyperplane rounding technique studied in the last section works in three steps.
First, we apply semideﬁnite programming relaxation to the input instance to get a
semideﬁnite program. Next, we solve the semideﬁnite program and get a mapping
of the input variables to vectors in Sn. Finally, we select a hyperplane to cut the unit
sphere Sn into two parts and round the vectors to the one-dimensional unit sphere
S1. We observe, from the two examples of the last section, that the performance of
such an algorithm often depends on the angles θij = arccos vT
i vj between the vec-
tors on Sn. This observation suggests the following idea to improve the performance
of the hyperplane rounding–based approximation algorithms: Before the third step
of hyperplane rounding, shift the vectors on Sn so that the angles between these
vectors are changed to effect a better rounding result. In this section, we explore
this idea on some examples.
First, let’s look at the problem MAX-2SAT again. In the analysis of the perfor-
mance of Algorithm 9.B, we notice that the expected total weight is equal to
m

j=1
wj · θ0,j1 + θ0,j2 + θj1,j2
2π
.
(9.10)
Thus, we would like to ﬁnd a way of changing the angles θij between the vectors to
get a larger value for the above sum. To do so, we observe that, among the variables
in the integer quadratic program (9.8), y0 is a special one, as it is involved in every
term of the summation (9.10) above. Therefore, we may focus on changing the
angles θ0,i between vector v0 and other vectors vi. That is, we want to rotate the
vectors vi toward or away from the vector v0 to increase the sum (9.10). More
precisely, let f(θ) be a function deﬁned on θ ∈[0, π]. Then, we can deﬁne a rotation
operation on vectors vi, for i ̸= 0, as follows: For each vector vi, i ̸= 0, we map vi
to a new vector v′
i located in the plane spanned by vectors v0 and vi such that v′
i
lies on the same side of v0 as vi and forms an angle f(θ0,i) with vector v0.

9.4 Rotation of Vectors
353
Let θ′
i,j denote the new angle between v′
i and v′
j after the rotation. [Thus,
θ′
0,i = f(θ0,i) for all i = 1, . . ., 2n.] How do we choose the rotation function f
to maximize the sum
m

j=1
wj · θ′
0,j1 + θ′
0,j2 + θ′
j1,j2
2π
?
First, as a general rule, a rotation function f is usually required to satisfy the prop-
erty
f(π −θ) = π −f(θ),
so that the vectors vi move toward or away from the line passing through v0 in a
symmetric way. Next, for any ﬁxed rotation function f, we need to calculate θ′
i,j to
estimate the effect of the rotation on the sum (9.10),
Motivated by the analysis in the proof of Theorem 9.10, let us consider the fol-
lowing family of rotation functions:
fλ(θ) = (1 −λ)θ + λ · π
2 (1 −cos θ),
where λ is a parameter between 0 and 1. The angle θ′
j1,j2 under this rotation function
fλ can be computed as follows: First, from spherical trigonometry, we have
cos θj1,j2 = cos θ0,j1 cos θ0,j2 + cos β sin θ0,j1 sinθ0,j2,
cos θ′
j1,j2 = cos θ′
0,j1 cos θ′
0,j2 + cos β sin θ′
0,j1 sinθ′
0,j2,
where β is the angle between the plane spanned by vectors v0 and vj1 and the
plane spanned by vectors v0 and vj2. From these equations we obtain the following
formula for θ′
j1,j2:
θ′
j1,j2 = arccos
)
cos θ′
0,j1 cos θ′
0,j2
+

cos θj1,j2 −cos θ0,j1 cos θ0,j2

· sin θ′
0,j1 sin θ′
0,j2
sin θ0,j1 sin θ0,j2
*
.
We note that for a ﬁxed λ, θ′
j1,j2 is a function of variables θ0,j1, θ0,j2, and θj1,j2.
Let us denote it by gλ(θ0,j1, θ0,j2, θj1,j2). Then, from the proof of Theorem 9.10,
we see that the effect of the rotation fλ is, for each clause Cj, to use
fλ(θ0,j1) + fλ(θ0,j2) + gλ(θ0,j1, θ0,j2, θj1,j2)
2π
to approximate
3 −cos θ0,j1 −cos θ0,j2 −cos θj1,j2
4
.
Therefore, the reciprocal of the performance ratio of the new algorithm is at least

354
Semideﬁnite Programming
ρλ =
min
(θ1,θ2,θ3)∈Ω
2
π · fλ(θ1) + fλ(θ2) + gλ(θ1, θ2, θ3)
3 −cos θ1 −cos θ2 −cos θ3
,
where Ω is the area bounded by the following constraints:
0 ≤θi ≤π,
i = 1, 2, 3,
θ1 + θ2 + θ3 ≤2π.
By selecting the best λ, we obtain
m

j=1
wj · θ′
0,j1 + θ′
0,j2 + θ′
j1,j2
2π
≥ρ · opt2SAT,
where ρ = max0≤λ≤1 ρλ.
Unfortunately, it can be veriﬁed, through numerical evaluation, that this new ratio
ρ is actually very close to the ratio α = 0.878567 obtained without the rotation. How
do we get a more signiﬁcant improvement over ρ? We notice that the estimate of ρλ
is made over the feasible domain Ω and may have been too loose. It is easy to see that
when the feasible domain shrinks, the minimum value increases. This observation
suggests that we should try to add some constraints to shrink the feasible domain Ω
and get a greater ρ. We note that for any yi, yj, yk ∈{1, −1}, they must satisfy
yiyj + yjyk + ykyi ≥−1,
yiyj −yjyk −ykyi ≥−1,
−yiyj + yjyk −ykyi ≥−1,
−yiyj −yjyk + ykyi ≥−1.
This means that we can add constraints
uij + ujk + uki ≥−1,
uij −ujk −uki ≥−1,
−uij + ujk −uki ≥−1,
−uij −ujk + uki ≥−1
to the semideﬁnite program (9.9) about MAX-2SAT. This means that Ω can be con-
strained by
cos θ1 + cos θ2 + cos θ3 ≥−1,
cos θ1 −cos θ2 −cos θ3 ≥−1,
−cos θ1 + cos θ2 −cos θ3 ≥−1,
−cos θ1 −cos θ2 + cos θ3 ≥−1,
0 ≤θi ≤π,
i = 1, 2, 3,
θ1 + θ2 + θ3 ≤2π.
(9.11)

9.4 Rotation of Vectors
355
With these constraints, we get a smaller Ω and a greater ρ. To be more precise,
let Ω1 denote the area bounded by the constraints of (9.11). Also, for 0 ≤λ ≤1, let
ρλ =
min
(θ1,θ2,θ3)∈Ω1
2
π · fλ(θ1) + fλ(θ2) + gλ(θ1, θ2, θ3)
3 −cos θ1 −cos θ2 −cos θ3
,
and ρ = max0≤λ≤1 ρλ. Based on this setting, Feige and Goemans [1995] and Zwick
[2000] have computed that ρλ ≥0.93109 for λ = 0.806765.
We summarize the above discussion in the following approximation algorithm
for MAX-2SAT.
Algorithm 9.C (Second Semideﬁnite Programming Approximation for MAX-2SAT)
Input: A CNF formula with clauses C1, . . ., Cm, each with at most two literals,
weights w1, . . ., wm, and a real number 0 ≤λ ≤1.
(1) Formulate the following semideﬁnite program:
maximize
m

j=1
wj · 3 −u0,j1 −u0,j2 −uj1,j2
4
subject to
u0,2n+1 = 1,
ui,n+i = −1,
1 ≤i ≤n,
uii = 1,
0 ≤i ≤2n + 1,
u0i + u0j + uij ≥−1,
1 ≤i < j ≤2n + 1,
u0i −u0j −uij ≥−1,
1 ≤i < j ≤2n + 1,
−u0i + u0j −uij ≥−1,
1 ≤i < j ≤2n + 1,
−u0i −u0j + uij ≥−1,
1 ≤i < j ≤2n + 1,
U ⪰0.
(2) Solve the above semideﬁnite program to obtain U ∗;
Compute v0, v1, . . ., v2n+1 by the Cholesky factorization;
Compute v′
1, v′
2, . . ., v′
n from v0, v1, . . ., vn, where each v′
i is obtained by
rotating vi on the plane spanned by vectors v0 and vi to form an angle
θ′
0,i = fλ(θ0,i) with v0.
(3) Choose a random vector a uniformly from S2n+2;
For i ←0 to n do
if aTv′
i ≥0 then yi ←1 else yi ←−1.
(4) For i ←1 to n do
if yi ̸= y0 then xi ←TRUE else xi ←FALSE.
(5) Output x.
Theorem 9.11 The expected total weight of satisﬁed clauses obtained by Algorithm
9.C is at least ρλ · opt2SAT, and ρλ ≥0.93109 when λ = 0.806765.

356
Semideﬁnite Programming
In the above, we used fλ(θ) to rotate vectors. An alternative way to perform
the rotation of the vectors is to calculate the new vectors directly from Cholesky
factorization.
Algorithm 9.D (Third Semideﬁnite Programming Approximation for MAX-2SAT)
Input: Same input as Algorithm 9.C.
(1) Same as step (1) of Algorithm 9.C.
(2) Solve the above semideﬁnite program to obtain U ∗;
Compute vectors v′
0, v′
1, . . ., v′
2n+1 through Cholesky factorization of
λU ∗+ (1 −λ)I, where I is the identity matrix of order 2n + 2.
(3)–(5) Same as steps (3)–(5) of Algorithm 9.C.
For this approximation, it can be veriﬁed that the expected total weight of satis-
ﬁed clauses is at least
ζλ · opt2SAT,
where
ζλ =
min
(θ1,θ2,θ3)∈Ω1
2
π · arccos(λ cos θ1) + arccos(λ cos θ2) + arccos(λ cos θ3)
3 −cos θ1 −cos θ2 −cos θ3
,
and Ω1 is the region deﬁned by (9.11). As ζλ has a simpler expression than ρλ,
this second way of rotation has been used more often in the literature. However, no
solid results of comparison between these two ways of rotation have been obtained
regarding which one will give us a better performance ratio.
Most approximation problems that have been studied with the method of semidef-
inite programming relaxation are maximization problems. In the following we con-
sider a minimization problem.
SCHEDULING ON PARALLEL MACHINES (SCHEDULE-PM): Given n
jobs J = {1, 2, . . ., n}, m machines M = {1, 2, . . ., m}, and the
processing time pij for job j ∈J on machine i ∈M, schedule all
jobs to m machines to minimize the total weighted completion time
n
j=1 wjCj, where Cj is the completion time of job j (i.e., the total
processing time of the ﬁrst k jobs on machine i if job j is assigned as
the kth job on machine i).
For the case of m = 1, we can ﬁnd the best scheduling by a simple greedy
algorithm. For i ∈M and j, k ∈J, deﬁne j ≺i k if [wj/pij > wk/pik] or
[wj/pij = wk/pik and j < k].
Lemma 9.12 For the problem SCHEDULE-PM with m = 1, an optimal solution is
to schedule all jobs in ordering ≺1.
Proof. Suppose job j is scheduled right after job k, but j ≺1 k. Exchanging job j
and job k, we reduce the objective function value by

9.4 Rotation of Vectors
357
wjp1k −wkp1j ≥0.
□
From the above lemma, we know that if jobs j1, j2, . . ., jk are assigned to ma-
chine i, then the scheduling of these jobs on machine i is ﬁxed according to ≺i.
Therefore, the problem SCHEDULE-PM can be formulated as the following integer
quadratic program, where xij ∈{0, 1} is the variable indicating whether job j is
assigned to machine i:
minimize
n

j=1
wj
m

i=1
xij

pij+

k≺ij
xikpik

subject to
m

i=1
xij = 1,
j = 1, 2, . . ., n,
xij ∈{0, 1}.
(9.12)
In the rest of this section, we consider only the case of m = 2. We introduce n+2
variables y1, y2, . . ., yn, yn+1, yn+2 ∈{−1, 1} satisfying the following constraints:
(a) yn+1yn+2 = −1.
(b) x1j = 1 if and only if yj = yn+1 (and, hence, x2j = 1 if and only if yj =
yn+2).
From these constraints, we have, for i ∈{1, 2} and j, k ∈J,
xij = 1 + yn+iyj
2
,
xijxik = 1 + yjyk + yn+iyj + yn+iyk
4
.
Substituting these formulas into (9.12), we obtain a new integer quadratic program:
minimize
n

j=1
wj
n+2

i=n+1
1 + yiyj
2
· pij+

k≺ij
1 + yjyk + yiyj + yiyk
4
· pik

subject to
y2
j = 1,
j = 1, 2, . . ., n + 2,
yn+1yn+2 = −1.
By the semideﬁnite programming relaxation, we get the following semideﬁnite pro-
gram:
minimize
n

j=1
wj
n+2

i=n+1
1 + uij
2
· pij+

k≺ij
1 + ujk + uij + uik
4
· pik

subject to
ujj = 1,
j = 1, 2, . . ., n + 2,
un+1,n+2 = −1,
U ⪰0.

358
Semideﬁnite Programming
We can now apply the hyperplane rounding technique with the rotation of vectors to
design an approximation algorithm for SCHEDULE-PM. Here, variables yn+1 and
yn+2 are the special vectors. We can rotate each vector of v1, v2, . . ., vn toward or
away from vn+1 to ﬁnd a better performance ratio. The details are left to the reader
as an exercise.
Finally, let us make some remarks about the rotation. Given a semideﬁnite pro-
gramming relaxation, how do we rotate vectors to get a better rounding? A general
method is called the outward rotation. Let θij denote the angle between vectors vi
and vj before the rotation, and θ′
ij the angle between them after the rotation. In an
outward rotation, we rotate vectors so that π/2 > θ′
ij > θij if 0 < θij < π/2, and
π/2 < θ′
ij < θij if π/2 < θij < π, for all vectors vi and vj. This can be achieved
by embedding the original vector space to a larger vector space and then rotating the
vectors out of the original space. We note that if the objective function of some prob-
lem attains its maximum value at some conﬁgurations in which many angles are less
than π/2, then the outward rotation is potentially helpful. This is indeed the case for
many maximization problems. On the other hand, for minimization problems, the
“inward rotation,” in which 0 < θ′
ij < θij for 0 < θij < π/2 and θij < θ′
ij < π for
π/2 < θij < π, seems to be more helpful. The reader is referred to, for instance,
Zwick [1999] for more details.
9.5
Multivariate Normal Rounding
There is another rounding technique, called multivariate normal rounding, in the
semideﬁnite programming-based approximation. We demonstrate the idea on the
problem MAX-CUT.
Algorithm 9.E (Multivariate Normal Rounding for MAX-CUT)
Input: A graph G = (V, E) and nonnegative edge weights wij, for i, j ∈V .
(1) Construct the semideﬁnite program (9.6).
(2) Find the optimal solution U∗of the semideﬁnite program (9.6).
(3) Generate a random vector y from a multivariate normal distribution with mean
0 and covariance matrix U ∗; that is, y ∈N(0, U∗).
(4) For i ←1 to n do
if yi ≥0 then xi ←1 else xi ←−1.
(5) Output the cut S = {i | xi = 1}.
It should be pointed out ﬁrst that although we did not state it explicitly, we actu-
ally need to apply the Cholesky factorization to implement step (3). More precisely,
step (3) can be implemented as follows:
(3.1) Compute V with V TV = U ∗by Cholesky factorization.
(3.2) Choose a ∈N(0, I);
Set y ←V a.

9.5 Multivariate Normal Rounding
359
Next, let us show that Algorithm 9.E has the same performance ratio as Algo-
rithm 9.A. To see this, we only need the following property of the random vector y,
which plays a similar role to Lemma 9.7 for hyperplane rounding.
Deﬁne, for any real number x,
sgn(x) =
 1,
if x ≥0,
−1,
if x < 0.
Lemma 9.13 For y ∈N(0, U ∗),
E[sgn(yi) · sgn(yj)] = 2
π arcsin u∗
ij.
Proof. Let y ∈N(0, U ∗). It can be found from Johnson and Kotz [1972] that
Pr[yi ≥0, yj ≥0] = Pr[yi < 0, yj < 0] = 1
4 + 1
2π arcsin u∗
ij,
Pr[yi ≥0, yj < 0] = Pr[yi < 0, yj ≥0] = 1
4 −1
2π arcsin u∗
ij.
Hence, we get
E[sgn(yi) · sgn(yj)] = Pr[yi ≥0, yj ≥0] + Pr[yi < 0, yj < 0]
−Pr[yi ≥0, yj < 0] −Pr[yi < 0, yj ≥0]
= 2
π arcsin u∗
ij.
□
Theorem 9.14 The expected value of the total weight of the output of Algorithm
9.E is at least α · optCUT, where α = 0.878567.
Proof. The proof is essentially identical to that of Theorem 9.9 by noting that
arcsin x = π/2 −arccos x.
□
For another example of the application, consider the following problem.
MAXIMUM BISECTION (MAX-BISEC): Given a graph G = (V, E),
where V = {1, 2, . . ., n}, and a nonnegative weight wij for each edge
{i, j} in E, ﬁnd a partition (V1, V2) of the vertex set V to maximize
the total weight of edges between V1 and V2 under the condition that
|V1| = |V2|.
This problem can be formulated as
maximize
1
4

1≤i,j≤n
wij(1 −xixj)
subject to
n

i=1
xi = 0,
x2
i = 1,
i = 1, 2, . . ., n.
(9.13)

360
Semideﬁnite Programming
For each variable xi, introduce a vector variable vi = (xi, 0, . . ., 0)T . Then xixj =
vi · vj. Note that n
i=1 xi = 0 is equivalent to 
1≤i,j≤n xixj = 0. Therefore, the
quadratic program (9.13) is equivalent to the following:
maximize
1
4

1≤i,j≤n
wij(1 −vi · vj)
subject to

1≤i,j≤n
vi · vj = 0,
vi · vi = 1,
i = 1, 2, . . ., n,
vi ∈S1,
i = 1, 2, . . ., n,
(9.14)
where S1 = {(1, 0, . . ., 0), (−1, 0, . . ., 0)} is the one-dimensional unit sphere.
Now, we relax S1 to the n-dimensional unit sphere Sn = {v | ∥v∥= 1}. Then the
above formulation becomes a vector program equivalent to the following semideﬁ-
nite program:
maximize
1
4 W • (J −U)
subject to
J • U = 0,
uii = 1,
i = 1, 2, . . ., n,
U ⪰0,
(9.15)
where W = (wij), U = (uij), and J is the n × n matrix with every entry having
value 1.
Suppose U ∗is an optimal solution of the semideﬁnite program (9.15). How can
we round U ∗randomly to obtain a cut for G and keep it a balanced partition?
In the following, we employ, besides multivariate normal rounding, an additional
technique called vertex swapping to solve this problem (see step (5) below).
Algorithm 9.F (Semideﬁnite Programming Approximation for MAX-BISEC)
Input: A graph G = (V, E), weight wij for each edge {i, j} ∈E.
(1) Construct the semideﬁnite program (9.15).
(2) Find the optimum solution U ∗of (9.15).
(3) Generate a random vector y from a multivariate normal distribution with
mean 0 and covariance matrix U ∗; that is, y ∈N(0, U∗).
(4) If |{i | yi ≥0}| ≥n/2 then S ←{i | yi ≥0} else S ←{i | yi < 0}.
(5) For each i ∈S do ζ(i) ←
j̸∈S wij;
Sort S such that S = {i1, i2, . . ., i|S|}, with ζ(i1) ≥ζ(i2) ≥· · · ≥ζ(i|S|);
Set SA ←{i1, i2, . . ., in/2}.
(6) Output the cut (SA, V −SA).

9.5 Multivariate Normal Rounding
361
To estimate the weight of the bisection cut (SA, V −SA), let us deﬁne three
random variables:
w = w(S) =

i∈S,j̸∈S
wij,
m = |S|(n −|S|),
and
z = z(S) = w
w∗+ m
m∗,
where
w∗= 1
4

1≤i,j≤n
wij(1 −u∗
ij)
and
m∗= n2
4 .
Lemma 9.15 In Algorithm 9.F, if S satisﬁes z = z(S) ≥c, then
w(SA) =

i∈SA,j̸∈SA
wij ≥2(√c −1)w∗.
Proof. Assume w(S) = λw∗and |S| = βn. Then m/m∗= 4β(1 −β) and
z = λ + 4β(1 −β).
From the deﬁnition of SA, it is easy to see that
w(SA) ≥n · w(S)
2|S|
.
Therefore,
w(SA) ≥w(S)
2β
= λw∗
2β = z −4β(1 −β)
2β
· w∗.
Let us study the function
g(β) = z −4β(1 −β)
2β
.
Rewrite it as
16β2 −8β(2 + g(β)) + 4z = 0;
or, equivalently,

4β −(2 + g(β))
2 −

2 + g(β)
2 + 4z = 0.
It follows that
2 + g(β)2 −4z ≥0
and, hence,
g(β) ≥2(√z −1) ≥2(√c −1).
□
Next, we want to estimate E(z). We ﬁrst establish a lemma on the function
arcsin x.

362
Semideﬁnite Programming
Lemma 9.16 For any 0 ≤x ≤1,
1 −2
π arcsin x ≥α(1 −x),
where α = 0.878567.
Proof. From Lemma 9.8, we have
π/2 −φ
π
≥α · 1 −cos(π/2 −φ)
2
,
for any φ satisfying 0 ≤π/2 −φ ≤π, or, equivalently, −π/2 ≤φ ≤π/2. Thus,
we get
1 −2
π φ ≥α (1 −sin φ),
for −π/2 ≤φ ≤π/2.
□
Lemma 9.17 E[z] ≥2α, where α = 0.878567.
Proof. Note that
w(S) = 1
4

1≤i,j≤n
wij(1 −sgn(yi)sgn(yj)),
|S|(n −|S|) = 1
4

1≤i,j≤n
(1 −sgn(yi)sgn(yj)).
Therefore, by Lemmas 9.13 and 9.16,
E[w] = 1
4

1≤i,j≤n
wi,j

1 −2
π arcsin u∗
ij

≥1
4

1≤i,j≤n
wi,j α (1 −u∗
ij) = α w∗.
Also, notice that U ∗satisﬁes J • U ∗= 0, or 
1≤i,j≤n u∗
ij = 0. Therefore, we
have
E[m] = 1
4

1≤i,j≤n

1 −2
π arcsin u∗
ij

≥1
4

1≤i,j≤n
α (1 −u∗
ij) = α m∗.
Together, we get E[z] ≥2α.
□
When c = 2α, 2(√c −1) ≈0.651. Therefore, we obtain the following result.
Theorem 9.18 There is a polynomial-time randomized approximation algorithm
for MAX-BISEC, which produces a cut with the expected total weight of the cut
at least 0.651 times the weight of the optimal cut.

Exercises
363
The vector rotation technique can also be used together with multivariate normal
rounding by taking y ∈N(0, λU ∗+(1−λ)I). A further development is to replace
the identity matrix I by
P =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
1
τ
τ
· · ·
τ
τ
1
τ 2
· · ·
τ 2
...
...
...
...
...
τ
τ 2
τ 2
· · ·
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
for some parameter τ. It has been found that sometimes this replacement may im-
prove the performance ratio of the approximation algorithms based on multivariate
normal rounding.
Exercises
9.1 Prove the following properties of positive semideﬁnite matrices.
(a) A matrix A is positive semideﬁnite if and only if A is a nonnegative linear
combination of matrices of the type vvT , where v is a vector.
(b) If A and B are positive semideﬁnite matrices, then Tr(AB) ≥0. More-
over, the equality sign holds if and only if AB = 0.
(c) A matrix A is positive semideﬁnite if and only if A • B ≥0 for every
positive semideﬁnite matrix B.
9.2 Let Q be a positive semideﬁnite matrix, b a vector, and c a real number.
Show that the ellipsoid {x | xT Qx + bT x + c ≤0} is a spectrahedron.
9.3 Show that En = {U ∈Sn | uii = 1, U ⪰0}, called an elliptope, is a
spectrahedron with 2n vertices, where a vertex is a matrix in form vvT .
9.4 A face of a spectrahedron is the intersection of a hyperplane and the spec-
trahedron.
(a) Show that the smallest face of a spectrahedron G containing point ¯x is
FG(¯x) = {x ∈G | Null(Q0 −Q(¯x)) ⊆Null(Q0 −Q(x))},
where Q(x) = m
i=1 xiQi, G = {x | Q(x) ⪰Q0} and, for a matrix A,
Null(A) = {y | Ay = 0}.
(b) Construct a spectrahedron such that the dimensions of its faces are triangu-
lar integers k(k + 1)/2 for k = 0, 1, . . ., n.
9.5 Consider a spectrahedron G = {x | Q(x) ⪰Q0}, where Q(x) =
m
i=1 xiQi. A plate of G of order k is deﬁned to be the closure of a connected
component of {x ∈G | rank(Q0 −Q(x)) = k}.

364
Semideﬁnite Programming
(a) Find all plates of the following spectrahedron:

x ∈R3  x2
1 + (x2 −2)2
4
+ x2
3
4 ≤1, xT x ≤1

.
(b) Show that the relative interior of any face is contained in exactly one plate.
(c) Show that every spectrahedron has ﬁnitely many plates.
(d) Show that every plate of a spectrahedron is a face.
9.6 Consider the following multiquadratic program:
minimize
xTQ0x + 2bT
0 x + c0
subject to
xTQix + 2bT
i x + ci = 0,
i = 1, 2, . . ., m.
First, we rewrite it as follows:
minimize
U • Q0 + 2bT
0 x + c0
subject to
U • Qi + 2bT
i x + ci = 0,
i = 1, 2, . . ., m,
U −xxT = 0.
By relaxing the constraint U −xxT = 0 to U −xxT ⪰0, we obtain
minimize
U • Q0 + 2bT
0 x + c0
subject to
U • Qi + 2bT
i x + ci = 0,
i = 1, 2, . . ., m,
U −xxT ⪰0.
This relaxation is called the convexiﬁcation relaxation of multiquadratic program-
ming. Prove that U −xxT ⪰0 if and only if
⎛
⎝U
x
xT
1
⎞
⎠⪰0.
9.7 Recall that a clique of a graph G = (V, E) is a vertex subset in which every
two vertices are adjacent to each other, and an independent set of G is a vertex subset
in which every two vertices are not adjacent to each other. Assume G = (V, E) and
V = {1, 2, . . ., n}. The characteristic vector x of a vertex subset V ′ is deﬁned by
xi = 1 if i ∈V ′ and xi = 0 if i ̸∈V ′.
(a) Prove that if u and v are characteristic vectors of a clique and an indepen-
dent set, respectively, then uT v ≤1.
(b) Let INDEP(G) be the convex hull of the characteristic vectors of all indepen-
dent sets in G. Prove that INDEP(G) is a subset of the following polyhedron:

Exercises
365
QINDEP(G) = {x ≥0 | (∀u, u is a characteristic
vector of a clique of G) xTu ≤1}.
(c) Consider the maximum independent set problem:
maximize
n

i=1
xi
subject to
xixj = 0,
(i, j) ∈E,
xi(xi −1) = 0,
i ∈V.
Find its convexiﬁcation relaxation.
9.8 Let Cn denote the convex hull of all matrices vvT for v ∈{−1, +1}n. For
a matrix A = (aij), let fo(A) = (f(aij)). Show that
Cn ⊆En ⊆

sino
 π
2 U
  U ∈En

.
9.9 Consider a positive semideﬁnite matrix A = (aij) of order n. Show that if
aii = 1 for all 1 ≤i ≤n, then |aij| ≤1 for all 1 ≤i, j ≤n.
9.10 Show that the following system of relaxed optimality conditions has a
unique solution (U ∗, x∗, Z∗):
Qi • U = ci,
i = 1, 2, . . ., m,
m

i=1
xiQi + Z = Q0,
U, Z ⪰0,
U • Z = 0.
9.11 Consider the Frobenius norm ∥A∥= (Tr(AAT ))1/2. Show that the opti-
mal solution of the problem of minimizing Tr(V 2 + V DV ) over the ellipsoid
{DV | ∥V −1/2DV V −1/2∥≤1}
is
DV = −V 3/∥V 2∥.
9.12 Design approximation algorithms for the following problems using the
semideﬁnite programming relaxation with hyperplane rounding:
(a) MAX-BISEC.
(b) MAX-k-VC: Given a graph G = (V, E) with nonnegative edge weights
wij, ﬁnd a subset S ⊆V of k vertices that maximizes the total weight of
edges covered by S.

366
Semideﬁnite Programming
(c) MAXIMUM CUT IN A DIGRAPH (MAX-DICUT): Given a directed graph
G = (V, E) with nonnegative edge weights wij, ﬁnd a subset S ⊆V that
maximizes the total weight of the directed cut δ+(S) = {(i, j) ∈E | i ∈
S, j ̸∈S}.
(d) MAX-k-UNCUT: Given a graph G = (V, E) with nonnegative edge weights
wij and an integer k > 0, ﬁnd a subset S ⊆V of k vertices that maximizes
the total weight of edges that do not cross S and V −S.
(e) DENSE-k-SUBGRAPH: Given a graph G = (V, E) with nonnegative edge
weights wij and an integer k > 0, ﬁnd a subset S ⊆V of k vertices that
maximizes the total weight of edges in the subgraph induced by S.
(f) MAXIMUM RESTRICTED CUT (MAX-RES-CUT): Given a graph G =
(V, E) with nonnegative edge weights wij and two disjoint edge subsets
E+ and E−, ﬁnd a subset S ⊆V that contains exactly one endpoint of each
edge in E−and either two endpoints or none of the endpoints of each edge
in E+, to maximize the total weight of the cut δ(S) = {{i, j} ∈E | i ∈
S, j ̸∈S}.
9.13 Study approximations to the following problems by the semideﬁnite pro-
gramming relaxation with multivariate normal rounding:
(a) MAX-2SAT.
(b) MAX-k-VC.
9.14 Suppose there are m unit vectors v1, v2, . . ., vm in the unit sphere Sn.
Choose a random unit vector a from Sn. Show that
Pr[sgn(aTvi) = sgn(aTvj) = sgn(aTvk)] = 1 −θij + θik + θjk
2π
,
where θij = arccos(vT
i vj).
9.15 Design approximation algorithms by the method of semideﬁnite program-
ming for the following problems:
(a) MAX-(n/2)-VC: Given a graph G = (V, E) with nonnegative edge weights
wij, ﬁnd a subset S ⊆V of |V |/2 vertices that maximizes the total weight
of edges covered by S.
(b) MAX-(n/2)-DENSE-SUBGRAPH: Given a graph G = (V, E) with nonneg-
ative edge weights wij, for each edge {i, j} ∈E, ﬁnd a subset S ⊆V
of |V |/2 vertices that maximizes the total weight of edges in the subgraph
induced by S.
(c) MAX-(n/2)-UNCUT: Given a graph G = (V, E) with nonnegative edge
weights wij, ﬁnd a subset S ⊆V of |V |/2 vertices that maximizes the total
weight of edges that do not cross S and V −S.

Exercises
367
(d) MAXIMUM BISECTION ON DIGRAPHS (MAX-DIBISEC): Given a directed
graph G = (V, E) with nonnegative edge weights wij, partition the vertices
into two sets A and B of equal size that maximize the total weight of arcs
from A to B.
9.16 Let v1, . . ., v5 be ﬁve unit vectors in the n-dimensional unit sphere.
Choose a random hyperplane H by uniformly choosing a random normal vector.
For any set V of vectors, let PrH(V ) denote the probability of V being separated
by the random hyperplane H. Denote θij = arccos(vT
i vj). Prove the following
facts:
(a) PrH(v1, v2, v3) = (θ12 + θ23 + θ13)/(2π).
(b) PrH(v1, v2, v3, v4) = 1 −V/π2, where V is the volume of a spherical
tetrahedron with dihedral angles λ12, λ13, λ23, λ14, λ24, λ34, and λi1i2 =
π −θi3i4, for any permutation (i1, i2, i3, i4) of (1, 2, 3, 4).
(c) PrH(v1, v2, v3, v4, v5) = 1
2

1≤i<j<k<l≤5 PrH(vi, vj, vk, vl)
−1
4

1≤i<j≤5 PrH(vi, vj).
9.17 Apply the vector rotation techniques to design approximation algorithms
for the following problems:
(a) SCHEDULE-PM with m = 2.
(b) MAXIMUM SPLITTING SET: Given a ﬁnite set X = {1, 2, . . ., m}, a col-
lection C = {S1, S2, . . ., Sn} of subsets of X, and a nonnegative weight
wj for each subset Sj in C, ﬁnd a partition (S, X −S) of X that maximizes
the total weight of subsets Sj ∈C that are split by partition (S, X −S) (i.e.,
the total weight of sets Sj ∈C such that Sj ∩S ̸= ∅̸= Sj ∩(X −S)).
(c) MAXIMUM NOT-ALL-EQUAL SATISFIABILITY (MAX-NAE-SAT): Given
n Boolean variables and m clauses over these variables, and a nonnegative
weight wj for each clause Cj, ﬁnd an assignment to variables that maxi-
mizes the total weight of all clauses that contain at least one true literal and
at least one false literal.
9.18 Assume y0, y1, y2, y3, y4 ∈{1, −1}. Show the following facts:
(a) y0 = y1 does not hold if and only if (1 −y0y1)/2 = 1.
(b) y0 = y1 = y2 does not hold if and only if (3 −y0y1 −y0y2 −y1y2)/4 = 1.
(c) y0 = y1 = y2 = y3 does not hold if and only if
4 −(y0 + y1)(y2 + y3)
4
≥1, 4 −(y0 + y2)(y3 + y1)
4
≥1, and
4 −(y0 + y3)(y1 + y2)
4
≥1.

368
Semideﬁnite Programming
(d) y0 = y1 = y2 = y3 = y4 does not hold if and only if
5 −(yi0yi1 + yi1yi2 + yi2yi3 + yi3yi4 + yi4yi0)
4
≥1 and
5 −(yi0 + yi4)(yi1 + yi2 + yi3) + yi0yi4
4
≥1,
for all permutations (i0, i1, i2, i3, i4) of (0, 1, 2, 3, 4).
9.19 Consider the following generalization of the problem MAX-3SAT, where
k is a constant greater than 2:
MAX-kSAT: Given n Boolean variables and m clauses each containing
at most k literals and having a nonnegative weight, ﬁnd an assignment
of variables such that the total weight of satisﬁed clauses is maximized.
Use the facts developed in Exercise 9.18 and the vector rotation technique to design
approximations for MAX-3SAT and MAX-4SAT.
9.20 A function g : Sn →R is called a packing function if
(i) g is convex,
(ii) g(λM) = λg(M) for all λ ≥0 and M ∈Sn, and
(iii) g(M) ≥0 for all M ⪰0.
Show that the following functions are packing functions:
(a) g(M) = A • M, where A ⪰0.
(b) g(M) = 
i,j |mij| = max{M • Z | |zij| ≤1, 1 ≤i, j ≤n}, where
M = (mij) and Z = (zij).
9.21 A semideﬁnite program is called a packing semideﬁnite program if it is of
the following form:
maximize
C • X
subject to
gi(X) ≤1,
i = 1, 2, . . ., m,
Tr(X) ≤ωx (or Tr(X) = ωx),
X ⪰0,
where C ⪰0 and the functions gi(X), for i = 1, 2, . . ., m, are packing functions.
Prove the following results on packing semideﬁnite programs:
(a) The semideﬁnite program (9.6) for MAX-CUT can be written as a packing
semideﬁnite program.

Historical Notes
369
(b) The following semideﬁnite program obtained from the coloring of a graph
G = (V, E) can be written as a packing semideﬁnite program:
maximize
z
subject to
xii = 1,
i = 1, 2, . . ., m,
z ≤−xij,
{i, j} ∈E,
X ⪰0,
where X = (xij).
(c) For any ε > 0, there exists an algorithm faster than O(n3.5) for packing
semideﬁnite programs, which produces a feasible solution within ε from
the optimal solution.
Historical Notes
Semideﬁnite programming is a rapidly growing area in optimization. It ﬁrst ap-
peared in the study of graph optimization problems by Lov´asz [1979]. It became an
active area of research starting with Alizadeh [1991], who gave the ﬁrst polynomial-
time algorithm for solving semideﬁnite programs. Later, it was found that many
properties of, and algorithms for, linear programming can be extended to semidef-
inite programming (see Alizadeh [1991, 1995], Alizadeh et al. [1994, 1997], An et
al. [1998], and de Klerk et al. [1998]). The ﬁrst work on the applications of semidef-
inite programming to the design of approximation algorithms belongs to Goemans
and Williamson [1995b]. They improved approximations for MAX-CUT and MAX
2SAT with semideﬁnite programming relaxation and hyperplane rounding. Feige
and Goemans [1995] discovered the vector rotation technique and used it to im-
prove the performance of hyperplane rounding. This technique is further analyzed
and applied to many different problems (see Halperin et al. [2001, 2002], Alon et
al. [2001], Zwick [1998, 2000, 2002], and Galbiati and Mafﬁoli [2007]). Zwick
[1999] discussed the general ideas of the outward rotation technique. Bertsimas and
Ye [1998] proposed multivariate normal rounding. This rounding technique can also
be used together with the vector rotation technique (see Bertsimas and Ye [1998],
Han, Ye, and Zhang [2002], Han, Ye, Zhang, and Zhang [2002], Yang et al. [2003],
Zhang et al. [2004], and Fu et al. [1998]). Feige and Langberg [2006] proposed a
general rounding approach, which includes several well-known rounding techniques
as special cases. In addition to the problems MAX-CUT and MAX-2SAT, applica-
tions of semideﬁnite programming in approximation have been extended to many
other combinatorial optimization problems, including variations of graph-cutting
and set-splitting problems [Halperin and Zwick, 2001b; Zhang et al., 2004], varia-
tions of the satisﬁability problem [Halperin and Zwick, 2001a; Zhang et al., 2004],
the graph coloring problem [Karger et al., 1994; Iyengar et al., 2009], and schedul-
ing problems [Skutella, 2001; Yang et al., 2003]. See also Ye [2001], Bertsimas and
Ye [1998], Frieze and Jerrum [1995], Goemans and Williamson [1995b], Nesterov

370
Semideﬁnite Programming
[1998], Zwick [1998, 1999, 2000, 2002], Zhao et al. [1998], and Fu et al. [1998] for
other applications.
Many new directions in the research of semideﬁnite programming–based ap-
proximation have been explored. Arora and Kale [2007] introduced the primal-dual
schema in semideﬁnite programming to the design of approximation algorithms.
Klein and Lu [1998] and Iyengar et al. [2009] gave faster solutions for semideﬁ-
nite programs arising from the study of approximations for the maximum cut and
graph coloring problems. Most semideﬁnite programming–based approximation al-
gorithms use random rounding. Mahajan and Ramesh [1999] gave a derandomiza-
tion method for some of them. Thus, the performance ratio of some random ap-
proximation algorithms can actually be reached by deterministic algorithms. An-
jos and Wolkowicz [2002] strengthened semideﬁnite programming relaxations and
obtained a hierarchy of such relaxations. Chlamtac [2007] used this hierarchy of
semideﬁnite programming relaxations to design new approximations. Goemans and
Williamson [2004] introduced the complex semideﬁnite programming to the design
of approximation algorithms for the problem MAX 3-CUT. For a more complete list
of references, the reader is referred to Pardalos and Ramana [1997] and Pardalos and
Wolkowicz [1998].

10
Inapproximability
The problems that exist in the world today cannot be
solved by the level of thinking that created them.
— Albert Einstein
In this chapter, we turn our attention to a different issue about approximation al-
gorithms. We study how to prove inapproximability results for some NP-hard opti-
mization problems. We are not looking here for a lower bound for the performance
ratio of a speciﬁc approximation algorithm, but, instead, we try to ﬁnd a lower bound
for the performance ratio of any approximation algorithm for a given problem. Most
results in this study are based on advanced developments in computational complex-
ity theory, which is beyond the scope of this book. Therefore, we limit ourselves to
fundamental concepts and results, often with proofs omitted, which are sufﬁcient to
establish the inapproximability of many combinatorial optimization problems.
10.1
Many–One Reductions with Gap
We have seen some inapproximability results in Chapter 1. For instance, we showed
that the general case of the traveling salesman problem (TSP) does not have a
polynomial-time c-approximation for any c > 1 unless P = NP. The proof of this
result is based on a simple polynomial-time reduction from the Hamiltonian circuit
problem (HC) to TSP in the following form: For each instance G = (V, E) of HC,
the reduction maps it to an instance (H, d) of TSP, where H is the complete graph
with vertex set V , and d is the cost function with the following properties (see Figure
10.1):
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9_10, 
© Springer Science+Business Media, LLC 2012 
371

372
Inapproximability
opt =
|
G
V
G has a
|
opt > c
| V |
 Hamitonian
cycle.
cycle.
 Hamitonian
 has no 
HC
TSP
Figure 10.1:
Reduction from HC to TSP.
(a) If G contains a Hamiltonian cycle, then H has a tour with cost |V |.
(b) If G does not have a Hamiltonian cycle, then the shortest tour of H has cost
greater than c|V |.
Thus, there is a gap of a factor c between the shortest tours of the output graphs
in the two different cases. This gap allows us to conclude that polynomial-time c-
approximations do not exist for TSP unless HC can be solved in polynomial time
(i.e., unless P = NP).
This proof technique can be generalized to other optimization problems. In the
following,for an instance x of an optimization problem Π, we write opt(x) to denote
the objective function value of the optimal solution of x.
Deﬁnition 10.1 Let 0 < α < β.
(a) We say a minimization problem Π has an NP-hard gap of [α, β] if there exist
an NP-complete problem Λ and a polynomial-time many–one reduction f from Λ
to Π with the following properties:
(i) If x ∈Λ, then opt(f(x)) ≤α, and
(ii) If x ̸∈Λ, then opt(f(x)) > β.
(b) We say a maximization problem Π has an NP-hard gap [α, β] if there exist an
NP-complete problem Λ and a polynomial-time many–one reduction f from Λ to Π
with the following properties:
(i) If x ∈Λ, then opt(f(x)) ≥β, and
(ii) If x ̸∈Λ, then opt(f(x)) < α.
Figure 10.2 shows the reduction from Λ to a minimization problem Π with a gap
[α, β].

10.1 Many–One Reductions with Gap
373
f
f
<
opt
opt >
α
β
Λ
Π
minimization problem
Yes
No
NP−complete problem
Figure 10.2:
Reduction from an NP-complete problem to a minimization prob-
lem.
Lemma 10.2 Assume that Π is an optimization problem with an NP-hard gap
[α, β], with 0 < α < β. Then there is no polynomial-time (β/α)-approximation
for problem Π unless P = NP.
Proof. We prove the theorem for the case where Π is a minimization problem. The
proof for maximization problems is similar.
Assume that f is a reduction from an NP-complete problem Λ to Π satisfying
properties (i) and (ii) of Deﬁnition 10.1(a). Suppose, for the sake of contradiction,
that there is a polynomial-time (β/α)-approximation A for problem Π. We may
then construct a polynomial-time algorithm for problem Λ as follows:
(1) On input instance x of problem Λ, compute the instance y = f(x) of problem
Π.
(2) Run algorithm A on instance y to get an (β/α)-approximation s for y.
(3) Return YES if and only if the objective function value of solution s for prob-
lem Π is less than or equal to β.
It is easy to verify the correctness of the above algorithm: If x ∈Λ, then opt(y) ≤
α, and hence the objective function value of any (β/α)-approximation solution for
y is at most β. On the other hand, if x ̸∈Λ, then the objective function value of
any solution for y must be greater than β. Therefore, the cutoff point β in the above
algorithm solves the problem Λ at instance x correctly.
□
Now, let us see some applications of this proof technique.
We ﬁrst consider a simple example. A vertex coloring of a graph G = (V, E) is
a mapping c : V →Z+ such that c(u) ̸= c(v) if {u, v} ∈E.
GRAPH COLORING (GCOLOR): Given a graph G = (V, E), ﬁnd a
vertex coloring of G using the minimum number of colors.

374
Inapproximability
Theorem 10.3 The problem GCOLOR does not have a polynomial-time((4/3)−ε)-
approximation for any ε > 0 unless P = NP.
Proof. The following is a well-known NP-complete problem:
GRAPH-3-COLORABILITY (3GCOLOR): Given a graph G = (V, E),
determine whether G has a vertex coloring using at most three colors.
Let f be the identical mapping from 3GCOLOR to GCOLOR; that is, f(G) = G.
Note that if G ̸∈3GCOLOR, then the chromatic number of G is at least 4. It implies
that for 0 < ε < 1, GCOLOR has an NP-hard gap [3, 4 −ε]. Note that (4 −ε)/3 >
(4/3) −ε. Therefore, by Lemma 10.2, there is no polynomial-time ((4/3) −ε)-
approximation for GCOLOR unless P = NP.
□
We now consider another problem.
METRIC-k-CENTERS: Given n cities with a metric distance table be-
tween them, and an integer k > 0, select k cities to place warehouses
such that the maximal distance of a city to a nearest warehouse is min-
imized.
It is known that METRIC-k-CENTERS has a polynomial-time 2-approximation (see
Exercises 10.2 and 10.3). The following result indicates that this is the best possible.
Theorem 10.4 There is no polynomial-time(2 −ε)-approximation for METRIC-k-
CENTERS for any ε > 0 unless P = NP.
Proof. In a graph G = (V, E), a set D ⊆V is called a dominating set if every v ∈V
either is in D or is adjacent to a vertex u ∈D. The following problem is known to
be NP-complete.
DOMINATING SET (DS): Given a graph G = (V, E) and an integer
k > 0, determine whether G has a dominating set of size ≤k.
Deﬁne a reduction f from DS to METRIC-k-CENTERS as follows: On an in-
stance (G, k) of DS, f((G, k)) consists of the graph G, a distance table d, and the
same integer k, where
d(u, v) =
 1,
if {u, v} ∈E,
2,
otherwise.
We note that if G has a dominating set D of size at most k, then, for the instance
(G, d, k) of problem METRIC-k-CENTERS, we can choose the cities in D to place
warehouses so that every city is within distance 1 to a warehouse. On the other hand,
if G does not have a dominating set of size k, then for any k choices of locations
for warehouses, there must be at least one city u ∈V whose distance from any
warehouse is at least 2. This means that METRIC-k-CENTERS has an NP-hard gap

10.1 Many–One Reductions with Gap
375
of [1, 2 −ε] for any ε > 0. By Lemma 10.2, there is no polynomial-time (2 −ε)-
approximation for METRIC-k-CENTERS.
□
Recall the bottleneck Steiner tree problem (BNST), which asks, on a given set
of terminals in the rectilinear plane, for a Steiner tree with at most k Steiner points,
which minimizes the longest edge in the tree. In Section 3.4, we showed that BNST
has a polynomial-time 2-approximation. The following result indicates that it is the
best possible.
Theorem 10.5 The problem BNST in the rectilinear plane does not have a polyno-
mial-time (2 −ε)-approximation for any ε > 0 unless P = NP.
Proof. The following restricted version of the planar vertex cover problem is known
to be NP-complete [Garey and Johnson, 1977, 1979]:
PLANAR-CVC-4: Given a planar graph G = (V, E) with all vertices
of degree at most 4, and a positive integer k > 0, determine whether
there is a connected vertex cover of G of size k.
We note that for any input instance (G = (V, E), k) of PLANAR-CVC-4, we
can embed G into the rectilinear plane so that all edges are horizontal or vertical
segments of length at least 2k + 2, and they do not cross each other except at the
endpoints. Now, we deﬁne a set P (G) of terminals for the problem BNST as fol-
lows: For each edge e of the embedded graph G of length d, we put ⌈d⌉−1 terminals
on the interiorof e such that the length between any two adjacent terminals is at most
1, and the ﬁrst and last terminals have distance exactly 1 to the two end vertices of
e. That is, the edge e of G becomes a path p(e) in P (G) (see Figure 10.3).
Clearly, if G has a connected vertex cover C of size k, then selecting all k vertices
in C as Steiner points gives us a Steiner tree on P (G) with k Steiner points such
that the rectilinear length of each edge in the tree is at most 1. This means that
the rectilinear length of each edge in any optimal solution of the input P (G) is at
most 1.
Next, assume that G has no connected vertex cover of size k. We claim that on
input P (G), any Steiner tree with k Steiner points must have an edge of rectilinear
length ≥2. Suppose, for the sake of contradiction, that on input P (G), there is a
Steiner tree T with k Steiner points such that the rectilinear length of each edge in
the tree is at most 2 −ε. Note that P (G) has the following properties:
(a) Any two terminals on two different edges of the embedded G have distance
at least 2.
(b) Any two terminals on two nonadjacent edges of the embedded G have dis-
tance at least 2k + 2.
From property (b), two terminals on two nonadjacent edges cannot be connected
through k Steiner points. Therefore, in any full Steiner component of T, all terminals
lie on either the same edge or two adjacent edges. From property (a), we know that
if a full Steiner component F of T contains two terminals lying on two different

376
Inapproximability
(a)
(b)
Figure 10.3:
(a) A planar graph G. (b) The constructed graph P (G). The dark
circles • indicate the candidates of Steiner points, and the light circles ◦indicate
terminals.
edges e1 and e2 of G, then it must contain at least one Steiner point. Thus, we may
move a Steiner point to the location of the vertex in G that covers the two edges
e1 and e2, and remove other Steiner points in F (cf. Figure 10.3). That is, we can
convert T to a new Steiner tree T ′ with at most k Steiner points such that all Steiner
points in T ′ lie at the locations of the vertices in the embedded G. However, this
means that the Steiner points of T ′ form a connected vertex cover of G of size at
most k, which is a contradiction to our assumption. Thus, the claim is proven.
The above analysis showed that BNST has an NP-hard gap of [1, 2 −ε] for any
ε > 0. The theorem now follows from Lemma 10.2.
□
10.2
Gap Ampliﬁcation and Preservation
In the last section, we showed how to use a reduction with a gap from an NP-
complete problem Λ to prove an optimization problem Π having an NP-hard gap and
establish a lower bound for the performance ratio of algorithms for Π. Sometimes,
it is more convenient to reduce from an optimization problem Λ known to have an
NP-hard gap [α, β] to another optimization problem Π to obtain an NP-hard gap
[α′, β′] for Π. Such a reduction is called a gap-preserving reduction. If the ratio
β′/α′ for Π is greater than the starting ratio β/α of Λ, then we say the reduction is
a gap-amplifying reduction (see Figure 10.4).
The following is an example of gap-amplifying reductions.
EDGE-DISJOINT PATHS (EDP): Given a graph G = (V, E) and a list
L = ((s1, t1), (s2, t2), . . ., (sk, tk)) of k pairs of vertices, ﬁnd edge-

10.2 Gap Ampliﬁcation and Preservation
377
<
opt > β
f
f
minimization
Π
minimization Λ
<
opt
α
opt > β
|
opt
α
|
Figure 10.4:
Gap ampliﬁcation.
disjoint paths that maximize the number of connected pairs (si, ti) in
the list L.
We let EDP-c denote the problem EDP with the size of the list L equal to a
constant c. The problem EDP-2 is known to be NP-hard; that is, it is NP-hard to
determine whether two pairs of vertices can be connected by two edge-disjoint paths
in G (see Exercise 10.7). It follows from this fact that EDP has an NP-hard gap
[1 + ε, 2] for any ε > 0. In the following, we amplify this gap to obtain a better
lower bound for approximating the problem EDP.
Theorem 10.6 The problem EDP has no polynomial-time(m0.5−ε)-approximation
for any 0 < ε < 1/4 unless P = NP, where m is the number of edges in the input
graph.
Proof. We will construct a gap ampliﬁer from EDP-2 to the general case of EDP.
Consider an instance of EDP-2 consisting of a graph G = (V, E) and two pairs
(u1, v1) and (u2, v2) of vertices. We construct a graph H that consists of k(k −1)/2
copies of G and 2k additional vertices s1, . . ., sk, t1, . . ., tk, which are connected
as shown in Figure 10.5, where
k =
5|E|
2 + 1
(1−2ε)/4ε6
.
That is, a copy of G is connected to other copies of G or vertices si, tj through ver-
tices u1, u2, v1, and v2. For instance, vertex u1 of a copy of G in the main diagram
of Figure 10.5 is connected to vertex v1 of the copy of G to its left, or to a vertex si
if it is a leftmost copy of G in the diagram. In addition, the list of pairs of vertices
in H to be connected consists of (si, ti), i = 1, 2, . . ., k.
Clearly, if G contains two edge-disjoint paths connecting pairs (u1, v1) and
(u2, v2), respectively, then H contains k edge-disjoint paths connecting all k pairs

378
Inapproximability
G
G
G
G
G
G
G
G
G
G
G
s1
s2
sk
t
t 2
1
t k
u
v
u
v
1
1
2
2
Figure 10.5:
Gap-amplifying reduction from EDP-2 to EDP.
(s1, t1), (s2, t2), . . ., (sk, tk), respectively. On the other hand, if G does not contain
two edge-disjoint paths connecting pairs (u1, v1) and (u2, v2), respectively, then
H can have at most one path connecting a given pair (si, ti) of vertices for some
i = 1, 2, . . ., k. Thus, the NP-hard gap [1 + ε, 2] of EDP-2 is ampliﬁed to a bigger
NP-hard gap [1 + ε, k]. Note that the number of edges in H is
m = k(k −1)
2
· |E| + k2 ≤k2|E|
2 + 1

≤k2+(4ε)/(1−2ε).
Thus, k ≥m0.5−ε, and the theorem follows from Lemma 10.2.
□
Gap-preserving reductions are an important tool for proving the inapproximabil-
ity of an optimization problem. To demonstrate its power, we borrow an inapprox-
imability result from Section 10.4.
MAXIMUM 3-LINEAR EQUATIONS (MAX-3LIN): Given a system of
linear equations over GF (2), where each equation contains exactly
three variables, ﬁnd an assignment to variables that satisﬁes the maxi-
mum number of equations.
It will be established in Section 10.4, by H˚astad’s three-bit PCP theorem, that
MAX-3LIN has an NP-hard gap of [(0.5 + ε)m, (1 −ε)m] for any ε > 0, where m
is the number of input equations.
Theorem 10.7 The problem MAX-3SAT does not have a polynomial-time (8/7 −
ε)-approximation for any ε > 0 unless P = NP.

10.2 Gap Ampliﬁcation and Preservation
379
Proof. We will construct a gap-preserving reduction from MAX-3LIN to MAX-
3SAT. Consider a system E of m linear equations over GF (2). For each equa-
tion e in E of the form xi ⊕xj ⊕xk = 1, we introduce four clauses: fe =
(xi ∨xj ∨xk) ∧(xi ∨¯xj ∨¯xk) ∧(¯xi ∨xj ∨¯xk) ∧(¯xi ∨¯xj ∨xk). For each
equation e′ in E of the form xi ⊕xj ⊕xk = 0, we also introduce four clauses:
fe′ = (¯xi ∨¯xj ∨¯xk) ∧(xi ∨xj ∨¯xk) ∧(xi ∨¯xj ∨xk) ∧(¯xi ∨xj ∨xk). Note that
the equation e (or e′) and clauses in fe (or, respectively, in fe′) have the following
relationship:
(i) If an assignment satisﬁes e (or, e′), then the same assignment satisﬁes four
clauses in fe (or, respectively, in fe′).
(ii) If an assignment does not satisfy e (or, e′), then the same assignment satisﬁes
exactly three clauses in fe (or, respectively, in fe′).
Let f(E) be the 3CNF formula obtained from the above transformation; that is,
f(E) is the conjunct of all fe’s over all equations e in E. We note that for any
assignment, each fe has exactly three or four satisﬁed clauses. Therefore, we have
the following properties:
(a) If the optimal solution of MAX-3LIN on instance E satisﬁes fewer than (0.5+
ε)m equations in E, then the optimal solution of MAX-3SAT on f(E) satisﬁes
fewer than (3.5 + ε)m clauses in f(E).
(b) If there is an assignment for E that satisﬁes at least (1 −ε)m equations, then
the same assignment satisﬁes at least (4 −ε)m clauses in f(E).
Thus, MAX-3SAT has an NP-hard gap of [(3.5 + ε)m, (4 −ε)m]. By Lemma 10.2,
MAX-3SAT cannot have a polynomial-time (4−ε)/(3.5+ε)-approximation unless
P = NP. Note that
4 −ε
3.5 + ε −→8
7 ,
as ε →0. This completes the proof of this theorem.
□
Theorem 10.8 The problem MIN-VC does not have a polynomial-time (7/6 −ε)-
approximation for any ε > 0 unless P = NP.
Proof. We construct a gap-preserving reduction from MAX-3LIN to MIN-VC. Let
E be a system of m linear equations over GF (2). For each equation of the form
xi ⊕xj ⊕xk = 1 (or of the form xi ⊕xj ⊕xk = 0), we construct a complete graph
of four vertices labeled with four satisfying assignments of the equation as shown
in Figure 10.6(a) (or, respectively, in Figure 10.6(b)).
Thus, we have totally constructed m complete graphs of order 4. Next, we con-
nect two vertices with an edge if they contain a conﬂicting assignment (i.e., if there
exists a variable xi such that xi = 0 in the label of one vertex and xi = 1 in the
label of the other vertex). Now, we have obtained a graph G with 4m vertices with
the following properties:

380
Inapproximability
kx
=
=
= 1
(a)
(b)
ix
jx
=
= 0
kx = 1
jx
kx
=
= 0
ix = 1
ix
kx
=
= 0
jx = 1
ix
jx
kx
=
=
= 00
kx
ix
jx
=
=
=
0
1
ix
kx
jx
=
=
=
1
0
jx
kx
ix
=
=
=
1
0
ix
jx
Figure 10.6:
Two graphs whose four vertices are labeled with satisfying assign-
ments of xi ⊕xj ⊕xk = 1 [part (a)] or that of xi ⊕xj ⊕xk = 0 [part (b)].
(a) If there is an assignment to variables that satisﬁes at least (1 −ε)m equations
in E, then this assignment satisﬁes the labels of at least (1 −ε)m vertices
simultaneously. From our construction, these (1 −ε)m vertices are indepen-
dent, and so the set of remaining vertices is a vertex cover for G. Therefore,
G has a vertex cover of size at most 4m −(1 −ε)m = (3 + ε)m.
(b) If no assignment can satisfy (0.5 + ε)m or more equations in E, then no as-
signment can simultaneously satisfy the labels of (0.5+ε)m or more vertices.
As the labels of vertices in an independent set can be satisﬁed simultaneously,
we see that every independent set of G has size less than (0.5+ε)m. It follows
that each vertex cover has size greater than 4m −(0.5 + ε)m = (3.5 −ε)m.
It follows that MIN-VC has an NP-hard gap of [(3+ε)m, (3.5−ε)m] for any ε > 0.
By Lemma 10.2, MIN-VC does not have a polynomial-time (3.5 −ε)/(3 + ε)-
approximation unless P = NP. The proof of theorem is completed by noting that
3.5 −ε
3 + ε −→7
6
as ε →0.
□
10.3
APX-Completeness
In the last section, we used gap-preserving reductions to get strong inapproxima-
bility results. However, for problems having approximations with constant perfor-
mance ratios, gap-preserving reductions are often too strong for proving their in-
approximability. To study weaker inapproximability results on these problems, we
introduce an approximation-preserving reduction.

10.3 APX-Completeness
381
h(x)
g
)
(y
x
h
solutions
feasible
solutions
feasible
y
g
Π
Λ
Figure 10.7:
An L-reduction.
Deﬁnition 10.9 Let Λ and Π be two optimizationproblems. We say Λ is L-reducible
to Π, and write Λ ≤P
L Π if there are two polynomial-time mappings h and g satis-
fying the following conditions (see Figure 10.7):
(L1) h maps an instance x of Λ to an instance h(x) of Π such that
optΠ(h(x)) ≤a · optΛ(x)
for some constant a, where optΛ(x) denotes the optimal objective func-
tion value of problem Λ on input x.
(L2) g maps solutions of Π for instance h(x) to solutions of Λ for instance x
such that, for any solution y of h(x),
|objΛ(g(y)) −optΛ(x)| ≤b · |objΠ(y) −optΠ(h(x))|
for some constant b > 0, where objΛ(g(y)) is the objective function value
of the solution g(y) for instance x.
As an example, consider the following subproblems of MIN-VC.
MIN-VC-b: Given a graph G = (V, E) in which every vertex has de-
gree at most b, ﬁnd the minimum vertex cover of G.
We have the following L-reduction between these subproblems.
Theorem 10.10 For any b ≥4, MIN-VC-b ≤P
L MIN-VC-3.
Proof. Given a graph G = (V, E) in which every vertex has degree at most b, we
modify graph G into a new graph G′ as follows: For each vertex x of degree d in G,
construct a path Px of 2d −1 vertices to replace it as shown in Figure 10.8. Note
that this path has a unique minimum vertex cover Cx of size d −1 (the light circles

382
Inapproximability
x
C’
d
d
x
C
1
2
3
3
1
vertices in
vertices in
2
Figure 10.8:
Path Px.
in Figure 10.8). This vertex cover, however, covers only edges in path Px. The set
C′
x of vertices in Px but not in Cx (the dark circles in Figure 10.8) is also a vertex
cover of Px. This vertex cover C′
x has size d, but it also covers all other edges that
are incident on path Px (i.e., those edges that are indicent on x in the original graph
G).
Let m = |E| and n = |V |. If G has a vertex cover S, then we can obtain a vertex
cover
S′ =
 (
x∈S
C′
x
   (
x̸∈S
Cx

of size |S| + 2m −n for G′. Conversely, for each vertex cover S′ for G′, we can
construct a vertex cover S = {x | C′
x ∩S′ ̸= ∅} for G. Note that if, for some
x ∈V , C′
x ∩S′ ̸= ∅, then Px ∩S′ has size at least degG(x). Therefore, we have
|S| ≤|S′| −(2m −n).
An immediate consequence of the above relationship is that
opt(G′) = opt(G) + 2m −n,
where opt(G) (and opt(G′)) is the size of the minimum vertex cover in G (and,
respectively, G′). Note that m ≤b · opt(G). Thus,
opt(G′) ≤(2b + 1) · opt(G);
that is, condition (L1) holds. Note that |S| ≤|S′| −(2m −n) is equivalent to
|S| −opt(G) ≤|S′| −opt(G′).
Therefore, condition (L2) also holds, and the proof of the theorem is complete.
□
L-reductions are useful in proving problems not having PTAS, due to the follow-
ing two properties.
Lemma 10.11 If Π ≤P
L Γ and Γ ≤P
L Λ, then Π ≤P
L Λ.

10.3 APX-Completeness
383
h(x)
g /
g /(y)
g /(y)
h
feasible
solutions
g
solutions
feasible
solutions
feasible
y
h(x)
(
)
h
h
g(
Γ
Π
x
Λ
/
/
)
Figure 10.9:
Proof of Lemma 10.11.
Proof. Suppose Π ≤P
L Γ via mappings h and g and Γ ≤P
L Λ via mappings h′ and g′.
It is easy to verify that Π ≤P
L Λ via mapping h′ ◦h and g ◦g′ (see Figure 10.9). □
Lemma 10.12 If Π ≤P
L Λ and Λ has a PTAS, then Π has a PTAS.
Proof. Suppose Π ≤P
L Λ via mappings h and g, and let a and b be the constants
satisfying conditions (L1) and (L2). Consider the following four cases. We prove
that in each case, if Λ has a PTAS, then Π has a PTAS.
Case 1. Both Π and Λ are minimization problems. Then we have, for any instance
x of Π and any solution y of Λ for instance h(x),
objΠ(g(y))
optΠ(x)
= 1 + objΠ(g(y)) −optΠ(x)
optΠ(x)
≤1 + ab · objΛ(y) −optΛ(h(x))
optΛ(h(x))
.
It follows that if y is a (1 + ε)-approximation for instance h(x), then g(y) is a
(1 + abε)-approximation for instance x.
Case 2. Π is a minimization problem and Λ is a maximization problem. Then we
have, for any instance x of Π and any solution y of Λ for instance h(x),
objΠ(g(y))
optΠ(x)
= 1 + objΠ(g(y)) −optΠ(x)
optΠ(x)
≤1 + ab · optΛ(h(x)) −objΛ(y)
optΛ(h(x))
≤1 + ab · optΛ(h(x)) −objΛ(y)
objΛ(y)
.
It follows that if y is a (1 + ε)-approximation for instance h(x), then g(y) is a
(1 + abε)-approximation for instance x.
Case 3. Π is a maximization problem and Λ is a minimization problem. Then we
have, for any instance x of Π and any solution y of Λ on instance h(x),

384
Inapproximability
optΠ(x)
objΠ(g(y)) =
optΠ(x)
optΠ(x) −optΠ(x) + objΠ(g(y))
=

1 −optΠ(x) −objΠ(g(y))
optΠ(x)
−1
≤

1 −ab · objΛ(y) −optΛ(h(x))
optΛ(h(x))
−1
.
It follows that if y is a (1 + ε)-approximation for instance h(x), then g(y) is a
1/(1 −abε)-approximation for instance x.
Case 4. Both Π and Λ are maximization problems. Then, similar to case 3, we
have, for any instance x of Π and any solution y of Λ on instance h(x),
optΠ(x)
objΠ(g(y)) =

1 −optΠ(x) −objΠ(g(y))
optΠ(x)
−1
≤

1 −ab · optΛ(h(x)) −objΛ(y)
optΛ(h(x))
−1
≤

1 −ab · optΛ(h(x)) −objΛ(y)
objΛ(y)
−1
.
It follows that if y is a (1 + ε)-approximation for instance h(x), then g(y) is a
1/(1 −abε)-approximation for instance x.
□
In addition to L-reductions, a weaker type of reductions, called E-reductions, has
also been used in the study of the inapproximability of problems having constant-
ratio approximations. This reduction has the following properties:
(a) If Π ≤E Σ and Σ ≤E Λ, then Π ≤E Λ.
(b) If Π ≤E Λ and Λ has a PTAS, then Π has a PTAS.
(c) If Π ≤P
L Λ, then Π ≤E Λ.
Since we will, in this section, mainly use L-reductions to establish inapproximabil-
ity results, we omit the formal deﬁnition of the E-reduction and the proofs of the
above properties (see Exercise 10.8).
Let NPO denote the class of optimization problems Π with the following prop-
erties:
(a) Its feasible solutions are polynomial-time veriﬁable; that is, given an instance
x and a candidate y of its feasible solution,of size |y| ≤|x|O(1), it is decidable
in time polynomial in |x| whether y is a feasible solution of x.
(b) Its objective function is polynomial-time computable; that is, given an in-
stance x and a feasible solution y of x, the objective function value objΠ(y)
can be computed in polynomial time in |x|.

10.3 APX-Completeness
385
Let APX denote the class of all NPO problems that have polynomial-time r-
approximation for some constant r > 1. For instance, the problems MIN-VC,
EUCLIDEAN-TSP, NSMT, BNST, and METRIC-k-CENTERS all belong to APX.
On the other hand, it is known that if P ̸= NP, then the problems TSP, MIN-SC,
MIN-CDS, CLIQUE, and GCOLOR do not belong to APX (see Sections 10.5 and
10.6).
To study the inapproximability of problems in APX, we generalize the notion of
completeness from decision problems to optimization problems. For a class C of op-
timization problems and a reduction ≤R among optimization problems, a problem
Λ is called C-hard if for every problem Π ∈C, Π ≤R Λ. If Λ is already known to
be in C, then Λ is said to be C-complete.
Papadimitriou and Yannakakis [1993] studied a subclass MAXSNP of APX,
and showed MAXSNP-completeness, under the L-reduction, for many problems,
including MIN-VC-b for b ≥3. Khanna et al. [1999] showed that APX is the clo-
sure of MAXSNP under E-reduction, in the sense that every problem Π ∈APX is
E-reducible to some problem Λ ∈MAXSNP. Therefore, an MAXSNP-complete
problem under the L-reduction is also APX-complete under the E-reduction. (In the
following,we will write APX-completeness to denote APX-completeness under the
E-reduction.)
Theorem 10.13 The problem MIN-VC-3 is APX-complete.
Note that BNST and METRIC-k-CENTERS are in APX, but they don’t have
PTASs unless P = NP. Therefore, we have
Theorem 10.14 An APX-complete problem has no PTAS unless P = NP.
Thus, we can use L-reductions and APX-completeness to prove a problem in
APX having no PTAS. The following are some examples.
VERTEX COVER IN CUBIC GRAPHS (VC-CG): Given a cubic graph
G, ﬁnd a minimum vertex cover of G. (A cubic graph is a graph in
which every vertex has degree 3.)
Theorem 10.15 The problem VC-CG is APX-complete.
Proof. Since VC-CG is clearly in APX, it sufﬁces to prove that it is APX-hard. To
do so, we construct an L-reduction from MIN-VC-3 to VC-CG.
Consider an instance of MIN-VC-3, that is, a graph G = (V, E) in which each
vertex has degree at most 3. Suppose that G has i vertices of degree 1 and j vertices
of degree 2. Construct a new graph H as follows: H has a cycle of size 2(2i+j), and
2i+j triangles. Each triangle has two vertices connecting to two adjacent vertices in
the cycle, as shown in Figure 10.10. In each triangle of H, call the vertex that is not
connected to the cycle a free vertex. We note that we need 2i + j vertices to cover
the cycle, and two vertices to cover each triangle. Thus, a minimum vertex cover for
H has size ≥3(2i + j). In fact, it is easy to see that there exists a minimum vertex
cover of H of size 3(2i + j) that contains all free vertices.
Next, construct a graph G′ from G and H as follows:

386
Inapproximability
free vertices
Figure 10.10:
Graph H.
(a) For each vertex x of degree 1 in G, use two edges to connect x to two free
vertices of H.
(b) For each vertex x of degree 2 in G, use one edge to connect x to one free
vertex of H.
(c) Each free vertex of H is connected to exactly one vertex in G.
Clearly, G′ is a cubic graph. In addition, G has a vertex cover of size s if and
only if G′ has a vertex cover of size s′ = s + 3(2i + j). That is,
opt(G′) = opt(G) + 3(2i + j),
where opt(G) (and opt(G′)) denotes the size of the minimum vertex cover of G
(and, respectively, G′). Note that G has at least (i + 2j)/2 edges, and each vertex in
G can cover at most three edges. Therefore, i + 2j ≤6 · opt(G), and so 2i + j ≤
2i + 4j ≤12 · opt(G). It follows that
opt(G′) ≤37 · opt(G),
and condition (L1) holds for the reduction from G to G′.
Next, to prove condition (L2), we note that for each vertex cover S′ of size s′
in G′, we can obtain a vertex cover S of size s ≤s′ −3(2i + j) in G by simply
removing all vertices in S′ \ V . It follows that
s −opt(G) ≤s′ −opt(G′).
Therefore, condition (L2) also holds for this reduction.
□

10.3 APX-Completeness
387
v2
v1
v4
v3
e1
e2
e3
e4
e5
e6
v3
v1
v4
v2
e1
e4
e6
e5
e3
e2
a
b
a
b
b
a
1
1
2
2
3
3
Figure 10.11:
Construction from G to G′.
In the following, we consider a problem that originated from the study of social
networks. We say a vertex subset D ⊆V of a graph G = (V, E) is a majority-
dominatingset of G if, for every vertex v not in D, at least one half of the neighbors
of v are in D.
MAJORITY-DOMINATING SET (MAJ-DS): Given a graph G = (V, E),
ﬁnd a majority-dominating set D ⊆V of the minimum cardinality.
Theorem 10.16 The problem MAJ-DS is APX-hard.
Proof. We will construct an L-reduction from VC-CG to MAJ-DS. For a cubic
graph G = (V, E), we ﬁrst construct a bipartite graph H = (V, E, F ), where
{v, e} ∈F if and only if v is an endpoint of e in G. Next, we add to H six ad-
ditional vertices ai, bi for i = 1, 2, 3, and the following additional edges to form a
graph G′ (see Figure 10.11):
(i) {ai, bi}, for i = 1, 2, 3;
(ii) {a1, e}, for all e ∈E; and
(iii) {a1, v}, {a2, v}, {a3, v}, for all v ∈V .
We claim that G has a vertex cover of size at most k if and only if G′ has a majority-
dominating set of size at most k + 3.
To show our claim, we ﬁrst assume G has a vertex cover C of size k. Let D =
C ∪{ai | i = 1, 2, 3}. In the following we verify that D is a majority-dominating
set for G′.
(1) Each bi has only one neighbor ai ∈D.

388
Inapproximability
(2) Each e = {u, v} ∈E has three neighbors, a1, u, and v. Among them, a1 ∈D
and at least one of u or v is in D, because C is a vertex cover of G.
(3) Each v ∈V −C has six neighbors, among which a1, a2, a3 ∈D.
Conversely, suppose D is a majority-dominating set of size k + 3 for G′. Note
that if bi ̸∈D, then ai ∈D. In the case that bi ∈D and ai ̸∈D, we may replace
bi by ai and the resulting set (D −{bi}) ∪{ai} is still a majority-dominating set
of size at most k + 3. Therefore, we may assume, without of loss of generality, that
bi ̸∈D and ai ∈D, for i = 1, 2, 3. Note that each v ∈V has degree 6 and it has
neighbors a1, a2, a3 in D. In addition, each vertex e = {u, v} ∈E has degree 3,
with one of its neighbors a1 ∈D. Therefore, if there is a vertex e = {u, v} ∈E
belonging to D, then we may replace e by u, and the resulting vertex subset is still
a majority-dominating set of size at most k + 3. It follows that we may assume,
without loss of generality, that no e ∈E belongs to D.
Now, let C = D −{a1, a2, a3}. Then C ⊆V and |C| ≤k. Note that each
e = {u, v} ∈E has three neighbors, a1, u, and v. Since e has degree 3 and hence
has at least two neighbors in D, we must have either u ∈C or v ∈C. That is, C is
a vertex cover of G. This completes the proof of our claim.
Now, suppose G has a minimum vertex cover of size optVC. Then by the claim,
G′ has a minimum majority-dominating set of size optMDS = optVC + 3. That is,
optMDS = optVC + 3 ≤4 · optVC.
Moreover, let D be a majority-dominating set of size k′ for G′. Then, from the
proof of our claim, we can construct a vertex cover C of size at most k′ −3 for G.
Therefore,
|C| −optVC
 ≤
k′ −(optVC + 3)
 =
|D| −optMDS
.
Therefore, VC-CG is L-reducible to MAJ-DS. It follows that MAJ-DS is APX-
hard.
□
10.4
PCP Theorem
The following is a well-known characterization of the complexity class NP:
Proposition 10.17 A language L belongs to class NP if and only if there exist a
language A in class P, and a polynomial p, such that
x ∈L ⇐⇒(∃y, |y| ≤p(|x|))(x, y) ∈A.
That is, for a language L ∈NP, an input x is in L if and only if there is a proof
y of length p(|x|) such that the correctness of the proof, i.e., whether (x, y) ∈A or
not, can be veriﬁed in polynomial time. We may reformulate this characterization
as a proof system for the language L ∈NP:
(a) The proof system for L consists of a prover and a veriﬁer.

10.4 PCP Theorem
389
(b) On input x, the prover presents a proof y of length p(|x|) for some polynomial
p.
(c) The veriﬁer determines, from x and y, in polynomial time whether or not to
accept.
(d) If x ∈L, then there exists a proof y on which the veriﬁer accepts.
(e) If x ̸∈L then, for all proofs y, the veriﬁer rejects.
The PCP theorem presents a stronger characterization of the class NP in terms
of a new proof system. In this new proof system, the veriﬁer can use randomness to
reduce the amount of information of the proof y that he or she needs to read in order
to decide whether to accept or reject. More precisely, a probabilistically checkable
proof system P CP c(n),s(n)(r(n), q(n)) can be described as follows:
(a) The proof system for L consists of a prover and a veriﬁer.
(b) On input x, the prover presents a proof y of length p(|x|) for some polynomial
p.
(c) The veriﬁer uses r(n) random bits within polynomial time to compute q(n)
locations of y, and reads these q(n) bits of y. Then the veriﬁer determines,
from x and the q(n) bits of y, whether or not to accept x.
(d) If x ∈L, then there exists a proof y relative to which the veriﬁer accepts x
with probability ≥c(n).
(e) If x ̸∈L, then, for all proofs y, the veriﬁer accepts x with probability ≤s(n).
For example, the characterization of class NP given in Proposition 10.17 can be
rephrased in terms of the PCP systems as follows: Every problem in NP has a proof
system P CP1,0(0, p(n)) for some polynomial p.
The following result is a milestone in the study of PCP systems.
Theorem 10.18 (PCP Theorem) The problem SAT has a probabilistically check-
able proof system P CP1,1/2(O(log n), O(1)).
The result that MAXSNP-complete problems do not have PTAS provided P ̸=
NP was ﬁrst proved based on the PCP theorem. However, as we pointed out in
Section 10.3, this conclusion can be derived without using the PCP theorem. Never-
theless, it may still provide additional information about the NP-hard gaps of these
problems.
Theorem 10.19 The problem MAX-SAT has an NP-hard gap [αm, m] for some
0 < α < 1, where m is the number of clauses in the input CNF formula.
Skecth of Proof. We will construct a reduction from SAT to MAX-SAT with gap
[αm, m]. Let F be a Boolean formula, that is, an instance of the problem SAT. We
will construct a CNF formula F ′ of m = q(|F |) clauses, for some polynomial q,
such that

390
Inapproximability
(a) If F ∈SAT then F ′ is satisﬁable, and
(b) If F ̸∈SAT, then at most αm clauses in F ′ can be satisﬁed.
Let S ∈P CP1,1/2(c1 log n, c2) be a PCP system for SAT, where c1 and c2 are
two positive constants. Assume that the prover always writes down a proof y of
p(n) bits, for some polynomial p, for an instance F of size n. Then the veriﬁer of
the system S works as follows:
(1) The veriﬁer uses a random string r of c1 logn bits to compute a set Ar of c2
locations of the proof.
(2) The veriﬁer reads the c2 bits of the proof at these locations. (Call them yr.)
(3) The veriﬁer decides in deterministic polynomial time, from F and yr, whether
or not to accept F .
Note that the above system can be modiﬁed to execute step (3) before step (2).
That is, we can use Boolean variables xi, for i = 1, 2, . . ., p(n), to represent the
ith bit of the proof y, and formulate step (3) as a Boolean formula over variables in
{xi | i ∈Ar}. This Boolean formula has size O(c2). We can further transform this
Boolean formula into a CNF formula of size O(c2) (some new variables zj may be
introduced during this transformation). Call this CNF formula Fr.
Since the veriﬁer can use only c1 log n random bits, there are at most 2c1 log n =
nO(c1) possible random strings r, and hence at most nO(c1) formulas Fr. Let F ′ be
the conjunct of all these formulas Fr. Then F ′ is a CNF formula of size O(c2) ·
nO(c1) = nO(1). We verify that F ′ satisﬁes the required conditions:
First, if F ∈SAT, then there exists a proof y relative to which the veriﬁer accepts
F with probability 1. This means that the assignment τ, with τ(xi) = the ith bit of
y, satisﬁes all CNF formulas Fr, and hence F ′ is satisﬁable.
On the other hand, if F ̸∈SAT, then the veriﬁer accepts F with probability at
most 1/2, no matter what proof y is provided. This means that, for any assignment τ
on variables xi’s, at least half of the formulas Fr are not satisﬁed. Assume that each
CNF formula Fr contains at most c clauses, and that F ′ contains m clauses. Then for
any assignment τ, at least m/(2c) clauses of F ′ are not satisﬁed. Or, equivalently,
for any assignment τ, at most m(1 −1/(2c)) clauses of F ′ can be satisﬁed.
The above reduction shows that MAX-SAT has an NP-hard gap [αm, m] for α =
1 −1/(2c).
□
The following extension of the PCP theorem is very useful in getting better NP-
hard gaps.
Theorem 10.20 (H˚astad’s 3-Bit PCP Theorem) For any 0 < ε < 1, 3-SAT has a
proof system P CP1−ε,0.5+ε(O(log n), 3). More precisely, the veriﬁer in this system
computes three locations i, j, k of the proof and a bit b from a random string of
length O(logn), and accepts the input if and only if yi ⊕yj ⊕yk = b, where yi is
the ith bit of the proof.

10.5 (ρ ln n)-Inapproximability
391
Now, we apply this stronger PCP system to get the NP-hard gap for the problem
MAX-3LIN deﬁned in Section 10.2.
Theorem 10.21 For any 0 < ε < 1/4, the problem MAX-3LIN has an NP-hard
gap [(0.5 + ε)m, (1 −ε)m], where m is the number of equations in the input.
Proof. We reduce 3SAT to MAX-3LIN as follows. By H˚astat’s 3-bit PCP theorem,
3SAT has, for any 0 < ε < 1/4, a proof system S in P CP1−ε,0.5+ε(c log n, 3), for
some c > 0, in which the veriﬁer produces, for any given random string r of length
c logn, an equation xi ⊕xj ⊕xk = b. For each 3CNF formula F, we construct
the instance E of MAX-3LIN that consists of all possible equations xi ⊕xj ⊕
xk = b produced by the veriﬁer of the proof system S on input F, over all possible
random strings r of length c log n. Since the random string r has length c log n, the
total number of equations in E is bounded by 2c log n = nO(1). Therefore, this is a
polynomial-time reduction.
Now we verify that this reduction preserves the NP-hard gap of [(0.5+ε)m, (1−
ε)m]. First, if F ∈3SAT, then there exists a proof y whose bit values satisfy the
random equation xi ⊕xj ⊕xk = b with probability ≥1 −ε. This means that there
exists an assignment to variables xi that satisﬁes, among m possible equations, at
least (1 −ε)m of them. Conversely, if F ̸∈3SAT, then the bit values of any given
proof can satisfy a random equation xi ⊕xj ⊕xk = b with probability ≤0.5 + ε.
This means that, for any assignment to variables xi, at most (0.5 + ε)m out of m
equations are satisﬁed.
The above reduction established the NP-hard gap [(0.5 + ε)m, (1 −ε)m] for
MAX-3SAT.
□
Corollary 10.22 The problem MAX-3LIN does not have a polynomial-time(2−ε)-
approximation for any ε > 0 unless P = NP.
10.5
(ρ ln n)-Inapproximability
In this section, we study a class of NPO problems that are (ρ ln n)-inapproximable
for some constant ρ > 0 (under certain complexity-theoretic assumptions). Among
such (ρ ln n)-inapproximability results, the set cover problem MIN-SC plays a crit-
ical role similar to that of MAX-3LIN for the constant-ratio inapproximability re-
sults. Under the assumption that NP ̸⊆DTIME(nO(log log n)),1 many optimization
problems have been proved to be O(ρ lnn)-inapproximable through gap-preserving
reductions from MIN-SC.
Recall that MIN-SC is the problem that, on a given set S and a collection C of
subsets of S, asks for a subcollection C′ of C of the minimum cardinality such that
 C = S. The basic (ρ ln n)-inapproximability result about MIN-SC is as follows.
1The class DTIME(nO(log log n)) consists of all languages that are decidablein time nO(log log n)
by a deterministic Turing machine.

392
Inapproximability
S1
S 2
x
m
n
x
x2
1
p
q
S
Figure 10.12:
Graph G in the proof of Theorem 10.24.
Theorem 10.23 The problem MIN-SC does not have a polynomial-time (ρ ln n)-
approximation for any 0 < ρ < 1 unless NP ⊆DTIME(nO(log log n)), where n is
the size of the base set S. Furthermore, this inapproximability result holds for the
case when the size of the input collection C is no more than the size of the base set
S.
We now apply this result to establish more (ρ ln n)-inapproximability results. We
ﬁrst look at the connected dominating set problem MIN-CDS studied in Chapter 2.
Theorem 10.24 The problem MIN-CDS does not have a polynomial-time(ρ ln n)-
approximation for any 0 < ρ < 1 unless NP ⊆DTIME(nO(log log n)).
Proof. Suppose MIN-CDS has a polynomial-time (ρ ln n)-approximation for some
0 < ρ < 1. Choose a positive integer k0 > ρ/(1 −ρ). Then ρ(1 + 1/k0) < 1. Let
ρ′ be a positive number satisfying ρ(1 +1/k0) < ρ′ < 1. We show that the problem
MIN-SC has a polynomial-time approximation with performance ratio ρ′ ln n, and
hence, by Theorem 10.23, NP ⊆DTIME(nO(log log n)).
Let S = {x1, x2, . . ., xn} and C = {S1, S2, . . ., Sm} be an input instance to
MIN-SC, where each Sj, j = 1, 2, . . ., m, is a subset of S. From Theorem 10.23,
we may assume, without loss of generality, that m ≤n. We ﬁrst check, for each
subcollection C′ ⊆C of size ≤k0, whether it is a set cover of S or not. There are
only O(nk0) many such subcollections, and so this step can be done in polynomial
time in n.
If no set cover of cardinality ≤k0 is found, then we construct a reduction from
the instance (S, C) to a graph G for problem MIN-CDS. The graph G is deﬁned as
follows: It has m+n+2 vertices, labeled x1, x2, . . ., xn, S1, S2, . . ., Sm, p, and q.
In addition, G contains the following edges: {p, q}; {Sj, p}, for all j = 1, 2, . . ., m;
and {xi, Sj} if xi ∈Sj (see Figure 10.12).
Now, we observe the following relationships between C and G:
(1) Assume that C has a set cover of size k. Then graph G has a connected
dominating set of size k + 1. Indeed, if C′ is a set cover for S, then C′ ∪{p} forms
a connected dominating set for G.

10.5 (ρ ln n)-Inapproximability
393
(2) Assume that G has a connected dominating set D of size k. Then, we can
ﬁnd a set cover C′ ⊆C of size at most k −1. To see this, we note that if D is
a connected dominating set of G, then D′ = D ∩{S1, S2, . . ., Sm, p} is still a
connected dominating set of G. Indeed, D must contain p in order to dominate q
and to connect to any vertex Sj in D. Thus, q can be removed from set D if q ∈D.
Moreover, if xi ∈D −D′ for some i = 1, . . ., n, then xi must be connected to p
through some vertex Sj in D. Also, every vertex Sℓdominated by xi is dominated
by p. Thus, xi can be removed from D. It follows that D′ −{p} must be a set cover
of S of size k −1.
Now, suppose the minimum set cover of C contains k subsets. Note that, from
our preprocessing, we know that k > k0. From the above two properties, we know
that the minimum connected dominating set of G contains k + 1 vertices. Applying
the polynomial-time (ρ ln n)-approximation for MIN-CDS on instance G, we get a
connected dominating set D of G of size ≤(ρ ln(m+n+2))(k+1). From property
(2), we can obtain a set cover C′ ⊆C of S of size at most
ρ ln(m + n + 2)(k + 1) −1 < ρ

1 + 1
k0

1 + ln 3
lnn

(ln n)k.
When n is sufﬁciently large, C′ is a (ρ′ lnn)-approximation solution for the instance
(S, C) of the problem MIN-SC.
□
In Chapter 2, we showed that the weighted connected vertex cover problem
(MIN-WCVC) has a polynomial-time (1+lnn)-approximation. We show here that
this is the best possible polynomial-time approximation for this problem.
Theorem 10.25 There is no no polynomial-time (ρ ln n)-approximation for the
problem MIN-WCVC, for any 0 < ρ < 1, unless NP ⊆DTIME(nO(log log n)),
where n is the number of vertices in the input graph.
Proof. By Theorem 10.23, it sufﬁces to show that if MIN-WCVC has a polynomial-
time r-approximation, so does MIN-SC.
Let S = {x1, x2, . . ., xn} and C = {S1, S2, . . ., Sm} be an input instance to
MIN-SC, where each Sj, j = 1, 2, . . ., m, is a subset of S. We construct a graph G
as follows: G has n + m + 1 vertices labeled x1, x2, . . ., xn, S1, S2, . . ., Sm, and
p, and has the following edges connecting the vertices: {Sj, p} for j = 1, 2, . . ., m,
and {xi, Sj} if xi ∈Sj (see Figure 10.13). Furthermore, for each vertex in G, we
assign weight to it as follows: Each vertex Sj, for j = 1, 2, . . ., m, has weight
w(Sj) = 1, and all other vertices u have weight w(u) = 0. We have thus obtained
an instance (G, w) of MIN-WCVC.
Suppose D is an r-approximation to the problem MIN-WCVC on the instance
(G, w). Let C1 = D ∩C. Then we claim that C1 is a set cover of the instance (S, C).
To see this, suppose otherwise that xi, for some i = 1, . . ., n, is not covered by
any subset in C1. Let Sj1, Sj2, . . ., Sjk be the sets in C that contain xi. Then k ≥1
as  C = S. Since C1 does not cover xi, none of the sets Sj1, . . ., Sjk is in C1. It
follows that D ∩{Sj1, . . ., Sjk} = ∅. Now consider the following two cases.

394
Inapproximability
S
x
S
x
S
xn
m
p
1
2
1
2
Figure 10.13:
Graph G in the proof of Theorem 10.25.
Case 1. xi ̸∈D. In this case, none of the edges between xi and Sj1, Sj2, . . ., Sjk
in G is covered by D. This is a contradiction to the assumption that D is a vertex
cover of G.
Case 2. xi ∈D. Since D ∩{Sj1, . . ., Sjk} = ∅, D must contain p in order to
cover edges between p and Sj1, . . ., Sjk. However, this means that p and xi are not
connected in D, which is a contradiction to the assumption that D is connected. So,
the claim is proven.
Now, from the deﬁnition of weight w, we see that w(D) = |C1|. We now prove
that C1 is an r-approximation to the problem MIN-SC on the instance (S, C). To
see this, consider an optimal solution C∗of MIN-SC for the instance (S, C). Let
D∗= C∗∪{p} ∪{x1, x2, . . ., xn}. Then D∗is a connected vertex cover of G with
w(D∗) = |C∗|. Moreover, we note that D∗is a minimum connected vertex cover of
G. Indeed, if there were a connected vertex cover D′ of G with w(D′) < w(D∗),
then, by the same argument above, we see that set C′ = D′ ∩C would be a set cover
of (S, C) with |C′| = w(D′) < w(D∗) = |C∗|, contradicting the optimality of C∗
for the instance (S, C). It follows that
|C|
|C∗| = w(D)
w(D∗) ≤r.
□
The following problem arises from the study of trafﬁc in wireless networks:
CONNECTED DOMINATING SET WITH SHORTEST PATHS (CDS-SP):
Given a graph G = (V, E), ﬁnd the minimum connected dominating
set C satisfying that for every pair of vertices (u, v), there is a shortest
path from u to v such that all of its intermediate vertices belong to set
C.
Lemma 10.26 Let C be a connected dominating set of a graph G. Then the follow-
ing two conditions about C are equivalent:
(1) For every pair of vertices u and v in G, there is a shortest path (u, w1, . . ., wk,
v) such that all of its intermediate vertices w1, w2, . . ., wk belong to set C.
(2) For every pair of vertices u and v in G of distance 2, there exists a shortest
path (u, w, v) such that w belongs to set C.

10.5 (ρ ln n)-Inapproximability
395
m
S
S
S
x
x
x
2
1
1
2
q
p
n
Figure 10.14:
Graph G constructed in the proof of Theorem 10.27.
Proof. It is trivial to see that (1) implies (2). We now show that (2) implies (1).
Consider two vertices u and v. Suppose there is a shortest path (u, w1, . . ., wk, v)
between them. Then, by condition (2), there exist vertices s1, s2, . . ., sk in C such
that (u, s1, w2), (s1, s2, w3), (s2, s3, w4), . . ., (sk−1, sk, v) are all shortest paths.
This implies that (u, s1, s2, s3, . . ., sk−1, sk, v) is also a shortest path between u
and v, with all intermediate vertices belonging in C.
□
Theorem 10.27 The problem CDS-SP does not have a polynomial-time (ρ ln δ)-
approximation for any 0 < ρ < 1, unless NP ⊆DTIME(nO(log log n)), where δ is
the maximum vertex degree of the input graph.
Proof. We will construct a reduction from MIN-SC to CDS-SP. Suppose (S, C) is
an input instance of MIN-SC, where S = {x1, x2, . . ., xn} and C is a collection of
subsets S1, S2, . . ., Sm of S. We deﬁne a graph G of m + n + 2 vertices, labeled
S1, . . ., Sm, x1, . . ., xn, p, and q. In addition, it has the followingedges: {p, Sj} and
{q, Sj}, for j = 1, 2, . . ., m; {q, xi}, for i = 1, 2, . . ., n; and {xi, Sj} if xi ∈Sj
(see Figure 10.14).
We claim that C has a set cover of size at most k if and only if G has a connected
dominating set of size at most k + 1 satisfying condition (2) of Lemma 10.26. The
claim holds trivially in the case of |C| = 1. In the following,we assume that |C| ≥2.
First, assume that C has a set cover A of size at most k. Then it is easy to verify
that D = A ∪{q} is a connected dominating set of G satisfying condition (2)
of Lemma 10.26. Indeed, for a pair of vertices u and v of distance 2 in G with
u ̸= p ̸= v, (u, q, v) must be a shortest path with q ∈D. For a pair of vertices p
and v with distance 2, v must belong to {xi | 1 ≤i ≤n} ∪{q}. If v = xi for
some i = 1, 2, . . ., n, then there must be a set Sj ∈A such that xi ∈Sj and, hence,
(p, Sj, xi) is a shortest path with Sj ∈D. If v = q, then for any Sj ∈A, (p, Sj, q)
is a required shortest path.

396
Inapproximability
Conversely, assume that G has a connected dominating set D of size at most k+1
satisfying condition (2) of Lemma 10.26. Note that the distance from p to each xi,
for i = 1, 2, . . ., n, is 2, and every shortest path from p to xi must pass a vertex Sj
for some j = 1, 2, . . ., m. Therefore, A = {Sj | Sj ∈D} is a set cover for S.
Moreover, we note that, for any two distinct sets Sj, Sk in C, the distance between
vertices Sj and Sk is 2, and the intermediate vertex of any shortest path between Sj
and Sk does not belong to C = {S1, S2, . . ., Sm}. Thus, D must contain at least
one vertex not in C. It follows that |A| ≤k.
Let optSC and optCDS denote, respectively, the size of the minimum set cover in
C and that of the minimum connected dominating set of G satisfying condition (2)
of Lemma 10.26. The claim above shows that optCDS = optSC + 1. Now, suppose
G has a polynomial-time approximation solution D of size at most (ρ ln δ)optCDS
for some constant ρ < 1. Note that, by Theorem 10.23, we may assume that m ≤
n. Thus, δ ≤2n. From the claim, we can ﬁnd a polynomial-time approximation
solution for MIN-SC of size at most
ρ ln(2n)(optSC + 1) < 1
2 (ρ + 1) lnn · optSC,
for sufﬁciently large n and sufﬁciently large optSC. (Note that, for any constant
α, we can check in polynomial-time whether optSC ≤α.) It follows that NP ⊆
DTIME(nO(log log n)).
□
The above results imply that the problems MIN-SC and CDS-SP are not in APX
if NP ̸⊆DTIME(nO(log log n)). This result can be further improved to hold with the
weaker condition of P ̸= NP, using the following different lower-bound result for
MIN-SC.
Theorem 10.28 Assume that P ̸= NP. Then there exists a constant c > 0 such that
the problem MIN-SC does not have a polynomial-time (c ln n)-approximation.
Corollary 10.29 If P ̸= NP, then MIN-SC and CDS-SP are not in APX.
10.6
nc-Inapproximability
In this section, we study optimization problems that are not approximable with the
performance ratio nc for some constant c > 0, unless P = NP. We ﬁrst introduce
a well-known NP-hard optimization problem. Recall that a clique of a graph G is a
complete subgraph of G.
CLIQUE: Given a graph G, ﬁnd a clique C of G of the maximum car-
dinality.
For a graph G = (V, E), deﬁne its complement to be G = (V, E), where
E = {{u, v} | u, v ∈V } −E. It is clear that a vertex subset S ⊆V of a graph
G = (V, E) is independent in G if and only if it induces a clique in G. In other
words, CLIQUE and MAXIMUM INDEPENDENT SET (MAX-IS) are complemen-
tary problems with the following property: An approximation algorithm for one of

10.6 nc-Inapproximability
397
them can be converted to an approximation algorithm for the other one with the
same performance ratio.
GRAPH COLORING (GCOLOR) and CLIQUE are the ﬁrst two problems proved
to be nc-inapproximable by exploring the properties of the PCP systems.
Theorem 10.30 The problems CLIQUE and MAX-IS do not have polynomial-time
(n1−ε)-approximations for any ε > 0 unless P = NP, where n is the number of
vertices in the input graph.
Theorem 10.31 The problem GCOLOR does not have a polynomial-time (n1−ε)-
approximation for any ε > 0 unless P = NP, where n is the number of vertices in
the input graph.
Many nc-inapproximabilityresults are proved through gap-preserving reductions
from these three problems. We present two examples in this section. First, we con-
sider the following problem. Recall that for a given collection of sets, a set packing
is a subcollection of disjoint sets.
MAXIMUM SET PACKING (MAX-SP): Given a collection C of subsets
of a ﬁnite set S, ﬁnd a maximum set packing in C.
Theorem 10.32 The problem MAX-SP does not have a polynomial-time (n1−ε)-
approximation for any ε > 0 unless P = NP, where n is the number of subsets in
the input collection.
Proof. We can reduce MAX-IS to MAX-SP. Let G = (V, E) be an input instance
of MAX-IS. For each v ∈V , let Ev be the set of edges incident upon v. Consider
the instance (E, C) of MAX-SP, where C = {Ev | v ∈V }. Clearly, a vertex subset
V ′ ⊆V is an independent set of G if and only if {Ev | v ∈V ′} is a set packing for
the collection C. Therefore, if MAX-SP has a polynomial-time nc-approximation
for some 0 < c < 1, so does MAX-IS, and, by Theorem 10.30, P = NP.
□
The next problem is a variation of GCOLOR.
CHROMATIC SUM (CS): Given a graph G = (V, E), ﬁnd a vertex
coloring φ : V →N+ for G that minimizes the sum 
v∈V φ(v) of the
colors.
Theorem 10.33 The problem CS has no polynomial-time (n1−ε)-approximation
for any ε > 0 unless P = NP, where n is the number of vertices in the input graph.
Proof. Assume that the problem CS has a polynomial-time nc-approximation algo-
rithm A for some 0 < c < 1. Let G be an input instance for the problem GCOLOR,
and assume that the chromatic number of G is equal to k. Then the optimal chro-
matic sum of G is at most kn. Therefore, algorithm A, when run on graph G, pro-
duces a vertex coloring with the sum of colors bounded by kn1+c. It follows that at
least half of the vertices in G are colored by the colors in {1, 2, . . ., ⌊2knc⌋}. Let us

398
Inapproximability
ﬁx the coloring of these vertices. For the remaining ⌊n/2⌋vertices, we apply algo-
rithm A to these vertices again, and use up to ⌊2k(n/2)c⌋new colors to color half
of these vertices. In this recursive way, we can ﬁnd a vertex coloring for G using at
most
2knc
∞

i=0
1
(2c)i = O(knc)
colors. This means that GCOLOR has a polynomial-time (nc′)-approximation for
some c < c′ < 1. By Theorem 10.31, P = NP.
□
In addition to the above three problems, the following problem also plays an
important role in connecting the theory of computational complexity to the theory
of inapproximability. We say a subset A of the vertex set V of a graph G = (V, E)
is regular if all vertices in A have the same degree.
LABEL COVER (LC): Given a bipartite graph G = (U, V, E), in which
the set U is regular, an alphabet Σ of potential labels for vertices, and a
mapping σ(u,v) : Σ →Σ, for each edge (u, v) ∈E, ﬁnd a vertex label
τ : U ∪V →Σ that maximizes the number of satisﬁed edges, where
an edge (u, v) is satisﬁed by τ if σ(u,v)(τ(u)) = τ(v).
The problem LC has a polynomial-time nc-approximation for some constant c.
Indeed, the best-known performance ratio of an approximation algorithm for LC is
lower than nε for any ε > 0. To further discuss the hardness of approximation for
this problem, we formulate a subproblem of LC with gaps. For an input instance
(G, Σ, σ) of LC, let opt(G) denote the maximum number of satisﬁed edges by any
labeling of vertices.
LC-GAP(α, k): For an input instance (G = (U, V, E), Σ, σ) of LC,
with |Σ| = n, |U| = |V | = O(nk), |E| = m, and having the prop-
erty that either opt(G) = m or opt(G) < αkm, determine whether
opt(G) = m or opt(G) < αkm.
The following result has been proved in the theory of computational complexity.
Theorem 10.34 There exists a constant 0 < α < 1 such that for every positive
integer k, the problem LC-GAP(α, k) is not in P unless NP ⊆DTIME(nk).
By choosing appropriate values for k, we get the following inapproximability
results for LC.
Corollary 10.35 (a) The problem LC does not have a polynomial-time (ρ log n)-
approximation for any ρ > 0 unless NP ⊆DTIME(nO(log log n)).
(b) The problem LC does not have a polynomial-time (2log1−ε n)-approximation
for any ε > 0 unless NP ⊆DTIME(nlogO(1) n).

Exercises
399
More inapproximability results can be established from the above results about
LC. For instance, an O(logn) lower bound for the problem MIN-SC can be proven
using Corollary 10.35(a) (see Exercises 10.30 and 10.31).
It is interesting to point out that, in addition to the (ρ ln n)- and nc-inapproxi-
mability results, there are also problems of which the best performance ratio lies
strictly between these two bounds. The following are two examples.
DIRECTED STEINER TREE (DST): Given an edge-weighted directed
graph G = (V, E), a source node s, and a terminal set P , ﬁnd a directed
tree containing paths from s to every terminal in P such that the total
edge-weight is minimized.
It is known that the problem DST has a polynomial-time nc-approximation for
any c > 0, and hence its hardness of approximation is weaker than that of CLIQUE.
It is also known that DST cannot be approximated in polynomial time within a fac-
tor of log2−ε n of the optimal solution unless NP has quasi-polynomial-time Las
Vegas algorithms (i.e, unless problems in NP can be solved by probabilistic algo-
rithms with zero error probability that run in time O(nlogk n) for some constant
k > 0).
GROUP STEINER TREE (GST): Given an edge-weighted graph G =
(V, E), a root vertex r ∈V , and k nonempty subsets of vertices,
g1, g2, . . ., gk, ﬁnd a tree in G with the minimum total weight that con-
tains root r and at least one vertex from each subset gi, i = 1, . . ., k.
It has been proven that the problem GST has a polynomial-time O(log3 n)-
approximation, but no polynomial-time O(log2−ε n)-approximation for any ε > 0,
unless NP has quasi-polynomial-time Las Vegas algorithms. For details of the re-
sults about these two problems, the reader is referred to Charikar et al. [1999], Garg
et al. [2000], and Halperin and Krauthgamer [2003].
Exercises
10.1 Consider the problem k-CENTERS which is a generalization of the prob-
lem METRIC-k-CENTERS such that the input distance table between cities may not
satisfy the triangle inequality. Prove, using the many–one reduction with gap, that
there is no polynomial-time constant approximation for k-CENTER unless P = NP.
10.2 Show that the following greedy algorithm is a 2-approximation for the
problem METRIC-k-CENTERS: First, pick any city to build a warehouse. In each
of the subsequent k −1 iterations, pick a city that has the maximum distance to any
existing warehouse, and place a warehouse in this city.
10.3 Let a graph G and a distance table d between vertices in G be an input
instance to the problem METRIC-k-CENTERS.

400
Inapproximability
(a) Sort the edges in G in nondecreasing order, and let Gi denote the graph
of the same vertex set but having only the ﬁrst i edges. Show that solving
METRIC-k-CENTERS on instance (G, d) is equivalent to ﬁnding the mini-
mum index i such that Gi contains a dominating set of size k.
(b) Based on part (a) above, we can design an approximation algorithm for
METRIC-k-CENTERS as follows: Find the minimum index i such that Gi
has a maximal independent set D of size ≤k, and build warehouses at
each v ∈D. Prove that this algorithm is a 2-approximation for METRIC-k-
CENTERS.
10.4 Show that the bottleneck Steiner tree problem (BNST) in the Euclidean
plane cannot be approximated in polynomial time with a performance ratio smaller
than
√
2, provided P ̸= NP.
10.5 Show that if P ̸= NP, then the following problem has no polynomial-time
(2 −ε)-approximation for any ε > 0:
Given a set of points in the Euclidean plane and a set of disks that
cover all given points, ﬁnd a subset of disks covering all points such
that the maximum number of disks containing a common given point is
minimized.
10.6 Let α > 0 be a constant. Show that statement (1) below implies statement
(2).
(1) It is NP-hard to approximate CLIQUE within a factor of α.
(2) It is NP-hard to approximate CLIQUE within a factor of α2.
10.7 Show the following results on the problem EDP.
(a) Given a graph G and two pairs (u1, v1) and (u2, v2) of vertices in G, it
is NP-complete to determine whether G contains two edge-disjoint paths
connecting the two given pairs, respectively.
(b) The problem EDP does not have a polynomial-time (2 −ε)-approximation
for any ε > 0 unless P = NP.
(c) The problem EDP has a polynomial-time √m -approximation, where m is
the number of edges in the input graph.
10.8 For a solution y to an instance x of a problem Π in NPO, deﬁne its error by
E(x, y) = max
objΠ(y)
optΠ(x), optΠ(x)
objΠ(y)

−1.
A problem Π is E-reducible to a problem Λ, denoted by Π ≤E Λ, if there exist
polynomial-time computable functions f, g and a constant β such that
(1) f maps an instance x of Π to an instance f(x) of Λ and there exists a polyno-
mial p(n) such that optΛ(f(x)) ≤p(|x|)optΠ(x).

Exercises
401
(2) g maps solutions y of f(x) to solutions of x such that E(x, g(y)) ≤β ·
E(f(x), y).
Show the following:
(a) If Π ≤E Γ and Γ ≤E Λ, then Π ≤E Λ.
(b) If Π ≤E Λ and Λ has a PTAS, then Π has a PTAS.
(c) If Π ≤P
L Λ, then Π ≤E Λ.
10.9 Show that the following problems are APX-hard:
(a) CONNECTED-MAJ-DS: Given a connected graph G = (V, E), ﬁnd a con-
nected majority-dominating set of the minimum cardinality. (A connected
majority-dominating set is a majority-dominating set that induces a con-
nected subgraph.)
(b) MAX-3-COLOR: Given a graph G = (V, E), ﬁnd a vertex coloring using
three colors such that the total number of edges with two endpoints having
different colors is maximized.
10.10 Show that the PCP theorem holds if and only if Theorem 10.19 holds.
10.11 Show that the problem MAX-CUT does not have a polynomial-time
(17/16 −ε)-approximation for any ε > 0 unless P = NP.
10.12 Show that the problem MAX-2SAT does not have a polynomial-time
(22/21 −ε)-approximation for any ε > 0 unless P = NP.
10.13 Show that the network Steiner minimum tree problem (NSMT) does not
have a polynomial-time (96/95)-approximation unless P = NP.
10.14 Show that the problem METRIC-TSP does not have a polynomial-time
(3813/3812 −ε)-approximation for any ε > 0 unless P = NP.
10.15 Design a polynomial-time O(lnδ)-approximation for the problem CDS-
SP, where δ is the maximum vertex degree of the input graph.
10.16 Let G = (V, E) be a connected graph, in which each edge is associated
with a set of colors c : E →2N. A set of colors is called a color covering if all edges
in those colors contain a spanning tree of G. Also, for each v ∈V , we deﬁne the
set of colors of v to be the set of colors associated with edges incident on v. Show
that for each of the following problems, if NP ̸⊆DTIME(nlogO(1) n), then it has no
(ρ ln n)-approximation for any ρ < 1:
(a) Given a graph G = (V, E) and edge-color sets c : E →2N, ﬁnd a color
covering of the minimum cardinality.
(b) Given a graph G = (V, E) and edge-color sets c : E →2N, ﬁnd a subset
S ⊆V of the minimum cardinality such that the set of colors of all vertices
in S forms a color covering.

402
Inapproximability
(c) Given a graph G = (V, E) and edge-color sets c : E →2N, with the
property that the set of edges in any ﬁxed color forms a connected subgraph,
ﬁnd a color-connected subset S ⊆V of the minimum cardinality such that
the set of colors of all vertices in S forms a color covering.
10.17 For each of the following problems, show that it does not have a polyno-
mial-time approximation with performance ratio ρ ln n for any 0 < ρ < 1 unless
NP ⊆DTIME(nO(log log n)):
(a) WSID (deﬁned in Section 2.5).
(b) DST.
(c) NODE WEIGHTED STEINER TREE (NWST): Given a graph with node
weight and a set of terminals, ﬁnd a Steiner tree interconnecting all ter-
minals such that the total node weight is minimized.
(d) The special case of NWST in which all nodes of the input graph have
weight 1.
(e) The special case of DST in which the input graph is acyclic.
10.18 Explain why the proof of Theorem 10.25 fails for the unweighted con-
nected vertex cover problem.
10.19 Show that the problem of ﬁnding the minimum dominating set in a given
graph has no polynomial-time (ρ ln n)-approximation for 0 < ρ < 1 unless NP ⊆
DTIME(nO(log log n)).
10.20 Design a polynomial-time algorithm for the following problem:
Given a graph G = (V, E), ﬁnd the minimum dominating set satisfying
that for every shortest path (u, w1, . . ., wk, v) in G, all intermediate
nodes w1, w2, . . ., wk belong to the dominating set.
10.21 The domatic number of a graph is the maximum number of disjoint dom-
inating sets in the graph. Show that the domatic number cannot be approximated
within a factor of ρ ln n in polynomial time for any 0 < ρ < 1 unless NP ⊆
DTIME(nO(log log n)).
10.22 A binary matrix is ¯d-separable if all Boolean sums of at most d columns
are distinct. Consider the following problem:
MINIMUM ¯d-SEPARABLE SUBMATRIX (MIN- ¯d-SS): Given a binary
matrix M, ﬁnd a minimum ¯d-separable submatrix with the same num-
ber of columns.
Show that there is a constant c > 0 such that MIN- ¯d-SS has no polynomial-time
(c lnn)-approximation unless NP ⊆DTIME(nO(log log n)).

Exercises
403
10.23 A binary matrix is d-separable if all Boolean sums of d columns are dis-
tinct. Consider the following problem:
MINIMUM d-SEPARABLE SUBMATRIX (MIN-d-SS): Given a binary
matrix M, ﬁnd a minimum d-separable submatrix with the same num-
ber of columns.
Show that there is a constant c > 0 such that MIN-d-SS has no polynomial-time
(c lnn)-approximation unless NP ⊆DTIME(nO(log log n)).
10.24 A binary matrix is d-disjunct if for every d + 1 columns C0, C1, . . ., Cd,
there is a row at which C0 has entry 1 but all of C1, . . ., Cd have entry 0. Consider
the following problem:
MINIMUM d-DISJUNCT SUBMATRIX (MIN-d-DS): Given a binary
matrix M, ﬁnd a minimum d-disjunct submatrix with the same num-
ber of columns.
(a) Show that there is a constant c > 0 such that MIN-d-DS has no polynomial-
time (c lnn)-approximation unless NP ⊆DTIME(nO(log log n)).
(b) Show that the special case of MIN-d-DS in which each row of the binary
matrix contains at most two 1s is APX-complete.
10.25 Consider the following problem:
BUDGETED MAXIMUM COVERAGE: Given a ﬁnite set S, a weight
function w : S →N on elements of S, a collection C of subsets of
set S, a cost function c : C →N on sets in C, and a budget L, ﬁnd a
subcollection C′ ⊆C with its total cost no more than the budget L such
that the total weight of the covered elements is maximized.
Show that this problem does not have a polynomial-time (
e
e−1 −ε)-approximation
for any ε > 0 unless NP ⊆DTIME(nO(log log n)).
10.26 Show that for any ε > 0, it is NP-hard to approximate the following
problem within a factor of n1−ε:
Given a graph G, ﬁnd a maximal independent set in G of the minimum
cardinality.
10.27 Study the hardness of approximation for the following problems:
CONNECTED SET COVER: Given a collection C of a ﬁnite set S and a
graph G with vertex set C, ﬁnd a minimum set cover C′ ⊆C such that
the subgraph induced by C′ is connected.
MAXIMUM DISJOINT SET COVER: Given a collection C of a ﬁnite set
S, ﬁnd a partition of C into the maximum number of parts such that
each part is a set cover.

404
Inapproximability
10.28 Consider the following problem:
MAXIMUM CONSTRAINT GRAPH (MAX-CG): Given an alphabet Σ
and a directed graph G = (V, E) with each edge (u, v) ∈E labeled
with a mapping σ(u,v) : Σ →Σ, ﬁnd a mapping τ : V →Σ that maxi-
mizes the number of satisﬁed edges, where an edge (u, v) is satisﬁed if
σ(u,v)(τ(u)) = τ(v).
Answer the following questions and prove your answers:
(a) Is MAX-CG in APX?
(b) Is MAX-CG APX-hard for the alphabet Σ with |Σ| ≥2?
10.29 Show that every APX-complete problem has an NP-hard gap [α, β] with
ratio β/α greater than 1.
10.30 Let B be a ground set, and C = {C1, . . ., Cm} a collection of subsets of
B. We say (B, C) is an (m, ℓ)-system if any subcollection of ℓsubsets chosen from
{C1, . . ., Cm, C1, . . ., Cm} that covers B must contain both Ci and Ci for some
i = 1, 2, . . ., m. Prove by the probabilistic method that, for any 0 < ℓ< m, there
exists an (m, ℓ)-system with a ground set B of size O(2ℓℓlog m).
10.31 In this exercise, we construct a reduction from LC to MIN-SC to establish
an O(logn) lower bound for the performance ratio for any approximation of MIN-
SC. Let (G = (U, V, E), Σ, σ) be an input instance of LC, with |Σ| = n, |U| =
|V | = O(nk), and |E| = m. Choose ℓ= O(log n) and k = O(log log n) so that
αkℓ2 < 2. Let C = {C1, . . ., Cm} be an (m, ℓ)-system with a ground set B, as
constructed from Exercise 10.30. Let S = E × B, and deﬁne a collection F of
subsets of S as follows: For each vertex v ∈V and x ∈Σ, construct a subset Sv,x
of E × B as
Sv,x =
(
u:(u,v)∈E
{(u, v)} × Cx.
For each vertex u ∈U and x ∈Σ, construct a subset Su,x of E × B as
Su,x =
(
v:(u,v)∈E
{(u, v)} × Cσ(u,v)(x).
Prove that this reduction has the following two properties:
(1) If the instance (G, Σ, σ) of LC has a labeling τ that satisﬁes all edges, then
the instance (S, F) of MIN-SC has a set cover of size 2n.
(2) If every labeling for the instance (G, Σ, σ) of LC can satisfy at most αkm
edges, then every set cover for the instance (B, F) of MIN-SC has size at
least ℓn/4.
10.32 Show that the problem LC with the gap [m/ log3 m, m] is not in P unless
NP ⊆DTIME(nO(log log n)), where m is the number of edges in the input graph.

Historical Notes
405
Historical Notes
Inapproximability results and the concept of approximation-preserving reductions
have been studied since the 1970s (see, e.g., Garey and Johnson [1976], Sahni and
Gonzalez [1976], Ko [1979], and Ausiello et al. [1980]). However, the develop-
ment of the theory of inapproximability ﬂourished only in the 1990s through the
study of PCP systems, which was inspired by the study of interactive proof sys-
tems [Feige et al., 1991]. The notion of L-reductions was introduced by Papadim-
itriou and Yannakakis [1988]. They also introduced the class MAXSNP and showed
many MAXSNP-complete problems. Khanna et al. [1999] generalized it to APX-
completeness. APX-hardness of VC-CG (Theorem 10.15) and MAJ-DS (Theorem
10.16) are from Du, Gao, and Wu [1997] and Zhu et al. [2010], respectively.
The PCP theorem, with its application to the inapproximability of MAX-SAT
was established in Arora et al. [1992, 1998] and Arora and Safra [1992, 1998], and
received a lot of attention. Nowadays, due to the work of Khanna et al. [1999],
the PCP theorem is no longer required to get the inapproximability of MAX-
SAT or many other optimization problems. However, the PCP system remains an
important tool to study inapproximability. H˚astad’s 3-Bit PCP theorem [H˚astad,
2001] is an important version. Many constant lower bounds for performance ra-
tios were established from this theorem, including MAX-3SAT, MIN-VC, METRIC
TSP [B¨ockenhauser et al., 2000], and NSMT [Chlebik and Chlebikoca, 2002]. An-
other important result is the proof for the lower bound of the performance ratio of
MIN-SC. Lund and Yannakakis [1993] obtained the ﬁrst lower bound that MIN-SC
does not have a polynomial-time (ρ ln n)-approximation for any 0 < ρ < 1/4 un-
less NP ⊆DTIME(npoly(log n)). The current best bounds (Theorems 10.23 and
10.28) are given by Feige [1996] and Raz and Safra [1997], respectively. The
(ρ ln n)-inapproximability of MIN-CDS (Theorem 10.24), MIN-WCVC (Theo-
rem 10.25), and CDS-SP (Theorem 10.27) are from Guha and Khuller [1998], Fu-
jito [2001], and Ding et al. [2010], respectively. For the problem CLIQUE, H˚astad
[1999] established the lower bound n1−ε for its performance ratio, using a stronger
complexity-theoretic assumption of NP ̸= ZPP. Zuckerman [2006, 2007] deran-
domized his construction and weakened the assumption to P ̸= NP. The best-known
approximation algorithm for GCOLOR generates a coloring of size within a fac-
tor O(n(logn)−3(log log n)2) of the chromatic number [Halld´orsson, 1993]. The
(n1−ε)-inapproximability for GCOLOR was proved by Zuckerman [2006, 2007]
under the assumption P ̸= NP. The inapproximability of CHROMATIC SUM (The-
orem 10.33) is due to Bar-Noy et al. [1998]. The problem LABEL COVER and its
inapproximability (Theorem 10.34) are studied in Arora et al. [1993].
Exercise 10.5 is from Erlebach and van Leeuwen [2008]. The notion of E-
reductions and its basic properties (Exercise 10.8) are due to Khanna et al. [1999].
The lower bound of 96/95 for the performance ratio of NSMT (Exercise 10.13) is
from Chlebik and Chlebikoca [2002]. The lower bound of 3813/3812 for the perfor-
mance ratio of METRIC-TSP (Exercise 10.14) is from B¨ockenhauer et al. [2000].
The inapproximability of domatic numbers (Exercise 10.21) is due to Feige et
al. [2002]. Exercises 10.22, 10.23, and 10.24(a) are from Du and Hwang [2006],

406
Inapproximability
and Exercise 10.24(b) is from Wang et al. [2007]. The inapproximability of BUD-
GETED MAXIMUM COVERAGE (Exercise 10.25) is due to Khuller et al. [1999].
Exercise 10.26 is from Halld´orsson [1993]. The problem CONNECTED SET COVER
is studied in Zhang, Gao, and Wu [2009].

Bibliography
Agarwal, P.K., van Kreveld, M. and Suri, S. [1998], Label placement by maximum indepen-
dent set in rectangles, Comput. Geom. Theory Appl. 11, 209–218.
Ageev, A.A. and Sviridenko, M. [2004], Pipage rounding: A new method of constructing
algorithms with proven performance guarantee, J. Comb. Optim. 8, 307–328.
Agrawal, A., Klein, P. and Ravi, R. [1995], When trees collide: An approximation algorithm
for the generalized Steiner problem on networks, SIAM J. Comput. 24, 440–456.
Alizadeh, F. [1991], Combinatorial Optimization with Interior Point Methods and Semidef-
inite Matrices, Ph.D. Thesis, Computer Science Department, University of Minnesota, Min-
neapolis, Minnesota.
Alizadeh, F. [1995], Interior point methods in semideﬁnite programming with applications to
combinatorial optimization, SIAM J. Optim. 5, 13–51.
Alizadeh, F., Haeberly, J.-P. A. and Overton, M. [1994], Primal-dual interior point meth-
ods for semideﬁnite programming, Technical Report 659, Computer Science Department,
Courant Institute of Mathematical Sciences, New York University, New York.
Alizadeh, F., Haeberly, J.-P. A. and Overton, M. [1997], Complementarity and nondegeneracy
in semideﬁnite programming, Math. Program. 77, 111–128.
Alon, N., Goldreich, O., Hastad, J. and Peralta, R. [1992], Simple constructions of almost
k-wise independent random variables, Random Struc. Algorithms 3, 289–304.
Alon, N., Sudakov, B. and Zwick, U. [2001], Constructing worst case instances for semideﬁ-
nite programming based approximation algorithms, SIAM J. Disc. Math. 15, 58–72.
Alzoubi, K.M., Wan, P. and Frieder, O. [2002], Message-optimal connected dominating sets
in mobile ad hoc networks, Proceedings, 3rd ACM International Symposium on Mobile ad
hoc Networking and Computing, pp. 157–164.
D.-Z. Du et al., Design and Analysis of Approximation Algorithms,  
Springer Optimization and Its Applications 62, DOI 10.1007/978-1-4614-1701-9, 
© Springer Science+Business Media, LLC 2012 
407

408
Bibliography
Amb¨uhl, C. [2005], An optimal bound for the MST algorithm to compute energy efﬁcient
broadcast trees in wireless networkds, Proceedings, 32nd International Colloquium on Au-
tomata, Languages and Programming, Lecture Notes in Computer Science 3580, Springer,
pp. 1139–1150.
Amb¨uhl, C., Erlebach, T., Mihal´ak, M. and Nunkesser, M. [2006], Constant-approximation
for minimum-weight (connected) dominating sets in unit disk graphs, Proceedings, 9th Inter-
national Workshop on Approximation Algorithms for Combinatorial Optimization Problems
and International Workshopon Randomization and Computation, Lecture Notes in Computer
Science 4110, Springer, pp. 3–14.
An, L.T.H., Tao, P.D. and Muu, L.D. [1998], A combined d.c. optimization-ellipsoidal
branch-and-bound algorithm for solving nonconvex quadratic programming problems, J.
Comb. Optim. 2, 9–28.
Anjos, M.F. and Wolkowicz, H. [2002], Strengthened semideﬁnite relaxations via a second
lifting for the Max-Cut problem, Disc. Appl. Math. 119, 79–106.
Arkin, E.M., Mitchell, J.S.B. and Narasimhan, G. [1998], Resource-constructed geometric
network optimization, Proceedings, 14th Symposium on Computational Geometry, pp. 307–
316.
Armen, C. and Stein, C. [1996], A 2 2
3-approximation algorithm for the shortest superstring
problem, Proceedings, 7th Symposium on Combinatorial Pattern Matching, Lecture Notes
on Computer Science 1075, Springer, pp. 87–101.
Arora, S. [1996], Polynomial-time approximation schemes for Euclidean TSP and other geo-
metric problems, Proceedings, 37th IEEE Symposium on Foundations of Computer Science,
pp. 2–12.
Arora, S. [1997], Nearly linear time approximation schemes for Euclidean TSP and other
geometric problems, I, Proceedings, 38th IEEE Symposium on Foundations of Computer
Science, pp. 554–563.
Arora, S. [1998], Polynomial-time approximation schemes for Euclidean traveling salesman
and other geometric problems, J. Assoc. Comput. Mach. 45, 753–782.
Arora, S., Babai, L., Stern, J. and Sweedyk, Z. [1993], The hardness of approximate optima
in lattices, codes, and systems of linear equations, Proceedings, 34th IEEE Symposium on
Foundations of Computer Science, pp. 727–733.
Arora, S., Grigni, M., Karger, D., Klein, P. and Woloszyn, A. [1998], A polynomial time
approximation scheme for weighted planar graph TSP, Proceedings, 9th ACM-SIAM Sympo-
sium on Discrete Algorithms, pp. 33–41.
Arora, S. and Kale, S. [2007], A combinatorial, primal-dual approach to semideﬁnite pro-
grams, Proceedings, 39th ACM Symposium on Theory of Computing, pp. 227–236.
Arora, S., Lund, C., Motwani, R., Sudan, M. and Szegedy, M. [1992], Proof veriﬁcation and
hardness of approximation problems, Proceedings, 33rd IEEE Symposium on Foundations of
Computer Science, pp. 14–23.
Arora, S., Lund, C., Motwani, R., Sudan, M. and Szegedy, M. [1998], Proof veriﬁcation and
hardness of approximation problems, J. Assoc. Comput. Mach. 45, 753–782.

Bibliography
409
Arora, S., Raghavan, P. and Rao, S. [1998], Polynomial time approximation schemes for
Euclidean k-medians and related problems, Proceedings, 30th ACM Symposium on Theory
of Computing, pp. 106–113.
Arora, S., Rao, S. and Vazirani, U. [2004], Expenderﬂows, geometric embeddings, and graph
partitionings, Proceedings, 36th ACM Symposium on Theory of Computing, pp. 222–231.
Arora, S. and Safra, S. [1992], Probabilistic checking of proofs: A new characterization of
NP, Proceedings, 33rd IEEE Symposium on Foundations of Computer Science, pp. 2–13.
Arora, S. and Safra, S. [1998], Probabilistic checking of proofs: A new characterization of
NP, J. Assoc. Comput. Mach. 45, 70–122.
Ausiello, G., D’Atri, A. and Protasi, M. [1980], Structural preserving reductions among con-
vex optimization problems, J. Comput. Systems Sci. 21, 136–153.
Bafna, V., Berman, P. and Fujito, T. [1999], A 2-approximation algorithm for the undirected
feedback vertex set problem, SIAM J. Disc. Math. 12, 289–297.
Baker, B.S. [1983], Approximation algorithms for NP-complete problems on planar graphs,
Proceedings, 24th IEEE Symposium on Foundations of Computer Science, pp. 265–273.
Baker, B.S. [1994], Approximation algorithms for NP-complete problems on planar graphs,
J. Assoc. Comput. Mach. 41, 153–180.
Bar-Ilan, J., Kortsarz, G. and Peleg, D. [2001], Generalized submodular cover problem and
applications, Theoret. Comput. Sci. 250, 179–200.
Bar-Noy, A., Bar-Yehuda, R., Freund, A., Naor, J. and Shieber, B. [2001], A uniﬁed approach
to approximating resource allocation and scheduling, J. Assoc. Comput. Mach. 48, 1069–
1090.
Bar-Noy, A., Bellare, M., Halld´orsson, M.M., Shachnai, H. and Tamir, T. [1998], On chro-
matic sums and distributed resource allocation, Inform. Comput. 140, 183–202.
Bar-Yehuda, R., Bendel, K., Freund, A. and Rawitz, D. [2004], Local ratio: A uniﬁed frame-
work for approximation algorithms. In memoriam: Shimon Even 1935–2004, ACM Com-
put. Surv. 36, 422–463.
Bar-Yehuda R. and Even, S. [1981], A linear time approximation algorithm for the weighted
vertex cover problem, J. Algorithms 2, 198–203.
Bar-Yehuda, R. and Even, S. [1985], A local-ratio theorem for approximating the weighted
vertex cover problem, Annals Disc. Math. 25, 27–46.
Bar-Yehuda, R. and Rawitz, D. [2004], Local ratio with negative weights, Oper. Res. Lett. 32,
540–546.
Bar-Yehuda, R. and Rawttz, D. [2005a], On the equivalence between the primal-dual schema
and the local ratio technique, SIAM J. Disc. Math. 19, 762–797.
Bar-Yehuda, R. and Rawitz, D. [2005b], Using fractional primal-dual to schedule split inter-
vals with demands, Proceedings, 13th European Symposium on Algorithms, Lecture Notes in
Computer Science 3669, Springer, pp. 714–725.
Bellare, M., Goldreich, O. and Sudan, M. [1995], Free bits, PCPs and non-approximability—
towards tight results, Proceedings, 36th IEEE Symposium on Foundations of Computer Sci-
ence, pp. 422–431.

410
Bibliography
Berman, P., DasGupta, B., Muthukrishnan, S. and Ramaswami, S. [2001], Efﬁcient approxi-
mation algorithms for tiling and packing problem with rectangles, J. Algorithms 41, 443–470.
Bertsimas, D. and Teo, C.-P. [1998], From valid inequalities to heuristics: A uniﬁed view of
primal-dual approximation algorithms in covering problems, Oper. Res. 46, 503–514.
Bertsimas, D., Teo, C.-P. and Vohra, R. [1999], On dependent randomized rounding algo-
rithms, Oper. Res. Lett. 24, 105–114.
Bertsimas, D. and Ye, Y. [1998], Semideﬁnite relaxations, multivariate normal distributions,
and order statistics, in Handbook of Combinatorial Optimization, Vol. 3, D.-Z. Du and P.M.
Pardalos (eds.), Kluwer, pp. 1–19.
Bland, R.G. [1977], New ﬁnite pivoting rules of the simplex method, Math. Oper. Res. 2,
103–107.
Blum, A., Jiang, T., Li, M., Tromp, J. and Yannakakis, M. [1991], Linear approximation of
shortest superstrings, Proceedings, 23rd ACM Symposium on Theory of Computing, pp. 328–
336.
Blum, A., Jiang, T., Li, M., Tromp, J. and Yannakakis, M. [1994], Linear approximation of
shortest superstrings, J. Assoc. Comput. Mach. 41, 630–647.
B¨ockenhauer, H.-J., Hromkovic, J., Klasing, R., Seibert, S. and Unger, W. [2000], An im-
proved lower bound on the approximability algorithms of metric TSP with sharpened trian-
gle inequality, Proceedings, 17th Symposium on Theoretical Aspects of Computer Science,
Lecture Notes on Computer Science 1770, Springer, pp. 382–394.
Borchers, A. and Du, D.-Z. [1995], The k-Steiner ratio in graphs, Proceedings, 27th ACM
Symposium on Theory of Computing, pp. 641–649.
Butenko, S. and Ursulenko, O. [2007], On minimum connected dominating set problem in
unit-ball graphs, preprint.
Byrka, J. [2007], An optimal bifactor approximation algorithm for the metric uncapacitated
facility location problem, Proceedings, 10th International Workshop on Approximation Al-
gorithms for Combinatorial Optimization Problems, pp. 29–43.
Cadei, M., Cheng, M.X., Cheng, X. and Du, D.-Z. [2002], Connected domination in mul-
tihop ad hoc wireless networks, Proceedings, 6th Joint Conference on Information Science,
pp. 251–255.
Calinescu, G., Chekuri, C., P´al, M. and Vondrk, J. [2007], Maximizing a submodular set
function subject to a matroid constraint, Proceedings, 12th International Integer Program-
ming and Combinatorial Optimization Conference,Lecture Notes in Computer Science4513,
Springer, pp. 182–196.
Chan, T.M. [2003], Polynomial-time approximation schemes for picking and piercing fat
objects, J. Algorithms 46, 178–189.
Chan, T.M. [2004], A note on maximum independent sets in rectangle intersection graphs,
Inform. Process. Lett. 89, 19–23.
Charikar, M., Chekuri, C., Cheung, T.-Y., Dai, Z., Goel, A., Guha, S. and Li, M. [1999],
Approximation algorithms for directed Steiner problems, J. Algorithms 33, 73–91.
Charnes, A. [1952], Optimality and degeneracy in linear programming, Econometrica 20,
160–170.

Bibliography
411
Chen, J.-C. [2007], Iterative rounding for the closest string problem, Computing Research
Repository, arXiv:0705.0561.
Chen, Y.P. and Liestman, A.L. [2002], Approximating minimum size weakly-connecteddom-
inating sets for clustering mobile ad hoc networks, Proceedings, 3rd ACM International Sym-
posium on Mobile ad hoc Networking and Computing, pp. 165–172.
Cheng, X., DasGupta, B. and Lu, B. [2001], A polynomial time approximation scheme for
the symmetric rectilinear Steiner arborescence problem, J. Global Optim. 21, 385–396.
Cheng, X., Huang, X., Li, D., Wu, W. and Du, D.-Z. [2003], Polynomial-time approximation
scheme for minimum connected dominating set in ad hoc wireless networks, Networks 42,
202–208.
Cheng, X., Kim, J.-M. and Lu, B. [2001], A polynomial time approximation scheme for the
problem of interconnecting highways, J. Comb. Optim. 5, 327–343.
Cheriyan, J., Vempala, S. and Vetta, A. [2006], Network design via iterative rounding of
setpair relaxations, Combinatorica 26, 255–275.
Chlamtac, E. [2007], Approximation algorithms using hierarchies of semideﬁnite program-
ming relaxations, Proceedings, 48th IEEE Symposium on Foundations of Computer Science,
pp. 691–701.
Chlebik, M. and Chlebikoca, J. [2002], Approximation hardness of the Steiner tree problem
on graphs, Proceedings, 8th Scandinavia Workshop on Algorithm Theory, Lecture Notes on
Computer Science 2368, Springer, pp. 170–179.
Christoﬁdes, N. [1976], Worst-case analysis of a new heuristic for the travelling salesman
problem, Technical Report, Graduate School of Industrial Administration, Carnegie-Mellon
University, Pittsburgh, PA.
Chudak, F.A., Goemans, M.X., Hochbaum, D.S. and Williamson, D.P. [1998], A primal-
dual interpretation of two 2-approximation algorithms for the feedback vertex set problem in
undirected graphs, Oper. Res. Lett. 22, 111–118.
Chung, F.R.K. and Gilbert, E.N. [1976], Steiner trees for the regular simplex, Bull. Inst. Math.
Acad. Sinica 4, 313–325.
Chung, F.R.K. and Graham, R.L. [1985], A new bound for Euclidean Steiner minimum trees,
Ann. N.Y. Acad. Sci. 440, 328–346.
Chung, F.R.K. and Hwang, F.K. [1978], A lower bound for the Steiner tree problem, SIAM
J. Appl. Math. 34, 27–36.
Chv´atal, V. [1979], A greedy heuristic for the set-covering problem, Math. Oper. Res. 4, 233–
235.
Cook, S.A. [1971], The complexity of theorem-proving procedures, Proceedings, 3rd ACM
Symposium on Theory of Computing, pp. 151–158.
Cormem, T.H., Leiserson, C.E. and Rivest, R.L. [1990], Introduction to Algorithms, McGraw-
Hill, New York.
Courant, R. and Robbins, H. [1941], What Is Mathematics?, Oxford University Press, New
York.

412
Bibliography
Czumaj, A., Gasieniec, L., Piotrow, M. and Rytter, W. [1994], Parallel and sequential ap-
proximation of shortest superstrings, Proceedings, 4th Scandinavian Workshop on Algorithm
Theory, pp. 95–106.
Dahlhaus, E., Johnson, D.S., Papadimitriou, C.H., Seymour, P.D. and Yannakakis, M. [1994],
The complexity of multiterminal cuts, SIAM J. Comput. 23, 864–894.
Dai, D. and Yu, C. [2009], A 5 + ϵ-approximation algorithm for minimum weighted domi-
nating set in unit disk graph, Theoret. Comput. Sci. 410, 756–765.
Dantzig, G.B. [1951], Maximization of a linear function of variables subject to linear inequal-
ities, in Activity Analysis of Production and Allocation (Cowles Commission Monograph 13),
T.C. Koopmans (ed.), John Wiley, New York, pp. 339–347.
Dantzig, G.B. [1963], Linear Programming and Extensions, Princeton University Press,
Princeton, NJ.
Dantzig, G.B., Ford, L.R. and Fulkerson, D.R. [1956], A primal-dual algorithm for linear
programs, in Linear Inequalities and Related Systems, H.W. Kuhn and A.W. Tucker (eds.),
Princeton University Press, Princeton, NJ, pp. 171–181.
Das, B. and Bharghavan, V. [1997], Routing in ad hoc networks using minimum connected
dominating sets, Proceedings, IEEE International Conference on Communications, Vol. 1,
pp. 376–380.
Deering, S., Estrin, D., Farinacci, D., Jacobson,V., Lui, C. and Wei, L. [1994], An architecture
for wide area multicast routing, Proceedings, ACM SIGCOMM 1994, pp. 126–135.
Ding, L., Gao, X., Wu, W., Lee, W., Zhu, X. and Du, D.-Z [2010], Distributed construction
of connected dominating sets with minimum routing cost in wireless networks, Proceedings,
30th International Conference on Distributed Computing Systems, pp. 448–457.
Drake, D.E. and Hougardy, S. [2004], On approximation algorithms for the terminal Steiner
tree problem, Inform. Process. Letters 89, 15–18.
Du, D.-Z. [1986], On heuristics for minimum length rectangularpartitions, Technical Report,
Mathematical Sciences Research Institute, University of California, Berkeley.
Du, D.-Z., Gao, B. and Wu, W. [1997], A special case for subset interconnection designs,
Disc. Applied Math. 78, 51–60.
Du, D.-Z., Graham, R.L, Pardalos, P.M., Wan, P.-J., Wu, W. and Zhao, W. [2008], Analysis
of greedy approximation with nonsubmodular potential functions, Proceedings, 19th ACM-
SIAM Symposium on Discrete Algorithms, pp. 167–175.
Du, D.-Z., Hsu, D.F. and Xu, K.-J. [1987], Bounds on guillotine ratio, CongressusNumeran-
tium 58, 313–318.
Du, D.-Z. and Hwang, F.K. [1990], The Steiner ratio conjecture of Gilbert–Pollak is true,
Proc. National Acad. Sci. 87, 9464–9466.
Du, D.-Z. and Hwang, F.K. [2006], Pooling Designs and Nonadaptive Group Testing, World
Scientiﬁc, Singapore.
Du, D.Z., Hwang, F.K., Shing, M.T. and Witbold, T. [1988], Optimal routing trees, IEEE
Trans. Circuits 35, 1335–1337.

Bibliography
413
Du, D.-Z. and Ko, K.-I. [2000], Theory of Computational Complexity, Wiley Interscience,
New York.
Du, D.-Z. and Ko, K.-I. [2001], Problem Solving in Automata, Languages, and Complexity,
John Wiley & Sons, New York.
Du, D.-Z. and Miller, Z. [1988], Matroids and subset interconnection design, SIAM J.
Disc. Math. 1, 416–424.
Du, D.-Z., Pan, L.Q., and Shing, M.-T. [1986], Minimum edge length guillotine rectangular
partition, Technical Report 02418-86, Mathematical Sciences Research Institute, University
of California, Berkeley.
Du, D.-Z. and Zhang, Y. [1990], On heuristics for minimum length rectilinear partitions,
Algorithmica 5, 111–128.
Du, D.-Z., Zhang, Y. and Feng, Q. [1991], On better heuristic for Euclidean Steiner minimum
trees, Proceedings, 32nd IEEE Symposium on Foundations of Computer Science, pp. 431–
439.
Du, H., Jia, X., Wang, F., Thai, M. and Li, Y. [2005], A note on optical network with non-
splitting nodes, J. Comb. Optim. 10, 199–202.
Du, X., Wu, W. and Kelley, D.F. [1998], Approximations for subset interconnection designs,
Theoret. Comput. Sci. 207, 171–180.
Eriksson, H. [1994], MBONE: The multicast backbone, Comm. Assoc. Comput. Mach. 37,
54–60.
Erlebach, T., Jansen, K. and Seidel, E. [2001], Polynomial-time approximation schemes for
geometric graphs, Proceedings, 12th ACM-SIAM Symposium on Discrete Algorithms, pp.
671–679.
Erlebach, T. and van Leeuwen, E.J. [2008], Approximating geometric coverage problems,
Proceedings, 19th ACM-SIAM Symposium on Discrete Algorithms, pp. 1267–1276.
Feige, U. [1996], A threshold of ln n for approximating set cover (preliminary version), Pro-
ceedings, 28th ACM Symposium on Theory of Computing, pp. 314–318.
Feige, U. [1998], A threshold of ln n for approximating set cover, J. Assoc. Comput. Mach.
45, 634–652.
Feige, U. and Goemans, M.X. [1995], Approximating the value of two prover proof systems,
with applications to MAX 2SAT and MAX DICUT, Proceedings, 3rd Israel Symposium on
Theory of Computing and Systems, pp. 182–189.
Feige, U., Goldwasser, S., Lovsz, L., Safra, S. and Szegedy, M. [1991], Approximating clique
is almost NP-complete, Proceedings, 32nd IEEE Symposium on Foundations of Computer
Science, pp. 2–12.
Feige, U., Halld´orsson, M., Kortsarz, G. and Srinivasan, A. [2002], Approximating the do-
matic number, SIAM J. Comput. 32, 172–195.
Feige, U. and Langberg, M. [2001], Approximation algorithms for maximization problems
arising in graph partition, J. Algorithms 41, 174–211.
Feige, U. and Langberg, M. [2006], The RPR2 rounding technique for semideﬁnite programs,
J. Algorithms 60, pp. 1–23.

414
Bibliography
Fleischer, L., Jain, K. and Williamson, D.P. [2001], An iterative rounding 2-approximation al-
gorithm for the element connectivity problem, Proceedings, 42nd IEEE Symposium on Foun-
dations of Computer Science, pp. 339–347.
Foulds, L.R. and Graham, R.L. [1982], The Steiner problem in phylogeny is NP-complete,
Adv. Appl. Math. 3, 43–49.
Freund, A. and D. Rawitz, D. [2003], Combinatorial interpretations of dual ﬁtting and primal
ﬁtting, Proceedings, 1st Workshop on Approximation and Online Algorithms, Lecture Notes
in Computer Science 2909, Springer, pp. 137–150.
Frieze, A. and Jerrum, M. [1995], Improved approximation algorithms for MAX k-CUT and
MAX BISECTION, Proceedings,4th International Integer Programming and Combinatorial
Optimization Conference, Lecture Notes in Computer Science 920, Springer, pp. 1–13.
Fu, M., Luo, Z.-Q. and Ye, Y. [1998], Approximation algorithms for quadratic programming,
J. Comb. Optim. 2, 29–50.
Fujito, T. [1998], A uniﬁed approximation algorithm for node-deletion problems, Disc. Appl.
Math. 86, 213–231.
Fujito, T. [1999], On approximation of the submodular set cover problem, Oper. Res. Lett.
25, 169–174.
Fujito, T. [2001], On approximability of the independent/connected edge dominating set
problems, Infom. Process. Lett. 79, 261–266.
Fujito, T. and Yabuta, T. [2004], Submodular integer cover and its application to produc-
tion planning, Proceedings, 2nd International Workshop on Approximation and Online Algo-
rithms, Lecture Notes in Computer Science 3351, Springer, pp. 154–166.
Funke, S., Kesselman, A., Meyer, U. and Segal, M. [2006], A simple improved distributed
algorithm for minimum CDS in unit disk graphs, ACM Trans. Sensor Networks 2, 444–453.
Gabow, H.N. and Gallagher, S. [2008], Iterated rounding algorithms for the smallest k-edge
connected spanning subgraph, Proceedings, 19th ACM-SIAM Symposium on Discrete Algo-
rithms, pp. 550–559.
Gabow, H.N., Goemans, M.X., Tardos, E. and Williamson, D.P. [2009], Approximating the
smallest k-edge connected spanning subgraph by LP-rounding, Networks 53, pp. 345–357.
Galbiati, G. and Mafﬁoli, F. [2007], Approximation algorithms for maximum cut with limited
unbalance, Theoret. Comput. Sci. 385, 78–87.
Gandhi, R., Khuller, S., Parthasarathy, S. and Srinivasan, A. [2006], Dependent rounding and
its applications to approximation algorithms, J. Assoc. Comput. Mach. 53, 324–360.
Gao, X., Huang, Y., Zhang, Z. and Wu, W. [2008], (6 + ε)-approximation for minimum
weight dominating set in unit disk graphs, Proceedings, 14th International Conference on
Computing and Combinatorics, pp. 551–557.
Garey, M.R., Graham, R.L. and Johnson, D.S. [1977], The complexity of computing Steiner
minimal trees, SIAM J. Appl. Math. 32, 835–859.
Garey, M.R. and Johnson, D.S. [1976], The complexity of near-optimal graph coloring, J. As-
soc. Comput. Mach. 23, 43–49.

Bibliography
415
Garey, M.R. and Johnson, D.S. [1977], The rectilinear Steiner tree is NP-complete, SIAM J.
Appl. Math. 32, 826–834.
Garey, M. R. and Johnson, D. S. [1979], Computersand Intractability: A Guide to the Theory
of NP-Completeness, W. H. Freeman and Company, New York.
Garg, N., Konjevod, G. and Ravi, R. [2000], A polylogarithmic approximation algorithm for
the group Steiner tree problem, J. Algorithms 37, 66–84.
Ge, D., He, S., Ye, Y. and Zhang, S. [2010], Geometric rounding: A dependent rounding
scheme, J. Comb. Optim. (to appear).
Ge, D., Ye, Y. and Zhang, J. [2010], Linear programming-based algorithms for the ﬁxed-hub
single allocation problem, preprint.
Gilbert, E.N. and Pollak, H.O. [1968], Steiner minimal trees, SIAM J. Appl. Math., 16, 1–29.
Goemans, M.X., Goldberg, A., Plotkin, S., Shmoys, D., Tardos, E. and Williamson, D.P.
[1994], Approximation algorithms for network design problems, Proceedings, 5th ACM-
SIAM Symposium on Discrete Algorithms, pp. 223–232.
Goemans, M.X. and Williamson, D.P. [1994], New 3
4-approximation algorithms for the max-
imum satisﬁability problem, SIAM J. Disc. Math. 7, 656–666.
Goemans, M.X. and Williamson, D.P. [1995a], A general approximation technique for con-
strained forest problems, SIAM J. Comput. 24, 296–317.
Goemans, M.X. and Williamson, D.P. [1995b], Improved approximation algorithms for
maximum cut and satisﬁability problems using semideﬁnite programming, J. Assoc. Com-
put. Mach. 42, 1115–1145.
Goemans, M.X. and Williamson, D.P. [1997], The primal-dual method for approximation
algorithms and its application to network design problems. in Approximation Algorithms for
NP-Hard Problems, D. Hochbaum (ed.), PWS Publishing Company, Boston, MA, Chapter 4.
Goemans, M.X. and Williamson, D.P. [2004], Approximation algorithms for Max-3-Cut and
other problems via complex semideﬁnite programming, J. Comput. System Sci. 68, 442–470.
Gonzalez, T. and Zheng, S.Q. [1985], Bounds for partitioning rectilinear polygons, Proceed-
ings, 1st Symposium on Computational Geometry, pp. 281–287.
Gonzalez, T. and Zheng, S.Q. [1989], Improved bounds for rectangular and guillotine parti-
tions, J. Symbolic Comput. 7, 591–610.
Graham, R.L. [1966], Bounds on multiprocessing timing anomalies, Bell System Tech. J. 45,
1563–1581.
Graham, R.L. and Hwang, F.K. [1976], Remarks on Steiner minimal trees, Bull. Inst. Math.
Acad. Sinica 4, 177–182.
Guha, S. and Khuller, S. [1998a], Approximation algorithms for connected dominating sets,
Algorithmica 20, 374–387.
Guha, S. and Khuller, S. [1998b], Improved methods for approximating node weighted
Steiner trees and connected dominating sets, Lecture Notes on Computer Science 1530,
Springer, pp. 54–66.

416
Bibliography
Guha, S. and Khuller, S. [1998c], Greedy strikes back: Improved facility location algorithms,
Proceedings, 9th ACM-SIAM Symposium on Discrete Algorithms, pp. 228–248.
Guo, L., Wu, W., Wang, F. and Thai, M. [2005], Approximation for minimum multicast route
in optical network with nonsplitting nodes, J. Comb. Optim. 10, 391–394.
Gusﬁeld, D. and Pitt, L. [1992], A bounded approximation for the minimum cost 2-SAT
problem, Algorithmica 8, 103–117.
Halld´orsson, M.M. [1993a], A still better performance guarantee for approximate graph col-
oring, Inform. Process. Lett. 45, 19–23.
Halld´orsson, M.M. [1993b], Approximating the minimum maximal independence number,
Inform. Process. Lett. 46, 169–172.
Halld´orsson, M.M. and Wattenhofer, R. [2009], Wireless communication is in APX, Pro-
ceedings, 36th International Colloquium on Automata, Languagesand Programming, Part I,
pp. 525–536.
Halperin, E. and Krauthgamer, R. [2003], Polylogarithmic inapproximability, Proceedings,
35th ACM Symposium on Theory of Computing, pp. 585–594.
Halperin, E., Livnat, D. and Zwick, U. [2002], MAX CUT in cubic graphs, Proceedings,13th
ACM-SIAM Symposium on Discrete Algorithms, pp. 506–513.
Halperin, E., Nathaniel, R. and Zwick, U. [2001], Coloring k-colorable graphs using smaller
palettes, Proceedings, 12th ACM-SIAM Symposium on Discrete Algorithms, pp. 319–326.
Halperin, E. and Zwick, U. [2001a], Approximation algorithms for MAX 4-SAT and round-
ing procedures for semideﬁnite programs, J. Algorithms 40, 184–211.
Halperin, E. and Zwick, U. [2001b], A uniﬁed framework for obtaining improved approx-
imation algorithms for maximum graph bisection problems, Proceedings, 8th International
Integer Programming and Combinatorial Optimization Conference, Lecture Notes in Com-
puter Science 2081, Springer, pp. 210–225.
Han, Q., Ye, Y., Zhang, H. and Zhang, J. [2002], On approximation of max-vertex-cover,
Eur. J. Operat. Res. 143, 342–355.
Han, Q., Ye, Y. and Zhang, J. [2002], An improved rounding method and semideﬁnite pro-
gramming relaxation for graph partition, Math. Program. 92, 509–535.
H˚astad, J. [1999], Clique is hard to approximate within n to the power 1−ϵ, Acta Math. 182,
105–142.
H˚astad, J. [2001], Some optimal inapproximability results, J. Assoc. Comput. Mach. 48, 798–
859.
Hausmann, D., Korte, B. and Jenkyns, T.A. [1980], Worst case analysis of greedy type algo-
rithms for independence systems, Math. Program. Study 12, 120–131.
Hochbaum, D.S. [1997a], Approximating covering and packing problems: Set cover, ver-
tex cover, independent set, and related problems, in Approximation Algorithms for NP-Hard
Problems, D.S. Hochbaum (ed.), PWS Publishing Company, Boston, pp. 94–143.
Hochbaum, D.S. [1997b], Various notions of approximations: good, better, best, and more,
in Approximation Algorithms for NP-hard Problems, D.S. Hochbaum (ed.) PWS Publishing
Company, Boston, pp. 346–398.

Bibliography
417
Hochbaum, D.S. and Maass, W. [1985], Approximation schemes for covering and packing
problems in image processing and VLSI, J. Assoc. Comput. Mach. 32, 130–136.
Hsieh, S.Y. and Yang, S.-C. [2007], Approximating the selected-internal Steiner tree, Theo-
ret. Comput. Sci. 38, 288–291.
Hunt III, H.B., Marathe, M.V., Radhakrishnan, V., Ravi, S.S., Rosenkrantz, D.J. and Stearns,
R.E. [1998], Efﬁcient approximations and approximation schemes for geometric problems,
J. Algorithms 26, 238–274.
Hwang, F.K. [1972], On Steiner minimal trees with rectilinear distance, SIAM J. Appl. Math.
30, 104–114.
Ibarra, O.H. and Kim, C.E. [1975], Fast approximation algorithms for the knapsack and sum
of subset proble, J. Assoc. Comput. Mach. 22, 463–468.
Iyengar, G., Phillips, D.J. and Stein, C. [2009], Approximating semideﬁnite packingprogram,
Optimization Online, http://www.optimization-online.org/DB HTML/
2009/06/2322.html.
Jain, K. [2001], A factor 2 approximation algorithm for the generalized Steiner network prob-
lem, Combinatorica 21, 39–60.
Jain, K., Mahdian, M., Markakis, E., Saberi, A. and Vazirani, V. [2003], Greedy facility loca-
tion algorithms analyzed using dual-ﬁtting with factor-revealing LP, J. Assoc. Comput. Mach.
50, 795–824.
Jain, K. and Vazirani, V. [2001], Approximation algorithms for metric facility location and
k-median problems, using the primal-dual schema and Lagrangian relaxation, J. Assoc. Com-
put. Mach. 48, 274–296.
Jenkyns, T.A. [1976], The efﬁcacy of the “greedy” algorithm, Congressus Numerantium 17,
341–350.
Jiang, T., Lawler, E.B. and Wang, L. [1994], Aligning sequences via an evolutionary tree:
Complexity and algorithms, Proceedings, 26th ACM Symposium on Theory of Computing,
pp. 760–769.
Jiang, T. and Wang, L. [1994], An approximation scheme for some Steiner tree problems
in the plane, Proceedings, 5th International Symposium on Algorithms and Computation,
Lecture Notes in Computer Science 834, Springer, pp. 414–422.
Johnson,D.S. [1974], Approximation algorithms for combinatorial problems, J. Comput. Sys-
tems Sci. 9, 256–278.
Johnson, N. and Kotz, S. [1972], Distributed in Statistics: Continuous Multivariate Distribu-
tion, John Wiley & Sons, New York.
Karger, D., Motwani, R. and Sudan, M. [1994], Approximate graph coloring by semideﬁnite
programming, Proceedings, 35th IEEE Symposium on Foundations of Computer Science,
pp. 1–10.
Karloff, H. and Zwick, U. [1997], A 7/8-approximation algorithm for MAX 3SAT?, Proceed-
ings, 38th IEEE Symposium on Foundations of Computer Science, pp. 406–415.
Karmarkar, N. [1984], A new polynomial-time algorithm for linear programming, Proceed-
ings, 16th ACM Symposium on Theory of Computing, pp. 302–311.

418
Bibliography
Karp, R.M. [1972], Reducibility among combinatorial problems, in Complexity of Computer
Computations, E.E. Miller and J.W. Thatcher (eds.), Plenum Press, New York, pp. 85–103.
Karp, R.M. [1977], Probabilistic analysis of partitioning algorithms for the traveling salesman
problem in the plane, Math. Operat. Res. 2, 209–224.
Khachiyan, L.G. [1979], A polynomial algorithm for linear programming, Doklad. Akad.
Nauk., USSR Sec. 244, 1093–1096.
Khanna, S., Motwani, R., Sudan, M. and Vazirani, U. [1999], On syntactic versus computa-
tional views of approximability, SIAM J. Comput. 28, 164–191.
Khanna, S., Muthukrishnan, S. and Paterson, M. [1998], On approximating rectangle tiling
and packing, Proceedings, 9th ACM-SIAM Symposium on Discrete Algorithms, pp. 384–393.
Khot, S. [2001], Improved inapproximability results for MaxClique, chromatic number and
approximate graph coloring, Proceedings, 42nd IEEE Symposium on Foundations of Com-
puter Science, pp. 600–609.
Khot, S. [2002], On the power of unique 2-prover 1-round games, Proceedings, 34th ACM
Symposium on Theory of Computing, pp. 767–775.
Khot, S., Kindler, G., Mossel, E. and O’Donnell, R. [2007], Optimal inapproximability results
for MAX-CUT and other 2-variable CSPs?, SIAM J. Comput. 37, 319–357.
Khuller, S., Moss, A. and Naor, J. [1999], The budgeted maximum coverage problem, Inform.
Process. Lett. 70, 39–45.
Kim, D., Wu, Y., Li, Y., Zou, F. and Du, D.-Z. [2009], Constructing minimum connecteddom-
inating sets with bounded diameters in wireless networks, IEEE Trans. Parallel Distributed
Systems 20, pp. 147–157.
Klee, V.L. and Minty, G.J. [1972], How good is the simplex algorithm?, in Inequalities III,
O. Shisha (ed.), Academic Press, New York, pp. 159–175
Klein, P. and Lu, H.-I. [1998], Space-efﬁcient approximation algorithms for MAXCUT and
COLORING semideﬁnite programs, Proceedings, 9th International Symposium on Algo-
rithmd and Computation, Lecture Notes in Computer Science 1533, Springer, pp. 387–396.
Klein, P. and Ravi, R. [1995], A nearly best-possible approximation for node-weighted
Steiner trees, J. Algorithms 19, 104–115.
Klerk, E. de [2002], Aspects of Semideﬁnite Programming: Interior Point Algorithms and
Selected Applications (Applied Optimization), Kluwer Academic Publishers, Dordreht, The
Netherlands.
Klerk, E. de, Roos, C. and Terlaky, T. [1998], Polynomial primal-dual afﬁne scaling algo-
rithms for semideﬁnite programming, J. Comb. Optim. 2, 51–70.
Ko, K. [1979], Computational Complexity of Real Functions and Polynomial Time Approxi-
mation, Ph.D. Thesis, Ohio State University, Columbus, Ohio.
Komolos, J. and Shing, M.T. [1985], Probabilistic partitioning algorithms for the rectilinear
Steiner tree problem, Networks 15, 413–423.
Korte, B. and Hausmann, D. [1978], An analysis of the greedy heuristic for independence
systems, Ann. Disc. Math. 2, 65–74.

Bibliography
419
Korte, B. and Vygen, J. [2002], Combinatorial Optimization: Theory and Algorithms, 2nd
Ed., Springer, Berlin.
Kosaraju, S.R., Park, J.K. and Stein, C. [1994], Long tour and shortest superstring, Proceed-
ings, 35th IEEE Symposium on Foundations of Computer Science, pp. 166–177.
Kumar, A., Manokaran, R., Tulsiani, M. and Vishnoi, N. [2010], On the optimality of a class
of LP-based algorithms, manuscript.
Lenstra, J.K., Shmoys, D.B. and Tardos, E. [1990], Approximation algorithms for scheduling
unrelated parallel machines, Math. Program. 46, 259–271.
Levcopoulos, C. [1986], Fast heuristics for minimum length rectangular partitions of poly-
gons, Proceedings, 2nd Symposium on Computational Geometry, pp. 100–108.
Lewin, M., Livnat, D. and Zwick, U. [2002], Improved rounding techniques for the MAX
2-SAT and MAX DI-CUT problems, Proceedings, 9th International Conference on Integer
Programming and Combinatorial Optimization, pp. 67–82.
Li, D., Du, H., Wan, P., Gao, X., Zhang, Z. and Wu, W. [2008], Minimum power strongly
connecteddominating sets in wireless networks, Proceedings,2008 International Conference
on Wireless Networks, pp. 447–451.
Li, D., Du, H., Wan, P., Gao, X., Zhang, Z. and Wu, W. [2009], Construction of strongly
connected dominating sets in asymmetric multihop wireless networks, Theoret. Comput. Sci.
410, 661–669.
Li, X., Gao, X. and Wu, W. [2008], A better theoretical bound to approximate connected
dominating set in unit disk graph, Proceedings, 3rd International Conference on Wireless
Algorithms, Systems and Applications, Lecture Notes in Computer Science 5258, Springer,
pp. 162–175.
Li, Y., Thai, M.T., Wang, F., Yi, C.W., Wan, P.-J. and Du, D.-Z. [2005], Greedy construction of
connected dominating sets in wireless networks, Europe J. Wireless Comm. Mobile Comput.
5, 927–932.
Lin, G.H. and Xue, G. [1999], Steiner tree problem with minimum number of Steiner points
and bounded edge-length, Inform. Process. Lett. 69, 53–57.
Lin, G.H. and Xue, G. [2002], On the terminal Steiner tree problem, Inform. Process. Lett.
84, 103–107.
Ling, A., Tang, L. and Xu, C. [2010], Approximation algorithms for MAX RES CUT with
limited unbalanced constraints, J. Appl. Math. Comput. 33, 357–374.
Lingas, A. [1983], Heuristics for minimum edge length rectangular partitions of rectilinear
ﬁgures, Proceedings, 6th GI-Conference, pp. 199–210.
Lingas, A., Pinter, R.Y., Rivest, R.L. and Shamir, A. [1982], Minimum edge length parti-
tioning of rectilinear polygons, Proceedings, 20th Allerton Conference on Communication,
Control and Computing, pp. 53–63.
Lov´asz, L. [1975], On the ratio of optimal integral and fractional covers, Disc. Math. 13,
383–390.
Lov´asz, L. [1979], On the Shannon capacity of a graph, IEEE Trans. Inform. Theory IT-25,
1–7.

420
Bibliography
Lu, B. and Ruan, L. [2000], Polynomial time approximation schemefor the rectilinear Steiner
arborescence problem, J. Comb. Optim. 4, 357–363.
Lund, C., and Yanakakis, M. [1994], On the hardness of approximating minimization prob-
lems, J. Assoc. Comput. Mach. 41, 960–981.
Mahajan, S. and Ramesh, H. [1999], Derandomizing approximation algorithms based on
semideﬁnite programming, SIAM J. Comput. 28, 1641–1663.
Mahdian, M., Ye, Y. and Zhang, J. [2002], Improved approximation algorithms for metric
facility location problems, Proceedings, 5th International Workshopon Approximation Algo-
rithms for Combinatorial Optimization Problems, pp. 229–242.
Mandoiu, I. and Zelikovsky, A. [2000], A note on the MST heuristic for bounded edge-length
Steiner trees with minimum number of Steiner points, Inform. Process. Lett. 75, 165–167.
Manki, M., Du, H., Jia, X., Huang, C.X., Huang, C.-H. and Wu, W. [2006], Improving con-
struction for connecteddominating set with Steiner tree in wireless sensornetworks,J. Global
Optim. 35, 111–119.
Melkonian, V. and Tardos, ´E. [2004], Algorithms for a network design problem with crossing
supermodular demands, Networks 43, 256–265.
Min, M., Huang, S.C.-H., Liu, J., Shragowitz, E., Wu, W., Zhao, Y. and Zhao, Y. [2003], An
approximation scheme for the rectilinear Steiner minimum tree in presence of obstructions,
in Novel Approaches to Hard Discrete Optimization, Fields Institute Communications Series,
American Mathematical Society, 37, pp. 155–163.
Min, M., Du, H., Jia, X., Huang, C.X., Huang, S. C-H. and Wu, W. [2006], Improving con-
struction for connecteddominating set with Steiner tree in wireless sensornetworks,J. Global
Optim. 35, 111–119.
Mitchell, J.S.B. [1996a], Guillotine subdivisions approximate polygonal subdivisions: A sim-
ple new method for the geometric k-MST problem, Proceedings,7th ACM-SIAM Symposium
on Discrete Algorithms, pp. 402–408.
Mitchell, J.S.B. [1996b], Guillotine subdivisionsapproximate polygonal subdivisions: A sim-
ple polynomial-time approximation scheme for geometric k-MST, TSP, and related problem,
manuscript.
Mitchell, J.S.B. [1997], Guillotine subdivisions approximate polygonal subdivisions: Part
III — Faster polynomial-time approximation scheme for geometric network optimization,
Proceedings, 9th Canadian Conference on Computational Geometry, pp. 229–232.
Mitchell, J.S.B. [1999], Guillotine subdivisions approximate polygonal subdivisions: Part II
— A simple polynomial-time approximation scheme for geometric k-MST, TSP, and related
problem, SIAM J. Comput. 29, 515–544.
Mitchell, J.S.B., Blum, A., Chalasani, P. and Vempala, S. [1999], A constant-factor approxi-
mation algorithm for the geometric k-MST problem in the plane, SIAM J. Comput. 28, 771–
781.
Navarra, A. [2005], Tight bounds for the minimum energy broadcasting problem, Proceed-
ings, 3rd International Symposium on Modeling and Optimization in Mobile, ad hoc, and
Wireless Networks, pp. 313–322.

Bibliography
421
Nemhauser, G.L. and Wolsey, L.A. [1999], Integer and Combinatorial Optimization, John
Wiley & Sons, New York.
Nesterov, Y.E. [1998], Semideﬁnite relaxation and nonconvex quadratic optimization, Optim.
Method. Software 9, 141–160.
Nielsen, F. [2000], Fast stabbing of boxes in high dimensions, Theoret. Comput. Sci. 246,
53–72.
Papadimitriou, C. and Yannakakis,M. [1988], Optimization, approximations, and complexity
classes, Proceedings, 20th ACM Symposium on Theory of Computing, pp. 229–234.
Pardalos, P.M. and Ramana, M. [1997], Semideﬁnite programming, in Interior Point Methods
of Mathematical Programming, Kluwer, Docdreht, The Netherlands, pp. 369–398.
Pardalos, P.M. and Wolkowicz, H. [1998], Topics in Semideﬁnite and Interior-Point Methods,
American Mathematical Society, Providence, RI.
Prisner, E. [1992], Two algorithms for the subset interconnection design problem, Networks
22, 385–395.
Ramamurthy, B., Iness, J. and Mukherjee, B. [1997], Minimizing the number of optical am-
pliﬁers needed to support a multi-wavelength optical LAN/MAN, Proceedings, 16th IEEE
Conference on Computer Communications, pp. 261–268.
Rao, S.B. and Smith, W.D. [1998], Approximating geometrical graphs via “spanners” and
“banyan,” Proceedings, 30th ACM Symposium on Theory of Computing, pp. 540–550.
Ravi, R. and Kececioglu, J.D. [1995], Approximation methods for sequence alignment under
a ﬁxed evolutionary tree, Proceedings, 6th Symposium on Combinatorial Pattern Matching,
Lecture Notes on Computer Science 937, Springer, pp. 330–339.
Ravi, R. and Klein, P. [1993], When cycles collapse: A general approximation technique
for constrained two-connectivity problems, Proceedings, 3rd MPS Conference on Integer
Programming and Combinatorial Optimization, pp. 39–56.
Raz, R. and Safra, S. [1997], A sub-constant error-probability low-degree test, and a sub-
constant error-probability PCP characterization of NP, Proceedings, 28th ACM Symposium
on Theory of Computing, pp. 474–484.
Robin, G. and Zelikovsky, A. [2000], Improved Steiner trees approximation in graphs, Pro-
ceedings, 11th ACM-SIAM Symposium on Discrete Algorithms, pp. 770–779.
Ruan, L., Du, H., Jia, X., Wu., W., Li, Y. and Ko, K. [2004], A greedy approximation for
minimum connected dominating sets, Theoret. Comput. Sci. 329, 325–330.
Rubinstein, J.H. and Thomas, D.A. [1991], The Steiner ratio conjecture for six points, J.
Combinatorial Theory, Ser. A, 58, 54–77.
Sahni, S. [1975], Approximate algorithms for the 0/1 knapsack problem, J. Assoc. Comput.
Mach. 22, 115–124.
Sahni, S. and Gonzalez, T. [1976], P-complete approximation algorithms, J. Assoc. Com-
put. Mach. 23, 555–565.
Salhieh, A., Weinmann, J., Kochha, M. and Schwiebert, L. [2001], Power efﬁcient topologies
for wireless sensor networks, Proceedings, 30th International Workshopon Parallel Process-
ing, pp. 156–163.

422
Bibliography
Sankoff, D. [1975], Minimal mutation trees of sequences, SIAM J. Appl. Math. 28, 35–42.
Schreiber, P. [1986], On the history of the so-called Steiner Weber problem, Wiss. Z. Ernst-
Moritz-Arndt-Univ. Greifswald, Math.-nat.wiss. Reihe 35.
Sivakumar, R., Das, B. and Bharghavan, V. [1998], An improved spine-based infrastructure
for routing in ad hoc networks, Proceedings, 3rd IEEE Symposium on Computers and Com-
munications.
Skutella, M. [2001], Convex quadratic and semideﬁnite programming relaxations in schedul-
ing, J. Assoc. Comput. Mach. 48, 206–242.
Slavik, P. [1997], A tight analysis of the greedy algorithm for set cover, J. Algorithms 25,
237–254.
Stojmenovic, I., Seddigh, M. and Zunic, J. [2002], Dominating sets and neighbor elimination
based broadcasting algorithms in wireless networks, IEEE Trans. Parallel Distr. Systems 13,
14–25.
Tarhio, J. and Ukkonen, E. [1988], A greedy approximation algorithm for constructing short-
est common superstrings, Theoret. Comput. Sci. 57, 131–145.
Teng, S.-H. and Yao, F.F. [1997], Approximating shortest superstrings, SIAM J. Comput. 26,
410–417.
Thai, M.T., Wang, F., Liu, D., Zhu, S. and Du, D.-Z. [2007], Connected dominating sets in
wireless networks with different transmission ranges, IEEE Trans. Mobile Comput. 6, 1–9.
Turner, J.S. [1989], Approximation algorithms for the shortest common superstring problem,
Inform. Comput. 83, 1–20.
Vavasis, S.A. [1991], Automatic domain partitioning in tree dimesions, SIAM J. Sci. Stat.
Comput. 12, 950–970.
Wan, P.-J., Alzoubi, K.M. and Frieder, O. [2002], Distributed construction of connecteddomi-
nating set in wireless ad hoc networks, Proceedings,21st Joint Conferenceof IEEE Computer
and Communications Societies.
Wan, P.-J., Wang, L. and Yao, F.F. [2008], Two phased approximation algorithms for mini-
mum CDS in wireless ad hoc networks, Proceedings, 28th IEEE International Conferenceon
Distributed Computing Systems, pp. 337–344.
Wang, F., Du, H., Jia, X., Deng, P., Wu, W. and MacCallum, D. [2007], Non-unique probe
selection and group testing, Theoret. Comput. Sci. 381, 29–32.
Wang, L. and Du, D.-Z. [2002], Approximations for bottleneck Steiner trees, Algorithmica
32, 554–561.
Wang, L. and Gusﬁeld, D. [1996], Improved approximation algorithms for tree alignment,
Proceedings, 7th Symposium on Combinatorial Pattern Matching, Lecture Notes on Com-
puter Science 1075, Springer, pp. 220–233.
Wang, L. and Jiang, T. [1996], An approximation scheme for some Steiner tree problems in
the plane, Networks 28, 187–193.
Wang, L., Jiang, T. and Gusﬁeld, D. [1997], A more efﬁcient approximation scheme for tree
alignment, Proceedings, 1st International Conference on Computational Biology, pp. 310–
319.

Bibliography
423
Wang, L., Jiang, T., and Lawler, E.L. [1996], Approximation algorithms for tree alignment
with a given phylogeny, Algorithmica 16, 302–315.
Wang, W., Zhang, Z., Zhang, W. and Du, D.-Z. [2009], An approximation algorithm for the
t-latency bounded information propagation problem in social networks, preprint.
Wesolowsky, G. [1993], The Weber problem: History and perspective. Location Science 1,
5–23.
Williamson, D.P. [2002], The primal dual method for approximation algorithms, Math. Pro-
gram. 91, 447–478.
Williamson, D.P., Goemans, M.X., Mihail, M. and Vazirani, V.V. [1995], A primal-dual ap-
proximation algorithm for generalized Steiner network problems. Combinatorica 15, 435–
454.
Willson, J., Gao, X., Qu, Z., Zhu, Y., Li, Y. and Wu, W. [2009], Efﬁcient distributed algo-
rithms for topology control problem with shortest path constraints, Disc. Math., Algorithms
and Applications 1, 437–461.
Wolsey, L.A. [1980], Heuristic analysis, linear programming and branch and bound, Math.
Program. Study 13, 121–134.
Wolsey, L.A. [1982a], An analysis of the greedy algorithm for submodular set covering prob-
lem, Combinatorica 2, 385–393.
Wolsey, L.A. [1982b], Maximizing real-valued submodular function: Primal and dual heuris-
tics for location problems, Math. Operat. Res. 7, 410–425.
Wu, J. and Li, H.L. [1999], On calculating connected dominating set for efﬁcient routing in
ad hoc wireless networks, Proceedings, 3rd ACM International Workshop on Discrete Algo-
rithms and Methods for Mobile Computing and Communications, pp. 7–14.
Wu, W., Du, H., Jia, X., Li, Y. and Huang, C.H. [2006], Minimum connected dominating sets
and maximal independent sets in unit disk graphs, Theoret. Comput. Sci. 352, 1–7.
Yan, S., Deogun, J.S. and Ali, M. [2003], Routing in sparse splitting optical networks with
multicast trafﬁc, Comput. Networks 41, 89–113.
Yang, H., Ye, Y. and Zhang, J. [2003], An approximation algorithm for scheduling two par-
allel machines with capacity constraints, Disc. Appl. Math. 130, 449–467.
Yannakakis, M. [1994], On the approximation of maximum satisﬁability, J. Algorithms 3,
475–502.
Ye, Y. [2001], A .699-approximation algorithm for Max-Bisection, Math. Program. 90, 101–
111.
Zelikovsky, A. [1993], The 11/6-approximation algorithm for the Steiner problem on net-
works, Algorithmica 9, 463–470.
Zelikovsky, A. [1997], A series of approximation algorithms for the acyclic directed Steiner
tree problem, Algorithmica 18, 99–110.
Zhang, J., Ye, Y. and Han, Q. [2004], Improved approximations for max set splitting and max
NAE SAT, Disc. Appl. Math. 142, 133–149.

424
Bibliography
Zhang, Z., Gao, X. and Wu, W. [2009], Algorithms for connected set cover problem and
fault-tolerant connected set cover problem, Theoret. Comput. Sci. 410, 812–817.
Zhang, Z., Gao, X., Wu, W. and Du, D. [2009], A PTAS for minimum connected dominating
set in 3-dimensional wireless sensor networks, J. Global Optimiz. 45, 451–458.
Zhao, Q., Karisch, S.E., Rendl, F. and Wolkowicz, H. [1998], Semideﬁnite programming
relaxations for the quadratic assignment problem, J. Comb. Optim. 2, 71–109.
Zhu, X., Yu, J., Lee, W., Kim, D., Shan, S. and Du, D.-Z. [2010], New dominating sets in
social networks, J. Global Optim. (published online in 2010).
Zong, C. [1999], Sphere Packing, Springer-Verlag, New York.
Zou, F., Li, X., Kim, D. and Wu, W. [2008a], Two constant approximation algorithms for
node-weighted Steiner tree in unit disk graphs, Proceedings, 2nd International Conference
on Combinatorial Optimizationa and Applications, pp. 21–24.
Zou, F., Li, X., Kim, D. and Wu, W. [2008b], Construction of minimum connected domi-
nating set in 3-dimensional wireless network, Proceedings, 3rd International Conference on
Wireless Algorithms, Systems, and Applications,, Lecture Notes in Computer Science 5258,
Springer, pp. 134–140.
Zou, F., Wang, Y., Xu, X., Li, X., Du, H., Wan, P.-J. and Wu, W. [2011], New approximations
for minimum-weighted dominating sets and minimum-weighted connected dominating sets
on unit disk graphs, Theoret. Comput. Sci. 412, 198–208.
Zuckerman, D. [2006], Linear degree extractors and the inapproximability of max clique and
chromatic number, Proceedings, 38th ACM Symposium on Theory of Computing, pp. 681–
690.
Zuckerman,D. [2007], Linear degree extractors and the inapproximability of Max Clique and
Chromatic Number, Theory Comput. 3, 103–128
Zwick, U. [1998], Approximation algorithms for constraint satisfaction problems involving
at most three variables per constraint, Proceedings, 9th ACM-SIAM Symposium on Discrete
Algorithms, pp. 201–210.
Zwick, U. [1999], Outward rotations: A tool for rounding solutions of semideﬁnite program-
ming relaxations, with applications to Max Cut and other problems, Proceedings, 10th ACM-
SIAM Symposium on Discrete Algorithms, pp. 679–687.
Zwick, U. [2000], Analyzing the MAX 2-SAT and MAX DI-CUT approximation algorithms
of Feige and Goemans, manuscript.
Zwick, U. [2002], Computer assisted proof of optimal approximability results, Proceedings,
13th ACM-SIAM Symposium on Discrete Algorithms, pp. 496–505.

Index
|A| (cardinality), 5
A • B (Frobenius inner product), 340
A ≻B (positive deﬁnite), 340
A ⪰B (positive semideﬁnite), 340
ΔDf(C), 50
Δxf(C), 50
ΔxΔyf(A), 62
G|S (induced subgraph), 60
≤P
m (polynomial-time many–one reduc-
tion), 19
≤P
L (L-reduction), 381
≤E (E-reduction), 400
Ω, 9
Ωf, 54
ρk, 89
⊎(union of Steiner trees), 95
ζ(T), 97
Active portal, 187
ACYCLIC DIRECTED STEINER TREE,
117; see also ADST
Adaptive partition, 123
ADST, 117
Agarwal, P.K., 164
Ageev, A.A., 296
Agrawal, A., 336
Ali Baba’s problem, 2
Alignment
lifted, see lifted alignment
minimum score, 111
dynamicprogramming algorithm,
111
of a tree, 112
minimum score, 112
of strings, 111
uniformly lifted, 120
Alizadeh, F., 345, 369
Alon, N., 369
Alzoubi, K.M., 243, 244
Amb¨uhl, 164
An, L.T.H., 369
Anjos, M.F., 370
Approximation
bounded, 27
linear, 27
Approximation algorithm, 4, 13
design of, 9
greedy strategy, 9
local search method, 9
power graph, 9
relaxation method, 9
restriction method, 9, 81
Approximation-preserving reduction,
380, 405
APX, 385
APX-complete problem
having no PTAS, 385
NP-hard gap, 404
425

426
Index
APX-completeness, 380, 385, 405
Arborescence, 108
Arborescence spanning tree, 212
Aristotle, 1
Arkin, E.M., 209
Armen, C., 243
Arora, S., 209, 370, 405
Assignment, 43, 213, 240; see also mini-
mum assignment
to a Boolean formula, 13
truth, 13
Assignment problem, 337
Ausiello, G., 405
Bafna, V., 337
Baker, B.S., 164
Banyan, 205
Bar-Noy, A., 337, 405
Bar-Yehuda, R., 336, 337
Basic feasible solution, 249
Bellare, M., 295
Berman, P., 164
Bertsimas, D., 296, 336, 369
Bhaghavan, V., 243
Binary tree
regular, 86
Binary tree structure, 186
Binary-tree partition, 207
Bland, R.G., 295
Blank symbol, 110
Blum, A., 49, 243
BNST, 104, 375, 385
NP-hard gap, 376
Steinerized spanning tree approxi-
mation, 107
B´ockenhauer, H.-J., 405
Boesky, I., 35
Boolean formula, 13
assignment, 13
clause, 20
conjunctive normal form, 20
literal, 20
planar, 32
satisﬁable, 13
Borchers, A., 89, 122
BOTTLENECK STEINER TREE, 104; see
also BNST
Bounded approximation, 27
Broadcasting routing, 108
Broadcasting tree, 222, 228
BROADCASTING TREE WITH MINIMUM
INTERNAL NODES, 228; see
also BT-MIN
BROADCASTING TREE WITH MINIMUM
POWER, 222; see also BT-MP
BT-MIN, 228, 234
greedy approximation, 232
BT-MP, 222
two-stage greedy approximation,
223
BUDGETED MAXIMUM COVERAGE,
403, 406
Burroughs, W.S., 211
Byrka, J., 337
C, 15
C-hard problem, 385
Cadei, M., 244
Calinescu, G., 296
Catalan numbers, 184
generating function, 184
CDS-SP, 394, 395
CDS-UDG, 129, 223, 241
PTAS, 131
two-stage approximation algorithm,
225
Chan, T.M., 164
Character string, see String
Characteristic vector, 364
Charging method, 169
Charikar, M., 399
Charnes, A., 295
Chen, J.-C., 296
Chen, Y.P., 243
Cheng, X., 164, 209, 244
Cheriyan, J., 296
Chlamtac, E., 370
Chlebik, M., 405
Chlebikoca, J., 405
Cholesky factorization, 343, 358
algorithm, 344
Christoﬁdes’s algorithm, 25, 29, 212
Christoﬁdes, N., 25, 33
CHROMATIC SUM, 397, 405; see also CS
Chung, F.R.K., 121
Church–Turing thesis
extended, 16
Chv´atal, V., 80, 295, 337

427
Clause, 20
Clause function, 29
Clique, 33, 364, 396, 405
CLIQUE, 385, 396, 400
Closed boundary segment, 171
CNF, 260
Color covering, 76, 401
Coloring
vertex, 373
Combinatorial rounding, 259
Complementaryslacknesscondition, 298,
299; see also dual comple-
mentary slackness condition,
primal complementary slack-
ness condition
Computational model, 15
Concatenation
of two strings, 215
Concave function, 76
Conditional probability, 281
Conjunctive normal form, 20; see also
CNF
Connected component
weakly, 222
CONNECTED DOMINATING SET IN A
UNIT DISK GRAPH, 129; see
also CDS-UDG
CONNECTED DOMINATING SET WITH
SHORTEST PATHS, 394; see
also CDS-SP
CONNECTED SET COVER, 403, 406
CONNECTED TARGET COVERAGE, 79;
see also CTC
CONNECTED-MAJ-DS, 401
Convex hull, 364
Convexiﬁcation relaxation, 364, 365
Cook, S.A., 20, 33
Cormen, T.H., 3
Courant, R., 121
Covering problems, 164
Covering-type problem, 310, 325, 336
Crossing, 276
Crosspoint, 178
endpoint, 194
interior, 194
CS, 397
CTC, 79
Curvature, 77
Cut hyperplane, 191
Cut plane, 191
Cycle base, 96
Cyclic shift, 217
Czumaj, A., 243
d-disjunct matrix, 403
¯d-separable matrix, 402
d-separable matrix, 403
Dahlhaus, E., 244
Dai, D., 164
Dantzig, G.B., 295, 336
Dark point, 169; see also 1-dark point, m-
dark point
Das, B., 243
DasGupta, B., 209
Data mining, 164
Davis, G., 123
De Klerk, E., 345, 369
Decision problem, 13, 17
Degree preservation, 283
DEGREE-RESTRICTED SMT, 209
Deming, B., 297
DENSE-k-SUBGRAPH, 366
Dependent randomized rounding, 296
Dependent rounding, 296
Derandomization, 280, 281, 349, 370
Ding, L., 405
Directed graph
weakly connected, 222
DIRECTED STEINER TREE, 399; see also
DST
DIRECTED TSP, 29, 49, 212
approximation algorithm, 214
Disk graph, 242
Divide and conquer, 9
Domatic number, 402, 405
Dominating set, 66, 129, 160, 374
connected, 66, 67, 79, 129, 160,
240–243
in a unit ball graph, 241
in a unit disk graph, 161, 163,
164, 244
in a digraph, 228
in a hypergraph, 79
in a unit disk graph, 164
in an intersection disk graph, 163
strongly connected, 228, 243
weakly connected, 79
weighted, 244
Index

428
Index
Double partition, 142, 155, 164
Downward monotone function, 312
Drake, D.E., 122
DS, 374
DST, 399, 402
Du, D.-Z., 15, 16, 80, 86, 89, 121, 122,
164, 208, 209, 244, 405
Du, X., 80
Dual complementary slacknesscondition,
299, 329, 336
Dual linear program, 298
Dual semideﬁnite program, 342
Dual-feasible solution
minimal, 332
Duality theory, 297
Dynamic programming, 9
E-reduction, 384, 400, 405
Edge
in a hypergraph, 55
EDGE-DISJOINT PATHS, 376; see also
EDP
EDP, 376, 400
EDPc, 377
EDP2
NP-hard, 377
Einstein, A., 371
Ellipsoid, 363
Ellipsoid method, 251, 273, 295
Erlebach, T., 164, 405
ESMT, 82, 122, 161, 206, 207, 209
MST approximation, 84, 85
EUCLIDEAN FACILITY LOCATION, 192,
206, 209
EUCLIDEAN GRADE STEINER TREE,
192, 206
EUCLIDEAN k-MEDIANS, 192, 206, 209
EUCLIDEAN k-SMT, 206
EUCLIDEAN STEINER MINIMUM TREE,
82; see also ESMT
EUCLIDEAN-TSP, 206, 207, 209, 385
Euler tour, 24
algorithm, 24
Even, S., 336, 337
Exact algorithm, 8
exp(λ), 288
Exponential distribution, 288
unit-, 287
Extended Church–Turing thesis, 16
Extreme point
in a polyhedron, 248
Face, 363, 364
FACILITY LOCATION, 327, 335, 337; see
also EUCLIDEAN FACILITY
LOCATION,
local ratio algorithm, 329
Feasible basis, 249
Feasible domain, 9
of a semideﬁnite program, 341
Feasible graph
for vertex subsets, 60
Feasible region, 9, 245
Feasible solution
minimal, 318
FEEDBACK VERTEX SET, 319; see also
FVS
Feedback vertex set, 319
minimal, 319
Feedback vertex set problem, 337
Feige, U., 80, 355, 369, 405
Feng, Q., 122, 164
Fermat problem, 121
Fermat, P., 121
Fleischer, L., 296
Ford, L.R., 336
Foulds, L.R., 121
FPTAS, 27
Freund, A., 337
Frieze, A., 369
Frobenius inner product, 340
Frobenius norm, 365
Fu, M., 369, 370
Fujito, T., 337, 405
Fulkerson, D.R., 336
Full component, 82
Funke, S., 244
FVS, 319
local ratio algorithm, 321
on tournaments, 335
Gabow, H.N., 279, 296
Galbiati, G., 369
Gallagher, S., 279, 296
Gandhi, R., 296
Gao, B., 405
Gao, X., 164, 406
Gap, 372

429
Gap-amplifying reduction, 376
Gap-preserving reduction, 376, 378, 391
Garey, M.R., 33, 80, 121, 375, 405
Garg, N., 399
Gauss, C.F., 121
GC, 64, 295, 303, 333
GC1, 304
local ratio algorithm, 317
primal-dual schema, 305, 307, 309
GCOLOR, 373, 385, 397, 405
Ge, D., 289, 296
Gekko, G., 35
GENERAL COVER, 64, 310; see also GC,
GC1
GENERALIZED SPANNING NETWORK,
272; see also GSN
GENERALIZED STEINER NETWORK,
293
Generating function, 183
Generic reduction, 20
Geometric problem, 191
Geometric rounding, 287, 294, 296
Gilbert and Pollak conjecture, 86, 121
Gilbert, E.N., 121
Goemans, M.X., 296, 336, 355, 369, 370
Goldberg, A., 296
Gonzalez, T., 209, 405
Graham, R.L., 14, 30, 33, 121
Graph, see also hypergraph, unit disk
graph, intersection disk graph,
directed graph
bi-directed, 222
color-covering, 76
dominating set, 66
induced subgraph, 60
k-colorable, 32
matching, 43
GRAPH COLORING, 373; see also
GCOLOR
Graph-coloring problem, 369
Graph-cutting problems, 369
Graph matroid, 41, 61
Graph-splitting problems, 369
GRAPH-3-COLORABILITY,374; seealso
3GCOLOR
Greedy algorithm, 116
Greedy approximation
two-stage, 219
Greedy strategy, 9, 35
Grid point, 167
Grigni, M., 209
Ground set, 49
GROUP STEINER TREE, 399; see also
GST
GSN, 272
Iterated rounding algorithm, 275
Guess-and-verify algorithm, 19
Guha, S., 80, 243, 244, 337, 405
Guillotine cut, 10, 165, 167; see also
1-guillotine cut, m-guillotine
cut, ( 1
3 , 2
3)-restricted guillo-
tine cut
Guillotine rectangular partition, 167; see
also 1-guillotine rectangular
partition, m-guillotine rectan-
gular partition
dynamic programming algorithm,
168
Guillotine, J.I., 165
Guo, L., 244
Gusﬁeld, D., 122, 295
Halld´orsson, M.M., 405, 406
Halperin, E., 369, 399
HAMILTONIAN CIRCUIT, 22; see also
HC
Hamiltonian circuit, 22, 213
Hamiltonian path, 23
Han, Q., 369
Hanan grid, 178
Harmonic function, 56
H˚astad, J., 405
H˚astad’s 3-bit PCP theorem, 378, 390,
405
Hausmann, D., 80
HC, 22, 371, 372
Heuristics, 13
versus approximation, 13
High-level programming language, 15
Hitting set, 31
Hochbaum, D.S., 164, 295
Hougardy, S., 122
Hsieh, S.Y., 122
Hsu, D.F., 208
Hunt III, H.B., 164
Hwang, F.K., 86, 121, 405
Index

430
Index
Hypergraph, 55
degree, 55
dominating set, 79
edge, 55
k-matching, 76
vertex, 55
Hyperplanerounding, 345, 347, 349, 352,
358, 365, 369
Ibarra, O.H., 33
ILP, see integer linear program
Inapproximability, 371, 405
(ρ ln n)-, 391
nc-, 396
Independent random rounding, 280
Independent set, 30, 289, 322, 364
in a rectangle intersection graph,
164
in an intersection disk graph, 161
maximal, 160, 224, 240–242
of disks, 136
Independent subset
in an independent system, 36
maximal, 36
Independent system, 36, 75, 76, 78
greedy algorithm, 80
Induced subgraph, 60
Inequality constraint
active, 276
Integer linear program, 246
Integer programming, 8
Integer quadratic program, 339
Interactive proof system, 405
INTERCONNECTING HIGHWAYS, 207,
209
Interior-point method, 251, 295
Intersection disk graph, 136
Intractable problem, 8, 14
Inward rotation, 358
Iterated patching procedure, 198
Iterated rounding, 272, 293
Iyengar, G., 369
Jain, K., 279, 296, 329, 336, 337
Java, 15
Jenkyns, T.A., 80
Jerrum, M., 369
Jiang, T., 164
Johnson, D.S., 33, 80, 121, 296, 375, 405
Johnson, N., 359
k-CENTERS, 399
k-MEDIAN, 337
k-SC, 31
k-SET COVER, 31; see also k-SC
k-SMT, 209
k-Steiner ratio, 89
k-TSP, 209
Kale, S., 370
Kamen, D., 245
Karger, D., 369
Karloff, H., 296
Karmarkar, N., 295
Karp, R.M., 33, 121, 164
Kececioglu, J.D., 122
Kelly, D.F., 80
Khachiyan, L.G., 295
Khanna, S., 164, 385, 405
Khuller, S., 80, 243, 244, 337, 405, 406
Kim, C.E., 33
Kim, J.-M., 209
Klee, V.L., 295
Klein, P., 244, 370
KNAPSACK, 2, 9, 17, 28, 29, 246
dynamic programming algorithm, 3
exact algorithm, 3
FPTAS, 27, 33
generalized greedy algorithm, 6,
289
greedy algorithm, 4, 247, 251, 289
polynomial tradeoff approximation
algorithm, 7
PTAS, 33
KNAPSACKD, 17, 20
nondeterministic algorithm, 18
Ko, K.-I., 15, 16, 405
Komolos, J., 164
Korte, B., 80
Kosaraju, S.R., 243
Kotz, S., 359
Krauthgamer, R., 399
L-reduction, 381–382, 405
LABEL COVER, 398, 405; see also LC
Laminar family, 276
Langberg, M., 369
LC, 398, 399
LC-GAP(α,k), 398

431
Lenstra, J.K., 295
Levcopoulos, C., 208
Lexicographical ordering method, 257,
295
Lexicographically less, 257
Lexicographically positive, 257
Li, D., 244
Li, H.L., 243
Li, Y., 244
Liestman, A.L., 243
Lifted alignment, 113, 120, 121
dynamic programming algorithm,
113
Lin, G.-H., 122
Linear approximation, 27
Linear program, 78, 245
nondegenerate, 250
residual, 274
standard form, 247
Linear programming, 5, 9, 11, 339
algorithms, 251
simplex method, 253
Lingas, A., 208
Literal, 20
Literal function, 29
Local ratio method, 11, 297, 315, 337
Local ratio theorem, 315
Local search, 10
log n, 3
Logic puzzle
and satisﬁability, 28
Loss(T), 97
loss(T), 97
Lov´asz, L., 80, 295, 369
LP, see linear program
Lu, B., 209
Lu, H.-I., 370
Lund, C., 80, 405
m-dark point
horizontal, 176, 179
one-sided, 208
vertical, 176, 179
m-guillotine cut, 175
boundary conditions, 175, 179
versus portal, 191
m-guillotine partition, 208
m-guillotine rectangular partition, 176
m-guillotine rectilinear Steiner tree, 179
dynamic programming algorithm,
182
Maass, W., 164
Mafﬁoli, F., 369
Mahajan, S., 349, 370
Mahdian, M., 337
MAJ-DS, 387, 405
APX-hard, 387
MAJORITY-DOMINATING SET, 387; see
also MAJ-DS
Majority-dominating set, 387
Makespan, 264
Mandoiu, I., 122
Map labeling, 164
Marginal distribution, 283
Matching, 43, 212
maximum, 212
Matrix
positive deﬁnite, 340
positive semideﬁnite, 340
symmetric, 339
Matroid, 40, 76
graph, 41
intersection, 41, 76
rank, 49
MAX-ASSIGN, 43
greedy algorithm, 44
MAX-BISEC, 359, 363, 365
semideﬁnite programming approxi-
mation, 360
MAX-CG, 404
MAX-CUT, 345, 347, 369, 401
linear programming-based approxi-
mation, 346
multivariate normal rounding, 358
semideﬁnite programming approxi-
mation, 347
MAX-DHC, 212
approximation algorithm, 213
MAX-DHP, 23, 30, 42, 212
greedy algorithm, 39, 44
with quadrilateral condition, 44
MAX-DIBISEC, 367
MAX-DICUT, 366
MAX-4SAT, 368
MAX-HC, 23, 30, 42, 212
greedy algorithm, 38
MAX-HP, 212
Index

432
Index
MAX-IR, 162
MAX-IS, 290, 396
MAX-ISS, 36, 40, 75
greedy algorithm, 36
MAX-k-CUT-HYPER, 290
MAX-k-UNCUT, 366
MAX-k-VC, 365, 366
MAX-kSAT, 368
MAX-(n/2)-DENSE-SUBGRAPH, 366
MAX-(n/2)-UNCUT, 366
MAX-(n/2)-VC, 366
MAX-NAE-SAT, 367
MAX-RES-CUT, 366
MAX-SAT, 280, 290, 293, 296, 405
NP-hard gap, 389
derandomization algorithm, 282
independent random rounding algo-
rithm, 280
MAX-SP, 397
MAX-3-COLOR, 401
MAX 3-CUT, 370
MAX-3DM, 43
MAX-3LIN, 378, 391
NP-hard gap, 378, 391
MAX-3SAT, 368, 379
NP-hard gap, 379
MAX-2SAT, 350, 354, 366, 369, 401
semideﬁnite programming approxi-
mation, 351, 355, 356
MAX-WH, 267, 286
pipage rounding algorithm, 269
MAX-WIS, 322, 336
local ratio algorithm, 323
on t-interval graphs, 337
MAX-WSAT, 294
Maximality property, 311
MAXIMUM ASSIGNMENT, 43; see also
MAX-ASSIGN
MAXIMUM BISECTION, 359; see also
MAX-BISEC
MAXIMUM BISECTION ON DIGRAPHS,
367; see also MAX-DIBISEC
MAXIMUM CONSTRAINT GRAPH, 404;
see also MAX-CG 404
MAXIMUM COVERAGE WITH KNAP-
SACK CONSTRAINTS, 291;
see also MAX-COVER-KC
MAXIMUM CUT IN A DIGRAPH, 366;
see also MAX-DICUT
MAXIMUM DIRECTED HAMILTONIAN
CIRCUIT, 212; see also MAX-
DHC
MAXIMUM DIRECTED HAMILTONIAN
PATH, 23; see also MAX-
DHP
MAXIMUM DISJOINT SET COVER, 403
Maximum-ﬂow minimum-cut theorem,
274
MAXIMUM HAMILTONIAN CIRCUIT, 23;
see also MAX-HC
MAXIMUM
INDEPENDENT
RECTAN-
GLES, 162; see also MAX-IR
MAXIMUM
INDEPENDENT
SET
IN
AN
INTERSECTION
DISK
GRAPH, 136; see also MIS-
IDG
MAXIMUM INDEPENDENT SUBSET, 36;
see also MAX-ISS
MAXIMUM k-CUT IN A HYPERGRAPH,
290; see also MAX-k-CUT-
HYPER
Maximum matching, 8, 212
MAXIMUM NOT-ALL-EQUAL SATISFI-
ABILITY, 367; see also MAX-
NAE-SAT
MAXIMUM RESTRICTED CUT, 366; see
also MAX-RES-CUT
MAXIMUM SATISFIABILITY, 280; see
also MAX-SAT
MAXIMUM SET PACKING, 397; see also
MAX-SP
MAXIMUM SPLITTING SET, 367
MAXIMUM 3-DIMENSIONAL MATCH-
ING, 43; see also MAX-3DM
MAXIMUM 3-LINEAR EQUATIONS,378;
see also MAX-3LIN
MAXIMUM-WEIGHT HITTING, 267; see
also MAX-WH
MAXIMUM-WEIGHT INDEPENDENT
SET, 322; see also MAX-WIS
MAXIMUM-WEIGHT SATISFIABILITY,
294; see also MAX-WSAT
MAXSNP, 385, 405
MAXSNP-complete problem, 389, 405
MAXSNP-completeness, 385
McDonald, J., 339
Melkonian, V., 296
Menotti, G. C., 1

433
METRIC FACILITY LOCATION, 337
METRIC-k-CENTERS, 374, 385, 399, 400
NP-hard gap, 375
METRIC-TSP, 401, 405
Miller, Z., 80
Min, M., 164, 244
MIN-CB, 65, 76
greedy algorithm, 65
MIN-CDS, 66, 68, 70, 73, 78, 80, 219,
385, 392
greedy algorithm, 71
two-stage greedy algorithm, 220
MIN-d-DS, 403
MIN- ¯d-SS, 402
MIN-d-SS, 403
MIN-EB, 108
MST approximation, 110
MIN-HS, 31
MIN-MR, 235, 243, 244
improved relaxation algorithm, 236
relaxation algorithm, 235
MIN-RP, 165, 166, 208, 209
1-guillotine rectangular partition
approximation, 173
m-guillotine rectangular partition
approximation, 177
hole-free, 166
MIN-RP1, 168, 205, 208
guillotine rectangular partition ap-
proximation, 168
MIN-SAT, 294
MIN-SC, 50, 68, 76, 80, 385, 391, 393,
395, 405
greedy algorithm, 51, 80
MIN-SMC, 54, 329, 337
greedy algorithm, 90, 117
with a nonlinear cost function, 77
MIN-2SAT, 260, 295
linear programming approximation,
261
MIN-VC, 30, 259, 295, 299, 379, 385,
405; see also MIN-VC-b
NP-hard gap, 380
MIN-VC-b, 381, 385
MIN-WCVC, 62, 64, 393
MIN-WHS, 55, 59
greedy algorithm, 55
MIN-WSC, 54, 59, 334
MIN-WVC, 60, 259, 299, 303, 315, 316,
332, 333
integer program, 300
linear programming approximation,
259
local ratio algorithm, 316, 317
primal-dual approximation, 301
Minimum assignment
canonical, 240
MINIMUM CONNECTED DOMINATING
SET, 66; see also MIN-CDS
MINIMUM CONVEX PARTITION, 191,
208
MINIMUM-COST BASE, 65; see also
MIN-CB
MINIMUM d-DISJUNCT SUBMATRIX,
403; see also MIN-d-DS
MINIMUM ¯d-SEPARABLE SUBMATRIX,
402; see also MIN- ¯d-SS
MINIMUM d-SEPARABLE SUBMATRIX,
403; see also MIN-d-SS
MINIMUM DIRECTED HAMILTONIAN
CIRCUIT, 212; see also DI-
RECTED TSP
MINIMUM EDGE-LENGTH RECTANGU-
LAR PARTITION,166; see also
MIN-RP, MIN-RP1
MINIMUM-ENERGY BROADCASTING,
108; see also MIN-EB
MINIMUM FEASIBLE CUT, 294
MINIMUM HITTING SET, 31; see also
MIN-HS
MINIMUM-LENGTH CONVEX PARTI-
TION, 206
Minimum perfect matching
algorithm, 25
Minimum s-t cut problem, 337
MINIMUM SET COVER, 50; see also
MIN-SC
Minimum spanning tree, 8, 24, 83, 102,
120, 212; see also MST
algorithm, 24
MINIMUM SUBMODULAR COVER, 54;
see also MIN-SMC
MINIMUM 2-SATISFIABILITY, 260; see
also MIN-2SAT
MINIMUM VERTEX COVER, 30; see also
MIN-VC
Index

434
Index
MINIMUM-WEIGHT CONNECTED VER-
TEX COVER, 62; see also
MIN-WCVC
MINIMUM-WEIGHT HITTING SET, 55;
see also MIN-WHS
MINIMUM-WEIGHT MULTICAST ROUT-
ING, 235; see also MIN-MR
MINIMUM-WEIGHT SET COVER, 54;
see also MIN-WSC
MINIMUM-WEIGHT VERTEX COVER,
60; see also MIN-WVC
Minty, G.J., 295
MIS-IDG, 136
PTAS, 141
Mitchell, J.S.B., 172, 209
Mitchell’s lemma, 172, 176, 180
Modular function, 49, 76
Monotone increasing function, 50, 53
MST, 83
mst(P), 83
MST(P : A), 92
mst(P : A), 92
Multicast routing, 235, 243
Multilayer partition, 136. 164
MULTIPLE SEQUENCE ALIGNMENT,
120; see also MSA
Multiquadratic program, 364
Multivariate normal rounding, 358, 360,
369
MULTIWAY CUT, 238; see also MWC
MWC, 238, 244
approximation algorithm, 238
N, 2
Negative correlation, 283
Nesterov, Y.E., 369
Network, 83, 222
NETWORK DESIGN, 310, 335
local ratio algorithm, 326
primal-dual schema, 311
Network design problem, 336
NETWORK STEINER MINIMUM TREE,
83; see also NSMT
Nielsen, F., 164
Node-deletion problem, 337
NODE WEIGHTED STEINER TREE, 402;
see also NWST
Nonadaptive partition, 123
Noncovering-type problem, 336
Nondegeneracy assumption, 250, 255,
289
Nondeterministic algorithm, 18
accepting the input, 18
computation paths, 18
nondeterministic move, 18
polynomial-time, 18
rejecting the input, 18
time complexity, 18
witness, 19
Nondeterministic Turing machine, 18
Nonsplitting node, 235
Nonsubmodular potential function, 66
NOT-ALL-EQUAL 3-SAT, 32
NP, 18, 388
NP-complete problem, 17, 20, 372
NP-completeness, 19, 33
NP-hard gap, 372
NP-hard problem, 20, 371
NPO, 384, 400
NSMT, 83, 95, 100, 102, 116, 121, 235,
385, 405
greedy algorithm, 97, 116
MST approximation, 83
Robin–Zelikovsky algorithm, 98
Objective function, 9, 245
1-dark point
horizontal, 169
vertical, 169
1-guillotine cut, 171, 209
boundary conditions, 172
1-guillotine rectangular partition, 171
dynamic programming algorithm,
172
ONE-IN-THREE 3-SAT, 32
( 1
3, 2
3)-guillotine rectilinear Steiner tree,
188
( 1
3, 2
3)-partition, 186
binary tree structure, 186
( 1
3, 2
3)-restricted guillotine cut, 186
Open boundary segment, 171
Opt, 2
opt, 2
Opt(I), 2
opt(I), 2
Optical network, 235
Optimal cut, 104

435
Optimal routing tree
dynamic programming, 208
Optimization problem, 9, 245
Orphan, 230
head, 230
Outward rotation, 358, 369
ov(s,t), 46
Overlap graph, 46
P, 16
versus NP, 19, 371
P(a, b)-restricted rectilinear Steiner tree,
195, 201
dynamic programming algorithm,
202
p-portal, 184, 187
(p1, p2)-portal, 201
Packing function, 368
Packing problems, 164
Packing semideﬁnite program, 368
Pan, L.Q., 208, 209
Papadimitriou, C., 385, 405
Pardalos, P.M., 370
PARTIAL VERTEX COVER, 318; see also
PVC
Partition, 10, 123, see also double parti-
tion, multilayer partition, tree
partition
adaptive, 123, 165, 192, 208
into hexagonal cells, 162
nonadaptive, 123
PARTITION, 22
Pascal, 15
Patching, 196, 198, 209
iterated, 198
PCP system, 389, 405
PCP theorem, 378, 388, 389, 401, 405;
see also H˚astad’s 3-bit PCP
theorem
Perfect matching, 25; see also minimum
perfect matching
Performance ratio, 4, 9, 23
Period, 217
Perturbation method, 295
Phylogenetic alignment tree, 158
t-restricted, 158
dynamic programming algorithm,
160, 163
PHYLOGENETIC TREE ALIGNMENT,
113; see also PTA
Phylogenetic tree alignment, 122
Pigeonhole principle, 51, 52
Pipage rounding, 267, 271, 290, 296
random, 282
Pitt, L., 295
Pivot, 253, 254
PLANAR-CVC-4, 375
PLANAR 3-SAT, 32
Plate, 363
Pollak, H.O., 121
Polygonal partition problem, 208
Polyhedron, 246, 340
Polymatroid, 54, 77, 78
dual, 78
Polymatroid function, 54, 93, 117
Polynomial-time algorithm, 4
pseudo, 4
Polynomial-time approximation scheme,
27; see also PTAS
fully, 27; see also FPTAS
Polynomial-time computability, 14
Polynomial-time reduction, 19, 371, 372
generic, 20
Portal, 184, see also two-stage portal
active, 187
endpoint, 194
interior, 194
Positive semideﬁnite matrix, 340, 363
Potential function, 35
maximal sets under, 54
monotone increasing, 50
nonsubmodular, 66
submodular, 49
Primal complementary slackness condi-
tion, 299, 302, 329, 336
Primal-dual approximation, 336
Primal-dual method, 336
Primal-dual schema, 11, 297, 303
equivalencewith local ratio method,
325, 337
in semideﬁnite programming, 370
Primal linear program, 298
Prisner, E., 80
PRIZE COLLECTING VERTEX COVER,
334
Probabilistically checkable proof system,
389; see also PCP system
Index

436
Index
Proof system, 388
Prover, 389
Pseudo-polynomial-time algorithm, 4
Pseudocode, 3, 15
Pseudospider, 231
legal, 231
PTA, 113, 121, 157
approximation, 160
lifted alignment approximation, 115
PTAS, 27, 382
PVC, 318, 335
Quadratic program, 339
Quadratic programming, 346
Quadrilateral condition, 43
Quadtree partition, 192, 207
Quaternary tree structure, 193
R, 2
R+, 2
Raghavan, P., 209
Ramana, M., 370
Ramesh, H., 349, 370
Random normal vector, 347
Random pipage rounding, 282, 286
Random rounding, 280, 370
independent, 280
Rank, 40
of a graph matroid, 61
of a matrix, 248
of a matroid, 49, 65, 77
Rao, S.B., 205, 209
Ravi, R., 122, 244
Rawitz, D., 337
Raz, R., 405
Rectangular partition
dynamic programming, 205
RECTILINEAR
STEINER
ARBORES-
CENCE, 191, 206, 209
RECTILINEAR
STEINER
MINIMUM
TREE, 82; see also RSMT
RECTILINEAR
STEINER
MINIMUM
TREE
WITH
RECTILINEAR
OBSTRUCTION, 161; see also
RSMTRO
Regular point, 82
Relaxation, 10, 211
to a linear program, 259
versus restriction, 238
Residual linear program, 274
Resolution method, 13
Resource allocation and scheduling prob-
lem, 337
Resource management problem, 2, 247,
250, 251, 289
PTAS, 251
Restriction, 10, 81, 211, 238
Robbins, H., 121
Robin, G., 97, 122
Robin–Zelikovsky algorithm, 98
Root
of a string, 216
Root-leaf path, 107
Rotation, see vector rotation
Rounding, 259, 345; see also combi-
natorial rounding, geometric
rounding, hyperplane round-
ing, pipage rounding, multi-
variate normal rounding, ran-
dom rounding, vector round-
ing
of solution, 11
RSMT, 82, 122, 178, 184, 201, 204, 206,
209
m-guillotine rectilinear Steiner tree
approximation, 182
( 1
3, 2
3)-guillotine rectilinear Steiner
tree approximation, 190
RSMT WITH OBSTRUCTIONS, 207
RSMTRO, 161
Ruan, L., 80, 209, 243
Rubinstein, J.H., 121
Safra, S., 405
Sahni, S., 33, 405
SAT, 13, 20, 389
nondeterministic algorithm, 19
SATISFIABILITY, 13; see also SAT
Satisﬁability problem, 369
SC, 22
SCDS, 228, 235, 243, 244
SCHEDULE-PM, 356, 367
hyperplane rounding, 358
vector rotation, 358
SCHEDULE-UPM, 295
SCHEDULING ON PARALLEL MA-
CHINES, 356; see also
SCHEDULE-PM

437
SCHEDULING ON UNRELATED PARAL-
LEL MACHINES,264; see also
SCHEDULE-UPM
Scheduling problem, 8, 369
Schreiber, P., 121
Schumacher, 121
Score
between two strings, 110
of an alignment, 111
SELECTED-INTERNAL STEINER TREE,
119; see also SIST
Semideﬁnite constraints, 339
Semideﬁnite program, 341,see also pack-
ing semideﬁnite program
dual program, 342
standard form, 341
Semideﬁnite programming, 339, 369
complex, 370
polynomial-time computability, 345,
369
Semideﬁnite programming relaxation,
339, 346, 365, 369
Separation oracle, 273
Set cover
connected
in a hypergraph, 79
SET COVER, 22; see also SC
Set cover problem
weighted, 336
sgn(x), 359
Shifting technique, 126, 155, 164, 193
Shing, M.-T., 164, 208, 209
Shortest path, 8
SHORTEST SUPERSTRING, 46; see also
SS
Simplex method, 251, 252, 290, 295
Simplex table, 254
SIST, 119
Sivakumar, R., 243
Skutella, M., 369
Slavik, P., 80
Smith, W.D., 205, 209
SMT, 82
Euclidean, 82
k-restricted
greedy algorithm, 92
k-restricted SMT approximation, 89
n-dimensional Euclidean, 115
network, 83
rectilinear, 82, 115
smtk(P), 89
smt(P), 83
Social network, 387
Span(L), 277
Spanner, 205
Spanning arborescence, 228
Spanning tree, 83, see also arborescence
spanning tree
minimum, see MST
Steinerized, 103
Spectrahedron, 340, 363
intersection, 341
Spherical trigonometry, 353
Spider, 230
legal, 230
Spider decomposition, 233, 244
Splitting node, 235
SS, 46, 76, 215, 219, 240, 243
and MAX-DHP, 49
greedy algorithm, 47
ST-MSP, 102, 120
Steinerized spanning tree approxi-
mation, 104
Stair, 205
Star, 222
Stein, C., 243
STEINER ARBORESCENCE, 209
STEINER FOREST, 310, 312, 314, 315
STEINER MINIMUM TREE, 30; see also
SMT
Steiner minimum tree, 82; see also SMT
k-restricted, 86
Steiner point, 82
Steiner ratio, 86, 116
in Euclidean plane, 86
in rectilinear plane, 86
Steiner tree, 82
acyclic directed, 122
bottleneck, 122
full component, 82
full tree, 82
k-restricted, 86
loss, 97
selected-internal, 119, 122
vertex-weighted, 242, 244
union, 95
with the minimum number of Stei-
ner points, 122
Index

438
Index
Steiner tree problem, 121
STEINER TREES WITH MINIMUM STEI-
NER POINTS, 102; see also
ST-MSP
Steiner vertex, 82
Steinerized spanning tree, 103
minimum, 103
optimal cut algorithm, 104
Stojmenovic, I., 243
String, 46
overlap, 46
preﬁx, 46
substring, 46
sufﬁx, 46
superstring, 46
STRONGLY CONNECTED DOMINATING
SET, 228; see also SCDS
Submodular function, 49, 52, 53, 62, 76,
78, 80, 92, 117, 291
ground set, 49
normalized, 54
strongly, 292
subject to matroid constraints, 296
Submodularity, 52
Substring, 46
Superstring, 46
minimal, 215
Supmodular function, 68, 223
weakly, 274, 292
Sviridenko, M., 296
Symmetric function, 314
Symmetric matrices, 339
SYMMETRIC RECTILINEAR STEINER
ARBORESCENCE, 191, 206,
209
SYMMETRIC STEINER ARBORESCENCE,
209
System of linear inequalities, 273
t-interval system, 335
Tardos, E., 296
Tarhio, J, 47
Teng, S.-H., 243
Teo, C.P., 336
Terminal, 82
TERMINAL STEINER TREE, 118; see
also TST
Terminal Steiner tree, 122
Thomas, D.A., 121
3-CNF, 20
3-DIMENSIONAL RSMT, 207
3GCOLOR, 374
NP-hard gap, 374
3-SAT, 20, 390
Threshold rounding, 260, 272
Time complexity, 15
bit-operation measure, 16
logarithmic cost measure, 15
nondeterministic algorithm, 18
pseudocode, 15
Turing machine, 16
Tournament, 335
Tractable problem, 8, 16
Tradeoff
between running time and perfor-
mance ratio, 5, 9
Traveling salesman problem, 8
TRAVELING SALESMAN PROBLEM, 23;
see also TSP
Tree alignment problem, 164
TREE PARTITION, 310, 312, 314
Tree partition, 157, 164
Tree structure
of quadtree partition, 196
Triangle inequality, 24, 76
Triplett, G., 81
TSP, 23, 24, 27, 30, 33, 76, 212, 235, 371,
372, 385
Euclidean, 26, 163
with triangle inequality
approximation algorithm, 24
Christoﬁdes’s algorithm, 25
TST, 118
Turing machine, 15, 16
nondeterministic, 18
time complexity
bit-operation, 16
Turner, J.S., 47, 49
2-CNF, 260
2-SAT, 262
polynomial-time algorithm, 262
Two-stage greedy approximation, 219
Two-stage portal, 201, 209
UDC, 124
partition algorithm, 124
UDC1, 128
Ukkonen, E., 47

439
Unit ball, 241
Unit ball graph, 164, 241
Unit disk, 123, 160, 240
UNIT DISK COVERING WITH RESTRIC-
TED LOCATIONS, 128; see
also UDC1
Unit disk graph, 129, 136, 162, 224, 240–
242
van Leeuwen, E.J., 405
Vavasis, S.A., 164
Vazirani, V., 329, 336
VC, 22
VC-CG, 385, 405
Vector program, 342
Vector rotation, 352, 358, 363, 367, 369;
see also outward rotation, in-
ward rotation
Vector rotation technique, 369
Vector rounding, 287, 296
Vector swapping, 360
Veriﬁer, 389
Vertex
in a hypergraph, 55
in a polyhedron, 248
of a feasible region, 248, 249
Vertex coloring, 373
Vertex cover, 30, 33
connected, 62, 63, 160
in a unit disk graph, 160
in an intersection disk graph, 161
VERTEX COVER, 22; see also VC
VERTEX COVER IN CUBIC GRAPHS,
385; see also VC-CG
VERTEX-WEIGHTED ST, 163
Violated set, 311
minimal, 311
Virtual backbone, 243
Wan, P.-J., 227, 243, 244
Wang, F., 406
Wang, L., 122, 164
Wang, W., 337
Wavelength-division multiplexing optical
network, 102
WCDS-UDG, 156
WCDS-UDG1, 157
WDM, 102
WDS-UDG, 142, 155, 161,162
on a large cell, 150
approximation algorithm, 153
on a small cell, 146
WDS-UDG1, 146
approximation algorithm, 150
Weight decomposition, 329
counting argument, 56
WEIGHTED CONNECTED DOMINATING
SET IN A UNIT DISK GRAPH,
157; see also WCDS-UDG
WEIGHTED DOMINATING SET
IN
A
UNIT DISK GRAPH, 142; see
also WDS-UDG
WEIGHTED
SUBSET
INTERCONNEC-
TION DESIGN, 60; see also
WSID
WEIGHTED UNIT DISK COVERING,143;
see also WUDC
Wesolowsky, G., 121
Williamson, D.P., 296, 336, 369, 370
Window, 169, 186
minimal, 179, 186
Wireless network, 108, 242
Wireless sensor network, 224, 243
Wolkowicz, H., 370
Wolsey, L.A., 80, 295, 296, 337
WSID, 60, 62, 80, 402
Wu, J., 243
Wu, W., 80, 405, 406
WUDC, 143
dynamic programming algorithm,
143
Xu, K.-J., 208
Xue, G., 122
Yabuta, T., 337
Yan, S., 244
Yang, H., 369
Yang, S.-C., 122
Yannakakis, M., 80, 296, 385, 405
Yao, F.F., 243
Ye, Y., 289, 296, 369
Yu, C., 164
Z, 2
Z+, 2
Zelikovsky, A., 97, 121, 122
Zhang, H., 369
Zhang, J., 289, 296, 369
Index

440
Index
Zhang, Y., 122, 164
Zhang, Z., 164, 406
Zhao, Q., 370
Zheng, S.Q., 209
Zhu, X., 405
Zou, F., 164
ZPP, 405
Zuckerman, D., 405
Zwick, U., 296, 355, 358, 369


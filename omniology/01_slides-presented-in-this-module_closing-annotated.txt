Machine Learning Specializa0on 
Recap &  
Look ahead 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
1 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
What we’ve learned 
©2016 Emily Fox & Carlos Guestrin 
2 

Machine Learning Specializa0on 
Module 1: Nearest neighbor search 
©2016 Emily Fox & Carlos Guestrin 
3 

Machine Learning Specializa0on 
4 
1-NN search 
©2016 Emily Fox & Carlos Guestrin  
Space of all articles, 
organized by similarity of text 
 
query article 
nearest neighbor 

Machine Learning Specializa0on 
5 
k-NN search 
©2016 Emily Fox & Carlos Guestrin  
Space of all articles, 
organized by similarity of text 
 
query article 
set of nearest neighbors 

Machine Learning Specializa0on 
6 
TF-IDF document representation 
Emphasizes important words 
 
- Appears frequently in document  (common locally) 
- Appears rarely in corpus (rare globally) 
Trade oﬀ: local frequency vs. global rarity 
©2016 Emily Fox & Carlos Guestrin  
Term frequency = 
Inverse doc freq. = log
# docs
1 + # docs using word
word counts 
tf * idf 

Machine Learning Specializa0on 
7 
Scaled Euclidean distance 
 distance(xi, xq) = 
             a1(xi[1]-xq[1])2 + … + ad(xi[d]-xq[d])2 
©2016 Emily Fox & Carlos Guestrin  
p
weight on each feature 
title  
abstract 
main body 
conclusion 
 

Machine Learning Specializa0on 
8 
Cosine similarity – normalize 
 
 
 
 
 
 
 
©2016 Emily Fox & Carlos Guestrin  
Similarity =         xi[j] xq[j] 
 
 
 
 
        =   xi
T
 xq  
  = cos(θ)  
d
X
j=1
(xi[j])2
         (xq[j])2 
d
X
j=1
d
X
j=1
||xi|| ||xq|| 
Feature 1 
Feature 2 
-  Not a proper 
distance 
metric 
-  Eﬃcient to 
compute for 
sparse vecs 
p
p
θ 

Machine Learning Specializa0on 
9 
To normalize or not? 
©2016 Emily Fox & Carlos Guestrin  
long document 
short tweet 
long document 
long document 
Normalizing can 
make dissimilar 
objects appear 
more similar 
Common 
compromise: 
Just cap maximum 
word counts 

Machine Learning Specializa0on 
10 
Complexity of brute-force search 
Given a query point, scan through each point 
- O(N) distance computations per 1-NN query! 
- O(Nlogk) per k-NN query! 
 
 
 
 
©2016 Emily Fox & Carlos Guestrin  
What if N is huge??? 
(and many queries) 

Machine Learning Specializa0on 
11 
KD-trees 
©2016 Emily Fox & Carlos Guestrin  
x[1] 
>.5 
Recursively partition the 
feature space 
Split dim 1 
Split value 2 
x[2] 
>.1 
Split dim 2 
Split value 2 
YES 
NO 
NO 
Pt 
x[1] 
x[2] 
2 
1.00 
4.31 
… 
… 
… 
YES 
Pt 
x[1] 
x[2] 
1 
0.00 0.00 
… 
… 
… 
Pt 
x[1] 
x[2] 
3 
0.13 
2.85 
… 
… 
… 

Machine Learning Specializa0on 
12 
Nearest neighbor with KD-trees 
©2016 Emily Fox & Carlos Guestrin  
1. Start by exploring leaf node containing query point 
2. Compute distance to each other point at leaf node 
3. Backtrack and try other branch at each node visited 
 
Update distance bound when new 
nearest neighbor is found 

Machine Learning Specializa0on 
13 
Nearest neighbor with KD-trees 
©2016 Emily Fox & Carlos Guestrin  
Use distance bound and bounding box of each node to 
prune parts of tree that cannot include nearest neighbor 

Machine Learning Specializa0on 
14 
Approximate k-NN with KD-trees 
©2016 Emily Fox & Carlos Guestrin  
Before: Prune when distance to bounding box > r 
Now: Prune when distance to bounding box > r/α 
 
Saves lots of search time at little cost in quality of NN! 

Machine Learning Specializa0on 
15 
Limitations of KD-trees 
©2016 Emily Fox & Carlos Guestrin  
•  Diﬃcult to implement 
•  Don’t tend to perform 
well in high dimensions 
•  Under some conditions, 
visit at least 2d nodes 

Machine Learning Specializa0on 
16 
Locality sensitive hashing 
©2016 Emily Fox & Carlos Guestrin  
#awesome 
#awful 
0 
1 
2 
3 
4 
… 
0 
1 
2 
3 
4 
… 
Line 1 
Line 2 
Line 3 
Bin index:  
[0 0 0] 
Bin index:  
[0 1 0] 
Bin index:  
[1 1 0] 
Bin index:  
[1 1 1] 

Machine Learning Specializa0on 
17 
LSH for approximate NN search 
©2016 Emily Fox & Carlos Guestrin  
Bin 
[0 0 0] 
= 0 
[0 0 1] 
= 1 
[0 1 0] 
= 2 
[0 1 1] 
= 3 
[1 0 0] 
= 4 
[1 0 1] 
= 5 
[1 1 0] 
= 6 
[1 1 1] 
= 7 
Data 
indices:  
{1,2} 
-- 
{4,8,11} 
-- 
-- 
-- 
{7,9,10} {3,5,6} 
#awesome 
#awful 
0 
1 
2 
3 
4 
… 
0 
1 
2 
3 
4 
… 
Line 1 
Line 2 
Line 3 
Bin index:  
[0 0 0] 
Bin index:  
[0 1 0] 
Bin index:  
[1 1 0] 
Bin index:  
[1 1 1] 
Next closest 
bins (ﬂip 1 bit) 
Query point here,  
but is NN? 

Machine Learning Specializa0on 
Module 2: k-means and MapReduce 
18 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
19 
Module 2: k-means and MapReduce 
©2016 Emily Fox & Carlos Guestrin 
Cluster 1 
Cluster 2 
Cluster 3 
Cluster 4 
Discover clusters of related documents 
 

Machine Learning Specializa0on 
20 
k-means algorithm  
0.  Initialize cluster centers 
1.  Assign observations to 
closest cluster center 
2.  Revise cluster centers 
as mean of assigned 
observations  
3.  Repeat 1.+2. until 
convergence 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
21 
A coordinate descent algorithm 
1.  Assign observations to closest cluster center 
2.  Revise cluster centers as mean of assigned 
observations  
3.  Revise cluster centers as mean of assigned 
observations  
4.  Repeat 1.+2. until convergence 
©2016 Emily Fox & Carlos Guestrin 
zi  arg min
j
||µj −xi||2
2
µj  arg min
µ
X
i:zi=j
||µ −xi||2
2
Alternating minimization 
1. (z given µ)   and   2. (µ given z)  
= coordinate descent 

Machine Learning Specializa0on 
22 
Convergence of k-means to local mode 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
23 
MapReduce framework 
©2016 Emily Fox & Carlos Guestrin 
Map Phase 
(k1,v1) 
(k2,v2) 
… 
(k1’,v1’) 
(k2’,v2’) 
… 
(k1’’’,v1’’’) 
(k2’’’,v2’’’) 
… 
Split data  
across machines 
Reduce Phase 
Shuﬄe Phase 
(k1,v1) 
(k2,v2) 
… 
(k3,v3) 
(k4,v4) 
… 
(k5,v5) 
(k6,v6) 
… 
Assign tuple (ki,vi) to  
machine  h[ki] 
M1 
M2 
M1000 
M1 
M2 
M1000 
Big Data 
Parallelize 
operations 
across 
multiple 
machines 

Machine Learning Specializa0on 
24 
MapReduce abstraction 
 Map:  
-  Data-parallel over elements,  
e.g., documents 
-  Generate (key,value) pairs 
•  “value” can be any data type 
 
Reduce: 
-  Aggregate values for each key 
-  Must be commutative-associative  
operation 
-  Data-parallel over keys 
-  Generate (key,value) pairs 
 
 
MapReduce has long history in functional programming 
- 
Popularized by Google, and subsequently by open-source Hadoop implementation from Yahoo! 
©2016 Emily Fox & Carlos Guestrin 
Word count example: 
 
map(doc) 
 for word in doc 
 
 emit(word,1) 
reduce(word, counts_list) 
 c = 0
  
 for i in counts_list 
 
 c += counts_list[i] 
 emit(word, c) 

Machine Learning Specializa0on 
25 
MapReducing 1 iteration of k-means 
Classify: Assign observations to closest cluster center 
 
 
Recenter: Revise cluster centers as mean of assigned 
observations  
1. 
Revise cluster centers as mean of assigned 
observations  
2.  Repeat 1.+2. until convergence 
©2016 Emily Fox & Carlos Guestrin 
zi  arg min
j
||µj −xi||2
2
µj = 1
nj
X
i:zi=k
xi
Map: For each data point, given ({µj},xi), emit(zi,xi) 
Reduce: Average over all points in cluster j (zi=k) 

Machine Learning Specializa0on 
Module 3: Mixture models 
26 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Mixture models 
©2016 Emily Fox & Carlos Guestrin 
Cluster 1 
Cluster 3 
27 
Cluster 4 
Cluster 2 
Probabilistic clustering model 
captures 
uncertainty 
in clustering 

Machine Learning Specializa0on 
Failure modes of k-means 
28 
©2016 Emily Fox & Carlos Guestrin 
disparate cluster sizes 
overlapping clusters 
diﬀerent shaped/
oriented clusters 

Machine Learning Specializa0on 
29 
Jumble of unlabeled images 
©2016 Emily Fox & Carlos Guestrin 
blue 

Machine Learning Specializa0on 
30 
Model of jumble of unlabeled images 
©2016 Emily Fox & Carlos Guestrin 
blue 
0.3 
blue 
0.8 
blue 
0.42 

Machine Learning Specializa0on 
31 
Mixture of Gaussians (1D) 
Each mixture component represents  
a unique cluster speciﬁed by: 
  
  
  
 
 
 
 
 
 {πk , μk , σk } 
 
 
©2016 Emily Fox & Carlos Guestrin 
x 
π2 
π1 
π3 
μ2 
μ1 
μ3 
σ2 
σ1 
σ3 
2 

Machine Learning Specializa0on 
32 
Mixture of Gaussians for 
clustering documents 
©2016 Emily Fox & Carlos Guestrin 
Space of all documents 
(really lives in RV for vocab size V) 
Make soft assignments 
of docs to each 
Gaussian 

Machine Learning Specializa0on 
33 
Restricting to diagonal covariance 
©2016 Emily Fox & Carlos Guestrin 
Each cluster has {πk , μk , Σk diagonal } 
 
Σ = 
 
 σ1
2
  
    σ2
2 
     
  σ3
2 
  
 
 
  
 
     
 
 σV
2 
V params 
0 
0 

Machine Learning Specializa0on 
34 
Inferring cluster labels 
Data 
  
 
 
 
 
 
  
©2016 Emily Fox & Carlos Guestrin 
EM algorithm à 
soft assignments 

Machine Learning Specializa0on 
35 
Expectation maximization (EM): 
An iterative algorithm 
1.  E-step: estimate cluster responsibilities 
given current parameter estimates 
2.  M-step: maximize likelihood over 
parameters given current responsibilities 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
36 
EM for mixtures of Gaussians 
in pictures - replay 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
37 
Relationship to k-means 
Consider Gaussian mixture model with 
 
 
 
 
 
 
and let the variance parameter σ à 0 
©2016 Emily Fox & Carlos Guestrin 
Σ = 
 
Spherically  
symmetric clusters 
Datapoint gets fully assigned to 
nearest center, just as in k-means 
 σ2
  
    σ2 
    σ2 
   
 
  
 
 σ2 
 

Machine Learning Specializa0on 
Module 4: Latent Dirichlet allocation 
38 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
39 
©2016 Emily Fox & Carlos Guestrin 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
SCIENCE 
experiment 
0.1 
test 
0.08 
discover 
0.05 
hypothesize 0.03 
climate 
0.01 
… 
… 
TECH 
develop 
0.18 
computer 
0.09 
processor 
0.032 
user 
0.027 
internet 
0.02 
… 
… 
SPORTS 
player 
0.15 
score 
0.07 
team 
0.06 
goal 
0.03 
injury 
0.01 
… 
… 
Topic vocab 
distributions: 
… 
Clustering: 
One topic indicator 
zi per document i 
All words come from 
(get scored under) 
same topic zi 
 
Distribution on 
prevalence of  
topics in corpus 
π = [π1  π2 … πK] 

Machine Learning Specializa0on 
40 
Comparing and contrasting 
Previously 
©2016 Emily Fox & Carlos Guestrin 
SCIENCE 
experiment 
0.1 
test 
0.08 
discover 
0.05 
hypothesize 0.03 
climate 
0.01 
… 
… 
TECH 
develop 
0.18 
computer 0.09 
processor 0.032 
user 
0.027 
internet 
0.02 
… 
… 
SPORTS 
player 
0.15 
score 
0.07 
team 
0.06 
goal 
0.03 
injury 
0.01 
… 
… 
Now 
Prior topic 
probabilities p(zi = k) = ⇡k
p(zi = k) = ⇡k
Likelihood 
under 
each topic 
… 
tf-idf vector 
compute likelihood of tf-idf 
vector under each Gaussian 
{modeling, complex, epilepsy, 
 modeling, Bayesian, clinical, 
 epilepsy, EEG, data, dynamic…}  
compute likelihood of the 
collection of words in doc 
under each topic distribution 

Machine Learning Specializa0on 
41 
©2016 Emily Fox & Carlos Guestrin 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
SCIENCE 
experiment 
0.1 
test 
0.08 
discover 
0.05 
hypothesize 0.03 
climate 
0.01 
… 
… 
TECH 
develop 
0.18 
computer 
0.09 
processor 
0.032 
user 
0.027 
internet 
0.02 
… 
… 
SPORTS 
player 
0.15 
score 
0.07 
team 
0.06 
goal 
0.03 
injury 
0.01 
… 
… 
Same topic 
distributions: 
… 
In LDA: 
One topic indicator 
ziw per word in doc i 
Each word scored 
under topic ziw 
 
Distribution on  
topics in document 
πi = [πi1  πi2 … πiK] 
0 
1 

Machine Learning Specializa0on 
42 
©2016 Emily Fox & Carlos Guestrin 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
TOPIC 1 
Word 1 
? 
Word 2 
? 
Word 3 
? 
Word 4 
? 
Word 5 
? 
… 
… 
TOPIC 2 
Word 1 
? 
Word 2 
? 
Word 3 
? 
Word 4 
? 
Word 5 
? 
… 
… 
TOPIC 3 
Word 1 
? 
Word 2 
? 
Word 3 
? 
Word 4 
? 
Word 5 
? 
… 
… 
Topic vocab 
distributions: 
… 
Document topic 
proportions: 
πi = [πi1  πi2 … πiK] 
0 
0.2 
0.4 
0.6 
? ? ? ? 

Machine Learning Specializa0on 
43 
Gibbs sampling for LDA 
©2016 Emily Fox & Carlos Guestrin 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
TOPIC 1 
experiment 
0.1 
test 
0.08 
discover 
0.05 
hypothesize 0.03 
climate 
0.01 
… 
… 
TOPIC 2 
develop 
0.18 
computer 
0.09 
processor 
0.032 
user 
0.027 
internet 
0.02 
… 
… 
TOPIC 3 
player 
0.15 
score 
0.07 
team 
0.06 
goal 
0.03 
injury 
0.01 
… 
… 
… 
0 
0.2 
0.4 
0.6 
Step 1: Randomly 
reassign all ziw based on 
-  doc topic proportions 
-  topic vocab distributions 
Draw randomly from 
responsibility vector 
[riw1 riw2 … riwK] 

Machine Learning Specializa0on 
44 
Gibbs sampling for LDA 
©2016 Emily Fox & Carlos Guestrin 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
TOPIC 1 
experiment 
0.1 
test 
0.08 
discover 
0.05 
hypothesize 0.03 
climate 
0.01 
… 
… 
TOPIC 2 
develop 
0.18 
computer 
0.09 
processor 
0.032 
user 
0.027 
internet 
0.02 
… 
… 
TOPIC 3 
player 
0.15 
score 
0.07 
team 
0.06 
goal 
0.03 
injury 
0.01 
… 
… 
… 
Step 2: Randomly 
reassign doc topic 
proportions based on 
assignments ziw in  
current doc 
 
Step 3: Repeat for all docs 
0 
0.2 
0.4 
0.6 
? ? ? ? 

Machine Learning Specializa0on 
45 
Gibbs sampling for LDA 
©2016 Emily Fox & Carlos Guestrin 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
… 
Step 4: Randomly 
reassign topic vocab 
distributions based on 
assignments ziw in  
entire corpus 
0 
0.2 
0.4 
0.6 
TOPIC 1 
Word 1 
? 
Word 2 
? 
Word 3 
? 
Word 4 
? 
Word 5 
? 
… 
… 
TOPIC 2 
Word 1 
? 
Word 2 
? 
Word 3 
? 
Word 4 
? 
Word 5 
? 
… 
… 
TOPIC 3 
Word 1 
? 
Word 2 
? 
Word 3 
? 
Word 4 
? 
Word 5 
? 
… 
… 

Machine Learning Specializa0on 
46 
Collapsed Gibbs sampling for LDA 
©2016 Emily Fox & Carlos Guestrin 
Modeling the Complex Dynamics and Changing
Correlations of Epileptic Events
Drausin F. Wulsina, Emily B. Foxc, Brian Litta,b
aDepartment of Bioengineering, University of Pennsylvania, Philadelphia, PA
bDepartment of Neurology, University of Pennsylvania, Philadelphia, PA
cDepartment of Statistics, University of Washington, Seattle, WA
Abstract
Patients with epilepsy can manifest short, sub-clinical epileptic “bursts” in
addition to full-blown clinical seizures. We believe the relationship between
these two classes of events—something not previously studied quantitatively—
could yield important insights into the nature and intrinsic dynamics of
seizures.
A goal of our work is to parse these complex epileptic events
into distinct dynamic regimes. A challenge posed by the intracranial EEG
(iEEG) data we study is the fact that the number and placement of electrodes
can vary between patients. We develop a Bayesian nonparametric Markov
switching process that allows for (i) shared dynamic regimes between a vari-
able number of channels, (ii) asynchronous regime-switching, and (iii) an
unknown dictionary of dynamic regimes. We encode a sparse and changing
set of dependencies between the channels using a Markov-switching Gaussian
graphical model for the innovations process driving the channel dynamics and
demonstrate the importance of this model in parsing and out-of-sample pre-
dictions of iEEG data. We show that our model produces intuitive state
assignments that can help automate clinical analysis of seizures and enable
the comparison of sub-clinical bursts and full clinical seizures.
Keywords:
Bayesian nonparametric, EEG, factorial hidden Markov model,
graphical model, time series
1. Introduction
Despite over three decades of research, we still have very little idea of
what deﬁnes a seizure. This ignorance stems both from the complexity of
epilepsy as a disease and a paucity of quantitative tools that are ﬂexible
TOPIC 1 
experiment 
0.1 
test 
0.08 
discover 
0.05 
hypothesize 0.03 
climate 
0.01 
… 
… 
TOPIC 2 
develop 
0.18 
computer 
0.09 
processor 
0.032 
user 
0.027 
internet 
0.02 
… 
… 
TOPIC 3 
player 
0.15 
score 
0.07 
team 
0.06 
goal 
0.03 
injury 
0.01 
… 
… 
… 
0 
0.2 
0.4 
0.6 
Randomly reassign ziw 
based on current 
assignments zjv of all 
other words in doc and 
corpus 

Machine Learning Specializa0on 
Collapsed conditional distribution 
47 
©2016 Emily Fox & Carlos Guestrin 
3 
? 
1 
3 
1 
epilepsy dynamic Bayesian 
EEG 
model 
Topic 1 
Topic 2 
Topic 3 
How much 
topic likes  
word 
How much  
doc likes  
topic 
Probability of assignment of word 
in doc i to topic k proportional to: 

Machine Learning Specializa0on 
48 
What to do with sampling output? 
Predictions: 
1.  Make prediction for each snapshot of randomly 
assigned variables/parameters (full iteration) 
2.  Average predictions for ﬁnal result 
Parameter or assignment estimate: 
-  Look at snapshot of randomly assigned  
variables/parameters that maximizes  
“joint model probability” 
©2016 Emily Fox & Carlos Guestrin 
iterations 
Joint model  
probability 

Machine Learning Specializa0on 
49 
Summary of what we learned 
Models 
Nearest neighbors 
Module 1 
Clustering 
Module 2, 3 
Mixture of Gaussians 
Module 3 
Latent Dirichlet 
allocation 
Module 4 
Algorithms 
KD-trees 
Module 1 
Locality sensitive 
hashing 
Module 1 
k-means 
Module 2 
MapReduce 
Module 2 
Expectation 
Maximization 
Module 3 
Gibbs sampling 
Module 4 
Core ML 
Distance metrics 
Module 1 
Approximation 
algorithms 
Module 1 
Unsupervised 
learning 
Module 2 
Probabilistic 
modeling 
Module 2, 3, 4 
Data parallel 
problems 
Module 2 
Bayesian inference 
Module 4 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Bonus content:  
Hierarchical clustering 
©2016 Emily Fox & Carlos Guestrin 
50 

Machine Learning Specializa0on 
Why hierarchical clustering? 
•  Avoid choosing # clusters beforehand 
•  Dendrograms help visualize  
diﬀerent clustering granularities 
-  No need to rerun algorithm 
•  Most algorithms allow user to choose 
any distance metric 
-  k-means restricted us to Euclidean distance 
51 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Why hierarchical clustering? 
52 
©2016 Emily Fox & Carlos Guestrin 
Can often ﬁnd more complex 
shapes than k-means or 
Gaussian mixture models 
k-means: spherical 
clusters 
Gaussian mixtures: 
ellipsoids 

Machine Learning Specializa0on 
Why hierarchical clustering? 
53 
©2016 Emily Fox & Carlos Guestrin 
Can often ﬁnd more complex 
shapes than k-means or 
Gaussian mixture models 
What about these? 

Machine Learning Specializa0on 
Two main types of algorithms 
Divisive, a.k.a top-down: Start with all data in 
one big cluster and recursively split. 
-  Example: recursive k-means 
 
Agglomerative a.k.a. bottom-up: Start with 
each data point as its own cluster. Merge 
clusters until all points are in one big cluster. 
-  Example: single linkage 
54 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Divisive clustering 
55 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Divisive in pictures – level 1 
56 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Divisive in pictures – level 2 
57 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Divisive: Recursive k-means 
58 
©2016 Emily Fox & Carlos Guestrin 
Wikipedia 

Machine Learning Specializa0on 
Divisive: Recursive k-means 
59 
©2016 Emily Fox & Carlos Guestrin 
Athletes 
Non-athletes 
Wikipedia 

Machine Learning Specializa0on 
Divisive: Recursive k-means 
60 
©2016 Emily Fox & Carlos Guestrin 
Athletes 
Non-athletes 
Wikipedia 
Baseball 
Soccer/ 
Ice hockey 
Musicians, 
artists, actors 
Scholars, politicians, 
government oﬃcials 
… 

Machine Learning Specializa0on 
61 
Divisive choices to be made 
•  Which algorithm to recurse 
•  How many clusters per split 
•  When to split vs. stop 
-  Max cluster size: 
number of points in cluster falls below threshold 
-  Max cluster radius:  
distance to furthest point falls below threshold  
-  Speciﬁed # clusters: 
split until pre-speciﬁed # clusters is reached 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Agglomerative clustering 
62 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Agglomerative: Single linkage 
63 
©2016 Emily Fox & Carlos Guestrin 
1.  Initialize each point to be its own cluster 

Machine Learning Specializa0on 
Agglomerative: Single linkage 
64 
©2016 Emily Fox & Carlos Guestrin 
1.  Initialize each point to be its own cluster 

Machine Learning Specializa0on 
Agglomerative: Single linkage 
65 
©2016 Emily Fox & Carlos Guestrin 
2.  Deﬁne distance between clusters to be: 
distance(C1,C2) =     
 
  
 min  d(xi, xj) 
xi in C1,  
xj in C2 
speciﬁed pairwise 
distance function 
Linkage criteria 

Machine Learning Specializa0on 
Agglomerative: Single linkage 
66 
©2016 Emily Fox & Carlos Guestrin 
3.  Merge the two closest clusters 

Machine Learning Specializa0on 
Agglomerative: Single linkage 
67 
©2016 Emily Fox & Carlos Guestrin 
4.  Repeat step 3 until all points are in one cluster 

Machine Learning Specializa0on 
Agglomerative: Single linkage 
68 
©2016 Emily Fox & Carlos Guestrin 
4.  Repeat step 3 until all points are in one cluster 

Machine Learning Specializa0on 
Agglomerative: Single linkage 
69 
©2016 Emily Fox & Carlos Guestrin 
4.  Repeat step 3 until all points are in one cluster 

Machine Learning Specializa0on 
Agglomerative: Single linkage 
70 
©2016 Emily Fox & Carlos Guestrin 
4.  Repeat step 3 until all points are in one cluster 

Machine Learning Specializa0on 
Agglomerative: Single linkage 
71 
©2016 Emily Fox & Carlos Guestrin 
4.  Repeat step 3 until all points are in one cluster 

Machine Learning Specializa0on 
Clusters of clusters 
72 
©2016 Emily Fox & Carlos Guestrin 
Just like our picture for divisive clustering… 

Machine Learning Specializa0on 
The dendrogram for  
agglomerative clustering 
73 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
The dendrogram 
•  x axis shows data points (carefully ordered) 
•  y-axis shows distance between pair of clusters 
74 
©2016 Emily Fox & Carlos Guestrin 
Cluster 
distance 
Data points 

Machine Learning Specializa0on 
The dendrogram 
•  x axis shows data points (carefully ordered) 
•  y-axis shows distance between pair of clusters 
75 
©2016 Emily Fox & Carlos Guestrin 
Cluster 
distance 
Data points 
Height here indicates 
min distance between 
blue pts and green pts 
(2 clusters) 

Machine Learning Specializa0on 
The dendrogram 
Path shows all clusters to which a point belongs 
and the order in which clusters merge 
76 
©2016 Emily Fox & Carlos Guestrin 
Cluster 
distance 
Data points 

Machine Learning Specializa0on 
Extracting a partition 
Choose a distance D* at which to cut dendogram 
77 
©2016 Emily Fox & Carlos Guestrin 
Data points 
D* 
Cluster 
distance 

Machine Learning Specializa0on 
Extracting a partition 
Every branch that crosses D* 
becomes a separate cluster 
78 
©2016 Emily Fox & Carlos Guestrin 
Data points 
D* 
Cluster 
distance 

Machine Learning Specializa0on 
Extracting a partition 
Every branch that crosses D* 
becomes a separate cluster 
79 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Agglomerative clustering details 
80 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
81 
Agglomerative choices to be made 
•  Distance metric: d(xi, xj) 
•  Linkage function: e.g.,  
•  Where and how to cut dendrogram 
©2016 Emily Fox & Carlos Guestrin 
min  d(xi, xj) 
xi in C1,  
xj in C2 
Data points 
D* 
Cluster 
distance 

Machine Learning Specializa0on 
82 
More on cutting dendrogram 
•  For visualization, smaller # clusters is preferable 
•  For tasks like outlier detection, cut based on: 
-  Distance threshold 
-  Inconsistency coeﬃcient  
•  Compare height of merge to average merge heights below 
•  If top merge is substantially higher, then it is joining two 
subsets that are relatively far apart compared to the 
members of each subset internally 
•  Still have to choose a threshold to cut at, but now in terms 
of "inconsistency" rather than distance 
•  No cutting method is "incorrect", some are just 
more useful than others 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Computational considerations 
•  Computing all pairs of distances is expensive 
-  Brute force algorithm is O(N2log(N)) 
 
 
•  Smart implementations use triangle inequality 
to rule out candidate pairs 
•  Best known algorithm is O(N2) 
83 
©2016 Emily Fox & Carlos Guestrin 
# datapoints 

Machine Learning Specializa0on 
Statistical issues 
Chaining: Distant points clustered together if 
there is a chain of pairwise close points between 
 
 
 
 
Other linkage functions can be more robust, but 
restrict the shapes of clusters that can be found 
-  Complete linkage:  
max pairwise distance between clusters 
-  Ward criterion:  
min within-cluster variance at each merge 
84 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
Hidden Markov models (HMMs): 
Another notion of “clustering” 
85 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
86 
So far, looked at clustering unordered data 
Data index (i.e., when 
observation was 
recorded) does not 
inﬂuence clustering  
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
What if we have time series data? 
87 
©2016 Emily Fox & Carlos Guestrin 
Would be hard to 
distinguish red, 
blue, and green 
clusters if we 
ignored order of 
data 

Machine Learning Specializa0on 
Example: Honey bee dances 
88 
©2016 Emily Fox & Carlos Guestrin 
Seq. 1 
Seq. 2 
Seq. 3 

Machine Learning Specializa0on 
Repeated patterns of dance transitions 
89 
©2016 Emily Fox & Carlos Guestrin 
Sequence 1 
Sequence 2 
Sequence 3 
turn 
right 
waggle 
dance 
turn 
left 
Cluster labels over time 

Machine Learning Specializa0on 
Similar ideas appear in many applications 
90 
©2016 Emily Fox & Carlos Guestrin 
John 
Jane 
Bob 
John 
J
i 
l 
l 
B
o
b
Turn  
right 
Turn  
left 
Waggle 
Waggle Turn  
right 
Jump-
ing 
jacks 
Side 
twists 
Run 
Squats 
Low 
volatility 
High  
vol 
Medium 
volatility 
Low 
vol 

Machine Learning Specializa0on 
91 
Hidden Markov model (HMM) 
As in mixture model… 
 
Every observation xt is associated with 
cluster assignment variable zt 
 
 
Each cluster has 
a distribution  
over observed  
values 
©2016 Emily Fox & Carlos Guestrin 
X 
0.3 
X 
0.8 
X 
0.42 

Machine Learning Specializa0on 
92 
Hidden Markov model (HMM) 
Diﬀerence from mixture model: 
Probability of (zt = k) depends on 
previous cluster assignment zt-1 
 
 
©2016 Emily Fox & Carlos Guestrin 
x 
π2 
π1 
π3 

Machine Learning Specializa0on 
93 
Inference in HMMs 
•  Learn MLE of HMM parameters using 
EM algorithm = Baum Welch 
•  Infer MLE of state sequence given ﬁxed 
model parameters using dynamic 
programming = Viterbi algorithm 
•  Infer soft assignments of state sequence 
using dynamic programming  
= forward-backward algorithm 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
What we didn’t cover 
94 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
95 
Other clustering + retrieval topics 
Retrieval: 
- Other distance metrics 
- Distance metric learning 
Clustering: 
- Nonparametric clustering 
- Spectral clustering 
Related ideas: 
- Density estimation 
- Anomaly detection 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
What’s ahead in this  
specialization 
96 
©2016 Emily Fox & Carlos Guestrin 

Machine Learning Specializa0on 
97 
This course is a part of the 
Machine Learning Specialization 
©2016 Emily Fox & Carlos Guestrin 
1. Foundations 
2. Regression 
3. Classiﬁcation 
4. Clustering 
& Retrieval 
5. Recommender 
Systems 
6. Capstone 

Machine Learning Specializa0on 
98 
5. Recommender Systems &  
Dimensionality Reduction    
Case study: Recommending Products 
•  Collaborative ﬁltering 
•  Matrix factorization 
•  PCA 
Models 
©2016 Emily Fox & Carlos Guestrin 
≈ 
=
Rating 
Parameters  
of model 
•  Coordinate descent 
•  Eigen decomposition 
•  SVD 
Algorithms 
•  Matrix completion, eigenvalues, 
cold-start problem, diversity, 
scaling up 
Concepts 

Machine Learning Specializa0on 
99 
6. Capstone: Build and deploy an intelligent  
application with deep learning 
©2016 Emily Fox & Carlos Guestrin 
Capstone 
project 
Recommenders 
Text 
sentiment 
analysis 
Computer 
vision 
Deep 
learning 
Deploy 
intelligent 
web app 

Machine Learning Specializa0on 
Thank you… 
100 
©2016 Emily Fox & Carlos Guestrin 


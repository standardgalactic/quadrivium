Advances in Intelligent Systems and Computing 689
Natalia Shakhovska
Volodymyr Stepashko    Editors 
Advances in 
Intelligent Systems 
and Computing II
Selected Papers from the International 
Conference on Computer Science and 
Information Technologies, CSIT 2017, 
September 5–8, Lviv, Ukraine

Advances in Intelligent Systems and Computing
Volume 689
Series editor
Janusz Kacprzyk, Polish Academy of Sciences, Warsaw, Poland
e-mail: kacprzyk@ibspan.waw.pl

About this Series
The series “Advances in Intelligent Systems and Computing” contains publications on theory,
applications, and design methods of Intelligent Systems and Intelligent Computing. Virtually
all disciplines such as engineering, natural sciences, computer and information science, ICT,
economics, business, e-commerce, environment, healthcare, life science are covered. The list
of topics spans all the areas of modern intelligent systems and computing.
The publications within “Advances in Intelligent Systems and Computing” are primarily
textbooks and proceedings of important conferences, symposia and congresses. They cover
signiﬁcant recent developments in the ﬁeld, both of a foundational and applicable character.
An important characteristic feature of the series is the short publication time and world-wide
distribution. This permits a rapid and broad dissemination of research results.
Advisory Board
Chairman
Nikhil R. Pal, Indian Statistical Institute, Kolkata, India
e-mail: nikhil@isical.ac.in
Members
Rafael Bello Perez, Universidad Central “Marta Abreu” de Las Villas, Santa Clara, Cuba
e-mail: rbellop@uclv.edu.cu
Emilio S. Corchado, University of Salamanca, Salamanca, Spain
e-mail: escorchado@usal.es
Hani Hagras, University of Essex, Colchester, UK
e-mail: hani@essex.ac.uk
László T. Kóczy, Széchenyi István University, Győr, Hungary
e-mail: koczy@sze.hu
Vladik Kreinovich, University of Texas at El Paso, El Paso, USA
e-mail: vladik@utep.edu
Chin-Teng Lin, National Chiao Tung University, Hsinchu, Taiwan
e-mail: ctlin@mail.nctu.edu.tw
Jie Lu, University of Technology, Sydney, Australia
e-mail: Jie.Lu@uts.edu.au
Patricia Melin, Tijuana Institute of Technology, Tijuana, Mexico
e-mail: epmelin@hafsamx.org
Nadia Nedjah, State University of Rio de Janeiro, Rio de Janeiro, Brazil
e-mail: nadia@eng.uerj.br
Ngoc Thanh Nguyen, Wroclaw University of Technology, Wroclaw, Poland
e-mail: Ngoc-Thanh.Nguyen@pwr.edu.pl
Jun Wang, The Chinese University of Hong Kong, Shatin, Hong Kong
e-mail: jwang@mae.cuhk.edu.hk
More information about this series at http://www.springer.com/series/11156

Natalia Shakhovska
• Volodymyr Stepashko
Editors
Advances in
Intelligent Systems
and Computing II
Selected Papers from the International
Conference on Computer Science and
Information Technologies, CSIT 2017,
September 5–8, Lviv, Ukraine
123

Editors
Natalia Shakhovska
Lviv Polytechnic National University
Lviv
Ukraine
Volodymyr Stepashko
IRTC ITS NASU
Kiev
Ukraine
ISSN 2194-5357
ISSN 2194-5365
(electronic)
Advances in Intelligent Systems and Computing
ISBN 978-3-319-70580-4
ISBN 978-3-319-70581-1
(eBook)
https://doi.org/10.1007/978-3-319-70581-1
Library of Congress Control Number: 2016950408
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made. The publisher remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
This book is designed for scientists and junior- or senior-level computer science
students with a basic background in intelligence systems, data mining, inductive
modeling, and computing. The international conference Computer Sciences and
Information Technologies is annually organized with the principal aim to discuss
modern trends in computer sciences, information technologies, applied linguistics,
and others related areas. To achieve this goal, various aspects of computer science
will be presented in 11 major topics: artiﬁcial intelligence; computational intelli-
gence; computer vision; information modeling of database and knowledge systems;
big data and cloud computing; intelligence control systems and technologies;
computational linguistics; cyber-physical systems; project management; software
engineering; IT education.
September 2017
Natalia Shakhovska
Program Committee Member
v

Contents
GMDH-Based Learning System for Mobile Robot Navigation
in Heterogeneous Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
Anatoliy Andrakhanov and Alexander Belyaev
Model of the Objective Clustering Inductive Technology
of Gene Expression Proﬁles Based on SOTA and DBSCAN
Clustering Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
Sergii Babichev, Volodymyr Lytvynenko, Jiri Skvor, and Jiri Fiser
From Close the Door to Do not Click and Back. Security
by Design for Older Adults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
Bartlomiej Balcerzak, Wieslaw Kopec, Radoslaw Nielek,
Kamil Warpechowski, and Agnieszka Czajka
The Popularization Problem of Websites
and Analysis of Competitors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
Taras Basyuk
Analysis of Uncertainty Types for Model Building
and Forecasting Dynamic Processes . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
Peter Bidyuk, Aleksandr Gozhyj, Irina Kalinina, and Victor Gozhyj
Dynamic Inertia Weight in Particle Swarm Optimization . . . . . . . . . . .
79
Bożena Borowska
Mathematical Method of Translation into Ukrainian Sign
Language Based on Ontologies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
Maksym Davydov and Olga Lozynska
Method of Parametric Identiﬁcation for Interval Discrete
Dynamic Models and the Computational Scheme
of Its Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
Mykola Dyvak, Natalia Porplytsya, Yurii Maslyak,
and Mykola Shynkaryk
vii

Mechanisms to Enhance the Efﬁciency of Maritime
Container Trafﬁc Through “Odessa” and “Chernomorsk”
Ports in the Balancing of Portfolios . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
Anatoly J. Gaida, Elena A. Zarichuk, and Konstantin V. Koshkin
Porting a Real-Time Objected Oriented Dependable Operating
System (RODOS) on a Customizable System-on-Chip . . . . . . . . . . . . . .
124
Muhammad Faisal and Sergio Montenegro
A General Game-Theoretic Approach to Harmonization
the Values of Project Stakeholders . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
Tigran G. Grigorian, Sergey D. Titov, Anatoliy Y. Gayda,
and Vladimir K. Koshkin
Information Technology for Assurance of Veracity of Quality
Information in the Software Requirements Speciﬁcation . . . . . . . . . . . .
166
Tetiana Hovorushchenko
A Multidimensional Adaptive Growing Neuro-Fuzzy System
and Its Online Learning Procedure . . . . . . . . . . . . . . . . . . . . . . . . . . . .
186
Zhengbing Hu, Yevgeniy V. Bodyanskiy, and Oleksii K. Tyshchenko
Method of Integration and Content Management
of the Information Resources Network . . . . . . . . . . . . . . . . . . . . . . . . . .
204
Olga Kanishcheva, Victoria Vysotska, Lyubomyr Chyrun,
and Aleksandr Gozhyj
Geoinformation Technology for Analysis and Visualisation
of High Spatial Resolution Greenhouse Gas Emissions Data
Using a Cloud Platform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
217
Vitalii Kinakh, Rostyslav Bun, and Olha Danylo
Comparative Analysis of Conversion Series Forecasting
in E-commerce Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230
Lyudmyla Kirichenko, Tamara Radivilova, and Illya Zinkevich
Performance Analysis of Open Source Machine Learning
Frameworks for Various Parameters in Single-Threaded
and Multi-threaded Modes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243
Yuriy Kochura, Sergii Stirenko, Oleg Alienin, Michail Novotarskiy,
and Yuri Gordienko
On Scalability of Predictive Ensembles and Tradeoff
Between Their Training Time and Accuracy . . . . . . . . . . . . . . . . . . . . .
257
Pavel Kordík and Tomáš Frýda
Agent DEVS Simulation of the Evacuation Process
from a Commercial Building During a Fire . . . . . . . . . . . . . . . . . . . . . .
270
Andrzej Kułakowski and Bartosz Rogala
viii
Contents

The Alpha-Procedure as an Inductive Approach to Pattern
Recognition and Its Connection with Lorentz Transformation . . . . . . . .
280
Tatjana Lange
Analyzing Project Team Members’ Expectations . . . . . . . . . . . . . . . . . .
299
Vira Liubchenko
The Contextual Search Method Based on Domain Thesaurus . . . . . . . .
310
Vasyl Lytvyn, Victoria Vysotska, Yevhen Burov, Oleh Veres,
and Ihor Rishnyak
Optimizing Wind Farm Structure Control . . . . . . . . . . . . . . . . . . . . . . .
320
Vitalii Kravchyshyn, Mykola Medykovskyy, Roman Melnyk,
and Marianna Dilai
Model of the System of Personalized Analysis of Financial
Condition of the Enterprise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
334
Melnykova Nataliia
Hybrid Sorting-Out Algorithm COMBI-GA with Evolutionary
Growth of Model Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
346
Olha Moroz and Volodymyr Stepashko
Development of Combined Information Technology
for Time Series Prediction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
361
Oksana Mulesa, Fedir Geche, Anatoliy Batyuk, and Viktor Buchok
Keyphrase Extraction Using Extended List of Stop Words
with Automated Updating of Stop Words List . . . . . . . . . . . . . . . . . . . .
374
Svetlana Popova and Gabriella Skitalinskaya
Innovative Concept of the Strict Line Hypergraph
as the Basis for Specifying the Duality Relation Between
the Vertex Separators and Cuts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
386
Artem Potebnia
Involutory Parametric Orthogonal Transforms of Cosine-Walsh
Type with Application to Data Encryption . . . . . . . . . . . . . . . . . . . . . . .
404
Dariusz Puchala
On the Asymptotic Methods of the Mathematical Models
of Strongly Nonlinear Physical Systems . . . . . . . . . . . . . . . . . . . . . . . . .
421
Petro Pukach, Volodymyr Il’kiv, Zinovii Nytrebych, Myroslava Vovk,
and Pavlo Pukach
Solution of the Discrete Ill-Posed Problem on the Basis
of Singular Value Decomposition and Random Projection . . . . . . . . . . .
434
Elena G. Revunova
Contents
ix

Methodology of Research the Library Information Services:
The Case of USA University Libraries . . . . . . . . . . . . . . . . . . . . . . . . . .
450
Antonii Rzheuskyi, Nataliia Kunanets, and Vasil Kut
The Method of Big Data Processing for Distance
Educational System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
461
Natalya Shakhovska, Olena Vovk, Roman Hasko,
and Yuriy Kryvenchuk
Developments and Prospects of GMDH-Based Inductive Modeling . . . .
474
Volodymyr Stepashko
Construction and Research of the Generalized Iterative GMDH
Algorithm with Active Neurons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
492
Volodymyr Stepashko, Oleksandra Bulgakova, and Viacheslav Zosimov
The Fast Fourier Transform Partitioning Scheme for GPU’s
Computation Effectiveness Improvement . . . . . . . . . . . . . . . . . . . . . . . .
511
Kamil Stokﬁszewski, Kamil Wieloch, and Mykhaylo Yatsymirskyy
Consolidation of Virtual Machines Using Stochastic Local Search . . . . .
523
Sergii Telenyk, Eduard Zharikov, and Oleksandr Rolik
Architecture and Models for System-Level Computer-Aided
Design of the Management System of Energy Efﬁciency of
Technological Processes at the Enterprise . . . . . . . . . . . . . . . . . . . . . . .
538
Taras Teslyuk, Ivan Tsmots, Vasyl Teslyuk, Mykola Medykovskyy,
and Yuriy Opotyak
Basic Components of Neuronetworks with Parallel Vertical Group
Data Real-Time Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
558
Ivan Tsmots, Vasyl Teslyuk, Taras Teslyuk, and Ihor Ihnatyev
Recommendation Systems as an Information and Technology
Tool for Virtual Research Teams . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
577
Nataliia Veretennikova and Nataliia Kunanets
Analytical Model for Availability Assessment of IoT Service Data
Transmission Subsystem. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
588
Bogdan Volochiy, Vitaliy Yakovyna, and Oleksandr Mulyak
Building Vector Autoregressive Models Using COMBI GMDH
with Recurrent-and-Parallel Computations . . . . . . . . . . . . . . . . . . . . . .
601
Serhiy Yeﬁmenko
Adaptive Enhancement of Monochrome Images with Low
Contrast and Non-uniform Illumination . . . . . . . . . . . . . . . . . . . . . . . . .
614
Elena Yelmanova and Yuriy Romanyshyn
x
Contents

Secure Routing in Reliable Networks: Proactive and Reactive
Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
631
Oleksandra Yeremenko, Oleksandr Lemeshko, and Anatoliy Persikov
Linguistic Comparison Quality Evaluation of Web-Site Content
with Tourism Documentation Objects . . . . . . . . . . . . . . . . . . . . . . . . . .
656
Pavlo Zhezhnych and Oksana Markiv
Author Index. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
669
Contents
xi

GMDH-Based Learning System for Mobile
Robot Navigation in Heterogeneous
Environment
Anatoliy Andrakhanov(&) and Alexander Belyaev
Department of Control Systems and Mechatronics, National Research Tomsk
Polytechnic University, Lenin Avenue, 30, Tomsk 634050, Russian Federation
rim1282a@gmail.com, belyaewas@mail.ru
Abstract. One of the key tasks of mobile robotics is navigation, which for
Outdoor-type robots is exacerbated by the functioning in an environment with a
priori of unknown characteristics of underlying surfaces. In this paper, for the
ﬁrst time, the learning navigation system for mobile robot based on the group
method of data handling (GMDH) is presented. The paper presents the results of
training of models both for evaluating the robot’s pose (coordinates and angular
orientation) in heterogeneous environment and classiﬁcation of the type of
underlying surfaces. In addition to the direct readings of the on-board sensors,
additional parameters (reﬂecting how the robot perceives the surface terrame-
chanics) were introduced to train the models. The results of testing of the
obtained models demonstrate their performance in an essentially heterogeneous
environment, when areas of the underlying surfaces are comparable with the
robot’s dimensions. This testiﬁes the operability of developed GMDH-based
learning system for mobile robot navigation.
Keywords: Mobile robot  Heterogeneous environment  Underlying surface
Testing ground  Navigation  Coordinates evaluation  Machine learning
Inductive modeling  GMDH  Active neuron  Festo Robotino
1
Introduction
Despite the rapid development of mobile robotics, the development of an intelligent
control system remains one of the main challenges in the creation of autonomous
robotic systems.
One of the key tasks is the navigation, which can be divided into 2 parts: the
estimation of the current position (coordinates and angular orientation) of the robot in
the working space and the development of control actions on the actuators to
sequentially achieve all the intermediate robot positions along the planned trajectory. In
this case, the solution of the second part of the problem is impossible without solving
of the ﬁrst. In some cases, the evaluation of the robot position in the environment can
be carried out only by means of on-board inertial system, because global positioning
systems (GPS) may be not exist (planetary rovers), may be inaccessible (ﬁre ﬁghting
robots, autonomous mining vehicles) or suppressed by electronic countermeasures
equipment (combat robots).
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_1

For outdoor-type mobile robots, this problem is exacerbated by the natural con-
ditions of the environment which is characterized by the a priori unknown of an
environment model, the heterogeneous characteristics of surfaces to be traversed, and
the difﬁculty of determining the features of the robot-terrain interaction based on
on-board sensor readings.
There are a great number of papers on this subject [1–9]. In a ﬁrst approximation
the four main approaches can be identiﬁed for solving navigation problem in a
heterogeneous environment (Table 1).
There is currently no generally accepted dominant methodology at present, and
each research group is trying to solve the problem in its own way. The authors of this
work believe that one of the most promising paths is to construct non-physical models
using the advantages of the inductive modeling approach.
The basic method of the inductive modeling approach is Group Method of Data
Handling (GMDH). To date, the most complete overview of the use of GMDH in
robotics is shown in the work [12]. This method was already used by the authors to
solve the problem of evaluating the robot’s pose in homogeneous and heterogeneous
environments and demonstrated an acceptable result [10, 11].
2
The Advantages of GMDH for Finding Dependencies
Based on the Analysis of Sensor Readings
A number of ﬁeld experiments were carried out in order to determine the interaction
between the robot’s propulsion system and the underlying surface of various types
based on on-board sensor readings.
Table 1. The man ways to solve the problem of a mobile robot’s navigation in a heterogeneous
environment
Approach to models
construction
Classiﬁcation of the underlying surface type
Used
Not used
Detection and analysis of a
physical patterns of
robot-terrain interaction
S. Khaleghian and S.
Taheri [9]
Sensors: 3-axis and
1-exis accelerometers,
encoders
Method: Fuzzy logic
L. Ojeda et al. [7]
Iagnemma K. et al. [8]
Sensors: encoders, gyroscopes,
accelerometers [8] + current
sensors [7]
Method: Wheel slip analysis
Construction of non-physical
models by means of machine
learning
DuPont E.M. et al. [6]
Sensors: 3-axis
gyroscope, 3-axis
accelerometer
Method: Probabilistic
neural network
A.A. Andrakhanov [10, 11]
Sensors: encoders, motor current
sensors
Method: Twice Multilayered
Modiﬁed Polynomial Neural Network
with active neurons
2
A. Andrakhanov and A. Belyaev

A serial-produced mobile platform Festo Robotino was used as the mobile robot.
Experiments were carried out at a specially designed testing ground consisting of 28
modules with different terramechanical characteristics (Fig. 1a). The testing ground
was designed in such a way that the terramechanical interaction of its areas with Festo
Robotino’s wheel system was correspond in terms of quality with the level of ter-
ramechanical interaction of the outdoor robot with the areas of the natural environment.
Figure 1b shows the robot’s test motions (along a square and a triangle path) with the
same motor speed setpoints under conditions of a homogeneous (an ideal ﬂat surface)
and heterogeneous (Testing Ground) environment. Holonomic character of robot
movements is provided due to 3 wheels of omnidirectional type, located at an angle of
120° with respect to each other (Fig. 1c).
It is clear from Fig. 1b that the areas of the testing ground have a signiﬁcant impact
on the nature of the robot’s movement. In order to maximize the transparency between
sensor readings and the nature of the robot’s motion, we decided to assign only the
simplest robot motions and only the homogeneous areas of the testing ground consisting
of modules of the same type. Using the simplest driving setpoints makes it possible to
eliminate the features of the robot’s kinematics and propulsion system, which in turn
allows it to make complex motions (curvilinear motion with rotational motion) without
being affected by the features of the environment. Moving in a homogeneous local area
eliminates the comlex inﬂuence of different local areas on the robot behavior because on
the testing ground the each wheel interacts with its local area (Fig. 1a).
Five types of local surfaces, which were different in terms of terramechanics, were
selected by expertise, as well as four of the simplest movements setpoints: three
translational motion, without rotational component (along the X-axis, along the Y-axis,
in the XY plane at the same X- and Y-axis speeds) and rotational motion without
translational component, all motions mentioned in relation to the robot’s local coor-
dinate system (Fig. 1c). The speed of the translational motion was set to be 100 mm/s
(for both the X- and Y-axes), and that of rotational motion was 24, 48 and 96 (deg/s).
Fig. 1. (a) Testing Ground appearance and the robot; (b) Trajectories for test motions (along a
square and a triangle path) in homogeneous (upper) and heterogeneous (lower) environments;
(c) arrangement of wheels in Robotino’s omni-drive system [13] (X, Y, u – Robotino’s local
coordinate system)
GMDH-Based Learning System for Mobile Robot Navigation
3

Sensor data subject to analysis included: {N} = {N1, N2, N3} – a set of incremental
encoders values, {x} = {x1, x2, x3} – values of the speeds of motors, {I} = {I1, I2,
I3} – values of the motors consumption currents, {g} = {gx, gy, gz} – angular velocity
values in relation to the X, Y and Z axes (Z-axis is perpendicular to XY-plane),
{a} = {ax, ay, az} – a set of acceleration values along the X, Y and Z axes.
Figure 2 shows the readings of three sensors when the robot traverses over ﬁve
different underlying surfaces.
Fig. 2. Sensor readings during the robot’s movement over different surfaces (horizontal axis for
all graphics is the time axis in milliseconds)
4
A. Andrakhanov and A. Belyaev

The analysis shows that, on the one hand, the sensor data are correlated to the type
of underlying surface and the nature of the robot movement on this surface. On the
other hand, the correlation is ambiguous.
For example, the average values of currents in the ﬁrst motor (I1) demonstrate the
laboriousness of overcoming the area (Fig. 2a and g), on the one hand, while on the
other hand, these values may be close despite the fact that the surfaces may have
different terramechanical characteristics (Fig. 2a and j). In addition, there are other
features in the sensor data, which reﬂect the interaction of the robot with a surface of a
particular type. In particular, the gyroscope data along the Z-axis (gz), shown in Fig. 2b
and h, clearly show the periodic behavior and the curve shape when moving over a
particular surface. However, these may not be shown in the sensor data (Fig. 2k).
A similar trend is observed in accelerometer readings (ay): the features may manifest
themselves (Fig. 2i), though not always (Fig. 2l and f).
Inasmuch as there are unique data properties for different subsets of sensors, it is
necessary to derive models on their basis by using machine learning and data mining
techniques. In our opinion, one promising tool for evaluating the surface type and the
coordinates using sensor readings is the inductive approach.
The advantages of GMDH (as a method of inductive modeling) for the afore-
mentioned research topics and developments are shown in Table 2. This method
provides maximal ﬂexibility at the stage of model construction both in handling the
parameters of data sample (the method of dividing the sample into the training and the
test parts, sorting by the dispersion of the output variable, etc.) and the training
algorithm parameters (for neural algorithms, it means selecting the maximum degree of
a partial description of a neuron, the number of selectable neurons in the layer, the
maximum number of layers in the network, etc.).
Table 2. The beneﬁts of GMDH in addressing the navigation problem
The problem of navigation in a
heterogeneous environment
The GMDH advantages
A variety of tools is necessary to generate
methods for evaluating coordinates and
determining the underlying surface type. For
instance, in the well-known paper [6], the
technical solution contained operations of
extracting the most signiﬁcant features
(principal component analysis), interpolation
(fundamental splines), clustering
(Eigen-transformation), and classiﬁcation
(probabilistic neural network)
The method includes a wide range of
algorithms for predicting, classifying,
clustering, identifying and data mining
The uniﬁed methodological basis for the
aforementioned spectrum of algorithms
contributes to the standardization of the
system’s program modules
(continued)
GMDH-Based Learning System for Mobile Robot Navigation
5

In addition, GMDH provides great opportunities in analyzing dependencies found
during the training phase: which sensor readings are used for the model’s output as the
input variables; with what degrees and/or coefﬁcients these variables are included in the
model; how often these variables are chosen by the neural network algorithm when
constructing a neural network structure from layer to layer sequentially, etc.
In order to ensure maximum access to these features and the advantages of GMDH,
the authors have decided to develop a navigational training system on its basis.
3
Navigation Learning System
3.1
Description of the System
The system architecture is represented in Fig. 3 (only the basic connections between
the modules are shown).
Table 2. (continued)
The problem of navigation in a
heterogeneous environment
The GMDH advantages
There are no simple and obvious correlations
between sensor readings and the
terramechanical properties of the underlying
surfaces
The most effective input variables (with
respect to some quality criterion) are selected
automatically from the set of variables
available to the system, and relationships in
data are interpreted
The resulting dependencies have an analytical
form (it is also typical for the GMDH-type
neural networks), which enhances the
capabilities for analysis and makes it possible
to interpret the results
The number of local areas that affect the
robot motion in different ways can be
arbitrary large. Therefore, it is necessary that
the functional dependences derived by the
onboard computing system should be
generalizable for other areas that have not yet
been traversed
The resulting dependencies have a
generalizing ability because an external
criterion of model quality is used (evaluation
of model parameters and selection of model
structure are performed using independent
data subsamples)
Considering that the size of local areas may
be relatively small, it is important that the
methods used to derive models be able to
work with short data samples
In some cases (for instance, the time limit for
making a decision, limited learning time, as
well as energy costs, and so on), it makes
sense to collect a relatively small number of
samples even if the size of areas is signiﬁcant
For short, inaccurate, or noisy data, an
optimal nonphysical model can be found,
whose accuracy is higher and structure is
simpler than the structure of a complete
physical model [14]
Finding a solution within a limited training
time is guaranteed. The system can calculate
the training time before training algorithm run
6
A. Andrakhanov and A. Belyaev

The navigation learning system (NLS) consists of the following modules:
• The Learning Subsystem that implements computational intelligence algorithms
within a framework of the inductive modeling approach.
• The Database which is necessary for collecting and storing sensor data as well as
learning results.
• The Test subsystem which is required to test the hardware of the system (sensors
and actuators of the robot, unit for determining coordinates and angular orientation)
and the learning subsystem. The hardware test is based on comparing the stored
data (sensor readings, the coordinates and angular orientation values) and the data
obtained as a result of the robot’s test movements. Testing of the learning subsystem
is done by models training on test data samples and comparing the obtained results
with the stored results.
• The Analysis subsystem which analyzes the previous results of models training in
order to determine the inﬂuence of different subsets of input variables, the variants
of splitting a data sample, the quality criteria and the algorithm parameters on the
quality of obtained models.
• The Operator interface that provides the operator’s access to data samples, as well
as to the results of testing, training and analysis.
Fig. 3. The navigation learning system architecture
GMDH-Based Learning System for Mobile Robot Navigation
7

The hardware which is external in relation to NLS includes the on-board sensors
and actuators of the Festo Robotino platform, the additional unit of inertial sensors
(three-axis gyroscope and three-axis accelerometer) and the Full HD camera to obtain
the coordinates and angular orientation of the robot.
This system was used for training and testing models in all the experiments
mentioned below.
3.2
The Models Training Algorithm
Twice-Multilayered Modiﬁed Polynomial Neural Network with Active Neurons
(TMMPNN) algorithm makes it possible to ﬁnd the optimal network structure (from
the view of the external criterion) and partial descriptions of neurons automatically (in
the self-organization mode). The concept of twice-multilayered polynomial neural
network algorithm was ﬁrst proposed by A.G. Ivakhnenko and J.A. Muller [15].
The modiﬁcation is that the generation of partial descriptions on each layer (starting
from the 2nd) involves not only the outputs of neurons of the previous layer, but also
the input variables. Thus, such modiﬁcation provides an opportunity to avoid losing
important input variables on the ﬁrst and subsequent layers of the network. The
structure of the modiﬁed polynomial neural network is shown in Fig. 4 [12]:
Fig. 4. Twice-multilayered
modiﬁed
GMDH-type
polynomial
neural
network
(x1; x2; xi; xj; xn1; xn are input variables; F1
kðxi; xjÞ are partial descriptions of k-th selected
neuron of the 1st layer; FR1
d
; FR1
k
; FR1
q
; FR1
w
are partial descriptions of neurons of the layer
(R–1); OUTR
a ; OUTR
n ; OUTR
u ; OUTR
z are partial descriptions of selected neurons of the output
layer)
8
A. Andrakhanov and A. Belyaev

Since this algorithm was described in detail in earlier papers [10, 12] and its
software implementation was published on the CD to the book [12], we will discuss
only the main points related to the settings of this algorithm in the following
experiments.
The classic combinatorial algorithm of GMDH [16] is used to search for partial
descriptions of neurons. In this case, two-input neurons were used limited by the
maximum polynomial degree 2 of the partial description:
Fl
k xi; xj


¼ a0 þ a1  xi þ a2  xj þ a3  x2
i þ a4  x2
j þ a5  xi  xj
ð1Þ
The regularity criterion was used as an external criterion for the selection of partial
descriptions of neurons:
CR ¼ 1
NB
X
NB
i¼1
fi  yi
ð
Þ2
ð2Þ
where NB is the number of rows of the testing data sample; fi is the output of the model
for row i; yi is the output value for row i of the data sample.
In case of network structure construction the regularity criterion was also used as an
external criterion for the selection of the best neurons of a layer. The external layer
criterion (the arithmetic mean value of the regularity criteria of the best neurons in a
given layer), the limit of maximal network capacity (maximal number of lay-
ers  number of selected best neurons in a layer) and the additional stopping criterion
(an improvement in the value for the external layer criterion should be more than e from
layer to layer) were used as stopping criteria of the expansion of network layers.
The algorithm, criteria and settings described were also used for the training of a
surface type classiﬁer. When using a trained classiﬁer, the threshold condition is
applied to the network output: «1» if the network output is greater than or equal to 0.5,
and «0» otherwise.
3.3
Forming Sets of Input Variables for Models Training
In Sect. 2, it was shown that it was difﬁcult to estimate the coordinates of the robot’s
position and the type of the underlying surfaces directly from the sensor readings. In
the paper [17], parameters reﬂecting how the robot sense the terramechanics of a
surface based on sensor readings were introduced. In this work, we also introduced
additional parameters of such type to increase the number of relevant variables on the
training stage.
The three parameters that characterize the displacement in a given local area are the
robot’s velocities in its local coordinate system:
Vx
Vy
X
0
@
1
A ¼ R 
 2
3 cos a  h
ð
Þ
2
3 sin a
ð Þ
2
3 cos a þ h
ð
Þ
 2
3 sin a  h
ð
Þ
 2
3 cos a
ð Þ
2
3 sin a þ h
ð
Þ
1
3L
1
3L
1
3L
0
@
1
A 
x1
x2
x3
0
@
1
A
ð3Þ
GMDH-Based Learning System for Mobile Robot Navigation
9

where Vx, Vy is the velocity along the X and Y axes of the robot’s local coordinate
system; X is the angular rotation velocity of the robot in the local coordinate system;
x1, x2, x3 are the angular speeds of wheels (associated with the speeds of the motors
through the 1:16 gear ratio); L is the distance from the center of the robot to the wheel
(125 mm); R is the wheel radius (40 mm); a is the robot orientation angle; h is the
wheel orientation angle (30°).
The kinematics Eq. (3) was also used by the authors to obtain the parameters of the
laboriousness of translational and rotational motion of the robot:
Ix
Iy
Iu
0
@
1
A ¼ R 
 2
3 cos a  h
ð
Þ
2
3 sin a
ð Þ
2
3 cos a þ h
ð
Þ
 2
3 sin a  h
ð
Þ
 2
3 cos a
ð Þ
2
3 sin a þ h
ð
Þ
1
3
1
3
1
3
0
@
1
A 
I1
I2
I3
0
@
1
A
ð4Þ
where Ix, Iy are the values of the currents that characterize the laboriousness of the
robot’s movement along the X and Y axes of the local coordinate system; Iu is the
value of the current which characterizes the laboriousness of the robot’s angular
rotation in the local coordinate system; I1, I2, I3 are the consumption currents of motors.
In (4), the consumption currents of motors have the same sign, which is determined
by the direction of the wheel rotation, as for the speeds of motors in (3). Unlike (3),
R and L values are not used in (4) because, ﬁrstly, they are not related to a geometric
transformation of the current vectors. Secondly, the dimension of the output quantities
and their physical meaning will be inconsistent with each other, which is unacceptable.
Thirdly, these values inﬂuence only the amplitude of the output values, which is not
important for GMDH, since the TMMPNN algorithm independently selects necessary
weighting coefﬁcients. The preliminary analysis showed an appropriate separability for
all ﬁve types of surfaces used in the experiments in case of use of parameters – Ix, Iy, Iu
(Fig. 5b, c and d).
Another parameter used, which characterizes the interaction of the robot with the
surface, is IR – the total consumption current of motors:
IR ¼ I1
j j þ I2
j j þ I3
j j
ð5Þ
As can be seen in Fig. 5a, the mean value of this parameter varies for different types
of surfaces, which makes it a useful variable both for classifying the surface type and
for estimating the coordinates and angular orientation. Since the robot arrives to dif-
ferent coordinates on different surfaces with the same motor velocities setpoints, the
coordinate estimation can be related to this parameter.
In addition to the aforementioned absolute parameters, the following relative
parameters were also introduced:
Tx ¼ Vx
Ix
; Ty ¼ Vy
Iy
; Tu ¼ X
Iu
; Tz ¼ gz
Iu
; T
x ¼ V
x  Vx
Ix
; T
y ¼
V
y  Vy
Iy
;
T
u ¼ X  X
Iu
; T
z ¼ X  gz
Iu
ð6Þ
10
A. Andrakhanov and A. Belyaev

The values Vx, Vy, and X are calculated using actual wheel speeds values, based on
(3), while Ix, Iy, and Iu are calculated using the current sensors values based on (4). The
values Vx
*, Vy
*, and X* are the setpoints of the robot’s movement velocities in the local
coordinate system.
It should be noted that the dimension of relative parameters has the physical
interpretation as a unit of the translational/rotational movement on a particular surface
for the expended current impulse, which is normalized to the same type and direction of
movement. In the case of a difference in the numerator between the setpoint and the
real velocity, the physical interpretation changes into: by how many millimeters/
degrees the actual displacement/rotation of the robot on the surface will differ from the
setpoint value after one current impulse.
Thus, while implementing inductive modeling, three sets of input variables were
used:
• {V1} = {{N},{x},{I},{g},{a}} are values obtained directly from the robot’s
sensors;
• {V2} = {Vx, Vy, X, Ix, Iy, Iu, IR} are absolute parameters obtained by means of
mathematical transformations of values measured by sensors;
Fig. 5. Analysis of the relevance of the parameters (the setpoints of the robot’s movement
velocities in the format – [Vx
* (mm/s), Vy
* (mm/s), X* (deg/s)]: (a) [0; 100; 0]; (b) [100; 100; 0];
(c) [0; 100; 0]; (d) [0; 0; 24])
GMDH-Based Learning System for Mobile Robot Navigation
11

•
V3
f
g ¼
Tx; Ty; Tu; Tz; T
x ; T
y ; T
u; T
z
n
o
are relative parameters obtained by means
of algebraic relations between values of the second and the ﬁrst sets.
The purpose of the experiments series was:
• Determination of the obtained models accuracy for robot pose evaluation taking
into account three sets of parameters;
• Estimation of the relevance of each set of parameters for the constructing of robot
pose estimation models;
• Determination of the accuracy of the underlying surface type classiﬁcation taking
into account three sets of parameters;
• Estimation of the relevance of each set of parameters for constructing classiﬁers of
the surface type;
• Testing the obtained models during robot movement in an essentially heterogeneous
environment, when dimensions of both the surfaces and the robot are comparable.
4
Results of Experiments
4.1
Results of Models Training
All experiments were carried out on ﬁve selected types (see Fig. 2) of surfaces using
the Festo Robotino mobile platform. There were 30 robot launches lasting 4 s with the
following combinations of robot movement setpoints (in the format [DX/Dt (mm/s),
DY/Dt (mm/s), Du/Dt (deg/s)]): [100,0,0], [0,100,0], [100,100,0], [0,0,24], [0,0,48],
[0,0,96]. Data sample was formed by dividing of the sensor readings into half-second
intervals, sensor values was averaged for these intervals (except values of {N}, the
increment of values per half-second intervals were calculated for this case). Thus, for
each robot launch four examples were included into the training data sample, four –
into the testing data sample.
All models were obtained with help on software that was published on the CD to
the book «GMDH-Methodology and Implementation in C» [12].
In the all experiments, the constraints were used to both the maximum power of
neuron (power – 2) and the network capacity (10 layers x 10 neurons per layer). The
choice of these parameters is due to the experience of our previous experiments,
including in [10, 11]. In particular, it was found that such a limitation on the network
capacity makes it possible to obtain the most stable (in terms of accuracy and bias)
models.
Since the absolute error in the determination of the coordinates used by the computer
vision unit is 1 mm, an additional criterion for stopping the network construction e = 0.1
was given. Data sample was divided into the two equal parts (training and testing).
The results of the experiments are shown in Table 3 (“[Avr]” is the arithmetic mean
error, “Max” is the maximum error, “GM” are denoted (“General Model”) the training
results of model on the combined data sample for all types of the surfaces) and Table 4.
12
A. Andrakhanov and A. Belyaev

The best trained models with minimum error are highlighted in bold in Tables 3 and 4.
The average values (“[Avr]”) of the coordinates and angular orientation in Table 3 are
less 1 (mm or deg) on the all types of underlying surfaces for the all subsets of input
variables.
Table 3. Results of models training for robot pose estimation
Value
Type Input variable set
{V1}
{V2}
{V3}
{V1}, {V2} {V1}, {V3} {V2}, {V3} All
Max
[Avr]
Max
[Avr]
Max
[Avr]
Max
[Avr]
Max
[Avr]
Max
[Avr]
Max
[Avr]
X, mm 1
3.6
6.0
6.8
4.6
3.6
4.9
4.6
2
10.5
9.3
9.4
10.9
10.5
9.7
10.9
3
5.9
8.1
6.4
6.3
5.9
6.6
6.1
4
2.2
2.1
2.8
2.0
2.3
1.8
2.0
5
7.2
7.2
6.8
6.1
7.2
6.3
6.4
GM
8.7
[2.02]
9.9
[1.7]
12.6
[2.16]
10.5
[1.7]
9.3
[1.8]
10.0
[1.8]
10.3
[1.7]
Y, mm 1
5.1
4.8
6.6
4.8
7.3
4.8
4.8
2
9.0
8.9
8.1
7.7
8.1
8.5
6.5
3
6.1
9.7
29.3
12.2
6.2
8.6
12.2
4
1.5
2.2
2.0
1.9
1.7
2.8
2.0
5
5.5
8.1
9.2
5.8
5.8
9.2
9.2
GM
9.9
[1.7]
15.1
[1.5]
243
[6.6]
9.0
[1.4]
9.9
[1.7]
15.1
[1.5]
9.0
[1.4]
u, deg 1
5.3
6.8
65.2
5.5
5.3
6.8
5.5
2
4.3
5.8
9.0
4.9
4.0
5.8
4.9
3
6.4
5.7
7.6
6.0
6.1
4.9
6.0
4
4.8
9.2
3.3
4.8
4.8
9.2
4.8
5
8.2
7.7
3.6
4.5
5.4
6.5
5.4
GM
14.6
[1.4]
10.3
[1.74]
305
[13.8]
10.6
[1.6]
14.6
[1.4]
11.1
[1.6]
10.6
[1.6]
Table 4. Percentage of correct classiﬁcation for trained classiﬁers
Type Input variable set
{V1} {V2} {V3} {V1}, {V2} {V1}, {V3} {V2}, {V3} All
1
81.2
85.0
81.2
86.3
82.5
88.0
82.1
2
96.6
95.7
83.3
97.0
96.2
95.7
97.0
3
86.3
85.0
81.6
84.6
85.0
81.6
86.8
4
97.9
98.7
79.5
97.8
97.9
98.7
98.3
5
78.6
82.1
79.9
79.5
78.2
79.5
80.8
GMDH-Based Learning System for Mobile Robot Navigation
13

Insomuch as this neural network is based on the inductive principles of
self-organization of models, the very process of the self-organization of its structure
serves not only as a means of obtaining the ﬁnal model but also as a tool for analysis.
Thus, based on the selection of appropriate input variables on each layer of the network
by active neurons, we can estimate the contribution of the sensor data to the overall
dependency. The received structures of the GMDH-type neural networks for the best
models of robot pose estimation are shown in Fig. 6.
Active neurons ﬁrst of all choose input parameters taking into account their direct
physical correspondence with the output variable, although the GMDH algorithm
constructs non-physical models. For example:
• In case XGM in Fig. 6 the neurons chosen the values of I1, I3 and N1, N3, which are
directly related to X-coordinate (see Fig. 1c). The ax is also directly related to the
output variable.
• In case YGM the neurons chosen the Vy and N2, which are directly related to Y-
coordinate (see Fig. 1c). The choice of IR is also associated with the Y-variable
evaluation, since, as mentioned above, for each type of surface this parameter has
different values (see Fig. 5a).
Fig. 6. Structures of twice-multilayered modiﬁed polynomial neural networks for the some best
trained models (“*” – average value per half-second interval, “Type” – the type of underlying
surface (see Table 3), the networks evaluates X, Y and u in 0.5 s of robot’s movement)
14
A. Andrakhanov and A. Belyaev

At the same time, the choice of some input parameters is not obvious, because the
features of robot-terrain interaction for speciﬁc types of surfaces are taken into account
(for example, gy and gz for XType1, gx and T*
u for YType2).
Analysis of classiﬁers shows that in order to construct better models, active neurons
in all cases use the variables of {V2}.
4.2
Results of Testing the Trained Models During the Robot Movement
in a Heterogeneous Environment
This section shows some results of testing of the trained models in an essentially
heterogeneous environment, when the areas of the surfaces are comparable with the
robot’s dimensions. Thus, the test conditions for navigation are much more difﬁcult
than at the training stage. First, at the training stage the robot moved along separate
homogeneous surfaces of several types. But in this test, the effects of not only the
inﬂuence but also the mutual inﬂuence of the properties of different surfaces on the
robot’s movement appear. Secondly, such transition zones between surfaces (when
different wheels are simultaneously located at different surfaces) appear often during
the movement, which accelerates the accumulation of navigation errors.
The task was to movement along the triangle (lengths of the sides are assigned by
operator) through the surfaces of different types, using only the on-board sensor
readings (without the signal from the global positioning system) and the trained
models. Trained models were used by the robot to determine the achievement of the
vertexes of triangle (with the aim of changing the movement direction) and to correct
the trajectory during the movement. To determine the deviations from the desired
trajectory the outputs of these models were used as a feedback signals instead of GPS
signals. With these deviations, the control signals for the robot motions are generated
by the method of proportional regulation well-known in the automatic control theory.
The purpose of the series of experiments was to determine the performance of the
best trained models (see Table 3) for two cases:
• movement for mentioned above conditions by means of models general to all types
of surfaces (denoted “GM” in Table 3);
• movement under the same conditions using coordinates evaluation models spe-
cialized for a speciﬁc type of surface (we denote this models as “MT” (“Model for
Type”)). These models are selected by signal from the corresponding classiﬁer. If
signals from all classiﬁers are absent or there are signals from several classiﬁers,
then the coordinates and orientation angle are evaluated by the “GM”-models.
In Fig. 7 shows the ﬁnal trajectory of the robot along the speciﬁed sides of triangle.
GMDH-Based Learning System for Mobile Robot Navigation
15

The achievement of the coordinates of the vertices of the triangle was estimated by
means of the best “GM” models for X, Y and u. Also these models were used for
trajectory correction during the robot movement. In this experiment, the length of the
side of triangle was set at 0.5 m. In combination with three different types of surfaces
and the robot’s diameter of 37 cm provides speciﬁed conditions for testing the models,
because about half of path the robot moves through the transition zones. For example,
already at points 7 and 12 the robot’s wheels are simultaneously located on two
surfaces: for the ﬁrst point – both on 3 and 5 types of surfaces, for the second point –
both on 4 and 5 types of surfaces.
In Fig. 8b and d shows the evaluation of the X and Y coordinates by means of both
the speciﬁed models (XType3, XType4, XType5, YType3, YType4, YType5) for three types of
surfaces (see Fig. 7) and best general models for all types of surfaces (XGM, YGM). As
XCSV and YCSV are denoted real coordinates of robot movement detected by computer
vision system (see Fig. 3). As can be seen from the curves of XCSV and YCSV, the
movement of the robot is complex (for example, deviations of coordinates for a period
of 15-23 robot’s steps and pause in the movement for a period of 30–57 steps) and is
not rectilinear, which indicates a signiﬁcant inﬂuence of the surfaces on it. It can also
be seen that on separate time intervals, different “MT”-models are more accurate than
general models. At the same time, general models (XGM, YGM) show an acceptable (in
average) result during the overall time of the movement.
Figure 8a and c show the errors in determining both X and Y coordinates by the best
“GM”-models (X_ErrorGM, Y_ErrorGM) and the set of specialized models (X_ErrorMT,
Y_ErrorMT) that are selected at the moment the classiﬁer of surface type is triggered
(output is equal “1”) in accordance with the above rule. In general, the trained clas-
siﬁcation models demonstrate their operability. However, in addition to its own clas-
siﬁcation errors (for example, at the interval 55–75 steps in Fig. 8f), there are errors
Fig. 7. Trajectory of robot movement obtained by means of “GM”-models
16
A. Andrakhanov and A. Belyaev

Fig. 8. Graphs of both outputs and errors for models of evaluating coordinates and for
classiﬁcation models during the robot movement
GMDH-Based Learning System for Mobile Robot Navigation
17

caused by the transition of the robot from one surface to another (for example, at the
interval of 15–20 steps in Fig. 8f and g, two classiﬁers are triggered simultaneously).
In Fig. 8a, b , c and d all values of output variables for each robot’s step are shown
in robot’s local coordinate system (Fig. 1c).
Based on the results of testing the models, we can conclude the following:
• With the simultaneous contact of the robot wheels with surfaces of different types,
there are classiﬁcation errors (i.e. errors of selection of “MT”-models) and errors of
“MT”-models. This leads to the fact that the use of general (i.e., averaged for all
types of surfaces) and speciﬁed models gives a comparable result.
• In general, all models (both pose estimation and classiﬁcations) demonstrate their
performance by an example much more complex than the conditions for their
training. This indicates the operability of developed learning navigation system (in
point of view quality of obtained models) and practical applicability of GMDH to
solving the problem of local navigation of a robot.
It should be noted that the purpose of this section was to test the obtained models,
and not to solve complex questions of developing a system for local navigation. The
focus of this study is the synthesis of models, but to increase the accuracy of the local
navigation system, it is necessary to consider a wider range of issues related more to the
stage of using the obtained models, rather than to the stage of their training. In order to
improve the quality of the obtained trajectory and the accuracy of movement to the
given coordinates, it is necessary to solve the following tasks: synthesis of a better
regulator for robot motion along the desired trajectory; the development of methods to
use the classiﬁers and “MT”-models according to considered conditions and etc.
In Fig. 8b and d the accuracy both “GM”- and “MT”-models during test motion
correlates with its accuracy on training stage (see Table 3).
5
Conclusion
We have obtained higher accuracy (arithmetic mean error is less) of models for
evaluating the coordinates and angular orientation than in our previous research [10,
11] due to the extension of the input parameters set (only {N}, {x} and {I} were used
in past).
In general case, it is not sufﬁcient to use a certain subset of input parameters to
obtain better models for different outputs (X, Y and u) and underlying surface types.
We recommend using parameters derived from sensor readings (for example, {V2} and
{V3}) to improve the quality of the models. For example, it is interesting to note that
the variables of {V2} were selected by active neurons to train classiﬁers for each of the
ﬁve types of surfaces (also see best training results (highlighted in bold) in Table 4).
Also it should be noted that the physical meaning of the set of parameters {V2} and
{V3} is not associated to this testing ground and this robot, which makes it possible to
use them in other projects on the same subject.
The results of testing of the trained models (both pose estimation and classiﬁcation)
demonstrate their performance in an essentially heterogeneous environment (the areas
of the surfaces are comparable with the robot’s dimensions). It is important to note that
18
A. Andrakhanov and A. Belyaev

the conditions for testing of models were much more complicated than the conditions
for their training. First, during the training there were no transition zones, when dif-
ferent wheels are simultaneously located at different surfaces. Secondly, the movement
in these zones was half of the path. Thirdly, the control actions on the motors were
changed dynamically (during the robot movement in Fig. 7) due to correction of both
the trajectory and the orientation angle (at the training stage, the control actions were
statically assigned from a speciﬁed set of motor velocities). As result, the parameters of
robot movement and, as a consequence, the readings of the sensors were signiﬁcantly
different from those observed at the training stage. Fourthly, the models (both for pose
estimation and classiﬁcation) were used as a feedback signal for motors control, which
could disturb stability of robot movement along the trajectory. Thus, testing was a very
serious test for bias of models. From this point of view, the results shown in Fig. 8
testify the practical possibility of models training based on GMDH for task of local
navigation in heterogeneous environment.
Future work is the development of methods and algorithms for applying both the
trained models and learning navigation system to construct the local navigation system
based on the readings of on-board sensors (in the absence of a GPS-signals) in
heterogeneous environment.
References
1. Rogers-Marcovitz, F., George, M., Seegmiller, N., Kelly, A.: Aiding off-road inertial
navigation with high performance models of wheel slip. In: Proceedings of the IEEE
International Conference on Intelligent Robots and Systems, pp. 215–222. IEEE, Vilamoura,
Portugal (2012)
2. Madhavan, R., Nettleton, E., Nebot, E., Dissanayake, G., Cunningham, J., Durrant-Whyte,
H., Corke, P., Roberts, J.: Evaluation of internal navigation sensor suites for underground
mining vehicle navigation. In: Proceedings of the IEEE International Conference on
Robotics and Automation, pp. 999–1004. IEEE, Detroit, MI, USA (1999)
3. Koch, J., Hillenbrand, C., Bems, K.: Inertial navigation for wheeled robots in outdoor
terrain. In: Fifth International Workshop on Robot Motion and Control, pp. 169–174. IEEE,
Dymaczewo, Poland (2005)
4. Bingbing, L., Adams, M., Ibañez-Guzmán, J.: Multi-aided inertial navigation for ground
vehicles in outdoor uneven environments. In: Proceedings of the IEEE International
Conference on Robotics and Automation, pp. 4703–4708. IEEE, Barcelona, Spain (2005)
5. Liu, Y., Xiong, R., Wang, Y., Huang, H., Xie, X., Liu, X., Zhang, G.: Stereo visual-inertial
odometry with multiple kalman ﬁlters ensemble. IEEE Trans. Indus. Electron. 63(10), 6205–
6216 (2016)
6. Dupont, E., Collins, E., Coyle, E., Roberts, R.: Terrain classiﬁcation using vibration sensors:
theory and methods. In: New Research on Mobile Robotics, pp. 1–41 (2010)
7. Ojeda, L., Cruz, D., Reina, G., Borenstein, J.: Current-based slippage detection and
odometry correction for mobile robots and planetary rovers. IEEE Trans. Robot. 22(2), 366–
378 (2006)
8. Iagnemma, K., Ward, C.: Classiﬁcation-based wheel slip detection and detector fusion for
mobile robots on outdoor terrain. Auton. Robots 26(1), 33–46 (2009)
9. Khaleghian, S., Taheri, S.: Terrain classiﬁcation using intelligent tire. J. Terramech. 71, 15–
24 (2017)
GMDH-Based Learning System for Mobile Robot Navigation
19

10. Andrakhanov, A.: Technology of autonomous mobile robot control based on the inductive
method of self-organization of models. In: Proceedings of the 7th International Symposium
“Robotics for Risky Environment – Extreme Robotics”, pp. 361–368. Saint-Petersburg,
Russia (2013)
11. Andrakhanov, A.: Navigation of autonomous mobile robot in homogeneous and heteroge-
neous environments on basis of GMDH neural networks. In: Proceedings of the 4th
International Conference on Inductive Modelling, pp. 133–138. Kiev, Ukraine (2013)
12. Tyryshkin, A., Andrakhanov, A., Orlov, A.: GMDH-based modiﬁed polynomial neural
network algorithm.In: GMDH-methodology and Implementation in C (With CD-ROM),
Imperial College Press, World Scientiﬁc, London (2015). ISBN: 978-1-84816-610-3
13. Robotino Manual (Order No. 544305). http://www.festo-didactic.com/ov3/media/customers/
1100/544305_robotino_deen2.pdf. Accessed 05 June 2017
14. Ivakhnenko, A., Ivakhnenko, G.: The review of problems solvable by algorithms of the
group method of data handling. Pattern Recogn. Image Anal. Adv. Math. Theory Appl. 5(4),
527–535 (1994)
15. Ivakhnenko, A., Ivakhnenko, G., Mueller, J.: Self-organization of neuronets with active
neurons. Int. J. Pattern Recogn. Image Anal. Adv. Math. Theory Appl. 4(4), 177–188 (1994)
16. Madala, H.R., Ivakhnenko, A.G.: Inductive Learning Algorithms for Complex System
Modeling. CRC Press, Boca Raton (1994). ISBN 0-8493-4438-7
17. Martin, S., Murphy, L., Corke, P.: Building large scale traversability maps using vehicle
experience. In: Desai, J.P., Dudek, G., Khatib, O., Kumar, V. (eds.) 13th International
Symposium on Experimental Robotics 2012, STAR, vol. 88, pp. 891–905. Springer,
Heidelberg (2013)
20
A. Andrakhanov and A. Belyaev

Model of the Objective Clustering Inductive
Technology of Gene Expression Proﬁles Based
on SOTA and DBSCAN Clustering Algorithms
Sergii Babichev1(B), Volodymyr Lytvynenko2, Jiri Skvor1, and Jiri Fiser1
1 Jan Evangelista Purkyne University, Usti nad Labem, Czech Republic
sergii.babichev@ujep.cz, jskvor@physics.ujep.cz, ithil@jf.cz
2 Kherson National Technical University, Kherson, Ukraine
immun56@gmail.com
http://www.sci.ujep.cz
Abstract. The paper presents the hybrid model of the objective cluster-
ing inductive technology based on complex using of the self-organizing
SOTA and the density DBSCAN clustering algorithms. The inductive
methods of complex systems analysis were used as the basis to implement
the objective clustering inductive technology of gene expression proﬁles.
To estimate the clustering quality for equal power subsets (include the
same quantity of pairwise similar objects) the complex multiplicative cri-
terion was calculated as the combination of the Calinski-Harabasz crite-
rion and WB-index. The external clustering quality criterion is calculated
as the normalized diﬀerence of the internal clustering quality criteria for
the equal power subsets. The ﬁnal decision concerning the determina-
tion of the optimal parameters of the clustering algorithm operation is
done based on the maximum value of the Harrington desirability func-
tion that takes into account both the character of the objects and the
clusters distribution in various clustering and the diﬀerence between clus-
tering, which are implemented on the equal power subsets. The studied
data grouping within the framework of the objective clustering inductive
technology was performed in two stages. Firstly, the studied gene expres-
sion proﬁles were grouped with the use DBSCAN clustering algorithm.
Then, the obtained set of gene expression proﬁles was divided into two
clusters using SOTA clustering algorithm. This step-by-step procedure of
the data clustering crates the conditions to save more useful information
for following data processing.
Keywords: Objective clustering · Inductive modeling
Gene expression proﬁles · Clustering quality criteria
SOTA clustering algorithm · DBSCAN clustering algorithm
1
Introduction
The gene regulatory network creation based on the gene expression proﬁles is
one of the current problems of the modern bioinformatics. The gene regulatory
c
⃝Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_2

22
S. Babichev et al.
network is a set of genes, which interact with each other to control the speciﬁc
cells functions. Qualitatively constructed gene regulatory network allows us to
study the inﬂuence of the corresponding group of genes or the individual genes on
functional possibilities of the biology objects in order to correct this process. The
gene expression proﬁles, which are obtained by DNA microarray experiments or
by RNA sequences technology are the basis to construct the gene regulatory
networks. High dimension of feature space is one of the distinctive peculiari-
ties of the studied proﬁles. About tens of thousands genes are contained in the
gene expression proﬁles. The creation of gene regulatory network based on the
whole dataset of the gene expression proﬁles is very diﬃcult problem because:
it requests large computer resources; it needs large time expenses to process the
information; the complexity of the obtained network complicates the interpreta-
tion of results. In this context, it is necessary ﬁrstly to divide the gene expression
proﬁles into subsets, each of which includes a group of genes that performs simi-
lar functions in the studied biological object. Biclustering technology is actual to
solve this problem nowadays. Implementation of this technology allows group-
ing the objects and the features according to their mutual correlation. So, in
the paper [14,17] authors provide a review of a large quantity of biclustering
approaches existing in the literature with analysis their advantages and disad-
vantages. In [7] authors have proposed and implemented the convex biclustering
method using gene expression proﬁles of the lung cancer patient. The authors
have shown the eﬃciency of the proposed method during simulation process.
However, it should be noted that one of the signiﬁcant problem of this technol-
ogy qualitative implementation is selection of the biclustering level during the
objects and the genes grouping. The qualitative validation of the obtained model
is another task, which has no solution nowadays. High dimension of the features
space promotes to the large quantity of the obtained biclusters. Limitation of
their quantity by removing of small biclusters leads to the loss of some useful
information. To solve this problem we propose the cluster-bicluster technology
the implementation of which involves two stage: clustering of the gene expression
proﬁles at the ﬁrst step and biclustering of the obtained clusters at the second
step. The reproducibility error is one of the current problems of the existing
clustering algorithms, in other words, successful clustering results obtained on
one dataset do not repeat while using another similar dataset. Reduction of this
error can be achieved by careful veriﬁcation of the obtained model using “fresh
information”, which was not used during the model making. A higher degree
of coincidence between the clustering results on the similar data corresponds
to a higher degree of the obtained model objectivity. This idea is the basis of
the objective clustering inductive technology, the main conception of which was
presented in [15] and further developed in [16,18,19]. The practical implemen-
tation of the objective clustering inductive technology is possible using various
clustering algorithms. The choice of the clustering algorithm is determined by
the structure and character of the studied data. The practical implementation
of this technology based on the agglomerative hierarchical and self-organizing
SOTA clustering algorithms were presented in [2,3]. One of the key conditions

Objective Clustering Inductive Technology
23
of successful implementation of this technology is careful determination of the
internal, the external and the complex balance clustering quality criteria, which
should take into account both the character of the objects grouping within the
clusters and the character of the clusters distribution in the features space. This
paper presents the research concerning the complex using of the density-based
DBSCAN (Density Based Spatial Clustering of Application with Noise)[9] and
self-organizing SOTA (Self Organizing Tree Algorithm)[8,10] clustering algo-
rithms within the framework of the objective clustering inductive technology.
The implementation of the proposed step-by-step procedure of the gene expres-
sion proﬁles grouping allows us to save more useful information of following data
processing.
The aim of the paper is development of the hybrid model of the objective
clustering inductive technology of gene expression proﬁles based on DBSCAN
and SOTA clustering algorithms.
2
Problem Statement
Let the initial dataset of the gene expression proﬁles is a matrix: A = {xij},
i = 1, . . . , n; j = 1, . . . , m, where n – is the quantity of genes observed, m – is the
quantity of the studied objects. The aim of the clustering process is a partition of
the genes expression proﬁles into non empty subsets of pairwise non-intersecting
clusters in accordance with the clustering quality criteria taking into account
the properties of the studied proﬁles:
K = {Ks}, s = 1, . . . , k; K1

K2

· · ·

Kk = A; Ki

Kj = ∅, i ̸= j,
where k – is the quantity of clusters, i, j = 1, . . . , k. The objective clustering
technology is based on the inductive methods of complex systems analysis, which
involves sequential enumeration of the clustering within a given range in order
to select from them the best variants. Let W – is a set of all admissible clustering
for given set A. The clustering is the best (an optimal) in terms of clustering
quality criteria QC(K) is the following condition is performed:
Kopt = arg min
K⊆W CQ(K)or Kopt = arg max
K⊆W CQ(K)
The clustering Kopt ⊆W is the objective if the diﬀerence of the objects and
clusters distribution in diﬀerent clustering for equal power subsets is minimal:
Kobj = arg min
K⊆W(QC(Kopt)A −QC(Kopt)B)
The architecture of the objective clustering inductive technology is presented
in Fig. 1. Implementation of the technology involves the following steps:
1. Problem statement. Clustering aim formation according to the stated task.
Studied data preprocessing and their formation as a matrix.

24
S. Babichev et al.
Fig. 1. Architecture of the objective clustering inductive technology
2. Determination of the aﬃnity function of the studied data. Division of the
initial dataset into two equal power subsets A and B using chosen aﬃnity
function. The equal power subsets include the same quantity of the pairwise
similar objects.
3. Choice of the clustering algorithm. Setup of its initial parameters. These
parameters are changed during the algorithm operation to obtain the diﬀerent
variants of the studied data clustering.
4. Data clustering on subsets A and B concurrently and clusters formation
within the range of the algorithm’s parameters change. If the clusters quan-
tity in various clustering diﬀers, it is necessary to change the setup of the
algorithm or to use another admissible clustering algorithm and to repeat the
step 5.
5. Calculation of the internal QCint, the external QCext and the complex balance
QCbal clustering quality criteria for the current clustering on equal power
subsets A and B.
6. Analysis of the complex balance clustering quality criterion values. In case of
absence of this criterion extremums or if their values are less than admissible
standards, choose another clustering algorithm and repeat the steps 4–7 of
this procedure.

Objective Clustering Inductive Technology
25
7. Fixation of the objective clustering in correspondents with the maximum
values of the complex balance clustering quality criterion.
The idea of the algorithm to divide the initial dataset of the objects Ω into
two equal power subsets ΩA and ΩB is stated in [15] and further developed in
[18]. Implementation of this algorithm involves the following steps:
1. Calculation of n × (n −1)
2
pairwise distances between the gene expression
proﬁles of the initial data. The result of this step is a triangular matrix of the
distances.
2. Allocation of the pairs of objects Xs and Xp, the distance between which is
minimal:
d(Xs, Xp) = min
i,j d(Xi, Xj);
3. Distribution of the object Xs to subset ΩA, and the object Xp to subset ΩB.
4. Repetition of the steps 2 and 3 for the remaining objects. If the quantity of
objects is odd, the last object is distributed to the both subsets.
The example of the objects and the clusters distribution in the objective clus-
tering inductive technology is shown in the Fig. 2. Obviously, that the best clus-
tering corresponds to the higher density of the objects distribution relative to
the mass centers of the clusters where these objects are and less density of the
clusters’ mass centers distribution in the feature space. Moreover, it is neces-
sary that the diﬀerence of the clustering results which are obtained on the equal
power subsets was minimal. Thus, to implement this technology it is necessary
to determine the gene expression proﬁles proximity metric, the internal, the
external and the complex balance clustering quality criteria.
Fig. 2. The example of the objects and the clusters distribution in the objective clus-
tering inductive technology in case of three clusters structure
3
Criteria to Estimate the Gene Expression Sequences
Proximity and Clustering Quality
It is obvious that the qualitative clustering corresponds to the high division abil-
ity of diﬀerent clusters and high density of the objects concentration inside the

26
S. Babichev et al.
clusters. Thus, it is necessary ﬁrstly to determine the proximity metric of the
gene expression proﬁles. The [4] presents the results of the research concerning
comparison of the three well know metrics eﬃciency to estimate the proximity
level of numeric vectors: Manhattan, Euclidean and Correlation distances. Eval-
uation of the eﬀectiveness of the studied metrics was performed using the model
data representing the gene expression proﬁles of the objects in two diﬀerent
clusters. Centers of the corresponding clusters are calculated by the formula:
CS =
1
NS
NS

i=1
xS
i ,
where NS is the quantity of gene expression proﬁles in cluster S, xS
i is i–th
sequence in cluster S. The research technique consists the next steps:
– calculation of the average distance dint from the proﬁles to the clusters’ cen-
ters, where these proﬁles are:
dint(XS,P , CS,P ) = 1
N (
NS

i=1
d(xS
i , CS) +
NP

j=1
d(xP
j , CP ));
– calculation the average distance dext from the proﬁles to the centers of the
neighbouring clusters:
dext(XS,P , CS,P ) = 1
N (
NS

i=1
d(xS
i , CP ) +
NP

j=1
d(xP
j , CS));
– calculation the relative coeﬃcient:
drel(XS,P , CS,P ) = dext(XS,P , CS,P )
dint(XS,P , CS,P );
It is obvious the higher value of the relative coeﬃcient corresponds to the higher
separating ability of the used proximity metric. In order to estimate the eﬀec-
tiveness of the metrics we used the data of the lung cancer patients of the
database Array Express [5], which includes the gene expression proﬁles of 96
patients, 10 of which were healthy and 86 patients were divided by the degree
of the health severity into three groups (Well, Moderate and Poor). Each of
the proﬁles includes 7129 genes expressions. Data preprocessing in order of gene
expression matrix formation was carried out accordingly to the technique, which
is presented in [1]. To choose the metrics of the gene expression proﬁles simi-
larity class of the health patient (10 proﬁles) and class of patients with Poor
state of health (21 proﬁles) were used. The results of the relative criteria values
distribution while using diﬀerent metrics to estimate the level of the gene expres-
sion proﬁles similarity are shown in Fig. 3. The analysis of the Fig. 3 allows us
to conclude that in case of the gene expression proﬁles the correlation metric
has higher separating ability in comparison with Euclid and Manhattan met-
rics because the values of the relative criterion which is calculated basing on
the correlation distance, are higher in comparison with the use of Euclid and
Manhattan distances.

Objective Clustering Inductive Technology
27
Fig. 3. The distribution of the relative criteria values using diﬀerent metrics to esti-
mate the gene expression proﬁles of the lung cancer patients: (a) Manhattan distance;
(b) Euclidean distance; (c) Correlation distance; (d) Average of all distances
3.1
Internal and External Clustering Quality Criteria
As it was noted herein before, it is obvious that the qualitative clustering cor-
responds to the high division ability of diﬀerent clusters and high density of the
objects concentration inside the clusters. Thus, the internal clustering quality
criterion should be complex and takes into account both the objects distribution
inside diﬀerent clusters and the clusters distribution in the features space. The
ﬁrst component of the complex internal criterion is calculated as average dis-
tance from the objects to the mass centers of the clusters, where these objects
are:
QCW = 1
N
K

s=1
Ns

i=1
d(xs
i, Cs)
The second component of this criterion, which takes into account the singularity
of the clusters distribution in the feature space, is calculated as an average
distance between the mass centers of the clusters:
QCB =
2
K(K −1)
K−1

i=1
K

j=i+1
d(Ci, Cj)
where K – is the quantity of clusters, N – is the general quantity of objects, Ns –
is the quantity of the objects in cluster s, xs
i – is the i-th vector in S cluster, Ci,
Cj and Cs – are the mass centers of the clusters i, j and S concurrently, d(· ) – is
the metric used to estimate the proximity level of the studied vectors. Various
combinations of these components allow obtaining the clustering quality criteria
for studied data subsets. During the simulation process the following internal
criteria to estimate the data grouping quality were used:

28
S. Babichev et al.
– Calinski-Harabasz [6]:
QCH = QCB(N −K)
QCW(K −1) ;
– WB index [20]:
QW B = KQCW
QCB ;
– Hartigan [12]:
QH = log2( QCB
QCW ).
In order to obtain more complete information about the eﬀectiveness of these
criteria operation the complex multiplicative criteria were calculated as follow:
QCCX1 = QCW B
QCCH
= K(K −1)QCW 2
(N −K)QCB2 ;
QCCX2 = QCH
QCCH
=
log2( QCB
QCW )(K −1)QCW
(N −K)QCB
;
QCCX3 = QCW BQCH = QCB(N −K)
QCW(K −1) log2( QCB
QCW );
QCCX4 = QCW BQCH
QCCH
= K(K −1)QCW 2
(N −K)QCB2 log2( QCB
QCW ).
The external clustering quality criterion is calculated as the normalized diﬀerence
of the internal clustering quality criteria for the equal power subsets A and B:
QCext(A, B) = |QCint(A) −QCint(B)|
QCint(A) + QCint(B) .
To estimate the eﬀectiveness of the internal and the external clustering quality
criteria within the framework of the objective clustering inductive technology
the gene expression proﬁles of the lung cancer patients were used [5]. Firstly, the
data were divided into two equal power subsets with the use of the algorithm
that had been presented in [15,18]. Then, each of the subsets was sequentially
divided into clusters from Kmin = 2 to Kmax = 5. In case of two-cluster struc-
ture in ﬁrst cluster there were the gene expression proﬁles of the healthy patients
(NORM) and gene expression of the patients with good state of health (WELL),
second cluster included the gene expression of the patients with poor (POOR)
and moderate (MODERATE) states. In case of three-cluster structure the ﬁrst
cluster contained the data of the healthy patients, the second – the data of the
patients with good state, the third cluster included the gene expression of the
patients with poor and moderate states. In case of four-cluster structure the ﬁrst
cluster contained the data of the healthy patients, the second – the data of the
patients with good state, the third cluster included the gene expression of the
patients with poor state and the fourth cluster contained the gene expression of

Objective Clustering Inductive Technology
29
the patients with moderate state. To obtain the ﬁve-cluster structure the gene
expression proﬁles of the patients with moderate state were divided into two
groups randomly. Objective clustering in this case corresponds to four-cluster
structure. To estimate the proximity level of the appropriate vectors the correla-
tion metric was used. Figure 4 shows the charts of the internal clustering quality
criteria for equal power subsets A and B versus the clusters quantity. Figure 5
presents the charts of the complex multiplicative internal criteria versus the clus-
ters quantity. Figure 6 shows the charts of the external clustering quality criteria,
which were calculated based on the internal criteria versus the clusters quantity.
Analysis of the charts which are shown in Fig. 4 and Fig. 5 allows us to conclude
that the internal clustering quality criteria give the same results in terms of the
local extremums existence. They have local extremums, which corresponds to
the objects division into 4 clusters, however, it should be noted that in case of
the QCH, QCCX2, QCCX3 and QCCX4 criteria use, the clustering, which cor-
respond to the objects division into 4 and 5 clusters are badly distinguished.
Analysis of the external criteria values, which are shown in Fig. 6, allows con-
cluding that in terms of the clustering objectivity (proximity level of the results,
which have been obtained on equal power subsets A and B) the QCH Hartigan
criterion and the QCCX2, QCCX3 complex criteria are ineﬀective, because they
have not a local minimums corresponding to the objects division into 4 clusters
(the objective clustering). The QCCX1 and QCCX4 criteria are the most infor-
mative to select the objective clustering, however, the QCCX1 criterion is more
preferable because it has more expressed local minimum, which corresponds to
four clusters existence in the obtained clustering.
Fig. 4. Charts of the internal clustering quality criteria versus the clusters quantity:
(a) WB index; (b) Calinski-Harabasz criterion; (c) Hartigan criterion

30
S. Babichev et al.
Fig. 5. Charts of the complex multiplicative internal clustering quality criteria versus
the clusters quantity
Fig. 6. Charts of the external clustering quality criteria versus the clusters quantity
3.2
Complex Balance Clustering Quality Criterion
It is obvious that the objective clustering corresponds to the minimum values of
the internal and the external clustering quality criteria. However, it is possible
that the extremums of these criteria correspond to diﬀerent clustering. Thus,
it is necessary to determine the complex balance clustering quality criterion
that takes into account both the character of the objects and the clusters dis-
tribution in various clustering and the diﬀerence between clustering, which are
implemented on the two equal power subsets. To calculate the complex balance
clustering quality criterion the Harrington desirability function [11] was used.
The implementation of this function involves transformation of the scales of the
internal and the external criteria into reaction scale the values of which are

Objective Clustering Inductive Technology
31
changed linearly within the range from −2 to 5. Then the private desirabilities
of the appropriate criteria are calculated by the formula:
d = exp(−exp(−Y ))
The chart of the Harrington desirability function versus the reaction index Y is
shown in Fig. 7. The transformation of the criteria scales into the reaction scales
were performed by linear equation:
Y = a −b · QC
The parameters a and b are determined empirically. The general desirability
index value is calculated as geometric average of the private desirabilities indexes:
D =
n




n

i=1
di
In case of the objective clustering inductive technology the general Harrington
desirability index was used as the complex balance criterion:
QCbal =
3	
QCint(1) + QCint(2) + QCext
The largest value of the complex balance criterion corresponds to the best para-
meters of the clustering algorithm operation.
Fig. 7. Chart of Harrington desirability function
4
Implementation of SOTA Clustering Algorithm Within
the Framework of the Objective Clustering Inductive
Technology
The SOTA clustering algorithm (Self-Organizing Tree Algorithm) [8] represents
a type of self-organizing neural networks based on the Kohonen maps and the

32
S. Babichev et al.
Fritzke algorithm of the spatial cell structure growing [10]. Opposed to the
Kohonen maps that reﬂect a set of high dimensional input data on the elements
of the two-dimensional array of small dimension, the SOTA algorithm generates a
binary topological tree. The Fritzke algorithm performs self-organization of out-
put nodes of the network in such a way that the quantity of the nodes increases
in the ﬁeld of the higher density of objects concentration and decreases in the
ﬁeld of the lower density. Two parameter are determined the eﬀectiveness of the
SOTA clustering algorithm operation: weight coeﬃcient of the sister’s cell (scell)
and maximum divergence coeﬃcient value. The weight coeﬃcients of the parent’s
and winner’ cells are determined automatically: pcell = scell · 5; wcell = pcell · 2.
This ratio is recommended by the algorithm’s authors. The block-scheme of the
objective clustering model based on the SOTA clustering algorithm is shown in
Fig. 8. Implementation of this model involves the following steps:
1. Presentation of the studied data as a matrix n×m, where n – is the quantity
of the studied proﬁles or the quantity of the rows and m – is the quantity of
the objects or the quantity of the columns.
2. Division of the initial dataset into two equal power subsets.
3. Setup of the SOTA clustering algorithm. Setting of the initial value of scell
weight coeﬃcient, the interval and the step of its change.
4. Data clustering on the equal power subsets A and B concurrently. Clusters
formation and the internal, the external and the balance clustering quality
criteria calculation within a range of the interval of the algorithm’s parameter
change.
5. Fixation of the optimal scell parameter corresponding to the maximum value
of the balance criterion.
6. Setting of the initial value of the maximum divergence parameter, the interval
and the step of its change. Repetition of the step 4 of this algorithm. Fixation
of the optimal maximum divergence parameter.
7. Full data clustering by the SOTA clustering algorithm using the optimal
parameters of the algorithm operation.
5
Implementation of DBSCAN Clustering Algorithm
Within the Framework of the Objective Clustering
Inductive Technology
DBSCAN clustering algorithm (Density Based Spatial Clustering of Application
with Noise Algorithm) [9] initially needs two parameters: EPS-neighborhood
of points (EPS) and the least quantity of the points within EPS-neighborhood
(MinPts). Choice of these parameters determines the character of the studied
objects grouping during the algorithm operation. In [9] authors proposed the
technology based on the sorted 4-dist graph. However, the implementation of
this technology does not allow determination of EPS and MinPts values exactly
and this fact inﬂuences the quality of the algorithm operation. To determine

Objective Clustering Inductive Technology
33
Fig. 8. Block-scheme of the objective clustering model based on the SOTA clustering
algorithm
EPS and MinPts values we propose to use the objective clustering inductive
technology. The structural scheme of the objective clustering model based on
DBSCAN clustering algorithm is presented in Fig. 9. Implementation of this
model involves the following steps:
1. The matrix of the studied data formation. The matrix contains n rows or
studied proﬁles and m columns or the objects.
2. Division of the initial dataset into two equal power subsets.
3. The distance matrix between studied proﬁles for both subsets is calculated
using correlation distance. This distance matrix is the input matrix for the
next step of the algorithm operation.
4. Setup of DBSCAN clustering algorithm, choice of the intervals and steps of
EPS and MinPts change.
5. Fixation of MinPts value (MinPts = 3). Initialization of EPS = EPSmin.

34
S. Babichev et al.
Fig. 9. Block-scheme of the objective clustering model based on the DBSCAN cluster-
ing algorithm
6. Data clustering on the two subsets A and B using DBSCAN algorithm in
range from EPSmin to EPSmax. Clustering ﬁxation at each step.
7. The internal, the external and the complex balance clustering quality criteria
is calculated at each step of the algorithm operation.
8. Analysis of the balance criterion values. Fixation of the optimal value EPS,
which corresponds to the maximum value of the balance clustering quality
criterion.
9. Data clustering on the two equal power subsets A and B in the range from
MinPtsmin to MinPtsmax. Clustering ﬁxation at each step.
10. Repetition of the steps 7 and 8 of this algorithm for MinPts values. Fixation
of EPS and MinPts optimal values which correspond to the maximum of the
complex balance clustering quality criterion.

Objective Clustering Inductive Technology
35
11. Studied data clustering using obtained parameters of DBSCAN algorithm
operation.
6
Experiment, Results and Discussion
To estimate the eﬀectiveness of the algorithm’s operation within the framework
of the proposed technology the genes expressions of the lung cancer patients
[5] were used. Firstly, the data were divided into two equal power subsets with
the use of the algorithm that was presented herein before The simulation was
carried out using software R [13]. Figure 10 shows the charts of the internal,
the external and the complex balance criteria versus EPS-neighborhood values
for gene expression proﬁles of the lung cancer patient. Two thousand proﬁles
were studied during the experiment. Firstly, these proﬁles were divided into two
equal power subsets using correlation metric. Then the dissimilarity matrices for
all pairs of the studied objects of the both subsets using correlation distance
Fig. 10. Charts of the internal, the external and the complex balance clustering quality
criteria versus EPS-neighborhood values for gene expression proﬁles of lung cancer
Fig. 11. Charts of the complex balance clustering quality criterion versus MinPts values
for gene expression proﬁles of lung cancer

36
S. Babichev et al.
was calculated. These dissimilarity matrices were used as the input data for
next steps of DBSCAN algorithm operation. Three values of EPS-neighborhood
were selected based on the maximum values of the complex balance criterion
which is shown in Fig. 10: EPS1 = 0,13; EPS2 = 0,17; EPS3 = 0,44. Figure 11
shows the charts of the complex balance criteria for selected EPS versus MinPts
values. The analysis of the charts allows concluding that the best clustering in
terms of maximum value of the complex balance clustering quality criterion is
achieved using the following parameters of DBSCAN algorithm: (a) EPS = 0,13,
MinPts = 3; (b) EPS = 0,17, MinPts = 8; (c) EPS = 0,44, MinPts = 6. However,
the detail analysis of the obtained results has shown what in the ﬁrst and in the
second cases there were diﬀer clusters quantity in the obtained clustering. Only
in case of EPS = 0,44 and MinPts = 6 both clustering contained the same quan-
tity of clusters. The initial dataset contained 2000 gene expression proﬁles. In
this case the studied data were divided in such a way: the ﬁrst cluster contained
1663 proﬁles, in the second cluster there were 16 proﬁles, there were 321 proﬁles
in the third cluster. The objects in the third were identiﬁed as the noise compo-
nent. The results of the simulation have shown also that the largest quantity of
the gene expression proﬁles are concentrated in the ﬁrst cluster. This fact can
be explained by the fact that these genes deﬁne the main processes, which are
carried out in biological organisms, therefore they have more correlation between
each other to compare with genes in other clusters or genes, which are identi-
ﬁed as the noise. The results of the internal criteria for the equal power subsets
A and B, the external and the balance criteria versus the weight parameter of the
sisters cell using SOTA clustering algorithm are presented in Fig. 12. The max-
imum divergence value in this case E = 0,001 was taken. As it can be seen from
Fig. 12, the internal clustering quality criteria CX 1 and CX 2, which have been
calculated on equal power subsets A and B do not allow determining the optimal
scell value corresponding the objective clustering of the studied data. The exter-
nal clustering quality criterion CQE has several local minimums corresponding
to the successful grouping of the studied vectors. However, the analysis of the
complex balance criterion values, which takes into account both the internal and
the external criteria, allows us to conclude that the best clustering corresponds
to the scell = 0,001. In this case the 6659 proﬁles were divided into two clusters.
The ﬁrst cluster contained 4276 proﬁles and the second – 2383 ones. Variation
of the maximum divergence value in the range from 0,001 to 1 has not changed
the obtained results. The obtained results create the conditions to create the
step-by-step technology of gene expression proﬁles grouping at early stage of the
gene regulatory network construction. Objective clustering based on DBSCAN
algorithm allows us to select the genes with higher level of their mutual correla-
tion. The noise component also is removed at this step. Then at the second step
of the proﬁles grouping the selected proﬁles are divided into two group using
SOTA clustering algorithm. At the third step of the gene expression proﬁles
grouping the biclustering technology is implemented on the obtained clusters.
To our mind the implementation of the proposed technology allows saving more
useful information to follow create the gene regulatory network.

Objective Clustering Inductive Technology
37
Fig. 12. The internal, the external and the balance criteria versus the weight coeﬃcient
of the sisters cell value of the SOTA clustering algorithm
7
Conclusion
The paper presents the model of the objective clustering inductive technology
of gene expression proﬁles based on DBSCAN and SOTA clustering algorithm.
The implementation of this technology involves the concurrent data clustering
on the two equal power subsets which include the same quantity of the pair-
wise similar objects. The correlation metric was used as the proximity metric
of the gene expression proﬁles. The internal clustering quality criteria take into
account both the character of objects distribution within clusters relative to
the mass center of the appropriate cluster and the character of the clusters
distribution in the features space. The external clustering quality criteria were
calculated as a normalized diﬀerence of the internal clustering quality criteria,
which were calculated on the equal power subsets A and B. The simulation
process involved sequential evaluation of the internal and external criteria for
clustering during increase the clusters quantity from Kmin to Kmax. The objec-
tive clustering corresponded to the global minimum of the external clustering
quality criterion. The gene expression sequences of the patients of the database
Array Express, which were investigated on the lung cancer, were used as the
experimental data. The quantity of the clusters was changed from 2 to 5 dur-
ing clustering process. The objective clustering corresponded to the four-cluster
structure. The results of the simulation have shown that the complex multiplica-
tive criterion, which is the combination of the WB-index and Calinski-Harabasz
criterion is the most eﬀective to determine the objective clustering. This criterion
has the clearly expressed minimum corresponding to the four-cluster structure
both in case of estimation of the character of the objects and the clusters dis-
tribution in the equal power subsets and in case of estimation of the result of
clustering diﬀerence on these subsets. The external criterion was calculated as
the normalized diﬀerence of the internal clustering quality criteria which are
calculated on the two equal power subsets. The general Harrington desirabil-
ity index based on the internal and external criteria was used as the complex
balance clustering quality criterion. Determination of optimal parameters of the

38
S. Babichev et al.
used algorithm operation has been performed based on the maximum value of
the complex balance clustering quality criterion during the algorithm operation.
The results of the simulation have shown the high eﬃciency of the proposed
technology. In case of DBSCAN clustering algorithm using the noise component
in terms of density of the objects distribution was selected during algorithm
operation. Implementation of the proposed technology also allows us to group
the gene expression sequences based on the similarity of their proﬁles. The gene
expression sequences with high correlation coeﬃcient were distributed into one
cluster. This fact allows us to select the groups of the gene expression sequences,
which determine the main processes in the biological organisms in order to study
and to correct these processes. In case of SOTA clustering algorithm using the
studied gene expression proﬁles were divided into two groups. This fact create
the conditions to create the step-by-step technology of gene expression proﬁles
grouping at early stage of the gene regulatory network construction. Objective
clustering based on DBSCAN algorithm allows selecting the genes with higher
level of their mutual correlation. Then the selected proﬁles are divided into two
group using SOTA clustering algorithm. The further perspective of the authors’
research is the development of the hybrid technology of the step-by-step gene
expression proﬁles grouping based on the complex use of the objective clustering
and biclustering technologies.
References
1. Babichev, S., Kornelyuk, A., Lytvynenko, V., Osypenko, V.: Computational analy-
sis of gene expression proﬁles of lung cancer. Biopolymers Cell 32(1), 70–79 (2016).
http://biopolymers.org.ua/content/32/1/070/
2. Babichev, S., Taif, M.A., Lytvynenko, V.: Inductive model of data clustering based
on the agglomerative hierarchical algorithm. In: Proceeding of the 2016 IEEE First
International Conference on Data Stream Mining and Processing (DSMP), pp. 19–
22 (2016). http://ieeexplore.ieee.org/document/7583499/
3. Babichev, S., Taif, M.A., Lytvynenko, V., Korobchynskyi, M., Taif, M.A.: Objec-
tive clustering inductive technology of gene expression sequences features. In:
Proceeding of the 13th International Conference Beyond Databases, Architec-
tures and Structures. Communication in Computer and Information Science,
Ustron, Poland, pp. 359–372 (2017). https://link.springer.com/content/pdf/10.
1007/978-3-319-58274-0 29.pdf
4. Babichev, S., Taif, M.A., Lytvynenko, V., Osypenko, V.: Criterial analy-
sis of gene expression sequences to create the objective clustering induc-
tive
technology.
In:
Proceeding
of
the
2017
IEEE
37th
International
Conference
on
Electronics
and
Nanotechnology
(ELNANO),
pp.
244–249
(2017).
http://apps.webofknowledge.com/full record.do?product=WOS&search
mode=GeneralSearch&qid=3&SID=U2bB7H8kqTrSyZ2eAKs&page=1&doc=2
5. Beer,
D.,
Kardia,
S.,
et
al.:
Gene-expression
proﬁles
predict
survival
of
patients
with
lung
adenocarcinoma.
Nat.
Med.
8(8),
216–224
(2002).
https://www.ncbi.nlm.nih.gov/pubmed/12118244
6. Calinski, T., Harabasz, J.: A dendrite method for cluster analysis. Commun. Stat.
3, 1–27 (1974)

Objective Clustering Inductive Technology
39
7. Chi, E., Allen, G., Baraniuk, R.: Convex biclustering. Biometrics 73, 10–19 (2016).
http://onlinelibrary.wiley.com/doi/10.1111/biom.12540/full
8. Dorazo, J., Corazo, J.: Phylogenetic reconstruction using an unsupervised growing
neural network that adopts the topology of a phylogenetic tree. J. Mol. Evol. 44(2),
226–259 (1997). https://www.ncbi.nlm.nih.gov/pubmed/9069183
9. Ester, M., Kriegel, H., Sander, J., Xu, X.: A density-based algorithm for discov-
ering clusters in large spatial datasets with noise. In: Proceedings of the Second
International Conference on Knowledge Discovery and Data Mining, Portland, pp.
226–231 (1996). http://dl.acm.org/citation.cfm?id=3001507
10. Fritzke,
B.:
Growing
cell
structures
a
self-organizing
network
for
unsu-
pervised
and
supervised
learning.
Neural
Netw.
7(9),
1441–1460
(1994).
http://www.sciencedirect.com/science/article/pii/0893608094900914
11. Harrington, J.: The desirability function. Ind. Qual. Control 21(10), 494–498
(1965). http://asq.org/qic/display-item/?item=4860
12. Hartigan,
J.:
Clustering
Algorithms.
Wiley,
New
York
(1975).
http://dl.acm.org/citation.cfm?id=540298
13. Ihaka,
R.,
Gentleman,
R.:
R:
a
linguage
for
data
analysis
and
graphics.
J.
Comput.
Graph.
Stat.
5(3),
299–314
(1996).
http://www.tandfonline.com/doi/abs/10.1080/10618600.1996.10474713
14. Kaiser, S.: Biclustering: Methods, Software and Application (2011). https://edoc.
ub.uni-muenchen.de/13073/
15. Madala,
H.,
Ivakhnenko,
A.:
Inductive
Learning
Algorithms
for
Com-
plex
Systems
Modeling,
pp.
26–51.
CRC
Press,
Boca
Raton
(1994).
http://www.gmdh.net/articles/theory/ch2.pdf
16. Osypenko, V.V., Reshetjuk, V.M.: The methodology of inductive system analysis
as a tool of engineering researches analytical planning. Agric. Forest Eng. 58, 67–71
(2011). http://annals-wuls.sggw.pl/?q=node/234
17. Pontes,
B.,
Giraldez,
R.,
Aguilar-Ruiz,
J.S.:
Biclustering
on
expression
data:
a
review.
J.
Biomed.
Inf.
57,
163–180
(2015).
https://www.ncbi.nlm.nih.gov/pubmed/26160444
18. Sarycheva, L.: Objective cluster analysis of data based on the group method of
data handling. Problems of Control and Automatics 2, 86–104 (2008)
19. Stepashko, V.: Elements of the Inductive Modeling Theory, State and Prospects of
Informatics Development in Ukraine, pp. 471–486. Scientiﬁc Thought, Kiev (2010).
Monograph/Team of autors
20. Zhao, Q., Xu, M., Frnti, P.: Sum-of-squares based cluster validity index and sig-
niﬁcance analysis. In: Proceeding of International Conference on Adaptive and
Natural Computing Algorithms, pp. 313–322 (2009). https://link.springer.com/
chapter/10.1007/978-3-642-04921-7 32

From Close the Door to Do not Click and Back. Security
by Design for Older Adults
Bartlomiej Balcerzak
(✉), Wieslaw Kopec, Radoslaw Nielek, Kamil Warpechowski,
and Agnieszka Czajka
Polish-Japanese Institute of Information Technology, ul. Koszykowa 86,
02-008 Warsaw, Poland
b.balcerzak@pjwstk.edu.pl
Abstract. With the growing number of older adults who adopt mobile tech‐
nology in their life, a new form of challenge faces both them, as well as the
software engineering communities. This challenge is the issue of safety, not only
in the context of risk older adults already face online, but also, due to the mobile
nature of the used applications, real life safety issues raising from the use of on-
line solutions. In this paper, we wish to use a case study they conducted in order
to address this issue of interrelating on-line and real life threats. We describe how
the observation from the case study relate to the collected body oﬀ knowledge in
the relevant topic, as well as propose a set of suggestion for improving the design
of applications in regards to addressing the issue of older adults safety.
1
Introduction
The trend of population aging is currently an undisputed fact, observed throughout the
globe, in developed and developing countries alike [23]. By 2050 over 20% of the popu‐
lation of the USA is projected to be 65+ [27], other developed countries, such as Japan
are also expected to become aging populations by that time. Although this trend has
been deemed a negative trajectory for population development [11], and a giant chal‐
lenge for social security frameworks, and in turn a major burden for the state, research
shows that this trend may bear out other results. For instance, Spijker et al. argument
that although demographic data are true in reality number of older adults requiring
assistance in UK and other countries actually been falling in recent years [36]. This point
of view is supported by Sanderson et al. [35] who point out that metrical age qualifying
to being older adult will increase in coming years.
In parallel to the mentioned trend, an increase in smartphone and on-line tool usage
among older adults can also be observed [43]. This trend of increasing usage of mobile
ICT can be observed worldwide, and already has inspired many researchers and
designers to introduce new and creative solutions tailored towards the needs of the older
adults [18]. When considered in relation to the ﬁndings suggesting that in future aging
societies, older adults may be more aﬄuent, healthy and tech savvy than their current
counterparts. This increase in use of mobile technology, opens up new possibilities for
addressing the issue of population aging, and creating solutions tailored to a new
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_3

growing customer base. With these new possibilities however, also come new chal‐
lenges. The demographics increase in the share of mobile solutions users leads to new,
and previously not addressed issues such as those related to the older adults safety.
Research shows that older adult are less likely to be tech savvy [9], and often as a
result, are more prone to suﬀer from threats related to their safety. Moreover, older adults
tend to be more aware of the risks they face on-line [1]. Due to the fact that on-line
solutions such as social media, communication and ﬁnancial services or other mobile
based apps increasingly blur the line between the online and real life situation, it is
understandable that, when addressing the safety issues of older adults, one must consider
both aspects of safety. Discussing this topic is our main aim in this paper.
For the sake of this paper, we made an explicit distinction between two aspects of
older adults safety; their on-line security, and real life safety. The former, as the name
suggest, refers to risk the older adult may face when interacting with ICT (i.e. identity
theft, scamming, phishing, spread of malware and spam), the latter on the other hand
deals with risks connected with interactions made in the real world, such as theft, assault,
burglary, traﬃc accidents on so on. Although these two aspects were deeply reviewed
by researchers (with work by [7, 9] serving as good examples), there has been no attempt,
to the best of our knowledge, the analyze the interaction between the two aspects, let
alone to analyze how developers can address them in the design process.
Therefore we decided, based also on previous endeavours and activities with older
adults in our LivingLab, to use a case study of an intergenerational design team during
a Hackathon event conducted at the Polish-Japanese Academy of Information Tech‐
nology in Warsaw, Poland (PJAIT). In this study we addressed the issue of the connec‐
tion between on-line security and real life safety in the design process by using partic‐
ipatory design approach. By describing the design process and group dynamics, as well
as the ﬁnal product, we wish to open further discussion, and provide some deeper insight
into a topic that, although important in lieu of the aforementioned trend in stronger
connection between the on-line and oﬀ-line, was not yet fully addressed by the research
community. In this paper we provide more comprehensive approach to the security of
older adults based on the concept described in case study section which we previously
brieﬂy outlined in our conference report [3].
The rest of the paper is organized as follows. In the Related work section, we provide
a comprehensive description of ﬁndings in regards to on-line security and real life safety
of older adults, as well as security design in software engineering, as well as participatory
design, and older adults motivation for taking part in design processes and similar tasks.
In the section LivingLab insight we describe a broader context of their empirical studies
and research activities with older adults in PJAIT LivingLab relevant to the above
mentioned topics. The Case study section present in detail the design session conducted
during the Hackathon, the speciﬁc features of the application created by the intergen‐
erational team, as well as the design process itself. The Conclusions and Future Work
section details how the observations made during the design session interact with the
ﬁndings related to the topics of on-line security and real life safety.
Security by Design for Older Adults
41

2
Related Works
2.1
Older Adults and On-line Security
According to studies done by [9] older adults are, on average less knowledgeable and
aware of on-line risks than younger adults. Also, as shown by [1], older adults show
limited trust to on-line technology, however this eﬀect diminishes with usage of ICT.
Another dimension crucial with on-line security is trust. Studies done by [19] as well as
[9] show that trust among older adults is not aﬀected by age, and relies more on expe‐
rience that comes with the use of ICT. Our previous study on location-based game with
mobile ICT technology [14] which we present brieﬂy in next section also showed the
connection between direct ICT usage and self-conﬁdence of older adults in context of
mobile technologies. Therefore, the design of applications that address the issue of trust
is important. The topic of safety, also plays a role in research related to applying smart
house technologies as tools for enabling older adult independence [28]. A topic also
related to online safety is the issue using data and text mining techniques for the detection
f scamming and the credibility of on-line material [13, 39].
2.2
Older Adults and Real Life Safety
For the sake of this paper, when reviewing the real-life hazards faced by older adults,
the authors will focus on a speciﬁc aspects of safety, which in the literature is refereed
to as neighborhood safety. This refers to risk related to social interaction in the nearest
environment of the older adult. As the comprehensive literature review conducted by [7]
suggests, ﬁve main themes constitute the overall framework of neighborhood safety.
These are: general neighborhood safety; crime-related safety; traﬃc-related safety; fall-
related safety; and proxies for safety (e.g., vandalism, graﬃti). This literature review
provides also a deep and comprehensive review of what main themes are the object of
focus among researchers interested in real life hazards and their perception by older
adults. It is noteworthy, that in the studies used in the review, main focus was put on
health issues and their relation with the feeling of safety. This is a theme which appears
in works such as those done by [40], where the authors show that physical health is a
correlate with the older adults sense of safety. Another interesting ﬁeld of study is related
to older adults emotion in relation with surrounding environment. We conducted a
preliminary study on mapping senior citizens’ emotions with urban space [24], which
is also connected with our previous work, namely the mentioned mobile game [14]
which was related to the topics of wellbeing and happiness of older adults. This issue is
a vital subject of many other interesting research, i.e. a longitudinal study on a sample
of over 10,000 older adults that pointed out that perception of one’s psychological well
being aﬀects the older adults feeling of safety [5]. To the best of the authors knowledge,
little focus was placed on interaction between thus deﬁned levels and perceptions of
neighborhood safety and use of mobile devices and apps. Therefore, besides our previous
endeavours mentioned above, we decided also to pursue this topic in exploratory in-
depth interviews with older adults form our LivingLab, which are elaborated in the next
section.
42
B. Balcerzak

2.3
User Participation and Interaction
Fundamental idea relevant for this case is connected with a concept of usercenter design,
as a part of general idea of human focused approach related to another important idea:
participatory design, sometimes called co-design. While there are diﬀerent origins for
the two latter terms they actually refer to the same idea of bottom-up approach which is
widely used besides software engineering from architecture and landscape design to
healthcare industry [37]. All those concepts put human in the center of designing process,
however, there is a small, but signiﬁcant distinction between user-center design and
participatory design: the former refers the process of designing FOR users, while the
latter WITH users [33, 34]. From the point of view of this study especially important
are concepts that are related to user competences and empowerment provided e.g. by
Ladner [16].
Another case is the work done by [41], describing a cooperation between seniors and
preschool children in a design task. An interesting observation made in this research
was the fact that both groups needed equally time together, and time in separation in
order to function properly. The broader context for these topics is covered by the contact
theory, a widely recognized psychological concept coined by Allport [2] and developed
for many years by other, e.g. Pettigrew [29]. According to the theory the problem of
intergroup stereotypes can be faced by intergroup contact. However, there is several
condition for optimal intergroup contact, but many studies proved that intergroup contact
is worthwhile approach since it typically reduces intergroup prejudice [30]. We had also
explored the topic of intergroup interaction in our various studies including mentioned
intergenerational location-based game and the hackathon based on previous works and
tools i.e. by Rosencranz [31].
2.4
Volunteering of Older Adults
The eﬀects of volunteering has on older adults is a developing ﬁeld of study within
various disciplines. The consensus that can be reached throughout various studies, is
that participation in volunteer activity has many positive eﬀects on the elderly. [17]
claim that older adults who frequently volunteer in various activities, tend to have
improved physical and mental health, compared to those who do not participate in
volunteering. The work of [22] extends this correlation to wellbeing (with volunteering
being correlated with higher levels of well-being) this is also reinforced by [8, 10]. This
is crucial since studies also show that in some regards, elders are more likely to be
engaged in volunteer activity [21].
Motivation for volunteering among older adults is also important. In research done
by [12] involving the comparison of older and younger adults when participating in a
crowd sourcing task of proof reading texts in Japanese, it was shown that older adults
where successfully motivated by the use of gamiﬁcation techniques within the task,
which means according to the most widespread deﬁnition of gamiﬁcation by Deterding
the use of game design elements in non-game contexts [6]. However, a similar task,
conducted by [4] on a group of American seniors showed a slightly diﬀerent pattern,
where older adults were bored by the task, and did not comprehend its’ signiﬁcance.
Security by Design for Older Adults
43

Nevertheless gamiﬁcation is strictly connected with motivation and therefore inevitably
leads to the psychological context, e.g. Zichermann claims that gamiﬁcation is 75% of
psychology and 25% of technology [42]. At this point it is worth mentioning that
according to the latest reviews more and more solutions are based on solid psychological
theories and frameworks [20]. One of the most important theoretical approach in the
ﬁeld of motivation is self-determination theory (SDT) developed by Ryan and Deci [32].
It is based on subtheories formerly developed by the authors of SDT: cognitive evalu‐
ation theory (CET) related to the intrinsic motivation and organismic integration theory
(OIT) related to extrinsic motivation. The theory was proven to be eﬀective to the elderly
as well, e.g. by Vallerand [38]. We had explored the topic of older adults volunteering
and motivation in our research i.e. Wikipedia content co-creation [25].
3
LivingLab PJAIT Insights
In this section we provide some insights from our experience with older adults within
Polish-Japanese Academy of Information Technology LivingLab. Further details of our
LivingLab, it’s origin and design as well as older adults activities are provided in separate
description [15].
3.1
About LivingLab PJAIT
The term LivingLab was coined by William Mitchell from MIT [26] and was used to
refer to the real environment, like a home or urban space, where routines and everyday
life interactions of users and new technology can be observed and recorded to foster the
process of designing new useful and acceptable products and services. The idea of
LivingLab is therefore inherently coupled with broad concept human-centered approach
described in previous section.
LivingLab at the Polish-Japanese Academy of Information Technology is a longterm
framework project, whose goals are related to social inclusion and active engagement
of the elderly in social life by facilitating the development of ICT literacy among them
as well as creating an active community of stakeholders who are both the beneﬁciaries
and enablers of research into their problems. This framework project has been estab‐
lished in a long-term partnership with the City of Warsaw.
Throughout recent years we have organized a number of activities for older adults
focused on various research areas relevant to the topics of this article as we mentioned
in related work section. In particular during an intergenerational location based game
older adults performed various everyday mobile ICT tasks, such as connecting to Wi-
Fi, browsing the information, scanning QR codes or taking panoramic photos with the
assistance of their teammates. On the other hand, the younger participants beneﬁted from
the background knowledge about the city and its history of the elderly. Thus a positive
bi-directional intergenerational interaction was observed alongside positive older adults
self-awareness of the technology in context of physical activities and well-being. In
other LivingLab activities, including on-line courses and crowdsourcing tasks as well
44
B. Balcerzak

as real life workshops and activities i.e. devoted to both application and content co-
creation we also noticed that security issues are important but a bit vague area for older
adults. Based on the literature review we decided to conduct a more in-depth qualitative
research described in next subsection.
3.2
Older Adults and Security
Based on literature and observations from LivingLab activities, workshops and consul‐
tations we decided to obtain a more in-depth insight in the topic, including both relevant
perspectives: on-line security and real life safety.
We used individual in-depth interviews in order to extract additional security
insights. In total, we conducted four such interviews with older adults from our
LivingLab aged 65+ , two less technology advanced (female participants P1 and P2)
and a pair of more advanced older adults (female P3 and male P4). To obtain more in-
depth information from them we decided to conduct individual semistructured inter‐
views related to several topics: Internet and mobile application usage, endeavours
towards on-line security and real life safety alongside with perception of potential inter‐
action between those two realms.
Based on interviews we ﬁgured three diﬀerent strategies toward security: caution,
separation and awareness. While awareness is related to the more technologically
advanced older adults, the two former strategies are interesting since they are represented
by older adults with lower ICT literacy. Generally speaking the caution strategy repre‐
sented by P1 was based on carefulness in both real life and on-line activities while
separation strategy represented by P2 was based on strong conviction on non-transition
between virtual and real realm. In particular P2 claimed that these two worlds should
not be comined.
First we asked participants about their Internet and mobile experience. There are
three major areas of their on-line interests:
– doing everyday tasks, like paying the bills –
keeping in touch with family and friends –
source of information. 
The latter was the most extensive category and included various topics from health and
medical issues (i.e. drugs and food ingredients, dietary information), through transpor‐
tation to cultural and political news. In this context P2 claimed that The world has gone
so far that it is diﬃcult to live without the Internet these days.
In order to obtain insight about on-line security we asked the participants about
securing themselves in virtual space. In this context participants were aware of anti-virus
protection as well trust issues, identity theft and identity veriﬁcation (the need of veri‐
ﬁcation the identity of on-line entities i.e. shops and companies). However, here we
observed the major diﬀerence in separate-world approach. P2 explicitly stated, that there
is no direct transition between the virtual and real realms. This was directly connected
Security by Design for Older Adults
45

with careless websurﬁng. Moreover P2 wasn’t afraid of her identity theft based on the
claim I am no one special, I am an ordinary Smith.
We also asked about the safeness endeavor made by the participant in their everyday
life. Besides personal physical safety measures like traﬃc safety, observation of the
surroundings and other persons they pointed out two major areas connected with virtual
space: ﬁnance and health. However they cannot establish the connection between the
two realms. In particular, besides techniques of safe cash carrying (P1: one can deposit
the cash in various piece of garment) they stated that in general they use credit cards
instead of cash and they deposit money in bank. In reference to health, besides physical
activity they pointed out healthy diet and food and medical ingredients veriﬁcation.
As we can see some factors from virtual space activities can be directly mapped and
connected with the real world. However, it was diﬃcult to the participants to ﬁnd the
connection by themselves. More technologically advanced older adults (P3 and P4),
were more aware of two realms i.e. bank account protection and identity theft alongside
with interface as a vital concept between virtual and real world. On the other hand older
adults with lower ICT literacy did not found themselves the connection between real
life safety and increasing on-line security and vice versa. Surprisingly even though they
provided a number of proper examples and behaviors from both realms they failed to
ﬁnd themselves spontaneously the connection. This leads to the conclusion that inevi‐
tably there is a room for designers to employ a participatory design approach in order
to obtain insights from older adults not only to better understand their need but also to
foster the process of idea development in order to ﬁnd better connection between on-
line and real life habits and safety. Because usually there is a generation gap between
software development teams and end-users in case of older adults application we also
decided to obtain a deeper insight into the dynamics of such collaboration, described in
next section, based on our experience in the ﬁeld of intergenerational interaction.
4
Case Study
4.1
Case Study Context
The case study, involving the intergenerational developer team, was observed during a
DEVmuster Hackathon organized in March 2016 in The Polish-Japanese Academy for
Information Technology in Warsaw Poland, during which older adults and students of
the academy had an opportunity to cooperate in designing application that would address
the needs of older adults. The team consisted of 4 males, all in their twenties, who were
students of the academy, and two older adults, a male, and a female, who were partici‐
pants of the PJAIT LivingLab, a framework presented in previous section, which
involves volunteers wishing to take part in various project aimed at activizing older
adults with the use of ICT.
The team formed during the ﬁrst hours of the event, during which ideas for the
potential app were discussed. Upon reviewing the opinions of the older adults, the
students decided to change their original idea of an application, in to an app designed
for exchange of favor between volunteers and older adults. The name ‘F1’ was chosen,
based on the function key for calling the help menu.
46
B. Balcerzak

4.2
Platform Architecture
The F1 platform was designed as client-server model and requires access to the Internet
for the proper functioning. It might be a serious limitation for less developed and less
populated countries but as the platform is intended to be deployed in Poland we decided
to sacriﬁce versatility for simplicity.
Access to the system is possible either through a web site or a dedicated mobile
application. Although both ways provide the same functionality there are also substantial
diﬀerences. Mobile application was designed to be most convenient for people oﬀering
support. Web site is more focused on posting requests for help. The reason for this
diﬀerentiation is that mobile application will be more frequently used by younger volun‐
teers and web browsers are better suited for older adults. In the case described in this
paper senior participants were more familiar with traditional desktop or mobile
computers than with smartphones, mainly due to their professional background and prior
LivingLab activities i.e. computer workshop organized by the City of Warsaw. More‐
over computer web browsers can be more suitable for people with certain disabilities –
e.g. visually impaired or with limited hand dexterity. On the other hand mobile devices
are becoming more and more widespread and the adoption of touch interface by the
elderly can be faster and more eﬀective than traditional computer interfaces (e.g.
observed in our previous research studies). Thus in ﬁnal product the application mode
(web site or mobile app) is intended to be freely interchangeable at any time by the user.
4.3
Functionality
As presented in Fig. 1 The web based application contains all the key information and
functions important for the user searching for the help of a volunteer.
The screen informs the user about his or her previous favor requests, as well as the
location of other users in the area. There is also an S.O.S button which can be used in
case of emergency.
The mobile application view, used by the potential volunteer is shown in Fig. 2c.
When using the mobile view, the user can view a map of the nearest area where favor
requests are marked. When the user selects a favor a brief description of the favor and
the requesting user is provided.
Security by Design for Older Adults
47

Fig. 2. Mobile application dedicated for those who oﬀer assistance.
Fig. 1. Main screen of web-based application focused mostly on people requiring assistance.
48
B. Balcerzak

4.4
Security Related Features
Another direct beneﬁt from applying participatory design approach refers to security
issues. During the pre-design phase, older adults voiced their concerns related to user
security, taking into account threats for both parties of the process. The main areas in
which they stated older adults require aid when it comes to security were related with
minimizing the risks of coming in contact with fake proﬁles, or malicious users, as well
as dealing with problems of potential identity theft. The older adults involved in the
project also stressed that the platform should be able to aid the older users when dealing
with emergencies when swift help is needed. These issues were addressed by applying
the following solutions into the design of the platform.
Trusted proﬁles Sign up is free for all and requires only a valid email account.
Lowering the barrier makes system more user-friendly but also prone to malicious users.
Discussions with prospect users during design phase reveal that the elderly are afraid of
letting unknown people visit their apartments. To address this problem a voluntary
procedure for conﬁrming proﬁles was implemented. User proﬁle might be conﬁrmed by
external organizations that are trustworthy: e.g. schools or local NGOs. Conﬁrmation
of the veriﬁed status is visible for everyone next to proﬁle picture – see Fig. 3.
Fig. 3. Pop-up window displaying the conﬁrmation details for selected volunteer
Challenge-response authentication Next to the threat of fake or malicious proﬁles
mentioned in previous section yet another problem was identiﬁed by participatory
approach. Even if identity of volunteer is conﬁrmed on the platform still we need a way
to conﬁrm it in real world when volunteer is knocking to the door of senior’s apartment.
This is the situation when digital system should face analogue world and bottom-up
approach proved to be helpful once again. Therefore, a standard challenge-response
authentication has been adapted and implemented. The platform generates two keywords
for both users. Elderly should ask about the right password before letting someone in.
Passwords are randomly selected from a subset of polish words to make them easy to
remember and dictation by entry phone.
Reputation score Limiting the amount of frauds is crucial for assuring wide accept‐
ance of the platform but it is not enough. Next to deliberate and planed frauds we can
Security by Design for Older Adults
49

also see a bad quality service. Therefore, the platform contains a reputation management
system. Every agreed and conducted service can be evaluated on Likert-type scale. To
make scale easier to understand for users ﬁrst two grades are red, neutral score is gray
and the two positive levels are green. Sum of all evaluation for give user are displayed
next to the picture – see Fig. 3.
Emergency button In real life exhaustive list of risks and threats is impossible to
complete. Therefore, an emergency button has been added to the F1 platform.
5
Conclusions and Future Work
As it was presented in the Related Work section and in our study, the issues of on-line
security and real life safety faced by older adults are quite diverse. The span a wide set
of threats ranging from health issues, to crime related issues, and problems connected
with the spread of malware and spam, and even though older adults can point out a
variety of those issues, usually it is diﬃcult for them to spontaneously ﬁnd the connection
between the two realms. However, in our case study we showed that participatory design
approach can beneﬁt both younger tech-minded developers and older adults as end-
users.
The observations made during the presented case study show, that the key for
creating an on-line solution, that would not only aid them in their daily lives, but also
but be resilient to safety and security issues mentioned in the literature, is not necessarily
to address all potential dangerous scenarios. From the description of the app created
through a participatory design within an intergenerational team, where older adults, were
not only ﬁnal users, but also active team members, one can see that the focal point for
addressing issues of safety, both on-line and in real life, is modeling of trust within the
user base of the application. Use of such elements like a reputation system, two step
password veriﬁcation between users who decided to exchange favors, as well as external
conﬁrmation of users provides a wide array of possibilities for encouraging the devel‐
opment of trust between older adults and younger volunteers. This corroborates the
general ramiﬁcations of the intergroup contact theory, however it extends its scope
beyond intergroup stereotyping into the ﬁeld of limiting the feeling of insecurity among
the group of older adults.
It is also important to notice that the participatory approach utilized by the team,
allowed to overcome the aforementioned issue of understanding the link between real
life safety and on-line security. In light of the results from the interviews conducted
within the framework of the LivingLab, where older adults had diﬃculties with
connecting the two aspects of safety, the outcome of the team design process yields great
promise.
The case study presented here, of course, serves mostly as a jumping-oﬀ point for
further considerations in a topic that, although important, is not, in the authors opinion,
amply researched. The ﬁndings made in this paper show, that by use of a participatory
design framework, it is possible not only to address general issues of on-line security,
but also, to create new methods of thinking about user safety outside of the paradigm of
software engineering. With the increasing role of mobile technologies in real life
50
B. Balcerzak

situation, these initial exploratory ﬁndings oﬀer interesting option for improving the
initial design process of mobile applications.
In their future work, we plan to further explore this interesting emerging ﬁeld of
study. With the observation made in this paper being mostly exploratory in nature, it
seems ﬁtting to conduct a set of more methodologically rigid tests with the aim of veri‐
fying, what form of participatory design can further improve the process of addressing
the threat on-line and real life threats faced by end users of an application.
Acknowledgments. This project has received funding from the European Union’s Horizon 2020
research and innovation programme under the Marie Skłodowska-Curie grant agreement No
690962.
References
1. Adams, N., Stubbs, D., Woods, V.: Psychological barriers to internet usage among older
adults in the UK. Med. Inf. Internet Med. 30(1), 3–17 (2005)
2. Allport, G.W.: The Nature of Prejudice. Basic Books, New York (1979)
3. Balcerzak, B., Kopeć, W., Nielek, R., Kruk, S., Warpechowski, K., Wasik, M., Węgrzyn, M.:
Press F1 for help: participatory design for dealing with on-line and real life security of older
adults. arXiv preprint arXiv:1706.10223 (2017)
4. Brewer, R., Morris, M.R., Piper, A.M.: “Why would anybody do this?”: Older adults’
understanding of and experiences with crowd work
5. Choi, Y.J., Matz-Costa, C.: Perceived neighborhood safety, social cohesion, and
psychological health of older adults. Gerontol. 1, gnw187 (2017)
6. Deterding, S., Dixon, D., Khaled, R., Nacke, L.: From game design elements to gamefulness:
deﬁning gamiﬁcation. In: Proceedings of the 15th International Academic MindTrek
Conference: Envisioning Future Media Environments, pp. 9–15. ACM (2011)
7. Forjuoh, S.N., Won, J., Ory, M.G., Lee, C.: 172 neighbourhood safety and injury prevention
among older adults: a systematic literature review (2016)
8. Greenﬁeld, E.A., Marks, N.F.: Formal volunteering as a protective factor for older adults’
psychological well-being. J. Gerontol. Ser. B: Psychol. Sci. Soc. Sci. 59(5), S258–S264
(2004)
9. Grimes, G.A., Hough, M.G., Mazur, E., Signorella, M.L.: Older adults’ knowledge of internet
hazards. Educ. Gerontol. 36(3), 173–192 (2010)
10. Hao, Y.: Productive activities and psychological well-being among older adults. J. Gerontol.
Ser. B: Psychol. Sci. Soc. Sci. 63(2), S64–S72 (2008)
11. Harper, S.: Economic and social implications of aging societies. Science 346(6209), 587–591
(2014)
12. Itoko, T., Arita, S., Kobayashi, M., Takagi, H.: Involving senior workers in crowdsourced
proofreading. In: Universal Access in Human-Computer Interaction. Aging and Assistive
Environments, pp. 106–117. Springer (2014)
13. Jankowski-Lorek, M., Nielek, R., Wierzbicki, A., Zieliński, K.: Predicting controversy of
Wikipedia articles using the article feedback tool. In: Proceedings of the 2014 International
Conference on Social Computing, SocialCom 2014, New York, NY, USA, pp. 22:1–22:7.
ACM (2014)
Security by Design for Older Adults
51

14. Kopeć, W., Abramczuk, K., Balcerzak, B., Juźwin, M., Gniadzik, K., Kowalik, G., Nielek,
R.: A location-based game for two generations: teaching mobile technology to the elderly
with the support of young volunteers. In: eHealth 360°, pp. 84–91. Springer (2017)
15. Kopeć, W., Skorupska, K., Jaskulska, A., Abramczuk, K., Nielek, R., Wierzbicki, A.:
LivingLab PJAIT: towards better urban participation of seniors. arXiv preprint arXiv:
1707.00030 (2017)
16. Ladner, R.E.: Design for user empowerment. Interactions 22(2), 24–29 (2015)
17. Lum, T.Y., Lightfoot, E.: The eﬀects of volunteering on the physical and mental health of
older people. Res. Aging 27(1), 31–55 (2005)
18. Massimi, M., Baecker, R.M., Wu, M.: Using participatory activities with seniors to critique,
build, and evaluate mobile phones. In: Proceedings of the 9th International ACM
SIGACCESS Conference on Computers and Accessibility, pp. 155–162. ACM (2007)
19. McCloskey, D.W.: The importance of ease of use, usefulness, and trust to online consumers:
an examination of the technology acceptance model with older consumers. J. Organ. End
User Comput. 18(3), 47 (2006)
20. Mora, A., Riera, D., González, C., Arnedo-Moreno, J.: A literature review of gamiﬁcation
design frameworks. In: 2015 7th International Conference on Games and Virtual Worlds for
Serious Applications (VS-Games), pp. 1–8. IEEE (2015)
21. Morrow-Howell, N.: Volunteering in later life: Research frontiers. J. Gerontol. Ser. B:
Psychol. Sci. Soc. Sci. 65(4), 461–469 (2010)
22. Morrow-Howell, N., Hinterlong, J., Rozario, P.A., Tang, F.: Eﬀects of volunteering on the
well-being of older adults. J. Gerontol. Ser. B: Psychol. Sci. Soc. Sci. 58(3), S137–S145
(2003)
23. United Nations: World Population Ageing 2015. Department of Economic and Social Aﬀairs
(2015)
24. Nielek, R., Ciastek, M., Kopec, W.: Emotions make cities live. Towards mapping emotions
of older adults on urban space. arXiv preprint arXiv:1706.10063 (2017)
25. Nielek, R., Lutostanska, M., Kopec, W., Wierzbicki, A.: Turned 70? It is time to start editing
Wikipedia. arXiv preprint arXiv:1706.10060 (2017)
26. Niitamo, V.-P., Kulkki, S., Eriksson, M., Hribernik, K.A.: State-of-the-art and good practice
in the ﬁeld of living labs. In: Technology Management Conference (ICE), 2006 IEEE
International, pp. 1–8. IEEE (2006)
27. Ortman, J.M., Velkoﬀ, V.A., Hogan, H., et al.: An Aging Nation: The Older Population in
the United States. US Census Bureau, Washington, DC (2014)
28. Peek, S.T., Aarts, S., Wouters, E.J.: Can smart home technology deliver on the promise of
independent living? A critical reﬂection based on the perspectives of older adults. In:
Handbook of Smart Homes, Health Care and Well-Being, pp. 203–214 (2017)
29. Pettigrew, T.F.: Intergroup contact theory. Annu. Rev. Psychol. 49(1), 65–85 (1998)
30. Pettigrew, T.F., Tropp, L.R.: A meta-analytic test of intergroup contact theory. J. Pers. Soc.
Psychol. 90(5), 751 (2006)
31. Rosencranz, H.A., McNevin, T.E.: A factor analysis of attitudes toward the aged. Gerontol.
9(1), 55–59 (1969)
32. Ryan, R.M., Deci, E.L.: Self-determination theory and the facilitation of intrinsic motivation,
social development, and well-being. Am. Psychol. 55(1), 68 (2000)
33. Sanders, E.B.-N.: From user-centered to participatory design approaches. In: Design and the
Social Sciences: Making Connections, pp. 1–8 (2002)
34. Sanders, E.B.-N., Stappers, P.J.: Co-creation and the new landscapes of design. Co-design
4(1), 5–18 (2008)
52
B. Balcerzak

35. Sanderson, W.C., Scherbov, S.: Faster increases in human life expectancy could lead to slower
population aging. PLoS ONE 10(4), e0121922 (2015)
36. Spijker, J., MacInnes, J.: Population ageing: the timebomb that isn’t. BMJ 347(6598), 1–5
(2013)
37. Szebeko, D., Tan, L.: Co-designing for society. Australas. Med. J. 3(9), 580–590 (2010)
38. Vallerand, R.J., O’Connor, B.P.: Motivation in the elderly: a theoretical framework and some
promising ﬁndings. Can. Psychol./Psychol. Can. 30(3), 538 (1989)
39. Wawer, A., Nielek, R., Wierzbicki, A.: Predicting webpage credibility using linguistic
features. In: Proceedings of the 23rd International Conference on World Wide Web, WWW
2014 Companion, New York, NY, USA, pp. 1135–1140. ACM (2014)
40. Won, J., Lee, C., Forjuoh, S.N., Ory, M.G.: Neighborhood safety factors associated with older
adults’ health-related outcomes: a systematic literature review. Soc. Sci. Med. 165, 177–186
(2016)
41. Xie, B., Druin, A., Fails, J., Massey, S., Golub, E., Franckel, S., Schneider, K.: Connecting
generations: developing co-design methods for older adults and children. Behav. Inf. Technol.
31(4), 413–423 (2012)
42. Zichermann, G., Cunningham, C.: Gamiﬁcation by design: implementing game mechanics
in web and mobile apps. O’Reilly Media, Inc., Sebastopol (2011)
43. Zickuhr, K., Madden, M.: Older adults and internet use. Pew Internet & American Life
Project, 6 June 2012
Security by Design for Older Adults
53

The Popularization Problem of Websites and Analysis
of Competitors
Taras Basyuk
(✉)
Information Systems and Networks Department,
Lviv Polytechnic National University, Lviv, Ukraine
Taras.M.Basyuk@lpnu.ua
Abstract. The features of popularizing websites in the global Internet and the
main factors that inﬂuence this process are analysed in the article. The analysis
of existing researches is conducted and implementation of search engines that
made it possible to identify the most important factors in the competitive promo‐
tion of the resource is described. The methods of assessing backlinks, anchors,
resource and content logical structure are developed. The possibilities to apply
thematic backlinks of competitors in the promoting website are determined. The
scheme, which reﬂects features of website structure design depending on types
of keywords, is developed.
Keywords: Popularization · Ranking · Search engine · Website · Backlinks ·
Anchor · Structure · Keywords
1
Introduction
The success of any website directly depends on its popularity among search engines.
Moreover, the position of Internet resources as a result of search request is an important
factor that directly aﬀects the number of visitors [1, 2]. Eﬀective search engine optimi‐
zation contributes to its display in the top positions of search engine ranking. Achieve‐
ment of this goal is only possible in case of thorough knowledge of ranking algorithms
as well as skilful application of technology of search engines optimization, which is
impossible without detailed consideration of problems of this sphere, the analysis of the
main methods and work principles [3].
As for search engines, their role is constantly increasing, and it is impossible to
imagine modern digital society without them. As is evidenced by short statistics of
GlobalStat: 12 billion of search operations were carried out in the world monthly in
2011, which comprised about 400 million of search requests a day, and in 2016 the
ﬁgure rose to 16.8 billion only on the US territory for desktop devices. This means that
averagely over 6500 search operations is performed every second, while Google Inc.
owns approximately 64% of the search market, and its search technology enables
processing more than 4,000 operations per second [4].
Today, only a few seconds are spent for searching the information whereas twenty
years ago we had to visit the library and spend two or three hours. This trend contributed
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_4

to the fact that the set of information resources, banking operations, social activities or
purchases is performed via global network. Given that, the presence of high ranking in
the search engines for business website is the key to the survival of the business, because
people are constantly searching services and products in the global network. The
achievement of such ranking is a great challenge, and basically relies on search engine
optimization (SEO) [5, 6].
As the popularity of any search engine depends on the relevance of search results,
so in applied algorithms and staﬀ of developers the main attention is paid to the relevance
of the search results. In turn it entrusts with huge amount of responsibilities on seo-
specialists, as they should be aware of the search engines that the techniques used by
them were not considered to be black optimization by search engines which can lead to
the imposition of sanctions or deletion of the search engine from directory. Given that,
the promotion of websites aimed at increasing their visibility in search engines is the
extremely urgent problem.
1.1
Relation of Highlighted Issue to Scientiﬁc and Practical Tasks
Solving of the problem of eﬀective websites promoting is an important task within the
development of both the IT industry and industry in general. Since there is no company
or entity, which in some way is not represented in the Global Internet that in its turn
promotes the appropriate mathematical and algorithmic apparatus for implementing this
process. Thus, the task of popularizing website can be divided into two types: the ﬁrst
– the actual analysis of search queries and formation of eﬀective resource structure;
second – the analysis of competitor websites that exist in this market segment. Under‐
standing of the advancing mechanism provides answer to questions about why two
similar-looking sites, with the same topic, take so diﬀerent places in the search results.
For example, one is on the top of search results [7, 8], and the other – on the third or
fourth page of search result. In view of this, there are following answers to questions:
how is the popularization of websites performed, which of the re-sources are preferred,
based on which features the “best” relevance is deﬁned - are becoming increasingly
important in today’s information society.
The evaluation of search engine by the user is the other side of the study. Speciﬁcally,
the main purpose of the latter is to obtain information relevant to its request. However,
requests may take a plurality of diﬀerent forms, considering this an important task is to
create a marketing strategy for the website (based on optimization mechanisms and
search rankings). All this will contribute to the study and understanding of the
psychology of target audience and thus provide the means of establishing contacts with
new users [9].
As a result of globality of assigned task there is no general solution for it, as for the
diﬀerent types of resources, internal and external optimization factors as well as for
diﬀerent structure of websites. Complexity of research lies also in the fact that web-sites
promotion is a complex and continuous process, in the course of which it is required to
apply advanced mathematical and algorithmic apparatus [10, 11]. The solution of this
task will help to determine the main factors inﬂuencing the analysis of competitor
resources and thus contribute to construction of optimized websites and provide means
Popularization Problem of Websites and Analysis of Competitors
55

of adapting known methods and algorithms to the tasks of construction of intelligent
information systems in this subject area.
1.2
Analysis of Recent Researches and Publications
The era of information society signiﬁcantly changed the concept of information resource
by which increasingly the website is understood that can be presented as follows: the
ordinary business cards (small website consisting of one or more pages, which contains
basic information about the organization, individuals, products or services, contact
information), powerful information portal (website containing a comprehensive amount
of information on any subject area or several areas) or online store of the global scope
(the site for selecting products from the catalog and selling them via the Internet). Thus,
the availability of the resource on the Internet is not only prestigious but also proﬁtable.
By analysing the situation, we can conclude that the rating of website is comparable
with the rating of its operations success. Considering conducted analysis of the literature
[5, 11, 12] as well as the review of relevant Internet resources [13, 14] today search
requests is often related to: research, search and purchase of goods and services. That
is why, global e-commerce market is the mostly expanded, which already imposes
ﬁnancial factors as on the stage of advancement of the information website for
complaints and product promotion as well as popularization, including website store
spaces in the global network. According to the conducted analysis, the popularization
of this area is determined by such factors as [11, 15]:
– the increase in number of users and improvement of the behavior culture in a global
network that promotes the domination of users aged 21–35 years, for whom the
possibility of purchase via network ranges from 52–63%;
– the introduction of high-speed wireless channels and performance of transactions by
means of mobile devices is at the level of 30%;
– preservation of the competitive advantage of goods by providing lower prices, wide
range of products and convenience of matching vendors’ oﬀers in comparison with
traditional trade;
– usability, due to the presence of perfect delivery system at the expense of increasing
number of goods outlets, the use of alternative delivery formats (automatic parcel
terminal).
It should be highlighted that the study, conducted by Yahoo company! showed certain
correspondence between the search in a global network and behavior beyond it. Specif‐
ically, the main ﬁndings were as follows [16, 17]: every dollar spent on advertisement
in the global Internet networks brings nine dollars of income in ordinary stores; search
marketing has a signiﬁcant impact on sales increase in the stores than in demonstrating
advertisements (almost three times). In view of above analysis, the popularization of
websites has a set of advantages for both information search and online commerce. With
regard to methodological research in this ﬁeld, the following works should be admitted:
the work of N.V. Yevdokimova on the basics of optimizing information content; A.
Peleschyshyna on positioning websites in the global information environment; A.
Yakovlyeva and V. Tkachyeva on the possibilities of promoting websites, I. Ashmanova
56
T. Basyuk

and A. Ivanova on optimizing and promoting websites. The authors in these works
consider general methods of improvement the websites position, but with the diversity
of works the lack of thorough research on the positioning of websites, the increase of
their popularity and competitive power is noticeable. In addition, there are no studies
on the allocation of main reasons that motivate web optimizers to promote resources in
top positions and analyze websites of competitors which are located at the same market
segment. This, in turn, determines the relevance of research on the development of new
methods and means of promoting websites in the global network that would have an
appropriate justiﬁcation and predictable performance.
2
The Main Objectives of the Research and Their Meaning
The main objectives of the research are as follows: the analysis of the functioning
features of search engines with further display of search results on user’s screen; meth‐
odological review of the structure and formulation of recommendations for the analysis
of websites-competitors.
According to indicated research tasks, for their solutions it is necessary to: deﬁne
the overall scheme of search and analyse the need for getting your website in the top of
search results system; develop a methodology for analysing websites of competitors
under the proposed factors. The solution of these problems will allow developing intel‐
ligent system, which will provide the following means: the accumulation of data as
regards parameters of optimization and will provide opportunities for their systemati‐
zation; analysis, implementation and support of new services of search engines; provi‐
sion of recommendations on the best options for promoting websites.
2.1
Features of Search Mechanisms
The use of search engines has been increased and improved signiﬁcantly in recent years,
but the basic principles of the search procedure remain the same. In general, most search
operations are of triple form: requirement, request and execution of request. The main
components of which are as follows:
– requirement - involves the emergence of the need for a response or solution of the
problem (in particular, the user can make navigation search (website search), trans‐
actional search (purchase) or information search (search of information);
– request - speciﬁes the need to implement a search request in form of line of words or
phrases;
– execution of request - deﬁnes the search results veriﬁcation, if the result is inaccurate
the speciﬁcation is made, the ultimate goal of which is to meet the needs of the user.
We know that the most relevant pages in the top of search results are displayed in the
upper left-hand corner of the screen. The above mentioned location is explained by the
fact that while working at a computer, the focus of person is concentrated on the left-
upper corner of the screen. As it can be noted the number of advertising as well as search
results is displayed elsewhere. Research in this area ascertains the fact that such location
Popularization Problem of Websites and Analysis of Competitors
57

is caused by the use of F-like pattern of eye movement (initially the focus is on the left-
upper corner of the screen, then moves vertically down the ﬁrst two or three search
results). Then, the view moves across the page on the ﬁrst result of paid page, then down
another few results, and then back to the other results of paid page. As it can be noted
in those areas the most relevant search pages and windows of contextual advertising are
displayed.
The study shows that the top position of website directly aﬀects the traﬃc that the
resource receives, approximately 62% of people, who go to one of the results of the ﬁrst
page of search results, are satisﬁed with it. By summarizing the research [16], conducted
during 2006–2016, it can be observed how the trend of clicks quantity has been changed
depending on the position in the search results (Fig. 1).
Fig. 1. Click through rate by search position
Consequently, it can be said that the absence of a website in the top search engine
signiﬁcantly reduces the number of entries on it and the number of potential clients on
its pages.
2.2
Analysis of Competitors’ Resources
Often there is a situation that execution of optimization measures as well as taking into
account the recommendations of ﬁltering algorithms (Google Panda, Google Penguin,
Google Calibri [18]) website is located on the second or third page of search results.
While the resource of competitor with a similar design and functionality occupies the
top positions. This situation is typical and quite common on the World Wide Web spaces.
In view of this, the task of analysing resource of competitor is important. Analysis of
competitors’ website ﬁrstly is necessary to understand what they do, how they move,
what methods are used, how actively the process of popularizing is performed, which
strategy and direction is chosen. As a result of conducted research the set, which includes
the main factors that are necessary to be considered while analysing competitor’s
resources, is conducted:
Resources of competitors = {Rl, Al, C, S}
(1)
58
T. Basyuk

where Rl is a returning links, Al is a anchor links, C is a content, S – is a structure.
Returning Links
Returning links can be analysed by using various services, among the most inﬂuential
the following are distinguished: SEOGadget (http://www.seogadget.ru/ backlinks),
Majesticseo (https://majestic.com/) XSeo (http://xseo.in/smz), Ahrefs.com (https://
ahrefs.com/). Mentioned services reﬂect the link that contain the speciﬁed competitor’s
domain link. By March 2016, besides this, resources reﬂect PR index of the resource
[19]. Page Rank (PR) displayed information about the theoretical attendance of the page,
which was calculated as the attendance probability of given web page by the user; at
that the amount of probabilities on all web pages of a network was equal to one, because
the user necessarily was visiting any page. Though the module operation for determining
the meaning of Page Rank was suspended, PageRank algorithm still are used in the
Google search engine to assess the quality of pages despite criticism of SEO optimizers
concerning the creation of search spam Spamdexing [20]. The feature of which is the
creation of websites and web pages for manipulation of search results in search
engines [21].
Methods of webspam are divided into two classes: Spam of content and spam of link.
The ﬁrst class includes replacement of logical representation of how a search engine
perceives the page content, the second is based on the features of the functioning of
ranking algorithms, created based on links, such as resource rating which is higher than
more ranked site is linked to it.
Most notable technologies at this are as follows: mirroring of sites (hosting of
multiple websites with conceptually similar content but with the use of different URL
- address); URL redirect (transferring of the user to another page without its inter‐
vention, for instance, with the help of meta updating of tags, flash, JavaScript, Java,
etc.); masking (analysis of request variables in which the site content is transmitted
by search engine different from what the user sees); keywords stuffing (includes
location of keywords within a page, with the purpose of raising its importance for job
search); wiki spam (spammers use open for editing wiki system with the aim to place
links from the wiki site to the spam site).
Regarding the analysis of returning links of competitors, it is important to understand
what kind of links they use, and which ones are most important to promote resource
because uncontrolled increase in links mass can cause the opposite eﬀect - the fall of
rating.
Determination of the importance of the returning link is proposed to carry out in
accordance with the developed method. Let Returning links – be the set of backlinks
competitor’s resource in the global network:
returning links = {returning linksi}NRl
i=1
(2)
where, NRl – is the number of returning links. Each of returning links is described by
corresponding topics of the resource to which they refer:
theme resource = {theme resourcei}NTr
i=1
(3)
Popularization Problem of Websites and Analysis of Competitors
59

where, NTr – is the number of diverse topics of resource with returning links of compet‐
itor’s website. Thus, the rate of the site importance is determined by the weight of
returning links and is used by search engines to provide a higher rating for the resource.
Search engine provides higher status to the website, to which the user addresses the
maximum number of “quality” external links with signiﬁcant weight. Given that let us
deﬁne returning links as a set of resource topics and reference weight:
returning linksi = {(theme resourcep, weight backlinksip)}NTrRLi
p=1
(4)
where, NTrRLi – is the number of returning links in thematic resource;
weight backlinksip – is the weight of reference in thematic resource of returning links.
The number of possible thematic resources to which the references can be done,
creates thematic catalog of returning links, which can be determined by:
theme resource cataguei = {(theme resourcep, nip)}NTrci
i=1
(5)
where, nip – is the number of pages that matches a resource theme; NTrci – is the number
of thematic resources of catalog.
Given that the feasibility of usage of competitor’s separate returning link in the
projected resource can be deﬁned by the ratio:
𝛿(analyzed sitei, competitor′s sitez) =
NTrci
∑
p=1
weight backlinkszinip
NTrz
∑
p=1
weight backlinkszi
(6)
where,
weight backlinkszi = weight backlinks (theme resource cataguez, theme resourcei) 
–
compliance of returning link weight and thematic resource, catalog.
The determination of actual weights of each separate returning links out of a plurality
of thematic resources is an important objective of the analysis of this parameter for
deﬁning the feasibility of their use in projected resource.
Anchor Links (Texts of Links)
Besides determining links to competitors’ websites it is necessary to understand by
which phrase (or text of link) are they addressed. For this, it is necessary to apply
advanced statistical service SemRush (https://www.semrush.com), which provides
detailed information on these parameters (Fig. 2).
This service displays a list of link texts, by which one can link to the appropriate
resource (in this case the National University of Lviv Polytechnic). As it can be seen
from Fig. 2, the total number of link texts is equal to 1564, and this is the total number
of unique Anchor on the website. Thus, statistics is available for every resource by which
phrases they move and how they are distributed on the website.
60
T. Basyuk

Content
The website content is another important factor which should be analysed, namely its
textual and graphical components. First of all, it is necessary to assess optimized text
and its authenticity as well as originality of the used images. After analysing the set of
resources, the following sources for ﬁlling site may be allocated:
– an independent content of website (author copywriting) - the best method of ﬁlling
that promotes high uniqueness, both in terms of technical and semantic aspects, but
the most labour-consuming in terms of writing time and the complexity of use;
– independent rewriting of texts from other sources - the most popular method because
of the relative simplicity and accessibility. Advantage - the processing of ﬁnished
material is easier than writing text from scratch, but the main drawback - by using
rewrite website risks to lose some people who have already met such information;
– independent translation of information from “foreign” resources - is often used for
the promotion of foreign news and popular media, the suﬃciently high level of
obtaining unique materials that are equally well rated by both users and search
engines is the advantage, drawback - it requires considerable time to obtain permis‐
sion from the text authors;
– independent processing of materials from paper sources - consists in scanning and
reprinting of articles from various publications that are not yet published on the
Internet. Advantage - the availability of a plurality of such sources that do not require
the possession of signiﬁcant knowledge on the topic of resource, drawback - the
process complexity in typing and checking the availability of the resource in a global
network;
Fig. 2. Statistics of anchor links for the lp.edu.ua resource
Popularization Problem of Websites and Analysis of Competitors
61

– formation of content via visitors of website - this opportunity came with the advent
of Web 2.0 and lies in the text ﬁlling with content of individual sections of the website
(forums, comments, social networks) by users. Advantage - the automatic content
ﬁlling of popular resource is the advantage, drawback - it is necessary constantly to
maintain the popularity of the resource and perform moderator functions for the
timely removal of false information.
There is no perfect method of ﬁlling the website content and each method has its
advantages and disadvantages, so while designing the content of the website following
basic principles should be observed: application of deﬁned terms (consist in the use of
expressions that are commonly-accepted and equally understandable to all community
of the website); the feasibility of use (foresees the availability of corresponding elements
and information relevant to thematic section); sustainability (determines the application
of the same navigation elements on a website with the overall graphic design); division
into parts (introduced information should be logically separated into components with
its subsequent deployment in diﬀerent subsections of the designed resource).
Structure
While analysing content resources of competitor it is very important to analyse the
structure of the website, its code and contents. Since the structure of each Internet
resource is closely connected with links, their clever location is the essential factor. Two
aspects of assessment are considered here, developer and user. From the developer’s
side website structure is divided into two parts: logical (set of pages, united by a common
design) and physical (an ordered set of ﬁles that form the backbone of the resource).
Websites with failed logical structure not only complicate their viewing, but also
contribute to the emergence of the idea that there is no stated information on the speciﬁed
website.
The determination of competition level in the topic of key elements (words, phrases,
etc.) directly inﬂuences the change of website structure. If the competition is consider‐
able, it is necessary to perform the expansion of the structure of designed website at the
expense of pages number increase, which is designed according to corresponding
requests. Besides, it is necessary to increase statistic weight of pages data by performing
relink operation. General method of website structure designing depending on keywords
varieties is represented on Fig. 3.
Summing up the information (Fig. 3) it can be emphasized that in the case of highly
competitive key phrases usage in designed website, it is appropriate to add some pages
to the website structure with further promotion and use of internal and external links, in
other cases – it is unnecessary to make any changes to the structure of the website.
Analysis of competitors’ websites is an important step in the popularization of indi‐
vidual Internet resource that allows us to understand how competitors behave, and what
should be done in order to go to the top position of search results.
62
T. Basyuk

3
Conclusion
The analysis of important methodological reworks in the ﬁeld of information content
optimization as well as positioning of websites that showed practical lack of research
process of identiﬁcation the main factors of competition between them, is carried out
herein. In view of this, the existent problems in the sphere of popularizing websites are
considered, the features of search engines functionality are shown and feasibility of
resources display in the top of search results is reasoned. The main factors (Returning
links, Anchor links, Content, Structure), which should be taken into account during the
analysis of websites of competitors are determined. The method of evaluating feasibility
of applying some returning links depending on the theme of resource was developed.
The general scheme of distribution of keywords on the pages of Internet resources is
developed on the basis of analysis of known structures of websites and features of
website structure construction are provided. The use of these measures promotes the
popularization of website and its promotion in the top of search engine.
Further research will be aimed at designing individual modules of intelligent system
of websites popularization and evaluating of their operation.
Fig. 3. Redistribution of keywords according to website pages
Popularization Problem of Websites and Analysis of Competitors
63

References
1. Grappone, J.: Search Engine Optimization (SEO): An Hour a Day. Wiley Publishing, Inc.,
New York (2013). 435 pages
2. Basyuk, T.M., Vasylyuk, A.S.: Ranking of websites on the Internet. Information systems and
networks. Visnyk of the Lviv Polytechnic National University, Lviv № 770, pp. 3–12 (2013).
(in Ukrainian)
3. Basyuk, T.: Inﬂuence of external factors on the position rankings website. In: Proceedings of
the V International Scientiﬁc-Practical Conference on Information Control Systems and
Technologies (ICST-ODESSA 2014), pp. 241–242 (2014)
4. Lella, A.: ComScore Releases February 2016 U.S. Desktop Search Engine Rankings. http://
www.comscore.com/Insights/Rankings/comScore-Releases-February-2016-US-Desktop-
Search-Engine-Rankings
5. Kennedy, G.: Seo: Marketing Strategies to Dominate the First Page (SEO, Social Media
Marketing). CreateSpace Independent Publishing Platform (2016). 122 pages
6. Basyuk, T.: Web site promotion on Google. In: Proceedings of the XVIII International
Scientiﬁc and Technical Conference on System Analysis and Information Technology, SAIT
2016, pp. 301–302 (2016)
7. Yakovlev, A., Tkachev, V.: Site Promotion. Fundamentals, secrets, tricks - SPb: BHV-SPb
(2015). 352 pages. (in Russian)
8. Basyuk, T.: Popularization of website and without anchor promotion. In: Proceedings of the
XI International Scientiﬁc and Technical Conference on Computer Science and Information
Technologies, CSIT-2016, pp. 193–195. IEEE (2016). https://doi.org/10.1109/STC-CSIT.
2016.7589904
9. Alpar, A., Koczy, M., Metzen, M.: SEO - Strategie, Taktik und Technik. Springer Gabler
(2015). 538 pages. (in German)
10. Peleschishin, A.: Positioning Sites in the Global Information Environment. Publisher Lviv
Polytechnic, Lviv (2007). 260 pages. (in Ukrainian)
11. Enge, E., Spencer, S., Stricchiola, J., Fishkin, R.: Mastering Search Engine Optimization.
O’Reilly Media, Sebastopol (2015). 994 pages
12. Semenyshyn, V., Oleksiv, I.: Theoretical approaches to communications management in IT
industry of Ukraine. Econtechmod Int. Q. J. 2(4), 67–72 (2013). New technologies and
modelling processes. Lublin, Rzeszow
13. Fedasyuk, D., Yakovyna, V., Serdyuk, P., Nytrebych, O.: Variables state-based software
usage model. Econtechmod Int. Q. J. Econ. Technol. 3(2), 15–20 (2014). New technologies
and modelling processes. Lublin, Rzeszow
14. Basyuk, T.: The main reasons of attendance falling of internet resource. In: International
Scientiﬁc and Technical Conference on Computer Science and Information Technologies
CSIT-2015, pp. 91–93. IEEE (2015). https://doi.org/10.1109/STC-CSIT.2015.7325440
15. Nielsen: Electronic commerce in the world. http://www.nielsen.com/ua/uk/insights/reports/
2016/E-commerce_worldwide.html
16. Petrescu, P.: Google organic click-through rates in 2014. https://moz.com/blog/google-
organic-click-through-rates-in-2014
17. Searchengineland: Yahoo’s ROBO study: search has big impact on oﬄine purchases. http://
searchengineland.com/yahoos-robo-study-search-has-big-impact-on-oﬄine-
purchases-11832
18. Slegg, J.: A complete guide to Panda, Penguin, and Hummingbird. https://www.search
enginejournal.com/seo-guide/google-penguin-panda-hummingbird
64
T. Basyuk

19. Schwartz, B.: Google has conﬁrmed it is removing Toolbar PageRank. Search Engine Land.
http://searchengineland.com/google-has-conﬁrmed-they-are-removing-toolbar-pagerank-
244230
20. Sullivan, D.: What is search engine spam? The video edition. http://searchengineland.com/
what-is-search-engine-spam-the-video-edition-15202
21. Zoltan, G., Hector, G.-M.: Web spam taxonomy. In: First International Workshop on
Adversarial Information Retrieval on the Web at the 14th International World Wide Web
Conference, Chiba, Japan (2005)
Popularization Problem of Websites and Analysis of Competitors
65

Analysis of Uncertainty Types for Model
Building and Forecasting Dynamic Processes
Peter Bidyuk1(&), Aleksandr Gozhyj2, Irina Kalinina2,
and Victor Gozhyj2
1 National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnik
Institute”, Kiev, Ukraine
pbidyke@gmail.com
2 Black Sea National University named after Petro Mohyla, Nikolaev, Ukraine
alex.gozhyj@gmail.com, irina.kalinina1612@gmail.com,
gozhyi.v@gmail.com
Abstract. The article deals with the issues of handling uncertainties in the
problems of modeling and forecasting dynamic systems within the framework of
the dynamic planning methodology. To analyze and take into account possible
structural, statistical and parametric uncertainties, the Kalman ﬁlter, various
methods for calculating missing data, numerous methods for estimating the
model parameters and the Bayesian approach to programming are used. The
questions of an estimation of quality of predicted decisions are considered.
Keywords: Dynamic planning  Stochastic uncertainty  Kalman ﬁlter
Uncertainties of model parameters  Uncertainties of a level (amplitude)
Probabilistic uncertainties  Bayesian networks
1
Introduction
Analysis of dynamic processes in the planning and decision-making procedures is an
urgent problem not only for ﬁnancial organizations and companies but for all industrial
enterprises, small and medium business, investment and insurance companies etc. This
problem is solved by using dynamic programming methodology. Dynamic planning
(DP)couldbedeﬁnedastheprocessofestimationbyanenterpriseofitscurrentstateonthe
market in comparison with other competing enterprises, and determining the further goals
as well as sequences of actions and resources that are necessary for reaching the goals
stated. This process of planning is performed continuously (or quasi-continuously) with
acquiring new information (knowledge) about market, technologies, forecast estimates of
necessary variables and situations. All this knowledge is used for correcting of actions and
activities of the enterprise and supporting its competitiveness with ﬂow of time.
Formally DP could be represented in the form:
DP ¼ fX0; G; R; DðtÞ; K; T; F; DDðtÞ; DRðtÞg;
Where X0 is initial state of an enterprise; G are the goals stated by the enterprise
management; R are resources that are necessary for reaching the goals stated. D(t) is a
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_5

sequence of actions that should be performed on the interval of planning; K is a new
knowledge about environment; T are new technologies. F are results of forecasting and
foresight; DD(t) are corrections that are to be performed for reaching the goals; DR(t) are
necessary extra resources. One of the main problems that are to be solved within the DP
paradigm is high quality forecasting of relevant processes.
Adequate models of the process and the forecasts generated with them help to take
into consideration a set of various inﬂuencing factors and make objective planning
managerial decisions. Another purpose of the studies is in estimating possible risks
using forecasts of volatility. There are several types of processes that could be
described with mathematical models in the form of appropriately constructed equations
or probability distributions. Among them are the processes with deterministic and
stochastic trends, and heteroscedastic processes. As of today the following mathe-
matical models are widely used for describing nonlinear dynamics of processes relevant
to planning: linear and nonlinear regression (logit and probit, polynomials, splines),
autoregressive integrated moving average (ARIMA) models, autoregressive condi-
tionally heteroscedastic models (ARCH), generalized ARCH (GARCH), dynamic
Bayesian networks, support vector machine (SVM) approach, neural networks and
neuro-fuzzy techniques as well as combinations of the approaches mentioned [1–5].
All types of mathematical modeling usually need to cope with various kinds of
uncertainties related to statistical data, structure of the process under study and its
model, parameter uncertainty, and uncertainties relevant to the models and forecasts
quality. Reasoning and decision making are very often performed with leaving many
facts unknown or rather vaguely represented in processing of data and expert estimates.
To avoid or to take into consideration the uncertainties and improve this way quality of
the ﬁnal result (processes forecasts and the planning decisions based upon them) it is
necessary to construct appropriate computer based decision support systems (DSS) for
solving multiple speciﬁc problems.
Selection and application of a speciﬁc model for process description and forecasts
estimation depends on application area, availability of statistical data, qualiﬁcation of
personnel, who work on the data analysis problems, and availability of appropriate
applied software. Better results for estimation of processes forecasts is usually achieved
with application of ideologically different techniques combined in the frames of one
computer system. Such approach to solving the problems of quality forecasts estima-
tion can be implemented in the frames of modern decision support systems (DSS). DSS
today is a powerful instrument for supporting user’s (managerial) decision making as
far as it combines a set of appropriately selected data and expert estimates processing
procedures aiming to reach ﬁnal result of high quality: objective high quality alter-
natives for a decision making person (DMP). Development of a DSS is based on
modern theory and techniques of system analysis, data processing systems, estimation
and optimization theories, mathematical and statistical modeling and forecasting,
decision making theory as well as many other results of theory and practice of pro-
cessing data and expert estimates [6–8].
The paper considers the problem analysis, accounting and handling of uncertainties for
solving the problems of modeling and estimating forecasts for selected types of dynamic
processes with the possibility for application of alternative data processing techniques,
modeling and estimation of parameters and states for the processes under study.
Model Building and Forecasting Dynamic Processes
67

2
Problem Formulation
The purpose of the study is as follows: (1) analysis of uncertainty types characteristic
for model building and forecasting dynamic processes; (2) selection of techniques for
taking into consideration of the uncertainties; (3) selection of mathematical modeling
and forecasting techniques for nonstationary heteroscedastic processes.
3
Coping with Uncertainties
All types of mathematical modeling with the use of statistical experimental data usually
need to consider various kinds of uncertainties caused by data, informational structure
of a process under study and its model, parameter uncertainty, and uncertainties rele-
vant to the quality of models and forecasts. In many cases a researcher has to cope with
the following basic types of uncertainties: structural, statistical and parametric. Struc-
tural uncertainties are encountered in the cases when structure of the process under
study (and respectively its model) is unknown or not clearly enough deﬁned (known
partially). For example, when the functional approach to model constructing is applied
usually we do not know object (or a process) structure, it is estimated with appropriate
model structure estimation techniques: correlation analysis, estimation of mutual
information, estimation of lags, testing for nonlinearity and nonstationarity, identiﬁ-
cation of external disturbances etc. Uncertainty could also be introduced by an expert
who is studying the process and provides its estimates for model structure, parameter
restrictions, selection of computational procedures etc. The sequence of actions nec-
essary for identiﬁcation, processing and taking into consideration of uncertainties could
be formulated as follows: – identiﬁcation and reduction of data uncertainty; – model
structure and parameters estimation; – reduction of uncertainties related to the model
structure and parameters estimation; – reduction of uncertainties relevant to expert
estimates; – estimation of forecasts and reduction of respective uncertainties; – selec-
tion of the best ﬁnal result. All the tasks mentioned above are usually solved
sequentially (in an adaptive loop) with appropriately designed and implemented DSS.
We consider uncertainties as the factors that inﬂuence negatively the whole process
of mathematical model constructing, forecasts and possible risk estimating and gen-
erating of alternative decisions. They are inherent to the process being studied due to
incomplete or noise corrupted data, complex stochastic external inﬂuences, incom-
pleteness or inexactness of our knowledge regarding the objects (systems) structure,
incorrect application of computational procedures etc. The uncertainties very often
appear due to incompleteness of data, noisy measurements or they are invoked by
sophisticated stochastic external disturbances with complex unknown probability dis-
tributions, poor estimates of model structure or by a wrong selection of parameter
estimation procedure. The problem of uncertainties identiﬁcation is solved with
application of special statistical tests and visual studying of available data.
As far as we usually work with stochastic data, correct application of existing
statistical techniques provides a possibility for approximate estimation of a system (and
its model) structure. To ﬁnd “the best” model structure it is recommended to apply
adaptive estimation schemes that provide automatic search in a pre-deﬁned range of
68
P. Bidyuk et al.

model structures and parameters (model order, time lags, and possible nonlinearities). It
is often possible to perform the search in the class of regression type models with the
use of information criterion of the following type [2]:
N log ðFPEÞ ¼ N log ðVNð^hÞÞ þ N log
N þ p
N  p


;
ð1Þ
where ^h is a vector of model parameters estimates; N is a power of time series used;
FPE is ﬁnal prediction error term; VNð^hÞ is determined by the sum of squared errors;
p is a number of model parameters. The value of the criteria (1) is asymptotically
equivalent to the Akaike information criterion with N ! ∞. As the amount of data, N,
may be limited, then an alternative, the minimum description length (MDL) criterion
MDL ¼ log ðVNð^hÞÞ þ p log ðNÞ
N
could be hired to ﬁnd the model that adequately represents available data with the
minimum amount of available information.
There are several possibilities for adaptive model structure estimation: (1) applica-
tion of statistical criteria for detecting possible nonlinearities and the type of nonsta-
tionarity (integrated or heteroskedastic process); (2) analysis of partial autocorrelation
for determining autoregression order; (3) automatic estimation of the exogeneous
variable lag (detection of leading indicators); (4) automatic analysis of residual prop-
erties; (5) analysis of data distribution type and its use for selecting correct model
estimation method; (6) adaptive model parameter estimation with hiring extra data;
(7) optimal selection of weighting coefﬁcients for exponential smoothing, nearest
neighbor and other techniques. The development and use of a speciﬁc adaptation
scheme depends on volume and quality of data, speciﬁc problem statement, require-
ments to forecast estimates etc.
The adaptive estimation schemes also help to cope with the model parameters
uncertainties. New data are used to re-compute model parameter estimates that cor-
respond to possible changes in the object under study. In the cases when model is
nonlinear alternative parameter estimation techniques (say, MCMC) could be hired to
compute alternative (though admissible) sets of parameters and to select the most
suitable of them using statistical quality criteria.
3.1
Processing Some Types of Stochastic Uncertainties
While performing practical modeling very often statistical characteristics (covariance
matrix) of stochastic external disturbances and measurement noise (errors) are unknown.
To eliminate this uncertainty optimal ﬁltering algorithms are usually applied that pro-
vide for a possibility of simultaneous estimation of object (system) states and the
covariance matrices. One of the possibilities to solve the problem is optimal Kalman
ﬁlter. Kalman ﬁlter is used to ﬁnd optimal estimates of system states on the bases of the
system model represented in a convenient state space form as follows:
Model Building and Forecasting Dynamic Processes
69

xðkÞ ¼ Aðk; k  1Þxðk  1Þ þ Bðk; k  1Þuðk  1Þ þ wðkÞ
ð2Þ
where x(k) is n-dimensional vector of system states; k = 0,1,2,… is discrete time;
u(k −1) is m – dimensional vector of deterministic control variables; w(k) is n -
dimensional vector of external random disturbances; A(k, k −1) is (n  m) - matrix of
system dynamics; is B(k, k −1) (n  m) matrix of control coefﬁcients. The double
argument (k, k −1) means that the variable or parameter is used at the moment k, but
its value is based on the former (earlier) data processing including moment (k −1).
Usually the matrices A and B are written with one argument like, A(k) and B(k), to
simplify the text. Besides the main task, optimal state estimation, Kalman ﬁlter can be
used to solve the following problems: computing of short-term forecasts, estimation of
unknown model parameters including statistics of external disturbances and mea-
surement errors (adaptive extended Kalman ﬁlter), estimation of state vector compo-
nents that cannot be measured directly, and fusion of data coming from various external
sources.
Obviously stationary system model is described with constant parameters like A,
and B. As far as matrix A is a link between two consequent system states, it is also
called state transition matrix. Discrete time k and continuous time t are linked to each
other via data sampling time: Ts : t = k Ts. In the classic problem statement for optimal
ﬁltering the vector sequence of external disturbances w(k) is supposed to be zero mean
white Gaussian noise with covariance matrix Q, i.e. the noise statistics are as follows:
E½wðkÞ ¼ 0;
8k;
E½wðkÞwTðjÞ ¼ QðkÞdkj;
where dkj is Kronecker delta-function: dk j ¼
0;
k 6¼ j
1;
k ¼ j

; QðkÞ is positively deﬁned
covariance (n  n) matrix. The diagonal elements of the matrix are variances for the
components of disturbance vector w(k). Initial system state x0 is supposed to be known
and the measurement equation for vector zðkÞ of output variables is described by the
equation:
zðkÞ ¼ HðkÞxðkÞ þ vðkÞ;
ð3Þ
where HðkÞ is ðr  nÞ observation (coefﬁcients) matrix; vðkÞ is r-dimensional vector of
measurement noise with statistics: E½vðkÞ ¼ 0; E½vðkÞvTðjÞ ¼ RðkÞdkj,
where RðkÞ is ðr  rÞ positively deﬁned measurement noise covariance matrix, the
diagonal elements of which represent variances of additive noise for each measurable
variable. The noise of measurements is also supposed to be zero mean white noise
sequence that is not correlated with external disturbance wðkÞ and initial system state.
For the system (2), (3) with state vector xðkÞ it is necessary to ﬁnd optimal state
estimate ^xðkÞ at arbitrary moment k as a linear combination of estimate ^xðk  1Þ at the
previous moment ðk  1Þ and the last measurement available, zðkÞ. The estimate of
state vector ^xðkÞ is computed as optimal one with minimizing the expectation of the
sum of squared errors, i.e.:
70
P. Bidyuk et al.

E½ð^xðkÞ  xðkÞÞTð^xðkÞ  xðkÞÞ ¼ min
K ;
ð4Þ
where xðkÞ is an exact value of state vector that can be found as deterministic part of
the state Eq. (2); K is optimal matrix gain that is determined as a result of minimizing
quadratic criterion (4).
Thus, the ﬁlter is constructed to compute optimal state vector ^xðkÞ in conditions of
inﬂuence of external random system disturbances and measurement noise. Here one of
possible uncertainties arises when we don’t know estimates of covariance matrices Q
and R. To solve the problem an adaptive Kalman ﬁlter is to be constructed that allows to
compute estimates of ^Q and ^R simultaneously with the state vector ^xðkÞ. Another choice
is in constructing separate algorithm for computing the values of ^Q and ^R. A convenient
statistical algorithm for estimating the covariance matrices was proposed [11]:
^R ¼ 1
2
^B1 þ A1ð^B1  ^B2ÞðA1ÞT


;
^Q ¼ ^B1  ^R  A^RA
T;
where
^B 1 ¼ E
½ zðkÞ  A zðk  1Þ ½ zðkÞ  A zðk  1Þ T


;
^B 2 ¼ E
½ zðkÞ 
f
A2 zðk  2Þ ½ zðkÞ  A2 zðk  2Þ Tg:
The matrices ^Q and ^R are used in the optimal ﬁltering procedure as follows:
SðkÞ ¼ APðk  1ÞAT þ ^Q;
DðkÞ ¼ SðkÞ½SðkÞ þ ^R#;
PðkÞ ¼ ½I  DðkÞSðkÞ;
k ¼ 0; 1; 2;    ;
where SðkÞ and PðkÞ are prior and posterior covariance matrices of estimates errors
respectively; the symbol “#” denotes pseudo-inverse; AT means matrix transposition;
DðkÞ is a matrix of intermediate covariance results. The algorithm was successfully
applied to the covariance estimating in many practical applications. The computation
experiments showed that the values of DðkÞ become stationary after about 20–25
periods of time (sampling periods) in a scalar case, though this ﬁgure is growing
substantially with the growth of dimensionality of the system under study. It was also
determined that the parameter estimators are very sensitive to the initial conditions of
the system. The initial conditions should differ from zero enough to provide stability
for the estimates generated.
Other appropriate instruments for taking into consideration the uncertainties are
fuzzy logic, neuro-fuzzy models, Bayesian networks, appropriate types of distributions
etc. Some of statistical data uncertainties such as missing measurements, extreme
values and high level jumps of stochastic origin could be processed with appropriately
selected statistical procedures. There exists a number of data imputation schemes that
help to complete the sets of the data collected. For example, very often missing
measurements for time series could be generated with appropriately selected distri-
butions or in the form of short term forecasts. Appropriate processing of jumps and
Model Building and Forecasting Dynamic Processes
71

extreme values helps with adjusting data nonstationarity and to estimate correctly the
probability distribution for the stochastic processes under study.
3.2
Processing Data with Missing Observations (Data Are in the Form
of Time Series)
As of today for the data in the time series form the most suitable imputation techniques
are as follows: simple averaging when it is possible (when only a few values are
missing); generation of forecast estimates with the model constructed using available
measurements; generation of missing estimates from distributions the form and
parameters of which are again determined using available part of data and expert
estimates; the use of optimization techniques, say appropriate forms of EM-algorithms
(expectation maximization); exponential smoothing etc. It should also be mentioned
that optimal Kalman ﬁlter can also be used for imputation of missing data because it
contains “internal” forecasting function that provides a possibility for generating
quality short-term forecasts [12]. Besides, it has a feature of fusion the data coming
from various external sources and improving this way the quality of state vector and its
forecasts.
Further reduction of this uncertainty is possible thanks to application of several
forecasting techniques to the same problem with subsequent combining of separate
forecasts using appropriate weighting coefﬁcients. The best results of combining the
forecasts is achieved when variances of forecasting errors for different forecasting
techniques do not differ substantially (at any rate the orders of the variances should be
the same).
3.3
Coping with Uncertainties of Model Parameters Estimates
Usually uncertainties of model parameter estimates such as bias and inconsistency
result from low informative data, or data do not correspond to normal distribution, what
is required in a case of LS application for parameter estimation. This situation may also
take place in a case of multicollinearity of independent variables and substantial
inﬂuence of process nonlinearity that for some reason has not been taken into account
when model was constructed. When power of the data sample is not satisfactory for
model construction it could be expanded by applying special techniques, or simulation
is hired, or special model building techniques, such as group method for data handling
(GMDH), are applied. Very often GMDH produces results of acceptable quality with
rather short samples. If data do not correspond to normal distribution, then ML tech-
nique could be used or appropriate Monte Carlo procedures for Markov Chains
(MCMC) [13]. The last techniques could be applied with quite acceptable computa-
tional expenses when the number of parameters is not large.
3.4
Dealing with Model Structure Uncertainties
When considering mathematical models it is convenient to use proposed here a uniﬁed
notion of a model structure which we deﬁne as follows: S ¼ r; p; m; n; d; w; l
f
g, where
r is model dimensionality (number of equations); p is model order (maximum order of
72
P. Bidyuk et al.

differential or difference equation in a model); m is a number of independent variables
in the right hand side of a model; n is a nonlinearity and its type; d is a lag or output
reaction delay time; is stochastic external disturbance and its type; l are possible
restrictions for the variables and/or parameters. When using DSS, the model structure
should practically always be estimated using data. It means that elements of the model
structure accept almost always only approximate values. When a model is constructed
for forecasting we build several candidates and select the best one of them with a set
model quality statistics. Generally we could deﬁne the following techniques to ﬁght
structural uncertainties: gradual improvement of model order (AR(p) or ARMA(p, q))
applying adaptive approach to modeling and automatic search for the “best” structure
using complex statistical quality criteria; adaptive estimation (improvement) of input
delay time (lag) and data distribution type with its parameters; describing detected
process nonlinearities with alternative analytical forms with subsequent estimation of
model adequacy and forecast quality. As another example of complex statistical model
adequacy and forecast quality criterion could be the following:
J ¼ 1  R2
		
		 þ a ln
X
N
k ¼ 1
e2ðkÞ
"
#
þ 2  DW
j
j þ b ln 1 þ MAPE
ð
Þ þ U ! min
^h i
;
where R2 is a determination coefﬁcient; DW is Durbin-Watson statistic; MAPE is mean
absolute percentage error for forecasts;
P
N
k ¼ 1
e2ðkÞ ¼ P
N
k ¼ 1
½yðkÞ  ^yðkÞ2 is sum of squared model errors; U is Theil coefﬁ-
cient that measures forecasting characteristic of a model; a, b are appropriately selected
weighting coefﬁcients; ^hi is parameter vector for the i-th candidate model. A criterion
of this type is used for automatic selection of the best candidate model. The criterion
also allows operation of DSS in the automatic adaptive mode. Obviously, other forms
of the complex criteria are possible. While constructing the criterion it is important not
to overweigh separate members in the right hand side.
3.5
Coping with Uncertainties of a Level (Amplitude) Type
The use of random (i.e. with random amplitude or a level) and/or non-measurable
variables leads to necessity of hiring fuzzy sets for describing such situations. The
variable with random amplitude can be described with some probability distribution if
the measurements are available or they come for analysis in acceptable time span.
However, some variables cannot be measured (registered) in principle, say amount of
shadow capital that “disappears” every month in offshore, or amount of shadow salaries
paid at some company, or a technology parameter that cannot be measures on-line due
to absence of appropriate gauge. In such situations we could assign to the variable a set
of possible values in the linguistic form as follows: capital amount = {very low, low,
medium, high, very high}. There exists a complete necessary set of mathematical
operations to be applied to such fuzzy variables. Finally fuzzy value could be trans-
formed into usual exact form using known techniques.
Model Building and Forecasting Dynamic Processes
73

3.6
Processing Probabilistic Uncertainties
To ﬁght probabilistic uncertainties it is possible to hire Bayesian approach that helps to
construct models in the form of conditional distributions for the sets of random vari-
ables. Usually such models represent the process (under study) variables themselves,
stochastic disturbances and measurement errors or noise. The problem of distribution
type identiﬁcation also arises in regression modeling. Each probability distribution is
characterized by a set of speciﬁc values that random variable could take and the
probabilities for these values. The problem is in the distribution type identiﬁcation and
estimating its parameters. The probabilistic uncertainty (will some event happen or not)
could be solved with various models of Bayesian type. This approach is known as
Bayesian programming or paradigm. The generalized structure of the Bayesian pro-
gram includes the following steps: (1) problem description and statement with putting
the question regarding estimation of conditional probability in the form: p ðXijD; KnÞ,
where Xi - is the main (goal) variable or event; the probability p should be found as a
result of application of some probabilistic inference procedure; (2) statistical (experi-
mental) data D and knowledge Kn are to be used for estimating model and parameters
of speciﬁc type; (3) selected and applied probabilistic inference technique should give
an answer to the question put above; (4) analysis of quality of the ﬁnal result. The steps
given above are to some extent “standard” regarding model constructing and com-
puting probabilistic inference using statistical data available. This sequence of actions
is naturally consistent with the methods of cyclic structural and parametric model
adaptation to the new data and operating modes (and possibly expert estimates).
One of the most popular Bayesian approaches today is created by the models in the
form of static and dynamic Bayesian networks (BN). Bayesian networks are proba-
bilistic and statistical models represented in the form of directed acyclic graphs
(DAG) with vertices as variables of an object (system) under study, and the arcs
showing existing causal relations between the variables. Each variable of BN is
characterized with complete ﬁnite set of mutually excluding states. Formally BN could
be represented with the four following components: N ¼ \V; G; P; T [ , where V
stands for the set of model variables; G represents directed acyclic graph; P is joint
distribution of probabilities for the graph variables (vertices), V ¼ fX1; . . .; Xng; and
T denotes conditional and unconditional probability tables for the graphical model
variables [14, 15]. The relations between the variables are established via expert
estimates or applying special statistical and probabilistic tests to statistical data (when
available) characterizing dynamics of the variables.
The process of constructing BN is generally the same as for models of other types,
say regression models. The set of the model variables should satisfy the Markov
condition that each variable of the network does not depend on all other variables but
for the variable’s parents. In the process of BN constructing ﬁrst the problem is solved
of computing mutual information values between all variables of the net. Then an
optimal BN structure is searched using acceptable quality criterion, say well-known
minimum description length (MDL) that allows analyzing and improving the graph
(model) structure at each iteration of the learning algorithm applied. Bayesian networks
provide the following advantages for modeling: the model may include qualitative and
quantitative variables simultaneously as well as discrete and continuous ones; number
74
P. Bidyuk et al.

of the variables could be very large (thousands); the values for conditional probability
tables could be computed with the use of statistical data and expert estimates; the
methodology of BN constructing is directed towards identiﬁcation of actual causal
relations between the variables hired what results in high adequacy of the model; the
model is also operable in conditions of missing data.
To reduce an inﬂuence of probabilistic and statistical uncertainties on models
quality and the forecasts based upon them it is also possible to use the models in the
form of Bayesian regression based on analysis of actual distributions of model vari-
ables and parameters. Consider a simple two variables regression
yðkÞjxðkÞ ¼ b1 þ b2xðkÞ þ uðkÞ;
k ¼ 0; 1; . . .; n:
It is supposed that of random values u1; . . .; un are independent and belong, for
example, to normal distribution, fuðkÞg  Nð0; r2
uÞ; here vector of unknown parame-
ters includes three elements, h ¼ ðb1; b2; r2
uÞT. The likelihood function for dependent
variable y ¼ ðy1; . . .; ynÞT and predictor x ¼ ðx1; . . .; xnÞT without proportion coefﬁ-
cient is determined as follows:
Lðyjx; b1; b2; ruÞ ¼ 1
rN
u
exp
 1
2r2
u
X
N
k¼1
½yðkÞ  b1  b2xðkÞ2
(
)
Using simpliﬁed (non-informative) distributions for the model parameters:
gðb1; b2; ruÞ ¼ g1ðb1Þg2ðb2Þg3ðruÞ;
g1ðb1Þ / const;
g2ðb21Þ / const;
g3ðruÞ / 1=ru;
and Bayes theorem it is possible to ﬁnd joint posterior distribution for the parameters in
the form [16]:
hðb1; b2; rujx; yÞ / 1
r
1
rN exp  1
2r2
X
N
k¼1
yðkÞ  b1  b2xðkÞ
ð
Þ2
"
#
;
 1\b1; b2\ þ 1;
0\ru\1
Maximum likelihood estimates for the model parameters are determined as follows:
^b1 ¼ y  ^b2x;
^b2 ¼
PN
k¼1 ½xðkÞ  x½yðkÞ  y
PN
k¼1 ½xðkÞ  x PN
k¼1 ½yðkÞ  y
;
Model Building and Forecasting Dynamic Processes
75

where x ¼ N1 PN
k¼1 xðkÞ;
y ¼ N1 PN
k¼1 yðkÞ, with unbiased sample estimate of
variance:
^r2
u ¼ s2 ¼
1
N  2
XN
k¼1 ½yðkÞ  ^b1  ^b2xðkÞ
Joint posterior density for the model parameters corresponds to two dimensional
Student distribution:
h1ðb1; b2jy; xÞ /
ðN  2Þs2

þ Nðb1  ^b1Þ2 þ ðb2  ^b2Þ2 PN
k¼1 xðkÞ2
þ 2ðb1  ^b1Þðb2  ^b2Þ PN
k¼1 xðkÞ
o0;5N
:
This way we get a possibility for using more exact distributions of models variables
and parameters what helps to enhance model quality. Using new observation x and
prior information regarding particular model it is possible to determine the forecast
interval for the dependent variable, y, as follows:
pðyjxÞ ¼
ZZZ
Lðyjx; b1; b2; rÞhðb1; b2; rÞjx; yÞdb1; db2; dr:
Another useful Bayesian approach is in hierarchical modeling that is based on a set
of simple conditional distributions comprising one model. The approach is naturally
combined with the theory of computing Bayesian probabilistic inference using modern
computational procedures [17]. The hierarchical models belong to the class of marginal
models where the ﬁnal result is provided in the form of a distribution P(y), where y is
available data vector. The models are formed from the sequence of conditional dis-
tributions for selected variables including the hidden ones. The hierarchical represen-
tation of parameters usually supposes that data, y, is situated at the lower (ﬁrst) level,
model parameters (second level) h ¼ ðhi; i ¼ 1; 2; . . .; nÞ, hi  Nðl; s2Þ, determine
distributions of dependent variables yi  Nðhi; r2Þ; i ¼ 1; 2; . . .; n, and parameters fhig
are determined by the pair, ðl; s2Þ, of the third level. Supposing the parameters r2 and
s2 accept known ﬁnite values, and parameter l is unknown with the prior pl, then joint
prior density for ðh; lÞ could be presented in the form: plðlÞ Q
i phðhijlÞ, and the prior
for parameter vector h will be deﬁned by the integral: pðhÞ ¼
R
plðlÞ Q
i
phðhijlÞdl.
4
Data, Model and Forecasts Quality Criteria
To achieve reliable high quality ﬁnal result of risk estimation and forecasting at each
stage of computational hierarchy separate sets of statistical quality criteria have been
used. Data quality control is performed with the following criteria:
– database analysis for missing values using developed logical rules, and imputation
of missed values with appropriately selected techniques;
76
P. Bidyuk et al.

– analysis of data for availability of outliers with special statistical tests, and pro-
cessing of outliers to reduce their negative inﬂuence on statistical properties of the
data available;
– normalizing of data in the selected range in a case of necessity;
– application of low-order digital ﬁlters (usually low-pass ﬁlters) for separation of
observations from measurement noise;
– application of optimal (usually Kalman) ﬁlters for optimal state estimation and
ﬁghting stochastic uncertainties;
– application of principal component method to achieve desirable level of orthogo-
nalization between the variables selected;
– computing of extra indicators for the use in regression and other models (say,
moving average processes based upon measurements of dependent variables).
It is also useful to test how informative is the data collected. Very formal indicator
for the data being informative is its sample variance. It is considered formally that the
higher is the variance the richer is the data with information. Another criterion is based
on computing derivatives with a polynomial that describes data in the form of a time
series. For example, the equation given below can describe rather complex process with
nonlinear trend and short-term variations imposed on the trend curve:
yðkÞ ¼ a0 þ
X
p
i¼1
aiyðk  iÞ þ c1k þ c2k2 þ . . . þ cmkm þ eðkÞ;
Where y(k) is basic dependent variable; ai, ci are model parameters; k ¼ 0; 1; 2; . . . is
discrete time; e(k) is a random process that integrates the inﬂuence of external distur-
bances to the process being modeled as well as model structure and parameters errors.
Autoregressive part of model (1) describes the deviations that are imposed on a trend,
and the trend itself is described with the m-th order polynomial of discrete time k. In this
case maximum number of derivatives could be m, though in practice actual number of
derivatives is deﬁned by the largest number i of parameter ci, that is statistically sig-
niﬁcant. To select the best model constructed the following statistical criteria are used:
determination coefﬁcient (R2); Durbin-Watson statistic (DW); Fisher F-statistic; Akaike
information criterion (AIC), and residual sum of squares (SSE). The forecasts quality is
estimated with hiring the criteria mentioned above in (1) and (2). To perform automatic
model selection the above mentioned combined criteria (1) could be hired. The power of
the criterion was tested experimentally and proved with a wide set of models and
statistical data. Thus, the three sets of quality criteria are used to insure high quality of
ﬁnal result.
5
Conclusions
The general methodology was proposed for mathematical modeling and forecasting
dynamics of economic and ﬁnancial processes that is based on the system analysis
principles. As instrumentation for ﬁghting possible structural, statistic and parametric
uncertainties the following techniques are used: Kalman ﬁlter, various missing data
Model Building and Forecasting Dynamic Processes
77

imputation techniques, multiple methods for model parameter estimation, and Bayesian
programming approach. The issues of estimating the quality of forecasted solutions are
considered.
References
1. Tsay, R.S.: Analysis of Financial Time Series. Wiley, Chicago (2010). 715 pages
2. Harris, L., Hong, X., Gan, Q.: Adaptive Modeling, Estimation and Fusion from Data.
Springer, Heidelberg (2002). 323 pages
3. Congdon, P.: Applied Bayesian Modeling. Wiley, Chichester (2003). 472 pages
4. De Lurgio, S.M.: Forecasting Principles and Applications. McGraw-Hill, Boston (1998).
802 pages
5. Taylor, S.J.: Modeling stochastic volatility: a review and comparative study. Math. Fin. 2,
183–204 (1994)
6. Burstein, F., Holsapple, C.W.: Handbook of Decision Support Systems. Springer-Verlag,
Heidelberg (2008). 908 pages
7. Hollsapple, C.W., Winston, A.B.: Decision Support Systems. West Publishing Company,
Saint Paul (1996). 860 pages
8. Bidyuk, P.I., Gozhyj, A.P.: Computer Decision Support Systems. Black Sea State University
named after Petro Mohyla, Mykolaiv (2012). 380 pages
9. Xekalaki, E., Degiannakis, S.: ARCH Models for Financial Applications. Wiley, Chichester
(2010). 550 pages
10. Chatﬁeld, C.: Time Series Forecasting. Chapman & Hall, CRC, Boca Raton (2000).
267 pages
11. Anderson, W.N., Kleindorfer, G.B., Kleindorfer, P.R., Woodroofe, M.B.: Consistent
estimates of the parameters of a linear system. Ann. Math. Stat. 40, 2064–2075 (1969)
12. Gibbs, B.P.: Advanced Kalman Filtering, Least-Squares and Modeling, Wiley, Hoboken
(2011). 267 pages
13. Gilks, W.R., Richardson, S., Spiegelhalter, D.J.: Markov Chain Monte Carlo in Practice.
Chapman & Hall, CRC, New York (2000). 486 pages
14. Jensen, F.V., Nielsen, T.D.: Bayesian Networks and Decision Graphs. Springer, New York
(2007). 457 pages
15. Zgurovsky, M.Z., Bidyuk, P.I., Terentyev, O.M., Prosyankina-Zharova, T.I.: Bayesian
Networks in Decision Support Systems. Edelweis, Kyiv (2015). 300 pages
16. Bernardo, J.M., Smith, A.F.M.: Bayesian Theory. Wiley, New York (2000). 586 pages
17. Bolstad, W.M.: Understanding Computational Bayesian Statistics. Wiley, Hoboken (2010).
334 pages
78
P. Bidyuk et al.

Dynamic Inertia Weight in Particle
Swarm Optimization
Bożena Borowska(&)
Institute of Information Technology, Lodz University of Technology,
Wólczańska 215, 90-924 Łodź, Poland
bozena.borowska@p.lodz.pl
Abstract. This paper proposes a particle swarm optimization method with a
novel strategy for inertia weight. Instead of a commonly used linear inertia weight,
a nonlinear, dynamic changing inertia weight is applied. The new presented
weight is a function of the worst and the best ﬁtness of individuals of a population.
In order to investigate the effectiveness of the proposed strategy tests on a set of
benchmark function were conducted. The results were compared with those
obtained through the LDW-PSO method, EWPSO and the RNW-PSO methods.
Keywords: Inertia weight  Particle swarm optimization
Swarm intelligence  Optimization
1
Introduction
Particle Swarm Optimization (PSO) is an optimization method modeled on the social
behavior of the group of organisms in their natural environment [1]. Proposed for the
ﬁrst time by Kennedy and Eberhart in 1995 [2, 3], now belongs to the most frequently
used evolutionary methods. Because of its many advantages such as simplicity, few
parameters to adjust and easy implementation, it has been applied in almost all ﬁelds of
science and engineering, where optimization is required [4–9]. A parameter that sig-
niﬁcantly inﬂuences the effectiveness of the particle swarm optimization method is
inertia weight. Its role is to control deviation of the particles from their original
direction and keep balance between local and global explorations. An incorrectly
selected value of inertia weight maintains a balance between local and global explo-
ration and can negatively affect the algorithm performance.
A factor of the inertia weight was ﬁrst proposed and introduced to the PSO method
by Shi and Eberhart [10, 11]. In the subsequent years, a lot of research on inertia weight
have been undertaken. Clerc [12] and Trelea [13] suggested that the inertia weight
should rather be a constant value. She and Eberhart [11, 14] recommended a linear
decreasing inertia weight (LDW). A ﬂexible inertia weight, that can be a positive or
negative real number, was used by Han et al. [15]. In order to avoid some troubles of
the LDW strategy connected with a poor local search ability at the beginning of the
method, and the lack of global search ability at the end of the method, Zhang et al. [16]
proposed a random inertia weight (RNW). PSO with random inertia weight was also
proposed by Niu et al. [17] and Eberhart and She [18]. Three different concepts of
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_6

inertia weight were investigated by Arumugan and Rao [19, 20] and Umapathy et al.
[21] They considered a constant inertia weight (CIW), time-varying inertia weight
(TVIW) and a global-local best inertia weight (GLbestIW) and reported that GLbestIW
outperforms the other methods in terms of high quality solution, consistency, faster
convergence and accuracy. Another approach was developed by Yang et al. [22]. In
their study, the inertia weight depends on two parameters: an aggregation degree factor
and an evolution speed factor, and is different for each individual of the swarm.
A modiﬁed inertia weight was also considered by Miao et al. [23]. In this case, weight
is updated by dispersion degree and advance degree factors. Performance of PSO with
nonlinear strategies was examined by Chauhan et al. [24]. The authors examined an
exponential self adaptive (DESIWPSO) and a dynamic (DEDIVPSO) inertia weight
based on Gompertz function and a ﬁne grained inertia weight (FGIWPSO). Another
exponential inertia weight was also developed by Ememipour et al. [25], Borowska
[26] and Ghali et al. [27]. A fuzzy adaptive inertia weight was proposed by Shi and
Eberhart [28]. An approach based on the fuzzy systems was also described in [29–33].
This paper presents a modiﬁed PSO method named DWPSO in which a new
strategy for inertia weight was developed. In DWPSO, the values of inertia weight are
dynamically changing and are determined based on ﬁtness function. The proposed
weight is a function of the best and the worst ﬁtness of particles. Moreover, the new
presented method was tested on a set of benchmark function. Then the results were
compared to the RNW-PSO method with a random inertia weight [16], the LDW-PSO
method with a linear decreasing inertia weight, and the nonlinear EWPSO method [26].
2
The PSO Method
The PSO algorithm belongs to the group of optimization methods based on the pop-
ulation. In case of PSO this population is called swarm and consists of individuals
named particles. Each particle is a point of the space of the feasible solutions. The
movement of the particle in this space enables the velocity vector. Initial location and
velocity of the particle are randomly generated at the beginning of the algorithm. The
quality of the particles is evaluated according to the ﬁtness function of the optimization
problem. In each iteration, particles update information about their own best position
(named pbest) found so far. The knowledge about the particle with the best ﬁtness
among all the particles in the whole swarm (named gbest) is also remembered and
updated in every iteration. The change of the velocity and location of the particles is
carried out according to the formula:
Vi ¼ wVi þ c1r1ðpbesti  XiÞ þ c2r2ðgbest  XiÞ
ð1Þ
Xi ¼ Xi þ Vi
ð2Þ
where Vi = (vi1, vi2, … , viD) is a velocity vector of the particle i in the D-dimensional
search space. Vector Xi = (xi1, xi2, … , xiD) represents a location of the particle
i. Factor w is the inertia weight. Vector pbesti = (pbesti1, pbesti2, … , pbestiD) means a
personal best location of the particle i and gbest = (gbest1, gbest2, … , gbestD) denotes
80
B. Borowska

a location of the particle with the best ﬁtness function among all the particles in the
whole swarm. The variables c1 and c2 are acceleration coefﬁcients. They decide how
strong the particle is inﬂuenced by its knowledge about its pbest and gbest value.
Parameters r1 and r2 represent randomly generated numbers between 0 and 1 to
maintain diversity of the population.
3
The Proposed DWPSO Algorithm
The proposed DWPSO algorithm is a variant of the particle swarm optimization
method in which the new strategy for determination of inertia weight was introduced.
In DWPSO a commonly used linear weight was omitted and replaced with an expo-
nential inertia weight. In the proposed approach, the inertia weight is changing
dynamically based on a ﬁtness of the particles in the swarm. In each iteration, the
particles of the swarm move in the search space according to Eqs. 1–2. After evaluating
the quality of the new location of the particles, the individuals with the best and the
worst ﬁtness are found and recorded. Then, on their basis, the new value of the weight
is counted. The new weight is represented by a nonlinear function of the best and the
worst ﬁtness of the particles. In each iteration, a different inertia weight is calculated
and applied for the whole swarm. The proposed strategy has been deﬁned as follows:
dw ¼ ððgbest  fmax=2Þ1=ðð ln½fminÞÞÞ=105fh
ð3Þ
wðt þ 1Þ ¼ wðtÞ  dwðtÞ
ð4Þ
where fmax and fmin are the values of maximal and minimal ﬁtness in the current
iteration, respectively. Factor fh is a randomly generated number in the range [0, 1].
4
Results
The simulation tests of the DWPSO method with proposed strategy was carried out on
a set of nonlinear benchmark functions depicted in Table 1.
Table 1. Optimization test functions
Function
Formula
Minimum Range of x
Sphere
f1 ¼ P
n
i¼1
x2
i
0
(−100, 100)
Rosenbrock f3 ¼ P
n1
i¼1
½100 xi þ 1  x2
i

2 þ ðxi  1Þ2
0
(−30, 30)
Ackley
f4 ¼ 20 exp 0:2
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n
P
n
i¼1
x2
i
s
 
!
 exp
1
n
P
n
i¼1
cosð2pxiÞ


þ 20 þ e
0
(−32, 32)
Rastrigin
f5 ¼ P
n
i¼1
ðx2
i  10 cos 2pxi
ð
Þ þ 10Þ
0
(−5.12, 5.12)
Griewank
f6 ¼
1
4000
P
n
i¼1
x2
i  Q
n
i¼1
cos
xiﬃ
i
p
 
þ 1
0
(−600, 600)
Dynamic Inertia Weight in Particle Swarm Optimization
81

The results of the tests were compared with the performance of PSO with a random
number inertia weight (RNWPSO), LDW-PSO with a linear decreasing weight as well
as EWPSO with an exponential inertia weight.
The DWPSO algorithm started with the inertia weight of 0.7 and was changing
according to the formula 3−4. The acceleration coefﬁcients c1, c2 used for executed
computation were set to 1.6. The simulations were performed with four dimension sizes
D = 10, 20 and 30 for N = 20, 40, 60 and 80 particles in the swarm respectively. Each
experiment was run 50 times. In all cases, the iteration number was 1000.
The exemplary results of the tests performed for 20, 40 and 80 particles of the
swarm are illustrated in Tables 2, 3 and 4. The presented values were averaged over 50
trials.
Table 2. Performance of the LDW-PSO, RNW-PSO, EWPSO and DWPSO algorithms for
Rosenbrock function
Population
size
Dimension Algorithm LDW-PSO
RNW-PSO
EWPSO
DWPSO
20
10
Mean
4.0933e+001 1.8401e+001 1.5489e+001 4.9787e+000
St. Dev.
2.8127e+001 2.3554e+001 2.0147e+001 6.0743e+000
Min
4.3607e+000 8.0913e−001 2.2132e−001 1.5060e−001
20
Mean
8.3192e+001 1.9984e+002 7.2504e+001 5.0429e+001
St. Dev.
4.5308e+001 2.8759e+002 1.3315e+002 4.1276e+001
Min
1.5216e+001 1.5318e+001 9.0325e−001 2.1551e+000
30
Mean
1.3507e+002 4.2856e+002 1.7210e+002 7.2359e+001
St. Dev.
1.5563e+002 5.3721e+002 1.5334e+002 5.5085e+001
Min
2.3142e+001 2.9608e+001 2.3512e+001 9.3401e+000
40
10
Mean
2.5379e+001 1.7240e+001 1.4231e+001 4.1631e+000
St. Dev.
1.7455e+001 3.2178e+001 5.3622e+000 9.1435e+000
Min
2.8927e−002 1.0806e+000 1.4523e−002 4.8498e−002
20
Mean
5.7344e+001 5.6102e+001 4.7376e+001 4.0605e+001
St. Dev.
5.3637e+001 4.9630e+001 4.8744e+001 6.4665e+001
Min
6.1431+000
5.7073e+000 1.9198e+000 1.5801e−001
30
Mean
7.1917e+001 6.9343e+001 6.7008e+001 5.2569e+001
St. Dev.
1.4425e+002 3.5032e+001 7.5692e+001 2.9028e+001
Min
1.1633e+001 1.0225e+001 6.8376e+000 2.5110e+000
80
10
Mean
1.8261e+001 1.5728e+001 1.2415e+001 5.5479e+000
St. Dev.
2.9027e+001 2.3569e+001 2.2178e+001 1.8712e+001
Min
1.2415e−001 2.8725e−003 9.4684e−003 2.2647e−002
20
Mean
4.4502e+001 3.3485e+001 2.2570e+001 2.0819e+001
St. Dev.
3.2417e+001 2.7279e+001 2.9342e+001 3.0246e+001
Min
1.5655e−001 5.2635e−001 1.5904e−005 1.5185e+000
30
Mean
6.6138e+001 4.2504e+001 3.9453e+001 3.8589e+001
St. Dev.
1.3050e+002 3.2997e+001 2.6149e+001 3.4396e+001
Min
9.5624e−001 1.1682e+001 1.2736e+001 1.3776e−002
82
B. Borowska

Table 3. Performance of the LDW-PSO, RNW-PSO, EWPSO and DWPSO algorithms for
Rastrigin function
Population
size
Dimension Algorithm LDW-PSO
RNW-PSO
EWPSO
DWPSO
20
10
Mean
6.4812e+000 6.3174e+000 6.1075e+000 7.7109e+000
St. Dev.
4.5160e+000 7.2541e+000 4.1338e+000 4.8600e+000
Min
3.3097e+000 3.0705e+000 9.9775e−001 9.9496e−001
20
Mean
4.5202e+001 5.3877e+001 4.3706e+001 3.7609e+001
St. Dev.
1.6925e+001 1.6786e+001 1.4537e+001 1.2697e+001
Min
2.2130e+001 3.1973e+001 2.5166e+001 1.9899e+001
30
Mean
8.1753e+001 8.8445e+001 8.3524e+001 7.1985e+001
St. Dev.
2.2488e+001 2.9007e+001 2.4530e+001 2.1340e+001
Min
4.5357e+001 6.2319e+001 3.8712e+001 4.5768e+001
40
10
Mean
4.3519e+000 4.3854e+000 4.1303e+000 4.7657e+000
St. Dev.
3.9689e+000 2.2077e+000 3.6194e+000 3.1695e+000
Min
1.0526e+000 2.8343e+000 1.2376e+000 1.9899e+000
20
Mean
3.6697e+001 3.4463e+001 3.1793e+001 2.3779e+001
St. Dev.
1.1455e+001 9.0821e+000 1.3508e+001 7.9528e+000
Min
1.1243e+001 1.5354e+001 1.9487e+001 2.9849e+000
30
Mean
7.3137e+001 7.2862e+001 6.7972e+001 6.3329e+001
St. Dev.
3.3256e+001 2.0617e+001 2.5146e+001 1.8331e+001
Min
3.6972e+001 4.1565e+001 3.3887e+001 2.7859e+001
80
10
Mean
2.7314e+000 2.5931e+000 2.4145e+000 2.5801e+000
St. Dev.
4.3898e+000 2.8769e+000 1.8860e+000 2.2603e+000
Min
9.7515e−001 9.9004e−001 9.7073e−001 0.0000e+000
20
Mean
3.1305e+001 3.0798e+001 2.8712e+001 2.1292e+001
St. Dev.
9.4789e+000 9.7543e+000 1.1676e+001 8.4036e+000
Min
1.6002e+001 1.6200e+001 1.0841e+001 8.9546e+000
30
Mean
5.7351e+001 5.5917e+001 5.5712e+001 5.3379e+001
St. Dev.
1.6879e+001 1.7884e+001 1.6234e+001 1.2905e+001
Min
4.3125e+001 3.3266e+001 3.6258e+001 2.9849e+001
Table 4. Performance of the LDW-PSO, RNW-PSO, EWPSO and DWPSO algorithms for
Griewank function
Population
size
Dimension Algorithm
LDW-PSO
RNW-PSO
EWPSO
DWPSO
20
10
Mean value 9.3627e−002 1.3901e−001 8.1573e−002 8.2364e−002
St. Dev.
7.9415e−002 4.8233e−002 3.5967e−002 5.5734e−002
Min
2.0163e−002 3.0994e−002 1.3029e−002 9.8573e−003
20
Mean value 5.8212e−002 5.6745e−002 4.3195e−002 3.1448e−002
St. Dev.
6.4934e−002 4.2267e−002 2.7830e−002 2.9442e−002
Min
2.0407e−005 00000e+000
00000e+000
0.0000e+000
30
Mean value 1.9235e−001 4.1733e−000 1.7908e−001 6.6079e−002
St. Dev.
2.7506e−001 3.7786e−000 2.6839e−001 1.2421e−001
Min
2.9618e−009 1.3545e−001 1.1424e−005 3.4376e−010
(continued)
Dynamic Inertia Weight in Particle Swarm Optimization
83

The average best ﬁtness in the following iterations for both DWPSO, EWPSO,
RNW-PSO algorithms and LDW-PSO model for 40 particles (swarm size) and 30
dimensions is illustrated in Figs. 1, 2, 3 and 4. The vertical coordinates indicate the
average best ﬁtness in a logarithmic scale.
The results of simulations show that the proposed DWPSO method is more
effective than the other methods investigated in this study. The dynamic strategy for
inertia weight introduced for DWPSO facilitates the algorithm to maintain a diversity
of the individuals in the search space and helps overcome the problem premature
convergence.
Table 4. (continued)
40
10
Mean value 7.6583e−002 7.2025e−002 6.8570e−002 6.9333e−002
St. Dev.
5.9771e−002 4.3162e−002 1.6405e−002 3.7984e−002
Min
6.7088e−003 5.9476e−002 1.8311e−002 2.7037e−002
20
Mean value 5.4734e−002 4.3979e−002 4.0720e−002 2.0134e−002
St. Dev.
5.1682e−002 3.1848e−002 2.8183e−002 1.7913e−002
Min
8.6707e−004 00000e+000
0.0000e+000 6.5454e−003
30
Mean value 3.4395e−002 3.2599e−002 2.4759e−002 1.8896e−002
St. Dev.
4.1504e−002 3.8676e−002 3.5383e−002 1.5956e−002
Min
6.2816e−003 7.9302e−003 3.3418e−013 2.1289e−005
80
10
Mean value 7.2003e−002 6.5499e−002 5.8677e−002 6.2356e−002
St. Dev.
4.1688e−002 5.4212e−002 3.2431e−002 2.2412e−002
Min
5.4053e−003 3.3827e−002 8.5972e−004 2.4603e−002
20
Mean value 2.8475e−002 2.5688e−002 2.0084e−002 1.0083e−002
St. Dev.
1.7142e−002 2.0213e−002 1.5755e−002 1.5652e−002
Min
0.0000e+000 00000e+000
0.0000e+000 0.0000e+000
30
Mean value 2.1659e−002 1.7706e−002 9.5268e−003 8.8063e−003
St. Dev.
1.8374e−002 1.8195e−002 1.5169e−002 1.3302e−002
Min
0.0000e+000 00000e+000
0.0000e+000 0.0000e+000
1.0E+01
1.0E+02
1.0E+03
1.0E+04
1.0E+05
1.0E+06
1.0E+07
1.0E+08
1.0E+09
0
100
200
300
400
500
600
700
800
900 1000
Average Best Fitness
Iterations
LDW-PSO
RNW-PSO
EWPSO
DWPSO
Fig. 1. The average best ﬁtness for Rosenbrock30 and the population of 40 particles
84
B. Borowska

5.0E+01
5.0E+02
0
100
200
300
400
500
600
700
800
900 1000
Average Best Fitness
Iterations
LDW-PSO
RNW-PSO
EWPSO
DWPSO
Fig. 2. The average best ﬁtness for Rastrigin30 and the population of 40 particles
1.0E-02
2.0E+00
4.0E+00
6.0E+00
8.0E+00
1.0E+01
1.2E+01
1.4E+01
1.6E+01
0
100
200
300
400
500
600
700
800
900 1000
Average Best Fitness
Iterations
LDW-PSO
RNW-PSO
EWPSO
DWPSO
Fig. 3. The average best ﬁtness for Ackley30 and the population of 40 particles
1.0E-02
1.0E-01
1.0E+00
1.0E+01
1.0E+02
1.0E+03
0
100
200
300
400
500
600
700
800
900 1000
Average Best Fitness
Iterations
LDW-PSO
RNW-PSO
EWPSO
DWPSO
Fig. 4. The average best ﬁtness for Griewank30 and the population of 40 particles
Dynamic Inertia Weight in Particle Swarm Optimization
85

In almost all cases (except D = 10), the average function values found by DWPSO
were lower than the results achieved by LDW-PSO and RNW-PSO, and lower or in
rare cases comparable to those obtained by EWPSO. Moreover, the lowest standard
deviation for DWPSO reported in most cases indicates its better stability compared to
remaining investigated method. Furthermore, the minimum value was also lower in
case of the DWPSO algorithm.. Additionally, in most simulations, DWPSO converged
faster than LDW-PSO, RNW-PSO and EWPSO (Figs. 1, 2, 3 and 4) and only at the
beginning, in the ﬁrst two hundred iterations, the algorithm converged a bit slower than
EWPSO or RNW-PSO (After ﬁrst two hundred iterations DWPSO was the fastest).
The RNW-PSO algorithm converged slower than EWPSO but still better than
LDW-PSO.
Different performance of the algorithms have been noticed only for Griewank and
Rastrigin functions and small dimensions. In case of Griewank function with D = 10
dimension size, the DWPSO algorithm performed a bit worse than EWPSO but better
than RNW-PSO and LDW-PSO. For Rastrigin function with D = 10 for 20 and 40
particles, DWPSO achieve worse results than the remaining algorithms even when the
minimum value was lower.
5
Summary
In this study, a modiﬁed particle swarm optimization algorithm named DWPSO with a
novel strategy for inertia weight has been proposed. In the considered approach, instead
of a commonly used constant or linear decreasing inertia weight, a dynamically
changing weight was adopted. Values of the inertia weight coefﬁcient depend on ﬁtness
of the individuals of the population. The effectiveness of the proposed strategy was
tested on a set of benchmark test functions. The results of the simulations were
compared with those obtained through the nonlinear EWPSO method, the RNW-PSO
method with a random inertia weight and the LDW-PSO method with a linear
decreasing inertia weight.
The use of the proposed strategy helps maintain diversity of individuals of the
population and performs better than the other investigated methods. Furthermore, the
DWPSO algorithm is also faster and more efﬁcient in avoiding the premature con-
vergence, compared to the other methods.
References
1. Robinson, J., Rahmat-Samii, Y.: Particle swarm optimization in electromagnetics. IEEE
Trans. Antennas Propag. 52, 397–407 (2004)
2. Kennedy, J., Eberhart, R.C.: Particle swarm optimization. In: IEEE International Conference
on Neural Networks, Perth, Australia, pp. 1942–1948 (1995)
3. Kennedy, J., Eberhart, R.C., Shi, Y.: Swarm Intelligence. Morgan Kaufmann Publishers, San
Francisco (2001)
4. Guedria, N.B.: Improved accelerated PSO algorithm for mechanical engineering optimiza-
tion problems. Appl. Soft Comput. 40, 455–467 (2016)
86
B. Borowska

5. Dolatshahi-Zand, A., Khalili-Damghani, K.: Design of SCADA water resource management
control center by a bi-objective redundancy allocation problem and particle swarm
optimization. Reliab. Eng. Syst. Saf. 133, 11–21 (2015)
6. Mazhoud, I., Hadj-Hamou, K., Bigeon, J., Joyeux, P.: Particle swarm optimization for
solving engineering problems: a new constraint-handling mechanism. Eng. Appl. Artif.
Intell. 26, 1263–1273 (2013)
7. Yildiz, A.R., Solanki, K.N.: Multi-objective optimization of vehicle crashworthiness using a
new particle swarm based approach. Int. J. Adv. Manuf. Technol. 59, 367–376 (2012)
8. Hajforoosh, S., Masoum, M.A.S., Islam, S.M.: Real-time charging coordination of plug-in
electric vehicles based on hybrid fuzzy discrete particle swarm optimization. Electr. Power
Syst. Res. 128, 19–29 (2015)
9. Yadav, R.D.S., Gupta, H.P.: Optimization studies of fuel loading pattern for a typical
pressurized water reactor (PWR) using particle swarm method. Ann. Nucl. Energ. 38,
2086–2095 (2011)
10. Shi, Y., Eberhart, R.: A modiﬁed particle swarm optimizer. In: IEEE International
Conference on Evolutionary Computation, pp. 69–73 (1998)
11. Shi, Y., Eberhart, R.C.: Parameter selection in particle swarm optimization. In: Proceedings
of the Seventh Annual Conference on Evolutionary Programming, New York, pp. 591–600
(1998)
12. Clerc, M.: The swarm and the queen: towards a deterministic and adaptive particle swarm
optimization. In: Proceedings of ICEC, Washington, D.C., pp. 1951–1957 (1999)
13. Trelea, I.C.: The particle swarm optimization algorithm: convergence analysis and parameter
selection. Inf. Process. Lett. 85, 317–325 (2003)
14. Shi, Y., Eberhart, R.C.: Empirical study of particle swarm optimization. In: Proceedings of
the Congress on Evolutionary Computation, vol. 3, pp. 1945–1950 (1999)
15. Han, Y., Tang, J., Kaku, I., Mu, L.: Solving uncapacitated multilevel lot-sizing problems
using a particle swarm optimization with ﬂexible inertial weight. Comput. Math Appl.
57, 1748–1755 (2009)
16. Zhang, L., Yu, H., Hu, S.: A new approach to improve particle swarm optimization. In:
Proceedings of the International Conference on Genetic and Evolutionary Computation,
pp. 134–139. Springer, Berlin (2003)
17. Niu, B., Zhu, Y.L., He, X., Wu, H.: A multi-swarm cooperative particle swarm optimizer.
Appl. Math. Comput. 185, 1050–1062 (2007)
18. Eberhart, R.C., Shi, Y.: Tracking and optimizing dynamic systems with particle swarms. In:
IEEE Congress on Evolutionary Computation, Seoul, Korea, pp. 94–97 (2001)
19. Arumugan, M.S., Rao, M.V.C.: On the performance of the particle swarm optimization
algorithm with various inertia weight variants for computing optimal control of a class of
hybrid systems. Discrete Dyn. Nat. Soc. 2006, 1–17 (2006)
20. Arumugan, M.S., Rao, M.V.C., Chandramohan, A.: A new and improved version of particle
swarm optimization algorithm with global-local best parameters. Knowl. Inf. Syst. 16, 331–
357 (2008)
21. Umapathy, P., Venkataseshaiah, C., Arumugam, M.S.: Particle swarm optimization with
various inertia weight variants for optimal power ﬂow solution. Discrete Dyn. Nat. Soc.
2010, 1–15 (2010)
22. Yang, X., Yuan, J., Yuan, J., Mao, H.: A modiﬁed particle swarm optimizer with dynamic
adaptation. Appl. Math. Comput. 189, 1205–1213 (2007)
23. Miao, A., Shi, X., Zhang, J., Wang, E., Peng, S.: A modiﬁed Particle Swarm Optimizer with
Dynamical Inertia Weight, pp. 767–776. Springer, Berlin (2009)
24. Chauhan, P., Deep, K., Pant, M.: Novel inertia weight strategies for particle swarm
optimization. Memetic Comput. 5, 229–251 (2013)
Dynamic Inertia Weight in Particle Swarm Optimization
87

25. Ememipour, J., Nejad, M.M.S., Rezanejad, M.M.J.: Introduce a new inertia weight for
particle swarm optimization. In: Proceedings of Fourth International Conference on
Computer Sciences and Convergence Information Technology, pp. 1650–1653 (2009)
26. Borowska, B.: Exponential inertia weight in particle swarm optimization. In: Advances in
Intelligent Systems and Computing, vol. 524, pp. 265–275. Springer International
Publishing, Heidelberg (2017)
27. Ghali, I., El-Dessouki, N., Mervat, A.N., Bakrawi, L.: Exponential particle swarm
optimization approach for improving data clustering. Int. J. Electr. Electron. Eng. 3–4,
208–212 (2009)
28. Shi, Y., Eberhart, R.C.: Fuzzy adaptive particle swarm optimization. In: Proceedings of the
Congress on Evolutionary Computation, vol. 1, pp. 101–106 (2001)
29. Tian, D., Li, N.: Fuzzy particle swarm optimization algorithm. In: International Joint
Conference on Artiﬁcial Intelligence, pp. 263–267 (2009)
30. Neshat, M.: FAIPSO: Fuzzy Adaptive Informed Particle Swarm Optimization. Neural
Comput. Appl. 23, 95–116 (2013)
31. Chen, T., Shen, Q., Su, P., Shang, C.: Fuzzy rule weight modiﬁcation with particle swarm
optimization. In: Soft Computing, pp. 1–15 (2015)
32. Mohiuddin, M.A., Khan, S.A., Engelbrecht, A.P.: Fuzzy particle swarm optimization
algorithms for the open shortest path ﬁrst weight setting problem. Appl. Intell. 1–24 (2016)
33. Chaturvedi, D.K., Kumar, S.: Solution to electric power dispatch problem using fuzzy
particle swarm optimization algorithm. J. Inst. Eng. India 96, 101–106 (2015)
88
B. Borowska

Mathematical Method of Translation into Ukrainian Sign
Language Based on Ontologies
Maksym Davydov
(✉) and Olga Lozynska
(✉)
Information Systems and Networks Department, Lviv Polytechnic National University,
Lviv, Ukraine
{Maksym.V.Davydov,Olha.V.Lozynska}@lpnu.ua
Abstract. This paper introduces the mathematical method for translation into
sign language based on ontologies. The modiﬁcation of aﬃx context-free
grammar (AGFL) that adds semantical attribute and a new form of production
called the “template production” is discussed. This new form helps to represent
ontology-based productions in a short and computationally inexpensive way. The
mathematical method utilizes dictionaries, ontology database, weighted aﬃx
context-free grammar (WACFG) parser, algorithm for transformation of constit‐
uency tree into dependency tree, and an algorithm for synthesis of Ukrainian sign
language glosses. The algorithm for selection and convertion of grammatically
augmented ontology (GAO) expressions into the set of WACFG productions is
suggested. The major increase in percentage of correctly parsed sentences was
achieved for Ukrainian sign language (UKL) and Ukrainian spoken language
(USpL). All algorithms are components of the translation system for Ukrainian
sign language. Simple video sequencing is utilized for sign language synthesis,
however any other sign animation tool can be used. Tasks that require further
research are deﬁned.
Keywords: Sentence parser · Ontology · Machine translation · Sign language
translation
1
Introduction
Development and study of methods for automatic translation into Ukrainian Sign
Language is an urgent task today. Due to results of a survey conducted by Lviv Poly‐
technic National University, more than 90% of people who communicate using sign
language would beneﬁt from devices for automatic translation into sign language and
more than 60% of them prefer using smartphone for performing this task. Nevertheless
there are many known scientiﬁc articles on this issue, the problem of translation into
Ukrainian Sign Language there are no viable solution for mobile devices yet.
Successful solution of Sign Language translation should be computationally eﬀec‐
tive, use limited storage, and require minimal bandwidth for communication with a
server.
In the proposed solution translation from Ukrainian spoken language into annotated
Ukrainian sign language and vice versa is divided into several steps: parsing sentences
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_7

using weighted aﬃx context free grammar, the transformation of constituency trees into
dependency trees using tree transformation algorithm, transformation of the dependency
trees into the sequence of UKL glosses using translation rules for sign language trans‐
lation, animation of glosses.
The use of ontologies and common sense databases play valuable role in parsing and
translation of such languages as Ukrainian spoken language that have ﬂexible word
order. Grammatically augmented ontology is an ontology extension that links phrases
to their meaning. The link is established via special expressions that connect phrase
meaning to grammatical and semantical attributes of words that constitute it. The paper
discusses an approach to sentence parsing that is based on integration of ontology rela‐
tions into productions of weighted aﬃx context-free grammar.
2
Related Work
A lot of linguistic problems such as machine translation, text recognition, information
retrieval and extraction require the automatic analysis of sentences.
There are two main approaches for machine translation: the rule-based approach and
the statistical approach. In the ﬁrst approach, human experts specify a set of rules to
describe the translation process. In another approach, large parallel corpora are used as
source of knowledge. Each of these approaches has its own advantages and challenges.
One of the steps of rule-based machine translation is the parsing input sentences.
The problem of sentence parsing has been already studied for a long time. There are
a lot of approaches for sentence parsing: syntactic sentence parsing based on generative
grammars [1], extended aﬃx grammar (EAG) [2], and stochastic context-free grammar
[3]; semantic sentence parsing based on predicate logic [4] and sub-domain driven
parsing [5]; syntactic sentence parsing based on semantic relations using statistics [6],
ontologies [7], tensor factorization [8] or mixed methods [9]; sentence parsing based on
ontologies.
The idea of ontology integration to the process sentence parsing is not new. In [10]
the generation of productions from ontologies for LTAG grammar parser was studied.
In the article by Faten Kharbat [11] the WordNet ontology [12] is utilized to be the
syntactic guide along with the Transition Network Grammar that helps to get better
translation from English to Arabic language. The approach based on rich ontologies was
used by Murat Temizsoy and Ilyas Cicekli [13] for Turkish language parsing. The
approach uses ontologies to improve text meaning representation model for parsed
sentences.
The work [14] describes a semi-automatic method for associating a Japanese lexicon
with a semantic concept taxonomy called an ontology, using a Japanese-English bilin‐
gual dictionary. They developed three algorithms to associate a Japanese lexicon with
the concepts of the ontology automatically: the equivalent-word match, the argument
match, and the example match. These algorithms were tested on a dataset of 980 nouns,
860 verbs and 520 adjectives and can be eﬀective for more than 80% of the words.
The new architecture for the translation (Italian – Italian Sign Language) that
performs syntactic analysis, semantic interpretation and generation are proposed in [15].
90
M. Davydov and O. Lozynska

They present some general issues of the ontological semantic interpretation that is based
on a syntactic analysis that is a dependency tree.
However, the problem of integrating hypernymy/hyponymy relations into the
process of sentence parsing was not previously studied. This article introduces a new
method that extends the system of productions using ontology relations. These relations
are expressions of GAO and hypernymy/hyponymy relations.
3
Main Part
3.1
Architecture and Components of the Developed System
The developed system consists of dictionaries, ontology database, weighted aﬃx
context-free grammar parser, algorithm for transformation of constituency tree into
dependency tree, and an algorithm for synthesis of Ukrainian sign language glosses.
There are two kinds of used dictionaries: Ukrainian morphology dictionary and
Ukrainian Sign Language dictionary.
There are two open-source dictionaries that are suitable for parsing Ukrainian
morphology: Spell-uk dictionary maintained by Andriy Rysin and a dictionary
supported by Mariana Romanyshyn and others from Grammarly. The ﬁrst dictionary is
widely known as it is used in OpenOﬃce spell checker. It is very memory eﬀective and
requires only 3 MB uncompressed and 500 KB when compressed. Unfortunately it has
no direct method to obtain word tags that are required for further parsing of sentences.
The second dictionary has more words and contains all necessary tags, but its size is
150 MB uncompressed and 17 MB in compressed state. Thus, the ﬁrst dictionary is
preferred for usage in mobile devices due to its compactness. In the developed system
both dictionaries were used: the Spell-uk dictionary was extended with special word
tagging rules, and words that were missing or were incorrectly tagged by spell-uk are
used from the second dictionary.
The result of tagging a word is a set of hypotheses for its base form and possible
grammatical attributes. Some mutually exclusive attributes can be included into the same
hypothesis in order to decrease possible search space. This attributes can be reﬁned later
in the syntax parser.
For example parsing of Ukrainian word “мaти” (mother) leads to the following
productions that where added to the set of WACFG productions:
noun[gf c1 n1] → <мaти>[gf c1 n1] → мaти[r],
noun[gm c1 c4 n*] → <мaт>[gm c1 c4 n*] → мaти[r],
verb[i n1 m-] → <мaти>[i n1 m-] → <мaти>[r],
where tags “gm” and “gf” mean male and female gender, “n1” and “n*” mean singular
and multiple number, “c1” and “c4” mean nominative and accusative cases respectively,
tag “i” means inﬁnitive, and tag “r” means word as it was written in the sentence (i.e.
terminal symbol of the grammar). All of these productions have the same structure:
part_of_speech[TAGS] → <base_form>[TAGS] → <word>[r].
Base form of the word can be used later when ontology hyponymy and hypernymy
relations are used.
Mathematical Method of Translation into Ukrainian Sign Language
91

The sentence parsing is done be means of WACFG parser that turned out to be very
eﬀective. Grammar productions are written in compact “template form” that assures low
memory usage and computationally eﬀective parsing. The system utilizes 230 produc‐
tions for parsing of Ukrainian language. Two examples of these rules with small descrip‐
tion are given below.
For example, production
NG[=] →adj[=]? AN(∗)[=] NG[c2]?
is used to describe a noun group that can be a noun with optional general adjective and
adjective of place. One of possible phrases that can be handled by this production is
“poзyмний cтyдeнт Львiвcькoї пoлiтexнiки” (smart student of Lviv Polytechnic
University). In the given production NG stands for Noun Group, AN stands for Anno‐
tated Noun, “c2” means genitive case, symbol “?” means optional symbol, “=” means
all standard attributes of this part of speech, and “*” denotes head word of the phrase
that is used later to obtain dependency tree.
Production
NG
[
C n ∗p3 gm gn gf
]
→NG(∗)[C]conj NG[C]
is used to describe pair of nouns, for example “милo i pyшник” (a soap and a towel).
Here “C” means that all words should have the same case and the result phrase should
have the same case, “p3” means the third person, “conj” means conjunction.
Besides regular productions that are used to describe the grammar of language being
parsed, grammatically augmented ontology productions are used. These productions
help to identify language constructions speciﬁc for particular subject area usage. For
example, in phrases “PLAY THE PIANO” and “PLAY FOOTBAL” the word PLAY
means completely diﬀerent actions that are translated into sign language diﬀerently.
Although such a colocation can be eﬀectively solved using statistical approach it can’t
be applied right now due to the lack of large parallel corpuses for sign language. Instead
of that the approach based on grammatically augmented ontologies is adopted. This
approach requires creation of ontology dictionaries for speciﬁc target areas that can be
eﬀectively achieved by means of the developed GAODL language described earlier
in [16].
The ontology is used to incorporate word abstraction rules into the grammar. For
example, productions that were generated for subject area education had the following
form:
<людинa> → <шкoляp> (human → pupil)
<мoжe-вчитиcь> → <людинa> (can-learn → human)
<нayкa> → <мaтeмaтикa> (science → math)
<мicтить-знaння> → <нayкa> (contains-knowledge → science)
<вчити-нaвчaти> → <вчити> (teach-proc → teach)
<вчити-нaвчaти> → <вчити-нaвчaти>(*) <мoжe-вчитиcь>[c4] (teach-smb →
teach-proc can-learn)
92
M. Davydov and O. Lozynska

<вчити-нaвчaти> → <вчити-нaвчaти> <мicтить-знaння>[c3] (teach-smth →
teach-proc contains-knowledge)
These rules can be eﬀectively used for semantical parsing of phrases like “вчити
шкoляpiв мaтeмaтики” (teach math to pupils). More information about use of gram‐
matically augmented ontologies for sentence parsing can be found in [17].
3.2
The Algorithm for Parsing Sentences
The problem of sentence parsing is formulated as a problem of ﬁnding a sequence of
productions that has the maximum weight and can be applied sequentially to some
starting attributed symbol (
S, As
) to produce a given sequence of terminals t1t2 … tn. The
weight of the sequence is calculated as a multiplication of weights of all contained
productions.
Fig. 1. The block scheme of sentence parsing algorithm.
Mathematical Method of Translation into Ukrainian Sign Language
93

The block scheme of parsing algorithm is shown on Fig. 1.
The algorithm provided above uses internal procedure for updating set of weighted
attributes symbols Q with set of possible left symbols L and set of possible right symbols
R by applying of rank 2. The block scheme of this procedure is depicted on Fig. 2.
Fig. 2. The block scheme of procedure for updating set of weighted attributes symbols Q.
If the right part of a production contains only one symbol, the weight of the produc‐
tion should not exceed 1 in order to avoid cyclic productions that increase weight of
non-terminal symbols during the bottom-up parsing procedure.
94
M. Davydov and O. Lozynska

3.3
Extending the Set of Productions with Ontology Relations
The grammar augmented ontology was introduced in [18]. Along with relations that are
common to ontology databases (hyponymy, hypernymy, meronymy, holonymy) GAO
contains relations that link synsets to expressions with associated grammatical attributes.
In order to beneﬁt from ontology knowledge new productions were added to gener‐
ative grammar. The addition of ontology productions into the generative grammar
extends the set of semantic attributes. Each synset of ontology was treated as semantic
attribute. For the purpose of eﬃciency semantic attributes and corresponding produc‐
tions were added only for hierarchies that contained words that were present in the
sentence being parsed.
The algorithm that adds new productions to syntactic parser is shown on Fig. 3. A
more detailed algorithm presented in [19].
Fig. 3. The algorithm that adds new productions to syntactic parser.
Mathematical Method of Translation into Ukrainian Sign Language
95

Each word w in the sentence can have several interpretations 𝜔1, 𝜔2, … , 𝜔r ∈𝛺,
where 𝛺 is a dictionary of all interpretations. Each interpretation 𝜔 uniquely deﬁnes its
semantic attribute SemAttr(𝜔), lexical category LexCat(𝜔) and the set of grammatical
attributes GrAttr(𝜔). A tuple e = ⟨(𝜎1, A𝜎1
)(𝜎2, A𝜎2
) … (𝜎∗, A∗
𝜎
) … (𝜎K, A𝜎K
)⟩ – each
expression in GAO, where 𝜎i is a synset that narrows the set of words that can appear
in the given position of the expression e and A𝜎i is a set of grammar attributes the word
is 
required 
to 
possess; 𝜎∗ 
is 
a 
head 
word 
of 
the 
expression. 
Let
𝛬= {noun, verb, adjective, adverb, …} be a set of all lexical categories and D𝛬:𝛬→2D
be a mapping from lexical category to the set of its attribute domains.
Productions that are generated from ontology expression have larger weight than
simple syntactic productions in order to dominate over them. Additional weight 𝛥w in
expression is devised from the admissibility of the expression in the given context or
text topic.
The result of paring the sentences “My father bought several candies at the table”
and “My father bought several candies at the shop” using ontologies is depicted on Fig. 4.
Fig. 4. The result of parsing sentences “My father bought several candies at the table” and “My
father bought several candies at the shop”. The weight of the ﬁrst parse is higher because one
ontology expression was used while the second parsing tree is based only on syntactic productions.
The next step after parsing a sentence is to convert constituency tree to dependency
tree. The algorithm of conversion takes constituency tree as input. The head of each
phrase is used to determine main word of the sentence and main word of each sub-phrase.
The core of the algorithm is to identify the head of each phrase in the constituency tree
and establish its relation with the head of its parent node.
96
M. Davydov and O. Lozynska

The transformation algorithm comprises of the following steps (Fig. 5):
1. Find word that is a head of the entire tree and mark all vertices in the tree that have
the same head.
2. Join vertices obtained in step 1 into a single vertex of the dependency tree.
3. Apply steps 1–2 recursively for every sub-tree of marked vertices.
4. Verify dependencies in the obtained dependency tree and mark edges where words
could not be in parent-child relationship as improper.
5. For all improper relations ﬁnd better correspondence by width-ﬁrst search in the
dependency tree for other possible parent word.
Fig. 5. Algorithm for transformation of constituency tree to dependency tree.
Mathematical Method of Translation into Ukrainian Sign Language
97

4
Results
The result of the transformation step is a dependency tree that can be used to produce
translation. An example of a tree for sentence “My father bought several candies at the
shop” is shown in Fig. 6.
Fig. 6. Dependency tree obtained for sentence “My father bought several candies at the shop”.
Fig. 7. Mobile app interface for entering text and translating into sign language.
98
M. Davydov and O. Lozynska

The translation is produced using transformation rules described in [20].
The information system of translation takes an input text of Ukrainian spoken
language. The input can be obtained by typing word with a screen keyboard or by means
of speech recognition (Fig. 7).
After entering the text, it is split into sentences and their topics are determined.
Productions from grammatically augmented ontology are added when possible and
sentences are parsed and translated. The translation is performed involving transfer rules
base, which consists of rules for sign language translation and reordering rules that are
used to generate the text in UKL.
Evaluation of translation results was performed by comparing sentences with trans‐
lations available in the database of test sentences. The database of WACFG productions
and the database of grammatically augmented ontology were updated by adding new
rules and new synsets respectively.
5
Conclusion
The mathematical method for translation into sign language based on ontologies are
described in the paper.
The developed information system consists of dictionaries, ontology database,
weighted aﬃx context-free grammar parser, algorithm for transformation of constitu‐
ency tree into dependency tree, and an algorithm for synthesis of Ukrainian sign
language glosses. The use of WACFG parser for sentence parsing has allowed to increase
the percentage of correctly translated sentences. The obtained sentence parsing trees are
more semantically rich than the parsing trees obtained by means of regular syntactic
parser. The system utilizes 230 productions for parsing of Ukrainian language. The
transformation algorithm from constituency tree into dependency tree has shown high
eﬃciency (89% correct sentences converted) and the possibility of its use in machine
translation systems.
Further research can be focused on improving the quality of the information system
for translation, adding new rules into WACFG parser, extending the set of synsets in
grammatically augmented ontology and devising new rules for the transformation algo‐
rithm.
References
1. Chomsky, N.: Three models for the description of language. IRE Trans. Inf. Theor. 2(3), 113–
124 (1956)
2. Oostdijk, N.: An extended aﬃx grammar for the english noun phrase. In: Aarts, J., Meijs, W.
(eds.) Corpus Linguistics. Recent Developments in the Use of Computer Corpora in English
Language Research, pp. 95–122. Rodopi, Amsterdam (1984)
3. Eddy, S.R., Durbin, R.: RNA sequence analysis using covariance models. Nucleic Acids Res.
22(11), 2079–2088 (1994)
4. Blackburn, P., Bos, J.: Representation and Inference for Natural Language: A First Course
in Computational Semantics. CSLI Publications, Stanford (2005)
Mathematical Method of Translation into Ukrainian Sign Language
99

5. Plank, B., Sima’an, K.: Subdomain sensitive statistical parsing using raw corpora. In:
Proceedings Sixth International Conference on Language Resources and Evaluation, pp. 465–
469. European Language Resources Association, Marrakech (2008)
6. Jiang, J.J., Conrath, D.W.: Semantic similarity based on corpus statistics and lexical
taxonomy. In: Proceedings of the International Conference on Research in Computational
Linguistics, Taiwan, pp. 19–33 (1997)
7. Rhee, S.K., Lee, J., Park, M.-W.: Ontology-based semantic relevance measure. In:
Proceedings of the First International Workshop on Semantic Web and Web 2.0 in
Architectural, Product and Engineering Design, Busan, Korea, pp. 63–68 (2007)
8. Anisimov, A., Marchenko, O., Vozniuk, T.: Determining semantic valences of ontology
concepts by means of nonnegative factorization of tensors of large text corpora. Cybern. Syst.
Anal. 50(3), 327–337 (2014)
9. Nagarajan, M., Sheth, A.P., Aguilera, M., Keeton, K., Merchant, A., Uysal, M.: Altering
document term vectors for classiﬁcation - ontologies as expectations of co-occurrence. In:
Proceedings of the 16th International Conference on World Wide Web, Banﬀ, Alberta,
Canada, pp. 1225–1226 (2007)
10. Unger, C., Hieber, F., Cimiano, P.: Generating LTAG grammars from a lexicon-ontology
interface. In: Proceedings of the 10th International Workshop on Tree Adjoining Grammars
and Related Formalisms (TAG+10), Yale University, pp. 61–68 (2010)
11. Kharbat, F.: A new architecure for translation engine using ontology: one step ahead. In:
Proceedings of The International Arab Conference on Information Technology (ACIT 2011),
Saudi Arabia, pp. 169–173 (2011)
12. Miller, G.A.: WordNet: a lexical database for English. Commun. ACM 38(11), 39–41 (1995)
13. Temizsoy, M., Cicekli, I.: An ontology based approach to parsing Turkish sentences. In:
Proceedings of AMTA 1998-Conference, pp. 124–135. Springer, Langhorne (1998)
14. Okumura, A., Hovy, E.H.: Building Japanese-English dictionary based on ontology for
machine translation. In: HLT 1994 Proceedings of the Workshop on Human Language
Technology, Plainsboro, pp. 141–146 (1994)
15. Lesmo, L., Mazzei, R., Radicioni, D.P.: An ontology based architecture for translation. In:
Proceedings of the Ninth International Conference on Computational Semantics, Oxford, UK,
pp. 345–349 (2011)
16. Lozynska, O.V., Davydov, M.V.: Domain-speciﬁc language for describing grammatically
augmented ontology. Control Syst. Mach. 4, 31–40 (2015)
17. Lozynska, O., Davydov, M.: Information technology for Ukrainian Sign Language translation
based on ontologies. Econtechmod Int. Q. J. 4(2), 13–18 (2015)
18. Davydov, M., Lozynska, O.: Spoken and sign language processing using grammatically
augmented ontology. Appl. Comput. Sci. 11(2), 29–42 (2015)
19. Davydov, M., Lozynska, O., Pasichnyk, V.: Partial semantic parsing of sentences by means
of grammatically augmented ontology and weighted aﬃx context-free grammar.
Econtechmod Int. Q. J. 6(2), 27–32 (2017)
20. Lozynska, O.V., Davydov, M.V., Pasichnyk, V.V.: Rule-based machine translation into
Ukrainian Sign Language. Inf. Technol. Comput. Eng. Sci. J. VNTU 1(29), 11–17 (2014)
100
M. Davydov and O. Lozynska

Method of Parametric Identiﬁcation
for Interval Discrete Dynamic Models
and the Computational Scheme of Its
Implementation
Mykola Dyvak1, Natalia Porplytsya1, Yurii Maslyak1(&),
and Mykola Shynkaryk2
1 Department of Computer Science,
Ternopil National Economic University, Ternopil, Ukraine
mdy@tneu.edu.ua, ocheretnyuk.n@gmail.com,
yuramasua@gmail.com
2 Department of Economical and Mathematical Methods,
Ternopil National Economic University, Ternopil, Ukraine
shmi@tneu.edu.ua
Abstract. A method of parametric identiﬁcation of interval discrete dynamic
models is considered. In the case of an interval data set, ﬁnding estimations for
parameters of such models requires solving an interval system of nonlinear
algebraic equations for some known vector of basic functions. The solution of
these equations forms a non-convex area in the parameter space which can
consist of several unconnected subareas. For solving this parametric identiﬁ-
cation problem, methods of random search are widely used including that based
on the procedure of the Rastrigin’s director cone having high time complexity.
Therefore, the detailed analysis of this parametric identiﬁcation method was
carried out in this work to reduce the time complexity. A new improved scheme
of computational implementation of the method is proposed which takes into
account areas of permissible values of the modeled characteristic. Results of the
comparative efﬁciency analysis of implementation scheme of the proposed
method and the known one are presented demonstrating that the time complexity
of the improved scheme of the method is at least twice less compared to the
known implementation scheme.
Keywords: Parametric identiﬁcation  Random search procedures
Interval data analysis  Discrete dynamic model
1
Introduction
There are many real-world tasks solution of which requires building mathematical
models in the form of discrete dynamic models (DDM). In particular, these are the
prediction task of the humidity distribution on the surface of the drywall sheet during
its manufacturing, the prediction task of the distribution of information signal maximal
amplitude of the surgical wound surface during thyroid surgery and prediction task of
dispersion of harmful vehicle emissions in the surface layer of the atmosphere [1–3].
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_8

When solving such kind of problems, the task of parametric identiﬁcation of the
DDM arises being, as it is well known, an NP-complexity task [4, 5]. Solving it
requires using an exhaustive search of all possible solutions until fulﬁlling some
speciﬁc criteria [6]. It is known that in case when a data set is of interval form, this
means ﬁnding estimations for DDM parameters by solving an interval system of
nonlinear algebraic equations (ISNAE) for some known vector of basic functions [7].
Note, that the solution of ISNAE is an area in the parameter space that is not
convex and can consist of several unconnected subareas. When solving the task of
estimation the solutions area of ISNAE, in case when structure of the model is pre-
sented in form of DDM, some difﬁculties arise, in particular, related to the recurrent
scheme of the calculations [8, 9]. That is why for solving the problem of parametric
identiﬁcation, methods based on random search procedures are widely used, for
example, based on the Rastrigin’s director cone [10, 11]. Using this method assumes,
instead of searching the whole area of ISNAE solutions, searching at least one of its
solutions, that is only one point from this area [12].
However, the practice shows that even if this method gives an opportunity to build
an adequate model in the form of DDM, it has still high time complexity. The solution
time can be very varied even in one series of experiments due to the «random char-
acter» of the used procedures. Therefore, the detailed analysis of this parametric
identiﬁcation method was carried out in this work to reduce the time complexity of its
using.
2
Statement of the Problem
Let us consider the problem of parametric identiﬁcation of the DDM in such general
form [2]:
v
_
i;j;h;k
h
i
¼ v
_
i;j;h;k; v
_ þ
i;j;h;k
h
i
¼~f T
v
_
0;0;0;0
h
i
; . . .; v
_
0;0;h1;0
h
i
; . . .;

v
_
i1;j1;h1;k1
h
i
;~ui;j;h;0; . . .;~ui;j;h;kÞ ~g
_
;
i ¼ 1; . . .; I; j ¼ 1; . . .; J; h ¼ 1; . . .; H; k ¼ 1; . . .; K;
ð1Þ
where ~f T 
ð Þ is vector of known basis functions deﬁning the DDM structure; vi;j;h;k is
the modeled characteristic at the point with discrete spatial coordinates i, j, h in discrete
time k; ~ui;j;h;0; . . .;~ui;j;h;k are vectors of input variables; ~g is a vector of unknown
parameters of DDM. Below, the model (1) will be called as interval discrete dynamic
model (IDDM).
Based on the requirements of ensuring accuracy of the model within the interval of
the experiment accuracy, the identiﬁcation of IDDM (1) will be realized using such
criterion [12]:
v
_
i;j;h;k; v
_ þ
i;j;h;k
h
i
 z
i;j;h;k; z þ
i;j;h;k
h
i
8i ¼ 1; . . .; I; 8j ¼ 1; . . .; J; 8h ¼ 1; . . .; H; 8k ¼ 1; . . .; K
ð2Þ
102
M. Dyvak et al.

where
v
_
i;j;h;k; v
_ þ
i;j;h;k
h
i
is an interval estimation of the modeled characteristic;
z
i;j;h;k; z þ
i;j;h;k
h
i
is an interval of possible measured values of the characteristic at the
point with discrete coordinates i, j, h in the time moment k.
By substituting recurrent expression (1) into (2), instead of the interval estimates
v
_
i;j;h;k; v
_ þ
i;j;h;k
h
i
together with the given initial interval values of the set elements.
v
_
0;0;0;0
h
i
 z0;0;0;0


; . . .; v
_
0;0;h1;0
h
i
 z0;0;h1;0


; . . .;
v
_
i1;j1;h1;k1
h
i
 zi1;j1;h1;k1


;
ð3Þ
and given vectors of input variables ~ui;j;h;0; . . .;~ui;j;h;k, we obtain the ISNAE:
v
_
0;0;0;0; v
_ þ
0;0;0;0
h
i
 z
0;0;0;0; z þ
0;0;0;0
h
i
;
...
v
_
i2;j2;h2;k2; v
_ þ
i2;j2;h2;k2
h
i
 z
i2;j2;h2;k2; z þ
i2;j2;h2;k2
h
i
;
v
_
i1;j1;h1;k1
h
i
¼~f T
v
_
0;0;0;0
h
i
; . . .; v
_
i2;j2;h2;k2
h
i
;~u0; . . .;~uk1


~g
_
;
z
i;j;h;k ~f T
v
_
0;0;0;0;
h
i
; . . .; v
_
i1;j1;h1;k1
h
i
;~ui;j;0; . . .;~ui;j;k


~g
_
 z þ
i;j;h;k;
z
i þ 1;j;h;k ~f T f T
v
_
0;0;0;0;
h
i
; . . .; v
_
i1;j1;h1;k1
h
i
;~ui;j;h;0; . . .;~ui;j;h;k


~g
_


 z þ
i þ 1;j;h;k;
i ¼ 2; . . .I; j ¼ 2; . . .; J; h ¼ 2; . . .; H; k ¼ 2; . . .; K:
8
>
>
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
>
>
:
ð4Þ
Then, the ISNAE (4) can be transformed into an optimization problem of searching
the minimum of such objective function:
ð5Þ
Figure 1 illustrates a fragment of solutions area of IDDM parametric identiﬁcation
problem. In the space of parameters, each point from deﬁnition domain of objective
function has coordinates
g
_
1; g
_
2


that are known values of IDDM parameter vector,
and d ~g
_
 
is a value of objective function calculated based on the IDDM (1), that
determines the quality of current IDDM parameter vector.
As it is shown in the ﬁgure, the objective function contains large number of local
and global extremums with d ~g
_
 
¼ 0 which increases the probability of ﬁnding
problem solution. But such large number of local extremum points substantially
complicates the problem of IDDM parametric identiﬁcation [2, 4].
Note that the points in the parameter space for which the condition d ~g
_
 
¼ 0 is
fulﬁlled provide the possibility to build an adequate model, namely such model that
satisﬁes condition (2).
Method of Parametric Identiﬁcation
103

Besides that, the objective function in the parameter space can be “torn”, namely it
can contain few unconnected subareas that conﬁrms the complexity of problem of
IDDM parameters identiﬁcation [12].
To calculate values of the objective function d ~g
_
 
, the following expressions are
used:
d ~g
_
 
¼
max
i¼1;...;I;j¼1;...;J;h¼1;...;H;k¼1;...;Kf mid ~f Ts
v
_
0;0;0;0
h
i
; . . .;



v
_
i1;j1;h1;k1
h
i
;~ui;j;h;0; . . .;~ui;j;h;k

~g
_s
 mid
zi;j;h;k



	
if v
_
i;j;h;k
h
i
\ zi;j;h;k


¼ ;; 9i ¼ 1; . . .; I; 8j ¼ 1; . . .; J; 9h ¼ 1; . . .; H; 9k ¼ 1; . . .; K;
ð6Þ
d ~g
_
 
¼
max
i¼1;...;I;j¼1;...;J;h¼1;...;H;k¼1;...;K wid ~f Ts
v
_
0;0;0;0
h
i
; . . .;


n
v
_
1;j1;0;0
h
i
; . . .; v
_
i1;j1;h1;k1
h
i
;~ui;j;h;0; . . .;~ui;j;h;k

~g
_s
 wid
~f Ts
v
_
0;0;0;0
h
i
; . . .; v
_
0;0;h1;0
h
i
; v
_
i1;0;0;0
h
i
; . . .;



v
_
i1;j1;h1;k1
h
i
;~ui;j;h;0; . . .;~ui;j;h;kÞ ~g
_s
\ zi;j;h;k

o
if ½v
_
i;j;h;k \ ½zi;j;h;k 6¼ ;; 8i ¼ 1; . . .; I; 8j ¼ 1; . . .; J; 8h ¼ 1; . . .; H; 8k ¼ 1; . . .; K:
ð7Þ
The functions mid 
ð Þ; wid 
ð Þ are the operations of determining the centers and the
widths of the intervals, respectively.
Expression (6) describes the approximation of the current vector ~g
_
to satisfactory
one as the difference between the most remote centers of predicted and experimental
intervals in the case when they do not intersect, and expression (7) is the smallest width
Fig. 1. Fragment of the two-dimensional space of solutions of the parametric identiﬁcation
problem.
104
M. Dyvak et al.

of the intersection between predicted and experimental intervals in the case of their
intersection at all discrete points.
Expressions (6) and (7) describe the quantiﬁed approximation of current parameter
vector to satisfactory one which provides the possibility to build an adequate mathe-
matical model in terms of fulﬁlling the condition (2).
3
Method of Parametric Identiﬁcation of IDDM Based
on a Random Search Procedure Using a Director Cone
Taking into account the features of IDDM parametric identiﬁcation problem, it is
expedient to use random search methods for its solving. In [12], the method of IDDM
parametric identiﬁcation is built based on procedure of random search of minimum of
the objective function d ~g
_
 
using Rastrigin’s director cone [10, 11]. Let us consider it
in more details.
At the initial iteration of the method (l = 0), the initial approximation of IDDM
parameter vector ~g
_
0 is randomly set. Then, P random points are generated in the
neighborhood of this approximation on the surface of imaginary hypersphere with
radius r, scilicet, within distance r from point ~g
_
0 in the parameter space based on the
uniform distribution law:
~g
_
p ¼ ~g
_
0 þ r ~np; p ¼ 1; . . .; P:
ð8Þ
Among all generated points, a point is selected that provides the minimal value of
the objective function:
~g
_
1 ¼ arg min
p¼1;...;P
d ~g
_
0 þ r ~np




:
ð9Þ
This obtained estimation of IDDM parameter vector is an approximation for the
next iteration. Additionally, a memory vector is calculated that determines successful
direction of search, in this procedure:
~w ¼ ~g
_
1 ~g
_
0


=r:
ð10Þ
At the next iteration, an imaginary hypercone is built in the parameter space with
top ~g
_
l which is current estimation of IDDM parameter vector with opening angle w and
axis ~wl. This hypercone “cuts” some surface from hypersphere with center in the point
~g
_
l and radius r. Again, P points are randomly generated on the obtained surface in the
parameter space based on uniform distribution law using expression (8). Note, in
expression (8) vector~np is calculated based on cone parameters limitations. Then again,
Method of Parametric Identiﬁcation
105

the point is selected that provides the minimum value of the objective function among
all generated points:
~g
_
l þ 1 ¼ arg min
p¼1;...;P
ðdð~g
_
l þ r ~npÞÞ:
ð11Þ
The obtained estimation of the IDDM parameter vector is an approximation for the
next iteration l + 1 of the search procedure. Moreover, an additional memory vector is
determined in this procedure:
~wl þ 1 ¼ a  ~wl þ b ~g
_
l þ 1 ~g
_
l
r
;
ð12Þ
where a, 0  a  1, is a forgetting coefﬁcient, b is a coefﬁcient of intensity of taking
into account of new information.
The search continues until the objective function decreases. If the value of the
function does not decrease at a particular iteration, we need to use a hypersphere
instead of using a cone as it is in the initial iteration for a given vector of parameter
estimations. If it is impossible to ﬁnd a point among all generated ones which would
decrease the objective function, one should usually reduce the radius r.
4
Scheme of Computational Implementation of the Method
of Parametric Identiﬁcation of IDDM
For implementation of the parametric identiﬁcation method described above, the fol-
lowing scheme of its computational implementation is proposed based on the dividing
of experimental sample described in [12]. Namely, under condition of large quantity of
interval data in the whole set, a procedure of dividing the entire set of interval data into
parts is used with further operating on the set parts. However, conditions of providing
of the given accuracy (2) for model in IDDM form should be fulﬁlled for the entire set
of the interval data.
Using the principle of data sample division being traditional for GMDH, the
interval data set is divided into two parts: main and testing.
If the quantity of interval data points in the main set is N0 ¼ I0 	 J0 	 H0 	 K0,
then the quantity of points in the testing set remains:
NP ¼ I 	 J 	 H 	 K  I0 	 J0 	 H0 	 K0:
ð13Þ
Under these conditions, the problem of IDDM parametric identiﬁcation is solved in
two stages. At the ﬁrst stage, we solve the problem (14) for the main part of the interval
data set:
ð14Þ
106
M. Dyvak et al.

where objective function dð~g
_
lÞ now is determined by expressions (6) and (7) at the set
of discrete values: i ¼ 1; . . .; I0; j ¼ 1; . . .; J0; h ¼ 1; . . .; H0; k ¼ 1; . . .; K0.
When such estimation ~g
_
l¼L 2 X0 (which provides dð~g
_
l¼LÞ ¼ 0) will be found at a
current iteration of the method for main part of interval data set, then we move to the
second stage of searching of IDDM parameters. This stage implies analysis of the
obtained current estimation of parameters for the testing part of the interval data set.
If the condition of ensuring of IDDM given accuracy for testing part of interval data
set for ~g
_
l¼L 2 X0 is fulﬁlled, then the found estimation of IDDM parameter vector
provides the possibility to build an adequate model. Otherwise, it is necessary to
continue searching for other optimal estimation of the IDDM parameters in the main
sample, scilicet, move to the ﬁrst stage of solving the IDDM parametric identiﬁcation
problem.
Results of analysis of the presented scheme of computational implementation for
the method of IDDM parametric identiﬁcation has shown that for all points generated at
a current iteration in the parameter space it is necessary to calculate the value of the
objective function. It is well known that this procedure is the most complex in the
method of IDDM parametric identiﬁcation [12]. In addition, as it was noted earlier, to
calculate the value of the objective function dð~g
_
Þ using expressions (6) and (7), ﬁrstly it
is necessary to predict the modeled characteristic values using the IDDM (1).
Therefore, we propose in this paper to consider an area of permissible values of the
modeled characteristic when implementing the parametric identiﬁcation method. This
will allow reducing the number of calculations of the objective function values dð~g
_
Þ
which, in turn, will reduce the time complexity of implementation of the IDDM
parametric identiﬁcation method.
To do that, before using the method, researcher should specify an area of per-
missible values of the modeled characteristic as follows:
ð15Þ
where ~vmin
i;j;h;k;~vmax
i;j;h;k are vectors of minimal and maximal permissible values of the
modeled characteristic, respectively; d is the IDDM order.
Note that the value of the interval (15) should be set by researcher empirically
based on the analysis of physical characteristics of the modeled process. Then the
following step of checking condition should be added to the computational scheme of
implementation of parametric identiﬁcation method:
½v
_
i;j;h;k  ½vmin
i;j;h;k; vmax
i;j;h;k;
8i ¼ d; . . .; I; 8j ¼ d; . . .; J; 8h ¼ d; . . .; H; 8k ¼ d; . . .; K:
ð16Þ
If condition (16) is fulﬁlled, then we calculate the objective function dð~g
_
Þ value for
the current estimation of the IDDM parameter vector, otherwise this point of the space
Method of Parametric Identiﬁcation
107

of solutions will not be taken into account in further computations and the objective
function dð~g
_
Þ will not be calculated for it.
Thus, the following scheme will be used for each generated point in the parameter
space ~g
_
p (p = 1,…, P). The below system (17) is formed that includes initial condi-
tions, an interval equation for the ﬁrst discrete step and limitation for permissible area
of the modeled characteristic in this discrete step:
½v
_
0;0;0;0; v
_ þ
0;0;0;0½z
0;0;0;0; z þ
0;0;0;0
..
.
½v
_
d;d;d;k; v
_ þ
d;d;d;k½z
d;d;d;k; z þ
d;d;d;k
½v
_
d þ 1;d;d;k ¼~f Tð½v
_
0;0;0;0; . . .; ½v
_
d;d;d;k;~u0; . . .;~ukÞ ~g
_
vmin
d þ 1;d;d;k ~f Tð½v
_
0;0;0;0;; . . .; ½v
_
d;d;d;k;~ui;j;0; . . .;~ui;j;kÞ ~g
_
 vmax
d þ 1;d;d;k
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
ð17Þ
Then the system (18) is formed which includes initial conditions, the result of
solving of the previous system (17), the interval equation for the next discrete step and
limitation for permissible area of the modeled characteristic in this step:
½v
_
0;0;0;0; v
_ þ
0;0;0;0½z
0;0;0;0; z þ
0;0;0;0
..
.
½v
_
d;d;d;k; v
_ þ
d;d;d;k½z
d;d;d;k; z þ
d;d;d;k
½v
_
d þ 1;d;d;k; v
_ þ
d þ 1;d;d;k½vmin
d þ 1;d;d;k; vmax
d þ 1;d;d;k
½v
_d þ 1; d þ 1; d; k ¼~f Tð½v
_
0;0;0;0; . . .; ½v
_
d þ 1;d;d;k;~u0; . . .;~ukÞ ~g
_
vmin
d þ 1;d þ 1;d;k ~f Tð½v
_
0;0;0;0; . . .; ½v
_
d þ 1;d;d;k;~ui;j;0; . . .;~ui;j;kÞ ~g
_
 vmax
d þ 1;d þ 1;d;k
8
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
:
ð18Þ
Generating
such
ISNAEs
is
continued
for
8i ¼ d; . . .; I,
8j ¼ d; . . .; J,
8h ¼ d; . . .; H, 8k ¼ d; . . .; K under condition that each previously generated system is
consistent. In such case, the value of the objective function dð~g
_
Þ is calculated for
current parameter vector. After that, one should repeat the procedure for the next
parameter vector ~g
_
p, p = 1, …, P.
In the case if any of generated systems is inconsistent, the process of generating of
such ISNAEs is stopped and the value of objective function is not calculated for the
current parameter vector.
Overall, consistency of all generated systems ensures fulﬁlling condition (16) on a
set of all discrete steps. In turn, the implementation of condition (16) means that the
current vector of parameters ~g
_
provides possibility to build such mathematical model
for which the predicted values of the modeled characteristic be matched with physical
features of this characteristic.
Overall, taking into account permissible values area when solving problem of
IDDM parametric identiﬁcation at all iterations of the method implementation will
provide reducing the time complexity of its using. Figure 2 shows the ﬂowchart of
108
M. Dyvak et al.

computational implementation scheme of parametric identiﬁcation of the IDDM
method with improved procedure for calculating values of the objective function dð~g
_
Þ.
5
Experimental Research
To conduct the experiments, a software system was developed for IDDM parametric
identiﬁcation. For development of the software system, the object-oriented approach,
C# programming language and .NET technology were used. For comparison of efﬁ-
ciency of the known and improved computational implementation scheme of the
Fig. 2. Flowchart of implementation of the improved computational scheme of the IDDM
parametric identiﬁcation method
Method of Parametric Identiﬁcation
109

parametric identiﬁcation method, ﬁve computational experiments were carried out. The
experiments were conducted on the example of solving the prediction task of the
humidity distribution on the surface of the drywall sheet during its manufacturing [3].
The values of relative humidity on the drywall sheet surface for ensuring manufac-
turing of high quality product must be in the range from 0.6% to 0.9%.
During each experiment, 40 researches were conducted, 10 of which using known
implementation scheme of the IDDM parametric identiﬁcation method and another 30
using the described improved scheme with different intervals of permissible values of
humidity on the surface of the drywall sheet.
When conducting researches using the improved scheme of the method imple-
mentation, the following variants of permissible values of the relative humidity were
used: ½vmin
k
¼ 0:001; vmax
k
¼ 1, ½vmin
k
¼ 0:001; vmax
k
¼ 2:5, ½vmin
k
¼ 0:001; vmax
k
¼ 5.
Note that based on physical characteristics of the modeled process, the permissible
values of the relative humidity were set constant at all grid nodes.
The following structure of the IDDM was used during the research:
½v
_
i;j;k; v
_ þ
i;j;k ¼ g1 þ g2  ðu1;0  u2;k=u2;0  u1;kÞ  ½v
_
i1;j1;k; v
_ þ
i1;j1;k
þ g3  ½v
_
i1;j2;k; v
_ þ
i1;j2;k þ g4  ½v
_
i;j1;k; v
_ þ
i;j1;k
þ g5  ðu1;0  u2;k=u2;0  u1;kÞ  ½v
_
i1;j;k; v
_ þ
i1;j;k;
ð19Þ
where v
_
i;j;k is the humidity level at the point with coordinates i, j on the surface of the
drywall sheet in the time moment k; u1;0, u1;k are temperatures in the drying oven at the
moment k; u2;0, u2;k are predicted moving speeds of drywall sheet in a drying oven for
the moment k.
The following initial values of the temperature and moving speed in a drying oven
were used for calculation to predict the humidity level: u1;0 = 120 ˚C, u2;0 = 0.25
m/min, u1;k = 125 ˚C, u2;k = 0.28 m/min.
The described software system was used for conducting of the experiments.
Figure 3 illustrates a screenshot of the software window with results of a research.
Fig. 3. Window of the software system with results of one conducted research
110
M. Dyvak et al.

Figure 4 demonstrates comparison of efﬁciency of the known implementation
scheme of the parametric identiﬁcation method and improved one with different
intervals of permissible values of humidity on the drywall sheet surface. The highest
values of time complexity indicator (in minutes) among ten researches within each
experiment are shown in the diagram.
6
Conclusion
The method of parametric identiﬁcation of interval discrete dynamic models is con-
sidered in the paper. An improved scheme of its computational implementation is
proposed. The improved scheme of the parametric identiﬁcation method takes into
account an area of permissible values for the modeled characteristic.
During the research, it was proved that the time complexity of the improved
computational implementation scheme of the IDDM parametric identiﬁcation method
is at least twice less compared to the known implementation scheme.
References
1. Ocheretnyuk, N., Voytyuk, I., Dyvak, M., Martsenyuk, Y.: Features of structure
identiﬁcation the macromodels for nonstationary ﬁelds of air pollutions from vehicles. In:
Proceedings of XIth International Conference on Modern Problems of Radio Engineering,
Telecommunications and Computer Science (TCSET 2012), p. 444. Lviv-Slavske (2012)
2. Porplytsya, N., Dyvak, M.: Interval difference operator for the task of identiﬁcation recurrent
laryngeal nerve. In: Proceedings of the 16th International Conference on Computational
Problems of Electrical Engineering (CPEE 2015), pp. 156–158 (2015)
Fig. 4. Comparison of time complexity
Method of Parametric Identiﬁcation
111

3. Porplytsya, N., Dyvak, M., Dyvak, T., Voytyuk, I.: Structure identiﬁcation of interval
difference operator for control the production process of drywall. In: Proceedings of 12th
International Conference on the Experience of Designing and Application of CAD Systems
in Microelectronics, CADSM 2013, pp. 262–264 (2013)
4. Fliess, M., Sira-Ramirez, H.: Closed-loop parametric identiﬁcation for continuous-time
linear systems via new algebraic techniques. In: Garnier, H., Wang, L. (eds.) Identiﬁcation of
Continuous-time Models from sampled Data, pp. 362–391. Springer (2008)
5. Graupe, D.: Identiﬁcation of systems. Technol. Eng. (1976). Journal no. 12205
6. Sean, L.: Essentials of Metaheuristics, 2nd edn. Lulu, Raleigh (2013)
7. Bowden, R.: The theory of parametric identiﬁcation. Econometrica 41, 1069–1074 (1973)
8. Shary, S.P.: Algebraic approach to the interval linear static identiﬁcation, tolerance, and
control problems, or one more application of Kaucher arithmetic. Reliable Comput. 2(1), 3–
33 (1996)
9. Walter, E., Pronzato, L.: Identiﬁcation of Parametric Model From Experimental Data.
Springer, Heidelberg (1997)
10. Rastrigin, L.A.: A Random Search. Znanie, Moscow (1979). (in Russian)
11. Rastrigin, L.A.: Adaptation of Complex Systems. Zinatne, Riga (1981). (in Russian)
12. Dyvak, T.: Method of parametric identiﬁcation of macro model in the form of interval
difference operator with dividing of data sample. Inductive Model. Complex Syst. 3, 49–60
(2011). (in Ukrainian)
112
M. Dyvak et al.

Mechanisms to Enhance the Efﬁciency
of Maritime Container Trafﬁc Through
“Odessa” and “Chernomorsk” Ports
in the Balancing of Portfolios
Anatoly J. Gaida, Elena A. Zarichuk, and Konstantin V. Koshkin(&)
National Shipbuilding University, Geroiv Ukrainy Ave., 9, Mykolaiv, Ukraine
{cetus,kkoshkin}@ukr.net, milena012@mail.ru
Abstract. Approaches to development classiﬁcation mechanisms in condition
of projects of marine container transportations by means of artiﬁcial neural
networks are investigated. It is established that growth rates of transportation
volumes not linearly depend on a ratio of volumes of import and export cargoes
with obviously expressed maximum. Its offered mechanism of efﬁciency
increase of the companies by a choice of a rational ratio volumes of import and
export cargoes on the basis of a neural network assessment of the previous
activity results of the company and its competitors which, unlike known
mechanisms, provides possibility of an effective assessment of the situation
which developed in the market of freight transportation.
Keywords: Management of projects  Sea transport  Classiﬁcation
Neural network
1
Introduction
Maritime transport provides signiﬁcant amounts of internal and external freight over
long distances, it is an important component of the economy of coastal and maritime
countries. In a competitive transport companies are faced with the problem of choosing
an effective business acquisition strategy that provides adequate load vessels at
admissible idle times.
The project management knows the position of the connection between resource
efﬁciency and uniformity of loading. The high cost of ships and their operation gen-
erates interest maritime transport companies to the fullest use of available resources
through the formation of portfolios of orders for shipping, which will provide a pos-
sible continuous and uniform loading of vessels [4, 5]. In addition in order to increase
proﬁts, such as loading of vessels can signiﬁcantly reduce the additional costs asso-
ciated with the peculiarities of navigation and safety of maritime transport, for example,
caused by the need to idle, as the vessel is anchored or admission to under loading ship
ballast water.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_9

1.1
Relation of Highlighted Issue to Scientiﬁc and Practical Tasks
In the simplest case, the condition of continuous and uniform loading of vessels is
achieved in a situation where in each port of loading and unloading cargo discharged is
replaced by an equivalent volume, weight and requirements for transporting cargo for
delivery to the ports on the route of the vessel. Unfortunately, such a balancing circuit
is often unachievable due to the impact of a number of factors, such as competition
between carriers, seasonal trafﬁc ﬂuctuations in the imports and exports volume of the
countries, the orientation of certain ports for certain types of goods, and etc. As a
consequence, marine transport companies have to contend with incomplete congestion
of vessels, lost proﬁts and the risk of possible loss of the transport market [7, 9].
The aim of the article: research of load balancing indicators for resources trans-
portation companies and the development of mechanisms to improve project man-
agement effectiveness of marine transport by improving the balance of the load on the
resources in accordance with the state of each company and the transport market as a
whole.
1.2
Analysis of Recent Researches and Publications
The methodology of the research. To achieve this goal will perform data collection
based on the results of marine transport companies work in “Odessa” and “Cher-
nomorsk” ports, to analyze the data to identify the relationships between levels of
balanced portfolio of orders and the growth in maritime freight transport methods of
comparative analysis, time series analysis, correlation and regression analysis, the
method of neural models.
Statement of the main research material. Delivery of goods through “Odessa” and
“Chernomorsk” ports provided by shipping lines: CMA, ACOL, ANL, APL, ARKAS,
BUL, COSCO, CSCL, ECM, EMES, EVERGR, HJS, HLC, HMM, KLINE, MAE,
MISC, MOL, MSC, NOR, NYK, OOCL, PIL, UASC, WHL, YMG, ZIM and others
[8–12]. Due to the signiﬁcant volume of initial data on volumes of freight trafﬁc
through “Odessa” and “Chernomorsk” ports on mentioned shipping lines such data are
represented in graphs (Fig. 1 – cargo turnover data; Fig. 2 – data on the ratio of import
and export cargo volume).
2008
2009
2010
2011
2012
2013
0
20000
40000
60000
80000
100000
120000
140000
160000
180000
CMA
ACOL
ANL
APL
ARKAS
BUL
COSCO
CSCL
ECM
EMES
EVERGR.
HJS
HLC
HMM
KLINE
MAE
MISC
MOL
MSC
NOR
NYK
OOCL
Others
PIL
UASC
WHL
YMG
ZIM
TEU
Fig. 1. The volume of freight trafﬁc (in TEU) through “Odessa” and “Chernomorsk” ports along
the lines of (2008–2013)
114
A. J. Gaida et al.

As is evident from Fig. 1, the volume of container shipping for the past few years in
general increased slightly – despite the considerable drop of transportation in 2009
caused by global crisis 2008–2009. After a recession in 2009 trafﬁc volumes generally
rose slightly, and according to the data for 2013 almost reached pre-crisis levels.
2
The Main Objectives of the Research and Their Meaning
Taking into account available data project management efﬁciency of each company can
be assessed through growth indicator volume transportation. Total volume of trafﬁc of
each company is:
Vi ¼
X
f
j¼s
vi;j;
ð1Þ
where: f, s – year end and beginning of the period, respectively; vi;j – the volume of
trafﬁc “i” company in the “j” year.
Then, the average annual trafﬁc growth of each company for a certain period can be
calculated as the ratio of in the relative increment in trafﬁc volume during that period to
a long period in years:
ki ¼ vi;f  vi;s
f  s
ð
ÞVi
;
ð2Þ
where: ki – growth coefﬁcient i company; vi;f – the volume of trafﬁc at the end of the
period; vi;s – the volume of trafﬁc at the end of the period.
Under the impact on trafﬁc volumes of random factors (as is the case here), the
average annual trafﬁc growth rate for each company for the analyzed period can be
more accurately expressed by the coefﬁcient a of the linear regression y ¼ ax þ b:
Fig. 2. Allocation of import and export cargoes (TEU) through “Odessa” and “Chernomorsk”
ports (2008–2013)
Mechanisms to Enhance the Efﬁciency of Maritime Container Trafﬁc
115

ai ¼
P
m
j¼1
vi;j  P
m
j¼1
j  m  P
m
j¼1
j  vi;j
P
m
j¼1
vi;j
 
!2
m  P
m
j¼1
v2
i;j
;
ð3Þ
where: ai – the growth factor of trafﬁc with linear regression, j – the position of the
indicator of trafﬁc volumes vi;j in the time series, m – the number of elements of the
time series.
The calculated gain coefﬁcients trafﬁc volumes for expression (2) are shown in
Table 1, for expression (3) – in Table 2. As can be seen from Fig. 1 and Tables 1, 2,
despite the unfavorable conjuncture in 2009, some companies were able to signiﬁcantly
increase the volume of trafﬁc. Among the leaders highlighted Maersk company
(shipping line MAE), which is not only the largest carrier, but also has a high average
annual growth of 8.8% (Table 2) and absolute growth of 33.2% (Fig. 1). Impressive
results have also demonstrated the company Mediterranean Shipping Company (MSC
line, respectively, 5.0% and 21.6%) and the company ZIM (eponymous line). The
latter, although it showed a slight decline in general, showed the best among large
companies after the collapse of the growth rate of 2009.
In order to identify efﬁciency of resource management consider the connection
between the import and export cargo volume (Fig. 2). In the most simple case, this link
can be expressed in terms of linear correlation coefﬁcient [1–3] calculated between the
amount of import and export goods for every single company. Company’s results of
correlation calculations are shown in Table 2 (in the calculation of available authors
full data have been used for months).
Table 1. The coefﬁcients of the average annual trafﬁc growth for expression (2)
№Line
Grow rate №Line
Grow rate
1 ACOL
−0,106
15 KLINE −0,129
2 ANL
0,089
16 MAE
0,059
3 APL
0,163
17 MISC
0,123
4 ARKAS
0,189
18 MOL
−0,046
5 BUL
0,265
19 MSC
0,039
6 CMA
−0,990
20 NOR
−0,118
7 COSCO
−0,103
21 NYK
−0,426
8 CSCL
−0,073
22 OOCL
0,042
9 ECM
−0,719
23 PIL
−0,164
10 EMES
−0,719
24 UASC
−0,751
11 EVERGR
0,224
25 WHL
−0,132
12 HJS
−0,842
26 YMG
−0,040
13 HLC
−0,092
27 ZIM
−0,024
14 HMM
0,149
28 Others
−0,081
116
A. J. Gaida et al.

As can be seen from Tables 1 and 2 growth data for individual companies have
signiﬁcant discrepancies. Further we will use the data from Table 2, which, unlike
Table 1, take into account the trafﬁc volumes for all the years of the period, and
therefore more accurately reﬂect the growth trends in the volume of trafﬁc companies.
As can be seen from Fig. 1 and Table 2, between the volume of import and export
cargo takes place signiﬁcant correlation due to aspiration provide company managers
possible more complete and balanced load of vessels. Obviously, this regularity is
achieved by the replacement of discharged goods at ports (most of which – import)
taking on board cargo (export).
Nevertheless, individual companies, this correlation is far enough from 1.0. For
example, for a large and rapidly increase volumes of trafﬁc, Maersk correlation coef-
ﬁcient of 0.959, this corresponds to the level of the eighth balanced load on resources
of the positions among more than 27 companies that provide transportation through
“Odessa” and “Chernomorsk” ports. Similar deviations are reviewed on several other
companies, which in this case indicates a mixed growth dependence on the level of
trafﬁc load balanced resource.
Table 2 shows that Maersk Company (MAE line) in terms of growth takes only
eighth position, which indicates the existence of reserves for future growth. The
position of Maersk becomes even more signiﬁcant because companies with smaller
volumes of freight trafﬁc and, consequently, larger relative overhead costs, able to get
ahead of Maersk in terms of growth.
In order to determine the relationship between the values of uniformity level of
loading resources and value of the relative growth of the company’s share in cargo
transportation, consider the relationship between the growth in trafﬁc volume and the
companies deﬁned above correlation coefﬁcients reﬂecting the regularity loading of
resources (Table 3, Fig. 3). Figure 3 also shows the linear regression (dashed line). As
can be seen from the ﬁgure, “a” coefﬁcient of linear regression equation y ¼ ax þ b is
Table 2. The coefﬁcients of the average annual trafﬁc growth for expression (3)
№Line
Grow rate №Line
Grow rate
1 ACOL
0,099
15 KLINE −0,113
2 ANL
0,306
16 MAE
0,088
3 APL
0,300
17 MISC
0,191
4 ARKAS
0,386
18 MOL
−0,019
5 BUL
−0,854
19 MSC
0,050
6 CMA
−0,078
20 NOR
−0,131
7 COSCO
−0,055
21 NYK
−0,331
8 CSCL
−0,039
22 OOCL
0,150
9 ECM
−0,761
23 PIL
−0,109
10 EMES
−0,761
24 UASC
−0,772
11 EVERGR
0,312
25 WHL
−0,109
12 HJS
−0,803
26 YMG
−0,047
13 HLC
−0,008
27 ZIM
−0,024
14 HMM
0,217
28 Others
−0,041
Mechanisms to Enhance the Efﬁciency of Maritime Container Trafﬁc
117

0.03, which generally indicates a slight increase with the growth of freight trafﬁc load
balanced.
In order to detect changes in trafﬁc volume trend performed polynomial smoothing
of rates depending on the company’s growth from a uniform level of resource uti-
lization. Smoothing results are presented in Fig. 4. A polynomial curve ﬂattening is
designed for smoothing exponent 3. Here, the trend line indicates that increasing
resource utilization level of uniformity results in a corresponding signiﬁcant change in
the rate of growth of the company’s operations. In the low level of resources balanced
load values (here – up to the value of the correlation coefﬁcient of 0.7) achieved
signiﬁcant growth, and above – a signiﬁcant drop of transportation volumes. Polyno-
mial fourth-order gives close to the Fig. 4, curve smoothing, which conﬁrms the
convex nature of the trend distribution with one peak.
Table 3. The coefﬁcients of the linear correlation between the volume of import and export
cargo
№Line
Correl. №Line
Correl.
1 ACOL
0,962
15 KLINE 0,757
2 ANL
0,977
16 MAE
0,959
3 APL
0,861
17 MISC
0,781
4 ARKAS
0,986
18 MOL
0,538
5 BUL
0,987
19 MSC
0,929
6 CMA
0,760
20 NOR
0,832
7 COSCO
0,740
21 NYK
0,949
8 CSCL
0,529
22 OOCL
0,656
9 ECM
0,271
23 PIL
0,803
10 EMES
0,995
24 UASC
0,895
11 EVERGR 0,805
25 WHL
0,605
12 HJS
0,923
26 YMG
0,773
13 HLC
0,830
27 ZIM
0,871
14 HMM
0,811
28 Others
0,762
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
1,1
-1,2
-1
-0,8
-0,6
-0,4
-0,2
0
0,2
0,4
f(x) = 0,03x - 0,17
K
Tu
Fig. 3. Dependence of the rate of the relative growth companies Tu from the uniform level of
loading resources K (dotted trend line set)
118
A. J. Gaida et al.

Considering the signiﬁcant ﬂuctuations in trafﬁc volumes and, above all, their
import component, in order to represent the company’s previous history in the trafﬁc
market, it is important to clear the performance of companies from changes caused by
ﬂuctuations in total trafﬁc volumes. This can be achieved most simply by calculating
the correlation coefﬁcient between the total volumes of trafﬁc and the company’s share
(including import and export shipments). Such data are presented below: for import and
export operations (Fig. 4), separately for import (Fig. 5) and separately for export
(Fig. 6) operations.
0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
1,1
-1,2
-1
-0,8
-0,6
-0,4
-0,2
0
0,2
0,4
K
Tu
Fig. 4. The results of the smoothing depending on the relative rates of growth of the company
Tu from the uniform level of loading resources K (third-degree polynomial trend line set a dotted
line)
Fig. 5. Dependence of the rates of relative growth of companies on the consistency of the
volumes of import trafﬁc
Fig. 6. Dependence of the rates of relative growth of companies on the consistency of the
volumes of export trafﬁc
Mechanisms to Enhance the Efﬁciency of Maritime Container Trafﬁc
119

As can be seen from Fig. 5, with a general drop in the volume of trafﬁc by import,
the growth rates of companies are also decreasing, thus the growth of companies is in a
negative correlation with the company following the current market trends. Similarly,
from Fig. 6, with the overall growth in the volume of trafﬁc by export, the growth rates
of companies also increase, so the growth of companies is in a positive correlation with
the company following the current market trends. At the same time, the correlation
between trafﬁc volumes and growth rates is insigniﬁcant and is 0.24. Obviously, dif-
ferently directed dependencies are a consequence of a signiﬁcant excess of imports
over exports to these ports.
The obtained results allow us to consider the problem of ﬁnding the optimum ratio
of volumes received/discharged as the problem of maximizing on a convex curve, and
the task of improve management efﬁciency – as the task of identifying dependencies,
ensuring high growth rates in the most successful from this point of view of companies.
Mathematical formulation of this problem can be worded as follows: it is necessary
to ﬁnd a value for the level of load evenly on resources which in the current market
position and market conditions will maximize the value of the index rate of growth of
the company. This problem can be solved as a problem of clustering and classiﬁcation
[13, 14].
Based on the available data for each prediction cargo turnover in some of the
following moment in time n + 1 can be performed. This forecast can be viewed as a
function of the time series of indicators of its previous cargo turnover viewed through
the ports, many companies state, working in these ports, and the ratio of values of
import and export goods (see Fig. 4).
vi;n þ 1 ¼ G Vi;n; Sn; xn þ 1


;
ð4Þ
where: xn þ 1 – expected ratios of values of import and export cargo at time n + 1.
Imagine the state of the “i” – company at the time “n” as a function of time series
the values of achieved cargo turnover:
si;n ¼ g vi;s; vi;s þ 1; . . .; vi;s þ n


;
ð5Þ
where: g – calculation function state – cargo turnover (see (1)).
The states of all companies can be represented as a matrix:
Sn ¼
s1;n; s2;n; . . .; sm;n


;
ð6Þ
The time series of cargo turnover – as a matrix:
Vi;n ¼
vi;s; vi;s þ 1. . .; vi;s þ n


;
ð7Þ
To substitute (5) and (6) in (4), we ﬁnd the expected cargo turnover of each
company at some assignment of ratios of import and export cargo.
The problem of calculating the forecast values can be solved by means of an
artiﬁcial neural network with back-propagation errors. The number of network inputs
should be chosen according to the number of time series values of n (n −1 value of a
120
A. J. Gaida et al.

time series and one coefﬁcient value of import and export cargo relations), the last
value of the time series in the training must be seen as a prediction result. With a
signiﬁcant number of time-series elements can be introduced horizon cyclically shifted
training the time series. Solution of the problem is a value ratio of received and
discharged cargoes, which ceteris paribus will provide the greatest increase in cargo
turnover.
Popular neural networks are networks with back-propagation errors. In the simplest
case, a single-layer network its output “s” can be represented by a vector of input
signals and “x” coefﬁcient vector “k” as:
sc ¼ f
X
n
i¼0
xiki;c þ bi;c
 
!
;
ð8Þ
where: sc – the output status for the class “c”; xi – i value of the parameter at the input;
ki;c – transfer coefﬁcient of “i” entry on “c”-output; “f” – the activation function; bi;c –
displacement.
As can be seen from (4), a single-layer network is able to reproduce the linear
dependence of the output values from the input and their subsequent transformation
activation function. In the case of more complex dependencies apply multilayer net-
work. For a two-layer network output value s:
sc ¼ f
X
n
j¼0
kj;cf
X
m
i¼0
xikj;i þ bj;i
 
!
 
!
 
!
;
ð9Þ
where: kj;c – transmission coefﬁcient intermediate “j” element of the intermediate layer
on “c” element output; xi – “i” value of the parameter at the input; kj;i – transmission
ratio “i” input of the input layer to the intermediate “j”-input.
Test ANNs trained on known data sets (Figs. 1 and 2, Table 2) has been imple-
mented with the aim of testing the quality of cargo turnover of volumes forecasting
growth.
Justiﬁcation of results. The results predict cargo turnover of individual companies
received for ﬁxed values of the correlation of import and export of goods are shown in
Fig. 7. In Fig. 6 shows the results of simulation of the relative of volumes cargo
turnover in some companies in 2013 with different ratios of volumes import and export
cargo.
From the modeling results can be seen that generally increase in the level of
resources provides balanced load cargo handling increase growth (Fig. 7).
At the same time, for ACOL lines, MAE, MSC growth reserves are almost
exhausted (Figs. 8 and 9), which clearly indicates the high quality of management in
these companies. For other companies, the growth of reserves are signiﬁcant.
The selection task factor of a parity of import and export goods, providing the
greatest increase in competition between companies can be resolved by known methods
of research.
In general, testing results have shown the ability to improve maritime transport
companies’ management.
Mechanisms to Enhance the Efﬁciency of Maritime Container Trafﬁc
121

0,2
0,3
0,4
0,5
0,6
0,7
0,8
0,9
1
1,1
-1,5
-1
-0,5
0
0,5
1
Fig. 7. The results of forecasting for companies that constantly were present on shipping market
via the “Odessa” and “Chernomorsk” ports (trend line shown in phantom)
Fig. 8. Forecast of the relative trafﬁc volumes for 2013 through the “Odessa” and
“Chernomorsk” for some companies at different values of the correlation of “imports” and
“exports” amounts (for testing)
Fig. 9. Forecast of the relative trafﬁc volumes for 2014 through the “Odessa” and
“Chernomorsk” for some companies at different values of the correlation of “imports” and
“exports” amounts
122
A. J. Gaida et al.

3
Conclusion
The obtained results show the possibility of increasing the effectiveness of management
of marine transportation in the form of the portfolio orders to the best, in terms of
growth in the volume of transported goods, by the relations “of import” and “export” of
goods.
Obtained results may be used in the formation and transport companies balancing
portfolios of projects, as well as in dealing with other project management tasks for
which the available data on the levels of uniformity of loading of resources and
associated performance of the company.
References
1. Boks, D., Dzhenkins, G.: Analysis of Time Series. Prognosis and Management [Analiz
vremennyih ryadov: prognoz i upravlenie]. Mir, Moscow (1974). (in Russian)
2. Byvshev, V.A.: Econometrics [Ekonometrika]. Finance and Statistics, Moscow (2008). (in
Russian)
3. Venikov, V.A.: The Theory of Similarity and Modeling [Teoriya podobiya i mod-
elirovaniya]. High School, Moscow (1976). (in Russian)
4. Gaida, A.J., Gordeev, B.N., Koshkin, K.V.: Features of Management Knowledge-Intensive
Industries in Shipbuilding Projects [Osobennosti upravleniya resursami proektov naukoem-
kih proizvodstv v sudostroenii]. NUS, Nikolaev (2011). (in Russian)
5. Koshkin, K.V., Gaida, A.J.: Recourse Management Portfolio Knowledge-Intensive Produc-
tion Projects in the System with Predictive Model [Upravlenie resursami portfelya proektov
naukoemkih proizvodstv v sisteme s prognoziruyuschey modelyu]. Publishing House
“Science Book”, Voronezh (2013). (in Russian)
6. Korn, G., Korn, E.: Mathematical Handbook for Scientists and Engineers [Spravochnik po
matematike dlya nauchnyih rabotnikov i inzhenerov]. Nauka, Moscow (1984). (in Russian)
7. Grebenik, E.: Two of Brooklyn: The Success Story of Yuri Gubankova [Dvoe iz Bruklina:
istoriya uspeha Yuriya Gubankova]. Forbes-Ukraine, Kiev (2015). (in Russian)
8. BlackSeaLines: Golden container-2008 [Zolotoy konteyner-2008]. Black Sea Lines, Odessa
(2009). (in Russian)
9. BlackSeaLines: Peak fall passed [Pik padeniya proyden]. Black Sea Lines, Odessa (2010).
(in Russian)
10. BlackSeaLines: Golden container 2010 [Zolotoy konteyner 2010]. Black Sea Lines, Odessa
(2011). (in Russian)
11. BlackSeaLines: Golden container 2011 [Zolotoy konteyner 2011]. Black Sea Lines, Odessa
(2012). (in Russian)
12. BlackSeaLines: Golden Container 2012 [Zolotoy konteyner 2012]. Black Sea Lines, Odessa
(2013). (in Russian)
13. Karpov, L.E., Yudin, L.E.: Adaptive management on precedents, based on the classiﬁcation
of the states of managed objects [Adaptivnoe upravlenie po pretsedentam, osnovannoe na
klassiﬁkatsii sostoyaniy upravlyaemyih ob’ektov]. In: Proceedings of the Institute for
System Programming of the Russian Academy of Sciences, Moscow (2007). (in Russian)
14. Zipkin,
J.Z.:
Fundamentals
of
the
Theory
of
Learning
Systems
[Osnovyi
teorii
obuchayuschihsya sistem]. Nauka, Moscow (1970). (in Russian)
Mechanisms to Enhance the Efﬁciency of Maritime Container Trafﬁc
123

Porting a Real-Time Objected Oriented Dependable
Operating System (RODOS) on a Customizable
System-on-Chip
Muhammad Faisal
(✉) and Sergio Montenegro
Institute of Aerospace Information Technology, Julius-Maximilians-University Wuerzburg,
Würzburg, Germany
{muhammad.faisal,sergio.montenegro}@uni-wuerzburg.de
Abstract. Modern semiconductor chips oﬀer a FPGA, A Hard Microcontroller
and a programmable Analog circuitry all integrated on a single chip, which gives
the system designer a full featured, easy-to-use design and development platform
where all the units are programmable and under full control of the designer,
Combining this state of the art silicon chip with a Real Time Operating Systems
(RTOS) gives an Engineer full power and all degree of freedoms to design end-
use applications with an unparalleled performance characteristic as far as speed,
Security, Simplicity, Flexibility and reliability is concerned. In this paper we
describe.
Keywords: FPGA · Real-Time-Operating-System · Porting
1
Introduction
Last decade has witnessed cutting edge inventions in the ﬁeld of Systems-on-Chip which
has provided engineers an out-of-the-box solution for high conﬁgurability, Reliability,
Accuracy, Area and power eﬃciency in an embedded systems. Over the years engineers
have relied on the traditional microcontrollers to design their systems but the micro‐
controllers have a bottleneck for the speed as the clock rate cannot go very high because
of large amount of heat generation so to overcome this limitation FPGAs had come in
this domain but the complex parallel design ﬂow of the FPGAs slows the development
time and dealing with the analog signals with external ADC and DAC is always chal‐
lenging but the fusion of the microcontroller, FPGA and programmable analog in a
single chip is an integrated and comprehensive approach to design an ideal platform for
control and processing (Fig. 1).
This fused single chip as shown in Fig. 2 has all the intelligence of a microcontroller,
Speed through FPGA and programmable Analog circuitry to deal with the real world
applications. Microcontroller is relieved from many initializations of the analog compo‐
nents as the analog controller takes this responsibility on his own shoulders. All the
programmable units on the single chip make the system highly ﬂexible and reduce the
footprint of the hardware. All the data is locally transferred between these programmable
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_10

computational units with no overhead as described in Fig. 2. This fused integrated chip
provides engineer great power but great power comes with great responsibility and to
fulﬁll this responsibility a RTOS is considered to be the best resource -manager of an
embedded systems. RODOS is a real-time-operating-system for embedded systems and
it was designed for application domains demanding high dependability [2]. RODOS is
jointly developed by the Central Core Avionics department of German Aerospace Center
and Chair of Aerospace Information Technology University of Wuerzburg Germany.
RODOS is speciﬁcally developed for aerospace applications as it has minimal footprint
but it is also very well suited to all applications that demand high dependability [3]. It
is used for the current micro satellite programs of German Aerospace Center (DLR).
RODOS runs on the operational satellite TET-1(Technology Experiment Carrier-1) [4]
and it will be used for the currently developed satellite BIROS (Bi-spectral Infrared
Optical System) [5]. RODOS is further enhanced and extended at the German Aerospace
Center as well as the department for aerospace information technology at the University
of Wuerzburg.
Fig. 2. Inter-system communication in mixed signal IC
This project is performed as a part for the Moon Landing mission of the Part-Time-
Scientist team of Germany which are building an Autonomous Landing and Navigation
Module (ALINA) as the first private mission to the Moon [6]. Being the project partner the
Aerospace Information Technology department in University of Wuerzburg is developing
the software for the navigation and control, It programs both the lander as well as rover on
the moon, The department will provide interfaces to monitor the flight to the moon, begin‐
ning at low earth orbit (LEO) until the landing. The Part-Time-Scientist has chosen the
ACTEL Smartfusion chip [7] as their on board computer. Aerospace Information
Fig. 1. Block diagram of mixed-signal integrated circuit
Porting RODOS on a Customizable System-on-Chip
125

Technology department in University of Wuerzburg is using RODOS in all the recently
launched satellites which it plans to use again for ALINA mission, That’s why porting
RODOS on Smart-Fusion platform was a big requirement of ALINA mission. Figure 3
shows the conceptual design of the on-board-processor for the ALINA project with RODOS
on Smart-Fusion. Basic strategy to port the RODOS on Smart-Fusion platform is described
in Fig. 4. First of all the hardware platform must be configured and in this case the micro‐
controller will be configured by programming the FPGA. The tool-chain has to be set for
cross-compiling and flashing the binaries on the target. The other important step is getting
the CPU and other peripheral register definitions in the relevant header files, Setting the
memory-map and also mentioning the starting address and sizes of volatile and non-vola‐
tile memory, Providing the mechanism for Timing, Context-Switching, Debugging and in
the end performing the test cases to verify the correct operation of RODOS on the Smart-
Fusion platform. Libero and Soft-Console were used as the software component from the
manufacturer of the chip in the design flow of the whole porting process. The experienced
which is gained in this porting process will set the guide line of the future porting of RODOS
on the new hardware architecture.
Smart Fusion Chip
Programmable 
Analog
FPGA
Fabric
Microcontroller
Susbsystems
RODOS
RODOS 
Applicaon
Fig. 3. RODOS application running on a mixed signal IC
Configure 
Hardware
and Compiler
•
As RODOS is based on C++ so we need to provide compiler support for constructors and destructors.
•
Determining the Smart Fusion‘s architecture for all its capabilities required by the Operating Systems.
•
Develop and configure the interface of Microcontroller and Analog Circuitry in FPGA.
Timing Interface
Contex Switching
Debug Console
Tests
•
Create the tests cases to 
verify our port. i.e Interrupt,
Preemption, Synchronization,
etc..
•
Adapt the RODOS timing Interface  with the Hardware timer present on Smart Fusion chip.
•
RODOS like other Operating Systems relies heavily on the system timers therefore implement
RODOS‘s hwGetNanoseconds() functions as the time resolution in RODOS is nanoseconds
•
Provide serial port transmit function for 
debugging.
•
RODOS has a PRINTF(character) which should
be implementd according to the the serial
portof Smart Fusion.
•
Implement following RODS functions for saving and loading context
__asmSwitchToContext(ctx) and __asmSaveContextAndCallScheduler()
by using the functionality of PENDSV exception of cortex m3.
Fig. 4. Porting procedure of RODOS on Smart-Fusion chip.
126
M. Faisal and S. Montenegro

2
Designing a Programmable Hardware
Before porting the RTOS on the customizable chips we must conﬁgure the peripherals
on Smart Fusion chip. These peripherals includes clock, SRAM, ENVRAM, AC
(Analog Controler) and I/O interfaces such as UART, SPI, I2c etc. Besides these periph‐
erals conﬁguration, three most important steps has to be performed as shown in the list
below:
• Setting Clock Speed.
• Configuring the microcontroller-sub-systems (MSS) and Generate the MSS
Component.
• Generating the Binary Files to Program the FPGA.
2.1
Setting the Clock Speed
Clock Management must be performed to conﬁgure the On-Chip oscillator. The input
clock frequency for the PLL has to be set as shown in the Fig. 5, CLKA the clock to the
PLL is set as 100 MHz, Conﬁgure the PLL to get the 80 MHz MSS clock frequency.
These settings are taken as the recommendations of the manufacturer’s startup guide,
Later it can be modiﬁed according to the requirement.
Fig. 5. Clock conﬁguration for the controller.
Conﬁguring the MSS and Generating the MSS Component
First the microcontroller-subsystem has to be generated as shown in Fig. 6. This micro‐
controller-subsystem must be set with proper clock speed and it has to conﬁgure the
peripheral of the available MSS on the chip. The Generation of the MSS components
means generating the peripheral drivers specially for timer and UART because these
Porting RODOS on a Customizable System-on-Chip
127

drivers are required for the scheduling and debugging respectively as the result of this
step the linker script and the startup ﬁles are also generated which are required for initi‐
alization and ﬂashing the high level code (C or C++) on the chip (Fig. 7).
Fig. 6. Micro-controller-subsystems
Fig. 7. MSS component
2.2
Programming the FPGA
Depending on the selection and the conﬁguration of the user, Hardware must be synthe‐
sized and routed and later the binary ﬁle should be generated to program the FPGA.
After this step all the communication path is established between the FPGA fabric and
the Microcontrollers as the FPGA get able to access the external memory controller
(EMC) of the microcontroller to communicate with external memory devices [8].
3
RODOS
RODOS is a real time operating systems which was purposely designed for Aerospace
Applications, It is already used in a real time satellite TET of German Aerospace Center,
which is already in the orbit. It is hoped that this thesis work will be a signiﬁcant step
to manage more sophisticated tasks in RODOS related to the modern high speed space
instruments and ever increasing complexities with the help of customizable-high-
performance-architecture in the future space missions (Fig. 8).
128
M. Faisal and S. Montenegro

Fig. 8. RODOS structure
3.1
RODOS Internal Directory Structure
RODOS kernel was developed using C++ but it is also compatible with C ﬁles without
any changes. RODOS directory structure is shown in the Fig. 9.
Fig. 9. RODOS main directory structure
The folder “api” contains the header ﬁles of RODOS kernel. This folder contains all
the ﬁles related to the declaration of all the kernel components like thread, FIFO, Sema‐
phore, Timers, Gateways etc. The “doc” directory contains the documentation about the
RODOS structure. “Make” folder contains all the shell scripts of RODOS compilation
on the diﬀerent platforms. The folder “support_libs” contains the libraries which are
supported by RODOS. Although there are numerous folders in RODOS but here only
those folders are described which are directly related to the porting process.
Porting RODOS on a Customizable System-on-Chip
129

Figure 10 shows the “src” directory which contains the hardware related implemen‐
tation of RODOS porting. It contains the Smart-Fusion directory which is related to the
hardware structure, All the porting steps are related to the ﬁles in this folder, It contains
the peripherals drivers, deﬁnition of debug board and speed, Stack deﬁnition and its size,
The hal-rodos folder contains all the ﬁles related to the implementation of the commu‐
nication protocol like UART, SPI, CAN, UDP etc. These protocols are provided with
the abstraction level i.e. the functions prototype, parameters, their names are same but
the user has to implement the functionality depending on the architecture of the
processor.
Fig. 10. RODOS sub-directories
4
Porting RODOS on Smart-Fusion
RODOS is already ported on a wide range of processor architecture, like PowerPC,
Leon, ARM, Blackﬁn, AVR etc. In this work we the cortex-M controller is being used
which is present on the Smart-Fusion chip. In this section the approach to port RODOS
on Smart-Fusion SOC is described in detail.
4.1
C++ Support of Compiler
As RODOS is based on C++ that’s why compiler support has to be provided to call the
global constructors and destructors. This call will initialize all the data members of a C
++ class before any objects of the class are created. The constructors are invoked before
calling the “main” function and destructors must be called after the “return” function [9],
Which is mainly invoking global Constructors in the _start function before executing
the main(). _start symbol is responsible to manage all the initialization tasks related to
the booting. This task is achieved through ﬁve Object ﬁles: crt0.o, crti.o, crtbegin.o,
crtend.o and crtn.o
130
M. Faisal and S. Montenegro

• crtbegin.o speciﬁes the starting point of the constructors in the memory.
• crtend.o represents the starting point of the destructors in the memory.
crtbegin.o and crtend.o are part of compiler. These ﬁles are also referred as auxiliary
startup ﬁles [10]. These constructors and destructors are called in the order. These static
constructors and destructors are addressed through the linker script. In RODOS compi‐
lation it is explicitly conﬁgured not to use these functions provided by the compiler as
RODOS uses the functions from the manufacturer who provides this in the startup ﬁles.
4.2
Hard and Soft Initialization at Startup
Before calling the main function in RODOS certain initialization has to be performed
both at hardware level and software level as shown in Fig. 11. On hardware-side the
clock must be enabled for all the basic components such as Timer, UART etc. The
Interrupt vector table also has to be initialized for the proper functionality of the
controller. One more important step has to be taken regarding the pendSV exception,
its priority must be conﬁgured to low for proper context-switching during the scheduling.
On RODOS’s side developer will have to provide the mechanism to call the global
constructors before calling the main. The stack size for each thread must be deﬁned in
RODOS’s params.h ﬁle, Here it is also required to identify the UART port to be used,
In the Smart-Fusion chip there are two serial ports and in this work UART0 is used for
printing the debugging messages. In RODOS’s hw_speciﬁc.h ﬁle the baud-rate also to
be deﬁned for the serial-communication. From the point of view of thread-initialization
the thread name, Its ID, Its priority and delay time must be set before it is called by the
scheduler.
  Hardware 
Inializaon 
  RODOS 
Inializaon 
  Thread 
Inializaon 
•
Inialize  IVT 
     (Interrup Vector Table) 
•
Inialize Clock 
 
 
•
Global Constructors 
•
Stack Size 
•
Thread List 
•
Seng debug-port  
•
Seng Baud-rate 
 
Priority 
Name 
Stack Pointer 
Context Pointer 
Fig. 11. Iitialization before executing tread
4.3
RODOS Timing Interface
Like any other operating system, RODOS is required to use a timer mechanism for
Scheduling and timing.
Porting RODOS on a Customizable System-on-Chip
131

There are two 32 bit-timers and one watchdog-timer available in SmartFusion which
can be used as one-shot and one periodic timer [13]. In this porting procedure Timer0
is used which is a periodic timer, The driver for this timer is produced as the result of
MSS component generation. The thread timing in RODOS is managed by following
three functions:
• void TIMx_init()
• void TIMx_IRQHandler()
• unsigned long long hwGetNanoseconds()
For porting RODOS on any architecture the developer must provide the implementa‐
tion of above three functions as far as timing operations are concerned: RODOS gives time
in nanoseconds related to the startup time. These function are defined in hw-timer.cpp
file. The implementations of these functions is depicted in Figs. 12, 13, 14 and 15.
Fig. 12. Timer initialization function (TIMx_init())
Fig. 13. Timer interrupt handler (TIMx_IRQHandler())
132
M. Faisal and S. Montenegro

Fig. 14. hwGetNanoseconds()
Fig. 15. Scheduling in RODOS
4.4
Context Switching
Every thread in any operating systems has its own stack i.e. the state of the CPU registers,
This state of the CPU registers during the execution of any thread is referred as the
context of the thread. When a high priority thread interrupts a low priority thread then
the ﬁrst job of CPU is to save the context of interrupted thread before serving the inter‐
rupts and when the interrupt service routine (ISR) is serviced then the CPU ﬁrst reload
the context (i.e. the values of core registers) of the blocked thread and then restore the
execution of the blocked thread as show in Fig. 16. In order to handle the context-
switching two functions must be implemented in RODOS:
Porting RODOS on a Customizable System-on-Chip
133

• void __asmSaveContextAndCallScheduler()
• void __asmSwitchToContext()
.  .
.  .
Blocked
Thread
Running
Thread
CPU
RAM
core  registers
r0
r1
r2
Restore
Context
Save
Context
sp
lr
Restore Context
Save Context
r13
Fig. 16. Context-switching in ARM
RODOS uses the PendSV Exception for context switching. These functions for
context switching are implemented in hw-speciﬁc.cpp ﬁle. As the CPU in Smart-Fusion
is based on ARM cortex-m3 and the context switch is based on the handling of ARM-
core registers that’s why all the context switching functionality was taken from previ‐
ously ported RODOS versions [14]. The pendsv software exception form ARM was
used to avoid the use of systick exception.
The systick exception was not used to avoid the danger of delay that might be intro‐
duced by the using systick timer because CPU can not be halted too long. PendSV is a
software interrupt mechanism provided by ARM to reduce the contex switching time
[15], PendSV exception provides a brute-force approach for context-switching.
First RODOS is in idle thread mode and when its time slot is over the timer timeout
occur then pendsv exception occurs by the yield function of RODOS, In this pendsv
handler all the context is saved. The context switching is managed by the pendSV
exception to avoid processor’s usage fault exception. Then the scheduler ﬁnd the next
thread to run, It loads the thread’s context and run the ﬁrst thread as shown in the
Fig. 16 and when its time slot is over the scheduler again searches next thread to run
and in this case threads2 starts execution and if during the execution of thread2 if any
ISR comes then the control is transferred to pendsv which saves the context and after
servicing the interrupt service routine the context of interrupted thread is loaded back
and thread2 resumes and If there is no other thread in the list then after thread2 completes
the Idle threads is called as shown in Fig. 17.
134
M. Faisal and S. Montenegro

Fig. 17. Context switching in RODOS
4.5
Serial Console (for Debugging)
To provide the runtime debugging for fault detection and correction a debug mechanism
is required. Normally there is a “printf” function on host computers but to get the inter‐
action with the embedded board, The developer has to implement the RODOS’s own
“PRINTF” function which is a serial output of the strings. There are two UART port
present on the Smart-Fusion and for the serial console but here in this porting procedure
UART0 is used. First the Initialization of Serial port for Debugging for printing
Messages is required as shown in Fig. 17 then the developer has to implement the UART
Transmit function of the chip in the RODOS PutcharNoWait function as shown in
Fig. 19. These debugging functions has to be implemented in RODOS’s hal_uart.cpp
(Fig. 18).
end
Set: 
•
Baudrate 
•
No of Data Bits 
•
Parity Bits 
Hw_HAL_UART::init(baudrate) 
Uart Init Funcon 
Provided by the
Smart Fusion 
Fig. 18. Initialization of UART for Debugging in RODOS
Porting RODOS on a Customizable System-on-Chip
135

Fig. 19. RODOS debug function (PRINTF)
5
Hardware Setup
To perform the porting procedure the Smart-Fusion hardware development kit from
Microsemi was used in combination with the GNU-ARM-Toolchain, the details of the
hardware and software components is given in the following list:
• GNU arm-toolchain
• A 8086 AMD 64-bit Host Computer
• Smart-Fusion AF200 Development Kit
• Lauterbach Power-Debug Module
The Hard and soft components were used as shown in the Fig. 20. ARM GNU tool-
chain was used for compilation, assembling and linking, A low cast Smart-Fusion
development kit was used as prototype to run the applications. The porting was tested
both on Windows and Linux, On Windows Smart-Fusion can be programmed and
debugged through in-system USB based programming but on Linux platform an external
debugger [16] was used as it was not possible to program it with USB.
136
M. Faisal and S. Montenegro

Gnu  
Tool-Chain
USB
JTAG
SmartFusion
Development Kit
Lauterbach Debug Pro
Fig. 20. Hardware-setup for porting
6
Execution of Test Cases
RODOS has some test cases to verify the porting. These tests are related to the Thread’s
timing, Preemption mechanism, Local Communication, Middleware, Gateway and link-
interface of RODOS. Here the results of the basics tests are described which veriﬁes the
timing, priority and memory sharing mechanism of RODOS.
6.1
Threads and Time
This is a thread which waits for a time point. It loops and waits 2 s in each loop in order
to check if the time diﬀerence of each execution is more than 2 s, because of the time
consumed in the thread execution. The code and result to verify the timing is given in
Figs. 21 and 22 respectively.
1 #i n c l u d e
” rodos . h”
2
s t a t i c
A p p l i c a t i o n
module01 ( ”TestTime” ) ;
3
4
c l a s s
TestTime
:
p u b l i c
Thread {
5 p u b l i c :
6
TestTime ()
:
Thread ( ” w a i t f o r ” )
{ }
7
void
run (){
8
i n t
cnt =0;
9
w h i l e (1){
10
cnt++;
11
s u s p e n d C a l l e r U n t i l (NOW( ) + 2∗SECONDS) ;
12
PRINTF( ” A f t e r
2
Seconds
:
%3.9 f %d\n” , SECONDS NOW( ) ,
cnt ) ;
13
}
14
}
15
void
i n i t ( )
{ PRINTF( ” Waiting
2
seconds ” ) ; }
16 };
17
18
s t a t i c
TestTime
testTime ;
Fig. 21. Timer testing thread
Porting RODOS on a Customizable System-on-Chip
137

Fig. 22. Result of timing test
1 #i n c l u d e
” rodos . h”
2
3
s t a t i c
A p p l i c a t i o n
module01 ( ” PreemptiveTest ” ,
2000);
4
c l a s s
H i g h P r i o r i t y T h r e a d :
p u b l i c
Thread{
5 p u b l i c :
6
H i g h P r i o r i t y T h r e a d ( )
:
Thread ( ” H i P r i o r i t y ” ,
25) {
7
}
8
9
void
i n i t ( )
{
10
x p r i n t f ( ”
h i p r i =
’∗’ ” ) ;
11
}
12
13
void
run ( )
{
14
w h i l e (1)
{
15
x p r i n t f ( ”∗” ) ;
16
FFLUSH ( ) ;
17
s u s p e n d C a l l e r U n t i l (NOW() + 500∗MILLISECONDS ) ;
18
}
19
}
20 };
21
22
c l a s s
LowPriorityThread :
p u b l i c
Thread {
23 p u b l i c :
24
LowPriorityThread ()
:
Thread ( ” L o w P r i o r i t y ” ,
10) {
25
}
26
27
void
i n i t ( )
{
28
x p r i n t f ( ”
l o p r i =
’ . ’ ” ) ;
29
}
30
31
void
run ( )
{
32
i n t 6 4 t
cnt = 0 ;
33
i n t 3 2 t
i n t e r v a l T o P r i n t = getSpeedKiloLoopsPerSecond ()
∗
10;
34
w h i l e (1)
{
35
cnt++;
36
i f
( cnt % i n t e r v a l T o P r i n t == 0)
{
37
x p r i n t f ( ” . ” ) ;
38
FFLUSH ( ) ;
39
}
40
}
41
}
42 };
43
44 /∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗/
45 H i g h P r i o r i t y T h r e a d
h i g h P r i o r i t y T h r e a d ;
46 LowPriorityThread
l o w P r i o r i t y T h r e a d ;
47 /∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗∗/
Fig. 23. Preemption code
138
M. Faisal and S. Montenegro

6.2
Preemption
To perform the priority testing, two threads were created as given in Fig. 23, One with
a high priority which is executed very shortly every 3 s and one with a low priority which
is executed constantly. The high priority thread (printing “*”) shall assume the CPU
when it needs it even if the low priority thread (printing “.”) does not suspend or yield
as depicted in Fig. 24.
Fig. 24. Preemption result
6.3
Local Communication
Communication between the diﬀerent threads can be established by using shared
memory, RODOS provides two mechanism to implement the data sharing among
threads, these mechanism includes, FIFO and Semaphore. Here simple FIFO test were
performed as shown in Fig. 25. For Synchronous communication from one single writer
to one single reader. Neither will be suspended. Writing to a full FIFO has no eﬀect and
returns 0. Reading from an empty FIFO returns 0. Figure 26 refers the outcome of the
local communication test.
Porting RODOS on a Customizable System-on-Chip
139

Fig. 25. Local communication thread
Sending 1 
Sending 2 
Sending 3 
Sending 4 
Sending 5 
Sending 6 
Sending 7 
Sending 8 
Sending 9 
Fifo full 
Fifo full 
Fifo full 
Fifo full 
Fifo full 
Fifo full 
Wainng 
3 seconds 
reading 1 
reading 2 
reading 3 
reading 4 
reading 5 
reading 6 
reading 7 
reading 8 
reading 9 
Sending 16
Sending 17
Sending 18
Sending 19
Sending 20
Sending 21
Sending 22
Sending 23
Sending 24
Fifo full 
Fifo full 
Fifo full 
Fifo full 
Fifo full 
Fifo full 
Wainng 3 
seconds 
reading 16
reading 17
reading 18
reading 19
reading 20
reading 21
reading 22
reading 23
reading 24
Sending 31
Sending 32
Sending 33
Sending 34
Sending 35
Sending 36
Sending 37
Sending 38
Sending 39
Fifo full 
Fifo full 
Fifo full 
Fifo full 
Fifo full 
Fifo full 
Wainng 3 
seconds 
reading 31
Fig. 26. Local communication result
6.4
Stack Usage
The stack utilization was also examined on the Smart-Fusion board with RODOS’s
threads. Every RODOS’s thread has its own stack which might be set in the parameter
ﬁle (params.h).
140
M. Faisal and S. Montenegro

Stack overﬂow might be lethal for any application, In this demo ﬁve threads were
created in RODOS to analyze the stack usage as shown in Figs. 27 and 28, One of thread
(Stack Consumer) was designed to consume all the stack memory and the results are
shown in Fig. 29.
Fig. 27. Stack-usage test part 1
Porting RODOS on a Customizable System-on-Chip
141

Fig. 28. Stack-usage test part 2
Fig. 29. Result for stack consumption test 1
142
M. Faisal and S. Montenegro

Fig. 30. Code for CPU-time consumption
Analysis of CPU consumption
The CPU-time consumption or CPU-usage was also analyzed with RODOS threads as
shown in Fig. 29 where two threads were created, One of the thread TestTime2 is printing
a message every 300 ms and the thread TestTime3 is printing the message every 100 ms.
The result for this test is shown in Fig. 31.
Fig. 31. Result for CPU-time consumption test
The thread named “waitAT_3” is printing more often as compared to thread named
“waitAt” that’s why its CPU consumption was larger whereas thread named “IdleTh‐
read” shows the time when the CPU was not busy or in IDLE state (Fig. 30).
7
Conclusion
This paper provides the details of porting a real time operating system RODOS on a
fully programmable mixed signal system-on-chip. RODOS was already ported on
Cortex-M0 based microcontroller so many of the routines such as context-switching
Porting RODOS on a Customizable System-on-Chip
143

were reusable. Here the test code for Thread Timing, Preemption and local communi‐
cation and result of these test cases were also presented which veriﬁes proper function‐
ality. The highly conﬁgurable nature of Smart-Fusion chip allows porting of a real time
operating system with best run-time characteristics with extremely optimized hardware
resources. As RODOS is designed speciﬁcally for Aerospace Applications that’s why
it is highly optimized in terms of performance and size which is the prime requirement
in aerospace application so RODOS running on this highly reconﬁgurable SOC delivers
immense capabilities to an On-board computer. In this work the FPGA was programmed
with minimal resources i.e. Clock, External SRAM, External Non-volatile memory,
UART and Timer the porting results of RODOS on this chip shows correct execution
of RODOS’s thread, Later for other functionalities FPGA program can be easily
extended to include I2C, SPI, Analog controller etc., The FPGA design ﬂow environ‐
ment will generate the relevant driver ﬁles which can be easily integrated into RODOS.
Acknowledgment. 
The authors would like to thank Mr. Erik Dilger who shared his
understanding about the internal structure of RODOS and how its threads work and also many
thanks to the support team in Lauterbach [16] who have provided signiﬁcant tips and help for
debugging the Smart-Fusion kit.
References
1. Rodos (operating system). https://en.wikipedia.org/wiki/Rodos
2. RODOS, Real time kernel design for dependability. http://www.dlr.de/irs/en/Portaldata/46/
Resources/dokumente/produkte/rodos.pdf
3. Dannemann, F., Montenegro, S.: Embedded Logging Frame-Work For SpaceCrafts DASIA
(Data Systems In Aerospace), Porto, Portugal (2013)
4. TET-1 (Technology Experiment Carrier-1). https://directory.eoportal.org/web/eoportal/
satellite-missions/t/tet-1
5. BIROS (Bi-spectral InfraRed Optical System). https://directory.eoportal.org/web/eoportal/
satellite-missions/b/biros
6. ALINA, The Autonomous Landing and Navigation Module, First Private Mission to the
Moon. http://ptscientists.com/
7. SmartFusion 
SoC 
FPGAs. 
https://www.microsemi.com/products/fpga-soc/soc-fpga/
smartfusion
8. Microsemi Application Note AC346 SmartFusion cSoC: Loading and Booting from External
Memories
9. Dassen, J.H.M.: Constructors and Destructors in C++. http://l4u-00.jinr.ru/usoft/WWW/
wwwdebian.org/Documentation/elf/node4.html
10. Rob Williams Section C++ global constructors and destructors in ELF in book Real-Time
Systems Development (2006)
11. Master thesis: RODOS in a Multicore environment by Tiago Lus Gonalves Duarte, University
of Minho in coordination with Julius–Maximilians University Wrzburg (2013)
12. Master thesis: Design and Implementation of Multi-core Support for an Embedded Real-time
Operating System for Space Applications by Wei Zhang, KTH Royal Institute of Technology,
May 2015
13. Actel SmartFusion MSS Timer Driver User’s Guide Version 2.0
144
M. Faisal and S. Montenegro

14. Contribution of Miroslaw Sulejczak and by Michael Ruﬀer for the context switching for
ARMcore in RODOS
15. Choi, H., Park, S.: Evaluations of hardware and software-based context switching methods
in Cortex-M3 for embedded applications. Int. J. Smart Home 9(2), 111–122 (2015)
16. Lauterbach Development Tools. http://www.lauterbach.com/frames.html?home.htm
Porting RODOS on a Customizable System-on-Chip
145

A General Game-Theoretic Approach
to Harmonization the Values of Project
Stakeholders
Tigran G. Grigorian(&), Sergey D. Titov, Anatoliy Y. Gayda,
and Vladimir K. Koshkin
National Shipbuilding University, Geroev Ukrainy Ave., 9, Mykolaiv, Ukraine
grigorian.tigran@gmail.com, {ss1-ss10,cetus}@ukr.net,
koshkin-vladimir@mail.ru
Abstract. The problem of project stakeholders values harmonization as a
solution of non-cooperative game between two players is stated. The concept of
the value balancing operation and the value harmonization process are pre-
sented. The alternative strategies of project team and stakeholders as players,
allowing to typify situations in real projects and reduce the variety of possible
behavior into fairly small amount of combinations are presented. The developed
model allows to obtain the recommendations for the use of pure and mixed
strategies aimed to maximization players’ values under different circumstancies
on the basis of the models of zero sum and bimatrix games. The models and
method presented allow to ensure the sustainability of project execution and
ﬁnalization. Further research tasks should be aimed at developing the means for
the increase the effectiveness of the payoff matrices building.
Keywords: Project management  Value  Stakeholders’ values
Value harmonization  Matrix games  Zero-Sum game  Bimatrix games
1
Introduction
The main project manager task is to ensure the implementation of the project and its
completion within the triangle of basic restrictions, taking into account the character-
istics of the environment. However, the concept of value-driven management is gaining
popularity, according to which the main task of the manager is to ensure the creation of
value in the form of project output and its delivery to stakeholders [1, 2]. This idea is
accentuated in IT-projects managed in accordance with the Agile and Lean method-
ologies [3–5]. Today the value becomes a key driver for project initiation, imple-
mentation and ﬁnalization. One of the most important tasks in ensuring of value
creation and delivering is its harmonization.
The reason to solve the value harmonization problem is a conﬂict of interests of a
systemic nature, which is almost inevitable in the process of project management, and
is due to the difference in stakeholder values and different perceptions of the situation
around the project and its output. Effectiveness and efﬁciency of project management
are directly related to the possibilities of forecasting such value conﬂicts, developing
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_11

scenarios for minimizing their negative impact on project implementation. The solution
of the problem is exacerbated by the unique character of the project and the turbulence
of its environment, which certainly affects the stakeholders. First of all harmonization
should be aimed at eliminating these conﬂicts, caused by the discrepancy of value
expectations and the perception of the product by stakeholders. The goal of stake-
holders’ value harmonization is to ensure the support and participation of stakeholders
during project implementation and, ultimately, the adoption of a product, aimed at
creating and delivering value. Thus, there is a need for models that will help us to
predict the inﬂuence of value conﬂicts in projects and develop decision support when
choosing the most effective models of manager behavior in certain situations.
2
Literature Review
A considerable amount of scientiﬁc work has been devoted to the study of value. It
must be noted that it is possible to allocate conditionally researches of values of
conceptual and descriptive types. The ﬁrst include the fundamental works of Rokich
[6], Schwartz [7], Hofstede [8]. Special works in the ﬁeld of project management are
devoted to studying values of the second type. The issues of value management in
project management are generalized and consistently set out in the international stan-
dard P2M [9]. Besides considerable attention to the value is paid in the basic principles
and practical recommendations of Agile [3, 4]. Kerzner and Saladis also point to the
need to eliminate the value conﬂict, noting that balancing the needs of stakeholders
becomes especially difﬁcult due to the internationalization of projects – it is necessary
to take cultural, ethical, religious and other factors into consideration [1]. And this also
conﬁrms the advisability of applying “soft” management techniques in solving the
problem of harmonization. They identify 6 types of conﬂicts by pair combinations of 4
groups of stakeholders, and indicate that balancing aimed at eliminating conﬂicts is an
extremely difﬁcult task. However, they do not provide any speciﬁc recommendations
for its solving.
In [2] the model of value harmonization, which allows assessing the stability of the
organization and facilitates the analysis of alternatives when choosing strategies for the
development of ﬁnancial institutions is presented. However, this model is used to
harmonize the values of the development of organizations not projects themselves, and,
moreover, does not sufﬁciently take into account the dynamic nature of the values
themselves. In addition, the application of various approaches aimed at harmonizing
the values proposed and used in general management cannot be applied to project
management because of uniqueness [1].
In general, two problems of value harmonization are declared and being solved in
project management [1, 10, 11]: (a) harmonization in accordance with the enterprise
development strategy, which can be reduced to ranking projects (subprojects) and
giving preferences to those that are more in line with the strategic values of the
organization; (b) harmonization between project stakeholder values, which is the sort of
balancing operation. The ﬁrst problem is successfully solved with the help of verbal
decision analysis methods [12]. This method and corresponding models show good
results in various applied ﬁelds, including management of project portfolios in the
A General Game-Theoretic Approach to Harmonization
147

nuclear power industry and municipal administration, decision-making in the man-
agement of outsourcing project teams, scope and schedule management in Agile
projects, etc. [13–15]. In contrast, in the ﬁeld of solving the problem of harmonization
of stakeholder values, there is a clear lack of research due to the complexity of this
problem, related to the following features: the uniqueness of the project, which sig-
niﬁcantly complicates the development of models, the logic of which is based on
precedents; the turbulence of project environment and, as a result, the high dynamics of
stakeholders’ evaluations; the subjective nature of value, complicating the processes of
its identiﬁcation, systematization and evaluation for making project decisions; the
insufﬁcient level of development of the methodological basis for value forecasting, due
to the lack of research aimed at developing models and methods for of value man-
agement. Another feature that signiﬁcantly complicates the solution of value harmo-
nization is the presence of a “soft” component due to the need to work with the
stakeholders of the project. In the works listed above and other studies, there are not
enough solutions aimed at balancing the value to maximize the effectiveness of pro-
jects. Thus, models and tools that will allow to harmonize project works in accordance
with stakeholder values and make decisions aimed at maximizing the satisfaction of
expectations through creating value in the project and managing its delivery to
stakeholders. The elements of the game-theoretic approach to harmonization the values
of project stakeholders on the ground of bimatrix games are considered in [16].
3
The Purpose of the Study
The aim of the article is to develop the models of decision support in the planning and
management of project works aimed at harmonization in accordance with the values of
stakeholders to improve the efﬁciency and effectiveness of project execution. To
achieve this goal, it is necessary to solve a number of tasks, the most important of
which are: the disclosure of the essence of harmonization of stakeholder values, the
choice of methods and the development of a harmonization model, and the deﬁnition of
logic and scenarios for its application.
4
The Research Method
The fundamental research of the nature of value is based on philosophical concepts and
psychological methods and models owing to the features of value. From the viewpoint
of socio-technical systems to which project management belongs, value management
models based on social methods for interviewing, data structuring and analysis,
statistical models etc.
In general, the concept of harmonization is related to the notion of balance (from fr.
balance – balance), deﬁned through an equilibrium, by which is meant the stable state
in which the body is located under the inﬂuence of static or dynamic forces or actions
[17]. In the context of this study, under the harmonization of stakeholders’ values, we
mean a set of actions that lead to an increase in one or more values for one or more
project stakeholders at a given time. Since stakeholders’ attitude can be either positive
148
T. G. Grigorian et al.

or negative, on the one hand, we have a certain unity of opinions and value relations to
the project and its output (cooperativeness), and on the other hand – the difference of
stakeholders’ views caused by their different value orientations (antagonism).
There are a signiﬁcant number of approaches, methods and models of the analysis
and management behavior in conﬂict situations. One of the approaches that show
successful results in solving the problem of conﬂict harmonization is game theory. Its
foundations are made in the fundamental work of Von Neumann and Morgenstern [18].
The use of game theory makes it possible to predict and choose the most effective
strategies in conﬂict situations. There are hundreds of different examples of conﬂict
situations, effectively modeled and managed on the basis of game theory. The choice of
a particular model is of fundamental importance for achieving the stated goal –
ensuring the efﬁciency and effectiveness of project management. And the concept of
Nash equilibrium plays a great role in solving this problem.
5
Main Research Material
A general expression for estimating the product value changes by a set of bal-
ancing operations. In the issues of value harmonization in projects, it is necessary to
take into account that, as it usually applied to the value, its reduction is inappropriate.
This reduces the attractiveness of the project output for stakeholders and adversely
affects their participation in the project and, as a result, on its progress and compliance
with the requirements of the project management triangle [19]. Therefore, we assume
that any set of k value-balancing operations should ultimately increase the total amount
of value Bk of project output:
Bk ¼
X
K
k¼1
zj  vij  Dbij [ 0
ð1Þ
where Dbij – the increment of i-th value in the evaluation made by j-th stakeholder, vij
– the evaluation of i-th value made by j-th stakeholder, zj – power (“weight”) of j-th
stakeholder, i = 1 … I, I – the number of values chosen, j = 1 … J, J – number of
stakeholders, K – total number of value-balancing operations.
In general, any elementary operation of value balancing can be aimed at increasing
the value of an output through a change in it or in a stakeholder relation, the evaluations
of which are analyzed. When changing an output, it is important to understand that the
task of increasing value at the initial stages leads to changing the product model from
the point of stakeholders views, and after delivering the minimum viable product to
further modiﬁcation of a product itself [5, 20].
The concept of project stakeholders’ value harmonization. The uniqueness of the
project management is determined by its nature: the features of a project, the charac-
teristics of its team, the turbulence of the environment and other factors. This, in turn,
exerts in the uniqueness of each project situation, which inﬂuences making appropriate
decisions.
A General Game-Theoretic Approach to Harmonization
149

The development of models for stakeholders’ values harmonization should take into
account key features of the value and processes of its management, which inﬂuence the
approach to balancing it. Various project stakeholders, driven by their values, relate
differently to project and its output. And for the convenience of obtaining and evalu-
ating information on project stakeholders’ preferences they need to be grouped on
different grounds during project analysis. According to the recommendations of
PMBoK, most often they use the attitude towards the project and its output. This
attitude can be positive or negative; however, it is present in any case. It is by the
presence of different attitude that we include the person in some group of project
stakeholders.
Rational structuring of project situations allows to develop typical form of analyzed
value conﬂict situations and to reduce the inﬂuence of project uniqueness in
decision-making on the basis of game theory. As a result, the application of this method
makes it possible to reduce the variety of possible situations in value-driven balancing
in project management into fairly small amount of combinations and thus to level out
the impact of the uniqueness of situations in project management.
Taking into account the project management features (uniqueness, turbulence,
subjectivity), it is most expedient to use harmonization of project works in accordance
with stakeholders’ values on the basis of non-cooperative matrix games theory. It gives
an opportunity to treat the absence of strict opposition in stakeholder interests while
balancing value in projects (i.e. the conﬂict is not strictly antagonistic).
The preservation of strategies leading to a state of equilibrium provides an oppor-
tunity to balance the interests of stakeholders and thus ensure the continuity of project
management aimed at the project implementation and completion, with the subsequent
product delivery to the customer. The strength of this approach is the ability to obtain
on its basis recommendations about the use of particular strategies by conﬂict partic-
ipants. Thus, the harmonization of stakeholder’ values can be reduced to a ﬁnal
non-cooperative matrix game of two players.
The model of the game problem solution fot the project stakeholders’ values harmo-
nization in general form. Players A and B have m and n strategies respectively: A1, A2,
…, Am and B1, B2, …, Bn. In general case payoffs of players A and B are given by
matrices A and B consequently:
A ¼
a11
a12
. . .
a1n
a21
a22
. . .
a2n
. . .
. . .
. . .
. . .
am1
am2
. . .
amn
0
B
B
@
1
C
C
A; B ¼
b11
b12
. . .
b1n
b21
b22
. . .
b2n
. . .
. . .
. . .
. . .
bm1
bm2
. . .
bmn
0
B
B
@
1
C
C
A
ð2Þ
If player A applies his strategy ai and player B uses strategy bj, then the players’
payoffs will be in the appropriate intersection of row i and column j in the payoff
matrices: aij for the A and bij – for B. From the viewpoint of balancing the stakeholders’
values, the player’s payoff is nothing more than an increment in value for the stake-
holder group deﬁned in (1).
In accordance with the theorem of Nash, any matrix game has at least one equi-
librium in pure or mixed strategy [21, 22]. The identiﬁcation of an equilibrium is
150
T. G. Grigorian et al.

important, since it allows to determine whether there exists such a combination of
strategies of players, the deviation from which does not lead to an payoff increase,
provided that the second player retains his choice [21–23]. Therefore, the main result of
solving the matrix game model shows pairs of player’ pure (i#, j; i, j#) and mixed
(x, y) strategies, which leads to a stable state of the project management system and
guarantees the highest game values mA and mB for both players A and B respectively.
Therefore, in general the problem of project stakeholders’ values harmonization can be
reduced to the linear programming problem on the polyhedral set:
W ! opt; fx; y 2 Xg:
ð3Þ
The conceptual model of project stakeholders’ values harmonization on the basis of
matrix games models is presented in Fig. 1.
There are ﬁve logical levels in the model (project, gamers, strategies, combinations
of gamers’ strategies and recommended strategies) that describe the concept and are
combined by the single hierarchy. This general approach gives an opportunity to
represent, interpret and solve various tasks of value harmonization for any project. In
every particular case this generalized model can be clariﬁed taking into account the
features of a project and harmonization purpose so as it will be discussed below.
The structure of project stakeholders’ values. Based on the generalization of value
analysis research, the grounds for determining the structure of the values balanced in
projects are identiﬁed. The basis of this set is basic human values, developed by
Schwartz and Hofstede’s cultural dimensions. Their combination allows us to com-
prehensively determine the project values for each individual. These values overlap
with the value structures proposed in P2M, PMBoK, Agile principles. However, the
latter complement their speciﬁc values associated with the project activities. To solve
the task of balancing the values of project stakeholders, two players are singled out
(conﬂict parties – a project manager with his team and a sponsor with customers of a
product), whose interaction determines the efﬁciency and dynamics of the project
implementation. Despite the fact that in general, the values and interests of a project
manager and team members do not coincide, in this solution they are grouped, as there
is a proximity of interests. In addition, a project manager has a certain amount of
power, allowing him to persuade team members to his side. Based on these reasons, the
ﬁnal structure of values determined for each of the players is developed and presented
in Tables 1 and 2.
The developed internal structure of value allows to make decisions aimed at pri-
oritizing and balancing project works according to evaluation of project stakeholders’
values.
The general strategies of projectstakeholders’ value balancing participant. To select
the type of game model, we will consider the features of the value conﬂict in project
management by the example of constructing a bimatrix game model for two players
corresponding to the project manager and his team (player A) and stakeholders groups
(player B). Then, in accordance with the general logic of behavior in a conﬂict and
taking into account the features of project management, for player A, such behavior
A General Game-Theoretic Approach to Harmonization
151

strategies can change the levels of value estimations from stakeholders’ viewpoint
(Fig. 2a):
– change product A1;
– change the attitude (perception) of stakeholders A2;
– change manager and team attitude A3;
– change product and the stakeholders’ perception (the combination of A1 and A2);
– change product and manager and team attitude (A1 and A3);
– change stakeholders’ perception and manager and team attitude (A2 and A3).
Bn
Gamers Level
Strategies 
Level
Combinatorial 
space 
of payoff matrices
Recommended 
(balanced) 
player strategies
Gamer A
Gamer B
1
A
…
B1
…
(aij,bij)
(i#j ,ij#)
(vA,vB)
(x ,y)
Pure 
strategies
Mixed 
strategies
Payoffs
Combination
s  
of player
strategies
Project Level
m
A
PROJECT
2
A
B2
Fig. 1. The conceptual model of value harmonization in project.
152
T. G. Grigorian et al.

Accordingly, project stakeholders can use the following strategies (Fig. 2b):
– active promotion of the product creation B1;
– positive attitude to the project and product B2;
– neutral attitude B3;
– negative attitude to the project and product B4;
– active resistance to the project and product B5.
The combination of these strategies makes it possible to simulate any project situ-
ation and gives grounds for the use of game models because of characterization and
repeatability. The variants of strategy combinations for the behavior of players are
determined by conditions of particular project environment: stakeholders’ relations,
experience and knowledge of a project manager, the standards of a company or a team,
the requirements declared in the project charter, etc. At the same time, the stakeholders
are grouped in accordance with their attitude to the project and its product, taking into
account the level of power and inﬂuence of each side on the recommendations of
PMBoK as it is shown in (1).
Table 1. The structure of player A (project manager and team) values
Values
Motivational goals
Value
type
Autonomy
Independence of thinking and action
Social
Engagement
Passion, novelty and life challenges
Social
Agility
Personal
Comfort
Pleasure and sensual (aesthetic) enjoyment
Personal
Realization of personal
strategy
Personal success, based on competence and social
standards
Personal
Safety
Security and harmony of relationships and myself
Personal
Ethics
Self-restraint in actions that could harm or violate
social norms
Social
Standards
implementation
Improvement the quality of project management
standards implementation
Personal
Table 2. The structure of player B (sponsor and stakeholders) values
Values
Motivational goals
Value
type
Competence
Independence of thinking and action
Personal
Engagement
Passion, novelty and life challenges
Personal
Achievements
Personal success, based on social standards
Personal
Wealth
Abundance of material and intangible assets
Personal
Power
Social status and prestige, control and domination over people
and resources
Social
Safety
Security and harmony of relationships and myself
Personal
Communication
Interaction oriented to the effective problem solution
Social
Sustainability
Acceptance and respect for traditional ideas of society, culture
and religion
Social
A General Game-Theoretic Approach to Harmonization
153

The model of project stakeholders’ value harmonization as a solution of the
zero-sum game. The game theory is closely related to linear optimization. It is known,
that each zero-sum game of two players A and B (winnings of player A are equal to
losses of player B and conversely) can be represented in the form of a linear opti-
mization task.
The mathematical model for solving the primal problem of harmonizing project
stakeholders’ values a as zero-sum game. Let A ¼ ½aijmn denote the payoff matrix
(2) of the ﬁrst player A. According to the general problem (3) the goal of the ﬁrst player
or objective function WI is to maximize his winning – the game value m:
WI ¼ m ! max
ð4Þ
In the game with payoff matrix A ¼ ½aijmn the strategies of A player are given by
an ordered set of x probabilities (frequencies) or m-dimensional vector. In case of
Active promotion
of the product
creation
B1
Positive
attitude to
the product
B2
Neutral
attitude to
the product
B3
Negative
attitude to
the product
B4
Active resistance
to the product
creation
B5
b) The strategies of player B
(sponsor and stakeholders)
Change
product
A1
Change
stakeholders’
perception
A2
Change
personal
attitude
A3
Change product
and stakeholders 
(A1+A2)
A4
Change yourself
and product
(A1+A3)
A5
Change yourself
and stakeholders
(A2+A3)
A6
a) The strategies of player A
(manager and project team)
Fig. 2. The strategies of value balancing game players A and B.
154
T. G. Grigorian et al.

multiple game repetition player A have to implement its set of pure strategies
½A1; A2; . . .Am with the values corresponding to x. Such a behavior of player A guar-
antees that game value m will not be reduced. A vector x ¼ ½x1; x2; . . .xm satisﬁes the
conditions:
X
m
i¼1
xi ¼ 1; xi  0; i ¼ 1; . . .; m:
ð5Þ
The player A have to ensure payoffs not less than the game value m while using pure
strategies ½A1; A2; . . .Am with different frequencies x ¼ ½x1; x2; . . .xm according to the
payoff matrix A ¼ ½aijmn. The constraint system of the classical linear optimization
task (polyhedron XI) in this case has a form:
XI :
a11x1 þ a21x2 þ . . . þ am1xm  m;
a12x1 þ a22x2 þ . . . þ am2xm  m;
                             
a1nx1 þ a2nx2 þ . . . þ amnxm  m;
x1 þ x2 þ . . .xm ¼ 1;
8
>
>
>
>
<
>
>
>
>
:
ð6Þ
xi  0; i ¼ 1; . . .; m:
This task is usually called the primal problem. The solution of this problem gives us
the frequencies x ¼ ½x1; x2; . . .xm and the game value m for the ﬁrst player A.
The mathematical model for solving the dual problem of harmonizing project stake-
holders’ values a as zero-sum game. The payoff matrix (2) for the second player B is
also A ¼ ½aijmn. The goal of the second player B or objective function WII is to
minimize his loss that is equal to the game value m:
WII ¼ m ! min
The strategies of the player B are given by an ordered set y of probabilities (frequencies)
or n-dimensional vector. In case of multiple game repetition player B have to implement its
set of pure strategies ½B1; B2; . . .Bn with the values corresponding to y. Such a behavior
of player B guarantees that his loss m will not be increased. The vector y ¼ ½y1; y2; . . .yn
satisﬁes the conditions:
X
n
j¼1
yj ¼ 1; yj  0; j ¼ 1; . . .; n:
The player B have to ensure losses not more than the game value m while using pure
strategies ½B1; B2; . . .Bm with different frequencies y ¼ ½y1; y2; . . .yn according to the
payoff matrix A. The constraint system of the classical linear optimization task
(polyhedron XI) in this case has a form:
A General Game-Theoretic Approach to Harmonization
155

XII :
a11y1 þ a21y2 þ . . . þ a1nxn  m;
a21y1 þ a22y2 þ . . . þ a2nxn  m;
                             
an1y1 þ an2y2 þ . . . þ amnyn  m;
y1 þ y2 þ . . .yn ¼ 1;
8
>
>
>
>
<
>
>
>
>
:
yj  0; j ¼ 1; . . .; n:
The task presented is usually called the dual problem. The solution of this problem
gives us the frequencies y ¼ ½y1; y2; . . .yn and the game value m for the second player B.
The features of the choice and application of strategies in gaming models in the project
management tasks. It is common to distinguish between solutions in pure or mixed
strategies for the zero-sum matrix game. The solution in pure strategies is always
associated with at least one saddle point – the equilibrium point. The saddle point is
deﬁned after calculating upper and lower values of a game. The lower game value a is
lowest guaranteed winning of the player A:
a ¼ max
i
min
j ðaijÞ
You should select the smallest values for each payoff matrix row (the guaranteed
winning of the ﬁrst player A) minðaijÞ ¼ ½a1; a2; . . .am for calculating this expression.
The second step is to choose the largest one among selected guaranteed winnings
max
i
min
j ðaijÞ. The upper game price b is the lowest guaranteed loss of the second
player B:
b ¼ min
j
max
i ðaijÞ
You should select the largest values for each payoff matrix column maxðaijÞ ¼
½b1; b2; . . .bn for calculating this expression. The next step is to choose the smallest
loss among the selected losses min
j
max
i ðaijÞ.
In case of the coincidence of numerical values a ¼ b we have a solution in pure
strategies. The corresponding strategies Ak, Bl are called optimal and are recommended
for the prior use by gamers
Ak ¼ Aopt; Bl ¼ Bopt:
In this case the game value is m ¼ akl.
The very point with the position ðk; lÞ in payoff matrix is the equilibrium point. Note
that the zero-sum game could have several equilibrium points, but they all have the
same numerical value.
It is known that a  b. In case of obtaining different values for a and b, we have a
solution in so-called mixed strategies x ¼ ½x1; x2; . . .xm of player A and y ¼
½y1; y2; . . .yn of player B. The situations ðx; yÞ allows us to calculate the game value on
156
T. G. Grigorian et al.

the basis of multiple game repetitions as a mathematical expectation Mðx; yÞ of a
random process ðx; yÞ with discrete random variables in a form of payoff matrix A that
is calculated by formula
Mðx; yÞ ¼
X
m
i¼1
X
n
j¼1
aijxiyj ¼ m:
This number is the mean expected value that is equal to the game value m. It must be
pointed out that
a  ¼
X
m
i¼1
X
n
j¼1
aijxiyj  b:
The presence of at least one zero value among mixed strategies known as the
degenerate solution. It is well known that the largest number of non-zero values in a
situation ðx; yÞ is equal to the payoff matrix A rank (the number of linearly independent
rows or columns) [24, 25]. Since
rang ðAmnÞ  minðm; nÞ;
this maximum number will be equal to the maximum of the smallest measurability of
payoff matrix. With this in mind, the question arises as to the usefulness of further
research of non-square payoff matrices from the viewpoint of mixed strategies exis-
tence. However, from a practical point of view, we can make a conclusion that it is
more proﬁtable for a player to have more strategies since it gives additional agility to a
project manager.
The application of the model for project stakeholders’ value balancing while solving
the primal problem in a zero-sum game. Let’s suppose that for a project the payoff
matrix describing the strategies presented in Fig. 3 and value estimations of the
stakeholders has the following form:
A ¼
1
7
9
6
3
9
1
1
2
1
6
5
2
7
5
8
7
1
4
1
5
1
8
5
5
5
5
4
1
8
0
B
B
B
B
B
B
@
1
C
C
C
C
C
C
A
The general primal optimization problem for player A stated in (4–6) is as follows:
WI ¼ m ! max
A General Game-Theoretic Approach to Harmonization
157

x1 þ 9x2 þ 6x3 þ 8x4 þ 5x5 þ 5x6  v;
7x1 þ x2 þ 5x3 þ 7x4 þ x5 þ 5x6  v;
9x1 þ x2 þ 2x3 þ x4 þ 8x5 þ 4x6  v;
6x1 þ 2x2 þ 7x3 þ 4x4 þ 5x5 þ x6  v;
3x1 þ x2 þ 5x3 þ x4 þ 5x5 þ 8x6  v;
x1 þ x2 þ x3 þ x4 þ x5 þ x6 ¼ 1;
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
xi  0; i ¼ 1; . . .; 6:
The lower and upper game values are a ¼ 2; b ¼ 7: The problem has a solution in
mixed strategies:
x ¼ ð0:19; 0; 0:30; 0:10; 0:19; 0:22Þ; y ¼ ð0:33; 0:21; 0:23; 0:09; 0:14Þ; m ¼ 4:8:
The decision warranties the following distribution (frequencies) of application of
mixed strategies in order to ensure stakeholders’ values harmonization (Fig. 3).
It is necessary to pay attention that the strategy A2 is degenerate and therefore does
not fall into the list of strategies recommended for player A.
The presented model is implemented in the Maple® – mathematical software
environment. The source code fragment associated with the primal problem solving is
presented below.
A1, 
19%
A3, 
30%
A4, 
10%
A5, 
19%
A6, 
22%
B1, 
33%
B2, 
21%
B3, 
23%
B4, 
9%
B5, 
14%
a) Gamer A frequencies
b) Gamer B frequencies
Fig. 3. The recommended frequencies of the mixed strategies use in zero-sum game WI.
158
T. G. Grigorian et al.

The given solution of the primal problem of harmonizing project stakeholders’
values a as zero-sum game of two players have a dual solution the analysis of which
lies outside of this study.
The model of project stakeholders’ value harmonization as a solution of the
bimatrix game. Let’s consider at a game in which two players A and B have their own
non-antagonistic interests. Each player chooses an independent strategy of behavior.
Player A has its own behavior strategies ½A1; A2; . . .Am and player B ½B1; B2; . . .Bn as
well.
The fundamental issue of bimatrix game theory is that in the case of choosing i-
strategy by the player A and j-strategy by the player B both players get different values
aij and bij. That is, unlike the matrix game, where bij ¼ aij each bimatrix game player
obtains personal subjective value. The brute-force search of combinatorial space in a
bimatrix game is given by two payoff matrices A and B (2).
A General Game-Theoretic Approach to Harmonization
159

For bimatrix games there can be solutions in both pure and mixed strategies. The
solution in pure strategies is related with the Nash equilibrium point respective to the
payoff matrix element ði#; j#Þ when the following conditions are met:
ai j#  ai#j#; i ¼ 1; . . .; m; bi#j  bi#j#; j ¼ 1; . . .; n:
ð7Þ
The optimization approach can be used for calculating A player mixed strategies
x ¼ ½x1; x2; . . .xm. The optimization task in this case has a form:
WA
I ¼ mA ! max
ð8Þ
XA
I :
a11x1 þ a21x2 þ . . . þ am1xm  m;
a12x1 þ a22x2 þ . . . þ a2mxm  m;
                             
am1x1 þ am2x2 þ . . . þ amnxm  m;
x1 þ x2 þ . . .xm ¼ 1;
8
>
>
>
>
<
>
>
>
>
:
ð9Þ
xi  0; i ¼ 1; . . .; m:
ð10Þ
The solution of this task gives us the frequencies x ¼ ½x1; x2; . . .xm and game value
mA for the ﬁrst player A. We also can use an optimization approach for deﬁnition the
mixed strategies y ¼ ½y1; y2; . . .yn of player B. The optimization task has the form:
WB
I ¼ mB ! max;
ð11Þ
XB
I :
b11y1 þ b21y2 þ . . . þ bm1yn  m;
b12y1 þ b22y2 þ . . . þ bm2yn  m;
                             
bm1y1 þ bm2y2 þ . . . þ bmnyn  m;
y1 þ y2 þ . . .yn ¼ 1;
8
>
>
>
>
<
>
>
>
>
:
ð12Þ
yj  0; j ¼ 1; . . .; n:
ð13Þ
The solution of the task gives us the frequencies y ¼ ½y1; y2; . . .yn and game value
mB for the second player B.
Combining calculations for pure and mixed strategies provides a comprehensive
vision of gamers’ optimal behavior.
The application of the model for project stakeholders’ value balancing while solving
the problem in a bimatrix game. Let’s suppose that for a project the payoff matrices
describing the strategies presented in Fig. 2 and value estimations of two players
A (project manager and his team) and B (project stakeholders) have the following
forms:
160
T. G. Grigorian et al.

A ¼
4
6
6
1
7
8
2
6
5
6
4
6
3
9
7
3
4
9
9
5
3
9
5
6
1
0
B
B
B
B
@
1
C
C
C
C
A
; B ¼
2
8
2
1
5
2
9
1
6
3
2
5
5
2
6
5
4
1
8
5
8
2
8
6
5
0
B
B
B
B
@
1
C
C
C
C
A
;
The general optimization task for player A in accordance with (8–10) is as follows:
WA
I ¼ mA ! max;
XA
I :
4x1 þ 8x2 þ 4x3 þ 3x4 þ 3x5  vA;
6x1 þ 2x2 þ 6x3 þ 4x4 þ 9x5  vA;
6x1 þ 6x2 þ 3x3 þ 9x4 þ 5x5  vA;
x1 þ 5x2 þ 9x3 þ 9x4 þ 6x5  vA;
7x1 þ 6x2 þ 7x3 þ 5x4 þ x5  vA;
x1 þ x2 þ x3 þ x4 þ x5 ¼ 1;
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
xi  0; i ¼ 1; . . .; 5:
The lower and upper game values for player A are aA ¼ 3; bA ¼ 7; and for player
B
aB ¼ 2; bB ¼ 6:
The
task
has
a
solution
in
mixed
strategies:
xA ¼
ð0:20; 0:37; 0:19; 0:01; 0:23Þ; yB ¼ ð0:08; 0:27; 0:08; 0:14; 0:43Þ; and game val-
ues mA ¼ 5:2; mB ¼ 4:8:
The solution obtained warranties the presence of equilibrium for the payoff matrices
at which the balance of the value interests of players A and B subject provided by
applying these strategies with given probability distributions (Fig. 4).
 
a) Gamer A frequencies 
b) Gamer B frequencies 
A1, 
20%
A2, 
37%
A3, 
19%
A4, 
1%
A5, 
23%
B1, 
33%
B2, 
21%
B3, 
23%
B4, 
9%
B5, 
14%
Fig. 4. The recommended frequencies of the mixed strategies use in bimatrix game WA
I .
A General Game-Theoretic Approach to Harmonization
161

The general optimization task for player B in accordance with (11–13) is as follows:
WB
I ¼ mB ! max;
XB
I :
2y1 þ 8y2 þ 2y3 þ y4 þ 5y5  vB;
2y1 þ 9y2 þ y3 þ 6y4 þ 3y5  vB;
2y1 þ 5y2 þ 5y3 þ 2y4 þ 6y5  vB;
5y1 þ 4y2 þ y3 þ 8y4 þ 5y5  vB;
8y1 þ 2y2 þ 8y3 þ 6y4 þ 5y5  vB;
y1 þ y2 þ y3 þ y4 þ y5 ¼ 1;
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
yj  0; j ¼ 1; . . .; 5:
The task has a solution in mixed strategies: xB ¼ ð0:14; 0:19; 0:17; 0:08; 0:42Þ;
yA ¼ ð0:28; 0:31; 0:20; 0:07; 0:14Þ; and game values mA ¼ 4:8; mB ¼ 5:2: The fre-
quencies of the gamers’ mixed strategies use for balancing value of the players A and
B subject is shown on Fig. 5.
The equilibrium in pure strategies is determined according to (7) for these payoff
matrices by two situations:
(1) for i# = 3 and j# = 5 – x ¼ ð0; 0; 1; 0; 0Þ; y ¼ ð0; 0; 0; 0; 1Þ; mA ¼ 7; mB ¼ 6:
(2) for i# = 4 and j# = 4 – x ¼ ð0; 0; 0; 1; 0Þ; y ¼ ð0; 0; 0; 1; 0Þ; mA ¼ 9; mB ¼ 8:
Each equilibrium situation is characterized by its payoffs. In the 2 solution
ði#; j#Þ ¼ ð4; 4Þ the equilibrium is characterized by a greater beneﬁt for project team
(player A). It is advisable to adhere the second pair of strategies during periods of stable
operation and while project is being implemented within the triple constraints. The
1solution should be used at the project milestones or during the period of deviations
from the project base plan or in the circumstances of stakeholder dissatisfaction. This is
one of the possibilities of project “soft” management based on the bimatrix game
applications.
A
a) Gamer
frequencies
b) Gamer B frequencies 
A1, 
14%
A2, 
19%
A3, 
17%
A4, 
8%
A5, 
42%
B1, 
33%
B2, 
21%
B3, 
23%
B4, 
9%
B5, 
14%
Fig. 5. The recommended frequencies of the mixed strategies use in bimatrix game WB
I .
162
T. G. Grigorian et al.

The presented model is implemented in the Maple® environment. The source code
fragment associated with the search for mixed strategies is presented below.
A General Game-Theoretic Approach to Harmonization
163

The joint application of the equilibrium conditions obtained through the use of the
model proposed and software developed makes it possible to solve various problems of
value-driven project management. The pure strategies allow to choose the logic of
behavior that ensures the most stable implementation of the project. And mixed
strategies help to control the balance of forces and contributions of stakeholder groups
to different attitudes to a project and its product.
6
Conclusions
The proposed game-theoretic approach to modeling the processes of value-driven
conﬂict management in projects, based on the use of zero-sum and bimatrix games,
allows to balance the execution of project works taking into account the values of
project stakeholders. The use of the apparatus of matrix games while solving the
problem of project’s work harmonization in accordance with the values of stakeholders
allows to “softly” manage project implementation processes at different stages. The
proposed method allows to form a general model of a typical situation of value conﬂict
that can be adapted to conditions of any particular project and used to the analysis and
adoption of appropriate management decisions. Further research should to be directed
to the analysis of the features of decision making and to increase the effectiveness of
determining the stakeholders’ values for the payoff matrices building.
References
1. Kerzner, H., Saladis, F.P.: Value-Driven Project Management, p. 281. Wiley, New-York
(2009)
2. Bushuev, S.D., Bushueva, N.S., Yaroshenko, R.F.: Model garmonizatsii tsennostey̆
programm razvitiya organizatsiĭ v usloviyakh turbulentnosti okruzheniya [The Model of
Value Harmonization for Program of Organization Development in Turbulent Environment].
Upravlinnia rozvytkom skladnykh system. [The Management of Complex Systems
Development]. Kyiv, vol. 10, pp. 9–13. KNUBA Publisher (2012)
3. Principles behind the Agile Manifesto. Manifesto for Agile Software Development (2001).
http://agilemanifesto.org/principles.html. Accessed 10 Dec 2016
4. Turner, M.: Microsoft Solutions Framework Essentials: Building Successful Technology
Solutions. Microsoft Press (2006)
5. Rice, E.: The Lean Startup: How Today’s Entrepreneurs Use Continuous Innovation to
Create Radically Successful Businesses, p. 336. Crown Business, New York (2011)
6. Rokeach, M.: The Nature of Human Values. Free Press, New York (1973)
7. Schwartz, S.: Basic Human Values: An Overview. The Hebrew University of Jerusalem
(2004)
8. Hofstede, G.: Dimensionalizing Cultures: The Hofstede Model in Context. Universities of
Maastricht and Tilburg (2011)
9. A Guidebook of Project & Program Management for Enterprise Innovation. PMJA (2005).
http://www.pmaj.or.jp/ENG/P2M_Download/P2MGuidebookVolume1_060112.pdf. Acces-
sed 10 Dec 2016
164
T. G. Grigorian et al.

10. Rach, V.A.: Tsinnist’ yak bazova katehoriya suchasnoyi metodolohiyi upravlinnya
proektamy [The value of a basic category of modern project management methodology].
In: Proceedings of VII international conference “Upravlinnya proektamy v rozvytku
suspil’stva” [Project management in social development], Kyiv, KNUBA Publisher,
pp. 167–168 (2010)
11. Anshin, V.M.: Issledovanie metodologii i faktorov tsennostno-orientirovannogo upravleniya
proektami v rossiyskih kompaniyah [The Research of methodologies and factors of
value-driven project management in Russian companies] Upravlenie proektami i program-
mami [Project and Program Management]. Izd. Dom “Grebennikov”, vol. 2(38), pp. 104–
110 (2014)
12. Larichev, O.I.: Verbalnyiy analiz resheniy [Verbal Decision Analysis]. In-t sistemnogo
analiza RAN [System Analysis Institute of Russian Academy of Sciences]. Nauka, Moscow,
p. 181 (2006)
13. Grigorian, T.G., Koshkin, K.V.: Value-driven decision-making while choosing outsourcers
in the projects of municipal water supply systems reconstruction. In: Proceedings of the 2015
IEEE 8th International Conference on Intelligent Data Acquisition and Advanced
Computing Systems: Technology and Applications, IDAACS 2015, pp. 527–530
14. Grigorian, T.G., Koshkin, K.V.: Improved models of value-oriented managing portfolios of
projects for reconstruction of water supply. Eastern Eur. J. Enterp. Technol. 2/3(74), 43–49
(2015)
15. Grigorian,
T.G.,
Shatkovskiy,
L.Y.:
Modely
protsessov
prynyatyya
reshenyy
pry
tsennostno-oryentyrovannom upravlenyy trebovanyyamy v IT-proektakh [The models of
decision-making processes with value-driven requirement management in IT-projects].
Upravlinnya proektamy ta rozvytok vyrobnytstva [Project management and development of
production]. Lugansk, vol. 2(58), pp. 81–98 (2016)
16. Grigorian, T.G., Titov, S.D., Gayda, A.Y., Koshkin, V.K.: A game-theoretic approach to
harmonization the values of project stakeholders. In: Proceedings of the XII-th International
Scientiﬁc and Technical Conference: Computer Sciences and Information Technologies,
CSIT 2017, pp. 527–530 (2017)
17. Merriam-Webster Dictionary. https://www.merriam-webster.com. Accessed 1 Mar 2017
18. Von Neumann, J., Morgenstern, O.: Theory of Games and Economic Behavior. Princeton
University Press (1953)
19. A Guide to the Project Management Body of Knowledge (PMBOK® Guide), Fifth Edition,
PMI, p. 590 (2013)
20. Denne, M., Cleland-Huang, J.: Software by Numbers, Low-Risk, High-Return Develop-
ment. Prentice-Hall (2003)
21. Nash, J.: Non-cooperative Games. Ann. Math. 54(2), 286–295 (1951)
22. Vorobyev, N.N.: Situatsii ravnovesiya v bimatrichnyh igrah [The situation of equilibrium in
bimatrix games]. Teoriya veroyatnostey i ee primeneniya [Probability Theory and its
Applications], vol. 3, pp. 318–331. Nauka Publisher, Moscow (1958)
23. Vorobyev, N.N.: Beskoalitsionnyie igryi [Noncooperative games], p. 495. Nauka Publisher,
Moscow (1984)
24. Lau, D.: Algebra und Diskrete Mathematik 1. Grundbegriffe der Mathematik, Algebraische
Strukturen 1, Lineare Algebra und Analytische Geometrie, Numerische Algebra. Zweite,
korrigierte und erweiterte Auﬂage, p. 485. Springer, Berlin (2007)
25. Lax, P.D.: Linear Algebra and Application, 2nd edn, p. 377. Wiley, New York (2007)
A General Game-Theoretic Approach to Harmonization
165

Information Technology for Assurance of Veracity
of Quality Information in the Software Requirements
Speciﬁcation
Tetiana Hovorushchenko
(✉)
Khmelnitsky National University, Institutska Street, 11, Khmelnitsky 29016, Ukraine
tat_yana@ukr.net
Abstract. The aim of this study is the development of information technology
of evaluating the suﬃciency of quality information in the software requirements
speciﬁcation (SRS) for assurance of veracity of quality information in the SRS.
This study is also devoted to design and research of the subsystem of evaluating
the suﬃciency of the SRS information for software quality assessment based on
the comparative analysis of ontologies. The developed information technology
and subsystem provide: evaluating the suﬃciency of the SRS information for
software quality assessment by the standard ISO 25010:2011 and based on the
metric analysis; identifying the missing (in the SRS) measures and (or) indicators
(if the SRS information is insuﬃcient); prioritization of the addition of the missing
measures and (or) indicators in the SRS; quantify evaluating the veracity of the
available in the SRS information for software quality assessment; increasing the
veracity of the quality information in the SRS; increasing the software quality
assessment at the early lifecycle stages.
Keywords: Software · Software Requirements Speciﬁcation (SRS) · Software
quality · Software quality information · Suﬃciency of quality information ·
Ontology
1
Introduction
Today almost all spheres of human activity are connected with information systems, the
basis of which is software. A key factor in ensuring the eﬀective using of software
products and one of the main user requirements to modern software is to achieve high
values of its quality. The need to ensure the quality of software follows from the fact
that software bugs and failures threaten by catastrophes resulting in human casualties,
environmental disasters, signiﬁcant time and ﬁnancial losses.
As statistics [1–5] show, there are currently problems in the ﬁeld of software quality
assurance – the large projects are still performed with the lag of schedule or cost over‐
runs, the developed software products often lack the necessary functionality, their
performance is low, and the quality doesn’t suit consumers.
A large number of software bugs occurs at the stage of requirements formation and
formulation – these errors constitute 10–23% of all bugs, and the greater the size of
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_12

software, the more errors are made in the stage of requirements formation and formu‐
lation [2, 6]. The vast majority of software-related crashes occurred due to false require‐
ments, not because of coding bugs [2, 6]. The earlier the defect (bug, trouble, drawback,
malfunction) will be revealed, the cheaper it will cost its correction – the cost of
correcting the incorrect requirements of the speciﬁcation, discovered after the release
of the product, is almost 100 times the cost of correcting the defects of the speciﬁcation,
assumed in the process of formation and formulation requirements [4].
In the process of formating and formulating the requirements there are the informa‐
tion losses due to incomplete and diﬀerent understanding of the needs and context of
information – especially these losses are signiﬁcant for software projects that are devel‐
oped at the junction of subject domains (for example, software for medicine), when it
is necessary to consider as standards for development of software, and the standards of
the subject domain, for which software is being developed. It’s diﬃcult to implement
such standards, and it is even more diﬃcult to verify the degree of consideration of the
recommendations of these standards.
Software projects with incomplete requirements and speciﬁcations cannot be
successful [2]. Under such circumstances, the analysis of the SRS, the ability to “cut
oﬀ” the software projects with the incomplete (with insuﬃcient information) speciﬁca‐
tion is the actual and very important task. Suﬃciency of information is one of the most
important aspects of software quality assessment. The quality and success of the soft‐
ware project implementation signiﬁcantly depend on the SRS, and on the suﬃciency of
the SRS information (the presence of all the information elements, which are necessary
to the software quality assessment). The insuﬃciency, inaccuracy and distortion of the
SRS information lead, respectively, to a decrease in the veracity of software quality
assessments, as well as to increase the gap of knowledge about software that results in
unpredictable emergent properties of software systems.
Currently, the software quality evaluation by standard ISO 25010:2011 [7] is as
follows (Fig. 1) – the software quality is evaluated on the basis of the characteristics,
the characteristics are evaluated on the basis of subcharacteristics, the subcharacteristics
are evaluated on the basis of measures, that are described in ISO 25023:2016 [8]. Eval‐
uation of software quality and complexity based on the metric analysis is as follows
(Fig. 2) – the software quality and complexity are calculated on the basis of the metrics,
and the metrics are calculated on the basis of the indicators. For metric analysis, 14
quality metrics and 10 complexity metrics with exact or predicted values at the design
stage have been selected [9]. The software quality measures, the software quality and
complexity indicators, which are deﬁned in the SRS, constitute the quality information
of the SRS.
So, the suﬃciency of quality information in the SRS is the presence in the speciﬁcation
of all information elements (measures and indicators), which are necessary to the soft‐
ware quality assessment.
Today the evaluation of measures for the software quality subcharacteristics and
characteristics, indicators for the software quality and complexity metrics is conducted
only at the stage of the quality evaluation for the ready source code [5]. But the software
requirements determine the required characteristics of the software quality, and also
aﬀect the methods of quantitative evaluation of software quality [5]. So, the SRS have
Information Technology for Assurance of Veracity of Quality Information
167

all measures and indicators, which are needed to the subcharacteristics and metrics
calculation [5]. So the information suﬃciency for future software quality assessment
can be evaluated on the basis of the SRS. And if some measures or indicators are absent,
then the SRS has insuﬃcient information for software quality assessment and the devel‐
opers have to make the necessary adjustments in the SRS.
The conducted analysis of standards [7, 8] showed that they are presented in natural
language in the textual form, so there is no mechanism for veriﬁcation of the results of
the implementation of these standards in the software development process. It has been
established that quality information is conveniently presented as ontologies, which
provide the reﬂection of cause-eﬀect relationships between concepts.
The analysis of known ontological models in the ﬁeld of software engineering has
shown that, at present, the ontological models of proﬁle for software certiﬁcation [10],
ontological models of intelligent decision support systems [11], ontological models for
a single coherent underpinning for all ISO/IEC JTC1’s SC7 standards [12, 13], the model
of domain ontology for ISO/IEC 24744 [14] and the model of domain ontology in the
software analysis and reengineering tools [15] have been developed. But nowadays there
aren’t ontological models of software quality based on ISO 25010:2011, ontological
models of software quality and complexity based on metric analysis, and ontological
models of the SRS in terms of the availability of information for the software quality
assessment.
ISO 25023:2016
ISO 25010:2011
ISO 25010:2011
Software quality assessment
8 software quality characteristics
31 software quality subcharacteristics
203 software measures
Fig. 1. The modern concept of software quality assessment by ISO 25010:2011
Software quality and complexity assessment (based on 
the metric analysis)
14 software quality & 10 software complexity metrics with 
exact or predicted values at the design stage
39 software quality indicators & 33 software complexity 
indicators
Fig. 2. The modern concept of assessment of software quality and complexity based on the metric
analysis
168
T. Hovorushchenko

The analysis of known methods showed that the methods of software development
on the basis of ontological models of tasks [16], methods of formation of normative
proﬁle in the software certiﬁcation [10], ontological approach to speciﬁcation of prop‐
erties of software systems and their components [17], and methods of the SRS analysis
(Using natural language processing technique, Using CASE analysis method, QAW-
method, Using global analysis method, O’Brien’s approach, Method to discover missing
requirement elicitation, Selection of requirements elicitation technique, Comparison and
categorization of requirements elicitation techniques, Techniques for ranking and priori‐
tization of software requirements) [18–20] have been developed. But these methods are
devoted to monitor the implementation of requirements rather than on evaluating the
suﬃciency of the quality information in the SRS.
The analysis of known tools has shown that the number of tools have been developed,
in particular, the tools for constructing the software systems based on ontological models
of tasks [16], and the automated tools of the SRS analysis (IBM Rational RequisitePro,
IBM Rational/Telelogic DOORS, Borland Caliber RM, Sybase PowerDesigner, Open
Source Requirements Management Tool, Sigma Software, DEVPROM) [18–20]. But
these tools are not oriented to assessing the suﬃciency of the quality information in the
SRS.
Consequently, the known models, methods and tools don’t solve the problem of
evaluating the suﬃciency of quality information in the SRS. In addition, they all belong
to diﬀerent methodological approaches and don’t integrate among ourselves, that is,
nowadays the information technology of evaluating the suﬃciency of quality informa‐
tion in the SRS is absent.
The lack of the information technology of evaluating the suﬃciency of quality infor‐
mation in the SRS creates the actual scientiﬁc problem, one of the ways of solving which
is the development of the models, methods and tools of analyzing the suﬃciency of
quality information in the SRS. Therefore, the aim of this study is the development of
the information technology (models, methods and tools) of evaluating the suﬃciency of
quality information in the SRS.
2
Information Technology of Evaluating the Suﬃciency of Quality
Information in Software Requirements Speciﬁcation
The structure of the information technology (models, methods and tools) of evaluating
the suﬃciency of quality information in the SRS can be represented as follows – Fig. 3.
Figure 3 shows that the developed information technology consists of: (1) mathe‐
matical and ontological models of the software quality by the standard ISO 25010:2011
(were developed and represented in [21]); (2) mathematical and ontological models of
the software complexity and quality based on the metric analysis (were developed and
represented in [22]); (3) mathematical and ontological models of the SRS (were devel‐
oped and represented in [21, 22]); (4) methods of evaluating the suﬃciency of the SRS
information for software quality assessment (by the standard ISO 25010:2011) based
on the ontologies; (5) methods of evaluating the suﬃciency of the SRS information for
software complexity and quality assessment (on the basis of the metric analysis results)
Information Technology for Assurance of Veracity of Quality Information
169

based on the ontologies; (6) method of design results assessment and software charac‐
teristics prediction (was developed and represented in [23]); (7) subsystem of evaluating
the suﬃciency of the SRS information for software quality assessment based on the
comparative analysis of ontologies; (8) subsystem of software complexity and quality
evaluation and prediction based on the metric analysis results (was developed and repre‐
sented in [23]).
The practical implementation of the developed base (universal) ontological model
of the subject domain “Software Engineering” (part “Software Quality”) is the base
ontology of the subject domain “Software Engineering” (“Software Quality”), the
concept of which is represented on Fig. 4. The practical implementation of the developed
base (universal) ontological models of the subject domain “Software Engineering” (part
“Sofware Quality and Complexity”) is the base ontology of the subject domain “Soft‐
ware Engineering” (part “Software Quality and Complexity”), the concept of which is
represented on Fig. 5. The components of the base ontology of the subject domain
“Software Engineering” (“Software Quality”) are represented in [21], and the compo‐
nents of the base ontology of the subject domain “Software Engineering” (“Software
Quality and Complexity”) are represented in [22].
Fig. 4. Concept of the base ontology of the subject domain “Software Engineering” (“Software
Quality”)
Fig. 3. The structure of information technology of evaluating the suﬃciency of quality
information in the SRS
170
T. Hovorushchenko

Fig. 5. Concept of the base ontology of the subject domain “Software Engineering” (part
“Software Quality and Complexity. Metric analysis”)
Methods of evaluating the suﬃciency of the SRS information for software quality
assessment (by the standard ISO 25010:2011) based on the ontologies were developed
and detail represented in [21, 24].
The scheme of the method of evaluating the suﬃciency of the SRS information for
software quality assessment (by the standard ISO 25010:2011) based on the ontology
is represented on Fig. 6.
Fig. 6. The scheme of the method of evaluating the suﬃciency of the SRS information for
software quality assessment (by the standard ISO 25010:2011) based on the ontology
For the eliminate of the subjective evaluation and formal satisfaction of the software
quality, it’s necessity to consider the degree of severity of quality characteristics and
subcharacteristics, and their signiﬁcance. One of the problems of the known quality
Information Technology for Assurance of Veracity of Quality Information
171

models is the calculation of the signiﬁcance of the quality measures and characteristics.
Quality characteristics and subcharacteristics correlate with each other by the measures.
It was proven during the above software quality modeling. The existence of such corre‐
lations between subcharacteristics increases the signiﬁcance and weight of software
quality measures. Scheme of the method of evaluating the weights of software quality
measures is represented on Fig. 7.
Fig. 7. Scheme of the method of evaluating the weights of software quality measures
The weights of the software quality measures were estimated by the method of eval‐
uating the weights of software quality measures [21]. During the software quality
assessment by ISO 25010:2011, it’s important to satisfy the availability of measures
with larger weights in the SRS for ensuring the appropriate level of veracity of infro‐
mation. Weighted ontology of the subject domain “Software Engineering” (part “Soft‐
ware quality”) is the ontology, in which the software quality measures have weights
with purpose of the recommendation of further satisfaction of these measures in the SRS.
The weighted base ontology of the subject domain “Software Engineering” (part “Soft‐
ware quality”) was developed on the basis on the base ontology of the subject domain
“Software Engineering” (part “Software quality”) with addition of information about
the weights of software quality measures [21].
The scheme of the method of evaluating the suﬃciency of the SRS information for
software quality assessment (by the standard ISO 25010:2011) based on the weighted
ontology is represented on Fig. 8.
172
T. Hovorushchenko

Fig. 8. Scheme of the method of evaluating the suﬃciency of the SRS information for software
quality assessment (by the standard ISO 25010:2011) based on the weighted ontology
For forming the logical conclusion about suﬃciency of the SRS information for
software quality assessment by ISO 25010:2011 the production rules were formed on
the basis of the developed base and the weighted base ontologies for subject domain
“Software engineering” (part “Software quality”). 138 production rules (for the each of
measures) have the form “if-then” and were constructed as follows: if measure is missing
in the concrete SRS, then: the counters of missing measures for appropriate subcharac‐
teristics are increased by 1 and the counters of missing measures for appropriate char‐
acteristics are increased by the quantity of subcharacteristics of this characteristic, for
Information Technology for Assurance of Veracity of Quality Information
173

calculation of which the SRS information is insuﬃcient; the weight of the focused
measure is assigned to the element of array of the missing measures weights (index of
which is the focused measure).
The rule No. 139 has the form: if counters of missing measures for the all 31 software
quality subcharacteristics are simultaneously equal to 0, then the SRS information is
suﬃcient for calculation of all software quality subcharacteristics, else: the SRS infor‐
mation is insuﬃcient for the calculation of some software quality subcharacteristics
(with indicating the subcharacteristics, for calculation of which the SRS measures are
insuﬃcient). The rule No. 140 has the form: if counters of missing measures for the all
8 software quality characteristics are simultaneously equal to 0, then the SRS informa‐
tion is suﬃcient for calculation of all software quality characteristics, else: the SRS
information is insuﬃcient for the calculation of some software quality characteristics
(with indicating the characteristics, for calculation of which the SRS measures are
insuﬃcient); array of the missing measures weights should be sorted in descending the
values of elements (weights of missing measures); indices of those elements of the sorted
array of the missing measures weights, which aren’t equal 0, should be displayed – as
the recommended priority of addition of the missing measures in SRS [24].
The scheme of method of forming the logical conclusion about suﬃciency of the
SRS information for software quality assessment by ISO 25010:2011 is represented on
Fig. 9.
The methods of evaluating the suﬃciency of the SRS information for software
complexity and quality assessment (on the basis of the metric analysis results) based on
the ontologies were developed and detail represented in [22, 25]. These methods are
similar to the above methods of evaluating the suﬃciency of the SRS information for
software quality assessment (by the standard ISO 25010:2011) based on the ontologies
(Figs. 6 and 8). The weights of software complexity and quality indicators were calcu‐
lated in [22] by the method of evaluating the weights of software quality measures
(Fig. 7).
For forming the logical conclusion about suﬃciency of the SRS information for
software quality and complexity assessment by the metric analysis results the production
rules were formed on the basis of the developed base and the weighted base ontologies
for subject domain “Software engineering” (part “Software quality and complexity.
Metric analysis”). 42 production rules (for the each of indciators) have the form “if-
then” and were constructed as follows: if indicator is missing in the concrete SRS, then:
the counters of missing indicators for appropriate metrics are increased by 1; the weight
of the focused indicator is assigned to the element of array of the missing indicators
weights (index of which is the focused indicator). The rule No. 43 has the form: if
counters of missing indicators for the all 24 software quality and complexity metrics are
simultaneously equal to 0, then the SRS information is suﬃcient for calculation of all
software metrics, else: the SRS information is insuﬃcient for the calculation of some
software metrics (with indicating the metrics, for calculation of which the SRS indicators
are insuﬃcient); array of the missing indicators weights should be sorted in descending
the values of elements (weights of missing indicators); indices of those elements of the
sorted array of the missing indicators weights, which aren’t equal 0, should be displayed
– as the recommended priority of addition of the missing indicators in the SRS [25]. The
174
T. Hovorushchenko

method of forming the logical conclusion about suﬃciency of the SRS information for
software complexity and quality assessment by the metric analysis results is similar to
the above method of forming the logical conclusion about suﬃciency of the SRS infor‐
mation for software quality assessment by ISO 25010:2011 (Fig. 9).
Concept of method and subsystem of design results assessment and software char‐
acteristics prediction is represented on Fig. 10.
Fig. 9. Scheme of the method of forming the logical conclusion about suﬃciency of the SRS
information for software quality assessment by ISO 25010:2011
Information Technology for Assurance of Veracity of Quality Information
175

Fig. 10. The concept of method and subsystem of design results assessment and software
characteristics prediction
For the completion of the proposed information technology of evaluating the suﬃ‐
ciency of quality information in the SRS, it’s necessary to develop (to design and realize)
the subsystem of evaluating the suﬃciency of the SRS information for software quality
assessment based on the comparative analysis of ontologies.
3
Subsystem of Evaluating the Suﬃciency of Software
Requirements Speciﬁcation Information for Software Quality
Assessment Based on the Comparative Analysis of Ontologies
The inputs of the subsystem of evaluating the suﬃciency of the SRS information for
software quality assessment based on the comparative analysis of ontologies are the sets:
(1) {qms1, …, qmsnm} (nm ≤ 138) available in the SRS software quality measures
(according to standards [7, 8], the software quality subcharacteristcs depend on 203
measures, but only on 138 diﬀerent measures); (2) {sqcxi1, …, sqcxini} (ni ≤ 42) avail‐
able in the SRS software quality and complexity indicators (the selected in [9, 23] soft‐
ware metrics depend on 72 indicators, but only on 42 diﬀerent indicators).
The results of the developed subsystem are: (1) conclusion about the suﬃciency of
the SRS information for software quality assessment by the standard ISO 25010:2011;
(2) recommendations about necessity and priority of the addition of the measures in the
SRS for software quality assessment by ISO 25010:2011; (3) evaluation of the veracity
of the available in the SRS information for software quality assessment by ISO
25010:2011; (4) conclusion about the suﬃciency of the SRS information for software
complexity and quality assessment by the metric analysis results; (5) recommendations
about necessity and priority of the addition of the indicators in the SRS for determining
the software complexity and quality by the metric analysis results; (6) evaluation of the
veracity of the available in the SRS information for software quality assessment by the
metric analysis results.
The concept of subsystem of evaluating the suﬃciency of the SRS information for
software quality assessment based on the comparative analysis of ontologies is repre‐
sented on Fig. 11. The structure of subsystem of evaluating the suﬃciency of the SRS
information for software quality assessment based on the comparative analysis of ontol‐
ogies is represented on Fig. 12.
176
T. Hovorushchenko

Fig. 11. The concept of subsystem of evaluating the suﬃciency of the SRS information for
software quality assessment based on the comparative analysis of ontologies
The developed subsystem consists of the next components: (1) module of introduc‐
tion of the SRS measures – collects the user information about the available values of
measures {qms1, …, qmsnm} (nm ≤ 138) in the SRS for concrete software; (2) module
of introduction of the SRS indicators – collects the user information about the available
values of indicators {sqcxi1, …, sqcxini} (ni ≤ 42) in the SRS; (3) module of the user
support – provides to the user the information about the structure of the SRS; about the
SRS measures, which are necessary for software quality assessment by ISO 25010; about
the SRS indicators, which are necessary for determining the software complexity and
quality based on the metric analysis results; about the process of the forming the results
of the described subsystem; (4) module of evaluating the suﬃciency of the SRS infor‐
mation for software quality assessment by the standard ISO 25010:2011 – works
according to methods of evaluating the suﬃciency of the SRS information for software
quality assessment (by ISO 25010:2011) based on the ontologies. The generation and
ﬁlling of the ontology template for assessing the quality of the concrete software are
performed, considering introduced the available measures {qms1, …, qmsnm}
(nm ≤ 138). The comparative analysis of the ontology for the concrete software with
the developed base ontology for subject domain “Software engineering” (part “Software
quality”) is performed. The result of this comparative analysis is the list of missing
measures (in the concrete SRS). If during the comparative analysis of ontologies the
diﬀerences were not identiﬁed, then information of the SRS is suﬃcient for software
quality assessment by ISO 25010. If during the comparative analysis of ontologies the
diﬀerences were identiﬁed, then the available in the SRS measures are insuﬃcient for
some subcharacteristics and characteristics calculation, then the comparative analysis
of the ontology for the concrete software with the developed weighted base ontology
for subject domain “Software engineering” (part “Software quality”) is performed, and
sorting of all missing (in the SRS) measures in descending the values of weights is
conducted, i.e. priority of the addition of these measures in the SRS is established. The
quantitative evaluation of the veracity of the available in the SRS information for the
software quality assessment is calculated; (5) module of evaluating the suﬃciency of the
SRS information for determining the software complexity and quality based on the metric
analysis results – works according to methods of evaluating the suﬃciency of the SRS
Information Technology for Assurance of Veracity of Quality Information
177

information for software complexity and quality assessment (on the basis of the metric
analysis) results based on the ontologies. The generation and ﬁlling of the ontology
template for assessing the quality and complexity of the concrete software are
performed, considering introduced the available indicators {sqcxi1, …, sqcxini}
(ni ≤ 42). The comparative analysis of the ontology for the concrete software with the
Fig. 12. The structure of subsystem of evaluating the suﬃciency of the SRS information for
software quality assessment based on the comparative analysis of ontologies
178
T. Hovorushchenko

developed base ontology for subject domain “Software engineering” (part “The software
quality and complexity. Metric analysis”) is performed. The result of this comparative
analysis is the list of missing indicators (in the concrete SRS). If during the comparative
analysis of ontologies the diﬀerences were not identiﬁed, then information of the SRS
is suﬃcient for software quality and complexity assessment based on the metric analysis.
If during the comparative analysis of ontologies the diﬀerences were identiﬁed, then the
available in the SRS indicators are insuﬃcient for some metrics calculation, then the
comparative analysis of the ontology for the concrete software with the developed
weighted base ontology for subject domain “Software engineering” (part “The software
quality and complexity. Metric analysis”) is performed, and sorting of all missing (in
the SRS) indicators in descending the values of weights is conducted, i.e. priority of the
addition of these indicators in the SRS is established. The quantitative evaluation of the
veracity of the available in the SRS information for the software quality and complexity
assessment is calculated; (6) knowledge base – contains the base and the weighted base
ontologies for subject domain “Software engineering” (part “Software quality”, part
“Software quality and complexity. Metric Analysis”), the formed ontologies for the
concrete software, and production rules of forming the logical conclusion about the
suﬃciency of the SRS information for software quality assessment by ISO 25010:2011
and for software complexity and quality assessment by the metric analysis results; (7)
module of the results display – the components of this block display the formed conclu‐
sions, recommendations and evaluations to user.
4
Experiments: Evaluating the Suﬃciency of the Information
of the SRS of Automated System for Large-Format Photo Print
for Software Quality Assessment
For experiment the SRS of automated system (AS) for large-format photo print was
analyzed. The measures, which are available in this SRS, were identiﬁed. The ontology
for this software was developed [21].
The comparison (in Protégé 4.2) of the developed ontology for AS for large-format
photo print with the base ontology for subject domain “Software engineering” (part
“Software quality”) provides the conclusion, that in the developed ontology for the
concrete software 4 measures are absent: “Number Of Functions”, “Operation Time”,
“Number Of Data Items”, “Number Of Test Cases” (Fig. 13).
Then the set of missing measures is: {Number Of Functions, Operation Time,
Number Of Data Items, Number OF Test Cases}. The ﬁnding the rule for each element
of this set among 138 rules for the measures is performed. According to these rules, the
counters of missing measures are counted.
According to the rule No. 139, the fact was established, that the available measures
in the SRS of AS for large-format photo print are insuﬃcient for calculation of following
subcharacteristics: Functional Completeness, Functional Correctness, Functional
Appropriateness, Maturity, Availability, Fault Tolerance, Recoverability, Time Behav‐
iour, Resource Utilization, Capacity, Appropriateness Recognisability, Learnability,
Information Technology for Assurance of Veracity of Quality Information
179

Operability, Modularity, Analysability, Modiﬁability, Testability, Conﬁdentiality,
Integrity, CoExistence, Interoperability, Adaptability, Replaceability.
According to the rule No. 140, the fact was established, that the available measures
in the SRS AS for large-format photo print are insuﬃcient for calculation of all 8 soft‐
ware quality characteristics. Thus, the lack of 4 measures in the SRS led: to the impos‐
sibility of calculating the 23 (from 31) subcharactersitics, to the impossibility of calcu‐
lating all 8 software quality characteristics with high veracity and, respectively, to the
impossibility of software quality assessment with high veracity. After establishing the
fact of insuﬃciency of information of the SRS of AS for large-format photo print: sorting
the array of the missing measures weights in descending the values of elements was
conducted; displaying the indices of those elements of the sorted array of the missing
measures weights, which aren’t equal 0. Sorted list of missing in the SRS measures in
descending the weights: (1) Operation Time (17/138); (2) Number of Functions
(11/138); (3) Number of Data Items (8/138); (4) Number of Test Cases (5/138). This
list represents the recommended priority of the addition of missing measures in the SRS
of AS for large-format photo print.
Next, the evaluation of the veracity of the available in the SRS information for soft‐
ware quality assessment is done (according to the method of forming the logical conclu‐
sion about suﬃciency of the SRS information for software quality assessment by ISO
25010:2011 [21, 24]). So, for the analyzed SRS of AS for large-format photo print the
conclusion about insuﬃcient data for software quality assessment was formed by the
developed subsystem, and the veracity of the available in the SRS information for the
software quality assessment by ISO 25010:2011 is 76%.
Because the proposed methods of evaluating the suﬃciency of the SRS information
for software quality assessment (by ISO 25010:2011) based on the ontology are iterative,
and there are subcharacteristics and characteristics, for calculation of which the meas‐
ures of SRS are insuﬃcient, then the addition of the necessary measures in the SRS was
Fig. 13. Comparison of ontology for concrete software (AS for large-format photo print) with
the base ontology of the subject domain “Software Engineering” (part “Software Quality”)
180
T. Hovorushchenko

held. After addition of the SRS of AS for large-format photo print, the ontology (version
2) for this software was re-developed. The comparison of the re-developed ontology
with the base ontology for subject domain “Software engineering” (part “Software
quality”) provides the conclusion, that 2 measures were added in the SRS: “Number Of
Functions” (2nd in the sorted list), “Number Of Data Items” (3rd in the sorted list). Then
the set of missing measures is: {Operation Time, Number Of Test Cases}. The ﬁnding
the rule for each element of this set is performed.
According to the rule No. 139, the fact was established, that the available measures
in the SRS of AS for large-format photo print are still insuﬃcient for calculation of 18
subcharacteristics (with indicating these subcharacteristics), but addition 2 measures in
the SRS made possible the calculation of: Functional Completeness, Capacity, Appro‐
priateness Recognisability, Analyzability, Replaceability.
According to rule No. 140, the fact was established, that the available measures in
the SRS of AS for large-format photo print are still insuﬃcient for calculation of all 8
software quality characteristics. After establishing the fact of insuﬃciency of informa‐
tion of the SRS: sorting the array of the missing measures weights in descending the
values of elements was conducted; displaying the indices of those elements of the sorted
array of the missing measures weights, which aren’t equal 0. Sorted list of missing (after
addition) in the SRS measures in descending the weights: (1) Operation Time; (2)
Number of Test Cases.
Next, the evaluation of the veracity of the available (after addition) in the SRS infor‐
mation for software quality assessment is done (according to the method of forming the
logical conclusion about suﬃciency of the SRS information for software quality assess‐
ment by ISO 25010:2011 [21, 24]). So, for the analyzed SRS of AS for large-format
photo print the conclusion about still insuﬃcient information for software quality
assessment was formed by the developed subsystem, and the veracity of the available
(after addition) in the SRS information for the software quality assessment by ISO
25010:2011 is 88%.
The process of addition the necessary measures in the SRS is iterative. It can be
continued until all quality characteristics and subcharacteristics will be possible to
calculate or until the conclusion will be formed, that the SRS information are insuﬃcient
for software quality assessment. The customer of developed AS for large-format photo
print has decided that further complement of SRS is economically inexpedient.
The gain of the veracity of the SRS information for software quality assessment by
ISO 25010:2011 after addition of necessary measures in the SRS is 12% (according to
the method of forming the logical conclusion about suﬃciency of the SRS information
for software quality assessment by ISO 25010:2011 [21, 24]). So, the developed infor‐
mation technology and subsystem of evaluating the suﬃciency of the quality information
in the SRS provides the increase of the veracity of the SRS information for the software
quality assessment by ISO 25010:2011 by 12% for AS for large-format photo print.
Let’s consider the functioning of the developed information technology and
subsystem for evaluating the suﬃciency of the quality information in the SRS for metric
analysis. For experiment the SRS of AS for large-format photo print was analyzed. The
indicators, which are available in this SRS, were identiﬁed. The ontology for this soft‐
ware metric analysis was developed [22].
Information Technology for Assurance of Veracity of Quality Information
181

The comparison (in Protégé 4.2) of the developed ontology for AS for large-format
photo print with the base ontology for the subject domain “Software Engineering” (part
“Software complexity and quality. Metric analysis”) provides the conclusion, that in the
developed ontology for the concrete software metric analysis 9 indicators are absent:
“Control Variables”, “Cost Of One Line”, “Project Duration”, “Project Type”, “Quantity
Of Code Lines”, “Quantity Of Links Of Each Module”, “Quantity Of Modules”, “Share
Of Design Stage In Lifecycle”, “Total Quantity Of Operators”. Then the set of missing
indicators is: {Control Variables, Cost Of One Line, Project Duration, Project Type,
Quantity Of Code Lines, Quantity Of Links Of Each Module, Quantity Of Modules,
Share Of Design Stage In Lifecycle, Total Quantity Of Operators}. The ﬁnding the rule
for each element of this set among 42 rules for the indicators is performed. According
to these rules, the counters of missing indicators are counted.
According to the rule No. 43, the fact was established, that the available indicators
in the SRS of AS for large-format photo print are insuﬃcient for calculation of the 20
(from 24) metrics with high veracity and, respectively, and for metric analysis with high
veracity. After establishing the fact of insuﬃciency of information of the SRS of AS for
large-format photo print: sorting the array of the missing indicators weights in
descending the values of elements was conducted; displaying the indices of those
elements of the sorted array of the missing indicators weights, which aren’t equal 0. So,
for increasing the veracity of the SRS information for software metric analysis the next
indicators should be added in the SRS in this consistency: (1) Quantity Of Code Lines,
(2) Quantity Of Modules, (3) Project Duration, (4) Total Quantity Of Operators, (5) Cost
Of One Line, (6) Project Type, (7) Share Of Design Stage In Lifecycle, (8) Control
Variables, (9) Quantity Of Links Of Each Module.
Next, the evaluation of the veracity of the available in the SRS information for metric
analysis is done (according to the method of forming the logical conclusion about suﬃ‐
ciency of the SRS information for software quality assessment by metric analysis results
[22, 25]). So, for the analyzed SRS of AS for large-format photo print the conclusion
about insuﬃcient data for metric analysis was formed by the developed subsystem, and
the veracity of the available in the SRS information for the metric analysis is 42%.
The addition of the necessary indicators in the SRS was held. After addition of the
SRS of AS for large-format photo print, the ontology (version 2) for this software was
re-developed. The comparison of the re-developed ontology with the base ontology
provides the conclusion, that 2 indicators were added in the SRS: “Quantity Of Modules”
(2nd in the sorted list), “Total Quantity Of Operators” (4th in the sorted list). So, for
increasing the veracity of the SRS information for the metric analysis the next indicators
should be added in the SRS in this consistency: (1) Quantity Of Code Lines, (2) Project
Duration, (3) Cost Of One Line, (4) Project Type, (5) Share Of Design Stage In Lifecycle,
(6) Control Variables, (7) Quantity Of Links Of Each Module.
Next, the evaluation of the veracity of the available (after addition) in the SRS infor‐
mation for metric analysis is done (according to the method of forming the logical
conclusion about suﬃciency of the SRS information for metric analysis [22, 25]). So,
for the analyzed SRS of AS for large-format photo print the conclusion about still
insuﬃcient information for metric analysis was formed by the developed subsystem,
182
T. Hovorushchenko

and the veracity of the available (after addition) in the SRS information for the metric
analysis is 56%.
The customer of developed AS for large-format photo print has decided that further
complement of the SRS is economically inexpedient.
The gain of the veracity of the SRS information for metric analysis after addition of
necessary indicators in the SRS is 14% (according to the method of forming the logical
conclusion about suﬃciency of the SRS information for metric analysis [22, 25]). So,
the developed information technology and subsystem of evaluating the suﬃciency of the
quality information in the SRS provides the increase of the veracity of the SRS infor‐
mation for the metric analysis by 14% for AS for large-format photo print.
5
Conclusions
The information technology of evaluating the suﬃciency of quality information in the
SRS are ﬁrst time proposed in this paper. It designed to the support of the software
quality assessment at the early lifecycle stages. They provides: the conclusion about the
suﬃciency of the SRS information for software quality assessment by ISO 25010:2011
and based on the metric analysis results; the prioritization of the additions of the SRS
by the measures and (or) by the indicators (if the SRS information is insuﬃcient); the
quantitative evaluations of the veracity of the available in the SRS information for soft‐
ware quality assessment by ISO 25010 and by the metric analysis; the increasing the
software quality at the early lifecycle stages.
The subsystem of evaluating the suﬃciency of the SRS information for the software
quality assessment on the basis of the comparative analysis of the ontologies is ﬁrst time
proposed in this paper. It is the decision support system that provides the decision about:
the suﬃciency of the SRS information for the software quality assessment, the necessity
of the addition(s) of the measures and(or) the indicators in the SRS, the veracity of the
available in the SRS information for the software quality assessment by ISO 25010:2011
and by the metric analysis.
The experiments proved, that the use of the developed information technology of
evaluating the suﬃciency of quality information in the SRS provides the increase of the
veracity of the SRS information for software quality assessment based on ISO
25010:2011 by 12%, and based on the metric analysis results by 14% even after one
addition of the SRS for AS for large-format photo print.
The proposed information technology provides the increasing the veracity of the
quality information in the SRS, and improving the software quality at the early stages
of the lifecycle.
Information Technology for Assurance of Veracity of Quality Information
183

References
1. Jones, C., Bonsignour, O.: The Economics of Software Quality. Pearson Education, Boston
(2012)
2. Ishimatsu, T., Levenson, N., Thomas, J., Fleming, C., Katahira, M., Miyamoto, Y., Ujiie, R.:
Hazard analysis of complex spacecraft using systems-theoretic process analysis. J. Spacecraft
Rockets 51, 509–522 (2014)
3. Yourdon, E.:: Death March: The Complete Software Developer’s Guide to Surviving
“Mission Impossible” Projects, 2nd edn. Prentice Hall (2003)
4. Shamieh, C.: Systems Engineering for Dummies. Wiley Publishing (2011)
5. Maevskiy, D., Kozina, Y.: Where and when is formed of software quality? Electr. Comput.
Syst. 18, 55–59 (2016). (in Russian)
6. McConnell, S.: Code Complete. Microsoft Press (2013)
7. ISO/IEC 25010:2011: Systems and software engineering. Systems and software Quality
Requirements and Evaluation (SQuaRE). System and software quality models (2011)
8. ISO 25023:2016: Systems and software engineering. Systems and software Quality
Requirements and Evaluation (SQuaRE). Measurement of system and software product
quality (2016)
9. Fenton, N.: Software Metrics: A Rigorous Approach, 3rd edn. CRC Press (2014)
10. Shostak, I., Butenko, I.: Ontology approach to realization of information technology for
normative proﬁle forming at critical software certiﬁcation. Herald of the Military Institute of
Kiev National University named after Taras Shevchenko, vol. 38, pp. 250–253 (2012)
11. Lytvyn, V.: Modeling of intelligent decision support systems using the ontological approach.
Radio Electr. Comput. Sci. Manage. 2, 93–101 (2011). (in Ukrainian)
12. Henderson-Sellers, B., Gonzalez-Perez, C., McBride, T., Low, G.: An ontology for ISO
software engineering standards: 1) creating the infrastructure. Comput. Stan. Interfaces.
36(3), 563–576 (2014)
13. Ruy, F.B., Falbo, R.A., Barcellos, M.P., Guizzardi, G., Quirino, G.K.S.: An ISO-based
software process ontology pattern language and its application for harmonizing standards.
ACM SIGAPP Appl. Comput. Rev. 15(2), 27–40 (2015)
14. Hamri, M.M., Benslimane, S.M.: Building an ontology for the metamodel ISO/IEC24744
using MDA process. Int. J. Mod. Educ. Comput. Sci. 8, 48–60 (2015)
15. Jin, D., Cordy, J.R.: Ontology-based software analysis and reengineering tool integration: the
OASIS service-sharing methodology. In: IEEE International Conference on Software
Maintenance Proceedings, Budapest, pp. 613–616. IEEE (2005)
16. Burov, E.: Complex ontology management using task models. Int. J. Knowl. Based Intell.
Eng. Syst. 18(2), 111–120 (2014)
17. Babenko, L.: Ontological approach to speciﬁcation of software systems features and
components. Cybern. Syst. Anal. 1, 180–187 (2009). (in Russian)
18. Chen, A., Beatty, J.: Visual Models for Software Requirements. MS Press, Washington (2012)
19. Fatwanto, A.: Software requirements speciﬁcation analysis using natural language processing
technique. In: IEEE International Conference on Quality in Research Proceedings,
Yogyakarta, pp. 105–110. IEEE (2013)
20. Rehman, T., Khan, M.N.A., Riaz, N.: Analysis of requirement engineering processes, tools/
techniques and methodologies. Int. J. Inf. Technol. Comput. Sci. 5(3), 40–48 (2013)
21. Hovorushchenko, T.: Methodology of evaluating the suﬃciency of information for software
quality assessment according to ISO 25010. J. Inf. Organ. Sci. (2017, to appear)
184
T. Hovorushchenko

22. Hovorushchenko, T.: Models and methods of evaluation of information suﬃciency for
determining the software complexity and quality based on the metric analysis results. Centr.
Eur. Res. J. 2, 42–53 (2016)
23. Pomorova, O., Hovorushchenko, T.: Artiﬁcial neural network for software quality evaluation
based on the metric analysis. In: IEEE East-West Design and Test Symposium Proceedings,
Kharkiv, pp. 200–203. IEEE (2013)
24. Hovorushchenko, T.: Forming the logical conclusion about suﬃciency of information of
software requirements speciﬁcation for software quality assessment by ISO 25010:2011. In:
IEEE First Ukraine Conference on Electrical and Computer Engineering Proceedings, Kyiv,
pp. 789–794. IEEE (2017)
25. Hovorushchenko, T.: The rules and method of forming the logical conclusion about
suﬃciency of information for software metric analysis. In: IEEE International Scientiﬁc and
Technical Conference on Computer Science and Information Technologies, Lviv. IEEE
(2017, to appear)
Information Technology for Assurance of Veracity of Quality Information
185

A Multidimensional Adaptive Growing
Neuro-Fuzzy System and Its Online
Learning Procedure
Zhengbing Hu1, Yevgeniy V. Bodyanskiy2,
and Oleksii K. Tyshchenko2(&)
1 School of Educational Information Technology, Central China Normal
University, 152 Louyu Road, 430079 Wuhan, China
hzb@mail.ccnu.edu.cn
2 Control Systems Research Laboratory, Kharkiv National University of Radio
Electronics, 14 Nauky Ave., 61166 Kharkiv, Ukraine
yevgeniy.bodyanskiy@nure.ua, lehatish@gmail.com
Abstract. The paper presents learning algorithms for a multidimensional
adaptive growing neuro-fuzzy system with optimization of a neuron ensemble in
every cascade. A building block for this architecture is a multidimensional
neo-fuzzy neuron. The demonstrated system is distinguished from the
well-recognized cascade systems in its ability to handle multidimensional data
sequences in an online fashion, which makes it possible to treat non-stationary
stochastic and chaotic data with the demanded accuracy. The most important
privilege of the considered hybrid neuro-fuzzy system is its trait to accomplish a
procedure of parallel computation for a data stream based on peculiar elements
with upgraded approximating properties. The developed system turns out to be
rather easy from the effectuation standpoint; it holds a high processing speed and
approximating features. Compared to acclaimed countertypes, the developed
system guarantees computational simpleness and owns both ﬁltering and
tracking aptitudes. The proposed system, which is ultimately a growing
(evolving) system of computational intelligence, assures processing the
incoming data in an online fashion just unlike the rest of conventional systems.
Keywords: Learning method  Cascade system  Ensemble of neurons
Multidimensional neo-fuzzy neuron  Computational intelligence
Adaptive neuro-fuzzy system
1
Introduction
A great combination of different neuro-fuzzy systems is of considerable use nowadays
for a large variety of data processing problems. This fact should be highlighted by a
number of preferences that neuro-fuzzy systems hold over other existing methods, and
that comes from their abilities to get trained as well as their universal approximating
capacities.
A degree of the training procedure may be reﬁned by adapting both a network’s set
of synaptic weights and its topology [1–8]. This notion is the ground rules for evolving
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_13

(growing) systems of computational intelligence [9–11]. It stands to mention that
probably
one
of
the
most
prosperous
actualizations
of
this
attitude
is
cascade-correlation neural networks [12–14] by reason of their high level of efﬁcacy
and learning simplicity for both a network scheme and for synaptic weights. In general
terms, such sort of a network gets underway with a rather simple architecture con-
taining an ensemble of neurons to be trained irrespectively (a case of the ﬁrst cascade).
Every neuron in an ensemble can possess various activation functions as well as
learning procedures. Nodes (neurons) in the ensemble do not intercommunicate while
they are being learnt.
Eventually, when all the elements in the ensemble of the ﬁrst cascade have had their
weights adapted, the best neuron in relation to a learning criterion builds up the ﬁrst
cascade, and its synaptic weights are not able of being conﬁgured any longer. In the
next place, the second cascade is commonly formed by means of akin neurons in the
training ensemble. The sole difference is that neurons to be learnt in the ensemble of the
second cascade own an additional input (and consequently an additional synaptic
weight) which proves to be an output of the ﬁrst cascade. In similar fashion to the ﬁrst
cascade, the second one withdraws all elements except a single one, which gives the
best performance. Its synaptic weights should be ﬁxed afterwards. Nodes in the third
cascade hold two additional inputs, namely the outputs of the ﬁrst and second cascades.
The growing network keeps on adding new cascades to its topology until it gains the
required quality of the results received over the given training set.
By way of evading multi-epoch learning [15–23], various kinds of neurons
(preferably their outputs should depend in a linear manner on synaptic weights) may be
utilized as the network’s elements. This could give the opportunity to exploit some
optimal in speed learning algorithms and handle data as it arrives to the network. In the
meantime, if the system is being trained in an online manner, it looks impossible to
detect the best neuron in the ensemble. While handling non-stationary data objects, one
node in a training ensemble may be conﬁrmed to be the best element for one part of the
training data sample (but it cannot be selected as the best one for the other parts). It may
be recommended that all the units should be abandoned in the training ensemble, and
some speciﬁc optimization method (selected in agreement with a general quality cri-
terion for the network) is meant to be used for estimation of an output of the cascade.
It will be observed that the widely recognized cascade neural networks bring into
action a non-linear mapping Rn ! R1, which means that a common cascade neural
network is a system with a single output. By contrast, many problems solved by means
of neuro-fuzzy systems demand a multidimensional mapping Rn ! Rg to be executed,
that ﬁnally accounts for the fact that a number of elements to be trained in every
cascade is g times more by contrast to a common neural network, which makes this sort
of a system too ponderous. Hence, it seems relevant to operate a speciﬁc multidi-
mensional neuron’s topology as the cascade network’s unit with multiple outputs
instead of traditional.
The described growing cascade neuro-fuzzy system of computational intelligence is
actually an effort to develop a system for handling a data stream that is fed to the
system in an online way and that is in possession of a far smaller amount of parameters
to be set as opposed to other widely recognized analogues.
A Multidimensional Adaptive Growing Neuro-Fuzzy System
187

2
An Architecture of the Hybrid Growing System
A scheme of the introduced hybrid system is represented in Fig. 1. In fact, it coincides
with architecture of the hybrid evolving neural network with an optimized ensemble in
every cascade group of elements to have been developed in [24–29]. A basic dis-
similarity lies in a type of elements utilized and learning procedures respectively.
A network’s input can be described by a vector signal x k
ð Þ ¼ x1 k
ð Þ; x2 k
ð Þ; . . .;
ð
xn k
ð ÞÞT, where k ¼ 1; 2; . . . stands for either a plurality of observations in the
“object-property” table or an index of the current discrete time. These signals are
moved to inputs of each neuron MN½m
j
in the system (j ¼ 1; 2; . . .; q denotes a number
of neurons in a training ensemble, m ¼ 1; 2; . . . speciﬁes a cascade’s number). A vector
output ^y½mj k
ð Þ ¼
^y½mj
1
k
ð Þ;^y½mj
2
k
ð Þ; . . .;^y½mj
d
k
ð Þ; . . .;^y½mj
g
k
ð Þ

T
is eventually produced,
d ¼ 1; 2; . . .; g. These outputs are in the next place fed to a generalizing neuron
GMN½m to reproduce an optimized vector output ^y½m k
ð Þ for the cascade m. Just as the
input of the nodes in the ﬁrst cascade is x k
ð Þ, elements in the second cascade take g
additional arriving signals for the obtained signal ^y½1 k
ð Þ, neurons in the third cascade
have 2g additional inputs ^y½1 k
ð Þ;^y½2 k
ð Þ, whilst neurons in the m-th cascade own
m  1
ð
Þg additional incoming signals ^y½1 k
ð Þ;^y½2 k
ð Þ; . . .;^y½m1 k
ð Þ. New cascades are
becoming a part of the hybrid system within the learning procedure just as it turns out
Fig. 1. An architecture of the growing neuro-fuzzy system.
188
Z. Hu et al.

to be clear that an architecture with a current amount of cascades does not provide the
required accuracy.
Since a system signal in a conventional neo-fuzzy neuron [30–32] is governed by
the synaptic weights in a linear manner, any adaptive identiﬁcation algorithm [33–35]
may actually be applied to learning the network’s neo-fuzzy neurons (like either the
exponentially-weighted least-squares method in a recurrent form
w½mj
d
k þ 1
ð
Þ ¼ w½mj
d
k
ð Þ þ
P½mj
d
k
ð Þ yd k þ 1
ð
Þ 
w½mj
d
k
ð Þ

T
l½mj
d
k þ 1
ð
Þ


a þ l½mj
d
k þ 1
ð
Þ

T
P½mj
d
k
ð Þl½mj
d
k þ 1
ð
Þ
l½mj
d
k þ 1
ð
Þ;
P½mj
d
k þ 1
ð
Þ ¼ 1
a
P½mj
d
k
ð Þ 
P½mj
d
k
ð Þl½mj
d
k þ 1
ð
Þ l½mj
d
k þ 1
ð
Þ

T
P½mj
d
k
ð Þ
a þ l½mj
d
k þ 1
ð
Þ

T
P½mj
d
k
ð Þl½mj
d
k þ 1
ð
Þ
0
B
@
1
C
A
8
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
:
ð1Þ
(here yd k þ 1
ð
Þ; d ¼ 1; 2; . . .; g speciﬁes an external learning signal, 0\a  1 marks a
forgetting factor) or the gradient learning algorithm with both tracking and ﬁltering
properties [35])
w½mj
d
k þ 1
ð
Þ ¼ w½mj
d
k
ð Þ þ
yd k þ 1
ð
Þ 
w½mj
d
k
ð Þ

T
l½mj
d
k þ 1
ð
Þ
r½mj
d
k þ 1
ð
Þ
l½mj
d
k þ 1
ð
Þ;
r½mj
d
k þ 1
ð
Þ ¼ ar½mj
d
k
ð Þ þ l½mj
d
k þ 1
ð
Þ


2
; 0  a  1:
8
>
>
>
>
<
>
>
>
>
:
ð2Þ
An architecture of a typical neo-fuzzy neuron (Fig. 2) as part of the multidimensional
neuron MN½1
g
in the cascade system is abundant, since a vector of input signals x k
ð Þ
(the ﬁrst cascade) is sent to same-type non-linear synapses NS½1j
di
of the neo-fuzzy
neurons, where each neuron obtains a signal ^y½1j
d
k
ð Þ; d ¼ 1; 2; . . .; g at its output. As a
result, components of the output vector ^y½1j k
ð Þ ¼
^y½1j
1
k
ð Þ;^y½1j
2
k
ð Þ; . . .;^y½1j
g
k
ð Þ

T
are
computed irrespectively.
This fact can be missed by introducing a multidimensional neo-fuzzy neuron [36],
whose architecture is shown in Fig. 3 and is a modiﬁcation of the system proposed in
[37]. Its structural units are composite non-linear synapses MNS½1j
i , where each synapse
contains h membership functions l½1j
li
and gh tunable synaptic weights w½1j
dli . In this
way, the multidimensional neo-fuzzy neuron in the ﬁrst cascade contains ghn synaptic
weights, but only hn membership functions. That’s g times smaller in comparison with
a situation if the cascade is formed of common neo-fuzzy neurons.
Assuming a hn  1
ð
Þ – vector of membership functions
l½1j k
ð Þ ¼
l½1j
11 x1 k
ð Þ
ð
Þ; l½1j
21 x1 k
ð Þ
ð
Þ;. . .; l½1j
h1 x1 k
ð Þ
ð
Þ;. . .;l½1j
li
xi k
ð Þ
ð
Þ;. . .;l½1j
hn xn k
ð Þ
ð
Þ

T
and a g  hn
ð
Þ – matrix of synaptic weights
A Multidimensional Adaptive Growing Neuro-Fuzzy System
189

W½1j ¼
w½1j
111
w½1j
112
  
w½1j
1li
  
w½1j
1hn
w½1j
211
w½1j
212
  
w½1j
2li
  
w½1j
2hn
..
.
..
.
..
.
...
...
...
w½1j
g11
w½1j
g12
  
w½1j
gli
  
w½1j
ghn
0
B
B
B
B
B
B
@
1
C
C
C
C
C
C
A
;
the output signal MN½1
j
can be written down at the k – th time moment in the form of
^y½1j k
ð Þ ¼ W½1jl½1j k
ð Þ:
ð3Þ
Learning the multidimensional neo-fuzzy neuron may be carried out applying either
a matrix modiﬁcation of the exponentially-weighted recurrent least squares method (1)
in the form of
W½1j k þ 1
ð
Þ ¼ W½1j k
ð Þ þ y k þ 1
ð
Þ  W½1j k
ð Þl½1j k þ 1
ð
Þ


l½1j k þ 1
ð
Þ

TP½1j k
ð Þ
a þ l½1j k þ 1
ð
Þ
ð
ÞTP½1j k
ð Þl½1j k þ 1
ð
Þ
;
P½1j k þ 1
ð
Þ ¼ 1
a
P½1j k
ð Þ  P½1j k
ð Þl½1j k þ 1
ð
Þ l½1j k þ 1
ð
Þ

TP½1j k
ð Þ
a þ l½1j k þ 1
ð
Þ
ð
ÞTP½1j k
ð Þl½1j k þ 1
ð
Þ
 
!
; 0\a  1
8
>
>
>
>
>
<
>
>
>
>
>
:
ð4Þ
Fig. 2. An architecture of the traditional neo-fuzzy neuron.
190
Z. Hu et al.

or a multidimensional version of the algorithm (2) [38]:
W½1j k þ 1
ð
Þ ¼ W½1j k
ð Þ þ y k þ 1
ð
Þ  W½1j k
ð Þl½1j k þ 1
ð
Þ
r½1j k þ 1
ð
Þ
l½1j k þ 1
ð
Þ

T
;
r½1j k þ 1
ð
Þ ¼ ar½1j k
ð Þ þ l½1j k þ 1
ð
Þ

2; 0  a  1;
8
>
<
>
:
ð5Þ
here y k þ 1
ð
Þ ¼ y1 k þ 1
ð
Þ; y2 k þ 1
ð
Þ; . . .; yg k þ 1
ð
Þ
ð
ÞT:
The rest of cascades are trained in a similar fashion, while a vector of membership
functions l½mj k þ 1
ð
Þ in the m-th cascade enlarges its dimensionality by
m  1
ð
Þg
elements which are guided by the preceding cascades’ outputs.
Fig. 3. An architecture of the multidimensional neo-fuzzy neuron.
A Multidimensional Adaptive Growing Neuro-Fuzzy System
191

3
Output Signals’ Optimization of the Multidimensional
Neo-fuzzy Neuron Ensemble
Outputs generated by the neurons in each ensemble are combined by the corresponding
neuron GN½m, whose output accuracy ^y½m k
ð Þ must be higher than the accuracy of any
output ^y½m
j
k
ð Þ. This task can be solved through the use of the neural networks’
ensembles approach. Although the well-recognized algorithms are not designated for
operating in an online fashion, in this case one could use the adaptive generalizing
forecasting [39, 40].
Let’s introduce a vector of ensemble inputs for the m-th cascade
^y½m k
ð Þ ¼
^y½m
1
k
ð Þ;^y½m
2
k
ð Þ; . . .;^y½m
q
k
ð Þ

T
;
then an optimal output of the neuron GN½m, which is intrinsically an adaptive linear
associator [1–8], can be deﬁned as
^y½m k
ð Þ ¼
X
q
j¼1
c½m
j ^y½m
j
k
ð Þ ¼ c½mT^y½m k
ð Þ
or with additional constraints on unbiasedness
X
q
j¼1
c m
½ 
n
¼ ETc m
½  ¼ 1
ð6Þ
where c m
½  ¼
c m
½ 
1 ; c m
½ 
2 ; . . .; c m
½ 
q

T
and E ¼ 1; 1; . . .; 1
ð
ÞT are q  1
ð
Þ – vectors.
The vector of generalization coefﬁcients c½m can be found with the help of the
Lagrange undetermined multipliers’ method. For this reason, we’ll introduce a k  g
ð
Þ
– matrix of reference signals and a k  gq
ð
Þ – matrix of ensemble’s output signals
Y k
ð Þ ¼
yT 1
ð Þ
yT 2
ð Þ
..
.
yT k
ð Þ
0
B
B
B
@
1
C
C
C
A; ^Y½m k
ð Þ ¼
^y½mT
1
1
ð Þ
^y½mT
2
1
ð Þ
. . .
^y½mT
q
1
ð Þ
^y½mT
1
2
ð Þ
^y½mT
2
2
ð Þ
  
^y½mT
q
2
ð Þ
...
...
...
...
^y½mT
1
k
ð Þ
^y½mT
2
k
ð Þ
  
^y½mT
q
k
ð Þ
0
B
B
B
B
@
1
C
C
C
C
A
;
a k  g
ð
Þ matrix of innovations
V½m k
ð Þ ¼ Y k
ð Þ  ^Y½m k
ð ÞI  c½m
192
Z. Hu et al.

and the Lagrange function
L½m k
ð Þ ¼ 1
2 Tr V½mT k
ð ÞV½m k
ð Þ


þ k ETc½m  1


¼ 1
2 Tr Y k
ð Þ  ^Y½m k
ð ÞI  c½m

T
Y k
ð Þ  ^Y½m k
ð ÞI  c½m


þ k ETc½m  1


¼ 1
2
X
k
s¼1
y s
ð Þ  ^y½m s
ð Þc½m

2 þ k ETc½m  1


:
ð7Þ
Here I is a g  g
ð
Þ identity matrix,  is the tensor product symbol, k stands for an
undetermined Lagrange multiplier.
Solving the Karush-Kuhn-Tucker system of equations
rc½mL½m k
ð Þ ¼
X
k
s¼1
^y½mT s
ð Þy s
ð Þ þ^y½mT s
ð Þ^y½m s
ð Þc½m


þ kE ¼ ~0;
@L½m k
ð Þ
@k
¼ ETc½m  1 ¼ 0
8
>
>
>
<
>
>
>
:
allows obtaining the desired vector of generalization coefﬁcients as follows
c½m k
ð Þ ¼ c½m k
ð Þ þ P½m k
ð Þ 1  ETc½m k
ð Þ
ETP½m k
ð ÞE
E
ð8Þ
where
P½m k
ð Þ ¼
X
k
s¼1
^y½mT s
ð Þ^y½m s
ð Þ
 
!1
;
c½m k
ð Þ ¼ P½m k
ð Þ
X
k
s¼1
^y½mT s
ð Þy s
ð Þ ¼ P½m k
ð Þp½m k
ð Þ;
8
>
>
>
>
>
<
>
>
>
>
>
:
c½m k
ð Þ is an estimate of the traditional least squares method obtained by the previous k
observations.
In order to research vector properties of the obtained generalization coefﬁcients, we
should make some obvious transformations. Considering that a vector of learning
errors for the neuron GMN½m can be written down in the form
e½m k
ð Þ ¼ y k
ð Þ  ^y½m k
ð Þ ¼ y k
ð Þ  ^y½m k
ð Þc½m ¼ e½m k
ð Þ
¼ y k
ð ÞETc½m  ^y½m k
ð Þc½m ¼
¼
y k
ð ÞET  ^y½m k
ð Þ


c½m ¼ t½m k
ð Þc½m;
the Lagrange function (7) can be also put down in the form
A Multidimensional Adaptive Growing Neuro-Fuzzy System
193

L½m k
ð Þ ¼ 1
2
X
k
s¼1
c½mTt½m s
ð Þt½mT s
ð Þc½m þ k ETc½m  1


¼ 1
2 c½mTR½m k
ð Þc½m þ k ETc½m  1


and then solving a system of equations
rc½mL½m k
ð Þ ¼ R½m k
ð Þc½m þ kE ¼ ~0;
@L½m
@k ¼ ETc½m  1 ¼ 0;
8
>
<
>
:
we receive
c½m k
ð Þ ¼
R½m k
ð Þ

1
E ET R½m k
ð Þ

1
E

1
;
k ¼ 2ET R½m k
ð Þ

1
E
8
>
>
<
>
>
:
where R½m k
ð Þ ¼ P
k
s¼1
t½m s
ð Þt½mT s
ð Þ ¼ V½mT k
ð ÞV½m k
ð Þ:
The Lagrange function’s value can be easily written down at a saddle point
L k
ð Þ ¼
ET R½m k
ð Þ

1
E

1
;
analyzing which by the Cauchy-Schwarz inequality, it can be shown that the gener-
alized output signal ^y½m k
ð Þ is not inferior to accuracy of the best neuron ^y½mj k
ð Þ,
j ¼ 1; 2; . . .; q in an ensemble of output signals.
In order to provide information processing in an online manner, the expression (8)
should be performed in a recurrent form which acquires the view of (by using the
Sherman-Morrison-Woodbery formula)
P m
½  k þ 1
ð
Þ ¼ P m
½  k
ð Þ  P m
½  k
ð Þ^y m
½ T k þ 1
ð
Þ I þ^y m
½  k þ 1
ð
ÞP m
½  k
ð Þ^y m
½ T k þ 1
ð
Þ

1
 ^y½m k þ 1
ð
ÞP½m k
ð Þ ¼
I  P½m k
ð Þ^y½mT k þ 1
ð
Þ^y½m k þ 1
ð
Þ

1
P½m k
ð Þ;
p½m k þ 1
ð
Þ ¼ p½m k
ð Þ þ^y½mT k þ 1
ð
Þy k þ 1
ð
Þ;
c½m k þ 1
ð
Þ ¼ P½m k þ 1
ð
Þp½m k þ 1
ð
Þ;
c½m k þ 1
ð
Þ ¼ c½m k þ 1
ð
Þ þ P½m k þ 1
ð
Þ ETP½m k þ 1
ð
ÞE

1
1  ETc½m k þ 1
ð
Þ


E:
8
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
:
ð9Þ
194
Z. Hu et al.

Unwieldiness of the algorithm (9), that is in fact the Gauss-Newton optimization
procedure, has to do with inversion of g  g
ð
Þ – matrices at every time moment k. And
when this value g is large enough, it is much easier to use gradient learning algorithms
to tune the weight vector c½m k
ð Þ. The learning algorithm can be obtained easily enough
if the Arrow-Hurwitz gradient algorithm is used for a search of the Lagrange function’s
saddle point which takes on the form in this case
c½m k þ 1
ð
Þ ¼ c½m k
ð Þ  gc k þ 1
ð
Þrc½mL½m k
ð Þ;
k k þ 1
ð
Þ ¼ k k
ð Þ þ gk k þ 1
ð
Þ @L½m k
ð Þ
@k
8
>
<
>
:
ð10Þ
or speciﬁcally for (10)
c½m k þ 1
ð
Þ ¼ c½m k
ð Þ þ gc k þ 1
ð
Þ ^y½mT k
ð Þe½m k
ð Þ  k k
ð ÞE


;
k k þ 1
ð
Þ ¼ k k
ð Þ þ gk k þ 1
ð
Þ ETc½m k þ 1
ð
Þ  1


8
>
<
>
:
ð11Þ
where gc k þ 1
ð
Þ, gk k þ 1
ð
Þ are some learning rate parameters.
The Arrow-Hurwitz procedure converges to a saddle point of the Lagrange function
when a range of learning rate parameters gc k þ 1
ð
Þ and gk k þ 1
ð
Þ is sufﬁciently wide.
However, one could try to optimize these parameters to reduce training time. For this
purpose, we should write down the expression (10) in the form
^y½m k
ð Þc½m k þ 1
ð
Þ ¼ ^y½m k
ð Þc½m k
ð Þ  gc k þ 1
ð
Þ^y½m k
ð Þrc½mL½m k
ð Þ;
y k
ð Þ  ^y½m k
ð Þc½m k þ 1
ð
Þ ¼ y k
ð Þ  ^y½m k
ð Þc½m k
ð Þ þ gc k þ 1
ð
Þ^y½m k
ð Þrc½mL½m k
ð Þ:
8
<
:
ð12Þ
A left side of the expression (12) describes an a posteriori error ~e½m k
ð Þ, which is
obtained after one cycle of parameters’ tuning, i.e.
~e½m k
ð Þ ¼ e½m k
ð Þ þ gc k þ 1
ð
Þ^y½m k
ð Þrc½mL½m k
ð Þ:
Introducing the squared norm of this error
~e½m k
ð Þ

2¼
e½m k
ð Þ

2 þ 2gc k þ 1
ð
Þe½mT k
ð Þ^y½m k
ð Þrc½mL½m k
ð Þ
þ g2
c k þ 1
ð
Þ ^y½m k
ð Þrc½mL½m k
ð Þ

2
and minimizing it in gc k þ 1
ð
Þ, i.e. solving a differential equation
@ ~e½m k
ð Þ

2
@gc
¼ 0;
A Multidimensional Adaptive Growing Neuro-Fuzzy System
195

we come to an optimal value for a learning rate parameter
gc k þ 1
ð
Þ ¼  e½mT k
ð Þ^y½m k
ð Þrc½mL½m k
ð Þ
^y½m k
ð Þrc½mL½m k
ð Þ
k
k2
:
Then the algorithms (10) and (11) can be ﬁnally put down as follows
rc½mL k
ð Þ ¼  ^y½mT k
ð Þe½m k
ð Þ  k k
ð ÞE


;
c½m k þ 1
ð
Þ ¼ c½m k
ð Þ þ e½mT k
ð Þ^y½m k
ð Þrc½mL½m k
ð Þ
^y½m k
ð Þrc½mL½m k
ð Þ
k
k2
rc½mL k
ð Þ;
k k þ 1
ð
Þ ¼ k k
ð Þ þ gk k þ 1
ð
Þ ETc½m k þ 1
ð
Þ  1


:
8
>
>
>
>
>
>
<
>
>
>
>
>
>
:
ð13Þ
The procedure (13) is computationally much easier than (9), and if there are no con-
straints (6) it turns into a multidimensional modiﬁcation of the Kaczmarz-Widrow-Hoff
algorithm which is widely spread in the problems of ANNs learning.
Elements of a generalization coefﬁcients’ vector can be interpreted as membership
levels, if a constraint on synaptic weights’ non-negativity for the generalizing neuron
GMN½m is introduced into the Lagrange function to be optimized, i.e.
X
q
j¼1
~c½m
j
¼ ET~c½m ¼ 1;
0 ~c½m
j
 1 8j ¼ 1; 2; . . .; q:
ð14Þ
Introducing the Lagrange function with additional constraints-inequalities
~L½m k
ð Þ ¼ 1
2 Tr V½mT k
ð ÞV½m k
ð Þ


þ k ET~c½m  1


 qT~c½m
¼ 1
2 Tr Y k
ð Þ  ^Y½m k
ð ÞI  ~c½m

T
Y k
ð Þ  ^Y½m k
ð ÞI  ~c½m


þ k ET~c½m  1


 qT~c½m
¼ 1
2
X
k
s¼1
y s
ð Þ  ^y½m s
ð Þ~c½m

2 þ k ET~c½m  1


 qT~c½m
(here q is a q  1
ð
Þ – vector of non-negative undetermined Lagrange multipliers) and
solving the Karush-Kuhn-Tucker system of equations
r~c½m~L½m k
ð Þ ¼ ~0;
@~L½m k
ð Þ
@k
¼ 0;
qj 	 0 8j ¼ 1; 2; . . .; q;
8
>
>
>
<
>
>
>
:
196
Z. Hu et al.

an analytical solution takes on the form
~c½m k
ð Þ ¼ P½m k
ð Þ p½m k
ð Þ  kE þ q


;
k ¼ ETP½m k
ð Þp½m k
ð Þ  1 þ ETP½m k
ð Þq
ETP½m k
ð ÞE
8
>
>
<
>
>
:
and having used the Arrow-Hurwicz-Uzawa procedure, we obtain a learning algorithm
of the neuron GMN½m in the view of
~c½m k þ 1
ð
Þ ¼ c½m k þ 1
ð
Þ  P½m k þ 1
ð
Þ ETc½m k þ 1
ð
Þ  1 þ ETP½m k þ 1
ð
Þq k
ð Þ
ETP½m k þ 1
ð
ÞE
E
þ P½m k þ 1
ð
Þq k
ð Þ;
q k þ 1
ð
Þ ¼ Pr þ q k
ð Þ  gq k þ 1
ð
Þ~c½m k þ 1
ð
Þ


:
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
ð15Þ
The ﬁrst ratio (15) can be transformed into the form of
~c½m k þ 1
ð
Þ ¼ c½m k þ 1
ð
Þ  P½m k þ 1
ð
Þ ETP½m k þ 1
ð
Þq k
ð Þ
ETP½m k þ 1
ð
ÞE E þ P½m k þ 1
ð
Þq k
ð Þ
¼ c½m k þ 1
ð
Þ þ
I  P½m k þ 1
ð
ÞEET
ETP½m k þ 1
ð
ÞE


P½m k þ 1
ð
Þq k
ð Þ
ð16Þ
where c½m k þ 1
ð
Þ is deﬁned by the ratio (8),
I  P½m k þ 1
ð
ÞEET ETP½m k þ 1
ð
ÞE

1


is a projector to the hyperplane ~c½mT k þ 1
ð
ÞE ¼ 1. It can be easily noticed that the
vectors E and
I  P½m k þ 1
ð
ÞEET ETP½m k þ 1
ð
ÞE

1


P½m k þ 1
ð
Þq k
ð Þ are orthogo-
nal, so we can write down the ratios (14) and (15) in a simpler form
~c½m k þ 1
ð
Þ ¼ c½m k þ 1
ð
Þ þ Prc½mTE¼1 P½m k þ 1
ð
Þq k
ð Þ


;
q k þ 1
ð
Þ ¼ Pr þ q k
ð Þ  gq k þ 1
ð
Þ~c½m k þ 1
ð
Þ


:
8
>
<
>
:
Then the learning algorithm of the generalizing neuron with the constraints (14) ﬁnally
takes on the form
A Multidimensional Adaptive Growing Neuro-Fuzzy System
197

P½m k þ 1
ð
Þ ¼ P½m k
ð Þ  P½m k
ð Þ^y½mT k þ 1
ð
Þ I þ^y½m k þ 1
ð
ÞP½m k
ð Þ^y½mT k þ 1
ð
Þ

1
¼
I  P½m k
ð Þ^y½mT k þ 1
ð
Þ^y½m k þ 1
ð
Þ

1
P½m k
ð Þ;
p½m k þ 1
ð
Þ ¼ p½m k
ð Þ þ^y½mT k þ 1
ð
Þy k þ 1
ð
Þ;
c½m k þ 1
ð
Þ ¼ P½m k þ 1
ð
Þp½m k þ 1
ð
Þ;
c½m k þ 1
ð
Þ ¼ c½m k þ 1
ð
ÞP½m k þ 1
ð
Þ ETP½m k þ 1
ð
ÞE

1
1  ETc½m k þ 1
ð
Þ


E;
~c½m k þ 1
ð
Þ ¼ c½m k þ 1
ð
Þ  P½m k þ 1
ð
Þ ETP½m k þ 1
ð
Þq k
ð Þ
ETP½m k þ 1
ð
ÞE E þ P½m k þ 1
ð
Þq k
ð Þ;
q k þ 1
ð
Þ ¼ Pr þ q k
ð Þ  gq k þ 1
ð
Þ~c½m k þ 1
ð
Þ


:
8
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
>
:
ð16Þ
The learning procedure (16) can be considerably simpliﬁed similar to the previous one
with the help of the gradient algorithm
~c½m k þ 1
ð
Þ ¼ ~c½m k
ð Þ  gc k þ 1
ð
Þr~c½m~L½m k
ð Þ;
k k þ 1
ð
Þ ¼ k k
ð Þ þ gk k þ 1
ð
Þ ET~c½m k þ 1
ð
Þ  1


;
q k þ 1
ð
Þ ¼ Pr þ q k
ð Þ  gq k þ 1
ð
Þ~c½m k þ 1
ð
Þ


:
8
>
>
>
<
>
>
>
:
Carrying out transformations similar to the abovementioned ones, we ﬁnally obtain
r~c½m~L k
ð Þ ¼  ^y½mT k
ð Þe½m k
ð Þ  k k
ð ÞE þ q k
ð Þ


;
~c½m k þ 1
ð
Þ ¼ ~c½m k
ð Þ þ e½mT k
ð Þ^y½m k
ð Þr~c½m~L½m k
ð Þ
^y½m k
ð Þr~c½m~L½m k
ð Þ

2
r~c½m~L½m k
ð Þ;
k k þ 1
ð
Þ ¼ k k
ð Þ þ gk k þ 1
ð
Þ ET~c½m k þ 1
ð
Þ  1


;
q k þ 1
ð
Þ ¼ Pr þ q k
ð Þ  gq k þ 1
ð
Þ~c½m k þ 1
ð
Þ


:
8
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
:
ð17Þ
The algorithm (17) comprises the procedure (13) as a particular case.
4
Experimental Results
To illustrate the effectiveness of the suggested adaptive neuro-fuzzy system and its
learning procedures, we have actualized an experimental test by means of handling the
chaotic Lorenz attractor identiﬁcation. The Lorenz attractor is a fractal structure which
matches the Lorenz oscillator’s behavior. The Lorenz oscillator is a three-dimensional
dynamical system that puts forward a chaotic ﬂow that is also renowned for its lem-
niscate shape. As a matter of fact, a state of the dynamical system (three variables of the
198
Z. Hu et al.

three-dimensional system) is evolving with the course of time in a complex
non-repeating pattern.
The Lorenz attractor may be exempliﬁed by a differential equation in the form of
_x ¼ r y  x
ð
Þ;
_y ¼ x r  z
ð
Þ  y;
_z ¼ xy  bz:
8
>
<
>
:
ð18Þ
This system of Eq. (18) can be also put down in the recurrent form
x i þ 1
ð
Þ ¼ x ið Þ þ r y ið Þ  x ið Þ
ð
Þdt;
y i þ 1
ð
Þ ¼ y ið Þ þ rx ið Þ  x ið Þz ið Þ  y ið Þ
ð
Þdt;
z i þ 1
ð
Þ ¼ z ið Þ þ x ið Þy ið Þ  bz ið Þ
ð
Þdt
8
>
<
>
:
ð19Þ
where parameter values are: r ¼ 10; r ¼ 28; b ¼ 2:66; dt ¼ 0:001.
A data set was acquired with the beneﬁt of (19) which comprises 10000 samples,
where 7000 points establish a training set, and 3000 samples make up a validation set.
In our system, we had 2 cascades containing 2 multidimensional neurons each and
a generalized neuron in each cascade. The ﬁrst neuron in each cascade involves 2
membership functions. The graphical results are represented in Figs. 4, 5 and 6. One
can basically see the forecasting results for the last cascade in Table 1.
Fig. 4. Identiﬁcation by means of the Lorenz attractor. The X-component results.
A Multidimensional Adaptive Growing Neuro-Fuzzy System
199

Fig. 5. Identiﬁcation by means of the Lorenz attractor. The Y-component results.
Fig. 6. Identiﬁcation by means of the Lorenz attractor. The Z-component results.
200
Z. Hu et al.

5
Conclusion
The hybrid growing neuro-fuzzy architecture and its learning algorithms for the mul-
tidimensional growing hybrid cascade neuro-fuzzy system which enables neuron
ensemble optimization in every cascade were considered and introduced in the article.
The most important privilege of the considered hybrid neuro-fuzzy system is its trait to
accomplish a procedure of parallel computation for a data stream based on peculiar
elements with upgraded approximating properties. The developed system turns out to
be rather easy from the effectuation standpoint; it holds a high processing speed and
approximating features. It can be described by a rather high training speed which
makes it possible to process online sequential data. The distinctive feature of the
introduced system is the fact that every cascade is put together by an ensemble of
neurons, and their outputs are joined with the optimization procedure of a speciﬁc sort.
Thus, every cascade produces an output signal of the optimal accuracy. The proposed
system, which is ultimately a growing (evolving) system of computational intelligence,
assures processing the incoming data in an online fashion just unlike the rest of con-
ventional systems.
Acknowledgment. This research project is partially subvented by RAMECS and CCNU16A
02015.
References
1. da Silva, I.N., Spatti, D.H., Flauzino, R.A., Bartocci Liboni, L.H., dos Reis Alves, S.F.:
Artiﬁcial Neural Networks: A Practical Course. Springer, Cham (2017)
2. Cartwright, H.: Artiﬁcial Neural Networks. Springer, New York (2015)
3. Suzuki, K.: Artiﬁcial Neural Networks: Architectures and Applications. InTech, New York
(2013)
4. Koprinkova-Hristova, P., Mladenov, V., Kasabov, N.K.: Artiﬁcial Neural Networks:
Methods and Applications in Bio-/Neuroinformatics. Springer, Cham (2015)
5. Borowik, G., Klempous, R., Nikodem, J., Jacak, W., Chaczko, Z.: Advanced Methods and
Applications in Computational Intelligence. Springer, Cham (2014)
6. Graupe, D.: Principles of Artiﬁcial Neural Networks. Advanced Series in Circuits and
Systems. World Scientiﬁc Publishing Co. Pte. Ltd., Singapore (2007)
7. Haykin, S.: Neural Networks and Learning Machines, 3rd edn. Prentice Hall, New Jersey
(2009)
Table 1. Table of forecasting results
RMSE (the whole data set) 0.03453924
RMSE (on an interval when the last
cascade was added)
Neuron1
0.01954961
Neuron2
0.01975597
A generalized output
0.0191146
A Multidimensional Adaptive Growing Neuro-Fuzzy System
201

8. Hanrahan, G.: Artiﬁcial Neural Networks in Biological and Environmental Analysis. CRC
Press, Boca Raton (2011)
9. Lughofer, E.: Evolving Fuzzy Systems and Methodologies: Advanced Concepts and
Applications. Springer, Heidelberg (2011)
10. Angelov, P., Filev, D., Kasabov, N.: Evolving Intelligent Systems: Methodology and
Applications. Willey, Hoboken (2010)
11. Kasabov, N.: Evolving Connectionist Systems. Springer, London (2003)
12. Fahlman, S., Lebiere, C.: The cascade-correlation learning architecture. Adv. Neural. Inf.
Process. Syst. 2, 524–532 (1990)
13. Avedjan, E.D., Barkan, G.V., Levin, I.K.: Cascade neural networks. J. Avtomatika i
Telemekhanika 3, 38–55 (1999)
14. Prechelt, L.: Investigation of the CasCor family of learning algorithms. Neural Netw. 10,
885–896 (1997)
15. Bodyanskiy, Y., Dolotov, A., Pliss, I., Viktorov, Y.: The cascaded orthogonal neural
network. Int. J. Inf. Sci. Comput. 2, 13–20 (2008)
16. Bodyanskiy, Y., Viktorov, Y.: The cascaded neo-fuzzy architecture using cubic-spline
activation functions. Int. J. Inf. Theor. Appl. 16(3), 245–259 (2009)
17. Bodyanskiy, Y., Viktorov, Y.: The cascaded neo-fuzzy architecture and its on-line learning
algorithm. Int. J. Intel. Process. 9, 110–116 (2009)
18. Bodyanskiy, Y., Viktorov, Y., Pliss, I.: The cascade growing neural network using quadratic
neurons and its learning algorithms for on-line information processing. Int. J. Intell. Inf. Eng.
Syst. 13, 27–34 (2009)
19. Kolodyazhniy, V., Bodyanskiy, Y.: Cascaded multi-resolution spline-based fuzzy neural
network. In: Angelov, P., Filev, D., Kasabov, N. (eds.) Proceedings of the International
Symposium on Evolving Intelligent Systems, pp. 26–29. De Montfort University, Leicester
(2010)
20. Bodyanskiy, Y., Kharchenko, O., Vynokurova, O.: Hybrid cascaded neural network based
on wavelet-neuron. Int. J. Inf. Theor. Appl. 18(4), 335–343 (2011)
21. Bodyanskiy, Y., Grimm, P., Teslenko, N.: Evolving cascaded neural network based on
multidimensional Epanechnikov’s kernels and its learning algorithm. Int. J. Inf. Technol.
Knowl. 5(1), 25–30 (2011)
22. Bodyanskiy, Y., Vynokurova, O., Teslenko, N.: Cascaded GMDH-wavelet-neuro-fuzzy
network. In: Proceedings of the 4th International Workshop on Inductive Modelling, Kyiv,
pp. 22–30 (2011)
23. Bodyanskiy, Y., Vynokurova, O., Dolotov, A., Kharchenko, O.: Wavelet-neuro-fuzzy-
network structure optimization using GMDH for the solving forecasting tasks. In: Proceedings
of the International Workshop Inductive Modelling, Kyiv, pp. 61–67 (2013)
24. Bodyanskiy, Y., Tyshchenko, O., Kopaliani, D.: A hybrid cascade neural network with an
optimized pool in each cascade. Soft. Comput. 19(12), 3445–3454 (2015)
25. Bodyanskiy, Y., Tyshchenko, O., Kopaliani, D.: An evolving connectionist system for data
stream fuzzy clustering and its online learning. Neurocomputing (2017). http://www.
sciencedirect.com/science/article/pii/S0925231217309785
26. Bodyanskiy, Y., Tyshchenko, O., Kopaliani, D.: Adaptive learning of an evolving cascade
neo-fuzzy system in data stream mining tasks. Evolving Syst. 7(2), 107–116 (2016)
27. Bodyanskiy, Y., Tyshchenko, O., Deineko, A.: An evolving radial basis neural network with
adaptive learning of its parameters and architecture. Autom. Control Comput. Sci. 49(5),
255–260 (2015)
28. Hu, Z., Bodyanskiy, Y.V., Tyshchenko, O.K., Boiko, O.O.: An evolving cascade system
based on a set of neo-fuzzy nodes. Int. J. Intell. Syst. Appl. (IJISA) 8(9), 1–7 (2016)
202
Z. Hu et al.

29. Bodyanskiy, Y., Tyshchenko, O., Kopaliani, D.: A multidimensional cascade neuro-fuzzy
system with neuron pool optimization in each cascade. Int. J. Inf. Technol. Comput. Sci.
(IJITCS) 6(8), 11–17 (2014)
30. Miki, T., Yamakawa, T.: Analog implementation of neo-fuzzy neuron and its on-board
learning. In: Mastorakis, N.E. (ed.) Computational Intelligence and Applications, pp. 144–
149. WSES Press, Piraeus (1999)
31. Uchino, E., Yamakawa, T.: Soft computing based signal prediction, restoration and ﬁltering.
In: Ruan, D. (ed.) Intelligent Hybrid Systems: Fuzzy Logic, Neural Networks, and Genetic
Algorithms, pp. 331–349. Kluwer Academic Publishers, Boston (1997)
32. Yamakawa, T., Uchino, E., Miki, T., Kusanagi, H.: A neo fuzzy neuron and its applications
to system identiﬁcation and prediction of the system behavior. In: Proceedings of the 2nd
International Conference on Fuzzy Logic and Neural Networks, pp. 477–483 (1992)
33. Hoff, M., Widrow, B.: Adaptive switching circuits. In: Neurocomputing: Foundations of
Research, pp. 123–134 (1988)
34. Kaczmarz, S.: Approximate solution of systems of linear equations. Int. J. Control 53, 1269–
1271 (1993)
35. Ljung, L.: System Identiﬁcation: Theory for the User. Prentice Hall, Upper Saddle River
(1999)
36. Bodyanskiy, Y., Tyshchenko, O., Wojcik, W.: A multivariate non-stationary time series
predictor based on an adaptive neuro-fuzzy approach. Elektronika - konstrukcje, technologie,
zastosowania 8, 10–13 (2013)
37. Caminhas, W.M., Silva, S.R., Rodrigues, B., Landim, R.P.: A neo-fuzzy-neuron with real
time training applied to ﬂux observer for an induction motor. In: Proceedings 5th Brazilian
Symposium on Neural Networks, pp. 67–72 (1998)
38. Bodyanskiy, Y.V., Pliss, I.P., Solovyova, T.V.: Multistep optimal predictors of multidi-
mensional non-stationary stochastic processes. Doklady AN USSR A(12), 47–49 (1986)
39. Bodyanskiy, Y.V., Pliss, I.P., Solovyova, T.V.: Adaptive generalized forecasting of
multidimensional stochastic sequences. Doklady AN USSR A(9), 73–75 (1989)
40. Bodyanskiy, Y., Pliss, I.: Adaptive generalized forecasting of multivariate stochastic signals.
In: Proceedings of the International Conference on Latvian Signal Processing, vol. 2, pp. 80–
83 (1990)
A Multidimensional Adaptive Growing Neuro-Fuzzy System
203

Method of Integration and Content Management
of the Information Resources Network
Olga Kanishcheva1(✉), Victoria Vysotska2, Lyubomyr Chyrun2,
and Aleksandr Gozhyj3
1 National Technical University “KhPI”, Kharkiv, Ukraine
kanichshevaolga@gmail.com
2 Lviv Polytechnic National University, Lviv, Ukraine
{Victoria.A.Vysotska,Lyubomyr.V.Lyubomyr}@lpnu.ua
3 Black Sea National University named after Petro Mohyla, Nikolaev, Ukraine
alex.gozhyj@gmail.com
Abstract. The paper describes the integrated processing method of heteroge‐
neous information resources of web systems, which allows for further integration
and management. Our method involves the decomposition of the overall process
on subprocesses of value integration, data syntax, semantics, and structure. The
advantage of this approach to integration processes is the ability to execute them
at the metadata level. It allows reducing the number of hits to the data of web
systems, the volumes of which can be signiﬁcant. The paper presents a model of
the content life cycle in web systems. The model describes the processes of
processing information resources in web systems and simpliﬁes the technology
of integrated automation and content management. In this work, the main prob‐
lems of functional services of integration and content management were analyzed.
The proposed method gives the opportunity to create tools for working out of
information resources in web systems and implement the subsystem of integration
and content management.
Keywords: Content · Content analysis · Content monitoring · Content retrieval ·
Web system · Web resource · Data integration · Distributed data systems ·
Heterogeneous data · Business process · Content management system · Content
lifecycle
1
Introduction
One of the main features of our time is a constant growth rate of production information
[1–3]. This process is objective and usually deﬁnitely positive [4–8]. However, today
humanity faced with paradoxical situation. Progress in the information ﬁeld leads to a
decrease in the general level of awareness [9–11]. Besides the problem of information
increasing, there were a number of speciﬁc problems related with the rapid development
of information technology [12, 13]. It caused the such problems as: disproportionate
increase in information noise due to lack of structure of the information, appearance of
parasitic information (received as attachments), lack of relevance of formally relevant
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_14

(thematically appropriate) information to the actual needs of consumers, multiple dupli‐
cation of information (a typical example is the publication of the same message in
multiple publications) [14–18]. Because of these reasons, the traditional information
retrieval system gradually began to lose its relevance. The reason is not in physical
volumes of information ﬂow, but in their dynamics and a constant systematic recovery
of information. This is not always apparent regularity. Media generates large dynamic
information ﬂows and requires new approaches for information processing [19–24]. The
solution can only be found in automation of identifying the most important components
in content streams [26, 27]. That is why resource-monitoring systems closely related to
content analysis. They have used over the recent years. This promising trend is obtained
through content monitoring. Its appearance was caused primarily by challenges in the
systematic tracking of trends and processes in an updated environment. Content moni‐
toring often realizes a meaningful analysis of information ﬂows in order to obtain the
necessary qualitative and quantitative sections. However, the quality is not constant over
a predeﬁned period [4–8]. The most important methodological component content
monitoring is content analysis [9–13].
In our work we propose the architecture of intellectual system of content formation
and additional modules of content processing. It allows automating of identifying the
most important components in content streams.
The paper is organized as follows: Sect. 2 discusses related work and summarizes
diﬀerent approaches for creation of intellectual information systems of content forma‐
tion. In Sects. 3 and 4 we describe our method for content creation, general scheme of
content management module and our experiments analysis. Finally, in Sect. 5 we present
the conclusion and future work.
2
Related Works
Information system – a set of organizational and technical means for storing and
processing content in order to ensure information needs of users. On the Internet, the
information system or web-based system acts as a collection of websites that are part of
the same network for a particular factor [28–36]. A website is a collection of web pages
available on the Internet which are combined both in content and navigation. Physically,
a website can be hosted on one or several servers. The network of websites on the theme
of “urban sites” should cover the cities of Ukraine. There are many information systems
which specialize in submitting pages related to cities. In the Internet environment, there
are three types of traﬃc to the site. The ﬁrst is from the search engines, the second is
the site navigation, the third is the navigation from the bookmarks and direct hits to the
site through the URL of the browser’s tape. We will focus on the ﬁrst type of traﬃc for
this Internet project. For this, customers need to update the website in time and add only
the current content; a moderator cannot do it manually because of information changes
in the Internet very fast space.
The work [37] introduce the current state of Internet provider development of tourist
services. Also, the factors which are an inﬂuence on the future of Internet tourism: the
institutional tension; competition for end users and investors; end-user expectations. It’s
Method of Integration and Content Management
205

important for further developing the provision of tourist services in Europe, where the
Internet plays a signiﬁcant role. This situation is facilitated by the rapid processing of
Internet information, which needs to be integrated in a timely manner from reliable
sources – information resources. The paper [38] deﬁnes a number of key changes in
information and communication technologies that gradually change the tourism
industry. Electronic tourism and the Internet, in particular, support interaction between
tourism companies and consumers, and as a result, they form a new approach to the
entire process of developing, managing and marketing tourist products and destinations
[39]. Therefore, all interested parties related to tourism and hospitality, gradually see
how their role changes, new opportunities and challenges emerge. Authors in work [40]
demonstrate the future of Internet tourism and show that the Internet tourism is centered
on consumer-centric technology to provide the services of new experienced and prompt
consumers. Therefore, traﬃc strategies are needed both at strategic and tactical levels
of management to develop an “information structure” for tourism organizations to
manage their internal functions, their relationships with partners, and their interaction
with all stakeholders, including consumers. Only those organizations that appreciate the
opportunities that modern ITs successfully bring and manage their resources in Internet
tourism can enhance their innovation and competitiveness in the future [41]. The e-
tourism industry is an experienced provider, and more of this experience needs to be
customized. Authors [42] analyze the using of Data Mining in Internet marketing for
the ﬁeld of e-tourism and e-management of customer relationships. In particular,
proﬁling clients, query routing, email ﬁltering, online auctions, and updates to electronic
directories are explained. But there are a number of problems related to the implemen‐
tation of the integration method and content management in the ﬁeld of Internet tourism.
These problems are (i) unstructured and the lack of one template sources of information;
(ii) the lack of research on demand for categories of operational tourist information
among consumers; (iii) the lack of requirements for the overall architecture of the inte‐
gration system, management, processing and (iv) providing the end user with timely and
reliable tourist information.
3
Our Method of Integration and Content Management
Web-system of the network consists of subsites, connected in a network. It should be
built in a modern style with the necessary functionality, optimized for search engines
and services, distributed by servers. The model of such a Web-system presents as
W =< X, C, T, H, S, U, Q, Y, 𝛼, 𝛽>,
(1)
where X = {x1, x2, … , xn} – incoming integrated data from various information
resources; C = {c1, c2, … , cm} – the generated content (information content, news, arti‐
cles, restaurants, etc.); T = {t1, t2, … , tl} – time indicator of the relevance of the gener‐
ated content to maintain its relevance; H = {h1, h2, … , hp} – a list of classiﬁed URLs of
information resources; S = {s1, s2, … , sk} – social content (results of conducting groups
in social networks, repositions of records, etc.); U = {u1, u2, … , ur} – UGC or results of
206
O. Kanishcheva et al.

the content generated by users (the ability to add content to the site through the promo‐
tion, as well as comments without promotion); Q = {q1, q2, … , qw} – user requests of
the necessary Web-system content; Y = {y1, y2, … , yf} – relevant content as a result of
system user queries; 𝛼 – process of necessary content integration from diﬀerent infor‐
mation sources; 𝛽 – a process of content management to generate lists of relevant content
according to user requests of the web system.
The formation of a list of relevant content directly proportional to the competently
rules for integrating data from diﬀerent sources Y = 𝛽◦𝛼 (ﬁltering, spamming and dupli‐
cation, classiﬁcation, etc.), i.e. Y
Y = 𝛽(Q, C, S, U, 𝛼(X, H, T)).
(2)
The process of integrating data from diﬀerent sources provide a superposition of
functions
C = 𝛼(X, H, T) = 𝛼7◦𝛼6◦𝛼5◦𝛼4◦𝛼3◦𝛼2◦𝛼1,
(3)
where 𝛼1 – collection of data from various sources, predetermined by the system moder‐
ator; 𝛼2 – ﬁltering the collected data (dividing the content into two arrays of recognized/
unrecognized, or identiﬁed/not identiﬁed, with the former working on the system, with
the other – the moderator); 𝛼3 – spamming and elimination (check with dictionaries of
ﬁlters for blocked linguistic variables); 𝛼4 – duplication selection and its elimination
(checking with existing content in the database); 𝛼5 – data formatting in accordance with
the requirements laid down in the particular web system (most often in the XML format);
𝛼6 – a general classiﬁcation (for example, content of a type of restaurant, accommoda‐
tion, location, tourism, help, etc.), if necessary, a detailed classiﬁcation (for example,
for the content of the type of restaurant – cafes, bars, pubs, restaurants, pizzerias, bistros,
clubs, coworking, time-clubs) and the storage of relevant data; 𝛼7 – display of new data
on the information resource of the web system.
The content management process of the web system gives a superposition
Y = 𝛽6◦𝛽5◦𝛽4◦𝛽3◦𝛽2◦𝛽1,
(4)
where 𝛽1 – the function of processing the user’s request (identiﬁcation, that is, spam/
non-spam, with no spam save (for further analysis of statistics hits and searches), spam
save the request for further analysis by the moderator, the allocation of word markers
(classiﬁcation linguistic variables) and the classiﬁcation, that is, to which column of the
content belongs (for example, the search for the restaurant), 𝛽2 – the selection of data
from the database of the web system according to the words speciﬁed in the request of
the markers and the formation relevant content; 𝛽3 – sorting relevant content before
displaying certain criteria (frequency, reading time, for registered users, taking into
account their preferences, etc.); 𝛽4 – displaying sorted relevant content; 𝛽5 – collecting
and storing statistics about the user’s activity with the displayed relevant user; 𝛽6 – a
recount of the content history, according to the latest statistics of user manipulation.
Method of Integration and Content Management
207

The Fig. 1 shows the conceptual model of city network. The network is divided on
sites by the speciﬁc context.
Network of city sites
Site 1
Site 2
Site N
…………
Content
UGC
Social 
component
Content
UGC
Social 
component
Content
UGC
Social 
component
Fig. 1. The conceptual model of information system “Networks of city sites”
The Fig. 2 presents the usage diagram, where we present all actors for our information
system and all action variants.
View
content
Commentation
Content 
evaluation
Voting
Editing 
content
Moderation
User
Admini-
strator
Search in the 
database
Saving to the 
database
Fig. 2. The usage diagram of information system “Networks of city sites”
The Fig. 3 shows classes and attributes of information system “Networks of city
sites”. The network consists of sites, where each site consists of certain classes: content,
UGC, social component etc. The diagrams (Figs. 4 and 5) present the structure of one
site throughout the network, it includes the “Functional Site” package, “Restaurants”,
“Locations”, “Accommodation”, “Tourism” etc.
Sites
+ field: ID
+ field: Name
+ field: URL
+method(type): delete
+method(type): add
+method(type): edit
Site content
+ field: ID
+ field: Section
+ field: Categories
+ field: Materials
+ field: Users
+ field: Information
+ field: Site name
+method(type): delete
+method(type): add
+method(type): edit
UGS
+ field: ID
+ field: Comments
+ field: User
+ field: Content
+method(type): delete
+method(type): add
+method(type): edit
UGS
+ field: Id
+ field: Social networks
+ field: User
+ field: Content
+method(type): share
Fig. 3. The class diagram of information system “Networks of city sites”
208
O. Kanishcheva et al.

Site
Choice
Subscription
Successfully
Error
Gateway page
Comments
Control
Successfully
Error
Viewing content
yes
yes
no
no
Fig. 4. The activity diagram for customer functions of system “Networks of city sites”
Network of sites
Articles
Site functional
Restaurants
Residence
Locations
UGS
Social component
Adding
information
Other
Help
Tourism
Tourist firms
Consulates
Tourist Information Center
Coffee houses
Cafe bars
Pubs
Restaurants
Pizzerias / Bistro
Clubs
Coworking / Time Clubs
Hotels
Motels
Hostels
Museums
Theaters
Galleries
Cinemas
Concert halls
Sports facilities
Recreation
Attractions
City map
Bus timetable
Public transport map
Institutions and 
organizations
Medicine
Services
Banks and Finance
Shopping
Services
Education
Trade
Fig. 5. The package diagram of information system “Networks of city sites”
Information System is designed to solve the problem of ﬁnding the necessary infor‐
mation in the city. In Fig. 6 shows the database structure of the site for one city.
1. Content – the main table, which contains all information about the city from all
Sections.
2. Category – a table with a list of partitions, the cat_sub attribute will help to make a
recursive subcategory output.
3. Ratings – a table with material ratings. In this case, it’s the “I like” button. In the
rating_points attribute +1 is added when voting.
4. Comments – a table with comments on the site.
5. User – a table about all registered users.
6. Pages – a separate table from the general functional for static pages, for example,
such as bus schedules, contacts, etc.
7. Polls – a table about voting and voting lists in the widget on the site.
Method of Integration and Content Management
209

Fig. 6. The example of database structure
4
Experiments and Results
The Fig. 7 shows the prototype of structure and site makeup for all networks.
Fig. 7. The prototype of site makeup for the ﬁrst site
The design is divided into two columns. The main content and side bar. The sidebar
has navigation and widgets of diﬀerent types, and the main content. Each site has its
own structure for each city. And it is divided into the following main sections: Restau‐
rants, Accommodation, Locations, Tourism, Help, Gallery and subsidiaries. The
example of the site section “Restaurants” is shown in the Fig. 8a and the example of the
site section interface “Accommodation” is shown in the Fig. 8b.
210
O. Kanishcheva et al.

Fig. 8. The site sections “Restaurants” and “Accommodation”
The site also contains a morphological search (Fig. 9).
Fig. 9. The example of site with search form and results.
Our site has the polling module, social functions such as authorization, commentary,
subscription to materials, evaluation of materials and the photo gallery (Fig. 10).
Fig. 10. The photo gallery of our site.
Our site also has information widgets with the Private API and the Yandex API
(weather). Data from the API is collected into the database, and then outputs with using
the user’s geolocation, and upload on the server is reduced. Each site has registration,
proﬁle and a form for adding materials. The main feature is function search of similar
Method of Integration and Content Management
211

materials; it is generated by the collected data from the user. On the base of these data,
we can receive similar materials (Fig. 11).
Fig. 11. The window with similar materials.
We used the loadimpact.com service for load testing of our site. The results are shown
in Figs. 12 and 13. The server operates in regular mode with 25 simultaneous connec‐
tions.
Fig. 12. Results of load testing, part 1.
The next step in testing is the validation of the web portal for validity, using the free
service onlinewebcheck.com. We received 0 errors, but 22 attention messages.
The possibility of this information system is scalability. It can be easily scaled for
any site and under any subject. Everything is done for maximum speed and work with
large data sets. One of the best features of the information system in comparison with
analogs are:
• speed of page generation;
• an SSL certiﬁcate and TLS encryption;
• better content (by editors), not just a parser;
212
O. Kanishcheva et al.

• lack of news, all the news ﬂowed into groups in social networks;
• there are no such inactive sections as a bulletin board, a directory with vacancies,
real estate, etc. Since these sections are basically a parser from sites that are monop‐
olies (olx.ua, work.ua, lun.ua etc.);
• mobile site layout without dubbing content on a subdomain;
• RSS feed for each section;
• the main focus will be on social networks and visitors (mobile users) from these
networks.
5
Conclusions and Future Work
This work describes information about the structure of our information system, UML
diagrams and the subject area. We show methods and instruments for solving the
problem of building an information system of urban site network, and also show the
advantages of these tools. We have created a single information base, which allows you
to ﬁnd and provide the necessary information about certain cities of Ukraine. Such a
system allows users to observe the weather, look for a timetable for public transport,
view the movie poster in theaters, news, browse the directories of available phones, the
catalog of restaurants, all about tourism, city photos, monuments, addresses of institu‐
tions etc. In this case, the information system is an interactive catalog with dynamic
operational information, designed as a reference for each site.
References
1. Mobasher, B.: Data mining for web personalization. In: The Adaptive Web, pp. 90–135.
Springer, Heidelberg (2007)
2. Dinucă, C.E., Ciobanu, D.: Web Content Mining, vol. 85. University of Petroşani, Economics
(2012)
3. Xu, G., Zhang, Y., Li, L.: Web content mining. In: Web Mining and Social Networking, pp.
71–87. Springer, Boston (2011)
Fig. 13. Results of load testing, part 2.
Method of Integration and Content Management
213

4. Lande, D., Furashev, V., Braychevskiy, S., Grigoriyev, O.: Osnovy modelirovaniya i otsenki
yelektronnykh informatsionnykh potokov. Inzhiniring, Kyiv (2006)
5. Lande, D.: Nekotoriye metody analiza novostnykh informatsionnykh potokov. IKVT-2005,
vol. 93, pp. 277–287. DonNTU, Donetsk (2005)
6. Lande, D.: Skaner sistemy kontent-monitoringa, Otkrytyye informatsionnyye i kompyuterny
integrirovannyye tekhnologii, vol. 28, pp. 53–58. NAKU «KHAI», Kharkov (2005)
7. Bolshakova, Y., Klyshinskiy, E., Lande, D., Noskov, A., Peskova, O., Yagunova, Y.:
Avtomaticheskaya obrabotka tekstov na yestestvennom yazyke i kompyuternaya lingvistika.
MIEM, Moskva (2011)
8. Grigoriyev, A., Lande, D.: Adaptivnyy interfeys utochneniya zaprosov k sisteme kontent-
monitoringa. In: KHÍ mezhdunarodnaya nauchno-prakticheskaya konferentsiya, pp. 17–20.
UkrINTEI, Kyiv (2005)
9. Vysotska, V., Chyrun, L., Chyrun, L.: Information technology of processing information
resources in electronic content commerce systems. In: Computer Science and Information
Technologies, CSIT 2016, pp. 212–222 (2016)
10. Vysotska, V., Chyrun, L., Chyrun, L.: The commercial content digest formation and
distributional process. In: Proceedings of the XI-th International Conference on Computer
Science and Information Technologies, CSIT 2016, pp. 186–189 (2016)
11. Vysotska, V., Chyrun, L.: Analysis features of information resources processing. In:
Proceedings of the X-th International Conference on Computer Science and Information
Technologies, CSIT 2015, pp. 124–128 (2015)
12. Vysotska, V., Chyrun, L., Lytvyn, V., Dosyn, D.: Methods Based on Ontologies for
Information Resources Processing. LAP Lambert Academic Publishing, Germany (2016)
13. Lytvyn, V., Pukach, P., Bobyk, I., Vysotska, V.: The method of formation of the status of
personality understanding based on the content analysis. East. Eur. J. Enterp. Technol.
5/2(83), 4–12 (2016)
14. McGovern, G., Norton, R.: Content Criticsl. FT Press, Upper Saddle River (2001)
15. McKeever, S.: Understanding Web content management systems: evolution, life cycle and
market. Ind. Manage. Data Syst. 103(9), 686–692 (2003)
16. Rockley, A.: Managing Enterprise Content: A Uniﬁed Content Strategy. New Riders Press,
Reading (2002)
17. Khomytska, I., Teslyuk, V.: The Method of statistical analysis of the scientiﬁc, colloquial,
belles-lettres and newspaper styles on the phonological level. Adv. Intell. Syst. Comput.
512, 149–163 (2017)
18. Khomytska, I., Teslyuk, V.: Speciﬁcs of phonostatistical structure of the scientiﬁc style in
English style system. In: Proceedings of the XI-th International Conference on Computer
Science and Information Technologies, CSIT 2016, pp. 129–131 (2016)
19. Lytvyn, V., Vysotska, V., Veres, O., Rishnyak, I., Rishnyak, H.: Classiﬁcation methods of
text documents using ontology based approach. In: Advances in Intelligent Systems and
Computing, vol. 512, pp. 229–240. Springer, Cham (2017)
20. Vysotska, V.: Linguistic analysis of textual commercial content for information resources
processing. In: Modern Problems of Radio Engineering, Telecommunications and Computer
Science, TCSET 2016, pp. 709–713 (2016)
21. Lytvyn, V., Vysotska, V., Pukach, P., Bobyk, I., Pakholok, B.: A method for constructing
recruitment rules based on the analysis of a specialist’s competences. East. Eur. J. Enterp.
Technol. 6(84), 4–14 (2016)
22. Lytvyn, V., Vysotska, V., Pukach, P., Brodyak, O., Ugryn, D.: Development of a method for
determining the keywords in the slavic language texts based on the technology of web mining.
East. Eur. J. Enterp. Technol. 2/2(86), 4–12 (2017)
214
O. Kanishcheva et al.

23. Lytvyn, V., Vysotska, V, Veres, O., Rishnyak, I., Rishnyak, H.: Content linguistic analysis
methods for textual documents classiﬁcation. In: Proceedings of the XI-th International
Conference on Computer Science and Information Technologies, CSIT 2016, pp. 190–192
(2016)
24. Lytvyn, V., Vysotska, V.: Designing architecture of electronic content commerce system. In:
Proceedings of the X-th International Conference on Computer Science and Information
Technologies, CSIT 2015, pp. 115–119 (2015)
25. Basyuk, T.: The main reasons of attendance falling of internet resource. In: Proceedings of
the X-th International Conference on Computer Science and Information Technologies, CSIT
2015, pp. 91–93 (2015)
26. Burov, E.: Complex ontology management using task models. Int. J. Knowl. Based Intell.
Eng. Syst. 18/2, 111–120 (2014). IOS Press, Amsterdam
27. Lytvyn, V., Uhryn, D., Fityo, A.: Modeling of territorial community formation as a graph
partitioning problem. East. Eur. J. Enterp. Technol. 1/4(79), 47–52 (2016)
28. Kravets, P., Kyrkalo, R.: Fuzzy logic controller for embedded systems. In: Proceedings of
the 5th International Conference on Perspective Technologies and Methods in MEMS Design,
MEMSTECH (2009)
29. Mykich, K., Burov, Y.: Algebraic framework for knowledge processing in systems. In:
Advances in Intelligent Systems and Computing, pp. 217–228. Springer, Cham (2017)
30. Shakhovska, N., Vysotska, V., Chyrun, L.: Features of e-Learning realization using virtual
research laboratory. In: Proceedings of the XI-th International Conference on Computer
Science and Information Technologies, CSIT 2016, pp. 143–148 (2016)
31. Shakhovska, N., Vysotska V., Chyrun, L.: Intelligent systems design of distance learning
realization for modern youth promotion and involvement in independent scientiﬁc researches.
In: Advances in Intelligent Systems and Computing, vol. 512, pp. 175–198. Springer, Cham
(2017)
32. Lytvyn, V., Vysotska, V., Chyrun, L., Chyrun, L.: Distance learning method for modern youth
promotion and involvement in independent scientiﬁc researches. In: Proceedings of the IEEE
First International Conference on Data Stream Mining and Processing (DSMP), pp. 269–274
(2016)
33. Lytvyn, V., Tsmots, O.: The process of managerial decision making support within the early
warning system. Actual Probl. Econ. 11(149), 222–229 (2013)
34. Chen, J., Dosyn, D., Lytvyn, V., Sachenko, A.: Smart data integration by goal driven ontology
learning. In: Advances in Big Data. Advances in Intelligent Systems and Computing, pp.
283–292. Springer, Cham (2016)
35. Lytvyn, V., Dosyn, D., Smolarz, A.: An ontology based intelligent diagnostic systems of steel
corrosion protection. Elektronika 8, 22–24 (2013)
36. Pasichnyk, V., Shestakevych, T.: The model of data analysis of the psychophysiological
survey results. In: Advances in Intelligent Systems and Computing, vol. 512, pp. 271–282.
Springer, Cham (2016)
37. Rayman-Bacchus, L., Molina, A.: Internet-based tourism services: business issues and trends.
Futures 33(7), 589–605 (2001)
38. Buhalis, D., O’Connor, P.: Information communication technology revolutionizing tourism.
Tourism Recreation Res. 30(3), 7–16 (2005)
39. Buhalis, D., Law, R.: Progress in information technology and tourism management: 20 years
on and 10 years after the Internet - the state of eTourism research. Tourism Manage. 29(4),
609–623 (2008)
40. Werthner, H., Ricci, F.: E-commerce and tourism. Commun. ACM 47(12), 101–105 (2004)
Method of Integration and Content Management
215

41. Cardoso, J.: E-tourism: Creating dynamic packages using semantic web processes. In: W3C
Workshop on Frameworks for Semantics in Web Services (2005)
42. Olmeda, I., Sheldon, P.J.: Data mining techniques and applications for tourism internet
marketing. J. Travel Tourism Mark. 11(2-3), 1–20 (2002)
216
O. Kanishcheva et al.

Geoinformation Technology for Analysis and Visualisation
of High Spatial Resolution Greenhouse Gas Emissions Data
Using a Cloud Platform
Vitalii Kinakh1(✉)
, Rostyslav Bun1,2
, and Olha Danylo3
1 Lviv Polytechnic National University (LPNU), Lviv, Ukraine
kinakh.vitalii@gmail.com
2 University of Dąbrowa Górnicza, Dąbrowa Górnicza, Poland
3 International Institute for Applied Systems Analysis, Laxenburg, Austria
Abstract. The geoinformation technology for spatial analysis and visualisation
of greenhouse gas (GHG) emissions is proposed using Google Earth Engine cloud
technology as a key component of interaction with high-resolution spatial data.
This technology includes a website for spatial analysis and visualisation of vector
data, as well as an interactive site for deeper analysis of raster data on GHG
emissions. We use high-resolution vector data of emissions at the level of point,
line and areal emission sources, which are converted into raster emission data.
Emissions can be analysed within user-created polygons including calculation of
the total, speciﬁc, maximum or average emission magnitudes. There is also the
possibility to ﬁx and select pixels containing a certain interval of emission magni‐
tudes. Using Python’s Google Earth Engine module, we have created a website
where users can clip raster data from hand-drawn polygons that can be saved on
Google Drive. We have also used Python modules (Matplotlib, Pandas, Numpy)
for statistical analysis of raster data and histogram construction. Geoinformation
technology includes many sectors and categories of human activity included in
national inventory reports on GHG emissions, such as those regarding the burning
of fossil fuels for power and heat production, within the industrial, agricultural,
construction, residential, institutional and waste sectors, as well reports
addressing emissions caused by chemical processes. Implementation of the
proposed technology is presented using high spatial resolution greenhouse gas
emissions data from Poland.
Keywords: Geoinformation technology · Greenhouse gas · Emission data · High
resolution · Google Earth Engine · Spatial analysis · Raster map · Histogram ·
Pandas · Matplotlib
1
Introduction
In recent decades, scientists have identiﬁed global warming as a main cause of climate
change and many disastrous changes in diﬀerent regions of our planet. The temperature
increase is caused by anthropogenic greenhouse gas eﬀects, especially those due to the
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_15

emissions of carbon dioxide, methane, nitrous oxide and other greenhouse gases (GHG),
which are produced as a result of intense human activity. Scientists are now trying to
ﬁnd ways to stabilise and decrease GHG emissions into the atmosphere and increase
absorption of these gases.
For scientists, as well as practical specialists, eﬀective methods and tools for the
estimation of GHG emissions (also known as GHG inventory) are required to better
understand emission processes and structure in order to provide eﬀective control mech‐
anisms necessary to meet emission obligations, emission trade quotes, etc. [1]. Such an
inventory will also help to estimate the eﬀectiveness of low carbon technologies, renew‐
able energy sources, etc.
Emission inventories are usually performed at the country or regional level. However,
spatial estimates are highly needed for the analysis of geospatial patterns of emissions [2].
Such inventories may be used in atmospheric flow and climate models, and in tools to
support administrative decision making at the regional or local level [3, 4]. The main
benefit of such an approach is a spatial representation of emissions in the territories where
they actually occur. In the most cases, spatial analysis of emissions is carried out using a
grid with a determined size of cells. Such analyses include the calculation of emission
magnitudes, as well as uncertainties [5], and the results are represented as raster maps. A
huge effort has been made to increase the spatial resolution of GHG estimates since a
higher resolution better reflects the specifics of territorial emission and absorption
processes. Accordingly, the grid cell sizes have decreased from 1° latitude and longitude
for global fossil fuel CO2 emissions [6] to 1 km for a global fossil fuel CO2 emissions
inventory derived using a point source database and satellite observations of night time
lights as proxy data [2].
Nevertheless, other approaches also exist which present GHG emissions at the level
of emission sources and are based on vector maps of emissions [7] instead of a grid.
Such approaches provide increased resolution of inventory and decreased depth of
disaggregation of activity and proxy data, and make possible a spatial inventory for
separate categories of human activity, individual GHG, etc. [8].
An increasing number of people are now interested in the reduction of GHG emis‐
sions. As a result, there is demand for creating simple and handy solutions for the analysis
of emission processes and visualisation of results. The cloud-based platform Google
Earth Engine [9] provides many new opportunities for analysis and visualisation of
geospatial data, including GHG emissions [10]. At the same time, Python modules
(Pandas, Numpy and Matplotlib) provide statistical analysis for creating simple histo‐
grams for data distribution, etc.
The aim of this study is to present how cloud platform Google Earth Engine and
Python modules can be used to create simple and handy geoinformation technology
(GIS) for rapid analysis and visualisation of high spatial resolution GHG emissions data
in an easily understandable form.
218
V. Kinakh et al.

2
Speciﬁcs of Spatial Data Used
Spatial vector data on GHG emissions in Poland [11] was used as input data for the geoin‐
formation system developed in this study. These data were calculated for different types
of emission sources depending on the specifics of each category of human activity. Objects
with significant emissions (e.g. power plants, cement plants, refineries, etc.) are consid‐
ered as point-type emission sources and compared with the surrounding area [12, 13].
Lengthy objects with small widths that cause GHG emissions (e.g. road segments) are
considered as line-type emission sources [14]. Objects that emit GHG or absorb the carbon
dioxide from some area (e.g. settlements in the residential sector [15], arable lands in the
agricultural sector [16], etc.) are considered as area-type source/sink emission sources.
The above mentioned vector data include emissions of carbon dioxide, methane,
nitrous oxide and other GHG, as well as the total emission of all gases in CO2-equivalent.
The spatial resolution of the emissions data from point- and line-type sources is quite
high (i.e. several metres) as all point sources were inspected visually using Google Earth,
and line-type sources were built on the basis of Open Street Maps [17]. In contrast, the
spatial resolution of area-type sources is lower because they were created on the basis
of the Corine Land Cover Maps [18] with 100 m spatial resolution. Therefore, the spatial
resolution of these data does not generally exceed 100 m.
3
GIS Technology
3.1
Structure of Technology
The main components of the proposed GIS technology for spatial analysis and visuali‐
sation of high-resolution GHG emissions data are:
• operations with vector data on GHG emissions including website for spatial analysis
and visualisation of vector data on GHG emissions at the emission source level;
• conversion of vector data to raster including grid creation and calculation of gridded
emissions;
• operations with raster data on GHG emissions including: spatial analysis and visu‐
alisation of high-resolution raster data on GHG emissions at grid level using the
interactive website; band choice for deep analysis of GHG emissions; polygon crea‐
tion for deeper analysis of GHG emissions; calculation of the main parameters of
emission; ﬁxing interval (i.e. min, max) and selecting pixel values within this interval.
The main structure of this technology is shown in Fig. 1. We can see that all of the
above mentioned components are connected with the Google Earth Engine cloud plat‐
form as a core of the developed geoinformation system.
Geoinformation Technology for Analysis and Visualisation
219

Fig. 1. Structure of the GIS technology for spatial analysis and visualisation of high-resolution
GHG emissions data
3.2
Main Components
The website for spatial analysis and visualisation of vector data on GHG emissions at
the emission source level was carried out using Google Maps JavaScript API. The vector
spatial data were uploaded onto Google Fusion Tables and made accessible from Google
Earth Engine. These data are based on a high-resolution spatial inventory [11–16] and
include emission magnitudes of separate GHG from point-, line- and area-type emission
sources, as well as total emission of all gases in CO2-equivalent. Additionally, all spatial
data on emissions were aggregated into cells of a ﬁxed size. A 2 × 2 km grid was used
for the emissions data from Poland. Cells are further divided into smaller vector objects
if they are crossed by administrative boundaries of a province, district or municipality,
which helps to maintain administrative assignment of each elementary object and
provides the opportunity to sum GHG emissions within the bounds of each administra‐
tive object without loss of precision.
The main raster parameters, which are related to spatial resolution and map extent,
are set during the conversion of vector data (usually in tab MapInfo or ESRI shape ﬁle
formats) into raster/gridded data (in tiﬀ ﬁle format). High spatial resolution GHG emis‐
sions data from diﬀerent sectors and categories of human activity are organised as bands
of raster data. The bands contain information regarding the emission of diﬀerent GHG
220
V. Kinakh et al.

(carbon dioxide, methane and nitrous oxide), as well as the total emission of all gases
taking the global warming potentials of separate gases into account.
The interactive website for spatial analysis and visualisation of high-resolution raster
data on GHG emissions at the grid cells level was also carried out using Google Maps
JavaScript API. The additional functionality of this site provides the possibility to choose
the corresponding raster data band, create any arbitrary polygon and analyse the main
parameters of emission processes within its bounds. The user can chose an operation
such as calculation of the total, speciﬁc, maximum or average emission magnitude. Users
can also ﬁx minimum and maximum values of some interval and then select pixels for
which the magnitude is within this interval. The raster data include carbon dioxide,
methane and nitrous oxide emissions, as well total emissions in CO2-equivalent in the
energy sector, separately for electricity generation and heat production, extraction and
processing of fossil fuels, petroleum reﬁnement, transport, as well as for the manufac‐
turing, industry, construction, commercial, residential, agricultural and waste sectors,
among others.
The second interactive website for spatial analysis and visualisation of raster data
was developed using Google Earth Engine Python API. This tool provides additional
functionality such that users can draw a polygon, clip all cells within the polygon as
a separate raster image and export it to Google Drive. This functionality is a great
opportunity for users who do not have powerful computers as all operations are
processed in the cloud.
There are several positive features of using such a tool for spatial analysis of GHG
emissions. First, as the tool is based on Google Earth Engine, there are no strong
requirements of the user’s computer for interacting with it, aside from Internet access
and the availability of a browser with JavaScript support. Second, all data are stored on
the server such that users do not need to download and pre-process the data. Finally, the
speciﬁed emissions in a selected sector can be calculated for the area of interest without
handling the entire dataset.
In order to calculate total emissions, the analytical results at the level of separate
emission sources are aggregated to vector data in a regular 2 × 2 km grid. As a result of
this operation, emissions from point-, line- and area-type sources that are completely or
partially within these cells are summed [11]. The easy operation of these data and quick
implementation of analysis of the main parameters of the emission processes within the
user-determined territories demonstrates the need to create corresponding geoinforma‐
tion technology using Google Earth Engine.
4
Practical Implementation of Gridded Emissions
Practical implementation of the geoinformation technology for spatial analysis and
visualisation of high-resolution GHG emissions data is presented in Fig. 2 using the
example of total GHG emissions in the carbon dioxide equivalent from the residential
sector in Poland. The red-yellow colour scheme was selected for better visualisation of
the results.
Geoinformation Technology for Analysis and Visualisation
221

Fig. 2. Example of geospatial analysis of total GHG emissions in the residential sector in the
Lesser Poland province: (a) initial raster data and (b) polygon created with visual results of a deep
analysis (2010, 2 × 2 km grid, CO2-equivalent, Gg)
These GHG emissions data were obtained at the level of settlements as area-type
emission sources. For this purpose, the statistical data on fuel consumption at the munic‐
ipal level were disaggregated into the level of settlements using proxy data including:
maps of heating degree days and population density in Poland; data on access to energy
222
V. Kinakh et al.

sources; living area data; percentage of living area equipped with central heating and
hot water supply and data regarding the amount of heat energy provided to households.
A full description of this approach for high-resolution spatial inventory of GHG in the
residential sector is presented in [15]. Following this approach, data on the consumption
of coal, natural gas and biomass, as well as the corresponding emission factors, emis‐
sions of carbon dioxide, methane, and nitrous oxide at the level of settlements and total
CO2-equivalent emissions were computed [19].
These vector data on GHG emissions in Poland were aggregated into 2 × 2 grid cells
as vector map elements and then converted into a similar raster grid in tiﬀ format.
Implementation of these spatial data into the developed geoinformation system is illus‐
trated in Fig. 2 using the Lesser Poland province as an example. The upper part of
Fig. 2 demonstrates gridded emissions in the residential sector. One can see the highest
emissions in Kraków agglomeration and emissions of the largest towns of this region
including Tarnów and Nowy Sącz. In the bottom of Fig. 2, one can see emissions from
the neighbourhood of Zakopane. Although this territory is a resort area, it is densely
populated and wood is primarily used as fuel within the residential sector, which has a
higher emission coeﬃcient compared with other fossil fuel energy sources.
In Fig. 2b, one can see how the interactive website works. In particular, a user-formed
polygon around the Kraków agglomeration is shown with a box of results from addi‐
tional deep analysis of emission processes. Generalised parameters (e.g. mean and total
emission values within the polygon) are indicated within this box.
In order to create the website for analysis and extraction of images/maps, we used
Google Earth Engine Python’s API, Jinja2 HTML template (i.e. website templates) and
Google Drive API for saving images. Users can download images clipped within the
hand-drawn polygon for further analysis. This functionality is very helpful for inexper‐
ienced users who do not have powerful computers as such operations using a desktop
GIS involve signiﬁcant computational costs and all operations on our website are
processed in the cloud.
5
Statistical Analysis of Gridded Emissions
The geoinformation technology for spatial analysis of high-resolution GHG emissions
data developed in this study provides the possibility to calculate some additional param‐
eters of the raster data. For statistical analysis of emission magnitudes in pixels and the
creation of histograms and plots, we used the Python modules Matplotlib, Pandas and
Numpy [20]. We used raster maps in ESRI shape ﬁle format of high-resolution GHG
emissions data from Poland as input data [12–16]. Then we selected pixels with non-
zero emission magnitudes using the Pandas module. Due to the speciﬁcs of emission
processes in some categories of human activity (e.g. electricity generation, petroleum
reﬁnement, metallurgy, etc.), there are a signiﬁcant number of cells with a zero emission
magnitude. Using this dataset and the Matplotlib module, we created histograms of
emission magnitudes in CO2-equivalent for all sectors/subsectors covered by the high-
resolution spatial inventory of GHG. Plots and maps can be manipulated in two diﬀerent
ways using Matplotlib. The ﬁrst is an interactive way where one can zoom into a plot,
Geoinformation Technology for Analysis and Visualisation
223

view a chart from diﬀerent angles and view speciﬁc emission values or regions. The
second way is to save and work with the plot as a picture.
We show histogram examples in Fig. 3 for GHG emission magnitudes in Poland per
each 2 × 2 km cell created for:
– fossil fuels (i.e. liquid, solid and gaseous fuels) in the residential sector;
– wood/biomass in the residential sector;
– other categories including agriculture, forestry, ﬁshery etc.
We show similar histograms in Fig. 4 for GHG emission magnitudes per cell from
the use of fossil fuels in the:
– manufacturing industry and construction sectors;
– transport sector;
– energy sector (total emissions).
Both ﬁgures include calculated histograms for total emissions of all GHG (carbon
dioxide, methane and nitrous oxide) in CO2-equivalent terms. For better visualisation
of the results along the ordinate axis, the occupied area is displayed instead of the number
of pixels.
The histogram for each sector consists of two parts. The upper part demonstrates the
distribution of data from minimum to mean value. This is the main part of the GHG
emissions data and reﬂects emissions from the majority of cells. The lower part of each
histogram illustrates the full data distribution (i.e. from minimum to maximum). The
majority of the data is clearly within the ﬁrst part. This feature is characteristic for most
sectors including residential, agriculture/forestry/ﬁshery, manufacturing industry,
construction and transportation.
Emissions within the pixels from the residential sector are relatively small, both from the
burning of fossil fuels and from the use of wood/biomass (Fig. 3). For fossil fuels, a large
area (124,000 km2) is occupied by pixels with emissions less than 150 MgCO2-eq. Similarly,
for the use of wood, an area of 127,000 km2 is occupied by pixels with emissions less than
80 MgCO2-eq. Fossil fuel emissions within the agriculture/forestry/fishery sector are insig‐
nificant such that more than 89,000 km2 occupy pixels with emissions less than 20 MgCO2-eq.
For non-zero pixels of the manufacturing industry and construction sector in Poland,
the mean magnitude is higher (4.34 GgCO2-eq.) (Fig. 4), which is inﬂuenced by high
emissions point sources (e.g. metallurgical and chemical plants, etc.) even though the
total area occupied by these pixels is relatively small. The most ‘uniform’ emissions
distribution is in the transport sector, for which the mean value for non-zero pixels is
654 MgCO2-eq.
As can be seen in Fig. 4c, the distribution of GHG total emissions in the energy sector
in Poland is close to lognormal. The magnitudes of most pixels with a total area larger
than 25,000 km are within the range of 279–419 MgCO2-eq. The tail of this distribution,
however, reaches a maximum value of 26,000 GgCO2-eq, as can been seen in bottom part
of the histogram in Fig. 4c. The highest emissions are caused by power plants as emission
point sources although there are only a few such pixels, whereas there are 80 power
plants in Poland that use fossil fuels with power greater than 20 MW.
224
V. Kinakh et al.

Fig. 3. Histograms of emission magnitudes per cell in the residential sector due to (a) fossil fuels,
(b) wood and (c) in other categories such as agriculture, forestry etc. (Poland, 2010, 2 × 2 km
grid, CO2-equivalent, Gg/cell)
Geoinformation Technology for Analysis and Visualisation
225

Fig. 4. Histograms of emission magnitudes per cell in the (a) manufacturing industry and
construction sector, (b) transport sector and (c) energy sector (total emissions) including all
subsectors (Poland, 2010, 2 × 2 km grid, CO2-equivalent, Gg/cell)
226
V. Kinakh et al.

The presented data characterise the uneven distribution of emissions in the investi‐
gated region, as well as the expediency of such spatial inventories of GHG and potential
of the created geoinformation technology for spatial analysis. In general, all websites
and tools developed in this study can be used for spatial analysis of any raster map/image
available in Google Earth Engine, not only for GHG data.
6
Conclusions
The presented geoinformation technology for spatial analysis and visualisation of high-
resolution GHG emissions data uses the cloud-based platform Google Earth Engine.
Presented histograms and charts were created using Python modules Matplotlib, Pandas
and Numpy. Vector maps of emissions at the level of point-, line- and area-type sources
are used as input data. The analytical results are represented as raster emission maps.
Implementation of the developed technology is demonstrated using high spatial reso‐
lution GHG emissions data from Poland as a case study. This analysis includes emissions
in all main sectors and categories of human activity covered by the National Inventory
Reports [21] submitted to the United Nations Framework Convention on Climate
Change.
The use of vector data on GHG emissions in the created technology yields better
results in terms of uncertainty as well as usability. The methodology allows utilisation
of high-resolution input vector maps of point-, line-, and area-type emission sources,
and maintains high resolution in the results, which are independent of the grid size,
overlapping grids, etc. Consequently, it excludes the source location uncertainty as
a component of the total uncertainty. The use of raster data on GHG emissions in this
geoinformation technology provides the potential to perform eﬀective spatial analysis
at the regional and national scales, as well as deeper emissions analyses within user-
created polygons, etc.
The Google Earth Engine provides an opportunity to perform spatial analysis of
emission processes independent of computer characteristics and user experience with
regards to GHG emission inventories. The created geoinformation technology also
allows deeper analysis of generalised parameters of emission processes within user-
formed polygons with calculation of the total or speciﬁc, as well as maximum or average
emission magnitudes.
References
1. Ometto, J.P., Bun, R., Jonas, M., Nahorski, Z. (eds.): Uncertainties in Greenhouse Gas
Inventories: Expanding Our Perspective. Springer, Cham (2015)
2. Oda, T., Maksyutov, S.: A very high-resolution (1 km × 1 km) global fossil fuel CO2 emission
inventory derived using a point source database and satellite observations of nighttime lights.
Chem. Phys. 11, 543–556 (2011)
3. Déqué, M., Somot, S., Sanchez-Gomez, E., Goodess, C.M., Jacob, D., Lenderink, G.,
Christensen, O.B.: The spread amongst ENSEMBLES regional scenarios: regional climate
models, driving general circulation models and interannual variability. Clim. Dyn. 38(5),
951–964 (2012)
Geoinformation Technology for Analysis and Visualisation
227

4. Neale, R.B., Richter, J., Park, S., Lauritzen, P.H., Vavrus, S.J., Rasch, P.J., Minghua, Z.: The
mean climate of the community atmosphere model (CAM4) in forced SST and fully coupled
experiments. J. Clim. 26, 5150–5168 (2013)
5. Hogue, S., Marland, E., Andres, R.J., Marland, G., Woodard, D.: Uncertainty in gridded CO2
emissions estimates. Earth’s Future 4, 225–239 (2016)
6. Andres, R.J., Marland, G., Fung, I., Matthews, E.: A 1° × 1° distribution of carbon dioxide
emissions from fossil fuel consumption and cement manufacture, 1950–1990. Global
Biogeochem. Cycles 10(3), 419–429 (1996)
7. Bun, R., Gusti, M., Kujii, L., Tokar, O., Tsybrivskyy, Y., Bun, A.: Spatial GHG inventory:
analysis of uncertainty sources. A case study for Ukraine. Water Air Soil Pollut. Focus 7(4–5),
483–494 (2010)
8. Boychuk, K., Bun, R.: Regional spatial inventories (cadastres) of GHG emissions in the
energy sector: accounting for uncertainty. Clim. Change 124, 561–574 (2014)
9. Google Earth Engine: A planetary-scale platform for Earth science data and analysis. https://
earthengine.google.com. Accessed 5 July 2017
10. Lemoine, G., Léo, O.: Crop mapping applications at scale: using Google Earth Engine to
enable global crop area and status monitoring using free and open data sources. In: Remote
Sensing: Understanding the Earth for a Safer World, IGARSS 2015, Milan, pp. 1496–1499
(2015)
11. Bun, R., Nahorski, Z., Horabik-Pyzel, J., Danylo, O., Charkovska, N., Topylko, P.,
Halushchak, M., Lesiv, M., Striamets, O.: High resolution spatial inventory of GHG
emissions from stationary and mobile sources in Poland: summarized results and uncertainty
analysis. In: Proceedings of the 4th International Workshop on Uncertainty in Atmospheric
Emissions, Kraków, Poland, 7–9 October 2015, pp. 41–48. SRI PAS, Warsaw (2015)
12. Topylko, P., Halushchak, M., Bun, R., Oda, O., Lesiv, M., Danylo, O.: Spatial greenhouse
gas (GHG) inventory and uncertainty analysis: a case study for electricity generation in Poland
and Ukraine. In: Proceedings of the 4th International Workshop on Uncertainty in
Atmospheric Emissions, Kraków, Poland, 7–9 October 2015, pp. 49–56. SRI PAS, Warsaw
(2015)
13. Charkovska, N., Halushchak, M., Bun, R., Jonas, M.: Uncertainty analysis of GHG spatial
inventory from the industrial activity: a case study for Poland. In: Proceedings of the 4th
International Workshop on Uncertainty in Atmospheric Emissions, Kraków, Poland, 7–9
October 2015, pp. 57–63. SRI PAS, Warsaw (2015)
14. Boychuk, P., Nahorski, Z., Boychuk, K., Horabik, J.: Spatial analysis of greenhouse gas
emissions in road transport of Poland. Econtechmod 1(4), 9–15 (2012)
15. Danylo, O., Bun, R., See, L., Topylko, P., Xu, X., Charkovska, N., Tymków, P.: Accounting
uncertainty for spatial modeling of greenhouse gas emissions in the residential sector: fuel
combustion and heat production. In: Proceedings of the 4th International Workshop on
Uncertainty in Atmospheric Emissions, Kraków, Poland, 7–9 October 2015, pp. 193–200.
SRI PAS, Warsaw (2015)
16. Charkovska, N., Bun, R., Danylo, O., Horabik-Pyzel, J., Jonas, M.: Spatial GHG inventory
in the Agriculture sector and uncertainty analysis: a case study for Poland. In: Proceedings
of the 4th International Workshop on Uncertainty in Atmospheric Emissions, Kraków,
Poland, 7–9 October 2015, pp. 16–24. SRI PAS, Warsaw (2015)
17. Arsanjani, J., Zipf, A., Mooney, P., Helbich, M. (eds.): OpenStreetMap in GIScience -
Experiences, Research, and Applications. Springer, Cham (2015)
18. Corine Land Cover data (2016). http://www.eea.europa.eu/. Accessed 20 June 2017
228
V. Kinakh et al.

19. Eggleston, H.S., Buendia, L., Miwa, K., Ngara, T., Tanabe, K. (eds.): IPCC Guidelines for
National Greenhouse Gas Inventories, Prepared by the National Greenhouse Gas Inventories
Programme, IPCC (2006)
20. McKinney, W.: Python for Data Analysis, pp. 120–256. O’Reilly Media, Sebastopol (2012)
21. Poland’s National Inventory Report 2012, KOBIZE, Warsaw (2012). http://unfccc.int/
national_reports. Accessed 1 July 2017
Geoinformation Technology for Analysis and Visualisation
229

Comparative Analysis of Conversion Series Forecasting
in E-commerce Tasks
Lyudmyla Kirichenko
(✉), Tamara Radivilova, and Illya Zinkevich
Kharkiv University of Radioelectronics, Kharkiv, 61166, Ukraine
Lyudmyla.kirichenko@nuze.ua, tamara.radivilova@gmail.com
Abstract. The characteristic features of time series conversion, which arise in
the tasks of e-commerce are described. It is shown that these series are weakly
correlated, which does not allow to use traditional methods for their prediction.
Forecasting of the series is performed by methods of exponential smoothing,
neural network and decision tree using data from an online store. A comparative
analysis of the results is carried out. The advantages and disadvantages of each
method are considered.
Keywords: Time series · Conversion rate · Machine learning · Forecasting ·
Exponential smoothing · Decision tree · Long-term memory neural network
1
Introduction
Time series describe a wide range of phenomena, for example, they are the stock prices,
solar activity, the overall incidence rate and much more. Economic indicators can also
be considered as time series and you can try to ﬁnd not visible at ﬁrst glance laws, hidden
periodicity, to predict the moments when peaks appear, etc. At the moment is urgent
time-series analysis in the ﬁeld of e-commerce. E-commerce is in process of develop‐
ment, which is facilitated by new technologies, services and tactical tools [1]. For
successful sale in online stores, web analytics is used, which allows to work on optimi‐
zation, increase conversion and attendance of the electronic store.
To “survive” and stand out among the many online stores, it is important to under‐
stand the user’s behavior from the moment of the ﬁrst arrival on the site: to track his
movements, to know what products he looked at, put in the basket, where he clicked,
what saw, the time he left, how and when he returned. Web-analytics will help in this,
which involves ongoing collection, analysis and interpretation of data about visitors,
work with basic metrics. Careful analysis of the online store and user behavior is a
necessary stage of business development.
Quality web analytics of online store always begins with the visitor’s way that he
passed before making a purchase. The order processing consists of the following steps:
(1) product search; (2) add item to shopping cart; (3) go to the checkout page; (4) ﬁll
out and submitting the form; (5) go to the page of the order, payment. The main task of
analytics is to periodically ﬁnd and ﬁx the weak points in this chain. At each stage, the
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_16

user can stop without having made a purchase. Each of these stages is represented in the
form of a time series.
It is impossible to work on optimization, increase of conversion and site attendance
without web analytics. By using key performance indicators, it can be signiﬁcantly
improved proﬁt site. These indicators show how quickly and eﬃciently the business
grows. One of the advantages of running an online store is the transparency of key
performance indicators tracking and the ability to optimize processes for business
growth [2]. The competition in the ﬁeld of e-commerce is so great that an online store
that does not use analytics and metrics will not last long. Here are the main success
indicators that any online store should measure:
1. Attendance of the site - the number of users visiting the site, which are measured
in terms of daily audience, weekly and monthly. This will allow to evaluate the
incidence and bursts of site traﬃc and identify their causes.
2. Views commodity page – which pages visitors view on the site often and which
less. Analyzing the attendance of commodity pages, it is possible to understand the
customers’ shopping preferences and the way of their interacting with the site.
3. The average time on the site and the average number of pages viewed – if these
indicators are low, it is worth assessing the quality of traﬃc on site.
4. Exit pages - by analyzing the exit points of the site visitors (registration, shopping
cart, ordering), can better understand the reasons for the low conversion and opti‐
mize the site so that users stay on the site and complete purchases.
5. Channels to attract visitors - you need to track not just a source of attraction of
visitors, and their impact.
6. The overall conversion rate of an online store is the number of visitors who made
a purchase.
7. Indicator of return of visitors – the number of not only new, but also returned visitors
to the online store are analyzed. These will allow evaluate how the site is interesting
for the target audience.
8. Proﬁt from the buyer – proﬁt minus costs. This indicator provides an understanding
of how successful online store.
9. The failure rate is the number of orders that have been started, but not completed.
10. Number of products in the order – the number of products per order.
11. Average order value - total sales/orders quantity.
A store that is already selling on the Internet, often to increase proﬁts, it is suﬃcient
to pay attention to only a few metrics, one of which is the conversion. Conversion is the
most important parameter that characterizes the eﬀectiveness of the website promotion
process. Conversion is the ratio of the number of users who made purchase of a product
or service on your site to the number of users who came to your site for an advertising
link, ad, or banner. For example, if a site was visited by 100 people, but only 2 people
bought the product, then the conversion rate is equal to 2%. The increase of conversion
percent depends on many factors: from the design of the page to its functionality. Moni‐
toring conversion allows to understand in time that the e-shop needs to be improved.
Conversion is the main metric in the web analytics of all commercial sites [3].
Comparative Analysis of Conversion Series Forecasting
231

Analysis and forecasting of time series of daily value conversion percent plays
crucial value for optimizing the eﬃciency of the online business [4]. However, it should
be noted that almost all of the classical methods of time series forecasting based on the
calculation of the correlation between the time series values [5]. In the case of weakly
correlated time series, and also in the case when the time series has sparse zero values
structure, which is typical for many electronic sales sites, these methods do not ﬁt or
have a large error.
Neural network approach has been widely used to solve forecasting problems. Neural
networks allow you to model complex relationships between data as result of learning
by examples. However, the prediction of time series using neural networks has its draw‐
backs. First, for training the majority of neural networks, time series of a large length
are required. Secondly, the result essentially depends on the choice of the architecture
of the network, as well as the input and output data. Third, neural networks require
preliminary data preparation, or preprocessing. Preprocessing is one of the key elements
of forecasting: the quality of the forecast of a neural network can depend crucially on
the form in which information is presented for its learning. The general goal of prepro‐
cessing is to increase the information content of inputs and outputs. An overview of the
methods for selecting input variables and preprocessing is contained in [6].
Recently, for the analysis of the regularities of the time series, the methods of
machine learning [7, 8] have been increasingly used to detect various patterns in the
time series. In this case, logical methods are of particular value in the detection of such
patterns. These methods allow us to ﬁnd logical if-then rules. They are suitable for
analyzing and predicting both numerical and symbolic sequences, and their results have
a transparent interpretation.
The goal of the presented work is to carry out a comparative analysis of weakly
correlated time series forecasting, based on classical prediction methods and machine
learning ones such as neural networks and decision trees, using the data of a real online
store.
2
Input Data
Input data in the work were daily data from the online sales site, which included the
number of clicks on the site from social networks, the number of sales and the corre‐
sponding conversion rate. In addition, there was information about which language the
customer used, from which country the order was and other data [9].
Figure 1(at the top) shows typical time series of conversion rate. Series of conversion
rate are characterized by zero values, which signiﬁcantly complicates forecasting the
next day. The correlation function of the rate series is shown on Fig. 1(at the bottom).
Obviously, there is no correlation between the time series values.
232
L. Kirichenko et al.

Fig. 1. Time series of conversion rate and the correlation function
Figure 2 shows the histogram of the distribution density of typical conversion rate
series. It is easy to see that the percent from 0 to 2 is the highest, then there is a more
even distribution, but for each series of conversion rate there are bursts that are most
diﬃcult to forecast.
Fig. 2. Distribution density of typical conversion rate series
Comparative Analysis of Conversion Series Forecasting
233

3
Forecasting Methods
E-commerce is constantly evolving, it is facilitated by new technologies, services and
tactical tools. Suppliers, range of buyers, assortment of goods change regularly, that
leads to a rapid obsolescence of information. Therefore, forecasting methods that require
time series of great length, such as, for example, autoregressive and moving average
models, work poorly [10].
Methods of Exponential Smoothing. The basis for exponential smoothing is the idea
of a constant revision of the forecast values as the actual ones arrive. The model of
exponential smoothing assigns exponentially decreasing weights to observations as they
become outdated [5, 11]. Thus, the latest available observations have greater inﬂuence
on the forecast value than older observations.
The model of exponential smoothing has the form:
Z(t) = S(t) + εt, S(t) = 𝛼⋅Z(t −1) + (1 −𝛼) ⋅S(t −1),
(1)
where α is smoothing factor; 0 < α < 1; Z(t) is projected time series; S(t) is smoothed
time series; initial conditions are deﬁned as S(1) = Z(0). In this model, each subsequent
smoothed value S(t) is the weighted average between the previous value of the time
series Z(t−1) and the previous smoothed value S(t−1).
The value α is determined by how much the current series value should aﬀect the
next value. The closer α is to unity, the stronger the forecast takes into account the value
in the previous step. To ﬁnd the optimal value α, it is required to minimize the mean
error of the forecast. When the value α is automatically selected, all forecasts are calcu‐
lated for α that change with a given step, the mean error is calculated, and the value α
at which the error has the smallest value is selected.
3.1
Decision Tree Method
Methods of machine learning are an extremely broad and dynamically developing ﬁeld
of research using a lot of theoretical and practical methods. One of these methods is the
decision tree method [7, 12, 13]. The decision tree is a decision support tool used in
statistics and data analysis for predictive models.
In intellectual data analysis, decision trees can be used as mathematical and compu‐
tational methods to help describe, classify and summarize a set of data that can be written
as follows: (x, Y) = (x1, x2, x3, …, xk, Y). The dependent variable Y is the objective
variable that needs to be analyzed, forecast and generalized. A vector x consists of input
variables x1, x2, x3, etc., which are used to perform the task.
The decision tree method for classiﬁcation or prediction task is the process of
dividing the original data into groups until homogeneous (pure) subsets are obtained.
The set of rules, due to which there is such division, allows to make a forecast obtained
as a result of evaluating some input features x1, x2, x3 for new data.
The decision tree is a model that represents a set of rules for decision-making.
Graphically, it can be represented in tree structure form, where decision-making
234
L. Kirichenko et al.

moments correspond to the decision nodes. The data to be classiﬁed are at the root of
tree. In nodes, depending on decision made, a branching process occurs. Terminal nodes
are called leaf nodes. Each leaf is ﬁnal result of consistent decision-making and repre‐
sents value of objective variable, which was modiﬁed during movement from root to
leaf. Each internal node corresponds to one of input variables. Depending on decision
made at nodes, the process eventually stops in one of leaves, where a variable of response
is assigned a particular value.
The algorithm of learning (forming the tree) operates according to the principle of
recursive partitioning. The partitioning of data set (i.e., splitting into disjoint subsets) is
performed on the basis of the most suitable for this feature. A corresponding decision
node is created in tree, and process continues recursively to the stopping criterion.
There are various numerical algorithms for constructing decision trees. One of the
most famous is the algorithm called C5.0, developed by the programmer J.R. Quinlan.
In fact, the C5.0 is the standard for construction of decision trees. This program is
implemented on a commercial basis, but version built into the Python (and some other
packages) is available for free.
The algorithm implements the principle of recursive partitioning. The algorithm
starts with empty tree and complete data set. In nodes, starting from the root node, feature
is selected whose value is used to divide all data into two classes. After the ﬁrst iteration,
the tree appears with one node dividing the data set into two subsets. After that, this
process can be performed repeatedly, with respect to each of subsets for creating
subtrees. To separate data, we use conditions of form: {x < a}, {x > a}, where x is
feature, and a is some ﬁxed number. Such partitions are called “axis-parallel splits”.
Essentially, with each condition check, the data samples are sorted in such way that each
data element is determined to correspond only to one branch. Decision criteria divide
the original data set into disjoint subsets. The recursion terminates if subset in node has
same values of objective variable, so it does not add values to forecasts.
To create a decision tree, its needed to determine the features by which the partition
will be performed. In the case of the classiﬁcation of data samples, these values may be
the sampling values. From the set of attributes for the partition, it is required to choose
those that would allow to obtain as homogeneous (pure) sets as possible. Algorithm C5.0
uses as impurity measure of entropy concept, which is a measure of data disorder.
Using the entropy as measure of impurity sets that are result of partitioning, algorithm
can select feature on which partitioning will give the purest set (i.e., the set with lowest
entropy). These calculations are called “information gain”. The feature is determined
by the search method. For each feature, the value of information gain is calculated as
diﬀerence in entropy of sets before and after partitioning.
The higher information gain for selected feature, better this feature is suitable for
partitioning, since such partition will ensure that the most pure set is obtained. If, for
the selected characteristic, the value of information gain is close to zero, it means that
the partition by this feature is unpromising, since it does not lead to entropy decrease.
On the other hand, the maximum possible value of information gain is equal to the value
of entropy before the partition. This means that the entropy after partition will be zero,
i.e. resulting sets will be completely pure.
Comparative Analysis of Conversion Series Forecasting
235

The main advantages of the C5.0 algorithm for forecasting tasks: it is universal, it
solves well the problems of classiﬁcation and forecasting from diﬀerent areas; to
construct a decision tree, it selects from set of features only those that strongly inﬂuence
the result; it requires a relatively small amount of training sample. One of the signiﬁcant
advantages of the C5.0 algorithm is its ability to post-pruning of the built decision tree,
that is, cutting oﬀ those nodes and branches that have little impact on the forecast results.
The disadvantages are that the algorithm “gravitates” to split based on a large number
of levels; inaccuracies in classiﬁcation can arise from the fact that only “parallel-axis
axes” splits are used; decision trees sometimes turn out to be very large.
Neural Networks. It can be said that any neural network (NN) acts as follows: iteration
after iteration, it deforms the vector of input data in this way that as a result of defor‐
mation, the input data fall into the zones where we expect to see them at the output. In
ordinary neural networks, each individual sample is processed without taking into
account the inﬂuence of past information on current result. To solve this problem, recur‐
rent neural networks (RNN) were developed in the 1980s. These are networks that
contain feedback and allow to take into account the previous iterations. A recurrent
network can be viewed as several copies of the same network, each of which transmits
information of a subsequent copy. RNN resembles a chain, and its architecture is well
suited for working with sequences, lists and time series.
The scheme of the RNN operation looks like this: there is an input layer of neurons
that is projected onto a hidden layer (one or several), the outputs of the hidden layer are
transferred to output layer, and also copied to context layer, which at the next iteration
is perceived together with input layer and connected to hidden layer. Accordingly, cycle
is produced: a hidden layer - a context layer - a hidden layer. In progress new samples
will come in RNN, they will change the context and context circulating within the
network, will retain this information, which will aﬀect the current classiﬁcation. Over
the past few years, RNN has successfully applied to a lot of tasks: speech recognition,
language modeling, translation, image recognition, etc. [14]. But the main disadvantage
of classic RNN is to reduce the eﬀect of samples with increasing time delay. As a rule,
the maximum impact on response of RNN samples that were at previous iteration, two
iterations back, etc. have and the further, the less this eﬀect decreases. While quite often
there are situations when information important for correct forecasting is not on the
nearest samples, but on 10-20-30 iterations back.
Recently, the architecture of RNN which are called long short-term memory neural
networks (LSTM) has become popular. This is a special kind of recurrent neural
networks, which are capable of learning long-term dependencies. LSTM are speciﬁcally
designed to avoid the problem of long-term dependencies. Remembering information
for a long period of time is practically their default behavior [6, 15, 16].
A method that some elements in the context in the previous iterations provide a
greater inﬂuence on the result, while other elements have a smaller eﬀect was suggested.
In LSTM, it is proposed to extend the classical RNN schema with notion gate, which is
a memory gate and forget gate, and which determines how likely the given sample should
be forgotten or remembered for next iteration. The previous samples aﬀect saving or
deleting of sample. If they indicate that this information is important for future
236
L. Kirichenko et al.

classiﬁcation, more importance will be given. If the current information plays a weak
role for forecasting at subsequent iterations, impact will decrease.
Currently LSTM work incredibly well on a wide variety of tasks and are widely used.
Many impressive results of work of RNN were achieved precisely on the basis of LSTM
architecture [16].
The Forecast Errors. To obtain quantitative characteristics of the comparative anal‐
ysis of the models, the following characteristics of forecast errors were chosen [5, 11,
17]. The Mean Absolute Deviation (MAD) measures the accuracy of the forecast by
averaging the values of the forecast errors. Using MAD is most useful when the analyst
needs to measure the forecast error in the same units as the original series. This error is
calculated as follows:
MAD = 1
n
n
∑
t=1
|||X(t) −̂X(t)|||.
(2)
The average deviation (Mean Deviation, MD) allows to see how the forecast value
is overvalued or undervalued on average:
MD = 1
n
n
∑
t=1
X(t) −̂X(t).
(3)
Mean squared error (MSE) is another way of estimating the forecasting method.
Since each deviation value is squared, this method emphasizes large forecast errors. The
MSE error is calculated as follows:
MSE = 1
n
n
∑
t=1
(X(t) −̂X(t))2.
(4)
The Mean Absolute Percentage Error (MAPE) is calculated by ﬁnding the absolute
error at each time and dividing it by the actual observed value, with subsequent averaging
of the obtained absolute percent errors. This error is calculated as follows:
MAPE = 1
n
n
∑
t=1
|||X(t) −̂X(t)|||
X(t)
.
(5)
This approach is useful when the size or value of the predicted value is important
for estimating the accuracy of the forecast. MAPE emphasizes how large the forecast
errors are in comparison with the actual values of the series. This approach is useful
when the size or value of the predicted value is important for estimating the accuracy
of the forecast. MAPE emphasizes how large the forecast errors are in comparison with
the actual values of the series.
Comparative Analysis of Conversion Series Forecasting
237

4
Software Implementation of Machine Learning Methods
The methods and algorithms of Data Mining and machine learning must be implemented
in a certain programming language, in a certain environment, calculated on a certain
type of computer elements, etc. There are many tools available today to implement Data
Mining and Machine Learning algorithms [18].
One of the most widely used programming languages for solving application prob‐
lems is the Python language. Python is a general purpose programming language, which
means that people have built modules to create websites, interact with a variety of data‐
bases, and manage users. Python uses a large number of people and organizations around
the world, so it develops and is well documented; it is cross-platform and you can use
it free [12].
This language has several advantages. It is quite easy to learn and, as a rule, is a
language that needs a low entry level. A person who has basic knowledge in the theory
of algorithms and mathematics can simply master the basic functionality, methods and
syntax to solve applied problems. Python has implemented a large number of libraries,
which provide most of the available algorithms in a convenient way.
In general, for machine learning there are several basic Python libraries, which have
quite a big advantage compared to libraries of other programming languages. The main
one is: very detailed and qualitative documentation. Most of these libraries use the
NumPy library [19]. This is a library that allows you to quickly and eﬀectively work
with numeric data matrices, tables of numbers in diﬀerent formats, to carry out a large
number of typical operations that are required in the process of solving applied machine
learning tasks.
One of the great documentation libraries that implements most of the typical machine
learning methods is scikit-learn [20]. Dozens of algorithms are implemented in this
library for clustering, regression, classiﬁcation, reference vector method, linear and
logistic regression, and dozens of other algorithms. Each of the available algorithms has
a large number of parameters that can be customized to your task.
One very convenient language libraries in Python, which help to work with lots of
tabular data (often training and testing sample look like .csv-table with hundreds of
thousands and millions of rows and columns parameters) is a Pandas library [21]. It
allows to download data very quickly, preprocess it (to prepare it in a suitable format),
to send in a convenient form for processing by our algorithm, which we, for example,
have chosen from the Scikit-learn library.
Currently, due to the popularity of neural networks in Python, there are many libra‐
ries at quite diﬀerent levels of abstraction (from low to high-abstraction architecture
descriptions) allow construction of various neural networks.
One of the most commonly used libraries for low-level operations for the imple‐
mentation of neural network algorithms is the Theano Library [22]. It implements
complex matrix cartoons, rapid methods of convolution with multiplication, sampling,
regression methods, and all backend-logic of neural networks.
One key bonus of libraries is that in addition to CPU-realization (i.e., implementation
of algorithm’s operation on processor), Theano or TensorFlow libraries (by Google) are
open source (you can see and add modules that you need).
238
L. Kirichenko et al.

It should be noted that between the graphics card, the Python language and the library
written in this language there is one more layer - CUDA - a set of libraries, implemented
by NVidia, which allow eﬀectively and quickly perform calculations on its graphics
cards.
To work with web resources, the Scrapy library is used [23]. Scrapy is an application
framework for crawling web sites and extracting structured data which can be used for
a wide range of useful applications, like data mining, information processing or historical
archival. Scrapy provides a lot of powerful features for making scraping easy and eﬃ‐
cient, such as: built-in support for selecting and extracting data from HTML/XML
sources using extended CSS selectors and XPath expressions, with helper methods to
extract using regular expressions; for generating feed exports in multiple formats (JSON,
CSV, XML) and storing them in multiple backends (FTP, S3, local ﬁlesystem); an
interactive shell console (IPython aware) for trying out the CSS and XPath expressions
to scrape data, very useful when writing or debugging your spiders.
5
Research Results
The analysis of the conversion series for compliance with ARMA models was
performed. Figure 3 shows the typical values of the Akaike information criterion (AIC),
which is used exclusively for the selection of several statistical models for one set of
data [5, 9]. The values of the criterion indicate that the use of ARMA models in this case
is not appropriate.
Fig. 3. Values of the Akaike information criterion
To carry out the forecasting, time series were divided into two parts, where the ﬁrst
one was used to train the model, and the second one was applied to assess its plausibility.
The models were trained on the S last values of time series.
Checking the models for forecasting m values was carried out in the following way:
take the window of last S values from the ﬁrst part series and will do the forecast one
value ahead; then will move window one value forward, including the forecast of new
value in the window, and will again do the forecast, and so m times.
Figures 4, 5 and 6 presents the results of the forecasts of each model for 7 values
ahead or S = 20 and m = 1. The solid line shows the actual values. Figure 4 shows the
values obtained by the method of exponential smoothing. On Fig. 5 presents ones based
on decision tree. On Fig. 6 values obtained with the help of the LSTM neural network
are shown.
Comparative Analysis of Conversion Series Forecasting
239

Fig. 4. Forecasted values obtained by the method of exponential smoothing
Fig. 5. Forecasted values based on decision tree
Fig. 6. Forecasted values obtained with the help of the LSTM
The predicted values for S = 20 and m = 1 (this choice of parameters is determined
by the requirements of the online store) were computed for 100 values of the daily data
conversion rate and corresponding values of clicks and sales number and other data. The
results of calculations typical for most series are given in Table 1.
240
L. Kirichenko et al.

Table 1. Forecast errors
ES
DT
LSTM
MAD
7.65
6.08
1.2
MA
−1.99
1.32
0.91
MSE
64
53
22
MAPE
0.49
0.38
0.07
As a result of the analysis of the forecasts of diﬀerent values of S and m, the following
was established. The method of exponential smoothing, in spite of its simplicity and
non-exactingness in the amount of data, has in most cases comparatively small predic‐
tion errors. But at the same time, with the use of this method, some predicted values are
signiﬁcantly removed from real ones.
The decision tree method has proved to be inconvenient in the choice of parameters
and has errors comparable with errors of exponential smoothing, but without strongly
remote forecast values. The LSTM neural network, which has a more complex structure
and needs to be preliminarily trained on a rather large time series, has shown good results,
as well as in the overall forecast error, and in the remoteness of forecasts from real time
series values.
6
Conclusion
The results of a study of methods for predicting weakly correlated time series typical of
e-commerce conversion series have shown that exponential smoothing is the simplest,
fastest and most convenient to set up predictive method, but in the cases of complex or
long-term dependencies, it does not apply. The decision tree method is fast in learning,
not diﬃcult to understand, but inconvenient in the choice of parameters and does not
work well when learning on data that have many characteristics. The LSTM neural
network is a cumbersome, long learning, requires a lot of parameters that need to be
selected, but has a very good performance in forecasting and order of magnitude smaller
errors.
References
1. LPgenerator - Professional Landing Page is a platform to increase sales of your business.
http://lpgenerator.ru/blog/2015/07/02/kakoj-dolzhna-byt-veb-analitika-internet-magazina.
Accessed 10 July 2017
2. Stillwagon, A.: 14 Key Performance Indicators (KPIs) to Measure Customer Service. https://
smallbiztrends.com/2015/03/how-to-measure-customer-service.html. Accessed 10 July 2017
3. Conversion Probability Forecast. https://www.searchengines.ru/prognoz_veroyat.html.
Accessed 10 July 2017
4. Wei, D., Geng, P., Ying, L., Shuaipeng, L.: A prediction study on e-commerce sales based
on structure time series model and web search data. In: 26th Chinese Control and Decision
Conference, Changsha, China, pp. 1–4. IEEE (2014)
Comparative Analysis of Conversion Series Forecasting
241

5. Hanke, J.E., Wichern, D.: Business Forecasting, 9th edn. Pearson Prentice Hall, Upper Saddle
River (2008)
6. Guyon, I.J., Elisseeﬀ, A.: An introduction to variable and feature selection. J. Mach. Learn.
Res. 3, 1157–1182 (2003)
7. Ian, H.W., Frank, E., Hall, M.A., Pal, C.J.: Data Mining: Practical Machine Learning Tools
and Techniques, 4th edn. Morgan Kaufmann, Elsevier (2011)
8. Ismail, M., Mansur Ibrahim, M., Mahmoud Sanusi, Z., Nat, M.: Data Mining in electronic
commerce: beneﬁts and challenges. Int. J. Commun. Netw. Syst. Sci. 8, 501–509 (2015)
9. Kirichenko, L., Radivilova, T., Zinkevich, I.: Forecasting weakly correlated time series in
tasks of electronic commerce. In: 12th International Conference Computer Sciences and
Information Technologies, Lviv, Ukraine. IEEE (2017)
10. Qiang, X., Rui-Chun, H., Hui, L.: Data mining research on time series of e-commerce
transaction. Int. J. u- and e- Serv. Sci. Technol. 7(1), 9–18 (2014)
11. Hyndman, R., Koehler, A.B., Keith Ord, J., Snyder, R.D.: Forecasting with Exponential
Smoothing. Springer, Heidelberg (2008)
12. Cielen, D., Meysman, A., Ali, M.: Introducing data science: Big Data, machine learning, and
more, using Python tools. Manning Publications, Greenwich (2016)
13. Lantz, B.: Machine Learning with R, 2nd edn. Packt Publishing, Birmingham (2015)
14. The Unreasonable Eﬀectiveness of Recurrent Neural Networks. http://karpathy.github.io/
2015/05/21/rnn-eﬀectiveness/. Accessed 10 July 2017
15. Understanding LSTM Networks. http://colah.github.io/posts/2015-08-Understanding-
LSTMs. Accessed 10 July 2017
16. LSTM - network of long short-term memory. https://habrahabr.ru/company/wunderfund/
blog/331310/. Accessed 10 July 2017
17. White, D.S., Ariguzo, G.: A time-series analysis of U.S. e-commerce sales. Rev. Bus. Res.
11(4), 134–140 (2011)
18. Hongjiu, G.: Data mining in the application of e-commerce website. In: Du, Z. (ed.)
Intelligence Computation and Evolutionary Computation. Advances in Intelligent Systems
and Computing, vol. 180, pp. 493–497. Springer, Heidelberg (2013)
19. Numpy and Scipy Documentation. https://docs.scipy.org/doc/. Accessed 10 July 2017
20. Documentation of scikit-learn 0.18. http://scikit-learn.org/stable/documentation.html.
Accessed 10 July 2017
21. pandas: powerful Python data analysis toolkit. http://pandas.pydata.org/pandas-docs/stable/.
Accessed 10 July 2017
22. Theano Python library. http://deeplearning.net/software/theano/. Accessed 10 July 2017
23. Scrapy 1.4 documentation. https://docs.scrapy.org/en/latest/. Accessed 10 July 2017
242
L. Kirichenko et al.

Performance Analysis of Open Source Machine Learning
Frameworks for Various Parameters in Single-Threaded
and Multi-threaded Modes
Yuriy Kochura
(✉), Sergii Stirenko, Oleg Alienin, Michail Novotarskiy,
and Yuri Gordienko
National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”,
Kiev, Ukraine
iuriy.kochura@gmail.com
Abstract. The basic features of some of the most versatile and popular open
source frameworks for machine learning (TensorFlow, Deep Learning4j, and
H2O) are considered and compared. Their comparative analysis was performed
and conclusions were made as to the advantages and disadvantages of these plat‐
forms. The performance tests for the de facto standard MNIST data set were
carried out on H2O framework for deep learning algorithms designed for CPU
and GPU platforms for single-threaded and multithreaded modes of operation
Also, we present the results of testing neural networks architectures on H2O plat‐
form for various activation functions, stopping metrics, and other parameters of
machine learning algorithm. It was demonstrated for the use case of MNIST
database of handwritten digits in single-threaded mode that blind selection of
these parameters can hugely increase (by 2–3 orders) the runtime without the
signiﬁcant increase of precision. This result can have crucial inﬂuence for opti‐
mization of available and new machine learning methods, especially for image
recognition problems.
Keywords: Machine learning · Deep learning · TensorFlow · Deep learning4j ·
H2O · MNIST · Multicore CPU · GPU · Neural network · Classiﬁcation · Single-
threaded mode
1
Introduction
Machine learning (ML) is a subﬁeld of Artiﬁcial Intelligence (AI) discipline. This branch
of AI involves the computer applications and/or systems design that based on the simple
concept: get data inputs, try some outputs, build a prediction. Nowadays, machine
learning (ML) has advanced many ﬁelds like pedestrian detection, object recognition,
visual-semantic embedding, language identiﬁcation, acoustic modeling in speech recog‐
nition, video classiﬁcation, fatigue estimation [1], generation of alphabet of symbols for
multimodal human-computer interfaces [2], etc. This success is related to the invention
and application of more sophisticated machine learning models and the development of
software platforms that enable the easy use of large amounts of computational resources
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_17

for training such models [3]. The main aims of this paper are to review some available
open source frameworks for machine learning, analyze their advantages and disadvan‐
tages, and test one of them in various computing environments including CPU and GPU-
based platforms.
Also, we have tested the H2O system by using the publicly available MNIST dataset
of handwritten digits. This dataset contains 60,000 training images and 10,000 test
images of the digits 0 to 9. The images have grayscale values in the range 0:255.
Figure 3 gives an example images of handwritten digits that were used in testing. We
have trained the net by using the host with Intel Core i7-2700K insight. The computing
power of this CPU approximately is 29.92 GFLOPs.
In this paper, we also present testing results of various net architectures by using
H2O platform for single-threaded mode. Our experiments show that net architecture
based on cross entropy loss function, tanh activation function, logloss and MSE stopping
metrics demonstrates better eﬃciency by recognition handwritten digits than other
available architectures for the classiﬁcation problem.
This paper is structured as follows. The Sect. 2 State of the Art contains the short
characterization of some of the most popular and versatile available open source frame‐
works (TensorFlow, Deep Learning4j, and H2O) for machine learning and motivation
for selection of one of them for the performance tests. The Sect. 3 Performance Tests
includes description of the testing methodology, data set used, and results of these tests.
The Sect. 4 Discussion dedicated to discussion of the results obtained and lessons
learned. Also we present here our experimental results where we apply diﬀerent acti‐
vation functions and stopping metrics to the classiﬁcation problem with use case in
single-threaded mode. Section 5 contains the conclusions of the work.
2
State of the Art
During the last decade numerous frameworks for machine learning appeared, but their
open source implementations are seeming to be most promising due to several reasons:
available source codes, big community of developers and end users, and, consequently,
numerous applications, which demonstrate and validate the maturity of these frame‐
works. Below the short characterization of the most versatile open source frameworks
(Deep Learning4j, TensorFlow, and H2O) for machine learning is presented along with
their comparative analysis.
2.1
Deep Learning4j
Deep Learning4j (DL4J) is positioned as the open-source distributed deep-learning
library written for Java and Scala that can be integrated with Hadoop and Spark [4]. It
is designed to be used on distributed GPUs and CPUs platforms, and provides the ability
to work with arbitrary n-dimensional arrays (also called tensors), and usage of CPU and
GPU resources. Unlike many other frameworks, DL4J splits the optimization algorithm
from the updater algorithm. This allows to be ﬂexible while trying to ﬁnd a combination
that works best for data and problem.
244
Y. Kochura et al.

2.2
TensorFlow
TensorFlow is an open source software library for numerical computation was originally
developed by researchers and engineers working on the Google Brain Team within
Google’s Machine Intelligence research organization [5] for the purposes of conducting
machine learning and deep neural networks research. This software is the successor to
DistBelief, which is the distributed system for training neural networks that Google has
used since 2011. TensorFlow operates at large scale and in heterogeneous environments.
This system uses dataﬂow graphs to represent computation, shared state, and the oper‐
ations that mutate that state. It maps the nodes of a dataﬂow graph across many machines
in a cluster, and within a machine across multiple computational devices, including
multicore CPUs, general purpose GPUs, and custom-designed ASICs known as Tensor
Processing Units (TPUs). Such architecture gives ﬂexibility to the application developer:
whereas in previous “parameter server” designs the management of shared state is built
into the system, TensorFlow enables developers to experiment with novel optimizations
and training algorithms.
2.3
H2O
H2O software is built on Java, Python, and R with a purpose to optimize machine
learning for Big Data [6]. It is oﬀered as an open source platform with the following
distinctive features. Big Data Friendly means that one can use all of their data in real-
time for better predictions with H2O’s fast in-memory distributed parallel processing
capabilities. For production deployment a developer need not worry about the variation
in the development platform and production environment. H2O models once created
can be utilized and deployed like any Standard Java Object. H2O models are compiled
into POJO (Plain Old Java Files) or a MOJO (Model Object Optimized) format which
can easily embed in any Java environment. The beauty of H2O is that its algorithms can
be utilized by various categories of end users from business analysts and statisticians
(who are not familiar with programming languages using its Flow web-based GUI) to
developers who know any of the widely used programming languages (e.g. Java, R,
Python, Spark). Using in-memory compression techniques, H2O can handle billions of
data rows in-memory, even with a fairly small cluster. H2O implements almost all
common machine learning algorithms, such as generalized linear modeling (linear
regression, logistic regression, etc.), Naive Bayes, principal components analysis, time
series, k-means clustering, Random Forest, Gradient Boosting, and Deep Learning.
2.4
Parameters of Machine Learning
The Activation Functions. Activation functions also known as transfer functions are
used to map input nodes to output nodes in certain fashion [7] (see the conceptual scheme
of an activation function in Fig. 1).
Performance Analysis of Open Source Machine Learning Frameworks
245

Fig. 1. The role of activation function in the process of learning neural net.
Functions with dropout are used for reducing overfitting by preventing complex
co-adaptations on training data. This technique is known as regularization. Figure 2
demonstrate the difference between standard neural net and neural net after applying
dropout [8].
Fig. 2. An example of standard neural net on the left and neural net with dropout on the right
with 2 hidden layers.
Constant Parameters of the Training Model. We have used the network model with
such constant parameters, namely:
• Response variable column is C785
• Hidden layer size is [50, 50]
• Epochs are 500
246
Y. Kochura et al.

• Seed for random numbers is 2
• Adaptive learning rate is false
• Initial momentum at the beginning of training is 0.9
• Final momentum after the ramp is 0.99
• Input layer dropout ratio for improving generalization is 0.2
• Stopping criterion for classiﬁcation error fraction on training data is disable
• Early stopping based on convergence of stopping metric is 3
• Relative tolerance for metric-based stopping criterion is 0.01
• Compute variable impotence for input features is true
• Sparse data handling is true
• Force reproducibility on small data is true
Variable Parameters of the Training Model. We have used the network model with
such variable parameters, namely:
• Activation function: Tanh, TanhWithDropout, Maxout, MaxoutWithDropout, Recti‐
ﬁer, RectiﬁerWithDropout
• Metric to use for early stopping: logloss, misclassiﬁcation, MAE, MSE, RMSE and
RMSLE
• Loss function: Cross Entropy
Loss function is a function that used to measure the degree of ﬁt. The cross entropy
loss function for the distributions p and q over a given set is deﬁned as follows:
H(p, q) = H(P) + DKL(p ∥q)
(1)
where H(p) is the entropy of p, and DKL(p ∥q) is the Kullback–Leibler divergence of q
from p (also known as the relative entropy of p with respect to q). Cross entropy is always
larger than entropy.
2.5
Comparative Analysis
From the point of view of an end user, several aspects of these frameworks are of the
main interest. Except for performance and maturity, the open source frameworks could
be attractive and useful, if they have the wide language and operating system support
(see Table 1).
All of these frameworks are characterized by a quite wide ranges of supported
languages and operating systems. But nowadays it is not enough in the view of the fast
development of parallel and distributed computing like cluster and, especially, GPGPU
computing. In this connection, TensorFlow has clear notiﬁcation as to the pre-requisites
for NVIDIA GPGPU cards, that should have CUDA Compute Capability (CC) 3.0 or
higher. As to DL4J this is not clear because the developers stated just general support
of NVIDIA GPGPU cards from GeForce GTX to Titan and Tesla that have various CC
from 2.0 to 3.5. For H2O types of supported NVIDIA cards and CC are not speciﬁed,
but proposed in the branching sub-framework Deep Water. The additional important
aspects are the low entrance barrier and fast learning curve. They usually are based on
Performance Analysis of Open Source Machine Learning Frameworks
247

the convenient graphical user interface, workﬂow management, and visualization tools.
Now these features become “de facto standard” tools for integration of end users, work‐
ﬂows, and resources. The examples of their implementations (like WS-PGRADE/gUSE
[9], KNIME [10], etc.) and applications in physics [11], chemistry [12], astronomy [13],
brain-computing [14], eHealth [15] can be found elsewhere. In this context TensorFlow
and H2O propose web-based graphic user interfaces TensorBoard and Flow, respec‐
tively, which are actually workﬂow management and visualization tools. In contrast to
other frameworks H2O proposes the much shorter learning curve due to Flow, the web-
based and self-explanatory user interface. In general, Flow allows end users without
experience in software programming even to import remote data, create model, train it,
validate it, and then save the whole workﬂow. In addition, the machine learning model
developed in Flow can be compiled into Plain Old Java Files (POJO) format, which can
be easily embedded in any Java environment. Due to these advantages, now more than
5000 organizations currently use H2O, and many well-known companies (like Cisco,
eBay, PayPal, etc.) are using it for big data processing. This data set contains 785
columns. The ﬁnal column is the correct answer, 0 to 9. The ﬁrst 784 are the 28 × 28
grid of grayscale pixels, and each is 0 (for white) through to 255 (for black).
Table 1. Comparison of machine learning frameworks.
System (initial
release)
GPU support
GUI
Operating system
Language support
TensorFlow (2015)
NVIDIA GPUs
(CC 3.0 or higher)
TensorBoard
(workﬂow,
visualization)
Linux, macOS,
Windows, Android,
iOS
Python, C++
DL4J (2013)
NVIDIA GPUs
(Tesla, Titan)
–
Linux, macOS,
Windows, Android
Java, Scala, CUDA,
C, C++, Python
H2O (2011)
Deep Water,
NVIDIA GPUs
(CC not stated)
Flow (workﬂow,
visualization,
POJO)
Linux, macOS,
Windows
Java, Python, R
3
Performance Tests
The performance of the mentioned frameworks was a topic of many investigations
performed by developers of these frameworks and independent end users [16]. But
performance of H2O was not investigated thoroughly except for its developers for
unknown CPU and GPU platforms [17]. That is why H2O was selected for performance
tests in this paper.
The data set used in this work, called the “MNIST data,” was proposed in 1998 to
identify handwritten numbers. We have tested the H2O system by recognizing the
handwritten digits (Fig. 3) from the publicly available MNIST data set for machine
learning methods [18]. Now it is well-known “de facto standard” data set for a typical
“easy-for-humans-but-hard-for-machine” problem. The used MNIST database of hand‐
written digits has a training set of 60,000 examples, and a test set of 10,000 examples.
Each digit is represented by 28 × 28 = 784 gray-scale pixel values (features).
248
Y. Kochura et al.

Fig. 3. The examples of the handwritten digits from MNIST data set.
The tests were performed on diﬀerent platforms including Intel Core i5-7200U with
4 cores (CPU1), Intel Core i7-2700K with 8 cores (CPU2), NVIDIA Tesla K40 GPU
accelerator using single-threaded and multi-threaded modes of operation. The parame‐
ters of neural network were the same for the Deep Learning (CPU only) and Deep Water
(CPU+GPU) algorithms. The details of these platforms and modes of operation are
given above in Tables 2 and 3.
Table 2. Multi-threaded operation on CPUs.
Parameter’s name
Intel Core i5-7200U (CPU1)
Intel Core i7-2700K (CPU2)
GFLOPs
13.85
29.92
Duration
2 min 18 s
2 min 32 s
Training speed, obs/sec
23746
78972
Epochs
48.5953
108.3821
Iterations
103
65
Training logloss
0.0407
0.0297
Validation logloss
0.1584
0.1616
Table 3. Single-threaded operation on CPUs.
Parameter’s name
Intel Core i5-7200U (CPU1)
Intel Core i7-2700K (CPU2)
GFLOPs
13.85
29.92
Runtime
2 min 15 s
2 min 5 s
Training speed, obs/sec
13820
15174
Epochs
26
26
Iterations
26
26
Training logloss
0.0577
0.0577
Validation logloss
0.1664
0.1664
Performance Analysis of Open Source Machine Learning Frameworks
249

The performance tests were carried out with Rectiﬁer activation function for two
algorithms Deep Learning (CPU only) and Deep Water (CPU+GPU). The stopping
criterion was based on convergence of stopping_metric (equal to misclassiﬁcation). The
stop event occurs, if simple moving average of length k of the stopping_metric does not
improve for k: = stopping_rounds (equal to 3) scoring events. The relative tolerance for
metric-based stopping criterion was equal to 0.01. The typical convergence of training
(lower) and validation (upper) logloss values with epochs is shown on Fig. 4. The results
of these performance tests using H2O system are presented above in Tables 2, 3 and 4.
It should be noted that the results of learning neural network to recognize the handwritten
digits on CPUs and GPU by using multi-threaded mode of operation are inherently not
reproducible due to randomization. To estimate data scattering in multi-threaded modes
of operation the runs were repeated for 12 times with determination of mean and standard
deviation (Table 4).
Fig. 4. Evolution of training (lower) and validation (upper) logloss values.
250
Y. Kochura et al.

Table 4. Multi-threaded operation on GPU (1.43 TFLOPs)
Parameter’s name
Mean value
Standard deviation
Runtime
2 min 29 s
17.2 s
Training speed, obs/sec
18707
520
Epochs
42.24
5.66
Iterations
2475
332
Training logloss
0.285
0.0192
Validation logloss
0.437
0.0236
4
Discussion
The time of convergence for logloss values with epochs was not very diﬀerent for all
regimes, if the standard deviation (~17 s) of duration for multi-threaded operation on
GPU will be taken into account as an estimation (Fig. 5).
Fig. 5. Duration of training.
Despite the much higher computing power of GPU the better training speed was
observed for multi-threaded regime for CPU2 with 8 cores with speedup up to 5.2 in
comparison to single-threaded regime (Fig. 6). For CPU1 with 4 cores the similar
speedup for multi-threaded regime was equal to 1.7 in comparison to single-threaded
regime. As to GPU training speed these results can be explained by much bigger number
(by ~100 times) of performed iterations.
As it is well-known the logloss values are very sensitive to outliers and this tendency
is very pronounced in the case of GPU, where the much bigger iterations were used and
higher training logloss values were found (Fig. 7).
Performance Analysis of Open Source Machine Learning Frameworks
251

Fig. 7. Training logloss values.
The ratio of validation logloss (Fig. 8) to training logloss is equal to 1.53 for Deep
Water case, which is much lower in comparison to the same ratio 2.88 for Deep Learning
single-threaded case, and 3.89 and 5.44 even for Deep Learning multi-threaded case
CPU1 and CPU2, respectively. This allows to make assumption that the more iterations
in GPU mode give the more realistic model with the lower risk of overﬁtting.
Finally, in this paper we described the basic features of some open source frame‐
works for machine learning, namely TensorFlow, Deep Learning4j, and H2O. For
usability and performance tests H2O framework was selected. It was tested on several
platforms like Intel Core i5-7200U (4 cores), Intel Core i7-2700K (8 cores), Tesla K40
GPU with the goal to evaluate their performance in the context of recognizing hand-
written digits from MNIST data set. To reach this goal the same parameters of the neural
network were used for Deep Learning and Deep Water algorithms. The inﬂuence of
many other aspects like nature of data (for example, sparsity level and sparsity pattern),
number of hidden layers and their sizes should be taken into account for the better
comparative analysis, but these aspects were out of scope of the current work and will
be published in separate paper elsewhere [19].
Fig. 6. Training speed.
252
Y. Kochura et al.

We trained neural networks for classiﬁcation problems on publicly available MNIST
dataset of handwritten digits with use case in single-threaded mode. We found that
generalization performance has very strong dependence on activation function and very
slight dependence on stopping metric. Figure 9 shows the runtime values on the loga‐
rithm scale obtained for these diﬀerent architectures as training progresses.
Fig. 9. Runtime of learning nets diﬀerent architectures.
Figure 10 demonstrates the eﬀectiveness of using tanh activation function for all
stopping metrics that considered in this paper. In the case of the learning net based on
the tanh activation function, MAE and RMSLE stopping metric has achieved the logloss
value of 0.0104. These architectures demonstrate better training prediction ability than
others but take much time for building model.
Fig. 8. Validation logloss values.
Performance Analysis of Open Source Machine Learning Frameworks
253

Fig. 10. Training logloss of learning nets diﬀerent architectures.
In order to ﬁnd the best neural net architecture for digits recognition just needs to
look at the behavior of models on unknown data should be checked. Figure 11 shows
the validation error rates for diﬀerent architectures that are considered here. We see, the
best digit’s recognition results were achieved in the case of tanh activation function. The
type of stopping metric is very slightly eﬀects on the values of the validation error but
it does very much on the runtime of building model.
Fig. 11. Validation logloss of learning nets diﬀerent architectures.
254
Y. Kochura et al.

5
Conclusions
The work carried out and the results obtained allow us to make the following conclusions
as to H2O framework:
• H2O propose the unprecedentedly fast learning curve due to the available web-based
GUI, easy workﬂow management tools, and visualization tools for representation of
data.
• H2O allows the data scientists without any programming experience easily operate
by several deep learning backends (mxnet, Caﬀe, TensorFlow) with various activa‐
tion functions (rectiﬁer, tahn), various parameters of neural network, stopping
criteria, and convergence conditions.
• H2O propose opportunities for reproducible single-threaded and non-reproducible
multi-threading modes of operation for multicore CPUs and GPUs.
• Multi-threaded operations on CPUs give the smaller logloss values than single-
threaded operations, but the ratio of validation logloss to training logloss is much
lower in comparison to multi-threaded operations on GPU, which gives the more
realistic model with the lower risk of overﬁtting.
In this paper, we present the results of testing neural networks architectures on H2O
platform for various activation functions, stopping metrics, and other parameters of
machine learning algorithm. It was demonstrated for the use case of MNIST database
of handwritten digits in single-threaded mode that blind selection of these parameters
can hugely increase (by 2–3 orders) the runtime without the signiﬁcant increase of
precision. This result can have crucial inﬂuence for optimization of available and new
machine learning methods, especially for image recognition problems.
During the process of testing H2O, we found out that generalization performance
has very strong dependence on activation function and very slight dependence on stop‐
ping metric. The best results of recognition digits were achieved in case of using nets
architecture based on tanh activation function, logloss and MSE stopping metrics.
This paper summarizes the activities which were started recently and described
shortly in the previous student paper [20].
Acknowledgements. The work was partially supported by NVIDIA Research and Education
Centers in National Technical University of Ukraine “Igor Sikorsky Kyiv Polytechnic Institute”.
References
1. Gordienko, N., Stirenko, S., Kochura, Y., Alienin, O., Novotarskiy, M., Gordienko, Y., Rojbi,
A.: Deep learning for fatigue estimation on the basis of multimodal human-machine
interactions. In: XXIX IUPAP Conference on Computational Physics, CCP2017, Paris,
France (2017)
2. Hamotskyi, S., Rojbi, A., Stirenko, S., Gordienko, Y.: Automatized generation of alphabets
of symbols for multimodal human computer interfaces. In: Proceedings of Federated
Conference on Computer Science and Information Systems, FedCSIS-2017, Prague, Czech
Republic (2017)
Performance Analysis of Open Source Machine Learning Frameworks
255

3. Witten, I.H., Frank, E., Hall, M.A., Pal, C.J.: Data Mining: Practical Machine Learning Tools
and Techniques. Morgan Kaufmann (2016)
4. Team, D.J.D.: Deep Learning4j: open-source distributed deep learning for the JVM. Apache
Software Foundation License
5. Abadi, M., et al.: TensorFlow: a system for large-scale machine learning. In: 12th USENIX
Symposium on Operating Systems Design and Implementation, OSDI 2016, Savannah, GA,
USA, pp. 265–283 (2016)
6. Candel, A., Parmar, V., LeDell, E., Arora, A.: Deep Learning with H2O. AI Inc. (2016)
7. Activation 
Functions. 
https://medium.com/towards-datascience/activation-functions-in-
neural-networks-58115cda9c96
8. Srivastava, N., et al.: Dropout: a simple way to prevent neural networks from overﬁtting. J.
Mach. Learn. Res. 15(1), 1929–1958 (2014)
9. Kozlovszky, M., et al.: DCI bridge: executing WS-PGRADE workﬂows in distributed
computing infrastructures. In: Science Gateways for Distributed Computing Infrastructures,
pp. 51–67. Springer, Cham (2014)
10. O’Hagan, S., Kell, D.B.: Software review: the KNIME workﬂow environment and its
applications in genetic programming and machine learning. Genet. Program. Evolvable
Mach. 16(3), 387–391 (2015)
11. Gordienko, Y., et al.: IMP science gateway: from the portal to the hub of virtual experimental
labs in e-science and multiscale courses in e-learning. Concurrency Comput. Pract.
Experience 27(16), 4451–4464 (2015)
12. Herres-Pawlis, S., et al.: Quantum chemical meta-workﬂows in MoSGrid. Concurrency
Comput. Pract. Experience 27(2), 344–357 (2015)
13. Castelli, G., et al.: VO-compliant workﬂows and science gateways. Astron. Comput. 11, 102–
108 (2015)
14. Stirenko, S., et al.: User-driven intelligent interface on the basis of multimodal augmented
reality and brain-computer interaction for people with functional disabilities. arXiv:
1704.05915 (2017)
15. Gordienko, Y., et al.: Augmented coaching ecosystem for non-obtrusive adaptive
personalized elderly care on the basis of Cloud-Fog-Dew computing paradigm. In: 40th
International Convention on Information and Communication. Technology, Electronics and
Microelectronics (MIPRO) (2017)
16. Fox, J., Zou, Y., Qiu, J.: Software Frameworks for Deep Learning at Scale, Internal Indiana
University Technical Report (2016)
17. H2O Deep Learning Benchmark. https://github.com/h2oai/h2o-3/tree/master/h2o-algos/src/
main/java/hex/DeepLearning
18. LeCun, Y., Cortes, C., Burges, C.J.: The MNIST database of handwritten digits (1998). http://
yann.lecun.com/exdb/mnist
19. Kochura, Y., Stirenko, S., Rojbi, A., Alienin, O., Novotarskiy, M., Gordienko, Y.:
Comparative analysis of open source frameworks for machine learning with use case in single-
threaded and multi-threaded modes. In: IEEE XII International Scientiﬁc and Technical
Conference on Computer Sciences and Information Technologies, CSIT 2017, Lviv, Ukraine
(2017)
20. Kochura, Y., Stirenko, S., Gordienko, Y.: Comparative performance analysis of neural
networks architectures on H2O platform for various activation functions. In: YSF-2017, Lviv,
Ukraine
256
Y. Kochura et al.

On Scalability of Predictive Ensembles
and TradeoﬀBetween Their Training
Time and Accuracy
Pavel Kord´ık(B) and Tom´aˇs Fr´yda
Department of Computer Science, Faculty of Information Technology,
Czech Technical University in Prague, Prague, Czech Republic
kordikp@fit.cvut.cz
Abstract. Scalability of predictive models is often realized by data
subsampling. The generalization performance of models is not the only
criterion one should take into account in the algorithm selection stage.
For many real world applications, predictive models have to be scalable
and their training time should be in balance with their performance. For
many tasks it is reasonable to save computational resources and select an
algorithm with slightly lower performance and signiﬁcantly lower train-
ing time. In this contribution we made extensive benchmarks of predic-
tive algorithms scalability and examined how they are capable to trade
accuracy for lower training time. We demonstrate how one particular
template (simple ensemble of fast sigmoidal regression models) outper-
forms state-of-the-art approaches on the Airline data set.
Keywords: Combining classiﬁers · Regression models
Model blending · Scalability · Map reduce
1
Introduction
There are not many predictive modeling algorithms that can be considered scal-
able. Often, data subsampling in both dimensionality (number of attributes) and
numerosity (number of instances) is required in order to train model in reason-
able time. Data reduction is however not always good strategy, especially when
redundancies, correlations and noise cannot be further eliminated.
In this case, it is beneﬁcial to use a scalable predictive algorithm. Scalability
can be realized by clever training and base algorithms that can be scaled to
millions of instances [1]. Other approach is to use some form of famous map-
reduce strategy [2]. More general data manipulation strategies can be found in
ensemble methods. The popularity of ensemble methods in machine learning
is rising steadily and very complex forms of ensembles, including hierarchical
ensembles are described in the literature.
c
⃝Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_18

258
P. Kord´ık and T. Fr´yda
Moreover, it is necessary to take into account the computational complexity
of the algorithm selection process. It is not feasible to run all candidate algo-
rithms on a new data set to select the best performing one, simply because there
are inﬁnite number of available algorithms.
One approach how to select a training algorithm for a new data set in a rea-
sonable time is to use a meta-data collected during training on similar data sets.
Meta-learning approaches [3] utilizing meta-data have been studied intensively
in past few decades. They can predict performance of algorithms on new data
sets allowing to select good performing algorithm among multiple candidates.
Majority of meta-learning approaches [3–6] simply select from a set of few
predeﬁned fully speciﬁed data mining algorithms the one, producing models with
the best generalization performance for the given data.
More advanced meta-learning approaches combine algorithm selection and
hyperparameter optimisation such as CASH [7] elaborated within the INFER
project. In [8–10] data mining workﬂows are optimized including data cleaning
and preprocessing steps together with selected hyperparameters of modeling
methods.
We focus on modeling stage only and take one step further. In our approach
we optimize structure of algorithmic ensembles together with their hyperpara-
meters as explained in Sect. 4. In this way, we can discover new algorithmic
building blocks.
The hierarchical structure of algorithmic ensembles is represented by Meta-
learning algorithm templates. Our templates indicate which learning algorithms
are used and how their outputs are fed into other learning algorithms.
We use the genetic programming [11] to evolve structure of templates and a
parameter optimization to adjust parameters of algorithms for speciﬁc data sets.
Our approach allocates exponentially more time for evolution of above-average
templates similarly to the Hyperband approach [12].
Furthermore, templates evolved (discovered) on small data sets can be reused
as building blocks for large data sets.
Building predictive models on large data samples is a challenging task. Learn-
ing time of algorithms often grows fast with number of training data samples and
dimensionality of a data set. Hyper-parameter optimization can help us to gen-
erate more precise models for given task, but it adds signiﬁcant computational
complexity to the training process. We show that templates evolved on small
data subsamples can outperform state of the art algorithms including complex
ensembles in terms of performance and scalability.
First, we brieﬂy describe building blocks of predictive ensembles.
2
Base Algorithms and Ensembling Strategies
We build ensembles from fast weak learners [13]. Many of our base models resem-
ble neurons with diﬀerent activation functions. We can use base models to con-
struct both classiﬁcation and regression ensembles. In this contribution, we focus
on classiﬁcation tasks only, however regression models can also be present in clas-
siﬁcation ensembles.

On Scalability of Predictive Ensembles
259
The classiﬁcation task itself can be decomposed into regression subproblems
by separation of single classes from the others. These binary class separation
problems can be solved by regression models – by estimating continuous class
probabilities. The maximum probability class is then considered as output value.
The classiﬁer consisting of regression models is further referred to as Classiﬁer-
Model.
Fig. 1. Nested ensembles can be represented by a template. Using wildcards, speciﬁc
(or predeﬁned in other words) template can be generalized to represent set of templates.
2.1
Base Algorithms
Training regression models is fast and straightforward. We use several activation
functions, namely Sigmoid, SigmoidNorm, Sine, Polynomial, Gaussian,
Exponential and Linear.
To train coeﬃcients of linear or polynomial models, the General Least
Squares method [14] is applied. For models non-linear in their coeﬃcients, an
iterative optimization process is needed. We compute analytic gradients of error
for all fast regression models and employ quasi-Newton method [15] to optimize
their parameters.
As an example, we show how SigmoidNorm models are trained. The output
of the model yj for jth instance with target variable dj and input vector xj can
be computed as
yj =

1 + exp
n−1

i=0
aixij + an



	
ρij

−1
where coeﬃcients a should be optimized to ﬁt a training data and reduce the
error E = m
j=1 (yj −dj)2 of the model.
The gradient of error can be computed as ∇E =

∂E
∂a0 , ∂E
∂a1 , · · · , ∂E
∂an

, where
∂E
∂ai = m
j=1
∂E
∂yj · ∂yj
∂ai and
∂E
∂yj = 2 m
j=1 (yj −dj). The last partial derivative
∂yj
∂ai has to be computed for each coeﬃcient ai.

260
P. Kord´ık and T. Fr´yda
In case of the SigmoidNorm model from Eq. 2.1, we can compute partial
derivatives as follows
∂E
∂ai = m
j=1
∂E
∂yj · ∂yj
∂ρj · ∂ρj
∂ai . Then the components of the
gradient are
∂E
∂an
= −2
m

j=1
(1 + eρij)−2 · eρij,
∂E
∂ai
= −2
m

j=1

(yj −dj) · (1 + eρij)−2 · eρij · ai

.
Accordingly, we derived analytic gradients of error on training data for
Exponential, Gaussian, Sigmoid and Sine models. Gradients are then supplied
together with errors to the quasi-Newton optimization method [16] during the
training to speed up the convergence. More details can be found in [17] and the
source code is also available [18].
The LocalPolynomial base model as well as Neural Network (NN),
Support Vector Machine (SVM), Naive Bayes classiﬁer (NB), Decision
Tree (DT), K-Nearest Neighbor (KNN) were adopted from the Rapidminer
environment [19].
2.2
Ensembling Algorithms
The performance of models can often be further increased by combining or
ensembling [20–25] base algorithms, particularly in cases where base algorithms
produce models of insuﬃcient plasticity or models overﬁtted to training data
[26].
A detailed description of the large variety of ensemble algorithms can be
found in [27]. We brieﬂy describe the ensembling algorithms that are used in
our experiments. Bagging [28] is the simplest one; it selects instances for base
models randomly with repetition and combines models with simple average.
Boosting [23] specializes models on instances incorrectly handled by previous
models and combines them with weighted average. Stacking [22] uses a meta
model, which is learned from the outputs of all base models, to combine them.
Another ensemble utilizing meta models is the Cascade Generalization [29],
where every model except the ﬁrst one uses a data set extended by the output
of all preceding models. Delegating [30] and Cascading [31,32] both use a
similar principle: they operate with certainty of model output. The latter model
is specialized not only in instances that are classiﬁed incorrectly by previous
models, but also in instances that are classiﬁed correctly, but previous models
are not certain in terms of their output. Cascading only modiﬁes the probability
of selecting given instances for the learning set of the next model. Arbitrating
[33] uses a meta-model called referee for each model. The purpose of this meta-
model is to predict the probability of correct output. All methods used in this
study were implemented within the FAKE GAME open source project [18].

On Scalability of Predictive Ensembles
261
3
Meta-learning Templates
The meta-learning template [3] is a prescription how to build hierarchical super-
vised models. In the most complex case, it can be a collection of ensembling
algorithms and base algorithms combined in a hierarchical manner, where base
algorithms are leaf nodes connected by ensembling nodes. Regression models or
classiﬁers deeper in the hierarchy can be more specialized to a particular subset
of data samples or attributes. This scheme decomposes the prediction problem
into subproblems and combines the ﬁnal solution (model) from subsolutions. The
procedure of problem decomposition depends on ensembling methods. Typically,
it distributes data to member models and when all outputs are available, they
are combined to the ensemble output.
Similarly to the Holland’s schema theorem [34], we can deﬁne ﬁtness of a
template as average/maximum ﬁtness of individual algorithms represented by
this particular template. Wildcards here are used just as placeholders for random
decisions on type of ensembles or base algorithms and their parameters. On the
contrary, in rooted tree schema theory [35] wildcards represent sub-trees.
4
Discovering Templates
The meta-learning template can be designed manually using an expert knowledge
(for example, bagging boosted decision trees showed good results on several
problems) so it is likely to perform well on a new data set. This is however not
guaranteed.
In our approach we optimize templates on data sub-samples using a genetic
programming [11]. In this way, we can search the space of possible architectures
of hierarchical ensembles and optimize their parameters simultaneously.
5
Templates at Scale
Recent rise of big data modeling challenges scalability of predictive modeling
algorithms and tools. One obvious approach is to reduce dimensionality and
numerosity of data [36]. This approach works in most of the cases because big
data are often redundant. However for some data sets, the performance of pre-
dictors increase signiﬁcantly with growing number of instances used for training.
For such data, scalable algorithms [37] and tools [38,39] have been developed.
Most of these approaches are based on a map-reduce technique [40].
In this section, we show, that meta-learning can be also used at scale. Our
approach is inspired by [41], where classiﬁer selected on sub-samples work reason-
ably well on larger data sets. We evolve templates on a subset of 3000 randomly
selected instances. Then, evolved template can be executed on full data. When
we do not have enough time for the meta-learning template evolution, it is also
possible to generate the subset just for computing meta-features. Then we can
use a best performing template for the data set with most similar meta-features.

262
P. Kord´ık and T. Fr´yda
For the template execution we split large data into multiple disjoint sub-
sets and then use the map-reduce paradigm to train multiple instances of the
template. Prediction is made by reducing (majority voting) of models generated
from templates.
This approach is very similar to bagging except that we do not use the
bootstrap sampling.
5.1
Experiments
We have conducted experiments to get insight into the scalability of several
machine learning algorithms from H2O as well as our parallel training of tem-
plates. Our motivation is to show that proper algorithm selection is important
especially for large data sets and can be often done using a fraction of the data
set.
We have chosen two public data sets—Airline Delays which is available
through H2O [42] and HIGGS [43]. Those data sets are used for binomial clas-
siﬁcation of selected output attributes.
Fig. 2. Comparison of several machine learning algorithms in H2O.ai trained on sam-
ples with various sizes from Higgs [43] data set

On Scalability of Predictive Ensembles
263
Fig. 3. Predicting IsArrDelayed on Airline data set: comparison of algorithms in H2O.ai
trained on subsamples of increasing size.
We benchmark our paralelized templates to models available in H2O.ai imple-
mented using the map reduce approach. Generalized Linear Model [44] is
using logistic regression to deal with classiﬁcation problems. Naive Bayes classi-
ﬁer assumes Independence of input attributes and classiﬁes based on conditional
probabilities obtained from training data. Deep learning [38] is a feedforward
neural network with various activation functions in neurons. Distributed Ran-
dom Forest and Gradient Boosted Machine [45] are ensembles based on
decision trees. H2O Ensemble is an ensemble classiﬁer called Super Learner
by [46].
Following experiments use 1 000 000 randomly selected rows from each data
set. Then 50% rows is randomly selected as test set and the rest is then sampled
to subsets of growing size to examine scalability of algorithms. This sampled
data are randomly split to training set (80%) and validation set (20%).
At ﬁrst, we examined scalability of algorithms on the Higgs data set. Figure 2
shows learning time and performance of individual algorithms executed on sub-
sets of growing size. The best performance was achieved by Deep Learning which
was also reasonably fast. Gradient Boosting is faster, but it does not have capac-
ity to improve with bigger data subsets. Distributed Random Forest is also rea-
sonably accurate and fast, but it is dominated by Deep Learning on Higgs.

264
P. Kord´ık and T. Fr´yda
0
200
400
600
800
1000
1200
1400
Training time [s]
Generalized Linear Model
Gradient Boosting
H2O Ensemble
NaiveBayes
0.0
0.2
0.4
0.6
0.8
1.0
Subset Fraction
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Accuracy
Deep Learning
Distributed Random Forest
ENS - CascadeGenProb{5x Boosting{5x ClassifierModel{<outputs>x SigmoidNorm}}}
ENS - Sigmoid Norm classifier
Fig. 4. Predicting IsDepDelayed on Airline data set: comparison of algorithms in
H2O.ai trained on subsamples of increasing size.
Ensembles produced from templates are not very competitive on this data set.
Only complex hierarchical ensemble of decision trees is approaching the perfor-
mance of Distributed Random Forest, but it is much slower. Our implementation
is not optimized for H2O.ai.
Looking at the Fig. 3, where arrival delay is predicted on the Airlines data
set, results are completely diﬀerent. Our ensembles are both more accurate and
faster. The diﬀerence is so big, that we decided to analyze these results further.
We even simpliﬁed the prediction task by predicting the departure time with-
out removing the DepTime attribute.
The prediction problem becomes then quite trivial, because you can obtain
the target (is departure delayed?) by comparing DepTime and CRSDepTime
attribute. It is quite surprising that most of the classiﬁers are mislead by other
Fig. 5. Optimization of several machine learning algorithms using Random Search and
SMAC

On Scalability of Predictive Ensembles
265
Fig. 6. Decision boundaries of algorithms on problem of predicting aircraft departure
delay. Simple ensemble of sigmoid classiﬁers was able to generalize the relationship well,
whereas decision tree based ensembles overﬁtted the data. Deep Learning discovered
the relationship only on large data samples.

266
P. Kord´ık and T. Fr´yda
attributes and fail to discover this simple relationship. Figure 4 shows that again
our simple ensembles based on Sigmoidal model are able to learn fast and solve
the problem even on small subsets. H2O Ensemble and Deep Learning discov-
ered the relationship on 500 thousand instances and their learning time was
signiﬁcantly higher.
To ensure that the problem is not caused by improper parameter settings,
we run optimization of parameters on a subset of 100 thousand instances. The
list of parameters and their ranges are available [47]. Figure 5 shows that most
of the H2O algorithms are very sensitive to improper parameter settings. Deep
learning was able to converge in default parameter setting only, our assump-
tion is that parameters are controlled adaptively by default. Similarly, negative
impact was observed for Generalized Linear Model. For Gradient Boosting and
Distributed Random Forest, optimization discovered better performing conﬁgu-
ration, however the diﬀerence was not signiﬁcant. We also optimized number of
models in our hierarchical ensembles but apparently it had almost no eﬀect on
performance. Decision Tree based ensemble was unable to solve the task in any
conﬁguration which is consistent with poor performance of DT based ensembles
from H2O. On the other hand the Sigmoid based ensemble was able to discover
the relationship even with minimal number of models in the ensemble which is
consistent with previous experiments. From boxplots and distribution of individ-
ual results (red dots) the Bayesian Optimization (SMAC) method outperformed
the Random search.
Plots of class probabilities and decision boundaries helped us to reveal the
reason of poor performance of decision tree based ensembles. Figure 6 shows that
successful classiﬁers (ensemble of sigmoid models, Deep Learning) were able to
identify simple relation of two input attributes to departure delay prediction.
The relationship (decision boundary) is hard for decision trees to model with
their orthogonal decisions. It is also impossible to solve for Naive Bayes classiﬁer
assuming independence of input attributes.
Apparently, we were able to discover very eﬃcient template for this trivial
problem. We believe that our approach can contribute to evolve (discover) tem-
plates for diverse data sets and predictive tasks. Building library of algorithmic
templates can improve capacity of predictive modeling systems to solve diverse
tasks eﬃciently.
6
Conclusions
We show how ensembles of predictive models (meta-learning templates) can be
scaled up for large data sets modeling using the map-reduce approach. Bench-
marks revealed that our approach is able to produce algorithms competitive with
state of the art approaches for large scale predictive modeling. Ensembles of sim-
ple regression models can outperform popular algorithms in both generalization
ability and scalability as demonstrated on the Airlines data set.

On Scalability of Predictive Ensembles
267
Acknowledgments. This research was partially supported by the Data mining from
unstructured data (SGS16/119/OHK3/1T/18) grant of the Czech Technical Univer-
sity in Prague.
References
1. Segata, N., Blanzieri, E.: Fast and scalable local kernel machines. J. Mach. Learn.
Res. 11(June), 1883–1926 (2010)
2. Dean, J., Ghemawat, S.: MapReduce: simpliﬁed data processing on large clusters.
Commun. ACM 51(1), 107–113 (2008)
3. Kord´ık, P., ˇCern´y, J.: Self-organization of supervised models. In: Jankowski, N.,
Duch, W., Graczewski, K. (eds.) Meta-learning in Computational Intelligence.
Studies in Computational Intelligence, vol. 358, pp. 179–223. Springer, Heidelberg
(2011)
4. Sutherland, A., Henery, R., Molina, R., Taylor, C.C., King, R.: StatLog: Compari-
son of Classiﬁcation Algorithms on Large Real-World Problems. Springer, Heidel-
berg (1993)
5. Bensusan, H., Kalousis, A.: Estimating the predictive accuracy of a classiﬁer.
In: Proceedings of the 12th European Conference on Machine Learning. Springer
(2001)
6. Botia, J.A., Gomez-Skarmeta, A.F., Valdes, M., Padilla, A.: METALA: a meta-
learning architecture. In: Proceedings of the International Conference, Seventh
Fuzzy Days on Computational Intelligence, Theory and Applications (2001)
7. Thornton, C., Hutter, F., Hoos, H.H., Leyton-Brown, K.: Auto-WEKA: combined
selection and hyperparameter optimization of classiﬁcation algorithms. In: Pro-
ceedings of the 19th ACM SIGKDD International Conference on Knowledge Dis-
covery and Data Mining, pp. 847–855 (2013)
8. Salvador, M.M., Budka, M., Gabrys, B.: Automatic composition and optimisation
of multicomponent predictive systems. arXiv preprint arXiv:1612.08789 (2016)
9. Salvador, M.M., Budka, M., Gabrys, B.: Towards automatic composition of mul-
ticomponent predictive systems. In: International Conference on Hybrid Artiﬁcial
Intelligence Systems, pp. 27–39. Springer (2016)
10. Salvador, M.M., Budka, M., Gabrys, B.: Adapting multicomponent predictive sys-
tems using hybrid adaptation strategies with auto-WEKA in process industry. In:
International Conference on Machine Learning. AutoML Workshop (2016)
11. Koza, J.R.: Genetic programming. IEEE Intell. Syst. 14(4), 135–84 (2000)
12. Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., Talwalkar, A.: Eﬃcient
Hyperparameter Optimization and Inﬁnitely Many Armed Bandits. arXiv preprint
(2016)
13. Duﬀy, N., Helmbold, D.: A geometric approach to leveraging weak learners. In:
European Conference on Computational Learning Theory, pp. 18–33. Springer
(1999)
14. Marquardt, D.W.: An algorithm for least-squares estimation of nonlinear parame-
ters. J. Soc. Ind. Appl. Math. 11(2), 431–441 (1963)
15. Shanno, D.F.: Conditioning of Quasi-Newton methods for function minimization.
Math. Comput. 24(111), 647–656 (1970)
16. Biˇc´ık, V.: Continuous optimization algorithms. Master’s thesis, CTU in Prague
(2010)

268
P. Kord´ık and T. Fr´yda
17. Kord´ık, P., Koutn´ık, J., Drchal, J., Kov´aˇr´ık, O., ˇCepek, M., ˇSnorek, M.: Meta-
learning approach to neural network optimization. Neural Netw. 23(4), 568–582
(2010). 2010 special issue
18. The fake game environment for the automatic knowledge extraction, February
2011. http://www.sourceforge.net/projects/fakegame
19. Software: Rapid miner, data mining. http://rapid-i.com/
20. Brazdil, P., Giraud-Carrier, C., Soares, C., Vilalta, R.: Metalearning: Applications
to Data Mining. Cognitive Technologies. Springer, Heidelberg (2009)
21. Kuncheva, L.: Combining Pattern Classiﬁers: Methods and Algorithms. John Wiley
and Sons, New York (2004)
22. Wolpert, D.H.: Stacked generalization. Neural Netw. 5, 241–259 (1992)
23. Schapire, R.E.: The strength of weak learnability. Mach. Learn. 5(2), 197–227
(1990)
24. Woods, K., Kegelmeyer, W., Bowyer, K.: Combination of multiple classiﬁers using
local accuracy estimates. IEEE Trans. Pattern Anal. Mach. Intell. 19, 405–410
(1997)
25. Holeˇna, M., Linke, D., Steinfeldt, N.: Boosted neural networks in evolutionary
computation. In: Neural Information Processing. LNCS, vol. 5864, pp. 131–140.
Springer, Heidelberg (2009)
26. Brown, G., Wyatt, J., Tino, P.: Managing diversity in regression ensembles. J.
Mach. Learn. Res. 6, 1621–1650 (2006)
27. Brazdil, P., Giraud-Carrier, C., Soares, C., Vilalta, R.: Metalearning, Applications
to Data Mining. Cognitive Technologies. Springer, Heidelberg (2009)
28. Breiman, L.: Bagging predictors. Mach. Learn. 24(2), 123–140 (1996)
29. Gama, J., Brazdil, P.: Cascade generalization. Mach. Learn. 41(3), 315–343 (2000)
30. Ferri, C., Flach, P., Hern´andez-Orallo, J.: Delegating classiﬁers. In: Proceedings of
the Twenty-First International Conference on Machine Learning, ICML 2004, p.
37. ACM, New York (2004)
31. Alpaydin, E., Kaynak, C.: Cascading classiﬁers. Kybernetika 34, 369–374 (1998)
32. Kaynak, C., Alpaydin, E.: Multistage cascading of multiple classiﬁers: one man’s
noise is another man’s data. In: Proceedings of the Seventeenth International Con-
ference on Machine Learning, ICML 2000, pp. 455–462. Morgan Kaufmann Pub-
lishers Inc., San Francisco (2000)
33. Ortega, J., Koppel, M., Argamon, S.: Arbitrating among competing classiﬁers using
learned referees. Knowl. Inf. Syst. 3(4), 470–490 (2001)
34. Holland, J.H.: Adaptation in Natural and Artiﬁcial Systems: An Introductory
Analysis with Applications to Biology, Control, and Artiﬁcial Intelligence. U Michi-
gan Press, Ann Arbor (1975)
35. Rosca, J.P.: Analysis of complexity drift in genetic programming. In: Genetic Pro-
gramming, pp. 286–294 (1997)
36. Borovicka, T., Jirina Jr., M., Kordik, P., Jirina, M.: Selecting representative data
sets. In: Advances in Data Mining Knowledge Discovery and Applications. Intech
(2012)
37. Basilico, J.D., Munson, M.A., Kolda, T.G., Dixon, K.R., Kegelmeyer, W.P.: Comet:
a recipe for learning and using large ensembles on massive data. In: 2011 IEEE
11th International Conference on Data Mining, pp. 41–50. IEEE (2011)
38. Arora, A., Candel, A., Lanford, J., LeDell, E., Parmar, V.: Deep Learning with
H2O. H2O.ai, Mountain View (2015)
39. Meng, X., Bradley, J., Yuvaz, B., Sparks, E., Venkataraman, S., Liu, D., Freeman,
J., Tsai, D., Amde, M., Owen, S., et al.: MLlib: machine learning in apache spark.
JMLR 17(34), 1–7 (2016)

On Scalability of Predictive Ensembles
269
40. Chu, C., Kim, S.K., Lin, Y.A., Yu, Y., Bradski, G., Ng, A.Y., Olukotun, K.: Map-
Reduce for machine learning on multicore. Adv. Neural Inf. Process. Syst. 19, 281
(2007)
41. van Rijn, J.N., Abdulrahman, S.M., Brazdil, P., Vanschoren, J.: Fast algorithm
selection using learning curves. In: International Symposium on Intelligent Data
Analysis, pp. 298–309. Springer (2015)
42. H2O.ai: H2O: Scalable Machine Learning (2015)
43. Baldi, P., Sadowski, P., Whiteson, D.: Searching for exotic particles in high-energy
physics with deep learning. Nat. Commun. 5 (2014). Article no. 4308
44. Hussami, N., Kraljevic, T., Lanford, J., Nykodym, T., Rao, A., Wang, A.: Gener-
alized linear modeling with H2O (2015)
45. Click, C., Malohlava, M., Candel, A., Roark, H., Parmar, V.: Gradient boosting
machine with H2O (2016)
46. LeDell, E.: Scalable super learning. In: Handbook of Big Data, p. 339 (2016)
47. Software: Algorithmic templates for H2O.ai. https://github.com/kordikp

Agent DEVS Simulation of the Evacuation Process
from a Commercial Building During a Fire
Andrzej Kułakowski
(✉) and Bartosz Rogala
Faculty of Electrical Engineering, Automation Control and Computer Science,
Kielce University of Technology, Kielce, Poland
a.kulakowski@tu.kielce.pl
Abstract. The paper describes a simulation application for the process of evac‐
uating people from a building in the event of a ﬁre. This simulation was developed
as an agent system using the DEVS formalism. The following sections show the
design and implementation of such a system using the Adevs library. A series of
simulation tests were then performed for diﬀerent simulation scenarios.
Keywords: Computer simulation · Agent simulation · Building evacuation ·
DEVS
1
Introduction
This Simulating the evacuation process of crowded rooms enables to improve human
safety when designing new buildings. The ability to study the impact of changes signif‐
icantly improves the safety of newly designed objects. Simulation of a dangerous situa‐
tion, i.e. a ﬁre, requiring the evacuation of people from the building allows anticipating
its eﬀects and introducing changes to the project at an early stage. In recent years many
solutions have been developed that allow performing very advanced and meticulous
simulations. In the case of behavioral simulations, the integration with the multi-agent
system (M.A.S.) is a good solution. This type of simulation is gaining ﬂexibility because
each agent has its own perception, autoname, and the ability to communicate [1]. Each
agent has its own state, a set of behaviours and rules that determine the status of the
agent in the next step. The concept of agents represents the idea of an autonomous system
that perceives the environment and acts in it. The agent has an internal state representing
their knowledge and purpose, usually maximizing its usability [2–4]. Thanks to the
independence of individuals, multi-agent systems are most commonly used in social and
behavioral sciences, but also in such disciplines as epidemiology and ecology [2].
Examples of existing evacuation simulation solutions are described in [5–7].
2
Specifying the Simulation Environment
Creating each simulation requires specifying the requirements and assumptions of the
project. In the case of multi-agent simulations, apart from specifying the simulation stage
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_19

itself, i.e. the corresponding model, tools and parameters, consideration should also be
given to creating the environment itself.
Designing an evacuation system application requires the use of map representation
methods and path search algorithms. Mapping motion of an individual in a computer
program requires the implementation of one of the map representation methods. The
most commonly used solution is to create a navigation mesh that maps the area where
one can move. To designate a path on such a map, it is necessary to create a hybrid
forming a grid of triangles on the basis of its own shape [8]. To create a grid of triangles,
representing the map discussed, Delaunay triangulation has been used [9].
Agents of multi-agent systems need to adapt to the changing environment they are
in. In the case of human motion, this includes, eg. the observation of other agents,
avoiding collision and searching destination. Representations of the map serve as a basis
for ﬁnding a path while selecting an algorithm.
The main direct search strategies are: uniform search cost and heuristic search. The
A* algorithm combines both approaches. This allows A* to ﬁnd the optimal path while
being much more eﬃcient than other algorithms [8].
3
Tools Used
Creating any software development project requires the use of appropriate tools. For the
agent simulation of the evacuation process, the Adevs library was selected as the basis
for the simulation [12]. This library allows programming the behavior and relationships
between agents as well as the entire simulation. Because Adevs is written in C++, the
whole project has also been developed in this programming language. Creating a simu‐
lation environment and its visualization requires the use of tools for programming 2D
graphics and graphical user interfaces. In this case, the Qt library was selected to imple‐
ment all required elements.
Delaunay triangulation was used to obtain the Triangle [10] library. It allows creating
several types of high quality mesh using diﬀerent Delaunay and Voronoi triangulation
algorithms. The library uses its own ﬁles describing the polygons and the result ﬁles
with node points generated by the grid and the triangle neighbours. An important feature
of the program is the ability to handle “holes” in the polygons because they represent
obstacles on the map. In addition, the increase in the number of generated triangles is
transferred to the quality of the path being created [10].
The ﬁnal tool needed to create a simulation environment is the simulation library
itself. In this case, the Adevs library was selected. Adevs (A Discrete EVent System
Simulator) is a C++ library for modelling and simulation of DEVS models [11–13].
The library supports the basic model DEVS, Parallel DEVS and Dynamic Structure
DEVS (DSDEVS). Adevs is a free library described by [11] as the fastest among the
available ones. This is very important for large multi-model projects. Developing
discrete simulations in Adevs involves combining atomic models into more complex
structures [12].
The purpose of the formalism was to enable the construction of discrete models
controlled by events in a hierarchical, modular system. DEVS This is a formal
Agent DEVS Simulation of the Evacuation Process
271

speciﬁcation for a system scheme that enables modelling and analysis of discrete systems
that can be described as transient status tables [13].
4
Program Algorithm
Creating the application allows carrying out the entire simulation process, from creating
the whole environment, through simulation to visualizing the results. The application
design assumes that the whole process can be carried out in a few steps: creating a
simulation environment, drawing navigation nets for each ﬂoor of a building: deter‐
mining emergency exits and obstacles, connecting emergency exits between ﬂoors,
creating an area of ﬁre, then adding agents to the ﬂoor. The next step is setting and
performing the simulation parameters, then analyzing the results of the simulation and
its automatic visualization or manual visualization of each step.
4.1
Decision Agent Model
The agent model analyzes the environment in which it is located and communicates with
other agents to make a decision to change their own status. The current agent state is
determined by the decision tree. Agents have three basic parameters: knowledge of the
environment, degree of courage and speed with which they move. The courage and
knowledge parameters are used in taking the decision, additionally the environment and
other agents are taken into account. When the decision is made, the agent performs the
selected motion model [14, 15].
The agent determines the motion vector to the next point on the found route or to the
agent they follow. The next steps of the agent along the route are calculated by adding
motion vectors to the current position. If the next step is not possible, the agent checks
whether the points shifted by 45- and 90-degrees on both sides are possible to occupy,
if not, they remain stationary. Otherwise, the agent occupies the desired place and
updates its route.
The simulation uses the A* algorithm. The triangular grid found in this way will not
be optimal and does not correspond to the actual path travelled by the person possibly
moving in straight line. In addition, smoothening of the found route needs to be done,
including collisions with ﬁre and obstacles.
Simulation of real ﬁre is one of the most complex algorithmic problems. Since the
paper focuses on the evacuation process in multi-agent simulation, a simpler solution is
used. Fire is represented by an ellipse that grows with time. The simulation foresees the
ﬁre expansion, blocking subsequent triangles of the grid, thereby eliminating them from
locations available to agents. In addition, on the way agents check whether the next steps
do not lead them towards the ﬁre, if so, they update the route.
4.2
Simulation Process
The simulation algorithm is based on the diagram model available as part of the Adevs
library [12]. The simulation assumption is independent mapping of the building ﬂoors
272
A. Kułakowski and B. Rogala

(Fig. 1). Every ﬂoor has a list of agents who are currently on it. The agent can move
from one ﬂoor to another using the additional Atomic teleport class. The next steps in
the simulation are managed by the clock updating time, synchronizing the simulation
on the ﬂoors, and sending a command to calculate the next step in the simulation.
Fig. 1. Diagram of simulation model created with the use of DEVS formalism in the Adevs library
[14, 15].
The next simulation steps are initiated by the clock, sending the current time to all
ﬂoors. The ﬂoors perform the movements of all agents, checking whether the agent does
not leave the ﬂoor. In this case, the agent is sent by teleport to another ﬂoor if there is
enough space on the map. On the last stage of the step, the teleport checks the status of
each ﬂoor and then sends the information to the clock, continuing the loop or ending the
whole process.
5
Application Implementation
5.1
Creating the Environment Project
Each ﬂoor of the project is based on a plan loaded from the graphic ﬁle. Ready-made
design of the ﬂoor taking into account people, obstacles, emergency exits and ﬁres
(Fig. 2).
Agent DEVS Simulation of the Evacuation Process
273

Fig. 2. Reaction of an agent to ﬁre [14].
The graphic plan serves as a guide for creating the navigation grid. It consists of
three types of polygons that deﬁne the surface type: blue for ﬂoor, red for emergency
exits, and gray for obstacles. Polygons can be freely joined and modiﬁed by adding or
subtracting vertices. The program will automatically create the missing polygons of the
obstacle and will generate a new grid.
Then, on the ready map there are placed agents with the speciﬁed parameters and an
ellipse representing the ﬁre. Once all the ﬂoors are complete, the appropriate passages
must be connected.
The last step is generating a triangle grid and checking the connections. The mesh
compression can be controlled by setting the maximum area of the triangles. Triangu‐
lation is performed by Triangle external program and its results are loaded into the
program.
274
A. Kułakowski and B. Rogala

5.2
Evacuation Simulation
Simulation takes two parameters: the maximum time and the time deﬁning the simula‐
tion step. The simulation step determines the unit of time by which the agent’s position
is updated. The simulation ends when the maximum time is reached or when none of
the agents is able to move. The implementation in Adevs maps the model presented in
Fig. 1.
The simulation starts by sending an UP_TIME message containing the current time
to all ﬂoors by the clock. Each of them triggers all their agents to complete the next step,
deﬁned by their status. The ﬂoor examines the number of agents moving and the number
of the ones wanting to move to another ﬂoor.
The teleport receives messages from all ﬂoors, checking their type, determines
whether any agents need to be forwarded. If so, the teleport sends agents to the appro‐
priate ﬂoors and then expects a feedback. When all the ﬂoors make a move, the teleport
checks their status and sends the information about the next step to the clock. If all ﬂoors
have NO_MOVE responses, the simulation ends prematurely because all agents have
already left the building or are no longer able to move.
Each subsequent simulation step calculates the point at which the agent should be
located and writes them to the list. These values are used to recreate the positions of
agents at a given time and visualize their movements. The program allows playing the
simulation automatically or manually by moving the time slider.
6
Testing the Simulation
Each computer simulation requires a validation process for the simulation model.
Testing is necessary to determine whether the model meets the design assumptions. In
the case of the analyzed application, testing was performed on individual agent motion
systems. Three simulation scenarios are then presented to validate the appropriacy and
suitability of the applications in real-life situations [14].
6.1
Motion Systems
The agents’ motion required the implementation of collision and analysis of the
surrounding systems to represent reality most accurately. The ﬁrst is to bypass the ﬁre
while mapping out the agent’s route, escaping from the ﬁre when it is close, and dying
in the event of a rapidly expanding ﬂame (Fig. 2).
The other systems are the search for the leader (a trained person), the closest visible
exit, and the collision check. The program checks collisions with walls and other obsta‐
cles to determine what the agent can see. When an agent does not know the distribution
of rooms, but can see another agent possessing the knowledge, he follows him. If an
agent detects a collision possibility during a movement, it bypasses the obstacle. Of
course, it avoids colliding with another agent as well (Fig. 3).
Agent DEVS Simulation of the Evacuation Process
275

Fig. 3. Agent mobility: following the leader (on the left) and avoiding collisions between agents
(on the right) [14].
6.2
Simulation Scenarios
The ﬁrst scenario examines the eﬀect of the exit system in a commercial building during
the spread of a ﬁre (Fig. 4). Three simulations have been carried out with the following
assumptions: a two-speed ﬁre spreading in a restaurant building is being tested; the
setting and all agent parameters are the same for each simulation.
Fig. 4. Ready-made application during the evacuation simulation of a commercial building [14].
276
A. Kułakowski and B. Rogala

Runs of simulations: fire spreads at a speed of 1 cm/ s, the building has two exits; the
fire spreads at a speed of 5 cm/s, the building has two exits; the fire spreads at 5 cm/s, the
building has four exits.
The ﬁrst simulation showed that when the ﬁre develops slowly, potentially everyone
could escape, but three people had to approach the ﬁre very closely. In fact, these people
would be injured or would not be able to leave the building. In the second simulation,
the ﬁre blocked the passage, resulting in the death of four people in the 11th second.
Additional door in the third simulation signiﬁcantly shortened the evacuation time and
provided a safe route for all agents.
The second scenario analyzes the simulations of restaurant evacuation with diﬀerent
quantities and locations of obstacles (tables) (Fig. 5). It checks the time needed for total
evacuation and the average exit time of thirty one agents. There were six simulations
carried out. The ﬁre spreading speed was set to 0.1 cm/s.
Fig. 5. Layout of the simulation room, left: initial setting, deletion of three obstacles at the exits
and modiﬁcation of the setting [14].
The greatest impact on the speed of evacuation of all agents was increasing the
number of exits. By rearranging obstacles, you can reduce the average time an agent
needed to leave the room. Another solution is to remove some obstacles e.g. in the
vicinity of the exits, thus providing greater ﬂow capacity in critical locations.
In public buildings like banks or oﬃces, the clients do not always know the distri‐
bution of rooms. The last simulation checks the inﬂuence of agents’ knowledge in case
of changing the number of “employees” whom the agent can follow towards the exit.
All other simulation parameters are controlled. Oﬃce plans will be used in the analysis
(Fig. 6).
Agent DEVS Simulation of the Evacuation Process
277

Fig. 6. Room plan with clients only (pink) and “oﬃcials” (blue ones) [14].
The simulation shows that the agents who are unfamiliar with the layout of the
premises are not able to ﬁnd the way out themselves. Some panic or wander aimlessly
looking for the way. Increasing the number of agents who know the way and whom the
rest can follow, aﬀects the number of people rescued and the time of simulation. In the
third simulation, it can be observed that the time of the last evacuation and average time
are higher than before. This is due to the following and later loss of visual contact with
the “employee”. This results in an autonomous search for the way, which takes more
time. With the large number of agents who know the way and their proper location, the
clients follow them, leaving the building much faster.
The above simulations show how important building design is and how computer
simulation is helpful in this process. Simulation results analysis allows you to design
buildings in a faster, cheaper, and most importantly more secure way. In addition, the
simulation data helps in the event of an actual evacuation, allowing to predict the poten‐
tial number of people injured.
7
Summary
The paper presents selected theoretical aspects of computer simulation applied to human
evacuation. An algorithm and outline of application implementation is then presented
to create and simulate the evacuation process in the event of ﬁre. The tools used allow
you to create an entire simulation environment and modify it while conducting further
tests. Meeting the evacuation system requirements and testing it, shows the suitability
of such solutions in the real world.
278
A. Kułakowski and B. Rogala

Several evacuation scenarios are presented, showing how environmental changes
aﬀect the evacuation process. It has been shown that while designing a building antici‐
pating potential ﬁre sources and appropriate matching number of exits and evacuation
paths have a large impact on security. Such modiﬁcations not only secured the survival
of all agents, but also reduced the time needed to leave the building. Even minor changes
to the obstacle setting or the addition of trained agents (ﬁre wardens) allow for a signif‐
icant improvement in the speed of evacuation.
The present form of the application does not allow for the creation of fully realistic
simulations, but the presented solution of the multi-agent system allows for its further
development.
References
1. Klügl, F., Klubertanz, G., Rindsfüser, G.: Agent-based pedestrian simulation of train
evacuation integrating environmental data. In: KI 2009: Advances in Artiﬁcial Intelligenc,
Lecture Notes in Computer Science, vol. 5803, pp. 631–638. Springer, Heidelberg (2009)
2. Uhrmacher, A.M., Weyns, D.: Multi-Agent Systems: Simulation & Applications. CRC Press,
Boca Raton (2009)
3. Alonso, E., Karcanias, N., Hessami, A.G.: Multi-agent systems: a new paradigm for systems
of systems. In: ICONS 2013: The Eighth International Conference on Systems
4. Weiss, G.: Multiagent Systems: A Modern Approach to Distributed Artiﬁcial Intelligence.
The MIT Press, Cambridge (2000)
5. Van Schyndel, M.: Evacuation for a building with multiple levels. In: SYSC 5104, Carleton
University (2011)
6. Smith, J.L.: Agent-based simulation of human movements during emergency evacuations of
facilities, 30 September 2015. https://www.wbdg.org/pdfs/agent_based_sim_paper.pdf
7. Pathﬁnder Thunderhead Technical Reference, 30 Septemeber 2015 (2015). http://
www.thunderheadeng.com/downloads/pathﬁnder/tech_ref.pdf
8. Cui, X., Shi, H.: Direction oriented pathﬁnding in video games. Int. J. Artif. Intell. Appl.
(IJAIA) 2, 1 (2011)
9. Weatherill, N.P.: Delaunay triangulation in computational ﬂuid dynamics. Comput. Math
Appl. 24, 129–150 (1992)
10. Triangle, 10 June 2015 (2015). https://www.cs.cmu.edu/~quake/triangle.html
11. Van Tendeloo, Y., Vangheluwe, H.: The modular architecture of the Python(P)DEVS
simulation kernel work in progress paper. In: Wainer, G.A. (ed.) DEVS: Proceedings of the
Symposium on Theory of M&S, SCS International, pp. 387–392 (2013
12. Nutaro, J.: Adevs Manual, A Discrete EVent system Simulator, 8 June 2015 (2014). http://
web.ornl.gov/~1qn/adevs/index.html
13. Song, H.: Infrastructure for DEVS modelling and experimentation. Master thesis draft.
McGill’a Univesity, Montréal, Canada (2006). Supervisor: prof. Hans Vangheluwe
14. Rogala, B.: Project and application for building evacuation agent simulation process during
ﬁre. Master thesis, Kielce, Kielce University of Technology, Poland (2015). Supervisor:
dr.eng. A. Kułakowski
15. Kułakowski, A., Rogala, B.: Agent simulation of the evacuation process from a building
during a ﬁre. In: CSIT 2017, Lviv, Ukraine (2017)
Agent DEVS Simulation of the Evacuation Process
279

The Alpha-Procedure as an Inductive
Approach to Pattern Recognition and Its
Connection with Lorentz Transformation
Tatjana Lange(&)
University of Applied Sciences, 06217 Merseburg, Germany
tanja.28.lange@gmail.com
Abstract. The paper deals with problems which appear when solving the task
of pattern recognition in a feature space that is not identical with the space of the
unknown decisive key features. In a common sense it deals with the correctness
of solutions which are found in different coordinate systems. Even more, the
opportunity of constructing a feature space for the ﬁnal separation of classes by
selecting the features in pairs, how it is done by the Alpha-procedure, will be
investigated. Thereby, the problem of stability during the solving of pattern
recognition tasks will be considered from the point of view of transformation
groups. The possibility of avoiding the necessity of regularization by using the
geometric equiafﬁne Lorentz transformation will be shown, exploiting for that
aim as example the alpha-procedure.
Keywords: Pattern recognition  Alpha-procedure  Stability of reverse tasks
Kernel transform  Lorentz transformation
1
Introduction
The pattern recognition and classiﬁcation methods using the supervisor approach
developed strongly at the beginning of the sixties [1–10] and they have got a second
push with the development of computer technology [11–19]. The automatic methods of
pattern recognition and classiﬁcation are also often called Machine Learning.
Nowadays, pattern recognition methods are used in nearly all areas of social and
industrial life. They are not only applied to automatic reading and picture recognition,
voice recognition and translation, e.g. in automotive applications. We can ﬁnd them in
banking (e.g. fraud recognition), in agriculture and ecology (e.g. inﬂuence of different
factors on growth), in medicine (e.g. diagnostics or survival analyses), in molecular
biology (e.g. prediction of cellular location sites of protein), in pharmacology (e.g.
efﬁciency of medicine including placebo) or in manufacturing (e.g. material synthesis
or technology identiﬁcation). An exceptional topic is there current and future use in
criminology and early identiﬁcation of potential terrorists (e.g. with the help of the
so-called dragnet investigation).
The task of classiﬁcation and machine learning can be formulated in a most
common way as follows: In a training phase, a limited number of objects and their
afﬁliation to a certain pattern or class (e.g. out of 2 classes) is given. An amount of
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_20

features is proposed which can be owned by any of the objects of the two classes. The
task is to ﬁnd such a “decision rule”, that allows a routine classiﬁcation without
repeated thinking of completely new objects, which did not belong to the initial
“training sample” that was used during the training (or learning) phase.
For automatic computer-based recognition it is natural and comfortable to represent
such a task in a geometric manner where the objects together with the values of their
features are vectors in an n-dimensional vector space of the n proposed features.
Together with it there are two approaches known –the classical deductive approach
[2, 8, 10] and the more common inductive approach [3–5, 11].
In case of the deductive approach the separating hyperplane is searched in the
complete feature space or even in an extended space of all original features and
additional extended features (combinations of the original “elementary” features). Such
an extended space is called rectifying feature space. Together with it the dimension of
the space remains unchanged during the whole search process.
In contrast to the deductive approach the inductive methods search an appropriate
subspace of key features which separate the classes in an optimal way. These key
features are selected either from the original feature space or from the rectifying space.
The inductive methods of recognition as well as of identiﬁcation and modelling
[24–30], which are mathematically very similar sciences, are sometimes also called
selective methods depending on the kind of bottom-up or top-down selection. Both
kinds search the solution in reduced spaces or rather subspaces.
The inductive recognition method that is called Alpha-procedure [11] executes a
bottom-up selection of the feature space.
But there is one problem that needs to be considered: When we select the feature
space the solution of the task may be instable in the sense of Hadamard [34]. For
pattern recognition instability means that the rule separating the classes faultlessly
during the training phase can make a lot of mistakes for new objects with new data.
Thus, for the reduction of the feature space we need a scheme for the correct com-
parison of features by their separating (or recognizing) ability. Such a comparison is
normally performed stepwise by pairs or a by a small number of features. Consequently
we have to deal with different feature subspaces on the different selection stages, both
in sense of the dimensions and of the semantics of the features.
Therefore, if using an inductive selection of features serious problems with respect
to convergence/ asymptotic can appear.
But concerning the Alpha-procedure as well as the inductive methods at all the
question appears: Why do these methods work stable and well during the exploitation
phase, i.e. during the phase of classifying new objects, despite of the open issue
concerning the justiﬁability of the comparison of solutions in subspaces of the original
features during the training phase?
The aim of the current paper is the ﬁnding of an answer to the question above
showing the connection between the inductive construction of the separating plane and
the special Lorentz transformation.
Thereby, the paper is structured as follows:
• Section 2 describes the classiﬁcation task in a common form and focuses thereby
on the semantics of the notion key feature as an important element of the
The Alpha-Procedure as an Inductive Approach to Pattern Recognition
281

Alpha-procedure which differentiates the Alpha-procedure from other methods and
which gives a geometric description of the solution independently of the search
method.
• Section 3 brieﬂy considers the stability of the solution of a mathematical task from
the point of view of the theory of group transformations and their invariants.
• Section 4 describes the algorithm of the Alpha-procedure from a geometric point of
view. It also gives all what is needed for the understanding of the nature of the
Lorentz transformation und for its appropriate comparison with the algorithm of the
Alpha-procedure, again in a geometric manner.
• Section 5 pointers to sources where numeric examples are published which were
executed by a team around the author of this paper. It also provides a link to a
public R-package every one can use to test the Alpha-procedure with own data.
• Finally, Sect. 6 consists of some conclusions and a short outlook.
2
Common Characterization of the Pattern Recognition Task
The task of pattern recognition with the help of measured or observed data can be
described as follows:
Normally, we start with given (measured) data for different objects and their fea-
tures that are assigned to different classes by a “trainer” or “supervisor”. We call this set
of classiﬁed data the “training set”. The Table 1 gives shows an example consisting of
measured data xij of the features yj characterizing the objects xi, and the supervisor’s
assignments of the objects to the classes A or B.
The aim of the pattern recognition is to ﬁnd such a stable rule in form of a
hyperplane (Fig. 1) for the separation of objects using as basis just these training data
and the statements of the supervisor concerning the afﬁliation of the objects to the
classes A or B. This should be done in a way that in case of appearing new objects with
Table 1. Example of a “training set”.
Objects
xi
Features yj
Assignments by supervisor
y1
y2
… … yn
x1
x11 x12 … … x1n A
x2
x21 x22 … … x21 B
….
…
…
… … …
…
….
…
…
… … …
…
xp1
…
…
… … …
B
xp
xp1 …
… … xpn A
Note: Each row i corresponding to the object No. i can be
considered as vector xi in the n-dimensional feature space.
The xij are the elements of this vector.
282
T. Lange

their own measured values of the features one can automatically deﬁne with the help of
that rule to which class the new objects belong.
But the objects X x1; x2; . . .; xp


can be located in the feature space in a way that
they can be separated in the given space of features Y y1; y2; . . .; yn
ð
Þ only with the help
of a complicated curve (Fig. 2) or of a fractured line.
The reason for that problem can be, for example, one of the following:
1. A feature that is very important for the separation and which we will call key feature
has not been recorded with the original data. But there may be some other features
which are somehow mathematically connected with the missing key feature.
2. All key features are available in the original data table but together with that there
are other features recorded, which are not only useless but they are even more
disturbing the recognition procedure. In some applications, for example in case of
criminological dragnet investigations such harmful features may be even invaded
intentionally.
3. There also may be a combination of the ﬁrst two reasons.
Although the term key feature is self explanatory, the understanding of this term
should be described in a bit more detail. In Fig. 2 a separation of classes is shown that
Fig. 1. Object separation.
Fig. 2. Complicated separation of objects.
The Alpha-Procedure as an Inductive Approach to Pattern Recognition
283

was performed without the participation of the key feature y3. In Fig. 3 the same classes
are shown now clearly separated with the help of the y3. In Fig. 3 the key feature y3 is
demonstrated in a “clean” way as a feature that is independent of the features y1 and y2
and the separating plane is parallel to the plane built by y1 and y2. The separating rule
corresponding to Fig. 2 has a random character which reﬂects the randomness of the
choice of the training data set but not the dependence on the interrelation law of the
classes.
In case of Alpha-procedure it is assumed that the interrelation law of classes can
only
be
represented
by
a
modest
ensemble
of
key
features.
Thereby,
the
Alpha-procedure automatically searches for this ensemble of key features.
For the dealings with the ﬁrst reason a so-called rectifying space with rectifying
features is used. These rectifying features are combinations of the original features in
form of simple functions, for example yk ¼ x1  x2
5. Together with the original features
they will be expressed in a new extended data table where they appear as new columns
together with the original ones.
The use of the rectifying space can be interpreted as recognition by indirect
measurements.
In the second case only a reduction of the original feature space, best to a space of
only key features, makes sense. Only if there is an absolute need new rectifying
features, which are created from some remaining features, are added to the reduced
space.
Fig. 3. Separation of classes by key feature.
284
T. Lange

The problem is that we do not know in advance with which of the cases we have to
deal.
But normally the kind of application helps us a little with the indication. For
example, in case of automatic reading the second reason described above will hardly
appear. Thus, the majority of pattern recognition methods uses the rectifying space.
With it, the search of the separating hyperplane is performed in a constant feature space
that has been deﬁned only once before constructing the separating hyperplane. That
means the number and the semantics of the features during the optimization remain
unchanged (e.g. [10]).
Now, let us consider in a more detailed way the geometric appearance of the
solution of pattern recognition tasks in a common form.
For both the inductive or deductive approaches which are used for solving the tasks
of pattern recognition with the help of a “supervisor”, the ﬁnal result can be represented
as shown in Fig. 4 – independently of the used solution method (e.g. SVM [10, 12],
method of potential functions [8], alpha-procedure [8, 11–16, 31], DD-alpha [18] etc.).
The main distinction between the inductive and deductive approaches is lying in the
different interpretation of the separating hyperplane D(y) and its directing vector W.
In case of deductive approach the feature space corresponds to the complete set of
measured object features.
Fig. 4. Geometric depiction of the separation of classes.
The Alpha-Procedure as an Inductive Approach to Pattern Recognition
285

In the contrast, a reduced set of features is characteristic for the inductive approach.
This reduced feature set is selected in a certain way from the original (complete)
amount of features offered by the “customer” in form of measured data.
This reﬂects the differences between the possible mathematical approaches for
solving the tasks of pattern recognition as well as the different practical application
ﬁelds and it is just the subject of the current paper.
W (shown in Fig. 4) is an n-dimensional vector within the pattern space that is
called directing vector and which is perpendicular to the separating hyperplane that
separates the class A from class B.
Class A and class B represent different objects (points) within the feature space: any
of the p objects (1, 2, …, k, …, p) is deﬁned by n coordinates of the Euclidean space,
i.e. the measured values of the corresponding features (1, 2, …, i, …, n).
With other words, the objects are points, i.e. vectors x1; . . .:; xk; . . .:; xp within the
space Y, where p equals the number of objects of the training set.
During an initial training phase a “supervisor” deﬁnes to which of the classes each
of the objects belongs.
The abscissae of the space Y, y1; y2; . . .:; yi; . . .:; yn, represent the quantities which
characterize the objects to be recognized. They are called features or features. They can
be colors, weight etc. That means they can be expressed quantitatively. But they can
also be function of the characteristic quantities uj yi
ð Þ.
That means, each point (object) xk is represented by the projections yki of an n-
dimensional vector in the pattern space.
After the separation hyperplane D(y) and the directing vector W are found, the
points x1; . . .:; xk; . . .:; xp can be projected onto the straight line W (see Fig. 5). This can
be interpreted as transformation of the points from the form of expression in Fig. 4 into
the form, shown in Fig. 5. We will name this kind of transformation R-transformation
(or Recognition transformation).
Applying one of the recognition methods and getting the separating hyperplane that
is represented by its directing vector W we will afterwards use this hyperplane for the
separation of new objects, but now without the “supervisor”.
The task of pattern recognition can be mathematically reworded in the way that the
hyperplane D(y) can be interpreted as divisor that divides the feature space into two
parts. This space divisor will be then represented by the following equation:
F y; a
ð
Þ ¼
X
n
i¼1
aiui y
ð Þ ¼ 0
ð1Þ
Fig. 5. Projection of objects onto the directing vector W.
286
T. Lange

This can correspond, for example, to an n-dimensional regression plane in the n-
dimensional rectifying space.
The search of such a function F y; a
ð
Þ is often called reconstruction of generating
function or reconstruction of regularity.
Let us stick to the example of regression plane. Both tasks, the pattern recognition
and the reconstruction of generating function, can be solved either in a “deductive”
or in an inductive way, depending on whether the morphology of the solution feature
space is known (case 1) or is not known (case 2).
In case 1 the reconstruction of generation function leads to the ﬁnding of coefﬁ-
cients from Eq. (1) and the constructed hyperplane. With it, the hyperplane is searched
in a space of constant dimension and features, i.e. in a “deductive” way. A large
number of applications work just in this way, e.g. automated reading, recognition of
acoustic or optic patterns.
But there is also a big number of tasks where the morphology of the feature space is
not known exactly. Such situations we meet in medical diagnostics, in criminal search,
in ﬁnancials etc., which means wherever the so-called “subjective features” play their
role.
3
Short Survey of Problems and Different Ideas
in the Development of the Theory of Reconstructing
Generative Functions or Rather Regularities
Naturally it is expected that the generating function or rather the regularity should be
invariant, i.e. the law and the bonds should be unchanged if some of the surrounding
conditions change.
It is desirable to know what will remain invariant (unchanged) by which changes
and under which conditions. The answers to this question lifted mathematics, physics
and technology of twenties century to a new level of development.
The following notions are connected with these answers:
• Transformation, Groups of Transformation, Symmetric/ Reverse Group Elements,
Invariant of Group etc.,
• Kinds of space geometry where the transformation is performed (Euclidean, Hilbert,
Lorentz, Minkowski, Lobachevsky spaces, complex space etc.),
• Integral transform or rather kernel transform and the convergence of model function
to its generating function (singular integral),
• Kernel transform and minimization of the mean risk for reconstructing of generating
functions,
• Methods of searching the separating hyperplane for recognition tasks using the
kernel transform, such as
– Neuronal Networks of Novikoff,
– Methods of potential functions (Rozonoer),
– SVM Support Vector Machine (Vapnik),
• Stability of solution, regularization, metric.
The Alpha-Procedure as an Inductive Approach to Pattern Recognition
287

The following questions are also tied to this issue:
• What is transformation, what is group of transformations and its invariant?
• Why did the notion “groups of transformations” push the modern sciences so far?
Several mathematicians (Galois, Abel, Poincaré, Cartan, Lie, Pontryagin, Kol-
mogorov, Heaviside, Dirac) worked on the question of a stable representation of laws
from a common mathematical point of view when they developed the transformation
theory, the transformation groups, the invariants of transformation groups, long time
before the task of pattern recognition was formulated mathematically.
These theories help us to look at many different and well-known for a long time
mathematical instruments (e.g. algebraic, differential and integral equations) and ideas
in geometry from a very general point what allows us to use the rich experience of
geometric representations including the projective geometry.
The borders of the adequate work of the law, which is represented by the trans-
formation, are deﬁned by the so-called invariant of the transformation (or group of
transformations): The invariant describes what will remain unchanged when the
transformation (or group of transformations) is applied to a mathematical or geometric
object.
For example, in case of using a differential transformation (differential equation) the
invariants of the transformation are the eigenfunctions of the differential equation.
More concrete, in automatic control the invariant is given with the independence of
the eigenfunctions from the initial state and from the kind of the action at the input of
the object of the automatic control, which is described by a differential equation. In
case of operator equation the invariant is the eigenvector of the fundamental function.
As another example, the homothetic transformation (or resemblance transforma-
tion) of leafs shown in Fig. 6 belongs to the group of conformal transformations.
In case of the group of homothetic transformations the invariant, i.e. the feature of
the ﬁgures that does not change while performing the transformation, is the angle
between two tangents to any two intersecting curves of these ﬁgures.
Let us highlight another important invariant which geometric interpretation has
served the well-known idea of the Lorentz transformation that is described in more
detail in Sect. 4 of this paper. This is the integral invariant of dynamic systems
Fig. 6. Homothetic transformation.
288
T. Lange

(Poincaré [31], Cartan [33]) and also the concept of the phase space (Poincaré [32], Lie,
Kolmogorov, Pontryagin [32]), which is based on the integral invariant, are the basis
for the advanced theory of optimal control.
Further more, the integral invariant is also the basis for the equiafﬁne Lorentz
transformation conserving the volume as invariant.
In the further course the Sect. 4 of this paper tries to justify the correctness of the
reduction or rather of the selection of the feature space by the Alpha-procedure with the
help of the Lorentz transformation that can be considered as theoretical basis of this
inductive pattern recognition method.
The thing is that for thousands of years a lot of single experiences of work with
natural simple spatial (geometric) representations have been accumulated in mathe-
matics and technology. But only with the appearance of the notion group of trans-
formations (about the turn of 18th/19th centuries) introduced by Gaspard Monge,
Évariste Galois and Niels Abel, it became possible to unite algebraic representations
with geometric representations in a very general way. The inventor of descriptive
geometry, Monge, had his origin in geometrics, while Abel and Galois came from
algebra. But only for the last 30 years, with the development of computers, it became
possible to represent the Galois ﬁeld (as a special group of transformation) in a geo-
metric manner within the complex space, e.g. for the investigation of the stability of
automated control systems.
Let us consider in a bit more detailed way the notions kernel transform, stability of
solution and metric.
The kernel transform was investigated as singular integral by Lebesgue in 1909.
One of the means of representing functions is given with the singular integral:
fn x
ð Þ ¼
Zb
a
Kn x; t
ð
Þ  f tð Þ  dt
ð2Þ
that converges to its generating function f x
ð Þ if n ! 1 (with these or those limits of
function f). The function Kn x; t
ð
Þ is called kernel.
More or less at the same time J. Hadamard [34] invented the notion “stability of the
solution of a reverse task by measured data”:
Let us assume that the notion “solution” is deﬁned and only one solution z ¼ R u
ð Þ
out of the space F corresponds to any element u 2 U.
The task of deﬁning the solution z ¼ R u
ð Þ out of the space F using the original
(measured) data will be a stable task in the spaces (F, U), if for any number e [ 0 one
can ﬁnd such a number d eð Þ [ 0 in a way, that from the inequality qU u1; u2
ð
Þ  d eð Þ
follows qF z1; z2
ð
Þ  e, where z1 ¼ R u1
ð
Þ, z2 ¼ R u2
ð
Þ, u1; u2 2 U, z1; z2 2 F.
qU is the distance in the space U and qF is the distance in the space F.
Solving the reverse task for the kernel equation
Zb
a
K x; t
ð
Þ  z tð Þ  dt ¼ u x
ð Þ;
c  x  d
ð3Þ
means to search for the solution z tð Þ using the measured data u tð Þ.
The Alpha-Procedure as an Inductive Approach to Pattern Recognition
289

The tasks of reconstructing a generating function or a model of a dynamic system,
the tasks of ﬁnding a separating hyperplane in pattern recognition, the tasks of ﬁnding a
probability density function etc. – all these tasks are reverse tasks.
In the sixties Tikhonov [35] proposed a regularization method for the stabilization
of an instable (according to Hadamard) solution of kernel equations using measured
data. Then, Tikhonov generalized that method for other reverse tasks as a search of the
solution z out of A  z ¼ u by measured u, where A is any linear operator.
Here, z is the argument of the optimization functional
min q2 Az; u
ð
Þ þ a  X zð Þ


ð4Þ
where X zð Þ is a convex regularizing (“penalty”) functional.
The parameter a must be found with the help of additional information what
appears to be a quite difﬁcult task.
Concurrently with Tikhonv’s regularization another method of regularization was
invented – the validation (Jackknife, Bootstrap etc.).
4
The Recognizing Algorithm of Alpha-Procedure
and the Lorentz Transformation
All recognition methods mentioned above either use a stabilization/ regularization of
solution or they use special information that exclude instability. For example, in case of
Novikoff’s Neuronal Networks the stabilization is performed by a given (in advance)
point of rotation of the separating hyperplane. The method of potential functions uses
for the same purpose the predetermined width of the potential function. Besides, these
methods are developed and well applied for the search of the separating hyperplane in
the cases when the space is of constant dimension.
On the other hand in recognition cases of the type of “criminal search” with
so-called “subjective features” an automatic reduction of the space of the original
features is usually necessary because some of the features may be only disturbing.
But we know from the group theory of topologic transformations that the main
invariant of continuous group guarantying the convergence is the dimension of the
space. That means that the methods permitting a reduction of the space tend to have
instable solutions from a mathematical point of view. Therefore, even the use of
validation-type regularization may be problematic.
As known from the physics (R. Penrose, S.W. Hawking) and from Tikhonov’s
theory the choice of the right geometry of the space can solve the stability problem.
Now, let as show how the Alpha-procedure [11–16, 18] uses the equiafﬁne
Lorentz transformation with an invariant that ﬁts the given recognition task.
For that purpose we will, ﬁrst, shortly describe the Alpha-procedure. But before
starting an important note must be placed here: During the following description of the
Alpha-procedure where a new (reduced) feature space is created we will use the term
“property” for the original features and the term “feature” only for the selected and
for the so-called artiﬁcial features.
290
T. Lange

The Alpha-procedure is an advanced method for classifying objects belonging to
two different classes. The recognition algorithm is deﬁned during a training phase using
a training data set and a “supervisor” showing us which object belongs to which class.
All objects are characterised by properties or rather measured data.
The Alpha-procedure is based on a step-by-step selection of the best properties and
uses consistently only a subset of these properties. That means we work normally with
a reduced property or feature space.
The idea can be roughly described as follows:
First, we select the property with the best discrimination power, i.e. the property
that gives the best separation, as a basic features f0 and represent it together with its
values for the objects as an axis (Fig. 7).
Then we add a second property pk to the coordinate system and deﬁne the positions
of the objects in the plane that is built by the axes f0 and pk (Fig. 8).
After that we create a new axis and turn it around the origin of the coordinate
system by the angle a up to the moment when the projections of the objects onto this
new axis give us the best separation of the objects.
Fig. 7. Alpha-procedure (1)
Fig. 8. Alpha-procedure (2)
The Alpha-Procedure as an Inductive Approach to Pattern Recognition
291

We repeat this procedure for all properties and select the property that gives the best
separation on the new (artiﬁcial) axis. ~f1 This way we construct the 1st repère (Fig. 9).
In the third stage we add another property pj as a third axis and deﬁne the position
of the objects in a new plane that is created by the axes ~f1 and pj (Fig. 10).
Again we create a new axis on this new plane and turn it around the origin of the
coordinate system by the angle a up to the moment when the projections of the
objects onto this new axis provide the best separation. We repeat this procedure for all
remaining properties and select the best one (Fig. 11).
Using each time the last created artiﬁcial feature ~fr together with the next property
we create step by step 2-dimensional planes where we deﬁne the next artiﬁcial feature
~fr þ 1. The procedure stops when a faultless or at least the best separation is reached.
Fig. 9. Alpha-procedure (3)
Fig. 10. Alpha-procedure (4)
292
T. Lange

Note: As one can see from the described above the step-by-step bottom-up selection
is performed without leaving the bundle S.
Now, let us look at the special equiafﬁne Lorentz transformations. What are they
and which invariant do they propose for the pattern recognition?
For the characterization of a certain geometric ﬁgure and its location with the help
of numbers it is usually necessary to invent an ancillary reference system or a coor-
dinate system. The numbers x1; x2; . . .; xn we get this way do not only characterize the
geometric ﬁgure (to be investigated) but also its relationship with the reference system.
In case of changing the reference system the ﬁgure will be characterized by other
numbers x
0
1; x
0
2; . . .; x
0
n. When a certain expression f x1; x2; . . .; xn
ð
Þ describes the ﬁgure
in itself then it must not depend on the reference system, i.e. the following relation
should be valid:
f x1; x2; . . .; xn
ð
Þ ¼ f x
0
1; x
0
2; . . .; x
0
n


ð5Þ
All expressions which satisfy the relation above are called invariant.
There are three special Lorentz transformations known which are equiafﬁne, i.e.
these afﬁne transformations do not change the objects:
1. Any simple rotation of the space like a solid around the axis of a cone (see Fig. 12)
by a certain angle x is an equiafﬁne transformation of the space transforming the
cone into itself. We will call it x-transformation.
2. The reﬂection of the space on any plane p, intersecting the axis of the cone is also a
Lorentz transformation. We will call it p-transformation.
Fig. 11. Alpha-procedure (5)
The Alpha-Procedure as an Inductive Approach to Pattern Recognition
293

3. And, ﬁnally, let us consider the L-transformation (from Lorentz). Figure 13 illus-
trates the following: The compression (or rather the stretching) of the space towards
the plane P and the stretching (or rather the compression) away from the plane do
not change the volumes, because for any cut of the cone by the plane R that is
parallel to the plane Sxu, the equation p  q ¼ const: is valid. Here p and q are the
distances of any point of the hyperbola to its asymptotes (i.e. to the planes P and Q)
and P and Q there selves are the tangents of the cone connecting its generating line.
Fig. 12. Illustration to x- and p-transformation
Fig. 13. Illustration to L-transformation
294
T. Lange

Any point of the hyperbola remains on one and the same hyperbola, i.e. the
hyperbola goes over into itself. But the complete cone consists of such hyperbolas what
means that the L-transformation transforms the cone into itself.
The line bundle S (i.e. the lines intersecting in point S) maps onto itself unam-
biguously in all three cases of the described above transformations.
With other words, the line bundle S is the invariant for each of the three
transformations.
But the proof of that is possible only by using all three of them. Therefore, not
stopping on the well-known proof in detail, we will here only show the train of thought
of that proof.
In the Lorentz space one can get a projective P-plane to any cut P that is
perpendicular to the axis of the cone. To such a projective plane corresponds the
projective transformation K.
The basis for the proof is the ability to break down the projective transformation K
into a series of Lorentz transformations which correspond to the converting of any
repère M on the circular into any other repère M′ on the same circular (Fig. 14).
For this it is sufﬁcient to perform the transformations K ¼ L1  x  L1
2
(or the
transformations K ¼ L1  x  p  L1
2 ). The transformation L1 moves the ﬁrst repère M
to the centre O of the circular a, the transformation x turns it as needed and, ﬁnally, the
transformation L1
2
unites it with the second repère M′.
The proof of the uniqueness of the identical transformation is completely based on
the rules of the common projective geometry onto a plane.
It is obvious that the Alpha-procedure uses at every step the x-transformation of
Lorentz and at the turns in pairs the p-transformation. The vectors of the measured
objects are the line bundle S that is the invariant of these transformations. This way it is
guaranteed that two single features are stepwise comparable, i.e. the correctness of the
stepwise comparison is guaranteed.
The statements of the “supervisor” during the training process play not only the role
of automatic feedback but give also a visual feedback on the plane.
Fig. 14. Illustration to repère transformation
The Alpha-Procedure as an Inductive Approach to Pattern Recognition
295

5
On Numerical Examples
A lot of numerical examples executed by team of Ukrainian postgraduates at University
of Cologne under the lead of K. Mosler, R. Dyckerhoff and the author of this article are
published in [18, 20–23] and others.
Thereby the paper [20] focuses on the original Alpha-procedure brieﬂy described
above and in [18, 21, 22] on an advanced version that is known as DD-Alpha (com-
bining the Alpha-procedure with the Data Depth approach). Most of the examples also
compare the effectiveness of the Alpha- and DD-Alpha-procedures with other known
classiﬁcation methods.
The source [23] refers to an R-package including its description. This package is
free accessible and can be used by everyone for testing the DD-Alpha-Procedure with
his/her own data.
6
Summary and Further Investigations
It is known that the construction of a solution of the task of pattern recognition in a
subspace or, in a common sense, in a “non-adequate” space (e.g. in an overﬁtting
space) requires measures for ensuring the convergence and the stability of the solution.
It is shown above that such measures can be
• a “post-factum” regularization of Tikhonov type [2, 35] what means a two-criterial
optimization in ﬁxed subspaces or spaces [12],
• a “preventative” or “post-factum” validation or
• the use of geometric transformations together with an invariant that is appropriate
for the task (and this was the main topic of this paper).
Concerning the further investigations we can say the following:
• The principle of the geometric representation of the differential transformation can
be used for the construction of the separating plane.
• Pontryagin’s Maximum Principle can be probably used for the classiﬁcation of
functions which are represented in form of measured points.
• It is desirable to investigate the concept of Data Depth as transformation together
with it invariants with respect to its use for pattern recognition.
References
1. Rosenblatt, F.: Principles of Neurodynamics. Spartan Books, New York (1962)
2. Novikoff, A.: On convergence proofs for perceptrons. In: Proceedings of Symposium on
Mathematical Theory of Automata (XII). Polytechnic Institute of Brooklyn (1963)
3. Ivakhnenko, A.G.: Recognizing system “Alpha” as predicting learning ﬁlter and extremum
controller without searching oscillations. Automatics 4, 17–24 (1966)
4. Vassilev, V.I., Ivakhnenko, A.G., Reyzkii, V.E.: Algorithm of a recognizing system of
perceptron type with a correlator at the input. Automatics 1, 20–35 (1966). (in Russian)
296
T. Lange

5. Vassilev, V.I., Ivakhnenko, A.G., Reyzkii, V.E.: Recognition of moving bodies. Automatics
6, 47–52 (1967). (in Russian)
6. Ivakhnenko, A.G.: Self-learning systems of recognition and of automatic control. Tekhnika,
Kyiv (1969). (in Russian)
7. Vassilev, V.I.: Recognizing systems. Naukova Dumka, Kyiv (1969). (in Russian)
8. Aizerman, M.A., Braverman, E.M., Rozonoer, L.I.: The method of potential functions in the
theory of machine learning. Nauka, Moscow (1970). (in Russian)
9. Bock, H.H.: Automatische Klassiﬁkation. Vandenhoeck & Ruprecht, Göttingen (1974)
10. Vapnik, V., Chervonenkis, A.Y.: The Theory of Pattern Recognition. Nauka, Moscow
(1974)
11. Vassilev, V.I.: The reduction principle in pattern recognition learning (PRL) problem.
Pattern Recogn. Image Anal. 1(1), 23–32 (1991)
12. Cortes, C., Vapnik, V.: Support-vector networks. Mach. Learn. 20, 273–297 (1995)
13. Vassilev, V.I., Lange, T.: The principle of duality within the training problem during pattern
recognition (in Russian). Cybern. Comput. Eng. 121, 7–16 (1998)
14. Vassilev, V.I., Lange, T., Baranoff, A.E.: Interpretation of diffuse terms. In: Proceedings of
the VIII, International Conference KDS 1999, pp. 183–187, Kaziveli, Krimea, Ukraine
(1999). (in Russian)
15. Vassilev, V.I., Lange, T.: Reduction theory for identiﬁcation tasks. In: Proceedings of
International Conference on Control, Automatics-2000, pp. 49–53, Lviv (2000). (in Russian)
16. Vassilev, V.I., Lange, T.: The mutual supplementability of the Group Method of Data
Handling (GMDH) and the Method of Extreme Simpliﬁcations (MES). In: Proceedings of
International Conference on Inductive Modelling ICIM-2002, Lviv, vol. 1, pp. 68–71
(2002). (in Russian)
17. Steinwart, I., Christmann, A.: Support Vector Machines. Springer, New York (2008)
18. Lange, T., Mosler, K., Mozharovskyi, P.: Fast nonparametric classiﬁcation based on data
depth. Stat. Papers 55, 49–69 (2014). Springer
19. Hennig, C.: How many bee species? A case study in determining the number of clusters. In:
Spiliopoulou, M., Schmidt-Thieme, L., Janning, R. (eds.) Data Analysis, Machine Learning
and Knowledge Discovery, pp. 41–49. Springer, Heidelberg (2014)
20. Lange, T., Mozharovskyi, P.: The alpha-procedure: a nonparametric invariant method for
automatic classiﬁcation of multi-dimensional objects. In: Spiliopoulou, M., Schmidt-Thieme,
L., Janning, R. (eds.) Data Analysis, Machine Learning and Knowledge Discovery, pp. 79–
86. Springer, Heidelberg (2014)
21. Lange, T., Mosler, K., Mozharovskyi, P.: DDa-classiﬁcation of asymmetric and fat-tailed
data. In: Spiliopoulou, M., Schmidt-Thieme, L., Janning, R. (eds.) Data Analysis, Machine
Learning and Knowledge Discovery, pp. 71–78. Springer, Heidelberg (2014)
22. Mozharovskyi,
P.,
Mosler,
K.,
Lange,
T.:
Classifying
real-world
data
with
the
DDa-procedure. Adv. Data Anal. Classif. 9(3), 287–314 (2015)
23. Pokotylo, O., Mozharovskyi, P., Dyckerhoff, R.: Depth-Based Classiﬁcation and Calculation
of Data Depth. R package version 1.2.1, ddalpha. https://cran.r-project.org/web/packages/
ddalpha/index.html. Accessed 05 June 2017
24. Ivakhnenko, A.G.: Long-Term Prediction and Control of Complex Systems. Tekhnika, Kyiv
(1975). (in Russian)
25. Ivakhnenko, A.G.: An Inductive Method of Self-Organization of Models of Complex
Systems. Naukova Dumka, Kyiv (1982). (in Russian)
26. Farlow, S.J. (ed.): Self-Organizing Methods in Modelling. GMDH type Algorithms. Marcel
Dekker Inc., New York and Basel (1984)
The Alpha-Procedure as an Inductive Approach to Pattern Recognition
297

27. Ivakhnenko, A.G., Stepashko, V.S.: Noise-Immunity of Modelling. Naukova Dumka, Kyiv
(1985). (in Russian)
28. Akaike, H.: Experiences on development of time series models. In: Bozdogan, H. (ed.)
Proceedings of the First US/Japan Conference on the Frontiers of Statistical Modeling: An
Information Approach, vol. 1, pp. 33–42. Kluwer Academic Publishers, Dordrecht (1994)
29. Lange, T.: New structure criteria in GMDH. In: Bozdogan, H. (ed.) Proceedings of the First
US/Japan Conference on the Frontiers of Statistical Modeling: An Information Approach,
vol. 3, pp. 249–266. Kluwer Academic Publishers, Dordrecht (1994)
30. Stepashko, V.S.: Method of critical variances as analytical tool of theory of inductive
modeling. J. Autom. Inf. Sci. 40(3), 4–22 (2008)
31. Poincaré, H.: Les methods nouvelles de la mecanique celeste. Gauthier-Villars, Paris (1899)
32. Pontryagin, L.S., Boltyansky, V.G., Gamkrelidze, R.V., Mishchenko, E.F.: The Mathemat-
ical Theory of Optimal Processes. Nauka, Moscow (1969). (in Russian)
33. Cartan, E.: Leçons sur les invariants intégraux. Editions Hermann, Paris (1971)
34. Hadamard, J.: Sur les problèmes aux dérivées partielles et leur signiﬁcation physique. Bull.
Univ. Princeton 13, 49–52 (1902). Princeton
35. Tikhonov, A.N., Arsenin, V.Y.: Methods of Solving Incorrect Tasks. Nauka, Moscow
(1974). (in Russian)
298
T. Lange

Analyzing Project Team Members’
Expectations
Vira Liubchenko(&)
Odessa National Polytechnic University,
1 Shevchenko av., Odessa 65044, Ukraine
lvv@opu.ua
Abstract. This paper describes an approach for analyzing the project team
members’ expectations to achieve the personal goals as well as the project
objectives. There are described four types of expectations and suggested the
expectation map as an analytical tool. The paper introduces the important
antipatterns and the process of expectation map analysis.
Keywords: Project manager  Team member  Project objective
Personal expectation  Expectation map
1
Introduction
The key to a successful team is the alignment of objectives within the team. The
challenge of the project manager is setting a common goal the entire team is willing to
pursue. If the case of lack of a common goal, team members who disagree with the
objective in hand will feel reluctant to utilize their full effort, leading to failure to reach
the goal.
Software project teams coalesce and become more productive when they are
coordinated [1]. It takes time for teams to progress through the Tuckman stages of
forming, storming, norming, and ﬁnally to performing to optimize team output [2].
Developing the project team improves the people skills, technical competencies, and
overall team environment and project performance. The project managers should
identify, build, maintain, motivate, lead, and inspire project teams to achieve high team
performance and to meet the project’s objectives.
The team can work towards attaining the goals only if they exactly know what
management expects from them and what role they hold in the project. The project
manager needs to provide a structure for the project team and set expectations and
priorities as well as assign roles carefully. The expectations, as well as the overall goal,
should not be fuzzy.
The problem is the team members are not the predictable systems, and the project
manager is not able to use optimization techniques to achieve the goal. Therefore, the
project manager needs the tool supported the process of coordination the personal
objectives with the project goal; he/she has to be careful about the persons’ objectives.
For establishing persons’ objectives, the understanding of persons’ expectations that
will impact the effectiveness and motivation of team members is a constructive way.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_21

People are motivated if they feel they are valued in the project team and this value is
demonstrated by the attention to their expectation.
In the paper, we propose a simple tool for coordination of project objectives with
personal expectations and describe how to use it for analysis of current situation.
2
Team Common Goal
Team common goal is what separates a high performing team from a bad project
experience. Common goals are important because they bring people together and
encourage them to communicate problems and results. They allow for a much earlier
and faster recognition of problems in the project development.
To fully complete individual’s task roles, one needs to have clear expectations
about his subgoals, the paths to accomplish these subgoals, and the link between his
work and the work of others [3]. Because individuals’ roles are embedded in the larger
context of teams, the clarity of team goals and individual members’ roles in working
toward meeting the goals has a powerful impact on team effectiveness.
In [4] there was demonstrated that team goal setting was an effective team-building
tool for inﬂuencing cohesiveness in the teams. Cohesion had been deﬁned as “a
dynamic process that is reﬂected in the tendency for a group to stick together and
remain united in the pursuit of its instrumental objectives and/or for the satisfaction of
member affective needs” [5]. Cohesiveness is the extent to which team members stick
together and remain united in the pursuit of a common goal. A team is said to be in a
state of cohesion when its members possess bonds linking them to one another and to
the team as a whole.
When agreeing and prioritizing workloads with members of the team, the project
manager needs to ensure that individual members of the team are happy with their
workloads, and not working under undue stress. It is crucial both to the effectiveness
and the quality of the working atmosphere of the project team. The project manager
should ensure that team members are well prepared, that they understand what their
current objectives mean, how they will be measured and what manager’s expectations
are.
Involving individuals in the process of their objectives setting will improve
understanding of manager’s expectations and the expectations of the organization. The
project manager has to motivate to obtain the objectives as well as clearly demonstrate
the coherence between individual objectives and project goal. In other cases, team
members might divert themselves to other tasks due to a lack of belief or interest in the
goal.
The biggest and most important common goal for a team is to ﬁnish the project
successfully. However, this should not be the only goal. Many things can bring people
together and focus them on results and not on personal comfort. To negotiate the
individual objectives effectively, the project manager needs to know the expectations of
team members.
300
V. Liubchenko

3
Expectation Mapping
Academics and practitioners agree that expectations play a central role in project
organizations. For the latter, these expectations are typically set at the start of initiatives
and comprise iron triangle measurements, as well as more reﬁned metrics for expected
beneﬁts or user satisfaction. Usually, it pointed at the expectation of stakeholders and
senior management [6]. In addition, the researchers and practitioners pay attention to
customer expectation management [7]. Let us focus on the expectations of team
members, because of them deﬁne the personal objectives and cause the behavior during
the project implementation.
Usually, there are recognized four expectation types: must, will, should, and could
[8]. This classiﬁcation explains how expectations affect relationships and deﬁne
potential gain or damage due to gaps between expectation and performance. Different
expectation types have different impacts on interpersonal relations and relate to various
areas of the project implementation.
Also, by analogy with [9], we can distinguish four types of expectations:
– ideal expectations are visions, aspirations, needs, hopes and desires, related to the
participation in the project;
– normative expectations are expectations about what should or ought to happen,
mostly derived from what colleagues are told, or led to believe;
– predicted expectations are beliefs about what will happen and are likely to result
from individual experiences;
– unformulated expectations are not articulated expectations.
The most valuable for project manager are normative and predicted expectation
because they affect the personal preferences and can be articulated.
The best practice to discover personal expectations is one-to-one meetings. During
the series of such meetings, the project manager (or team leader) is gathering the
unstructured set of notes (mental or hand-written) about personal expectations of team
members. After ﬁnishing, he should arrange the notes into a useful model to help
understand the expectations of team members, identify holes and omissions in relations
between project objectives and personal expectations, and successfully plan the
motivation strategy for each team member.
As the model of team members’ expectations, we propose the expectation map. The
expectation map tool is a diagram representing how the personal expectations corre-
spond to the project objectives (see Fig. 1).
On the expectation map a set {v1, …, vM} is a set of project objectives, a set
{sj1, …, sjp} is a set of expectations of jth team member, an arrow between vi and sjk
represents correspondence between ith objective and kth expectation of jth team
member.
For example, the ith objective is formulated as “To improve customer satisfaction
rates by 50 percent by September 06 through improving user interface usability.” Let us
suppose kth expectation of jth team member is “To climb on career ladder as UI/UX
designer.” To visualize the existed correspondence the nodes vi and sjk should be
connected by the arrow from sjk to vi.
Analyzing Project Team Members’ Expectations
301

Expectation maps clearly lay out the mental perceptions of team members so that
project manager may identify disconnects in the personal expectations and project
objectives.
4
Formal Analysis of Expectation Map
Expectation map is not only the visualization tool but also analysis support tool. Let us
introduce the mapping M of expectations set onto objectives set such as M(sjk, vi) takes
place when kth expectation of jth team member corresponds to ith objective. Deﬁne a
set Svi
as a set of personal expectations corresponding to the objective vi:
Svi ¼
sjk : M sjk; vi




.
The ideal expectation map is formally described as
8vi Svi 6¼ ;
j
& 8sjk 2
[
M
i¼1
Svi
ð1Þ
In other words, for each project objective there was discovered the set of corre-
sponded personal expectations. The case looks like the most comfortable for a project
manager because he can ﬁnd motivated team members for all objectives. However,
sometimes, the ideal map contains the set of conﬂicting expectations (the antipattern
Sect. 4.3 described below).
The project manager can prioritize the project objectives accordingly with cardinal
numbers of the sets Svi. Therefore, the highest priority deﬁnes the objective interested
for the majority of team members. In other words, the highest prioritized objectives are
the most valuable ones from the team point of view.
Below we describe the important antipatterns for the expectation map analysis.
Fig. 1. The structure of expectation map.
302
V. Liubchenko

4.1
Unmet Expectation
The antipattern is formally described as
9sjk 62
[
M
i¼1
Svi
ð2Þ
In other words, there is the personal expectation not corresponded to any objective
(for example, sl2 at Fig. 1) at the map.
The project manager needs to make sure no team member is an island, or the project
might fail. If the unmet expectation is not the only expectation of particular team
member, the case is not crucial.
However, the project manager should negotiate the unmet expectation with the
team member as well as the vision of its realization. In the best case, the unmet
expectation is achievable during the project implementation, although it is not directly
related to the project objectives. Otherwise, the project manager should suggest the
ways realize the unmet expectation in the future.
Additionally, the project manager should analyses whether the unmet expectation is
an unreasonable one. Unreasonable expectations are those, which are impossible or
highly unlikely for any individual to meet. The project manager should understand
possible reasons behind unreasonable expectations, as well as their impact on the
motivation of the team member. Also, he has to keep in mind discovered expectation
gap during all project period.
4.2
Missed Objective
The antipattern is formally described as
9vi Svi ¼ ;:
j
ð3Þ
In other words, there is the project objective not corresponded to any personal
expectation (for example, v2 at Fig. 1) at the map.
The case is essential from project manager’s point of view. The lack of personal
interest in particular objective usually causes its ignoring. If it is not possible to drop
the missed objective, project manager should ﬁnd the way to make the objective
attractive for the team members.
In this situation, we can offer two solutions.
– The best solution. The project manager negotiates with the team member the rea-
sons for his expectations and shows that some his/her reasons are related to the
achievement of the missed project objective.
– The deferred solution. The project manager explains to the team member that
his/her knowledge and experience are necessary to achieve the missed project
objective. At the same time, both participants are looking for a compromise solution
that could potentially lead to a negative result in the future.
Analyzing Project Team Members’ Expectations
303

4.3
Conﬂicting Expectations
The antipattern should be taken into account when the cardinality of set Svi is greater
than one possibility, Svi
j
j [ 1. In this case, the relations between personal expectations
Svi and project objective vi can be different: positive in the case when the objective
achieving satisfy the expectations, negative in the case when failure to meet objective
satisfy the expectations.
The project manager has to understand that this antipattern indicates a precondition
for conﬂict in the team. Therefore, the right solution is the involvement of conﬂict
management techniques. It is the very ﬁrst stage of the conﬂict, so the probability of
ﬁnding win/win solution is high.
5
The Process of the Expectation Map Analysis
Finally, we describe the process of the expectation map analysis (see Fig. 2).
Firstly, the project manager has to ﬁnd out the personal expectations of the team
members and to visualize their relationship with project objectives. As mentioned
above, with this purpose, the project manager should use the one-to-one meeting; the
best condition is an informal atmosphere. The project manager has to pay attention to
the cognitive biases, which can lead to deviation from real expectations (for example,
attentional bias, bandwagon effect, choice-supportive bias, etc.). The cognitive biases
Fig. 2. DFD-diagram – expectation map analysis
304
V. Liubchenko

can inﬂuence not only on the team members when they formulate their expectations,
but also on project manager when he/she is building the expectation map.
Then the project manager should proceed to antipatterns analysis. It requires
additional one-to-one meetings with some team members as well as with project
stakeholders.
As a ﬁrst step of the analysis, Unmet Expectations antipattern is examined. The
project manager should negotiate each unmet expectation with its bearer. If the
expectation is unreasonable, it is necessary to explain the situation to team member and
control his behavior during project implementation period. Otherwise, the project
manager should negotiate with team member his expectations and try to ﬁnd a con-
nection between them and project objectives. Sometimes it is impossible; it is not
supposed to happen in the frame of the project. Such case should be negotiated
carefully to avoid falling short of expectation.
After this step, the expectation map usually is modiﬁed; we should point to the
particular situation. It appears when there are unmet expectations impossible for sat-
isfaction in the frame of the project. If the corresponding team member agrees with the
case for ineligibility of expectation, the project manager can consider it as conditionally
met.
As a second step of the analysis, Missed Objective antipattern is examined. It is
necessary to negotiate missed objectives with stakeholders to decide whether this
objective can be dropped. If it is not possible, the stakeholder should consider whether
the objective could be formulated with respect to some expectations.
If the ideal or conditionally ideal (with conditionally met expectations) map was not
gotten after previous steps, the project manager should return to the ﬁrst step. Other-
wise, he continues with Conﬂicting Expectation analysis.
The project manager should analyze subgroup of the team members corresponded
to the set of conﬂicting expectations. The team members with positive “expectations –
objective” correlation are possible resources appointed to work for corresponding
project objective. If such solution is not realizable, then it is necessary to understand
why team members have different interests. These reasons should be negotiated with
team members to deﬁne the actions that are acceptable to all parties.
6
Case Study
Let us see an example of expectation map analysis for students’ expectation man-
agement in the project on degree work development. The problem decided in the frame
of the project is the analysis of the geographical distribution and evaluation of
employment successfulness of universities graduates. It is an important issue not only
for particular University but also for whole regions. But there has not still developed
the automated tool, so that the solutions are usually time and cost ineffective because of
need in the organization of primary research with survey distribution, collecting and
analyzing covered a large number of people in different cities and countries. To reduce
the investigation efforts, the analyzing software based on data from social networks
should have been developed.
Respectively the software development project had four objectives:
Analyzing Project Team Members’ Expectations
305

– v1 – to avoid the need for the primary research for gathering the source data for
analysis;
– v2 – to realize the automated tool for collecting and analyzing data;
– v3 – to support the big data processing and renewal;
– v4 – to provide the different kinds of information representation.
There were ﬁve students involved in the project implementation. At the early stage,
their expectations were quite different; mined expectations are listed below:
– for student S1:
• s11 – to get the experience of teamwork;
• s12 – to improve the software design and development skills;
• s13 – to study new technologies;
– for student S2:
• s21 – to understand is it realistic to implement the software architecture;
• s22 – to get the experience in multiservice application development as a team
member;
• s23 – to work with big data;
– for student S3:
• s31 – to get expertise in the project implementation in Java from scratch to the
end;
• s32 – to enhance the teamwork skills;
• s33 – to develop the communication skills;
• s34 – to expand the professional horizons.
– for student S4:
• s41 – to learn how to use the API of social network fully;
• s42 – to discover the teamwork on joint project;
• s43 – to strengthen the knowledge in software architecture design;
• s44 – to examine some design patterns implemented them in code;
• s45 – to get the comments and pieces of advice from experienced mentors;
– for student S3:
• s51 – to get the experience of teamwork on large system development;
• s52 – to gain the skill of development the big data analysis systems;
• s53 – to learn how to use the modern development frameworks;
• s54 – to improve the knowledge of technologies for web application development;
• s55 – to enhance the skills in group working with version control systems;
The initial expectation map is shown in Fig. 3.
As we see the initial expectation map is not ideal because the condition (1) does not
hold. Therefore, we should have started with unmet expectation ﬁxing.
Nine unmet expectations could have been divided into three semantic groups –
teamwork, technology and speciﬁc groups. The teamwork group is the biggest one, it
contents {s11, s22, s32, s33, s42, s51, s55} and cannot be ignored. The development of the
analytical system was too great for individual graduate work and was planned for group
work. Therefore, the graduate work supervisor (as project manager) articulated it as v5 –
to organize teamwork on the software project. The technology group is represented by
s13, s34 (it was cleared up that it concerns new technologies and design patterns) and
306
V. Liubchenko

partially s55. During the one-to-one meeting, there were negotiated the requirement to
the developed system and found that the most required technologies are new for stu-
dents. The speciﬁc expectation s45 was not directly related to the project objectives, and
student understood the fact. Anyway, s45 was taken into account, and expertises from
industry gave mentor session for students’ team four times at the design stage of project
implementation. However, during the expectation map analysis, s45 was left as condi-
tionally met expectation.
As result of unmet expectation ﬁxing, we got the modiﬁed expectation map (see
Fig. 4).
As we see, the modiﬁed map does not include the missed objective. We described
above the speciﬁc of s45, so the modiﬁed map is conditionally ideal. It is possible to
prioritize the project objectives in order v2, v5, v3, v1 and v4. The conﬂicting expectation
Fig. 3. Initial expectation map for students group
Fig. 4. Modiﬁed expectation map for students group
Analyzing Project Team Members’ Expectations
307

analysis did not ﬁnd the conﬂicting expectations in sets Sv2; Sv5; Sv3. Therefore, the
process of expectation map analysis was ﬁnished after the ﬁrst iteration.
7
Conclusion
This paper introduced an empirically grounded approach to analyzing team members’
expectations with the aim of coordinate them with project objectives. It focused on two
types of expectations: normative and predictive expectations. It also suggested the
expectations map as a model of team members’ expectation. The important antipatterns
for expectation map analysis were formulated and discussed. The process of expec-
tation map analysis was ﬁnally described.
In the process of expectation map analysis, there is three complicated moments.
The ﬁrst moment appears during the initial one-to-one meeting when project manager
eliminates expectations of team members. Sometimes team members are not able to
formulate their expectations, sometimes they strive to be approved and express not
relevant expectations, and sometimes project manager does not understand right team
member and ﬁxes irrelevant expectations. The project manager has to use some
checking procedures because the principle “garbage in – garbage out” works perfectly
also for expectations map analysis. The second moment appears when project manager
tries to ﬁght with Unmet Expectation antipattern. He/she have to negotiate unmet
expectations with corresponded team members, and it is possible to arrive at the
problems presented above. The third moment appears when project manager works
with stakeholders to ﬁght with Missed Objective antipattern. The project manager has
to persuade stakeholders to reconsider (in some way) the project objectives, to
understand the points of view of all stakeholders and generalize them. Therefore, we
should pay attention at the process subjectivity and dependence on communication
skills of the project manager.
The pilot exploitation of expectation map analysis was realized for teams of soft-
ware development projects and study projects. Nevertheless, there are no limitations to
make use of it in other kinds of projects.
References
1. Software Extension to the PMBOK® Guide, 5th edn. Project Management Institute, Inc.
(2013)
2. Tuckman, B.: Developmental sequences in small groups. Psychol. Bullet. 63, 384–399 (1965)
3. Hu, J., Liden, R.C.: Antecedents of team potency and team effectiveness: an examination of
goal and process clarity and servant leadership. J. Appl. Psychol. 96(4), 851–862 (2011)
4. Senécal, J., Loughead, T.M., Bloom, G.A.: A season-long team-building intervention:
examining the effect of team goal setting on cohesion. J. Sport Exerc. Psychol. 30, 186–199
(2008)
5. Carron, A.V., Brawley, L.R., Widmeyer, W.N.: Measurement of cohesion in sport and
exercise. In: Duda, J.L. (ed.) Advances in Sport and Exercise Psychology Measurement,
pp. 213–226. Fitness Information Technology, Morgantown (1998)
308
V. Liubchenko

6. Schiff, J.L.: 11 project management tips for setting and managing expectations. http://www.
cio.com/article/2378680/project-management/11-project-management-tips-for-setting-and-
managing-expectations.html. Accessed 24 July 2017
7. Ojasalo, J.: Managing customer expectations in professional services. Manag. Serv. Qual. Int.
J. 11(3), 200–212 (2011)
8. Olkkonen, L., Luoma-aho, V.: Public relations as expectation management? J. Commun.
Manag. 18(3), 222–239 (2014)
9. Geurts, J.W., Willems, P.C., Lockwood, C., van Kleef, M., Kleijnen, J., Dirksen, C.: Patient
expectations for management of chronic non-cancer pain: a systematic review. Health Expect.
1–17 (2016)
Analyzing Project Team Members’ Expectations
309

The Contextual Search Method
Based on Domain Thesaurus
Vasyl Lytvyn(&), Victoria Vysotska(&), Yevhen Burov(&),
Oleh Veres, and Ihor Rishnyak
Lviv Polytechnic National University, Lviv, Ukraine
{Vasyl.V.Lytvyn,Victoria.A.Vysotska,Yevhen.V.Burov,
Oleh.M.Veres,Ihor.V.Rishnyak}@lpnu.ua
Abstract. The growth of volume of text resources on Internet and natural
limitations of human cognition justiﬁes the development of search enhancement
systems. This paper proposes a contextual search method based on domain
thesaurus. The method uses semantic metrics deﬁned on thesaurus and weighted
conceptual graph model for search enhancement. As an example a task of
searching for partners working in similar research area is analyzed. The effec-
tiveness of proposed method when evaluated using precision metric was better
compared to Jaccard’s and WUP methods.
Keywords: Thesaurus  Semantic metric  Search method  Linguistics
Graph model
1
Introduction
The number of Internet users is estimated to have reached several billion and continue
to grow [1]. World Wide Web contains billions of documents, and size of textual
information in them amounts to hundreds of terabytes [2]. However, our ability to
understand and process information is rather limited and remains the same [3].
Therefore, the demand for contextual search enhancing systems (CSES) continues to
grow [4]. It is important to develop automated systems, which are able to analyze and
represent the informational needs of their users with more precision [5]. For example,
in the area of scientiﬁc research it is important to ﬁnd partners for grant application
with speciﬁed competence or ﬁnd a reviewer for an article, having similar scientiﬁc
results [6–8]. In order to resolve this problem the usage of domain area thesaurus was
proposed [9, 10]. The system functions as follows. The user types a set of keywords
from thesaurus, which, according to his opinion, fully describes the speciﬁcs of
research area where partner’s search is performed [11–13]. CSES, based on those
keywords, thesaurus and some semantic metric, ﬁnds a set of relevant textual docu-
ments, rated according to this metric [14–16]. The authors of those documents will
form a pool of potential partners [17–20]. The central part of proposed system is
occupied by domain thesaurus acting as a kernel for system’s knowledge base [21–24].
Another important module will calculate the semantic distance between user’s request
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_22

and textual documents. The architecture of proposed system is shown on Fig. 1. The
module, which calculates the semantic distance, uses some semantic distance metric.
Let us review some semantic distance metrics that are used when resolving similar
problems.
2
Previous Research
There are several deﬁnitions of semantic metrics. In Table 1 are presented the methods
for evaluating text documents similarity that are based on:
• Frequencies of words usage in text document;
• The distances between words in a taxonomy;
• Frequencies and distances simultaneously.
Google distance is a degree of semantic coherence, which is calculated based on the
number of pages obtained by pursuing Google search for a given set of keywords. The
Table 1 shows the formula for calculation of the normalized Google distance
(NGD) for two terms: x i y, where M is the total number of web-pages indexed by
Google; f(x) and f(y) – number of pages containing keywords x i y, respectively f(x, y) –
number of pages containing both x, and y. If x and y are found on all pages together,
then we consider NGD = 0, if they occur only separately, then we consider NGD = ∞.
Let us select a class of metrics that compute similarity based on taxonomy data. These
metrics are used to compute the similarity of concepts in WordNet [27], GermaNet,
Wikipedia [25]. In [28] a formula is proposed that takes into account both the depth in
the hierarchy of concepts, and the depth of the lcs (least common subsume function):
wup C1; C2
ð
Þ ¼
lcs C1; C2
ð
Þ
depth C1
ð
Þ þ depth C2
ð
Þ
ð1Þ
Thesaurus
Request creation
Semantic distance
calculation
The source of
textual
documents
Textual documents ranking
The response
Contextual search enhancing system
 
Fig. 1. The architecture of CSES
The Contextual Search Method Based on Domain Thesaurus
311

Ryeznyk [29] proposed to assume that two words are more similar if concept which
relate these two words is more informative, that is placed lower in the taxonomy
(synset in WordNet). When constructing probabilistic function P(C), it is assumed that
the concept’s probability should not be changed while moving up the hierarchy:
res C1; C2
ð
Þ ¼
max
C2S C1;C2
ð
Þ  log P C
ð Þ
ð
Þ
½
. Thus, abstract concepts are less informative.
Ryeznyk proposed to estimate the probability using the frequency of concept’s syn-
onyms in a text document (TD), so:
P C
ð Þ ¼ freq C
ð Þ
N
 freq C
ð Þ ¼
X
n2words C
ð Þ
count n
ð Þ;
ð2Þ
where words (C) are nouns with value C; N is total number of nouns in text document.
In the paper [30] Ryeznik’s metric has been applied to Wikipedia and informative
category was calculated as a function of the hyponyms number (categories in Wiki-
pedia), but not statistically:
reshypo C1; C2
ð
Þ ¼ 1  log hypo lcs C1; C2
ð
Þ
ð
Þ þ 1
ð
Þ
log C
ð Þ
;
ð3Þ
Table 1.
Semantic metrics classiﬁcation
Formula/description of the algorithm
Title
1. Word frequency in text document
NGD x; y
ð
Þ ¼ max log f x
ð Þ;log f y
ð Þ
ð
Þlog f x;y
ð
Þ
log Mmin log f x
ð Þ;log f y
ð Þ
ð
Þ
Normalized distance Google (NGD)
jaccard x; y
ð
Þ ¼
Hits x^y
ð
Þ
Hits x
ð Þ þ Hits y
ð ÞHits x^y
ð
Þ
Jaccard [25]
2. Distances in the taxonomy of terms
Distance corresponds to the number of edges in the
shortest path between concepts
Metrics was used for the concepts of
Roget’s thesaurus [26]
lch C1; C2
ð
Þ ¼  log length C1;C2
ð
Þ
2D
Leacock and Chodorov 1997, [27]
pp. 265–283
wup C1; C2
ð
Þ ¼
lcs C1;C2
ð
Þ
depth C1
ð
Þ þ depth C2
ð
Þ
Wu and Palmer [28]
reshypo C1; C2
ð
Þ ¼ 1  log hypo lcs C1;C2
ð
Þ
ð
Þ þ 1
ð
Þ
log C
ð Þ
Metrics res [29], adapted to the taxonomy
of the Wikipedia categories
3. Frequency words and distances in the taxonomy
res C1; C2
ð
Þ ¼
max
C2S C1;C2
ð
Þ  log P C
ð Þ
ð
Þ
½

Distance res [30]
lin C1; C2
ð
Þ ¼
2log P C0
ð
Þ
ð
Þ
log P C1
ð
Þ
ð
Þ þ log P C2
ð
Þ
ð
Þ
Distance lin [31]
4. Text overlap
Text overlap (based on WordNet)
Lesk [32]
Extended gloss overlap – text overlap using the
neighbouring concepts from WordNet
Banerjee and Pedersen 2003 [33]
relategloss=text T1; T2
ð
Þ ¼ tanh
overlap T1;T2
ð
Þ
length T1
ð
Þ þ length T2
ð
Þ
Distance relate [25]
312
V. Lytvyn et al.

where lcs is the nearest common subsumer of concepts C1 and C2, hypo – number of
hyponyms of this subsumer, and C – total number of concepts in the hierarchy.
In [31] lin deﬁnes the similarity of objects A and B as the ratio of the amount of
information required to describe the similarity of A and B, to the amount of information
that fully describes A and B. To measure the similarity between words lin takes into
account the frequency distribution of words in text (similar to the Reznik’s measure):
lin C1; C2
ð
Þ ¼
2  log P C0
ð
Þ
ð
Þ
log P C1
ð
Þ
ð
Þ þ log P C2
ð
Þ
ð
Þ ;
ð4Þ
where C0 – nearest common super class in the concept hierarchy for both concepts
C1 and C2, P – probability of concept, calculated on the basis of its frequency in the
text document. It differs from the formula res by normalization method, correct
computation lin(x, x) (independent of the concept’s position in the hierarchy), takes
into account existence of common and distinctive properties in objects.
In the paper [25] similarity of the two texts T1 and T2 is calculated from the double
normalization (the length of the text and using hyperbolic tangent) as:
relategloss=text T1; T2
ð
Þ ¼ tanh
overlap T1; T2
ð
Þ
length T1
ð
Þ þ length T2
ð
Þ ;
ð5Þ
overlap T1; T2
ð
Þ ¼
X
n
m2;
ð6Þ
where n phrases m words overlap.
Thus, the analysis of existing metrics has shown that no semantic metric is based on
thesauri, and only a few of them take into account the taxonomy of concepts.
This article aims to develop and evaluate semantic metric based on domain the-
saurus and method of contextual search enhancement using this metric.
3
Thesaurus-Based Search Enhancement Method
Thesaurus is a list of logical-semantic relations between linguistic terms. The thesaurus
embraces not only the set of terms provided in the form of an alphabetical list of their
deﬁnitions, but also contains the models, which represent relationships between terms.
Based on the achievements of modern linguistics it gives in a compact and accessible
form the interpretation of terminological units from terminological dictionaries and
encyclopedias. The thesaurus contains terms in main research areas of theoretical and
applied linguistics: grammar, word formation, lexicology, semantics, lingvosemiotic,
computational linguistics, lexicography etc. For the purpose of this article, the terms
were selected from the abstracts of papers, published in the Ukrainian linguistic
periodicals in the 2009–2011. Building a thesaurus provides for the elucidation of the
main types of relations between concepts, the main ones are correlation, synonymy,
hyponymy-hypernymy, holonymy-meronymy.
The Contextual Search Method Based on Domain Thesaurus
313

The title of relation is a double predicate R(A, B), which binds headword from
article (A) and predicate term (B) [34]. The set of relations R is divided into types
(correlation, hyperonymy - hyponymy, synonymy, holonymy-meronymy) - R = {R1,
R2, …, Rk}. ni is the number of relations of type Ri in the thesaurus. So, the total
number of relations is N ¼ P
k
i¼1
ni. Let us assume that the weight of the relation is more,
when this type of relation is more frequent in the thesaurus. This weight of the ratio is
deﬁned as Li ¼ ni
N. Let us weigh our semantic network that deﬁnes the thesaurus. For
this purpose we deﬁne the weight of the relation between thesaurus terms. The smaller
the weight, the terms are more similar. Therefore, the weight of the arcs in semantic
network is deﬁned as inversely proportional to the weight of such relation that deﬁnes
this arc: li ¼ K
Li ¼ KN
ni , where K is some constant that speciﬁes the unit of weight
measurement for arcs in semantic network [35–37].
The weighted semantic network is used to ﬁnd potential partners who are engaged
in similar research issues in the domain for which the thesaurus was built.
To do this, a set of key terms C = {C1, C2, …, Cn} from the thesaurus should be
created, which is deﬁning the speciﬁc research issues. Search engine ﬁnds a set of
documents, which contain terms from the thesaurus. For each such document Ts a set
with capacity m is built containing the terms from the thesaurus that are frequently used
in Ts: ^Cs ¼
^Cs
1; ^Cs
2; . . .; ^Cs
m


. Using the Floyd-Warshall or Deikstra method [38] n  m
of the shortest distance ds
ij ¼ d Ci; Cs
j


between terms from sets C and ^Cs are obtained.
Then the distance to the document found Ts according to the formula: ds ¼ P
n
i¼1
P
m
j¼1
ds
ij is
calculated. The found documents are ranked according to increasing values ds. The
authors of the documents with the higher rank may be our potential partners. [39–46].
Let us consider an example of thesaurus for text’s linguistics domain. Corresponding
semantic network is shown on Fig. 2. In vertexes of the network are placed terms. The
links between terms are denoted by “S” – synonyms, “CR” – correlation, “Hol” –
holonym, “M” – meronym, “Hyp” – hypernym.
Rubrics
Header
Title
Text
Integrity
Interpretation
Сonsistency
Text
connectivity
Text
categorization
Cohesion
Definition
Narrative
Story
Discourse
M
S
CR
Hol
CR
Hol
Hol
S
CR
Hyp
CR
Hyp
S
Hyp
M
Hol
M
CR
CR
S
S
Fig. 2. Semantic network for thesaurus of text linguistics
314
V. Lytvyn et al.

After the application of the formulae (1) we obtain the values for networks edges:
for synonimical link – 1; correlation link – 1,2; holonym and meronim links – 1,4;
hypernym – 1,5. The conceptual graph with correspondingly weighted vertexes is
shown on Fig. 3. Let us ﬁnd the shortest paths between terms of this graph using
Floyd-Warshall algorithm [38]. The resulting matrix of distances is shown in Table 2.
Example. If the set C contain two terms C = {‘rubric’, ‘text’} and some textual doc-
ument T is described by a set ^C, which has three terms ^C = {‘title’, ‘cohesion’, ‘nar-
rative’}, then the distance to this text is: d = 2,2 + 6,4 + 4+2,4 + 3,8 + 1,4 = 20,2.
In order to evaluate the effectiveness of proposed semantic metric, the series of
experiments were performed. In them, the relevance of scientiﬁc articles (annotations,
documents), found in Internet was evaluated related to query, containing keywords
1.
Rubrics
2.
Header
3.
Title
5.
Text
6.
Integrity
9. Interpretation
7. Consistency
8.
Text
connectivity
11
Text
categorization
12.
Cohesion
14. Definition
10. Narrative
13.
Story
4. Discourse
1
1,2
1
1,2
1
1
1,2
1,4
1,4
1,2
1,2
1,4
1,4
1,4
1,5
1,2
1,5
1,4
1
Fig. 3. The weighted conceptual graph
Table 2.
The distances between terms in conceptual graph.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
1
1,2 2,2 3,8 2,6 4
5
5,2 5,2 6,4 5,4 4
7,4 6,2
2
1,2
1
2,6 1,4 2,8 3,8 4
4
5,2 4,2 2,8 6,2 5
3
2,2 1
3,6 2,4 3,8 4,8 5
5
6,2 5,2 3,8 7,2 6
4
3,8 2,6 3,6
1,2 2,6 3,6 3,8 1,4 2,6 4
2,6 3,6 2,4
5
2,6 1,4 2,4 1,2
1,4 2,4 2,6 2,6 3,8 2,8 1,4 4,8 3,6
6
4
2,8 3,8 2,6 1,4
1
1,2 4
5,2 1,4 1,2 6,2 5
7
5
3,8 4,8 3,6 2,4 1
2,2 5
6,2 1,5 1,2 7,2 6
8
5,2 4
5
3,8 2,6 1,2 2,2
5,2 6,4 1,5 1,4 7,4 6,2
9
5,2 4
5
1,4 2,6 4
5
5,2
1,2 5,4 4
2,2 1
10 6,4 5,2 6,2 2,6 3,8 5,2 6,2 6,4 1,2
6,6 5,2 1
2,2
11 5,4 4,2 5,2 4
2,8 1,4 1,5 1,5 5,4 6,6
1,4 7,6 6,4
12 4
2,8 3,8 2,6 1,4 1,2 1,2 1,4 4
5,2 1,4
6,2 5
13 7,4 6,2 7,2 3,6 4,8 6,2 7,2 7,4 2,2 1
7,6 6,2
3,2
14 6,2 5
6
2,4 3,6 5
6
6,2 1
2,2 6,4 5
3,2
The Contextual Search Method Based on Domain Thesaurus
315

from developed thesaurus. For comparison, the relevance of found texts to keywords
was also calculated, using other knows metrics – the metrics of Jaccard and WUP.
Ten experiments were executed for each method. The results were additionally
evaluated by an expert. From the set of found relevant documents he selected ones
which were truly relevant according to his opinion and search criteria with a purpose to
analyze the precision of compared methods.
precision ¼ number found relevant
number all found
ð7Þ
So, the effectiveness of semantic metrics were evaluated using precision parameter
s: s ¼ re=rm  100, where re ¼ Re
j
j – the cardinality of set Re of relevant documents,
found with corresponding method (according to opinion of domain expert);
rm ¼ Rm
j
j – the cardinality of set Rm of all documents, found with corresponding
method, ReRm. The results of all ten experiments are shown on Fig. 4. The precision
of relevant documents search performed with developed system is higher (dark col-
umns) than precision when using Jaccard and WUP methods.
Overall, the effectiveness of approach using thesaurus-based semantic metric
deﬁned by parameter s, in average is in the range from 10 to 20% better when com-
pared with other metrics. Higher efﬁciency is obtained due to usage of weighted
conceptual graph representing thesaurus.
4
Conclusion
The continuous growth of number and size of textual informational resources in
Internet requires development of machine-assisted search methods allowing to enhance
the relevance of search results with domain knowledge. An approach for building a
Jaccard
0
10
20
30
40
50
60
70
80
90
1
2
3
4
5
6
7
8
9
10
Precision
Experiment number
Jaccard
WUP
Thesaurtns
Fig. 4. The results of experiments aiming to determine the precision of search for relevant
documents with different methods
316
V. Lytvyn et al.

contextual search using semantic metric based on domain thesaurus was proposed. This
thesaurus is represented as semantic network graph. The vertexes of the graph are
weighted reverse proportionally to the number of relations of speciﬁed type. Semantic
metric is built using this semantic network graph. The application of proposed method
to the task of searching research partners is demonstrated.
Experiments have shown that the search using Jaccard’s metric only in 40% of
cases found the documents having the largest number of words in common with a
search query. The search using WUP metric based on number of common connections
also did not provide satisfactory results. On the other hand, the usage of domain
information from thesaurus and weighting of vertexes in conceptual graph allowed to
ﬁnd the documents which are more relevant to keywords provided by end-user.
References
1. The Internet Economy in the G-20 – BCG. https://www.bcg.com/documents/ﬁle100409.pdf
2. Vysotska, V., Chyrun, L., Chyrun, L.: Information technology of processing information
resources in electronic content commerce systems. In: Computer Science and Information
Technologies, CSIT 2016, pp. 212–222 (2016)
3. Vysotska, V., Chyrun, L., Lytvyn, V., Dosyn, D.: Methods Based on Ontologies for
Information Resources Processing. LAP Lambert Academic Publishing, Saarbrücken (2016)
4. Vysotska, V., Chyrun, L.: Analysis features of information resources processing. In:
Proceedings of Xth International Conference on Computer Science and Information
Technologies, CSIT 2015, pp. 124–128 (2015)
5. Gladun, A.: Building thesaurus of subject area as a tool for modeling information needs of a
user for Internet search. Comput. Inf. Technol. Rev. 1, 26–33 (2007)
6. Shakhovska, N., Vysotska, V., Chyrun, L.: Features of E-learning realization using virtual
research laboratory. In: Proceedings of the XIth International Conference on Computer
Science and Information Technologies, CSIT 2016, pp. 143–148 (2016)
7. Shakhovska, N., Vysotska V., Chyrun, L.: Intelligent systems design of distance learning
realization for modern youth promotion and involvement in independent scientiﬁc
researches. In: Advances in Intelligent Systems and Computing, pp. 175–198. Springer
(2017)
8. Lytvyn, V., Vysotska, V., Chyrun, L., Chyrun, L.: Distance learning method for modern
youth promotion and involvement in independent scientiﬁc researches. In: Proceedings of
the IEEE First International Conference on Data Stream Mining and Processing (DSMP),
pp. 269–274 (2016)
9. Gruber, T.: A translation approach to portable ontologies. Knowl. Acquisition 5(2), 199–220
(1993)
10. Gavrilova, T.: Knowledge Bases in Intelligent Systems. Piter, Saint Petersburg (2001).
384 pages
11. Lytvyn, V., Pukach, P., Bobyk, I., Vysotska, V.: The method of formation of the status of
personality understanding based on the content analysis. Eastern Eur. J. Enterp. Technol. 5/2
(83), 4–12 (2016)
12. Lytvyn, V., Vysotska, V., Veres, O., Rishnyak, I., Rishnyak, H.: Classiﬁcation methods of
text documents using ontology based approach. In: Advances in Intelligent Systems and
Computing, vol. 512, pp. 229–240. Springer International Publishing AG (2017)
The Contextual Search Method Based on Domain Thesaurus
317

13. Lytvyn, V., Vysotska, V., Pukach, P., Bobyk, I., Pakholok, B.: A method for constructing
recruitment rules based on the analysis of a specialist’s competences. Eastern Eur.
J. Enterp. Technol. 6/2(84), 4–14 (2016)
14. Lytvyn, V., Vysotska, V., Pukach, P., Brodyak, O., Ugryn, D.: Development of a method for
determining the keywords in the Slavic language texts based on the technology of web
mining. Eastern Eur. J. Enterp. Technol. 2/2(86), 4–12 (2017)
15. Lytvyn, V., Vysotska, V., Veres, O., Rishnyak, I., Rishnyak, H.: Content linguistic analysis
methods for textual documents classiﬁcation. In: Proceedings of the XIth International
Conference on Computer Science and Information Technologies, CSIT 2016, pp. 190–192
(2016)
16. Lytvyn, V., Vysotska, V.: Designing architecture of electronic content commerce system. In:
Computer Science and Information Technologies, CSIT 2015, pp. 115–119 (2015)
17. Khomytska, I., Teslyuk, V.: The method of statistical analysis of the scientiﬁc, colloquial,
Belles-Lettres and newspaper styles on the phonological level. In: Advances in Intelligent
Systems and Computing, vol. 512, pp. 149–163 (2017)
18. Khomytska, I., Teslyuk, V.: Speciﬁcs of phonostatistical structure of the scientiﬁc style in
english style system. In: Proceedings of the XIth International Conference on Computer
Science and Information Technologies, CSIT 2016, pp. 129–131 (2016)
19. Vysotska, V.: Linguistic analysis of textual commercial content for information resources
processing. In: Modern Problems of Radio Engineering, Telecommunications and Computer
Science, TCSET 2016, pp. 709–713 (2016)
20. Vysotska, V., Chyrun, L., Chyrun, L.: The commercial content digest formation and
distributional process. In: Proceedings of the XIth International Conference on Computer
Science and Information Technologies, CSIT 2016, pp. 186–189 (2016)
21. Lytvyn, V., Uhryn, D., Fityo, A.: Modeling of territorial community formation as a graph
partitioning problem. Eastern Eur. J. Enterp. Technol. 1/4(79), 47–52 (2016)
22. Lytvyn, V., Tsmots, O.: The process of managerial decision making support within the early
warning system. Actual Prob. Econ. 11(149), 222–229 (2013)
23. Chen, J., Dosyn, D., Lytvyn, V., Sachenko, A.: Smart data integration by goal driven
ontology learning. In: Advances in Big Data. Advances in Intelligent Systems and
Computing, pp. 283–292. Springer (2016)
24. Lytvyn, V., Dosyn, D., Smolarz, A.: An ontology based intelligent diagnostic systems of
steel corrosion protection. Elektronika 8, 22–24 (2013)
25. Strube, M.: WikiRelate! Computing semantic relatedness using Wikipedia. In: Proceedings
of the 21st National Conference on Artiﬁcial Intelligence. http://www.eml-research.de/
english/research/nlp/public
26. Jarmasz, M.: Roget’s thesaurus and semantic similarity. In: Proceedings of Conference on
Recent Advances in Natural Language Processing, Borovets, Bulgaria, pp. 212–219 (2003)
27. Fellbaum, C.: WordNet: an Electronic Lexical Database. MIT Press, Cambridge (1998).
423 pages
28. Wu, Z.: Verb semantics and lexical selection. In: Proceedings of ACL-94, pp. 133–138
(1994)
29. Resnik, P.: Disambiguating noun groupings with respect to WordNet senses. http://xxx.lanl.
gov/abs/cmp-lg/9511006
30. Resnik, P.: Semantic similarity in a taxonomy: an information-based measure and its
application to problems of ambiguity in natural language. J. Artif. Intell. Res. (JAIR) 11, 95–
130 (1999)
31. Lin, D.: An information-theoretic deﬁnition of similarity. In: Proceedings of International
Conference on Machine Learning (1998). http://www.cs.ualberta.ca/*lindek/papers.htm
318
V. Lytvyn et al.

32. Smirnov, A.: Ontologies in artiﬁcial intelligence systems. In: Artiﬁcial Intelligence News,
vol. 2, pp. 3–9. (in Russian)
33. Sovpel, I.: The system for automatic extraction of knowledge from text and it’s applications.
Artif. Intell. 3, 668–677 (2004). (in Russian)
34. Nikitina, S.E.: Thesaurus on theoretical and applied linguistics. Science, 14 (1978)
35. Lytvyn, V., Shakhovska, N., Pasichnyk, V., Dosyn, D.: Searching the relevant precedents in
dataspaces based on adaptive ontology. Comput. Prob. Electric. Eng. 2(1), 75–81 (2012)
36. Lytvyn, V., Dosyn, D.: Planning of intelligent diagnostics systems based domain ontology.
In: VIIIth International Conference on Perspective Technologies and Methods in MEMS
Design, vol. 103 (2012)
37. Lytvyn, V., Dosyn, D., Medykovskyj, M., Shakhovska, N.: Intelligent agent on the basis of
adaptive ontologies construction. In: Signal Modelling Control (2011)
38. Svami, M.: Graphs, Networks and Algorithms (1984). 256 pages
39. Montes-y-Gómez, M., Gelbukh, A., López-López, A.: Comparison of Conceptual Graphs.
Lecture Notes in Artiﬁcial Intelligence, vol. 1793 (2000)
40. Knappe, R., Bulskov, H., Andreasen, T.: Perspectives on ontology-based querying. Int.
J. Intell. Syst. (2004). http://akira.ruc.dk/*knappe/publications/ijis2004.pdf
41. Basyuk, T.: The main reasons of attendance falling of internet resource. In: Proceedings of
the Xth International Conference on Computer Science and Information Technologies, CSIT
2015, pp. 91–93 (2015)
42. Kravets, P., Kyrkalo, R.: Fuzzy logic controller for embedded systems. In: Proceedings of
5th International Conference on Perspective Technologies and Methods in MEMS Design,
MEMSTECH (2009)
43. Mykich, K., Burov, Y.: Algebraic framework for knowledge processing in systems. In:
Advances in Intelligent Systems and Computing, pp. 217–228. Springer (2017)
44. Pasichnyk, V., Shestakevych, T.: The model of data analysis of the psychophysiological
survey results. In: Advances in Intelligent Systems and Computing, vol. 512, pp. 271–282.
Springer International Publishing AG (2016)
45. Lytvyn, V.: Design of intelligent decision support systems using ontological approach. Int.
Q. J. Econ. Technol. New Technol. Model. Process. II(1), 31–38 (2013)
46. Burov, E.: Complex ontology management using task models. Int. J. Knowl. Based Intell.
Eng. Syst. Amsterdam 18(2), 111–120 (2014)
The Contextual Search Method Based on Domain Thesaurus
319

Optimizing Wind Farm Structure Control
Vitalii Kravchyshyn1(✉), Mykola Medykovskyy1, Roman Melnyk1,
and Marianna Dilai2
1 Department of Automated Control Systems, Lviv Polytechnic National University,
12, S. Bandera St., Lviv 79013, Ukraine
vitalik1991ua@gmail.com, medykmo@gmail.com, hjvfyfyfy@gmail.com
2 Department of Applied Linguistics, Lviv Polytechnic National University,
12, S. Bandera St., Lviv 79013, Ukraine
mariannadilai@gmail.com
Abstract. The paper presents a solution to the wind farm structure optimization
problem (determination of the optimal set of active wind turbines at a given time).
The signiﬁcance of wind parameters prediction for determining the wind farm
active set is justiﬁed. The theoretical solution to this problem enables reﬁning
control algorithms for the power-dynamic modes of wind farms in order to
increase their eﬃciency by reducing operational losses and electrodynamic over‐
load when switching wind turbines. It can be achieved by estimating the future
wind power capacity based on the short-term prediction of wind speed values in
combination with other mode parameters. The ﬁndings are particularly relevant
when electric power storage element is used in a wind farm structure. In this case,
the key variables that determine the mode parameters are current and future wind
speed, consumer load, current energy capacity of a storage battery, eﬀectiveness
and availability of wind turbines (WT). As a result, we receive the optimal number
of active (operating at a given time) wind turbines not only for the current, but
also the next parameters of power-dynamic modes minimizing switching opera‐
tions. The eﬀectiveness of the available methods of time series prediction depends
on the knowledge and skills of experts, and subjectivity in the development of
mathematical models. Hence, the feasibility of artiﬁcial neural networks appli‐
cation for prediction is surveyed. The research shows the results of the imple‐
mentation of such networks as a classical back propagation network, the Elman
network and the cascade neural network. The impact of the energy storage
element capacity on the number of switching operations in the wind farm structure
is investigated. We oﬀer the solution to the problem of optimal choice of the
energy storage element capacity which will balance meeting maximum load
requirements and the cost of its installation and operation. As a practical result,
the algorithm (rules of inference) for minimizing the number of switching oper‐
ations of the active set of WF is developed taking into account such parameters
as current wind speed, archival wind speed, predicted value of wind speed, current
load value, next load value, nominal output of a wind farm, types of wind turbines,
the number of wind turbines of each type, technical condition of each wind
turbine, power capacity of a storage battery, etc. The results of the experimental
studies of the developed algorithm eﬃciency are presented.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_23

Keywords: Wind farm · Neural networks · Wind speed prediction
Wind turbine
1
Introduction
The estimated capacity of the modern wind farms (WF) is about tens or even hundreds
of Megawatts. A wind farm consists of a set of automated wind turbines (WT), energy
storage elements, dispatch control systems for power-dynamic modes of parallel oper‐
ation with a distribution network [1, 2]. At the same time, increasingly frequently, in
addition to generating active electric power, wind farms perform functions of controlling
distribution networks modes in order to provide the required energy quality and reduce
its process loss [3]. The possibility of performing these functions is predetermined by
irregularity of load curve (there are idle capacities in the periods of daily, weekly and
annual minimum loads) and wind power capacity (excess of power generation capacity).
Moreover, a long-term operation of wind turbines requires scheduled outage and main‐
tenance work which makes it impossible to use them for production functions for some
period of time. Such restrictions may also be imposed during emergencies or in pre-
emergency situations. Consequently, at a given time, there is a need and it is possible
to use a certain set of wind turbines to generate active electric power, hereinafter they
will be referred to as an active set of a wind farm.
As for the remaining set of wind turbines, there is either no need or no possibility
to use them for the stated purpose. They currently perform other functions or are not
available for use. Taking into account the fact that each wind turbine may have
different nominal parameters (power capacity, operating wind speed range) and
performance criteria (service life, active mode time, objective parameters of mechan‐
ical and electromagnetic systems), we can state that the formation of the active set of
a wind farm is a topical problem of multi-parameter optimization researched in a
number of studies [4, 5].
The overview of well-known works shows that today insuﬃcient attention is devoted
to issues concerning the eﬃcient use of ﬁxed assets, that is, the reduction of operating
costs for energy production by ensuring uniform wear of system elements [6, 7]. Partic‐
ularly acute this issue is for WF operation due to the stochastic nature of the wind and
load curve which requires frequent switching of units. Switching operations of powerful
electromechanical systems trigger transition processes which as a result speed up
wearing out of elements. Consequently, uniform wear and minimization of WF
elements’ switching are important parameters for optimizing the modes and overall
performance of WF.
This paper presents the research ﬁndings obtained with regards to the stochastic
nature of the wind given a power storage element and the active power load curve.
The research goal is to optimize the control of power-dynamic modes of modern
wind farms with the energy storage element by minimizing the number of switching
operations based on the wind speed prediction, taking into account WT technical condi‐
tion and load curve requirements.
Optimizing Wind Farm Structure Control
321

2
Description of the Data
Recently, the problems of real-time data prediction have become particularly relevant
in the ﬁeld of optimization of various processes due to the rapid development of algo‐
rithms for ﬁnding an optimal and eﬀective solution. The main idea of using prediction
methods for determining the active set of a wind farm is to take into account the future
values of the wind speed looking for an eﬃcient solution, in order to minimize the
number of switching operations for the WF active set, and to enable more ﬂexible control
of the individual components of the system in order to improve their performance.
To date there are a large number of solutions that allow us to make predictions
employing probabilistic methods and subjective knowledge of experts [8–10]. The main
drawbacks of such methods are as follows: insuﬃcient accuracy of prediction, depend‐
ence of the results on the knowledge and qualiﬁcations of the expert in a particular
subject area, subjectivity in the development of the mathematical model and structure,
and, accordingly, the diﬃculties with the implementation of such models, etc.
That is why, in order to obtain an adequate solution for time series prediction, it is
appropriate to use models with artiﬁcial neural networks [11, 12]. The construction of
a neural network consists of two stages: the choice of the network type (architecture)
and the selection of weights, that is, the neural network training.
A speciﬁc type of transformation in the neural network is determined not only by
the characteristics of neural-like elements (activation functions, etc.), but also directly
depends on its architecture.
At the ﬁrst stage, it is necessary to select neurons that will be used in the network
(taking into account the number of inputs, activation functions), how they will be inter‐
connected, and which will be the input and output of the neural network. Based on the
functions performed by neurons in the network, they can be divided into three types:
input, output and intermediate [13]. In most networks, the type of neuron is associated
with its position in the network.
At the second stage, it is necessary to “train” the chosen network, that is, to select
the weight values, so that the network operates properly.
Three types of artiﬁcial neural networks are used to solve the problem of dynamic
wind speed prediction, namely the classical unidirectional artiﬁcial back propagation
network, the Elman neural network and the cascade neural network.
Two types of computer experiments were carried out to predict wind speed in
diﬀerent time intervals. The data were obtained from [14], measurements were taken
daily at 2:00, 5:00, 8:00, 11:00, 14:00, 17:00, 20:00, 23:00 at the height of 10–12 m:
1. For the ﬁrst experiment, a test sample of wind speed statistics was created for the
city of Askania-Nova for the period from January 1, 2014 to December 31, 2014
inclusive.
2. For the second experiment, four test samples of wind speed statistics were created
for the city of Askania-Nova for the periods from March 1 to March 31, 2014, from
June 1 to June 30, 2014, from September 1 to September 31, 2014, and from
December 1 to December 31, 2014.
322
V. Kravchyshyn et al.

The output reference data for both experiments were represented by the samples of
the wind speed statistics for Askania-Nova for similar periods of 2015, namely, from
March 1 to March 31, 2015, from June 1 to June 30, 2015, from September 1 to
September 30, 2015 and from December 1 to December 31, 2015.
As a result, the structure of the neural network which allows us to predict with a
relatively constant accuracy the wind speed value for the studied geographical location
was justiﬁed. The ﬂowchart of unidirectional back propagation network is given in
Fig. 1.
Fig. 1. The ﬂowchart of the unidirectional back propagation neural network
The ﬂowchart shows training neural network without hidden layers, 8 serial values
of the wind speed are fed into the input layer, and 1 new output value is obtained.
According to this scheme, the neuron uses a linear activation function to get the output
value. To train the neural network, a function that modiﬁes weights and biases according
to the elastic back propagation algorithm is used.
Fig. 2. The dynamics of wind speed variations on real-time and training samples for Askania-
Nova for the period of March 1–31, 2015 (1 - the reference values, 2 - the prediction obtained as
a result of the ﬁrst type of experiments, 3 - the prediction obtained as a result of the second type
of experiments)
Optimizing Wind Farm Structure Control
323

The results of the conducted experiments are given in Figs. 2, 3, 4 and 5:
Fig. 3. The dynamics of wind speed variations on real-time and training samples for the city of
Askania-Nova for the period of June 1–30, 2015 (1- the reference wind speed values, 2- the
prediction obtained as a result of the ﬁrst of type experiments, 3 - the prediction obtained as a
result of the second type of experiments)
Fig. 4. The dynamics of wind speed variations for the city of Askania-Nova for the period of
September 1–30, 2015 (1 – the reference wind speed values, 2 – the prediction obtained as a result
of the ﬁrst type of experiments, 3 – the prediction obtained as a result of the second type of
experiments)
324
V. Kravchyshyn et al.

Fig. 5. The dynamics of wind speed variations on real-time and training samples for the city of
Askania-Nova for the period of December 1–31, 2015 (1 – the reference wind speed value, 2 –
the prediction obtained as a result of the ﬁrst type of experiments, 3 – the prediction obtained as
a result of the second type of experiments)
3
Data Processing
The objective prediction quality assessment is an important point in the process of
prediction. Since prediction assessments are random variables, a set of statistical criteria
is used to determine it. To assess the prediction, the mean square error (MSE) can be
used. However, the MSE value is not suﬃcient to assess the prediction quality, since it
depends on the scale of the data. An in-depth assessment of the prediction quality is
achieved using criteria that give relative quality assessment (Theil’s coeﬃcient, or the
forecast inequality coeﬃcient) and relative assessment in percentage terms. The main
advantage of such assessments is the fact that they do not depend on the scale of data [15].
Theil’s coeﬃcient is deﬁned as:
U =
√
1
s
s∑
k=1
[
y(k + i) −̂y(k + i)
]2
√
1
s
s∑
i=1
y2(k + i) +
√
1
s
s∑
i=1
̂y2(k + i)
,
(1)
where S is the prediction horizon, y(k + i) is actual data values, ̂y(k + i) is prediction
assessments. Theil’s coeﬃcient is an important prediction model quality index. The
values of this coeﬃcient by deﬁnition are in the range from zero to one.
In this way prediction assessments approximate the actual values of the series and,
accordingly, the model has a high degree of adequacy. Based on the assessment it is
possible to determine the appropriateness of a model or a method for a time series
prediction.
In this research, in order to assess the prediction quality, Theil’s coeﬃcient, the mean
square error and the mean absolute deviation are calculated.
The prediction mean square error is calculated as follows:
Optimizing Wind Farm Structure Control
325

𝜛=
√
√
√
√
√
√
n∑
i=1
(
yi −y′
i
)
n
2
(2)
where yi is the reference value of the i-th element and y′
i is the predicted value of the
i-th element, n is the number of values in the sample.
The average relative prediction error is calculated as:
𝜑=
n∑
i=1
||yi −y′
i||
yi
n
∗100
(3)
The research ﬁndings are given in Table 1. The analysis of the results shows the
possibility of employing the structure of the studied neural network to predict wind speed
at any time of the year.
Table 1. The prediction results
March
June
September
December
exp1
exp2
exp1
exp2
exp1
exp2
exp1
exp2
Theil’s coeﬃcient
0,2708
0,2708
0,3299
0,3224
0,3186
0,3115
0,3276
0,3246
The mean square
error
3,3347
3,3344
2,2398
2,139
2,1549
2,0607
2,4429
2,3984
The average relative
error, %
34,6102
34,6039
34,6007
33,1866
36,8742
32,6494
30,5681
29,1537
Based on the research, the appropriateness of using unidirectional network with 8
input elements and 1 output element for wind speed prediction in the areas with high
wind power potential was justiﬁed.
To train the neural network we used the wind speed data for the study area measured
over the same period of the previous year in the ﬁrst experiment, and over the period of
one year in the second experiment.
The line charts of the Elman neural networks, the cascade neural network and the
unidirectional back propagation neural network for the short-term wind speed prediction
in the city of Drohobych, Lviv region, over the period from January 1 to January 31,
2015 inclusive are given in Fig. 6. Sixty four neurons are fed into the system’s input
layer; the result is one output wind speed value. To solve the problem of wind speed
prediction in the study area, three line charts of neural networks were designed. Each of
the line charts shows a diﬀerent type of neural networks, namely the Elman network,
the unidirectional back propagation neural network and the cascade neural network.
326
V. Kravchyshyn et al.

Fig. 6. The dynamics of wind speed on real-time and training samples for the city of Drohobych
over the period of December 1–14, 2015 (1 – the reference values of wind speed, 2 – the data
obtained by using the unidirectional back propagation neural network, 3 – the data obtained using
the Elman network, 4 – the data obtained using the cascade neural network)
The training of the neural networks was carried out and the analysis of the obtained
prediction data was conducted. The results of the wind speed prediction for the city of
Drohobych during the period from December 1 to December 14, 2015 are shown on the
charts.
The analysis of the results of wind speed prediction is presented in Table 2.
Table 2. The prediction results
Neural network structure
Back propagation Elman
Cascade
Theil’s coeﬃcient
0,043
0,042
0,0474
The mean square error
0,0559
0,0535
0,068
The average relative error, %
17,6988
18,1302
17,1876
The analysis of the ﬁndings allows us to draw conclusions on the accuracy of predic‐
tion achieved by using each of the studied types of neural networks (NN). The discrep‐
ancy coeﬃcient for each of NN types is less than 0.05%, and the average absolute error
varies in the range of 17%–18%.
The previous section explores the neural network structure which can be used for
the given wind power region to make the short-term wind speed prediction at any time.
Of course, the universality of the neural network structure increases the prediction error,
so in each particular case it may be necessary to improve the prediction accuracy by
changing neural network structure. The prediction accuracy is improved by applying
diﬀerent types of neural networks, methods (types) of NN training, diﬀerent number of
input and output signals.
The analysis of the results shows that it is appropriate to use artiﬁcial neural networks
for wind speed prediction. In addition, we have revealed the dependence of the deviation
of wind speed predicted values on the selected artiﬁcial neural network structure, the
size of training sample and the network learning function.
Optimizing Wind Farm Structure Control
327

Wind parameters prediction is essential for determining the wind farm active set, as
it allows us to revise solutions to the problems as follows:
• reducing the number of switching operations which is achieved based on the
predicted values of wind speed in conjunction with other parameters such as current
capacity of a storage battery, future load value and the number of wind turbines
available in a wind farm. As a result, we obtain the optimal set of active wind turbines
not only for the current but also the next parameters of the power-dynamic modes;
• improving wind farm overall operational eﬃciency by reducing operational losses.
As a result, we obtained advanced input data processing techniques, modiﬁcation of
dynamic programming method for determining wind farm active set, employing the
method of wind speed prediction to enhance the performance of the set of active wind
turbines, and improving basic energy parameters of a wind farm.
For the eﬀective employment of the developed method, it is necessary to form infer‐
ence rules which will ensure reduction in the number of the active wind turbines set
determinations and optimize the use of power-generating equipment.
The key parameters that should be considered include current wind speed, archival
wind speed, predicted value of wind speed, current load value, next load value, nominal
output of a wind farm, types of wind turbines, the number of wind turbines of each type,
technical condition of wind turbines, energy capacity of a storage battery, etc.
The algorithm for minimizing the number of switching operations of the active set
of WF considering the above parameters is as follows:
1. The future value is predicted based on the wind speed statistics. The input parameters
are as follows: P1 is the current load value, P2 is the next load value, 𝜐1 is the current
wind speed, 𝜐′ is the next predicted wind speed value, Q - the energy capacity of a
storage battery.
2. Given the load P1 ≥P2, and the wind speed 𝜐1 ≤𝜐′, the parameters P1 and 𝜐1 are used
as the input parameters for the method of dynamic determination of the active wind
turbines.
3. Given the load P1 ≥P2, and the wind speed 𝜐1 > 𝜐′, the parameters P1 and 𝜐′ are used
as the input parameters for the method of dynamic determination of the active wind
turbines.
4. Given the load P1 < P2, and the wind speed 𝜐1 ≤𝜐′, the parameters P2 and 𝜐1 are used
as the input parameters for the method of dynamic determination of the active wind
turbines.
5. Given the load P1 < P2, and the wind speed 𝜐1 > 𝜐′, the parameters P2 and 𝜐′ are used
as the input parameters for the method of dynamic determination of the active wind
turbines.
The use of such inference rules will reduce the number of repackings as the active
set of WF is determined taking into accounted current and next values of load and wind
speed, and the wind farm power output is to fully meet the needs of consumers for a
longer time interval. The obtained data are used as input parameters in determining the
active set of wind farm employing the method of dynamic programming and its
328
V. Kravchyshyn et al.

modiﬁcation. Obtaining the next value of the wind speed 𝜐2 consistency check is carried
out, the value is compared with 𝜐1, 𝜐′. If the wind speed value is within the range of [𝜐1, 𝜐′]
or higher than 𝜐1 and 𝜐′, any excess energy can be used to charge the storage element (in
case of incomplete charge). In other cases, it is advisable to use the storage element to
mitigate transients and reduce the number of switching operations of active wind
turbines. Therefore, to reduce the number of switching operations of a wind turbine it
is important to take into account the energy capacity of a storage battery.
The results of statistical analysis of wind speed, namely the probability of recurrence
of a particular wind speed and wind power potential can be used for substantiating wind
farm structure using energy storage element as a part of the input data in the analysis of
environmental factors that aﬀect the installation of wind farms, along with consumer
electrical load which is planned to be transferred to the wind farm in the future. They
also allow us to optimize the energy capacity of a storage element for the electrical load
curve.
On the other hand, since at present the cost of power storage elements is high, it
should be justiﬁed not only technically, but also economically. The problem of reason‐
able choice of an optimal eﬃcient and cost-eﬀective energy storage element which can
ensure balance between meeting maximum load requirements and the cost of installation
and operation is topical.
A computer simulation of wind farm control system was carried out using test data
to determine the optimal, in terms of the number of switching operations, storage element
size in percentage terms relative to the nominal output of the wind farm.
A random sample consists of 1000 load values in the interval [5000; 15000] kW.
The operating modes of the wind farm which consists of 60 wind turbines were inves‐
tigated. The key characteristics of the wind turbines are given in Table 3.
Table 3. The key characteristics of WF turbines
The type of WT
ENERCON E-48
ENERCON E-44
V52-850
The number of WT
20
20
20
Rotor power output
800 kW
900 kW
850 kW
Rotor diameter
48 m
44
52
Cut-in speed
3
3
3
Rated wind speed
17
14
14
Cut-out speed
25
25
25
Similarly, a random sample of 1000 wind speed values in the interval [5; 15] m/s
was created.
For each energy capacity of a storage battery 1000 experiments were carried out.
The actual power output of each wind turbine depends on the current wind speed
and operational characteristics of the wind turbine (the number of starts and stops, the
amount of generated power, the operating range of wind speeds, turbine’s technical
condition, operating time).
The results of computer simulation of the wind farm operation were analyzed imple‐
menting the developed modiﬁcation and the determined characteristics of the wind farm
Optimizing Wind Farm Structure Control
329

(the WT power output, the energy capacity of a storage element, the number of wind
turbines of each type).
The results of computer simulation of the wind farm control system, using the energy
storage element of diﬀerent capacities is presented in Tables 4 and 5.
Table 4. The results of using storage batteries of diﬀerent capacities in the structure of WF
(classical method of dynamic programming)
The energy capacity of
a storage battery (% of
determined WF power
output)
0
1
5
10
15
20
25
30
Maximum deviation %
−2,465
−2,405
−2,445
−2,445
−2,435
−2,435
−2,415
−2,42
The average
percentage of
underpacking/
repacking (%)
1,29
1,17
0,92
0,58
0,44
0,52
0,41
0,38
The number of
repackings
887
809
612
394
273
244
203
184
The number of
experiments
1000
Table 5. The results of using SB of diﬀerent capacities in the structure of WF (modiﬁcation of
the method of dynamic programming)
The energy capacity of
a storage battery (% of
determined WF power
output)
0
1
5
10
15
20
25
30
Maximum deviation %
−2,46
−2,45
−2,495
−2,44
−2,49
−2,125
−2,45
−2,355
The average
percentage of
underpacking/
repacking (%)
0,68
0,58
0,46
0,32
0,21
0,27
0,28
0,28
The number of
repackings
864
806
625
383
260
199
183
157
The number of
experiments
1000
Two types of experiments were carried out:
– employing the classical method of dynamic programming to solve the knapsack
problem when determining the active set of the wind farm;
– employing the developed modiﬁcation of the dynamic programming method for
determining the active set of the wind farm.
The First Type of Experiments
Figure 7 shows the results of the simulation of wind farm control system operation with
the energy storage element with diﬀerent capacities (0%, 1%, 5%, 10%, 15%, 20%, 25%,
30%) employing the classical method of dynamic programming (CDP).
330
V. Kravchyshyn et al.

Fig. 7. The dependence of the number of switching operations of the WF active set on the energy
capacity of the storage battery using the classical method of dynamic programming
Similarly, Fig. 8 shows the result of the simulation of wind farm control system
operation with the energy storage element with diﬀerent capacities (0%, 1%, 5%, 10%,
15%, 20%, 25%, 30%) implementing the modiﬁcation of the method of dynamic
programming (MDP).
Fig. 8. The dependence of the number of switching operations of the WF active set on the energy
capacity of the storage battery implementing the developed modiﬁcation of the method of dynamic
programming
The obtained results show the appropriateness of using energy storage element in
the WF structure, as it ensures signiﬁcant reduction in the number of redeterminations
of the active set of the wind farm. Figures 7 and 8 show that the use of energy storage
element (ESE) with the capacity of 10% of the nominal output of wind farm makes it
possible to reduce the number of redeterminations 2.25 times from 887 to 394 by CDP
Optimizing Wind Farm Structure Control
331

and 2.25 times from 806 to 383 by MDP. The dependence of the number of redetermi‐
nations of WF on the energy capacity of the charging battery shows their continuous
reduction with increasing SB energy capacity, however using the battery with the energy
capacity of 10% of the nominal output of WF, the percentage of the reduction in the
number of switching operations decreases gradually, increasing accordingly cost-eﬀec‐
tiveness of the application.
4
Conclusion
The paper presents the solution to the problem of wind farm structure optimization
(determination of the optimal set of active wind turbines at a given time). The importance
of wind parameters prediction for determining the active set of a wind farm is justiﬁed.
The theoretical solution to this problem enables developing algorithms for controlling
power-dynamic modes of WF in order to increase their eﬃciency by reducing opera‐
tional losses and electrodynamic overloads when switching wind turbines. The research
presents the results of using such artiﬁcial neural networks as the classical unidirectional
back propagation neural network, the Elman network and the cascade neural network.
The impact of the capacity of energy storage element on the number of switching
operations in the wind farm structure is investigated. We oﬀer the solution to the problem
of the optimal choice of the energy storage element in terms of its eﬃciency and cost-
eﬀectiveness, ensuring the balance between meeting maximum load requirements and
the cost of its installation and operation.
The algorithm (rules of logical output) for optimization of the number of switching
operations of the WF active set was developed based on such parameters as current wind
speed, archived wind speed, predicted wind speed value, current load value, next load
value, nominal output of wind farm, types of wind turbines, the number of WT of each
type, technical condition of each WT, energy capacity of a storage battery and others.
The results of the experimental studies illustrate the eﬃciency of the developed algo‐
rithm.
References
1. Kravchyshyn, V.S., Medykovskyy, M.O., Melnyk, R.V., Shunevych, O.B.: Research of
control modes of energy dynamical processes in power supply systems with a battery in
structure. Sci. J. Natl. For. Univ. Ukraine 26(7), 291–298 (2016). (in Ukrainian)
2. Kravchyshyn, V.S., Medykovskyy, M.O.: Control of wind power plant with battery. In:
Intellectual Systems for Decision Making and Problems of Computational Intelligence
ISDMCI 2016, Zaliznyy Port, Ukraine, 24–28 May 2016, pp. 83–85 (2016). (in Ukrainian)
3. Medykovskyy, M.O., Shunevych, O.B.: Modiﬁed Petri net for the structure analysis of wind
power plant, Motrol – Commission of motorization and energetic in agriculture – Lublin, vol.
14. no. 4, pp. 178–184 (2012). (in Ukrainian)
4. Medykovskyy, M.O., Shunevych, O.B.: Multicriteria method for evaluating the eﬀectiveness
of wind energy installations. J. Eng. Acad. Ukraine Kyiv 3–4, 240–245 (2010). (in Ukrainian)
332
V. Kravchyshyn et al.

5. Medykovskyy, M.O., Shunevych, O.B.: Investigation of the eﬀectiveness of methods for
determining weight coeﬃcients importance. Journal of Khmelnytsky National University:
Coll. scientiﬁctechnical. papers – Khmelnytskyi, no. 5, pp. 176-182 (2011). (in Ukrainian)
6. Medykovskyy, M.O., Shunevych, O.B.: Using integer programming for determination of
wind power. Model. Inf. Technol. Kyiv 57, 230–233 (2010). (in Ukrainian)
7. Medykovskyy, M.O., Tesluk, V.M., Shunevych, O.B.: Using the dynamic programming for
the task of equable using of wind turbines. J. “Tekhnichna Elektrodynamika” (“Technical
Electrodynamics”) 4, 135–137 (2014). (in Ukrainian)
8. Baklan, I.V., Stepankova, H.A.: Probabilistic models for analyzing and predicting time series.
In: Arts. Intelligence 2008, vol. 3, pp. 505–515 (2008). (in Ukrainian)
9. Kuchanskyy, O.Y., Biloshchytskyy, A.O.: Prediction of time series by a method of selective
comparison with the sample. Eastern Eur. J. Adv. Technol. 6(4), 13–18 (2015). (in Ukrainian)
10. Mazorchuk, M.S., Symonova, K.A., Hrekov, L.D.: Using of fuzzy logic methods and models
for modeling economic processes. Inf. Process. Syst. 9, 159–162 (2007). (in Russian)
11. Vernyhora, R.V., Yelnykova, L.O.: The possibility of using artiﬁcial neural networks when
predicting train work train directions. In: Proceedings of the Dnipropetrovsk National
University of Railway Transport named after Academician V. Lazaryan “The Problems of
the Transport Economics”, vol. 7, pp. 15–19 (2014). (in Ukrainian)
12. Kalinina, I.O.: Research of algorithms of training of neural networks in forecasting tasks,
Scientiﬁc works of Petro Mohyla Black Sea State University, Sir: Computer Technology, vol.
104, pp. 160–171 (2009). (in Ukrainian)
13. Boychuk, V.O., Novakevych, V.Y.: Modern artiﬁcial neural networks and approaches to their
modeling. Meas. Comput. Devices Technol. Process. 4, 216–219 (2014). (in Ukrainian)
14. Online resource: Weather Forecast. http://rp5.ua
15. Vartanyan, V.M., Romanenkov, Y.A., Kashcheeva, V.Y.: Estimation of the frequency
parameters of the Teyl-Weige model in short-range forecasting problems. Eastern Eur. J. Adv.
Technol. 1(5), 49–54 (2011). (in Russian)
Optimizing Wind Farm Structure Control
333

Model of the System of Personalized Analysis of Financial
Condition of the Enterprise
Melnykova Nataliia
(✉)
Lviv Polytechnic National University, S. Bandery Street, 12, Lviv 79013, Ukraine
melnykovanatalia@gmail.com
Abstract. This article reﬂects the peculiarities of valuation the ﬁnancial perform‐
ance of the enterprise, which ensure the implementation of qualitative personal‐
ized analysis, through the application of economical-mathematical methods,
methods of heuristic analysis and methods of artiﬁcial intelligence. This allowed
to predict actions to optimize ﬁnancial planning decisions. The model of a system
of personalized analysis of ﬁnancial state of the enterprise and the formation of
objective decisions concerning the ﬁnancial planning of its successful activities
was proposed.
Keywords: Comprehensive approach · Personalized data · Analysis of ﬁnancial
status · Financial planning · Personalization of data
1
Introduction
In the ﬁeld of ﬁnancial and economic activity signiﬁcantly increased the part of timely
and qualitative analysis of the ﬁnancial condition of the enterprises, the evaluation of
their liquidity, solvency and ﬁnancial durability, and ﬁnding the ways to consolidation
of ﬁnancial stability. Analysis of the enterprise ﬁnancial condition characterizes degree
of using the ﬁnancial resources and capital, obligations before the Condition, the owners,
staﬀ and other entities. In addition, it is one of the methods of adaptation to market
changeable conditions. Actuality of such investigation stipulated by high requirement
of objective evaluation of the ﬁnancial condition, which permit creation of personalized
solution to increase potential planning of enterprise ﬁnancial development.
There are number of approaches for evaluation the crisis conditions of enterprise,
each of them has own disadvantages and advantages. The current condition of person‐
alized evaluation of the enterprise ﬁnancial condition depends on large number of
parameters, which stipulate diﬃculties connected with detection the correlation struc‐
ture of these parameters. In the terms, when decisions passed on the basis of stochastic
incomplete information, the using of multidimensional statistical analysis and self-
organizing Kohonen maps is not only justiﬁed, but necessary. The problem of condition
analysis the and modelling of enterprise activity reﬂected in the works of many foreign
and domestic scholars, including I.O. Blank, N.N. Bureeva, G. Debok, T. Kohonen S.
Hayken [2, 3].
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_24

Looking the results of investigation to ﬁnancial analysis of enterprise condition,
remains unresolved question about methodology of analysis result inﬂuence to process
of enterprise ﬁnancial planning activity. At nowadays literature, scientists pay attention
to the problem of achieving eﬃciency of ﬁnancial planning mostly from positions of
management and prediction of cash ﬂows goes in according to existing theories, which
in its turn are not capable of fully, quantitatively predict future cash inﬂows. Financial
analysis and planning by specialists practically regarded as only the one eﬀect-oriented
planning system of enterprise [1, 4].
2
Formulation of the Problem
Today, any enterprise in one or another degree is constantly requires additional funding
sources. You need to ﬁnd them on the capital market, attracting potential investors and
creditors by objective informing them of your ﬁnancial and economic activity. To solve
these and many other questions allows the methodology of ﬁnancial analysis.
The process of enterprise ﬁnancial analysis provides including: eﬃciency increasing
of equity and proﬁtability of equity; attracting debt capital; evaluation of earned revenue,
working capital, ﬁnancial stability, liquidity and solvency of the enterprise; possibilities
of improving the eﬃciency of its functioning by using rational ﬁnancial policy [5, 6].
This approach to evaluation the activity of the enterprise is one of the methods of
observation and adaptation to market changeable conditions. Therefore, objective
personalized evaluation of enterprise ﬁnancial condition under the terms of diﬀerent
forms of property, acquires special signiﬁcance, as no one of the owners should not
ignore potential opportunities to increase the ﬁrm proﬁts.
3
Comparative Analysis of Existing Methods of Personalized
Approach
Theory and practice of ﬁnancial condition analysis of the business object includes
diﬀerent types of economic-mathematical methods and models, which conditionally
could be classiﬁed by groups: methods of correlation-regression analysis, methods of
mathematical programming, matrix methods and models, nonlinear models, other
economic-mathematical methods and models [6].
3.1
Methods of Correlation-Regression Analysis
Methods of correlation-regression analysis can be used for establishing the quantitative
dependence of those or other objective and subjective parameters of investigated enter‐
prise, where the nature of functional dependencies between them is uncertain. Pay
attention that the most processes which take place in the economics are random, than
the relationship between the factors that aﬀect to resulting variable is random magnitude,
at such case, the correlation expresses probabilistic dependence between variable param‐
eters of communication algorithm. Frequently, the correlation-regression analysis used
Model of the System of Personalized Analysis
335

on the stage of forming of representative statistical sample. This allows to exclude the
interdependent variables, there by decrease the dimension of table that contains the
statistics, but not reducing its signiﬁcance. As result, the researcher receives opportunity
to apply to the investigated phenomenon the most adequate model that can eﬀectively
solve the original problem, without overloading of its incoming statistical data [5, 7]
(Fig. 1).
Fig. 1. Stages of correlation-regression analysis of enterprise ﬁnancial condition
On the ﬁrst stage of correlation-regression analysis, determine indicators-factors of
correlation relationship. On the second, the communication density of eﬀective indica‐
tors with indicators-factors, which identiﬁes by appropriate coeﬃcient values of pair or
multiple correlation. The values of coeﬃcients indicate the probability of changes of
eﬀective indicator when changing the value of input variable.
The dependence of indicator from one factor in the most simpliﬁed form could be
expressed as linear dependence of type
P = k1p1 + k2,
(1)
Where P – evaluation indicator of the enterprise proﬁtability, k1, k2 – some of the
model parameters for determining the values of which is the used method of least
squares. More complex dependencies can be presented as the form of multiple linear
correlation
P = k0 + k1p1 + k2p2 + … + knpn.
(2)
The main task put before the analyst - forming the task of analytical investigation
and properly interpretation of its solution [8].
3.2
Methods of the Mathematical Programming
Methods of mathematical programming by its nature reduce to solving of conditional
optimization tasks with multiple variables. Mostly, the methods of mathematical program‐
ming used during the solving of tasks. They are by planning range and assortment of
products, determination of the optimal route, minimizing the production residues, adjusting
the level of reserves in the calendar planning of production, etc. [2]. Thus, the methods of
mathematical programming primarily designed for optimization of economic activity. It
336
M. Nataliia

allows the analyst to evaluate the degree of achievement, to determine the limited
resources, bottlenecks, the degree of competition and unprofitability.
As the mathematical methods cannot be applied directly to object under investiga‐
tion, the necessary term is to build adequate to the objective the enterprise mathematical
model. Under the mathematical model of object, phenomenon, system, means some
artiﬁcial system, physical or abstract, which simply reﬂects the structure and basic
development laws of real object by way that its investigation provides information about
the condition and behaviour of object under investigation.
Methods of the mathematical programming include methods of linear and dynamic
programming.
The analysts use the linear programming methods for numerous optimizing tasks
solving, where apply the functional dependence of the investigated phenomenon and
determined processes. Obtained results during the application of linear programming
methods enable the analyst to determine and analyse the potential possibility of value
changing of any of the parameters of object, and determine reserves of unrealized possi‐
bilities. The tasks of linear programming solve successfully by using of modern speci‐
alized software products. During the investigation of systems where the value of one or
more of the variables changes randomly using the methods of stochastic programming.
Deterministic mathematical model - the analytic representation of patterns by which
for given block of input data, on the output of the system could be obtained only one
result. Such model can show as probabilistic system then it is its simpliﬁcation and
determined system [10].
3.3
Matrix Methods and Model of Economic Analysis
Matrix methods and model of economic analysis allow to approach to analysis the most
systematically, by arranging as constituent elements of the system and the relationships
between them. Ground of those methods based on the linear and vector-matrix algebra,
which during the investigation of complex and large economic structures is often used.
The matrix models acquired widespread extension in the investigation ﬁeld of inter‐
sector balance, matrixes of multicriterion optimization, portfolio analysis, etc. In the
economic analysis model of intersector balance commonly used for calculation of tech‐
nological standards, balance analysis of the production process, the management by
resources and reserves of raw materials, etc. [6].
Matrix of multicriterion optimization used in economic analysis as method of
comparative, the rating evaluation of possible changes of parameters of the economic
system of enterprise activity to multicriterion basis. By form, it is singular matrix with
separation of given number of comparison criteria.
3.4
Method with Using the Game Theory
As part of other economic-mathematical methods and models, could be separated game
theory, the theory of mass service and the theory of fuzzy logic and neural networks.
Methods used during the solving game tasks widely used in the ﬁeld of making
management decisions on the stage of forming the alternatives and selection of the
Model of the System of Personalized Analysis
337

optimal strategy for enterprise functioning. Analytically, they could be presented as
game of two, three or more players, each of whom wants to maximize its proﬁts, mini‐
mize loss due to another, by choosing the optimal strategy [11].
3.5
Method of Operations Research
Method of operations research used in the analysis for obtaining of comparative evalu‐
ation of alternative solutions. The complexity of the method consist in that the researched
operations are open systems, therefore related to other actions, which at some point of
time are not interesting for researcher, however, aﬀect the progress of the analysis.
Running operations, analysis of factors related to speciﬁc tasks, comparison of the costs
and results. This should give the basis to analyst for separation of the most important in
the task and formulation conditions for the forming of decisions, after the selection of
the required indicators [12].
3.6
Methods of Artiﬁcial Intelligence
The methods described above for a long time were the main tools of economic-mathe‐
matical modelling, which is actively used by researchers and analysts in the process of
evaluating by them of various economic processes. However, most of them have a
number of disadvantages that are primarily associated with lots of restrictions and
conditions, leading to signiﬁcant reduction of the eﬀective solution of real tasks, which
generally do not meet requirements for the analysis of enterprise ﬁnancial condition.
Another signiﬁcant problem that occurs during using the classical methods of economic-
mathematical modelling, linked to their failure of operating by quality indicators. [17]
As result, analysts are forced to increasingly use methods of qualitative analysis of
economic systems, which eliminates the quantitative methods of mathematical model‐
ling of process of planning and optimization of their activity. Also the using of adequate
mathematical approaches for the analysis and predicting of activity development of the
economic systems allows increasing eﬃciency of their functioning and preventing addi‐
tional economic eﬀect.
So to change of the classic methods of economic-mathematical modelling become new
methods, including methods of fuzzy logic and artificial intelligence. It is methodology
and mathematical apparatus, which provides the ability to set and mathematically-
grounded solve even the problem for which unavailable full statistics or among the most
informative factors are only qualitative indicators, ensuring the possibility of adapting the
economic and mathematical models to changeable conditions in the economics [5, 13].
The model is built on the foundation of artiﬁcial intelligence is quite well-established
in solving complex tasks. Which consider the problems in the ﬁeld of economic analysis
and predicting changes in the stock indexes, during evaluation of reliability of borrower
in ﬁnancially-credit area, with deﬁned probability of enterprise bankruptcy and in the
researching of activity of the industrial and commercial enterprises by analysis of ﬁnan‐
cial and insurance risks, etc. [9, 10].
338
M. Nataliia

4
Search for Solutions for Analysing the Enterprises Financial
Condition
4.1
Evaluation of the Financial Success of the Enterprise
According to the results of the research of enterprise condition, determined that it would
be advisable to use relative dynamic indicators of ﬁnancial assets that calculated based
on the information contained in report about its ﬁnancial activity. The actual values of
the pace of change of certain dynamic eﬀectiveness indicator of enterprise activity will
show the shortcomings in its activity and previously the problem areas -bottlenecks.
Besides, proposed to analyze relative proﬁtability indicators, which values are taken into
account in coeﬃcients or in percentages.
A quick and objective evaluation of enterprise ﬁnancial success achieves by applying
various optimizing methods of analysis process taking into account contradictory inﬂu‐
ence factors.
In conditions of unstable operating environment and high innovation of organiza‐
tions development there are increasingly using methods of heuristic analysis based on
professional judgment, experience and intuition of specialists. Using these methods
allows eﬀectively carry current and strategic analysis, to give balanced evaluation of
property, ﬁnancial status of the organization and substantiate the prospects of its devel‐
opment [11, 12].
During taking into account the criteria for evaluating the enterprise, there is problem in
determining their amount for the analysis of financial condition and priority of their influ‐
ence to process of decisions optimization, because increasing of their number simultane‐
ously makes the analysis more accurate, but complicated. The possibility of using a multi-
purpose approach for solving planning tasks is to determine the optimal criterion, which is
important stage for multicriteria optimization. Consequently, the final result of decision
success depends on the correct criterion. During the choosing of criteria followed certain
principles, such as independence, coordination, principle of completeness, etc. [8].
4.2
Characteristics of the Main Factors of the Analysis of Enterprise Success
To characterize the elements of the system sets of enterprise ﬁnancial success, we take
into account the relative and absolute condition indicators of enterprise activity condi‐
tion. For this, we consider the characteristics of the system.
P = ⟨p1, … , pn⟩,
(3)
Where p1, … , pn - are elements of sets of relative ﬁnancial indicators of enterprise
(P), n - the number of indicators-factors.
In the Table 1 given elements of the set P, which are often considered to be mostly
used indicators-factors that reﬂect the results of processes: investment, liabilities,
indebtedness, asset management, capital formation, cost planning, proﬁt generation, etc.
K = ⟨k1 , … , km⟩,
(4)
Model of the System of Personalized Analysis
339

Table 1. Indicators-factors of enterprise valuation
Symbols
Indicators
p1
Current ﬁnancial investments
p2
Short-term liabilities
p3
Receivables
p4
Current assets
p5
Equity
p6
Total amount of property of the enterprise
p7
Attracted funds
p8
Prepaid expenses
p9
Net proﬁt
p10
Spending
Where k1, … , km - elements of coeﬃcients set of enterprise valuation (K), m - is the
number of evaluation coeﬃcients.
In the Table 2 given coeﬃcients that reﬂects results of processes: investing, liabili‐
ties, debt, asset management, capital formation, cost planning, proﬁt making, etc.
Table 2. The coeﬃcients of enterprise valuation
Symbols
Coeﬃcients of valuation
k1
Absolute liquidity coeﬃcient
k2
Intermediate coeﬃcient of coverage
k3
Total coverage coeﬃcient
k4
Coeﬃcient of ownership (autonomy)
k5
Coeﬃcient of borrowed funds
k6
The coeﬃcient of the correlation of borrowed and own
funds
k7
Coeﬃcient of mobility (manoeuvrability) of own funds
k8
Coeﬃcient of collateral availability of reversible assets
k9
Own reversible assets
k10
Proﬁt margin
k11
Proﬁtability indicator
S = ⟨s1, … , sl⟩,
(5)
Where s1, … , sl are elements of set of strategic decisions for enterprise activity
development (S), l - the number of solutions.
In the Table 3 given solutions which aﬀect the enterprise activity and characterize
strategic direction of changes for further development.
340
M. Nataliia

Table 3. Decisions of the development strategy
Symbols
Coeﬃcients of valuation
s1
Plan of the number and composition of employees
s2
Wage fund planning
s3
Use of the wage fund
s4
Plan for calculating the cost of the capital
s5
Plan of production and sales of products
s6
Plan for increasing productivity of work
s7
Plan for the use of material resources
H - characteristics of the enterprise determines the category of enterprise and its
sphere of activity, the signiﬁcance of which aﬀect to determination of evaluation of
enterprise activity condition.
A = ⟨a1, … , ai⟩,
(6)
Where a1, … , ai- elements of set of enterprise ﬁnancial evaluation (A), i - the number
of evaluations.
Table 4. The coeﬃcients of enterprise performance evaluation
Symbols
Coeﬃcients of valuation
a1
Non-proﬁt
a2
Unproﬁtable 
a3
Proﬁtable
a4
Cost eﬀective
a5
Liquid
a6
Illiquid
a7
Critically-liquid
a8
Not criticaly-liquid
a9
Ineﬃcient use of own funds
a10
Eﬃcient use of own funds
a11
Creditworthiness
a12
Non-creditworthy
a13
A large share of capital
a14
Insigniﬁcant share of capital
a15
Investment-dependent
a16
Independent of investments
a17
Has low opportunities for ﬁnancial maneuverability
a18
Has high opportunities for ﬁnancial maneuverability
a19
Insolvency
a20
Solvent
Model of the System of Personalized Analysis
341

In the Table 4 given possible estimates of enterprise ﬁnancial condition, which show
the problem areas of enterprise, i.e. “bottlenecks”.
Based on description of characteristics of system analysis of ﬁnancial success,
namely: sets of system input data, sets of coeﬃcients of enterprise valuation, sets of
strategic decisions to enterprise ﬁnancial success, sets of evaluations of enterprises
ﬁnancial success and enterprise characteristics can form set of product rules.
4.3
Financial Model of the Investigated Enterprise
For objective evaluation of enterprise it is necessary to construct its formal ﬁnancial
model, that allow to present it as some physical artiﬁcial system of enterprise success,
which simpliﬁes its ﬁnancial structure and provides information about its condition and
behaviour.
So the formal ﬁnancial model of enterprise presented as model of product expert
system, which is usually used to solve such class of tasks.
The knowledge base in accordance to structural scheme of system of personalized
evaluation is to select certain set of rules R [14]:
R = {R1, … , Rj},
(7)
Where products are
Rj = pj1
⋀
pj2
⋀
…
⋀
pjn →km
(8)
And the ﬁnite set of enterprise parameters P:
R1:Ψ →K
(9)
Where
Ψ = Ψ(pn
), pn ∈P, Ψ = P ∩H,
(10)
Ψ- set of time-independent characteristics (H) and time-independent parameters of
enterprise (P). An example of time-independent characteristic is: ownership, type of
economic activity, enterprise category, etc.
An example of rules is searching for strategic decisions and ﬁnancial policies of
enterprise based on selected characteristics and parameters.
The system of enterprise’s success, which provides search for evaluation of enter‐
prise ﬁnancial condition and search for business optimization solutions, is presented as:
Fe = ⟨P, K, H, A, S, G, R⟩,
(11)
Where Fe - system of enterprise’s success, P - set of system input data that charac‐
terize relative ﬁnancial indicators-factors of enterprise derived from dynamics of ﬁnan‐
cial assets that calculated on the basis of the information contained in the report about
its ﬁnancial activities, K - set of enterprise evaluation coeﬃcients, that shows to changes
342
M. Nataliia

of eﬀective indicator during changing the value of input variable, S - set of strategic
decisions to enterprise ﬁnancial success, R - product rules of decisions to strategy of
enterprise development, A - set of evaluations of enterprises ﬁnancial success, which
depends on the evaluation coeﬃcients, H - enterprise characteristics, G - indicator of
enterprise ﬁnancial policy [15].
At the same time, the relationship between the entities of system can be represented
as the binary relation, the partial example, Table 5.
Table 5. The partial example of binary relation of determining the evaluation coeﬃcients.
P
Indicators
K
Coeﬃcients
p1
Current ﬁnancial investments
k1
Absolute liquidity coeﬃcient
p2
Short-term liabilities
k1
Absolute liquidity coeﬃcient
p2
Short-term liabilities
k2
Intermediate coeﬃcient of coverage
p2
Short-term liabilities
k3
Total coverage coeﬃcient
p3
Receivables
k2
Intermediate coeﬃcient of coverage
p4
Current assets
k3
Total coverage coeﬃcient
p5
Equity
k4
Coeﬃcient of ownership (autonomy)
p5
Equity
k6
The coeﬃcient of the ratio of
borrowed and own funds
The value of parameter-factors determined by coeﬃcients of evaluation of ﬁnancial
success.
The values of the obtained coeﬃcients determine the result of evaluation of enter‐
prise ﬁnancial condition.
∃kK(k) →∃aA(a).
(12)
The results of the evaluation are determined by evaluation of condition, ﬁnancial
policy, enterprise characteristics and decisions to the actual directions of enterprise
activity improvement.
They can be presented as form of product rule to development strategy, as n -relation:
R2 = H × A × S × G,
(13)
or in the form of four:
(h, a, s, g) ∈R2.
(14)
The received rules determine the decision to review and change the planning of the
number and composition of employees, the wage fund, the calculation of capital cost,
production and sales of products, increasing of work productivity, using of material
resources and wage fund [16].
Model of the System of Personalized Analysis
343

5
Conclusions
Consequently, during the process of ﬁnancial analysis of enterprise condition used
diﬀerent techniques and methods of decisions optimization. We can assume that due to
taking into account the individual performance indicators of the enterprise, its ﬁnancial
success and various factors of inﬂuence, the use of diﬀerent optimization methods is
foreseen for analyze ﬁnancial success enterprise.
For diﬀerently tasks, appropriate to use:
• the multipurpose approach is expedient to use for complex analysis of enterprise
condition;
• the heuristic methods are eﬀectiveness in unstable environment of functioning and
high innovation of development;
• new methods, such as methods of fuzzy logic and artiﬁcial intelligence become for
determination of optimal solutions in conditions of unstable environment and solving
complex problems in the ﬁeld of economic analysis and forecasting modeling.
The proposed model of system of personalized analysis of enterprise success allows
to provide processes for ﬁnding objective and qualitative strategic decisions regarding
the determination of non-proﬁtability, proﬁtability, liquidity, eﬃciency of using own
funds, credit potential, dependence on investments, solvency of the enterprise and
ﬁnding opportunities for ﬁnancial maneuvers.
Thus, such approach to objective personalized evaluation of enterprise ﬁnancial
condition, subject to various forms of ownership and activities, becomes of particular
importance, since no owner abandons the potential opportunities to increase proﬁts and
develop his business.
References
1. Zavgorodnij, V.P.: Automatization of Business Accounting, Management, Analysis and
Audit, 768 p. A.S.K., Kyiv (1998)
2. Computerization of enterprise main stages and leading of control-auditing work – pledge of
its eﬃciency increasing. In: Financial Management, no. 1, pp. 53–56 (2002)
3. Yakovenko, S.I.: Re-engineering of business processes by way of management
informatization at the enterprises of Ukraine. Actual Probl. Econ. 9(39), 118–130 (2004)
4. Melnykova, N., Marikutsa, U., Slych, A.: The intelligent system architecture of personalized
management. In: XXIV Ukrainian-Polish Conference on CAD in Machinery Design.
Implementation and Educational Issues – CADMD 2016, Lviv, 21 October–22 October 2016,
pp. 27–28 (2016)
5. Abdikeev, N.M., Tyhomyrova, N.P.: Designing of Intellectual Systems at Economics. Edition
«Ekzamen», 528 p. (2004). Textbook/Under red
6. Mark, D.A.: Clement McGoen. Methodology of structure analysis and designing of SADT
(Structured Analysis & Design Technique). http://www.interface.ru/fset.asp?Url=/case/
sadt0.htm
344
M. Nataliia

7. Lytvyn, V., Shakhovska, N., Melnyk, A., Ryshkovets, Y.: Designing intelligent agents based
on adaptive ontologies. In: Materials of VI International Scientiﬁc Conference Intelligent
Decision Support Systems and Computational Intelligence Problems, Yevpatoria, 17–21 May
2010, vol. 2, pp. 401–404. KNTU, Kherson (2010)
8. Dunham, M.: Data Mining Introductory and Advanced Topics. Pearson Education Inc., Upper
Saddle River (2003)
9. Shakhovska, N., Cherna, T.: The method of automatic summarization from diﬀerent sources.
Econtechmod. Int. Q. J. 5(1), 103–109 (2016)
10. Shakhovska, N., Veres, O., Hirnyak, M.: Generalized formal model of big data. Econtechmod.
Int. Q. J. 5(2), 33–38 (2016)
11. Vinogradov, O.A.: Application information technology in providing marketing innovation.
Actual Probl. Econ. 10(52), 45–52 (2005)
12. Blundel, G.L.: Eﬀective Business Communications: Principles and Practices in the Era of
Informatization. Peter, St. Petersburg (2000)
13. Titorenko, G.A., Makarova, G.L., Daiitbegov, D.M., et al.: Information technologies in
marketing, 335 p. (2000)
14. Kalyanov, G.N.: Case-Technology. Consulting for Automation of Business Processes, 15th
edn., 320 p. Hot line - Telecom (2002)
15. Chernorutsky, I.G.: Methods of Decision Making, 416 p. BHV, St. Petersburg (2005)
16. Larichev, O.I.: Theory and Methods of Decision Making, and Chronicle of Events in Magical
Countries, 296 p. Logos, Moscow (2000)
Model of the System of Personalized Analysis
345

Hybrid Sorting-Out Algorithm COMBI-GA
with Evolutionary Growth of Model
Complexity
Olha Moroz(&) and Volodymyr Stepashko
Department for Information Technologies of Inductive Modelling,
International Research and Training Centre for Information Technologies
and Systems of the NASU and MESU, Glushkov Avenue 40,
Kyiv 03680, Ukraine
olhahryhmoroz@gmail.com, stepashko@irtc.org.ua
Abstract. Paper presents latest achievements in the development of the
sorting-out hybrid COMBI-GA algorithm based on a new mechanism of evo-
lutionary growth of models complexity to ﬁnd the optimal model structure. The
mechanism uses generation of model structures of GA initial population by
binomial random number generator with low probability and speciﬁc mutation
operator with adding some units in model structures. The effectiveness of this
mechanism is compared with two another mechanisms of evolutionary simpli-
ﬁcation (using binomial random number generator with big probability to form
the GA initial population and mutation operator with reducing amount of units
in model structures) and conventional random changing the complexity (using
binomial random number generator with average probability of forming the GA
initial population and mutation operator where gene values in a chromosome are
inverted according to a given probability). The presented experimental results
demonstrate that this new algorithm with evolutionary complication of model
structures performs quickly, accurately and reliably when solving both artiﬁcial
and real inductive modelling tasks.
Keywords: GMDH  Sorting-out algorithm  Combinatorial algorithm COMBI
Genetic algorithm GA  COMBI-GA algorithm  Model complication
Model structure generator
1
Introduction
GMDH algorithms [1] belong to the most effective means of solving inductive mod-
elling problems like structural and parametric identiﬁcation, forecasting, building
models of objects and processes from statistical or experimental data samples under
uncertainty conditions. They are based on the principle of model’s self-organization
[1]: with a gradual increase of the model structure complexity, the value of an external
criterion decreases, reaches a minimum, and then becomes unchanged or begins to
increase. The smallest value of the criterion deﬁnes a unique model of optimal com-
plexity. Here “model complexity” means the number of included variables in a linear
model structure or the number of nonlinear model parameters.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_25

GMDH algorithms may be divided into two main groups: sorting-out and iterative
ones. The main difference between them consists in using, respectively, search-type
and iteration-type generators of different model structures [2].
The very ﬁrst sorting-out algorithm COMBI [2] is practically successful only in
solving tasks with the number of input variables (arguments) less than 30 because of
using exhaustive search of models of different structures in a given basic class.
Sorting-out GMDH algorithms with incomplete (directed) model search such as
MULTI [3], BSS and FSS [4, 5], use speciﬁc ways of optimal structure search and
demonstrate polynomial computational complexity. They are able to signiﬁcantly
reduce the time and amount of required resources to ﬁnd the optimal model as com-
pared to the conventional COMBI algorithm and to solve problems with hundreds of
arguments. However, these algorithms have quite complex search schemes to ﬁnd the
optimal result and require signiﬁcant efforts for their implementation.
The very ﬁrst and the most known iterative-type GMDH algorithm of neural net-
work type called now Multilayered Iterative Algorithm MIA GMDH was created in
1968 by Prof. A.G. Ivakhnenko [1]. But it has such main drawbacks as high probability
of loss of relevant arguments and retention of irrelevant ones and it was the reason to
construct new kinds of the iterative GMDH algorithms [1, 6]. For more than ten last
years, several researchers actively develop hybrid structures of MIA and genetic
algorithms to increase the productivity and overcome these shortcomings [7].
Genetic algorithms belong to evolutionary methods being a powerful tool to ﬁnd
global solution of optimization problems. The GAs have quite clear structure and fairly
obvious realization. However, the use of GA in each task faces major difﬁculties:
deﬁning the method of coding solutions that form chromosome of an initial population,
deﬁning a size of the initial population and the number of selected individuals to apply
genetic operators, selection of appropriate operators (crossover, mutation, etc.) and
their probability, choosing stop criterion of the algorithm. These difﬁculties signiﬁ-
cantly affect the performance of any GA-based algorithm.
This paper considers only GMDH algorithms of sorting-out type which can ﬁnd
optimal model during small time. To enhance the productivity and accuracy of such
algorithms we apply the idea of COMBI and GA hybridization. The ﬁrst variant of a
COMBI-GA algorithm was studied in [8] and demonstrated very promising results.
The aim of this paper is to consider the effectiveness of a new modiﬁcation of the
hybrid COMBI-GA algorithm based on the usage of binomial random number gen-
erator [9] with small, average and high probability to form GA initial population.
Together with this, a new special mutation operator is examined allowing: (a) to
evolutionary increase the model complexity; (b) to randomly change the complexity;
and (c) to only evolutionary simplify the complexity.
The paper structure is as follows. Section 1 explains existing sorting-out algorithms
and place of COMBI-GA among them. Section 2 describes the basic principles of GA
and using genetic operators. Section 3 introduces a newly modiﬁed hybrid COMBI-GA
algorithm. Section 4 explains approaches to the algorithm research. Sections 5 and 6
present the results of its investigation in artiﬁcial tasks and application to solving a
real-world task respectively.
Hybrid Sorting-Out Algorithm COMBI-GA
347

2
Sorting-Out GMDH Algorithms
The article [10] presents a survey of the available techniques of exhaustive and partial
search of models in GMDH algorithms and their comparative analysis. It considers also
the analysis and classiﬁcation of models structure generators in most known sorting-out
GMDH algorithms.
The greatest effect on the speed and amount of calculations in this kind of algo-
rithms has the choice of the model structures generator, i.e. the mechanism of models
sorting. The disadvantage of different types of searching algorithms in comparison with
the combinatorial one is the risk of omitting the model of optimal complexity. But this
is balanced by their signiﬁcant advantages in speed and possibility to ﬁnd a model close
to the optimal one by the value of a given criterion.
Sorting-out GMDH algorithms enable to effectively solve actual problems of
artiﬁcial intelligence, system identiﬁcation and process prediction. They help to reduce
a subjective inﬂuence of a user on the results of modeling. The main feature of
sorting-out GMDH algorithms is that they search for the model of optimal structure
minimizing error of forecasting in a ﬁnite set of models with different structures.
A classiﬁcation of majority of known sorting-out GMDH algorithms can be pre-
sented by the diagram showed on Fig. 1 ﬁrstly published in [10].
Fig. 1. Classiﬁcation diagram of known GMDH algorithms of sorting-out type.
348
O. Moroz and V. Stepashko

The diagram shows that most of structure generators in sorting-out algorithms are
of searching type. Such sorting-out generators eliminate the main drawback of the
combinatorial algorithm, namely exhaustive search of all possible models. Due to that
they are able to solve high dimension problems, have much greater speed of calcula-
tions and require less computing resources.
The ﬁrst algorithm based on the directed sequential search of the optimal model
(the result of the exhaustive search by COMBI) was the «combinatorial-selective
multistage algorithm» MULTI [3]. It implements a step-by-step procedure of forming
model structures of incrementally increasing complexity of partial models and a
recurrent method of parameter estimation. Generated models remain in the original
basis and the number of their arguments corresponds to the current stage number.
Papers [4, 5] illustrate the effectiveness of sorting-out COMBI-based algorithms
with successive selection of informative arguments and/or elimination of spurious ones
in three cases: (1) passage from simple models to complex ones (Forward Successive
Selection, FSS); (2) passage from complex to simple models (Backward Successive
Selection, BSS); (3) combining of the two approaches (Combined Successive Selec-
tion, CSS). In the second case the result proved to be the best.
Another possible way to enhance the performance of sorting-out algorithms is
constructing hybrid structures of COMBI with some computational intelligence algo-
rithms. Our previous papers [8, 11, 12] showed that hybridization between COMBI and
genetic algorithm (GA) is relevant and promising for solving various high-dimension
modeling tasks with hundreds of input variables. The GA initial population was gen-
erated previously by the uniform random number generator. Here we show more
efﬁcient results by using the binomial random number generator.
3
Genetic Algorithm and Its Operators for Generating
Offspring
The genetic algorithm is one of the meta-heuristic procedures of global optimization
constructed as a result of generalization and simulation in artiﬁcial systems of such
properties of living nature as natural selection, adaptability to changing environmental
conditions, inheritance by offspring of vital properties from parents. The basic prin-
ciples of GA were formulated by John Holland [13]. Formally, GA can be represented
in such a way:
GA ¼ fP0; M; L; F; G; sg;
where P0 ¼ ða0
1; . . .; a0
MÞ is an initial population; a0
i is an individual of this population
treated as a candidate for the solution of the optimization problem presented in the form
of a chromosome; M is the population size (integer number); L is the length of each
chromosome of the population (integer number); F is a ﬁtness function (FF) of an
individual; G is a set of genetic operators; s is the algorithm stopping rule.
Chromosomes consist of genes that are usually encoded as values from the binary
alphabet {0, 1}, but other codes – alphabetic, decimal, etc., can be used too. In this
paper, only binary coding of chromosomes is considered.
Hybrid Sorting-Out Algorithm COMBI-GA
349

As input data for any GA initial population P0, a ﬁnite set of chromosomes (ele-
ments, individuals, specimens, etc.) is used each of which represents a potential
solution of the problem. Then the ﬁrst population of offspring P1 is formed from the
parent chromosomes P0 using some genetic operators, similarly the next population P2
is formed from the population P1 and so on. The process continues until the speciﬁed
stopping rule of the algorithm will be satisﬁed. An important feature of the GA work is
that with each step the mean FF value of the current population decreases and con-
verges to the solution of the optimization problem.
The central role in the GA is played by two types of operators for creating new
individuals (offspring) from the available ones in the current population, which can be
divided into two main groups. Operators of the ﬁrst type need two individuals (parents)
to create two offspring and rules for the exchange of genetic material between them.
Operators of the second type form one offspring from one individual, modifying it
according to certain rules. Operators of the ﬁrst type are for example crossover,
translocation, segregation, etc.; the second type operators include mutation, inversion,
transposition and others [13]. The most important among them are crossover (OC) and
mutation (OM) operators implemented not in all cases of the offspring generation
process but with a given probability which is close to 1 for crossover operators and to 0
for most mutation operators. In general, there are no strict limitations in the using of
different models of offspring creation because in solving optimization problems it is not
necessary to completely copy the laws of nature and to conﬁne oneself only to them. It
is more expedient to use the common sense and take into account the speciﬁcity of a
problem as much as possible.
In what follows, there is a brief description of the operators that were investigated.
One-point crossover operator (opCO) [13]. Two chromosomes of the population are
randomly selected, then a crossover point lk 2 [1, L – 1] is randomly generated where
L is the chromosome length. As a result of crossover of the pair of parental chromo-
somes relative to lk we get a pair of offspring: the ﬁrst one is a chromosome which in
positions from 1 to lk contains the genes of the ﬁrst parent, and at the positions from
lk + 1 to L is genes of the second parent; the second one is a chromosome which,
respectively, in positions from 1 to lk contains the genes of the second parent, and in
positions from lk + 1 to L genes of the ﬁrst parent.
Uniform crossover operator (ufCO) [13]. One of the existing descriptions of this kind
of OC is the following. A mask of crossover is randomly formed as a string of zeros
and units with the length equal to that of the chromosome. The mask indicates which
genes should be inherited from the ﬁrst parent and which from the second one, while
the two new offspring are formed according to such rule: unit in ith position of mask
means that ith element of the ﬁrst parent should be located in ith place of the ﬁrst
offspring, and zero means that ith the element of the second parent should be in ith place
of the ﬁrst offspring. Considering the ﬁrst parent as the second, and the second as the
ﬁrst one, the second offspring is obtained according to this speciﬁed rule. The mask of
crossover can be one and the same for all parents or a new for each pair.
Universal crossover operator (uvCO) [13]. Similarly to the uniform crossover, a binary
mask is formed with the length equal to that of the given chromosomes. The ﬁrst
350
O. Moroz and V. Stepashko

offspring is formed by a bitwise addition of the chromosome elements of the ﬁrst parent
and mask according to the following rules: 0 + 0 = 0, 0 + 1 = 1, 1 + 1 = 0. To obtain
the second child, they act in the same way with the second parent.
Ring crossover operator (rgCO) [14]. In this crossover, the parent chromosomes are
joined by the beginnings and ends forming a ring, and then a random point of its
rupture is generated. The straight line that passes through this point and the center of
the ring divides it into two parts. The ﬁrst descendant is formed clockwise from the
crossover point, and the second is against it.
Here we consider three gene mutation operators: M1, when according to a given
probability, e.g. 0.2, only zero gene values in a chromosome are inverted [11]; M2 is
standard gene mutation [13] of an individual with a given inversion probability of
several bits of a chromosome according to; M3, when, on the contrary, only unit gene
values in a chromosome are inverted with a given probability.
4
Hybrid COMBI-GA Algorithm
A general deﬁnition of the inductive modelling problem may be done as follows. Let us
given: a data set of n observations after m inputs x1, x2, … , xm and one output
y variables. The GMDH task is to ﬁnd a model y = f(x1, x2, … , xm, h) with minimum
value of a given model quality criterion C(f), where h is unknown vector of model
parameters. The optimal model is deﬁned as f* = argminUC(f), where U is a set of
models of various complexity, f 2 U. This problem consists of the discrete optimiza-
tion task for ﬁnding the best model and a continuous optimization task for estimation of
parameters.
The two tasks are generally solved by any of the sorting-out algorithms, including
COMBI, in the following steps:
(1) transformation of the initial data according to the chosen system of base functions,
typically polynomial;
(2) generation of the complete set of all possible structures of partial models or some
subset of them in the chosen basis;
(3) use the least squares method (LSM) to estimate coefﬁcients of a partial model;
(4) calculation of a given external selection criterion for each model;
(5) successive selection of best models by this criterion and choosing the model of
optimal complexity with the minimum criterion value.
The hybrid COMBI-GA algorithm uses generally the same procedure, but steps 2
and 5 are modiﬁed by applying GA (see below in more details).
To avoid the exhaustive search typical for COMBI, the developed COMBI-GA
algorithm forms gradually a set of the most promising structures of partial models and ﬁnds
the optimal one using genetic operators of selection, crossover and mutation that estab-
lishes a speciﬁc sorting-out mechanism. Formally, the algorithm can be deﬁned as follows:
COMBI-GA ¼ Z; y; f; X; D; CR; P0; H; M; G; k; F
h
i;
Hybrid Sorting-Out Algorithm COMBI-GA
351

where Z[n  r] is the measurement matrix of input variables of an object, r and n are
numbers of inputs and measurements respectively; y[n  1] is vector of measurements
of an output variable;
f[m  1] is vector of a given m base functions of input variables;
X[n  m] is the measurement matrix of base set of arguments;
D is a given rule of dividing matrix X[n  m] and vector y[n  1] to testing A and
checking B parts;
CR is an external selection criterion (as ﬁtness function) based on dividing the
sample (X, y);
P0 is a set of model structures of GA initial population consisting of binary chro-
mosomes (encoded structure of partial models);
H is size of initial population of models, H < m;
M is size of any next population, M > H;
G is set of genetic operators;
k is stopping rule of GA;
F is number of best partial models (freedom of choice) monitored during all iter-
ations of the algorithm, 1 < F  H.
This algorithm consists of the following steps:
Step 1. Calculating the matrix of the base set of arguments X[n  m] using the
input matrix Z and the vector of base functions f and dividing it and the output vector
of measurements y[n  1] according to the rule D in testing XA[nA  m] and checking
XB[nB  m] submatrices (nA + nB = n). Obviously, in the case of linear polynomial,
matrices X and Z are identical (m = r).
Step 2. Randomly generating the initial population P0 of the genetic algorithm.
Step 3. Calculating the coefﬁcients of each partial model by LSM or another
method using the training matrix of base arguments XA and output vector yA.
Step 4. Calculating the value of an external criterion CR (as the GA ﬁtness function)
for each partial model using the checking matrix XB and output yB.
Step 5. Forming the current population of partial models (chromosomes) of the size
H with better criterion values to form the next offspring. In addition, selection the best
F partial models that are potential solutions of the task of model building.
Step 6. Forming new population of M individuals applying genetic operators of
crossover and mutation to individuals of the current population.
Step 7. Checking a given GA stopping rule. If it is satisﬁed, then go to step 8,
otherwise go to step 3.
Step 8. Choosing F best models from the current population of the size H.
Step 9. The end.
5
Main Ideas of Numerical Experiments
The purpose of these experiments is to determine the effectiveness of the hybrid
COMBI-GA algorithm depending on the methods used to generate offspring of selected
elite individuals by genetic operators.
352
O. Moroz and V. Stepashko

In our numerical experiments, individuals (model structures) of GA initial popu-
lation are encoded as binary structural vectors with elements 0 and 1 indicating absence
or presence of a particular argument in the model respectively. As a ﬁtness function of
each individual we use standard GMDH external regularity criterion [1] based on
dividing the data sample into two subsamples, training and checking.
The stop criterion of the hybrid algorithm is achieving given accuracy for the
difference between minimum values of the ﬁtness function of two adjacent iterations.
In this paper, we consider main obtained results for such two variants:
Variant 1. The use of four crossover operators (uvCO, opCO, rCO, ufCO) in the GA
along with the 3-point mutation operator (3pMO) which performs most efﬁciently in
the COMBI-GA algorithm [11].
The research was carried out in [8] for tasks with three types of test data without
and with noise under such general conditions: the linear models class; the GMDH
regularity criterion is the GA ﬁtness function; the size of the initial population is 100
individuals; the probability of using a crossover operator is 0.9, the probability of
3pMO mutation is 0.2; the GA stopping rule is achieving the absolute value of the
difference between minimum values of the ﬁtness function of the previous and current
populations less than 10−7; the sample of the input data is divided into the training,
checking and examination subsamples at the ratio 0.5: 0.3: 0.2.
Variant 2. According to the results of crossover comparison in [12], we use the best
one-point crossover operator to form the offspring population from any pair of selected
elite individuals. To produce a new offspring by the one-point crossover, a chromo-
some point (between neighboring bits in a row vector) is randomly selected, the
fragment of binary string from the beginning of the ﬁrst parent chromosome to the
point is copied, and the rest (tale) is copied from the second parent.
Also we consider the following three types of mutation operators: M1 is the speciﬁc
gene mutation [7] where, according to a given probability, only zero gene values in a
chromosome are inverted; M2 is standard gene mutation [8] of an individual with the
inversion of several bits of a chromosome according to a given probability, e.g. 0.2;
M3, on the contrary to M1, only unit gene values in a chromosome are inverted
according to a given probability.
This research variant was carried out on test tasks. Two data samples with 20 and
1000 input arguments were artiﬁcially generated and half of them were used in both
cases as relevant ones. Both relevant and irrelevant arguments for the initial data
sample are randomly generated. A test “true” linear model with random parameters is
calculated using only relevant arguments.
6
Results of the Computational Experiments
Variant 1. The case of non-noisy data. Here we considered three problems with the
number of input variables 20, 50, and 200 arguments.
In each case, approximately half of elements of the randomly generated input
variable vector are true arguments. Consequently, they are taken into account when
calculating the output variable values by the preassigned formula.
Hybrid Sorting-Out Algorithm COMBI-GA
353

For example, for the data set with 20 input arguments, the model speciﬁed for the
experiments contained 8 informative arguments in the linear dependence [8]:
y x
ð Þ ¼ 0:5x2 þ 0:2x4 þ 2:4x5  0:6x9 þ 0:3x13 þ 2:1x15 þ 2x17 þ 0:8x19:
The matrix of arguments X and the model parameters were obtained using a
generator of uniformly distributed random numbers, the values of the arguments was in
the range from 0 to 20, and the values of the parameters ranging from –1 to 6. The
inﬂuence of genetic operators on the efﬁciency of the hybrid algorithm was investigated
with taking into account the completeness of restoring the true (exact) structure of the
model and the speed of the model constructing process.
The results of the research are given in Table 1 and show that the COMBI-GA
works most effectively with opCO which gives a fairly rapid decrease in the minimum
value of the ﬁtness function of each GA population that indicates the direction of the
search for the optimal model. The graphs of COMBI-GA convergence depending on
some genetic operators are shown in Fig. 2.
Table 1. The COMBI-GA efﬁciency dependent on a used GA operator (not-noisy data).
Number of
input variables
Time to ﬁnd optimal model, c
COMBI COMBI-GA
opCO uvCO
rCO
ufCO
20
84.9
0.08
0.084
0.09
0.081
50
–
1.36
4.7
4.4
2.2
200
–
74.15
100.5
80.63 84.1
Fig. 2. Convergence of the COMBI-GA using operators (opCO + 3pMO) (☆) and (rgCO +
3pMO) (Δ).
354
O. Moroz and V. Stepashko

The case of noisy data. For the case with 20 input arguments, the test model was the
same as in the example without noise.
With 20% noise, the COMBI and COMBI-GA algorithms with ufCO found,
respectively, the following optimal models of the same structure (except a few extra
uninformative arguments with small coefﬁcients):
y1 x
ð Þ ¼ 0:5x2 þ 0:26x4 þ 2:39x5 þ 0:01x7  0:61x9 þ 0:03x10  0:01x12
þ 0:3x13 þ 2:08x15 þ 1:99x17 þ 0:86x19
y2 x
ð Þ ¼ 0:01x1 þ 0:59x2 þ 0:16x4 þ 2:38x5  0:02x7  0:63x9 þ 0:04x12
þ 0:39x13  0:05x14 þ 2:14x15 þ 2x17 þ 0:74x19
Using other genetic operators, the COMBI-GA algorithm worked less efﬁciently.
In the case of 50% noisy data, COMBI built such optimal model:
y x
ð Þ ¼ 0:49x2 þ 0:35x4 þ 2:37x5 þ 0:02x7  0:62x9  0:08x10  0:02x12
þ 0:19x13 þ 0:11x14 þ 2:06x15 þ 1:97x17 þ 0.01x18 þ 0:94x19
The COMBI-GA with opCO worked most efﬁciently and found the optimal model:
y x
ð Þ ¼ 0:48x2 þ 0:211x4 þ 2:312x5
 0:63x9 þ 0:23x13 þ 2:14x15 þ 1:48x17 þ 0:867x19;
containing all the true arguments without presence of extra ones.
For the data set with 50 input arguments (25 relevant), the COMBI-GA was most
effective when using opCO. The optimal model contained 25 arguments with 20% and
24 with 50% of noise. Table 2 presents the COMBI-GA effectiveness with different
genetic operators. Figure 3 shows the convergence of COMBI-GA with opCO.
Table 2. The COMBI-GA efﬁciency dependent on a used GA operator (noisy data).
Number of
input variables
Time to ﬁnd optimal model, c
COMBI
20% of noise
50% of noise
COMBI-GA
COMBI-GA
oCO
rCO
oCO
rCO
20
85.2
0.158
0.17
0.185
0.2
50
–
0.51
0.7
0.53
0.8
Hybrid Sorting-Out Algorithm COMBI-GA
355

Variant 2. Unlike to the above COMBI-GA research, in this study the initial popu-
lation of model structures is generated by the binomial distribution with low, average,
and high probabilities, and the three mentioned above new versions of mutations are
used with these three generation variants of initial population. As a result, we have
three cases of experiments with corresponding mutation operators: Case 1 (0.1 + M1),
Case 2 (0.5 + M2), Case 3 (0.95 + M3). Level of noise is 20% in all three cases. The
Case 3 may be interpreted as an evolutionary simpliﬁcation of models instead of
complication as in the Case 1. The generation of model structures in the Case 2 is
random (irregular).
The optimal model is searched using COMBI-GA in the class of linear functions of
all the given arguments, 20 and 1000 respectively. In the case of 20 arguments we can
compare the results of modelling by COMBI and COMBI-GA.
As a result of hundred experiments it was found that for 20 input variables the
COMBI-GA algorithm works more reliably and accurately in Case 1 unlike to Case 2
and Case 3. The algorithm with models evolutionary simpliﬁcation (Case 3) performs
slowest due to solving high dimension linear systems to estimate model parameters.
Fig. 3. Convergence
of
COMBI-GA,
50
arguments
and
20%
noise,
using
operators
(opCO + 3pMO).
Fig. 4. Changes of average model complex-
ity dependent on number of iterations for three
variants of binomial distribution probability.
Fig. 5. Changes of average value of ﬁtness
function dependent on average models com-
plexity of current population.
356
O. Moroz and V. Stepashko

Gradual changes in the average model complexity of current population, charac-
terizing performance of COMBI-GA in all three cases, are presented on Fig. 4.
Figure 5 conﬁrms (simulates) the principle of models self-organization: with a
successive increasing of the average complexity of model structures of the current
population, the average value of the criterion decreases and reaches the minimum.
Optimal model structures obtained using COMBI and COMBI-GA algorithms are
the same and consist of 15 arguments including all 10 relevant ones in view of the
noise effect, 5 spurious arguments have very small coefﬁcients that illustrate adequate
work of COMBI-GA. The developed COMBI-GA algorithm has found the optimal
model during 0.3 s. whereas original COMBI for 85.2 s.
Figure 6 shows perfect capability for solving high-dimension problems in Case 1.
The COMBI-GA algorithm solves the task with 1000 input variables during 17 s.
according to given accuracy. In the other two cases algorithm signiﬁcantly loses the
accuracy and convergence.
Figure 7 shows that complexity of the COMBI-GA algorithm has not exponential
character in contrast to the COMBI algorithm, forms S-shaped curve typical for natural
processes and illustrate slower increasing the time to ﬁnd the optimal model depending
on the number of input arguments.
7
Real-World Data Example
Experiments with the use of COMBI-GA with opCO in the modiﬁcation based on the
Variant 2 were carried out for solving applied problem based on real-world dataset
taken from UCI Machine Learning Repository [15].
Fig. 6. COMBI-GA convergence for 1000
input variables using models evolutionary
complication, noise level 20%.
Fig. 7. Time to ﬁnd the optimal model, s.
Hybrid Sorting-Out Algorithm COMBI-GA
357

Problem of modelling player skills levels. Most studies in the area of cognitive science
investigate expertise by comparing experts with novices. In real-time strategy
(RTS) games, players develop game pieces called units with the ultimate goal to
destroy their opponent’s headquarters. RTS play can be considered as an area of
expertise; playing well requires a great deal of strategy and knowledge, and these
require a great deal of experience.
Data of the analysis of video game telemetry from RTS games are used here to
explore the development of expertise. The task deals with two types of variables:
• Hotkey usage variables. Players can customize the interface to select and control
their units or building more rapidly, thus ofﬂoading some aspects of manually
clicking on speciﬁc units to the game interface.
• Perception-Action-Cycle variables (PAC). Each variable pertains to a period of time
where players are ﬁxating and acting at a particular location. Many of these vari-
ables will therefore reﬂect both attentional and perceptual processes, and
cognitive-motor speed.
There are 18 input variables (both integer and continuous) in this task [16] and the
output value as LeagueIndex is coded 1 to 7 of player skills levels (Ordinal): Bronze,
Silver, Gold, Platinum, Master, GrandMaster, and Professional leagues. To build model
of the skills level, we use only a subset of 55 observations from the complete
3000-instances set.
Optimal model structure built by COMBI algorithm contains only 6 signiﬁcant
arguments out of all 18: x1, x7, x9, x11, x16, x17.
The structure of the best model constructed using both COMBI-GA with uniformly
generated initial population [8] and COMBI-GA with binomially generated initial
population contains the same set of signiﬁcant arguments, but it also includes three
additional arguments with small parameters. Hybrid COMBI-GA algorithm converges
in average during 0.1 s. The COMBI algorithm constructs the model during 31.5 s.
Fig. 8. Approximation of the output in the
case of uniformly generated initial population
Fig. 9. Approximation of the output value
using binomially generated initial population
358
O. Moroz and V. Stepashko

The COMBI-GA algorithm with binomially generated initial population more
accurately approximates the initial value on all three subsamples that is illustrated in
Fig. 9 in comparison with similar result obtained using uniformly generated initial
population, see Fig. 8. Approximations of the output value are given by grey lines.
The root-mean-square error of the model output on the subsamples A, B and C is
0.14, 0.26 and 0.53 respectively (using uniformly generated initial population) and
0.12, 0.23 and 0.51 (using binomial generated initial population).
8
Conclusion
The proper choice of GA operators essentially depends on the speciﬁcity and features
of a given task. In particular, on the test tasks, the best results were obtained using a
single-point crossover operator in the COMBI-GA algorithm.
Three operation modes of the algorithm were studied using GA initial population
generated by binomial random numbers generator with low, average, and high prob-
abilities of unit genes appearance in a chromosome. The three modes were combined
with the corresponding mutation operator M1, M2, or M3 (Case 1 (0.1 + M1), Case 2
(0.5 + M2), Case 3 (0.95 + M3)). It was discovered that the Case 1 with low proba-
bility in combination with speciﬁc mutation operator based on adding units in model
structures gives the best results regarding processing speed and accuracy. Such algo-
rithm is helpful when number of relevant arguments is unknown (in real-world prob-
lems); it works quickly, reliably and accurately.
Therefore, the use of evolutionary complication of models in COMBI-GA algo-
rithm is very effective to search true model when solving inductive modelling tasks
with large number of input variables during really short time.
In the future, it would be reasonable to study the task of setting the GA parameters,
particularly concerning how to choose the probability of genetic operators and binomial
generator adaptively and automatically. Also it is advisable to ﬁnd methods for quick
solving big systems of linear equations when using GA with evolutionary simpliﬁca-
tion of models (Case 3). Also the problem of development of hybrid structures of
COMBI with other evolutionary algorithms is of real interest.
References
1. Madala, H.R., Ivakhnenko, A.G.: Inductive Learning Algorithms for Complex Systems
Modeling. CRC Press, New York (1994)
2. Stepashko, V.S.: Combinatorial algorithm of the group method of data handling with optimal
model scanning scheme. Sov. Autom. Contr. 14(3), 24–28 (1981)
3. Stepashko, V.S.: A ﬁnite selection procedure for pruning an exhaustive search of models.
Sov. Autom. Contr. 16(4), 84–88 (1983)
4. Samoilenko, O., Stepashko, V.A.: Method of successive elimination of spurious arguments
for effective solution of the search-based modelling tasks. In: 2nd International Conference
on Inductive Modelling, pp. 36–39. IRTC ITS NASU, Kyiv (2008)
5. Samoylenko, A.A.: The weight criterion to evaluate the arguments informativeness in the
successive selection modeling methods, no. 2, pp. 33–39 (2013). (in Russian)
Hybrid Sorting-Out Algorithm COMBI-GA
359

6. Moroz, O., Stepashko, V.: On the approaches to construction of hybrid GMDH algorithms. In:
Proceedings of 6th International Workshop on Inductive Modelling IWIM-2013, pp. 26–30.
IRTC ITS NASU, Kyiv (2015)
7. Stepashko, V., Bulgakova, O.: Generalized iterative algorithm GIA GMDH. In: International
Conference on Inductive Modelling ICIM-2013, pp. 119–123. IRTC ITS NASU, Kyiv
(2013)
8. Stepashko, V., Moroz, O.: Hybrid searching GMDH-GA algorithm for solving inductive
modeling tasks. In: 1st International IEEE Conference 2016, Data Stream Mining &
Processing, pp. 350–355. IEEE (2016)
9. Srinivas, M., Patnaik, L.M.: Binomially distributed populations for modelling GAs. In: 5th
International Conference on Genetic Algorithms 1993, San Francisco, CA, USA, pp. 138–145
(1993)
10. Moroz, O.G., Stepashko, V.S.: Comparative analysis of model structures generators in
sorting-out GMDH algorithm. In: Inductive Modeling of Complex Systems, vol. 8, pp. 173–191.
IRTC ITS NASU, Kyiv (2016). (in Ukrainian)
11. Moroz, O.: Effectiveness of mutation operators in the genetic search for optimal model in a
sorting-out GMDH algorithm. In: 7th International Workshop on Inductive Modeling
(IWIM-2016), pp. 14–18. IRTC ITS NASU, Kyiv (2016)
12. Moroz, O.H.: Sorting-out GMDH algorithm with genetic search of optimal model. Contr.
Syst. Mach. 6, 73–79 (2016). (in Russian)
13. Holland, J.: Adaptation in Natural and Artiﬁcial Systems: An Introductory Analysis with
Application to Biology, Control, and Artiﬁcial Intelligence. MIT Press, Cambridge (1975).
University of Michigan
14. Kaya, Y., Uyar, M., Tekin, R.: A novel crossover operator for genetic algorithms: ring
crossover. Presented at CoRR (2011)
15. http://archive.ics.uci.edu/ml/. Accessed 21 May 2017
16. https://archive.ics.uci.edu/ml/datasets/SkillCraft1+Master+Table+Dataset. Accessed 24 Apr
2017
360
O. Moroz and V. Stepashko

Development of Combined Information
Technology for Time Series Prediction
Oksana Mulesa1(&)
, Fedir Geche1
, Anatoliy Batyuk2
,
and Viktor Buchok1
1 Department of Cybernetics and Applied Mathematics,
Uzhhorod National University, Sq. Narodna, 3, Uzhhorod 88000, Ukraine
{oksana.mulesa,fedir.geche}@uzhnu.edu.ua
2 ACS Department, Lviv Polytechnic National University, Lviv, Ukraine
abatyuk@gmail.com
Abstract. The task of designing information technology for time series fore-
casting, that bases on fuzzy expert evaluations was considered. A forecasting
model, part of which is an expert’s unit, were proposed. The algorithm of
synthesis predictive scheme based on the basic predictive models was devel-
oped. To determine expert evaluation of the forecast value, the task of fore-
casting was seen as the problem of numerical evaluation of object. The rules for
determining the collective numerical evaluations, that are based on fuzzy expert
assessments were developed. The developed rules take into account coefﬁcients
of experts’ competence and also their degree of conﬁdence for their own
assessments. The approaches to determining the competence coefﬁcients
members of the expert group were systematized. The analysis of features for
designing information-analytical system of time series forecasting were done.
The structural diagram of the analytical block of information-analytical system
for time series prediction, that based on the fuzzy expert estimates, was item-
ized. The designed information technology should be used for time series
forecasting in cases where it is necessary to take account the impact, on the
process that is studied, of temporary, informal factors.
Keywords: Information technology  Time series forecasting
Predictive scheme based on the basic predictive models
The task of numeric evaluation of the object  Fuzzy expert evaluations
Competence of expert  System model  Information-analytical system
1
Introduction
The development of information technology (IT) and their integration in all spheres of
human activity contributes to the rise efﬁciency of decision making processes in the
production and management [1, 2]. Development such new models, methods and tools
as would allow more perfectly tailored to suit each particular subject area contributes to
the emergence of new opportunities in the implementation and management of com-
plex systems. Typically, solving the problem of forecasting the main indicators of the
system, based on the known retrospective data, is an important aspect in the
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_26

development of IT. Therefore, the development of new and adaptation to the problem
of known methods of time series forecasting is an important step in the process of
development IT.
2
Forecasting Problem
The task of time series forecasting for the processes of various nature is arise during the
solving of a number of applied problems of health, economics, ﬁnance, environment,
etc. [3, 4].
Several paradigms for solving the task of time series forecasting are known from
the large number of scientiﬁc sources. Conventionally, all forecasting methods can be
divided into intuitive methods and formal methods. Formal methods are divided into
statistical methods among of them distinguish regression methods, autoregressive
models, methods of exponential smoothing and others [4, 5]; and structural methods
among of them are neural network models, models based on Markov chain, model
based on classiﬁcation-regression trees [3, 4]. To the intuitive methods, include expert
methods [6–8], methods of selective pattern matching [9] and others. In practice, also
often are used the genetic methods and the neural network methods and others [10, 11].
For the effective solving, a time-series forecasting problem usually is necessary to
study the several prediction methods and to select the best of them according to the
speciﬁed task.
3
The Mathematical Model for the Prediction Problem
The mathematical statement of the problem of time series forecasting is such [5]:
Let y1; y2; . . .; yn – is the time series. It is necessary to determine the values
yn þ 1; yn þ 2; . . .; yn þ T, where T – step of forecast.
Typically, in time series forecasting, the forecasting model is deﬁned as a func-
tional relationship that with accurate to random component et has such kind [5]:
yt ¼ Ft yt1; yt2; . . .
ð
Þ þ et
ð1Þ
In this case, forecasting methods are intended to determine of the most accurate
model parameters (1) to better reﬂection trends in time series. At that, we make the
assumption that the value that describe by the time series in the future will be devel-
oped by the same laws as the previous time intervals.
However, with such approaches is almost impossible to take into account subjective
factors that may have non-systematic or one-time effect on the studied processes. In a
number tasks such factors are not permanent or seasonal and often need the forecasting.
But they can have a signiﬁcant impact on predicted value. Such inﬂuence impairs the
precision of the forecast, which can be obtained by marked or other forecasting
methods [7]. Since, as a rule, this effect we can’t to forecast and to model, then the
using of expert assessments can be useful in getting its numerical characteristics as an
362
O. Mulesa et al.

additional source of information. Therefore, we will build a model of time series
forecasting, component of which is the expert’s unit [7, 8]:
Let the experts group with m experts is given: E ¼ E1; E2; . . .; Em
f
g. Each expert
Ei, (i ¼ 1; m) assesses the forecast values yn þ t, t ¼ 1; T by some way. It is necessary to
determine the total assessment of forecast value yn þ t, t ¼ 1; T based on expert
assessments.
We denote the experts’ found numerical assessments as ~ye;n þ t, t ¼ 1; T. Also, let
we have obtained forecasting values for the object of studied using the some method of
time series forecasting. We denoted these assessments as the ~yf ;n þ t, t ¼ 1; T. So the
resulting values can be calculated by the formula:
yn þ t ¼ a~ye;n þ t þ 1  a
ð
Þ~yf ;n þ t;
ð2Þ
where a 2 0; 1
½
 – a value that the a person who makes a decision is setting. This value
is expressing the degree of inﬂuence of expert assessments to the result of prediction.
Thus, the task of time series forecasting is divided into two tasks [12]:
– the task of determining the forecast values using the methods of time series
forecasting;
– prediction problem as a problem of numerical assessment of object.
4
An Applying of Predictive Scheme Based on the Basic
Forecasting Models for Solving the Problem of Time Series
Forecasting
To the models and methods that used effectively in solving the problem of time series
forecasting we can include [3, 5]:
– Winters method, which takes into account the seasonal components;
– regression model of forecasting;
– method of autoregression;
– the method of least squares with weights;
– predictive models of Brown and others.
In [13–15], was proved the effectiveness of the predictive scheme based on the
basic forecasting models. The algorithm for synthesis of this predictive scheme is given
in [13]. According to mentioned algorithm, the synthesis of predictive scheme is as
follows.
The functional relationship between the predicted value ~yt and the elements of time
series we will express this:
~yt ¼ f a1; . . .; ar; yt1; . . .; ytk; t
ð
Þ;
ð3Þ
where a1; a2; . . .; ar are the parameters of the model f, k is the depth of prehistory.
Development of Combined Information Technology
363

To ﬁnd the parameters a1; a2; . . .; ar, usually build the functional
L a1; . . .; ar
ð
Þ ¼
X
n
t¼1
yt  ~yt
ð
Þ2;
ð4Þ
which should to minimize.
Let a
1; . . .; a
r the values of parameters a1; a2; . . .; ar under which the functiona
L takes a minimum value. Then the predictive value ~yn þ s by the model f with optimal
parameters a
1; . . .; a
r can be found as follows:
~yn þ s ¼ f a
1; . . .; a
r; yt1; . . .; ytk; n þ s


;
ð5Þ
where s – is the step of prediction.
Depending on the type of functions f with the parameters a
1; . . .; a
r we have the
different optimal models of time series forecasting.
Let y1; y2; . . .; yt; . . .; yn is the time series. To build a predictive scheme, in the
beginning, we consider the method of autoregression by which we deﬁne the optimal
step of the prehistory k
s for a given time series with ﬁxed prediction step s. In model of
autoregression value of the parameter yt depends on yts; yts1; . . .; ytsks þ 1, where s
– is the step of prediction, ks – the parameter of prehistory with ﬁxed s. Predictive value
of ~yn þ s, by the method of autoregression, can be found with the following model:
~yn þ s ¼ aðsÞ
1 yn þ aðsÞ
2 yn1 þ . . . þ aðsÞ
ks ynks þ 1:
ð6Þ
Let aðsÞ
1
; . . .; aðsÞ
ks
are the optimal parameters of the model (6). Then we have such
model of autoregression:
~yt ¼ aðsÞ
1
yts þ aðsÞ
2
yts1 þ . . . þ aðsÞ
ks ytsks þ 1;
ð7Þ
where t  ks þ s:
It is obviously, that the value yt if s ¼ s0 depends on parameter ks 1  ks  n  s
ð
Þ.
To determine the optimal value of the parameter of prehistory ks if s ¼ s0, for a given
time series yt, we consider the normalized standard deviations:
d1 ¼
1
ns
P
n
t¼s þ 1
yt  aðsÞ
1
yts

2
;
d2 ¼
1
ns1
P
n
t¼s þ 2
vt  aðsÞ
1
yts  aðsÞ
2
yts1

2
;
. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .
dns ¼
yn  aðsÞ
1
yns  . . .  aðsÞ
nsy1

2
And we ﬁnd min d1; d2; . . .; dns
f
g ¼ dks . The value k
s determine the optimal value
of the parameter of prehistory for the autoregression model with ﬁxed s ¼ s0.
364
O. Mulesa et al.

Once we determined k
s (s ¼ s0), we consider the forecast results in different
models for time series prediction M1; M2; . . .Mq with the step of prediction s at the
moments of time t: n  k
s þ 1; n  k
s þ 2; . . .; n. Based on forecasts of different models
we construct the following table:
In the each column ynks þ 1, ynks þ 2,…, yn of Table 1 we ﬁnd the lowest value of
the quadratic deviation between the predictive and the real values of the relevant
elements of the time series. Mathematically, this can be written as:
Let
j1 ¼ n  k
s þ 1 and e1 ¼ min
yj1  ~yð1Þ
j1

2
; . . .; yj1  ~y q
ð Þ
j1

2


;
j2 ¼ n  k
s þ 2 and e2 ¼ min
yj2  ~yð1Þ
j2

2
; . . .; yj2  ~y q
ð Þ
j2

2


;
. . .::
jks ¼ n and eks ¼ min
yn  ~yð1Þ
n

2
; . . .; yn  ~y q
ð Þ
n

2


;
Then we deﬁne the sets I1; I2; . . .; Iks as follows:
I1 ¼
i 2 1; 2; . . .; q
f
g e1 ¼
yj1  y ið Þ
j1

2



;
I2 ¼
i 2 1; 2; . . .; q
f
g e2 ¼
yj2  y ið Þ
j2

2



;
. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .
Iks ¼
i 2 1; 2; . . .; q
f
g eks ¼ yn  y ið Þ
n

2

n
o
;
and we construct Table 2
where aps ¼
bk
ss ; if s 2 Is;
0
;
if s 62 Is;
(
Sp b
ð Þ ¼
X
k
s
j¼1
apj; 0\b  1;
p ¼ 1; 2; . . .; q; s ¼ 1; 2; . . .; k
s


:
Table 1. The predictive values of time series relatively to the basic models
Predicting
models
The value of the studied parameters
for the period n  k
s þ 1; n  k
s þ 2; . . .; n
ynks þ 1
ynks þ 2
  
yn
M1
~y 1
ð Þ
nks þ 1
~y 1
ð Þ
nks þ 2
  
~y 1
ð Þ
n
M2
~y 2
ð Þ
nks þ 1
~y 2
ð Þ
nks þ 2
  
~y 2
ð Þ
n
..
.
..
.
..
.
  
..
.
Mq
~y q
ð Þ
nks þ 1
~y q
ð Þ
nks þ 2
  
~y q
ð Þ
n
Development of Combined Information Technology
365

Using the Sp b
ð Þ and S b
ð Þ ¼ P
q
p¼1
Sp b
ð Þ we deﬁne weights coefﬁcients of forecasting
models Mp 1  p  q
ð
Þ which are included in the following predictive scheme
~yn þ s ¼ S1ðbÞ
SðbÞ ~yð1Þ
n þ s þ S2ðbÞ
SðbÞ ~yð2Þ
n þ s þ . . . þ SqðbÞ
SðbÞ ~yðqÞ
n þ s:
ð8Þ
Before using the predictive scheme (8) we should conduct a study of this scheme
relatively to the b. For it, we build the functional
LðbÞ ¼
X
k
s
i¼1
yji  S1ðbÞ
SðbÞ yð1Þ
ji
 S2ðbÞ
SðbÞ yð2Þ
ji
 . . .  SqðbÞ
SðbÞ yðqÞ
ji
	

2
;
ðji ¼ n  k
s þ iÞ;
and we minimize this functional by varying the value of b. For it, the interval (0, 1] is
divided into m equal intervals. Then we ﬁnd the values of L bi
ð Þ in the points bi ¼
i=m; ði ¼ 1; 2; . . .; mÞ on this interval. It is obviously, that the parameter m deﬁnes the
accuracy of the minimization of functional L b
ð Þ. Let b
m ¼ arg
min
1  i  m L bi
ð Þ. Then we
execute the forecast for time series by the scheme (8), replacing the b on the value b
m.
The remark. If b ¼ 1, then the model (8) does not take into account the distance
from the element yt to predictive value ~yn þ s.
5
The Way of Use the Fuzzy Models and Methods
of Determination the Numerical Evaluations of Object
to the Problem of Time Series Forecasting
We consider the problem of determining the numerical evaluation of object for the
problem of time series forecasting in such formulation [7]:
Let we have the expert group with m experts: E ¼ E1; E2; . . .; Em
f
g. The compe-
tence coefﬁcients of experts are such a1; a2; . . .; am (a1 2 0; 1
½
) and P
m
i¼1
ai ¼ 1.
Each of the experts Ei (i ¼ 1; m) for each of the predicted values yn þ t, t ¼ 1; T,
puts three numeric evaluations: pessimistic, realistic and optimistic. Assign them,
accordingly, y p
ð Þ
i;n þ t, y rð Þ
i;n þ t, y o
ð Þ
i;n þ t. Without decreasing generality of considerations,
Table 2. Parameters of the predictive scheme
Predicting models j1
j2
   jks
The resulting column
M1
a11 a12    a1ks
S1 b
ð Þ
M2
a21 a22    a2ks
S2 b
ð Þ
..
.
..
.
..
.
..
.
..
.
..
.
Mq
aq1 aq2    aqks
Sq b
ð Þ
366
O. Mulesa et al.

Let us consider that y p
ð Þ
i;n þ t  y rð Þ
i; n þ t  y o
ð Þ
i; n þ t. Let the experts assign the degrees of
conﬁdence for each of their assessments. That is l p
ð Þ
i;n þ t; l rð Þ
i; n þ t; l o
ð Þ
i; n þ t for pes-
simistic, realistic and optimistic assessment, accordingly. It is naturally to consider that
l rð Þ
i;n þ t [ l p
ð Þ
i;n þ t and l rð Þ
i;n þ t [ l o
ð Þ
i;n þ t.
In the earlier stages of processing the input data for exception such experts’
assessments that have certain features, it is proposed to apply such heuristics [16]:
Heuristic 1. We do the exception of group E those of the experts which are con-
ﬁdent in their assessments less then a speciﬁed threshold. I.e.
E :¼ En Ei
f
g;
8i ¼ 1; m :
li  D;
ð9Þ
where D 2 0; 1
ð
Þ is the speciﬁed threshold.
Heuristic 2. We do the exception of group E those of the experts who conﬁdent
equally in all their numerical assessments. I.e.
E :¼ En Ei
f
g; 8i ¼ 1; m :
li  min l1i; l2i
f
g
j
j\e;
ð10Þ
where e is the speciﬁed value of proximity for the degree of conﬁdence.
Suppose that after using the Heuristics 1 and the Heuristics 2 a group of experts is
such: E ¼ E1; E2; . . .; Em
f
g.
Then execute the normalization of values of the coefﬁcients competence of experts
as follows:
ai :¼
ai
P
n
i¼1
ai
:
ð11Þ
On the next stage in determining the numerical evaluation of object, for each expert
Ei 2 E (i ¼ 1; m) we build a fuzzy set Ai ¼
y; li y
ð Þ
ð
Þ
f
g [17]. The membership func-
tions of fuzzy sets will be deﬁned by the such rule:
li y
ð Þ ¼
0;
if y\y p
ð Þ
i
;
l p
ð Þ
i
;
if y ¼ y p
ð Þ
i
;
a1iy þ b1i;
if y 2 y p
ð Þ
i
; y rð Þ
i
h
i
;
l rð Þ
i ;
if y ¼ y rð Þ
i ;
a2iy þ b2i;
if y 2 y rð Þ
i ; y o
ð Þ
i
h
i
;
l o
ð Þ
i ;
if y ¼ y o
ð Þ
i ;
0;
if y [ y o
ð Þ
i :
8
>
>
>
>
>
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
>
>
>
>
>
:
ð12Þ
The coefﬁcients a1i; a2i; b1i; b2i should be determined by solving the corresponding
systems of linear algebraic equations.
We offer the following rules for determining the collective numerical evaluation of
the object [7, 8]:
Development of Combined Information Technology
367

Rule 1. Form the fuzzy set of collective numeric object evaluation C ¼
y; M y
ð Þ
ð
Þ
f
g
with membership function M y
ð Þ, which is calculated in the following way:
M y
ð Þ ¼
X
m
i¼1
aili y
ð Þ:
ð13Þ
The algorithm of the calculation of membership function of the resulting fuzzy set
should boil down to deﬁning the total of the corresponding membership functions in
the intervals of their monotony.
Rule 2. According to the accurate methods of determining collective numeric object
evaluation, calculate the total of fuzzy numbers:
C ¼ a1A1 þ a2A2 þ . . . þ anAn
¼S
c2Sc
max
a1;a2;...;an
ð
Þ:
ai2SAi;P
i
aiai¼c
min l1 a1
ð
Þ;l2 a2
ð
Þ;...;ln an
ð
Þ
f
g;
ð14Þ
where SAi are the carriers of corresponding fuzzy sets Ai, i ¼ 1; m; SC is the carrier of
fuzzy set C.
To determine the collective numerical evaluation of object we propose to use the
one of the following relationships:
y ¼ 1
l
X
l
i¼1
yi;
ð15Þ
y ¼ max
y
Arg max
y
M y
ð Þ
f
g


;
ð16Þ
y ¼ min
y
Arg max
y
M y
ð Þ
f
g


;
ð17Þ
where yi 2 Arg max
y
M y
ð Þ
f
g, l is the capacity of set Arg max
y
M y
ð Þ
f
g.
Thus obtained the numerical evaluations we will call as the experts’ forecasting
values.
6
Models and Methods for Determining the Coefﬁcients
of Experts’ Competence
Coefﬁcients of experts’ competence are parts of both conventional and fuzzy rules of
the collective numerical evaluation. Therefore, in the context of determining the
numeric evaluation of an object, we also have to solve the problem of determining the
competence of experts. Mathematically, the formulation of this problem can be written
as follows [18]:
368
O. Mulesa et al.

let we have the expert group E ¼ E1; E2; . . .; Em
f
g and a set of input data about this
experts N. Based on available data, it is necessary to determine the coefﬁcients of
experts’ competence a1; a2; . . .; am.
Depending on the initial conditions of the task, the set N may include:
– experts’ personal data such as information about their education, about place of
employment, about position, industry experience, and other indicators that can
characterize competence of the experts;
– self-evaluations of experts: levels of competence, that experts have identiﬁed for
themselves;
– mutual evaluations of experts;
– information on the results of previous examinations, which involved experts that are
considered and based on which to make conclusions about the competence of
experts.
In the depending on the nature of input data about experts, to evaluate their
competence, it is possible to use such groups of methods [19, 20]:
Group 1. Documental evaluations: let D ¼ D1; D2; . . .; Dr
f
g – are the parameters of the
documental assessments, the characteristics of experts, impact of which on competency
can be determined objectively; L ¼ L1; L2; . . .; Lr
f
g – are the corresponding weights of
parameters. Then, the coefﬁcients of experts’ competency can be determined by the
formula:
ai ¼
X
r
j¼1
LjD ið Þ
j ;
where D ið Þ
j
is the value of j-th parameter for the i-th expert.
Group 2. Mutual evaluations: let we have the set K ¼
k ið Þ
j
n
o
, i; j 2 1; 2; . . .; m
f
g of the
mutual experts’ evaluations, such that, if 9k ið Þ
j
2 K, it means, that expert with number j
estimated the competence of the expert with number i with the value k ið Þ
j
(if i ¼ j then
k ið Þ
j
is self-evaluation of i-th expert); if 9i; j 2 1; 2; . . .; m
f
g, that 9k ið Þ
j
2 K, it means that
j-th expert didn’t estimated the competence of i-th expert.
Then, the resulting coefﬁcients competence of experts can be calculated by the
formula:
ai ¼
P
j ¼ 1; m :
k ið Þ
j
2 K
k ið Þ
j
v ið Þ
j
K
ð Þ
;
where v ið Þ
j
K
ð Þ ¼
1; if 9k ið Þ
j
2 K;
0;
otherwise:
(
Development of Combined Information Technology
369

Group 3. Aposterior methods – the methods based on information on the results of
expert participation in the preliminary expertises. They are divided into statistical and
experimental methods. Separately, one can distinguish methods for determining the
competence of experts from the axiom of unbiasedness according to which, the
majorities’ opinion is competent [21].
There are other, more complex methods for determining the competence of experts
[22]. However, their practical application requires additional resources and extends the
time required to obtain results. Thus, the decision to choose the method for determining
the competence of experts should make the person who makes decisions based on how
much of it is important to improve the accuracy of the results, and how resources he or
she has for solving the problem.
7
Designing of the Information–Analytical System
for the Time Series Forecasting
Information-Analytical System, which implemented the developed models and meth-
ods based on fuzzy expert assessments for the time series forecasting, along with
considered models and methods is an integral part of the respective IT.
The process of developing information-analytical system should be carried out
according to the levels of system model [23, 24]:
goals ) problem model
ð
Þ ) methods algorithms
ð
Þ ) tools
The improving of the efﬁciency of decision-making processes which related to
forecasting of numerical indicators that based on retrospective data, is the primary
purpose of designing the information-analytical system.
According to the mentioned goal and the described models and methods, at the
design stage the information-analytical system it is necessary to implement the fol-
lowing tasks:
– the determination of the forecast value for predictable values;
– the determination of the expert numerical evaluation of forecast value for pre-
dictable values;
– the setting of the impact coefﬁcient of expert assessments to the result and calcu-
lating the resulting value of forecast.
Next set of mathematical models and methods are the basis of the information-
analytical system:
– the models and methods of forecasting based on time series;
– the models and methods of determining the competence of experts;
– fuzzy models and methods for determining the collective numerical assessment.
370
O. Mulesa et al.

Thus, the structure of the analytical unit, which is the basis of IAS, can be repre-
sented as (Fig. 1):
In view of this, we should solve such tasks in the designing of the information-
analytical system:
– the task of developing software for primary processing of the input data;
– the task of developing the procedure of synthesis predictive scheme based on the
forecasting models;
– the task of developing procedures for processing the results of expert surveys.
8
Summary and Conclusion
The paper contain a description of the study information technology for time series
forecasting based on the expert’s fuzzy assessments. The mathematical model of
determination the predictive values, that based on the results of expert polls, was done.
The algorithm of synthesis the predictive scheme that is based on the basic predictive
models, which use improves the result of the prediction. There are some heuristics and
rules of determining the collective numerical evaluation for the predicted value. Using
the proposed information technology for time series prediction allows to takes into
account impact of such factors that are temporary and can be displayed in numeric
experts’ evaluations. The approaches to determining the competence of experts as one
of the stages of expertise has been systematized.
The use of the proposed information technology will improve the accuracy of
forecasting for some applied tasks.
Fig. 1. Unit diagram of the analytical block of IAS
Development of Combined Information Technology
371

References
1. Tsmots, I.: Information Technology and Specialized Tools for Signal Processing and Image
Processing in Real Time. UAD, Lviv (2005)
2. Mulesa, O., Geche, F., Batyuk, A.: Information technology for determining structure of
social group based on fuzzy c-means. In: Xth International Scientiﬁc and Technical
Conference on Computer Sciences and Information Technologies (CSIT), pp. 60–62 (2015)
3. Kuharev, V.N., Sally, V.N., Erpert, A.M.: Economic-Mathematical Methods and Models in
the Planning and Management. Vishcha School, Kiev (1991)
4. Kozadaev, A.S., Arzamasians, A.A.: Prediction of time series with the apparatus of artiﬁcial
neural networks. The short-term forecast of air temperature. Bull. Univ. Tambov Ser. Nat.
Tech. Sci. 11(3), 299–304 (2006)
5. Snytiuk, V.Y.: Forecasting. Models. Methods. Algorithms: Tutorial. “Maklaut”, Kiev (2008)
6. Mendel, A.S.: Method counterparts in predicting short time series: expert-statistical
approach. Machine. Telemekh. №4, pp. 143–152 (2004)
7. Mulesa, O., Geche, F.: Designing fuzzy expert methods of numeric evaluation of an object
for the problems of forecasting. Eastern Eur. J. Enterp. Technol. 3(4(81)), 37–43 (2016).
https://doi.org/10.15587/1729-4061.2016.70515
8. Mulesa, O.: Heuristic rules of the collective numerical evaluation of object and their
application to the problem of time series prediction. In: Intelligent Decision Support Systems
and Computational Intelligence problems, pp. 208–210 (2016)
9. Kuchanky, A., Biloshchytskyi, A.: Selective pattern matching method for time-series
forecasting. Eastern Eur. J. Enterp. Technol. 6(4–78), 13–18 (2015). https://doi.org/10.
15587/1729-4061.2015.54812
10. Zaichenko, Y.P., Mohammed, M., Shapovalenko, N.V.: Fuzzy neural networks and genetic
algorithms in problems of macroeconomic forecasting. Scientiﬁc news “KPI” №4, pp. 20–
30 (2002)
11. Pukach, A., Teslyuk, V., Tkachenko, R., Ivantsiv, R.A.: Implementation of neural networks
for fuzzy and semistructured data. In: 11th International Conference the Experience of
Designing and Application on CAD Systems in Microelectronics (CADSM), pp. 350–352
(2011)
12. Mulesa, O., Geche, F., Batyuk, A., Buchok, V., Voloshchuk, V.: Information technology for
time series forecasting with considering fuzzy expert evaluations
13. Geche, F., Mulesa, O., Geche, S., Vashkeba, M.: Development the method of synthesis of
predictive scheme based on the basic forecasting models. Technol. Audit Reserves Prod. 3(2
(23)), 36–41 (2015). https://doi.org/10.15587/2312-8372.2015.44932
14. Geche, F., Mulesa, O., Myronuyk, I., Vashkeba, M.: Prediction the quantitative character-
istics of ofﬁcially registered HIV-infected people in the region. Technol. Audit Reserves
Prod. 4(2(24)), 34–39 (2015). https://doi.org/10.15587/2312-8372.2015.47907
15. Geche, F., Batyk, A., Mulesa, O., Vashkeba, M.: Development of effective time series
forecasting model. Int. J. Adv. Res. Comput. Eng. Technol. (IJARCET) 4(12), 4377–4386
(2015)
16. Mulesa, O.: Methods of considering subjective character of input data for the tasks of voting.
Eastern Eur. J. Enterp. Technol. 1(3(73)), 20–25 (2015). https://doi.org/10.15587/1729-
4061.2015.36699
17. Orlovskyi, S.A.: Decision Making with Fuzzy Initial Information. Nauka, Moscow (1981)
18. Gnatienko, G., Snityuk, V.: Experts’ Technology of Decision Making. Makaut, Kiev (2008)
372
O. Mulesa et al.

19. Korchenko, O.G., Hornitskyy, D.A., Zaharchuk, T.G.: Research priori estimation methods to
implement an expert examination in the ﬁeld of information security. Data Prot. 12 (4(49))
(2010). http://jrnl.nau.edu.ua/index.php/ZI/article/view/1976/1967
20. Kolpakova, T.A.: Determination of the competence of experts in making group decisions.
Radioelektronika, computer science, upravlinnya 1(24), 40–43 (2011)
21. Snityuk, V. E.: Models and methods of determining the competence of experts on the basis
of unbiasedness axiom. News CHITI, vol. 4, pp. 121–126 (2000)
22. Shanteau, J.: Competence in experts: the role of task characteristics. Organ. Behav. Hum.
Decis. Process. 53(2), 252–266 (1992)
23. Uyomov, A.: System Approach and General Theory of Systems. Mysl, Moscow (1978)
24. Timchenko, A.A.: Fundamentals of System Design and System Analysis of Complex
Objects. Lybed, Кiev (2000)
Development of Combined Information Technology
373

Keyphrase Extraction Using Extended List
of Stop Words with Automated Updating
of Stop Words List
Svetlana Popova1,2 and Gabriella Skitalinskaya3,4(B)
1 Saint-Petersburg State University, Saint-Petersburg, Russia
svp@list.ru
2 ITMO University, Saint-Petersburg, Russia
3 Institute of Technology Tallaght, Dublin, Ireland
gabriellasky@icloud.com
4 Moscow Institute of Physics and Technology (State University), Moscow, Russia
Abstract. In the paper we consider the problem of keyphrase extrac-
tion. Our tasks is to examine additionally the approach to keyphrase
extraction, which is based on the use of extended lists of stop words.
The second objective of the research is to test the approach for automatic
expansion of such extended lists. The obtained results allow to conﬁrm
the possibility of improving the quality of algorithms of keyphrase extrac-
tion. The results of experiments with the extended lists of stop words
show the potential of the proposed approach.
1
Introduction and State-of-the-Art
The problem of keyword extraction has a great number of applications, among
which we can distinguish tasks such as topic extraction, data structuring, data
clustering and classiﬁcation, ontology population, search of dependent concepts
in large data sets and other tasks. The keyphrase extraction problem expands
the problem of keyword extraction and requires not only deﬁning words that
are thematically relevant to the document, but also combining these words into
agreed phrases reﬂecting the main topics of the document.
In the ﬁeld of keyphrase extraction two main approaches can be identiﬁed.
The ﬁrst approach [1–4] considers the following steps. At the ﬁrst step words
are selected from the text. Keyphrases are created from these words at the next
step. It can be described as follows:
1. Ranking of single words and selecting the best ones.
2. Construction of keyphrases from the selected words.
The reported study was funded by RFBR according to the research project No. 16-
37-00430 mol-a and partially supported by the Government of Russian Federation,
Grant 074-U01.
c
⃝Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_27

Keyphrase Extraction Using Extended List of Stop Words
375
In the second approach, word sequences are extracted from the text (usually
referred to as candidate phrases), and in the next step keyphrases are selected
from the extracted set of word sequences. Usually, at this stage, either ranking
and selection of top-n phrases, or classiﬁcation is used. During classiﬁcation it
is determined whether a phrase should be assigned to the class of keyphrases or
not. The second approach [5–10] considers the following steps:
1. Construction of candidate keyphrases.
2. (a) Ranking of candidate phrases and selecting the best ones as keyphrases;
(b) Classiﬁcation of candidate phrases as keyphrases and not.
Within the framework of the ﬁrst approach one of the most popular method for
ranking words is based on graphs. In this case, text units (for example, words [1],
n-gramms or noun phrases [3]) become the vertices of the graph. The arcs show
the relationships between the elements forming the vertices. For example, two
vertex-words are joined by an arc, if in the text these words occur together in a
window of a given size. The weight of the arcs in this case can show how often
these words co-occur in the texts. After the graph is constructed, the weight of
each of its vertices is estimated. The vertices are ranked according to their weight
and the speciﬁed percentage of the best of them is selected for further work. For
example, let single words be used as vertices [1]. Words selected as the best after
the ranking are joined into phrases if in the text these words follow each other.
For vertex ranking, authors mostly use various modiﬁcations of the PageRank
formula [11], for example TextRank and its modiﬁcations [1]. In [2], the con-
struction of the graph takes into account the contents of k nearest documents.
In [4], information on the semantic proximity between the constructed vertices
is taken into account. Usually to calculate the semantic proximity WordNet or
Wikipedia are used. The results obtained in [4] are one of the best in the ﬁeld.
Within the framework of the second approach candidate keyphrase extrac-
tion is the ﬁrst step. To construct the phrases n-grammes, noun phrases, word
sequences corresponding to speciﬁed patterns, word sequences satisfying restric-
tions and other are used. For example, following restrictions can be used: absence
of stop words in a phrase, limits on word length, minimum frequency in the col-
lection, and other.
At the next stage of the second approach, the problem of selecting parameters
for the ranking or classiﬁcation algorithm is solved. As noted in [9], most of the
proposed algorithms use a combination of diﬀerent word characteristics within
an annotated document and the full collection. Examples of such characteristics
could be: the semantic similarity, the popularity of the phrase in a set of phrases
selected by the expert manually (for example, for a similar collection), lexical
and morphological analysis, various heuristics such as the length of the phrase
and the position of the phrase in the document (for example, heading, ﬁrst
paragraph, last paragraph, etc.).
In [12], the problem of automatically extracting keyphrases from single tweets
is explored. Authors propose a deep recurrent neural network model for the joint
processing of the keyword ranking, keyphrase generation, and keyphrase ranking
steps. Word embeddings were used as input to the neural network. The word

376
S. Popova and G. Skitalinskaya
embeddings were pre-trained vectors: part of a Google News dataset and a skip-
gram model were used to generate 300-dimensional vectors for 3 million words
and phrases. The authors report that the proposed approach can achieve better
results than currently presented in the ﬁeld.
The authors of [13,14] present a research in a ﬁeld close to extracting
keyphrases. The task came from the area of clustering the results of user requests.
The idea of this approach is to ﬁrst extract the key topics presented in the snip-
pets of the search results with further grouping of documents around the topics
received. To construct topics, the authors use the most frequent sub-sequences
of words extracted using suﬃx trees and its modiﬁcations [13,14]. These sub-
sequences that form the topics are, in fact, very close to keyphrases as deﬁned
in this paper.
The following articles oﬀer more detailed information on the ﬁeld of research.
The most complete overview of the current state-of-the-art is presented in [15].
Review, results and analysis of the well-known competition in the ﬁeld “SemEval-
2010 Task 5: Automatic Keyphrase Extraction from Scientiﬁc Articles” are pre-
sented in [9]. The following specialized events should be noted: SemEval 2017
Task 10: Extracting Keyphrases and Relations from Scientiﬁc Publications [16],
the ACL 2015 Workshop on Novel Computational Approaches to Keyphrase
Extraction [17].
In addition, we note some of the important observations in the ﬁeld.
Researchers note that nouns and adjectives are the most signiﬁcant parts of
speech when extracting keyphrases [1,3,18]. In [1,3] it is shown that restrictions
on the use of other parts of speech during the construction of phrases can sig-
niﬁcantly increase the results. The noticeable inﬂuence of information on parts
of speech on the task of extracting keyphrases was noted also in other works.
In [18], n-grams, NP-chunks, POS Tag Patterns were used to extract
keyphrases. The paper notes that the latter method was chosen because when
extracting keyphrases only as noun phrases about half of the phrases were not
extracted. 56 POS tag patterns were identiﬁed. Patterns were built on the basis
of manual marking of keyphrases in the training collection, which corresponded
to 10 or more phrases. 51 patterns contained one or more nouns. Five most
frequent patterns were identiﬁed, they consisted of nouns and adjectives. To
determine the keyphrases among the extracted sequences a classiﬁer was used.
That classiﬁer used 4 main parameters to estimate the word sequence weights.
One of the characteristics being the sequence of tags of the candidate phrase.
In the work it was shown that the use of only the mentioned feature has a sig-
niﬁcant eﬀect on the quality of extraction of keyphrases Thus, the information
on the sequence of parts of speech in the candidate-phrase is important. The
highest recall is achieved when extracting candidate phrases is using POS Tag
Patterns. This means that the use of linguistic patterns allows to build many
candidate phrases, which will include a large number of true keyphrases. Thus
another problem arises, there are too many incorrect phrases in the set of can-
didate phrases. Therefore, on the one hand, information about the sequences of
parts of speech in phrases is important, which justiﬁes the use of patterns, on the

Keyphrase Extraction Using Extended List of Stop Words
377
other hand, too many candidate phrases are extracted, which complicates the
stage of extracting keyphrases from them.
An interesting aspect of the selection and ranking of candidate phrases show
the results given in [6]: a large number of generated candidate phrases extremely
negatively aﬀects the quality of selection of keyphrases at the ranking stage. The
paper also shows that the use of information about the proximity of the phrase
to the beginning of the document works well in the case of scientiﬁc publications
and does not work for literary texts. This is an interesting observation, since one
of the main criteria in assessing the weight of a phrase (for scientiﬁc texts) in
research is the position of the phrase in relation to the beginning of the text.
Another important criterion is the length of the phrase. It is believed that
longer phrases should be given higher weight in ranking than shorter phrases,
due to the fact that the latter are more rare in the collection. In [19] it was shown
that when constructing a set of candidate phrases among single-word phrases,
the percentage of phrases that are not keyphrases is higher than for multi-word
phrases.
2
Approach
Our approach developed in [19,20] complements and expands the research area
and represents a new separate direction. It is based on the use of extended
lists of stop words. By stop words we understand words that can not be found
in keyphrases and at the same time are separators between phrases. First we
note that the task of extracting keyphrases is complicated by the fact that the
frequencies of words, which are stop words and which are not, can practically
be very similar. Another problem is that the word can be simultaneously a stop
word for some texts and a keyword for other texts. Therefore, it is important
to consider how the addition of each particular word to the list of stop words
is justiﬁed by measuring the ratio of the gain in quality (due to the fact that
the phrases become more precise) to loss in quality (since part of the correct
phrases containing the added word is lost). This ratio justiﬁes the use of diﬀerent
heuristics in the ﬁeld, which are determined based on the type of processed texts.
The latter leads to the assumption that the best approach to selecting stop words
is an approach based on the use of a training sample, which nevertheless requires
a annotated with keyphrases collection of texts. Although the creation of such
a collection can be time and labour-consuming, this approach allows to specify
the features and logic of phrases that are supposed to be extracted from the
text (for example: should the phrases be long or short, what should they reﬂect
(subject, opinion, location, etc.)).
In this paper, we will further consider the approach based on using extended
lists, and also oﬀer an approach for automatically updating such lists. Here
we consider only the ﬁrst stage of keyphrase extraction: the construction of the
candidate phrases. We show how the their quality can be improved. The received
updated set of phrases can be additionally ranked or classiﬁed. But since the ratio
of true-positive phrases to false-positive when applying the proposed approach
increases, we can expect higher quality when applying methods of ranking and
classiﬁcation.

378
S. Popova and G. Skitalinskaya
In the proposed approach candidate phrases are extracted as continuous
word sequences of maximum length of given parts of speech. Delimiters between
phrases are stop words, punctuation and words of other parts of speech. We use
word sequences instead of linguistic patterns. Our assumption is that using word
sequences is more eﬀective.
When extracting keyphrases using patterns higher recall and lower preci-
sion is achieved. Thus, more phrases are extracted correctly and more phrases
extracted are incorrect, which is explained as follows. When using patterns, more
phrases are extracted than when using sequences. For example, if the patterns
given are: “adjective + noun” and “adjective + adjective + noun”, then the
phrases “red car” and “good red car” will be extracted from the “good red car”
text. At the same time, if the candidate phrases are extracted as the longest
sequences of the speciﬁed parts of speech (nouns and adjectives), then from the
mentioned text only one phrase “good red car” will be retrieved.
In the case of using patterns, their choice is very important. One can, for
example, use the most frequent patterns of the gold standard. But our exper-
iments have shown that using the sequences of maximum length allows to get
higher quality from the point of view of the F1-score (see Sect. 3 for details) than
when using frequent patterns.
When extracting phrases as word sequences of maximum length, we must
deﬁne the parts of speech that words from the extracted phrases should satisfy. In
the ﬁeld it is shown that the main parts of speech are nouns and adjectives. This
is true for a suﬃciently large number of diﬀerent types of texts. In the previous
experiments [19,20] we worked with abstracts to scientiﬁc publications, where
we used sequences of nouns and adjectives. In the current study, we work with
messages from Internet forums and accept not only nouns and adjectives, but
also verbs, otherwise we get a very low Recall. This can be explained as follows.
When working with collections of abstracts of scientiﬁc publications, the analysis
of the gold standard shows that most of the “good” patterns contain only nouns
and adjectives. By saying “good” we mean that the use of this pattern allows
to obtain a high ratio of the number of true-positive divided by the number
of false-positive. In the case of the collection of posts from Internet car forums
with which we work in this study, some of the “best” patterns contain nouns,
adjectives and verbs. The latter determines the choice of parts of speech in this
work.
3
Quality Evaluation
The task of assessing the quality of keyphrases forms an independent direction
of research. In the ﬁeld researchers tried to use the following methods of quality
evaluation, that came from machine translation and summarization evaluations:
Bleu, Meteor, Nist, Rouge. Another and one of the most popular measures of
quality evaluation was the F1-score measure. This measure allows to integrate
information about the precision and recall of the extracted phrases.
Precision = |(C ∩G)|
|G|
(1)

Keyphrase Extraction Using Extended List of Stop Words
379
Recall = |(C ∩G)|
|C|
(2)
FScore = 2 × Precision × Recall
Precision + Recall
(3)
where |C ∩G| - is the number of correctly extracted phrases when processing
all the texts of the collection, |G| - is the total number of phrases automatically
extracted by the algorithm from all the texts of the collection, |C| - is the number
of all phrases in the “gold standard”. The gold standard for each text includes
an ideal list keyphrases manually tagged by an expert.
The F1-score method of evaluation does not impose restrictions on the num-
ber of phrases to be extracted. In [3], an approach based on the use of R-Precision
instead of the F-score is proposed. R-Precision is the Precision value, provided
that the number of keyphrases extracted is exactly the same as the number of
phrases in the gold standard.
To calculate F1-score and R-Precision, the information about the number of
correctly extracted phrases is needed. Let Gt be a set of automatically extracted
phrases from text t, and CT - a set of keyphrases of the gold standard for text.
Here the question arises as which of the phrases from Gt belong to Gt ∩Ct. In
many papers on keyphrase extraction the exact match is used. This means, that
the phrase k - “advanced automatic translation” - and the phrase g - “automatic
translation” - of the gold standard are recognized as diﬀerent. Similar conclusions
will be made, for example, for the following phrases: “good car” and “good auto”.
In [3], the following methods of determining the correctness of the extracted
phrases are suggested and compared:
– the exact match of two phrases (exact match);
– the coincidence of two phrases in the presence of only a morphological diﬀer-
ence (morph);
– the coincidence of two phrases, if the phrase k contains the phrase g (include);
– the coincidence of two phrases, if the phrase g contains the phrase k (part-of).
It is shown that the use of “part-of” is not successful. To evaluate the coin-
cidence of two phrases in [3], “exact match” and “include + morph” were used.
In [21], the number of ways to evaluate an extracted keyphrase is expanded. The
authors determine a ratio, which best reﬂects the human judgments of the cor-
rectness of a phrase. In the form proposed in [21], the number of words coinciding
in the automatically extracted phrase and in the phrase of the gold standard is
considered. Then the resulting value is divided by the length of the longest of
the two phrases.
However, the question of how to assess the quality of keyphrases remains
open. The authors oﬀer new approaches and methods, but the most frequent
and popular in use when evaluating exact matches is the F1-score, which we
also use for evaluation in this paper.

380
S. Popova and G. Skitalinskaya
4
Test Collections
A collection of posts from an Internet car forum related to the purchase or repair
of cars was obtained, where keyphrases were manually assigned. The keyphrases
should reﬂect the main content of the text, including: was the car being bought
or in service, location, the impression the customer has of the car salon/car
service and the reasons for such impression. The constructed collection consists
of 120 texts, among which half of the texts contain positive reviews and half are
negative. The texts were mixed and divided into two equal collections, hereinafter
collection 1 (T1) and collection 2 (T2). Table 1 provides examples of texts and
manually assigned keyphrases.
Table 1. Examples of texts and keyphrases of the “gold standard”

Keyphrase Extraction Using Extended List of Stop Words
381
5
Experiment Description
The goal of the research is to show the eﬀectiveness of the proposed approach.
The essence of the approach to keyphrase extraction is to use an extended list
of stop words. Before, it was shown the eﬀectiveness of this approach for the
English language (for example, by processing scientiﬁc publications) [19,20]. In
this work, the task was to test a similar approach for the Russian language and
texts of another format, such as social media posts. In addition we propose an
approach to automatic expanding of existing extended stop lists.
Two groups of experiments were carried out. The ﬁrst shows that using
extended lists of stop words can lead to an improvement in the quality of the
extracted keyphrases. The second group shows the potential of updating of the
list of stop words with additional words using word2vec models.
The keyphrase extraction process consists in ﬁnding sequences of words of
given parts of speech, where the sequences are as long as possible. Words of
distinct parts of speech, punctuation and stop words are used as separators.
This approach showed very good results as described in [20]. In the carried out
experiments only the following parts of speech were used: nouns, adjectives and
verbs.
5.1
Creation of Extended Lists of Stop Words
Using a training collection, we build an extended list of stop words. Let V be
the vocabulary of the training collection. Let S be the set of words included in
the extended list of stop words and let Sbase be the set of words included in the
standard list of stop words of the selected language. Then FScore evaluates the
quality of the algorithm, which uses a list of stop words S. The extended list of
stop words is extracted by the Algorithm 1.
Keyphrase extraction algorithm uses the obtained extended list of stop words
for processing the test collection.
5.2
Updating the Extended List of Stop Words
Collections used in the work are quite small and the language is quite diverse.
This leads to the fact that only part of the stop words for the test collection can
be found in the train collection. We assume that this problem can be partially

382
S. Popova and G. Skitalinskaya
solved with the help of word2vec models [22]. We decided to add to the extended
list of stop words - words that are semantically close to the words that already
in the list. Our assumption is that by adding words close to the existing stop
words, we will be adding words with similar meanings that might be used as
stop words in texts of other collections. We have added n (n = 2, n = 5) closest
words for each stop word to the ﬁnal list of stop words. For these purposes pre-
trained models from the RusVectores project were used [23]. The RusVectores
API allows to retrieve up to 10 closest words for each input word. The service
allows to work with ready-made models trained on diﬀerent corpora. In this work
we used the ruwikiruscorpora model, which has been trained on the Russian
National Corpus and Wikipedia. The model is a continuous bag-of-words model
with 300-dimensional vectors.
5.3
Texts Pre-processing
The texts in the training and test collections, as well as their gold standards,
underwent the following preprocessing: lemmatization and part-of-speech tag-
ging using MyStem [24] with the some changes. Since the texts often discuss the
maintenance of cars such commonly used abbreviations as “To” and “aBTO”,
which stand for “vehicle inspection” and “car” in the Russian language, are
tagged as a nouns during POS-tagging.
6
Experiment Results and Discussion
The experiments have been carried out for following cases:
– where the collection T1 was used to create an extended list of stop words,
and the collection T2 was used to test the obtained lists;
– for the opposite cases, where the collection T2 was used to construct the
extended list of stop words, and the collection T1 was used for testing.
The results are presented in Table 2 using the following denotations: T1 =>
T2 is used for the case when the extended list of stop words was extracted from
the collection T1, and the quality evaluation of the algorithm was performed on
the collection T2. The denotation T2 => T1 is accepted for the opposite case.
By “standard list of stop words” we mean the case when only the standard
stop words of the Russian language are used as stop words. “Extended list of stop
words” refers to cases in which extended lists of stop words have been used. The
parameter p indicates the value that was used in Algorithm 1 (see Sect. 5.1). The
notation “with additional words” is used for cases when words obtained using
word2vec models were added to the extended list of stop words. The parameter
n determines the number of closest words that was selected for each stop word
of the extended list of stop words. The quality of the automatically extracted
keyphrases was evaluated by comparing these phrases with phrases in the “gold
standard” (manually assigned). For evaluation, the F1-score measure was used
for the “exact match” case (for more details, see Sect. 4).

Keyphrase Extraction Using Extended List of Stop Words
383
Table 2. Experiment results
Stop
words
Standard list
of stop words
Extended list of stop words
p = 0.005 p = 0.005 with
additional words
p = 0.001 p = 0.001 with
additional words
n = 2
n = 5
n = 2
n = 5
T1 =>T2
0.24
0.24
0.31
0.25
0.24
0.23
0.24
T2 =>T1
0.24
0.25
0.25
0.26
0.27
0.28
0.25
The obtained results show that when using extended lists of stop words, the
quality of extracted keyphrases is improved in comparison with the case of using
the standard list of stop words. During the experiments no loss in the quality
of extraction of keyphrases when using extended lists of stop words has been
observed. This allows to assume that there exists some universality of words
that are more often not found than are found in keyphrases within a set of
similar documents. This means the same extended lists of stop words can be
applied to diﬀerent collections of the same type of texts. The proposed approach
works better when mostly identical phrases are used in the train and test collec-
tions. The main reason for this we see in the following. If the style of assigning
keyphrases is identical in both collections, it starts to follow certain patterns, for
example: which words and phrases the expert selects as keyphrases, what infor-
mation is sought to reﬂect in the phrases, which words are commonly not used
in phrases. The latter is close to the ﬁeld of authorship and stylometric analy-
sis. Since the proposed approach can identify some of the stylistic features (e.g.
untypical words), it introduces an improvement in the quality of the algorithm.
Updating the extended lists of stop words with new words using word2vec
models is, by our assumption, expedient. In most cases such an update allows
additional improving of the quality of work of the algorithms. Nevertheless, the
results obtained show that in some cases such an update may lead to worse results
in comparison with using just the extended list of stop words. In our study we
found that, when automatically updating the list of stop words, in spite of the
fact that most words are very similar in meaning to the original words of the
stop words list, there are exceptions. For example, for the word “somnevat’sa”
(translated as “to doubt”) the following closest words are extracted: to believe,
sure, to assure, to verify, to make sure. If, as a result of updating the extended
list of stop words, words that are very diﬀerent from those already in the list
are found, adding them to the ﬁnal list may lead to a list of lower quality. We
assume that in this case the problem is caused by the fact that we are using a
model trained on texts of a diﬀerent type (Wikipedia articles instead of texts
from Internet car forums). This means that the close words are extracted for the
texts on which the model was trained, which may not be similar to texts from
car forums. Despite this, the obtained results show the potential of the proposed
approach for updating extended lists of stop words. At the next stage of the
experiment, we plan to use our own model, which will be trained on texts from
Internet car forums.

384
S. Popova and G. Skitalinskaya
7
Conclusions
Two tasks have been identiﬁed in our research: (1) to further investigate the
approach to keyphrase extraction based on the use of extended lists of stop
words; (2) to test the approach for automatic updating of such lists.
We have shown that extended lists of stop words created for one collection
can be used for other similar collections and can improve the quality of the
extracted keyphrases. By similar collections we mean texts that are similar to
the texts in the training sample or are obtained from similar sources. It is shown
that the creation and use of extended lists of stop words works even for poorly
structured texts, such as messages from Internet car forums.
It should be noted, that words that appear in the extended lists of stop words,
in general, are not stop words (for example, such as “day”, “company”, “auto
loan”). These words fall into the extended lists of stop words only because their
use is not typical in phrases assigned by the expert. It is proposed to update
the obtained extended lists of stop words. To do this, words closest to the stop
words are found using pre-trained word2vec models and added to the ﬁnal stop
words list.
The positive results obtained during the experiments allow to conﬁrm the
possibility of improving the work of algorithms that extract keyphrases using
extended lists of stop words obtained from a training collection. The results
obtained for the updated extended lists of stop words show the potential of the
proposed approach.
References
1. Mihalcea, R., Tarau, P.: TextRank: bringing order into texts. Proc. EMNLP 85,
404–411 (2004)
2. Wan, X., Xiao, J.: Single document keyphrase extraction using neighborhood
knowledge. In: Proceedings of the 23rd National Conference on Artiﬁcial Intel-
ligence, vol. 2, pp. 855–860 (2008)
3. Zesch, T., Gurevych, I.: Approximate matching for evaluating keyphrase extrac-
tion. In: Proceedings of the 7th International Conference on Recent Advances in
Natural Language Processing, pp. 484–489 (2009)
4. Hasan, K.S., Ng, V.: Conundrums in unsupervised keyphrase extraction: making
sense of the state-of-the-art. In: Proceedings of the Conference Coling 2010 - 23rd
International Conference on Computational Linguistics, pp. 365–373, August 2010
5. Pudota, N., Dattolo, A., Baruzzo, A., Ferrara, F., Tasso, C.: Automatic keyphrase
extraction and ontology mining for content-based tag recommendation. Int. J.
Intell. Syst. 25, 1158–1186 (2010)
6. You, W., Fontaine, D., Barth`es, J.P.: An automatic keyphrase extraction system
for scientiﬁc documents. Knowl. Inf. Syst. 34, 691–724 (2013)
7. El-Beltagy, S.R., Rafea, A.: KP-miner: a keyphrase extraction system for English
and Arabic documents. Inf. Syst. 34, 132–144 (2009)
8. Tsatsaronis, G., Varlamis, I., Nørv˚ag, K.: SemanticRank: ranking keywords and
sentences using semantic graphs. In: Proceedings of the 23rd International Confer-
ence on Computational Linguistics, Coling 2010, pp. 1074–1082 (2010)

Keyphrase Extraction Using Extended List of Stop Words
385
9. Kim, S.N., Medelyan, O., Kan, M.Y., Baldwin, T.: Automatic keyphrase extraction
from scientiﬁc articles. Lang. Resour. Eval. 47, 723–742 (2013)
10. Popova, S., Skitalinskaya, G., Khodyrev, I.: Estimating keyphrases popularity in
sampling collections. In: Ciuciu, I., et al. (eds.) On the Move to Meaningful Internet
Systems: OTM 2015 Workshops, Lecture Notes in Computer Science, vol. 9416,
pp. 481–491 (2015)
11. Page, L., Brin, S., Motwani, R., Winograd, T.: The pagerank citation ranking:
bringing order to the web. World Wide Web Internet Web Inf. Syst. 54, 1–17
(1998)
12. Zhang, Q., Wang, Y., Gong, Y., Huang, X.: Keyphrase extraction using deep recur-
rent neural networks on Twitter. In: Proceedings of the 2016 Conference on Empir-
ical Methods in Natural Language Processing, EMNLP 2016, pp. 836–845 (2016)
13. Bernardini, A., Carpineto, C., D’Amico, M.: Full-subtopic retrieval with keyphrase-
based search results clustering. In: Proceedings - 2009 IEEE/WIC/ACM Interna-
tional Conference on Web Intelligence, WI 2009, vol. 1, pp. 206–213 (2009)
14. Zeng, H.J., He, Q.C., Chen, Z., Ma, W.Y., Ma, J.: Learning to cluster web search
results. In: Proceedings of the 27th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval, pp. 210–217 (2004)
15. Hasan, K.S., Ng, V.: Automatic keyphrase extraction: a survey of the state of the
art. In: Proceedings of the 52nd Annual Meeting of the Association for Computa-
tional Linguistics, 12 p. (2014)
16. Augenstein, I., Das, M., Riedel, S., Vikraman, L., McCallum, A.: SemEval 2017
task 10: Scienceie - extracting keyphrases and relations from scientiﬁc publications.
CoRR abs/1704.02853 (2017)
17. ACL: The 53rd annual meeting of the association for computational linguistics. In:
Proceedings of the ACL 2015 Workshop on Novel Computational Approaches to
Keyphrase Extraction (2015)
18. Hulth, A.: Improved automatic keyword extraction given more linguistic knowl-
edge. In: Language, pp. 216–223 (2003)
19. Popova, S., Khodyrev., I.: Ranking in keyphrase extraction problem, is it suitable
to use statistics of words occurrences. In: Proceedings of the Institute for System
Programming, vol. 26, pp. 123–136 (2014)
20. Popova, S., Kovriguina, L., Mouromtsev, D., Khodyrev, I.: Stop-words in keyphrase
extraction problem. In: Conference of Open Innovation Association, FRUCT, pp.
113–121 (2013)
21. Kim, S.N., Baldwin, T., Kan, M.Y.: Evaluating N-gram based evaluation met-
rics for automatic keyphrase extraction. In: Proceedings of the 23rd International
Conference on Computational Linguistics, Coling 2010, pp. 572–580 (2010)
22. Mikolov, T., Corrado, G., Chen, K., Dean, J.: Eﬃcient estimation of word rep-
resentations in vector space. In: Proceedings of the International Conference on
Learning Representations, ICLR 2013, pp. 1–12 (2013)
23. Kutuzov, A., Kuzmenko, E.: WebVectors: A Toolkit for Building Web Interfaces
for Vector Semantic Models, pp. 155–161. Springer International Publishing, Cham
(2017)
24. Segalovich,
I.,
Titov,
V.:
MyStem
(2011).
https://tech.yandex.ru/mystem/.
Accessed 19 July 2017

Innovative Concept of the Strict Line
Hypergraph as the Basis for Specifying
the Duality Relation Between the Vertex
Separators and Cuts
Artem Potebnia(&)
Kyiv National Taras Shevchenko University, Kyiv, Ukraine
potebnia@mail.ua
Abstract. This article presents the original approach for establishing the duality
relation between the vertex separators and cuts in hypergraphs. The analysis of
the existing mathematical structures providing the edge-focused representation
of graphs is followed by the identiﬁcation of the fundamental drawbacks
obstructing the formation of the duality relation on the basis of these structures.
With a view to ﬁlling this research gap, the article formulates the core concept of
the strict line hypergraph and introduces the new notions of the degenerate
connected components and vertex separators. The proposed structures underlie
the construction of the duality relation between the vertex separators and cuts in
the form of theorems accompanied with their rigorous proofs and discussions of
the speciﬁc situations. Finally, the article considers the application of the
established relation for linking the combinatorial optimization problems whose
solution spaces are composed of the vertex separators and cuts.
Keywords: Strict line hypergraph  Degenerate vertex separator
Degenerate connected component  Vertex separator  Cut  Involution
1
Introduction
The opportunity to present the mathematical object in the different formulations
focused on its particular properties and interlinked by the hidden relationship underlies
the most fruitful ways of generating the new knowledge in many areas of mathematics.
The principle of duality based on translating the primal objects into the dual ones
serves as a most widespread and fundamental implementation of such strategy. The
discovery and in-depth analysis of the duality relation provides a further comprehen-
sive insight into the nature of the related objects (representing “the two sides of the
same coin”) and considerably assists in examining their properties. By way of example,
the upper bound on the solutions of the maximization optimization problem could be
estimated by solving the corresponding dual problem [1].
In 2016 Artem Potebnia graduated from Kyiv National Taras Shevchenko University and currently
carries out his research on the personal initiative.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_28

The investigation of the duality principle in the combinatorial structures is espe-
cially attractive due to the opportunity to illustrate the constructive considerations in the
elegant geometrical form along with the great potential for the practical application of
the obtained results. In particular, the structural organization of a wide range of
complex intelligent systems could be naturally described by the models of the undi-
rected graphs and hypergraphs representing the relationships between the constituent
nodes of the system [2]. Therefore, the speciﬁcation of the duality relations in the graph
structures lays the theoretical background required for the development of the new
methods for testing and analyzing such models.
In this article, the undirected graph G ¼ V; E
ð
Þ is deﬁned by the vertex set
V equipped with the multiset (i.e. set allowing the presence of the repeated elements)
E composed of the edges e  V such that ej j  2. In contrast to the undirected graph, the
hypergraph ^G ¼ V; ^E


holds the multiset ^E comprised of the hyperedges ^e  V
having no cardinality limitation. Thus, the undirected graph constitutes the particular
case of the hypergraph. Remark that these formulations allow the presence of the
parallel edges or hyperedges as well as the loops and even parallel loops. Moreover,
such structures also could contain the loose edges or hyperedges that are fully deprived
of the incident vertices. Let us denote the classes of all undirected graphs and hyper-
graphs speciﬁed in such manner as C and ^C respectively. Conversely, their subclasses
C
0  C and ^C
0  ^C cover only those undirected graphs and hypergraphs that do not
contain any loose edge or hyperedge.
The analysis of the potential vulnerabilities to the structural attacks serves as an
indispensable preliminary stage in the development of the reliable attack-robust com-
plex intelligent systems [3]. In the graph-based models, the vertex separators and cuts
provide a natural representation for the attacks of two fundamental types (those based
on removing the entities and those involving the deletion of links between them)
leading to the violation of the system integrity. Formally, the set CV  V and multiset
C^E  ^E are considered respectively as a vertex separator and cut in the connected
hypergraph ^G only if the exclusion of their constituent elements from ^G results in
increasing the number of its connected components. Notice that in the particular case of
the connected undirected graph G, any cut takes the form of the multiset CE  E
satisfying the same requirements [4].
In many contexts, the investigation of the complex systems behavior faces a
challenge of the transition from the conventional description focused on the system’s
entities to the alternative description concentrated on the links between such entities.
The main idea of such viewpoint switching lies in representing the links of the analyzed
system by the vertices of the graph-based model. In turn, the application of the graph
analysis methods to such models could signiﬁcantly assist in producing the additional
information about the system structure (e.g. extract the implicit communities of links)
[5]. For example, the interference effects in the ad hoc wireless network could be
represented by the graph whose vertices reﬂect the communication channels estab-
lished between the network hosts. In addition, each pair of vertices representing the
channels that cannot be activated simultaneously due to the interference issues should
be connected by an edge. Under such approach, the transmission schedules in the
modeled network could be encoded by the vertex colorings of such graph [6].
Innovative Concept of the Strict Line Hypergraph
387

These considerations contribute to the reasonableness of developing the mathe-
matical apparatus for establishing the relationship between the structures representing
the attacks of different types in the graph-based models [7, 8]. Such relationship offers
the opportunity for comprehensively analyzing the vulnerabilities of the modeled
system as well as its parts separated after the attack implementation. Consequently, the
ultimate objective of this article lays in the construction the duality relation translating
the vertex separators into the corresponding “dual” cuts.
In order to ensure the clarity of presentation, the remainder of the article is outlined
according to the following structure. Section 2 analyses the concept of the dual graph
representing the most valuable related work on establishing the correspondence
between the cycles and cuts. Section 3 provides the critical overview of the known
theoretical notions intended for converting the node-centric view of the graph into the
edge-centric one, discusses the drawbacks of these notions, and detects the fundamental
impediments to achieving the stated objective. With a view to overcoming these
impediments, this article considers the widest class of hypergraphs with loose hyper-
edges ^C. In particular, Sect. 4 formulates the requisite concepts of the connected
component, vertex separator, and cut in such hypergraphs as well as discusses their
speciﬁc degenerate forms. Against this background, Sect. 5 introduces the original
concept of the strict line hypergraph, which is used in Sect. 6 for establishing the
duality relation between the vertex separators and cuts in the ^C class hypergraphs.
Finally, Sect. 7 deals with the concluding remarks.
2
Dual Graphs and Their Application for Establishing
the Correspondence Between the Cycles and Cuts
The concept of the dual graph represents the most substantial prior advance of the graph
theory in the context of establishing the duality relationships. Looking into detail, the
operation of constructing the dual graph DE G
ð Þ 2 C
0 is deﬁned for any undirected
connected graph G ¼ V; E
ð
Þ 2 C
0 speciﬁed together with its particular planar embed-
ding E G
ð Þ that divides the plane into a set of l faces X E G
ð Þ
ð
Þ ¼ x1; x2; . . .; xl
f
g (i.e.
regions restricted by the graph’s edges). Such partition serves as the basis for generating
all nodes vD and edges eD of the dual graph DE G
ð Þ. In particular, each face xi 2 X E G
ð Þ
ð
Þ
surrounding t edges of G on both sides is represented in the structure of DE G
ð Þ by the
separate vertex vD
i equipped with t loops corresponding to these edges. In turn, every
edge of the initial graph G shared by two distinct faces xi; xj 2 X E G
ð Þ
ð
Þ is mapped into
the edge of DE G
ð Þ connecting the vertices vD
i and vD
j [1, 9].
The construction of the dual graph DE G
ð Þ naturally involves the appropriate trans-
formation of all internal structures contained in the initial graph G. Most importantly,
the edges of any simple cycle SC (i.e. closed path without the repetitions of vertices) in
G are translated into the edges forming a minimal cut Cm
E (i.e. such cut that does not
include any other cut as its proper subset) in DE G
ð Þ and vice versa. In order to
demonstrate this relationship, let us consider an arbitrary unicyclic graph containing
n edges and having exactly one simple cycle including m  n edges. Its dual graph,
388
A. Potebnia

in turn, is composed of two vertices linked by m edges that constitute a minimal cut
corresponding to such cycle, while the remaining n  m edges are transformed into
loops. The example of graphs illustrating these considerations is presented in Fig. 1.
Apart from that, the concept of the dual graph plays a key role in establishing the
relationship between the minimal cuts and shortest paths in the planar networks given
by the undirected graphs with speciﬁed source and sink nodes lying on the boundary of
the inﬁnite face [10].
However, in the general case, the graph G could have multiple possible embeddings
E1 G
ð Þ; . . .; Eq G
ð Þ leading to the formation of the corresponding non-isomorphic dual
graphs DE1 G
ð Þ; . . .; DEq G
ð Þ. Such extreme sensitivity to the concrete embedding sufﬁ-
ciently complicates the analysis of the dual graphs and constitutes their primary
drawback. In particular, the restoration of the initial graph G always could be per-
formed by repetitive constructing the dual graph of DE G
ð Þ, but only under the speci-
ﬁcation of embedding E
0 DE G
ð Þ


such that DE
0 DE G
ð Þ
ð
Þ ¼ G.
Moreover, the dual graph DE G
ð Þ should be connected even if the graph G is dis-
connected since under any embedding E G
ð Þ each point inside every internal face could
be linked with some point belonging to the external inﬁnite face by a line crossing the
graph’s edges. This, in turn, allows deducing that the dual graphs are fundamentally
unsuitable for recovering the structure of the disconnected initial graphs.
3
Analysis of the Mathematical Structures Providing
the Edge-Focused Representation of Graphs
From the ﬁrst viewpoint, the basic idea of “transforming the graph’s edges into nodes”
that underlies establishing the duality relation between the vertex separators and cuts is
conceptually associated with the known structures of the line and medial graphs.
Fig. 1. Example of the planar unicyclic graph (a) having one highlighted simple cycle SC and its
dual graph (b) with indicated corresponding minimal cut Cm
E . The dual graph is shown in the special
embedding that allows the restoration of the initial graph, while its faces are denoted by xD
i .
Innovative Concept of the Strict Line Hypergraph
389

Deﬁnition 1. The line graph L G
ð Þ 2 C
0
S of the undirected graph G ¼ V; E
ð
Þ 2 C
0 is
represented by a pair
VL; EL
ð
Þ composed of the set VL of vertices vL
i 2 VL corre-
sponding to the edges ei 2 E of the original graph equipped with the following col-
lection of edges EL:
EL ¼
vL
i ; vL
j


2
VL
2


ei and ej are adjacent in graph G

	

:
In other words, the formation of the line graph is conducted through the application
of the L : C
0 ! C
0
S function (deﬁned as the set of ordered pairs G; L G
ð Þ
ð
Þ for all graphs
G 2 C
0) to the speciﬁed argument graph. Notice that the resulting line graphs, in
contrast to the original ones, are deprived of the multiple edges and loops, thereby,
forming the subclass C
0
S  C
0. In particular, multiset Mij ¼
vi; vj


vi; vj


2 E



composed of the parallel edges incident to the vi and vj vertices is reﬂected in the
structure of the corresponding line graph by the clique Cij  VL of cardinality Mij

.
Conversely, the loops vi
ð Þ 2 E receive the same representation in the target graph L G
ð Þ
as the pendant edges (i.e. edges having one “leaf” endpoint vl with degree d vl
ð Þ ¼ 1).
Analogously to the Mij collections, the multisets Mi ¼
vi
ð Þ vi
ð Þ 2 E
j
f
g comprised of the
parallel loops anchored to the same vertex vi are transformed into the cliques Ci  VL
including Mi
j
j nodes [11]. For example, Fig. 2b illustrates the structure of the line
graph constructed for the sample instance of the initial graph shown in Fig. 2a.
Fig. 2. Example of the initial planar graph (a) and the result of its transformation into the
corresponding line (b) and medial (c) graphs. To enhance the traceability of generating the medial
graph’s edges from the boundary walks of the initial graph, these edges are equipped with three
indices representing respectively the face of the initial graph and pair of edges lying on the
boundary of this face.
390
A. Potebnia

These considerations allow concluding that the function L does not hold the
injective property. This, in turn, dramatically restricts the opportunity to completely
reconstruct the argument graph G from the image L G
ð Þ. In order to illustrate the loss of
distinctness after applying the function L, let us consider the following list of graphs
containing m edges:
1. Multitriangle graphs Tm1;m2;m3 composed of three vertices joined respectively by m1,
m2, and m3 edges, where m1 þ m2 þ m3 ¼ m.
2. Multistar graphs Sm1;m2;...;mn
n
including one root vertex r and n  1 leaf nodes. Each
leaf node vi should be incident to mi edges r; vi
ð
Þ, while Pn
i¼1 mi ¼ m.
3. Dipole graphs Dm consisting of two vertices linked by m parallel edges, which
constitute the notable particular case of the multistar graphs when n ¼ 1.
4. Flower graphs Fm containing only one node r equipped with m parallel loops.
Clearly, all these graphs are indistinguishable in terms of the corresponding line
graphs taking the form of the complete graphs Km. In particular, Fig. 3 demonstrates
the sample instances of the listed graphs at m ¼ 6. In addition, the application of the
L transformation leads to the inevitable loss of all isolated (i.e. having zero degree)
vertices. Therefore, the amount of the information encapsulated in the line graphs L G
ð Þ
is insufﬁcient for recovering the arguments G containing loops, parallel edges, and
isolated vertices.
Remark that the non-injectivity of the L function implies that it is not self-inverse.
Moreover, its repeated application to some initial graph G results in the formation of
the graphs sequence given in the form L G
ð Þ; L L G
ð Þ
ð
Þ; L L L G
ð Þ
ð
Þ
ð
Þ; . . .. Typically, the
order of all graphs belonging to such sequence grows without bound, i.e. each sub-
sequent item has the greater number of vertices compared to its predecessor. Consid-
ering the initial graphs belonging to the C
0
S subclass, the only exceptions are the
sequences formed for few speciﬁc types of their connected components, such as the
cycles, paths (including ones having zero length and, thereby, composed of just one
isolated vertex), and graphs S1;1;1
3
[12]. These drawbacks clearly demonstrate that the
Fig. 3. Instances of the multitriangle T3;1;2 (a), multistar S2;1;3
3
(b), dipole D6 (c), and ﬂower F6
(d) graphs whose structure could not be derived from the corresponding line graphs represented
by the complete graphs K6.
Innovative Concept of the Strict Line Hypergraph
391

line graphs could not serve as the basis for establishing the duality of the vertex
separators and cuts.
On the contrary, the operation of the medial graph formation takes into account the
particular embedding E G
ð Þ of the graph G and, thereby, could be viewed as the
symbiosis of the ideas underlying the construction of the line and dual graphs. Let us
introduce the notations W x1
ð
Þ; . . .; W xl
ð
Þ for the closed walks composed of the edges
encountered while traversing the boundaries of the faces x1; . . .; xl 2 X E G
ð Þ
ð
Þ. Notice
that such walks could contain the repeating edges and play a key role in generating the
elements of the medial graph according to the next deﬁnition:
Deﬁnition 2. The medial graph ME G
ð Þ ¼ VM; EM
ð
Þ 2 C
0 of the planar undirected
graph G ¼ V; E
ð
Þ 2 C
0 under the embedding E G
ð Þ is constructed by mapping the edges
ei 2 E into the vertices vM
i 2 VM, while each pair of the subsequent edges ei and ej (not
necessary distinct) in every walk W xk
ð
Þ for k ¼ 1; . . .; l is reﬂected by the corre-
sponding edge eM
k;i;j 2 EM.
The construction of the medial graph is exempliﬁed in Fig. 2c. Notice that in
contrast to the line graphs, the medial graphs could contain loops and parallel edges
(compare Fig. 2b and c). However, the sensitivity to the particular embedding con-
tributes to the non-uniqueness of such structures, i.e. one initial graph G could be
associated with multiple non-isomorphic graphs ME1 G
ð Þ; . . .; MEp G
ð Þ. Moreover, the
operation of the medial graph formation is not self-inverse, leads to the elimination of
the isolated vertices and cannot be applied to the non-planar initial graphs G [13, 14].
This list of drawbacks precludes the usage of the medial graph concept for establishing
the duality relationship.
In summing up this section, unlike the cycles and cuts, the vertex separators and
cuts differ in the basic type of their constituent elements. This difference produces the
fundamental impediments to the establishment of the duality relation between such
structures in the undirected graphs belonging to the class C
0. In such graphs, the
removal of vertices automatically implies the exclusion of all their incident edges,
while the deletion of any edge does not affect the graph’s nodes. Accordingly, the
multiset containing all edges of any graph’s connected component having two or more
nodes serves as the universal cut. On the contrary, the conception of the universal
vertex separator composed of all nodes contained in the connected component having
two or more edges cannot take place in the C
0 class graphs. Another challenging issue
is associated with the strict limitation imposed on the number of vertices that could be
incident to any edge. This stimulates us to consider the widest class of hypergraphs
with loose hyperedges ^C and shows the need for introducing the new mathematical
structure that is conceptually similar to the line and medial graphs, but at the same time
overcomes their drawbacks discussed in this section.
392
A. Potebnia

4
Adaptation of the Connected Component, Cut, and Vertex
Separator Concepts for Hypergraphs with Loose
Hyperedges
The presence of the loose hyperedges in the ^C class hypergraphs allows overcoming
the fundamental restriction represented by the inevitable elimination of any hyperedge
when excluding all its incident vertices. Nevertheless, such interpretation of the
hyperedges as the independent structures leads to the existence of the nodeless
hypergraphs composed exclusively of the loose hyperedges. This, in turn, underlies the
need for introducing the following modiﬁed formulation of the connected component:
Deﬁnition 3. The connected component of the hypergraph ^G ¼ V; ^E


2 ^C is repre-
sented by the hypergraph ^F ¼ V^F; ^E^F


equipped with the vertex set V^F  V that is
either empty or composed of the nodes connected to each other by paths (i.e. the
sequences of hyperedges) and unreachable from any other vertex belonging to VnV^F.
In the case V^F 6¼ ;, the multiset ^E^F  ^E comprises all hyperedges that are incident to
the nodes of V^F. On the contrary, if V^F ¼ ;, the collection ^E^F should contain only one
loose hyperedge
;
f g 2 ^E having no incident vertices.
Under such approach, each loose hyperedge is covered by the separate connected
component represented by a pair ;;
;
f g
f
g
ð
Þ. Notice that such nodeless components are
referred to as degenerate in the remainder of this article. Conversely, the isolated nodes
vs 2 V deprived of the incident hyperedges are placed into the corresponding com-
ponents given by pairs
vs
f g; ;
ð
Þ. At the same time, all other components of ^G are
required to have both non-empty collections of vertices and hyperedges. With respect
to the proposed formulation of the connected component, we can deﬁne the vertex
separators and cuts in the ^C class hypergraphs in the following manner:
Deﬁnition 4. The non-empty set CV  V (multiset C^E  ^E) belonging to one connected
component ^F of the hypergraph ^G 2 ^C is considered as a vertex separator (cut) in ^G
only if the exclusion of its constituent vertices (hyperedges) from ^G results in splitting
^F into two or more disjoint connected components.
Figure 4 illustrates the connected components detected in the particular hypergraph
instance along with the few examples of vertex separators and cuts containing only one
element. Let us denote the spaces containing all possible cuts C^E and vertex separators
CV for any component ^F of the hypergraph ^G as C ^F
 
and VS ^F
 
respectively. Notice
that these spaces are closed under union of their elements, i.e. Ci
^E
S C j
^E 2 C ^F
 
for any
Ci
^E; C j
^E 2 C ^F
 
and Ci
V
S C j
V 2 VS ^F
 
for any Ci
V; C j
V 2 VS ^F
 
.
Moreover, the existence of the nodeless connected components allowed by Deﬁ-
nition 3 contributes to the necessity for distinguishing the degenerate form of the vertex
separators. Formally, the vertex separator CV could be considered as degenerate only if
the exclusion of its constituent nodes results in arising less than two non-degenerate
(i.e. having at least one vertex) connected components. In contrast to the
non-degenerate ones, such separators do not ensure the formation of the disjoint subsets
Innovative Concept of the Strict Line Hypergraph
393

of vertices and provide only the segregation of the new loose hyperedges by removing
all their incident vertices.
For example, the set C4
V ¼ v10
f
g highlighted in Fig. 4 serves as the degenerate
vertex separator for the component ^F1. This follows directly from the fact that the
removal of the vertex v10 produces two connected components given by pairs
V^F1n v10
f
g; ^E^F1n ^e9
f
g


and ;; ^e9
f
g
ð
Þ, while only the ﬁrst of them is non-degenerate.
Moreover, according to the proposed deﬁnitions, the entire vertex set V^F of any con-
nected component ^F for which V^F

  1 and ^E^F

  2 serves as the universal degen-
erate vertex separator because the removal of its nodes produces
^E^F

 loose
hyperedges.
Let us introduce the notation N DVS ^F
 
 VS ^F
 
for the subspace containing all
possible non-degenerate vertex separators for the connected component ^F. Like the
overall space VS ^F
 
, the subspace N DVS ^F
 
is closed under union of its elements. At
the same time, the division performed by excluding the hyperedges of any cut C^E always
leads to separating only non-degenerate connected components, which clearly shows the
fundamental absence of the degenerate form of cuts. In summarizing the above con-
siderations, all possible cuts, vertex separators, and non-degenerate vertex separators for
the whole hypergraph ^G having n connected components ^F1; ^F2; . . .; ^Fn are encom-
passed respectively by the spaces C ^G
 
¼ Sn
i¼1 C ^Fi


, VS ^G
 
¼ Sn
i¼1 VS ^Fi


, and
N DVS ^G
 
¼ Sn
i¼1 N DVS ^Fi


.
Clearly, the removal of the isolated vertices or loose hyperedges from the hyper-
graph ^G is simply equivalent to the elimination of the corresponding connected
components. Therefore, such connected components are associated with the empty
Fig. 4. Example of the ^C class hypergraph with 7 detected connected components. The
degenerate connected component is outlined by the dashed border, while the dotted line depicts
the loose hyperedge. The examples of possible vertex separators and cuts are highlighted by the
ellipses. Moreover, the ellipses with dashed border indicate the degenerate vertex separators.
394
A. Potebnia

spaces of the vertex separators and cuts (e.g. the components ^F6 and ^F7 in Fig. 4).
More generally, VS ^F
 
¼ ; only if ^E^F

  1, while C ^F
 
¼ ; only if V^F

  1. For
example, the component ^F5 in Fig. 4 does not have enough elements for constructing
either vertex separators or cuts, i.e. VS ^F5


¼ ; and C ^F5


¼ ;. In turn, the compo-
nents ^F3 and ^F4 illustrate the case in which only one of the spaces is empty, i.e.
VS ^F3


¼ ; and C ^F4


¼ ;, while C ^F3


¼
C3
^E
n
o
and VS ^F4


¼
C3
V


.
Let us take a closer look at the component ^F2 of the hypergraph instance shown in
Fig. 4. In this case, the spaces VS ^F2


and C ^F2


are composed respectively of 14
vertex separators and 12 cuts given by the following expressions:
VS ^F2


¼
v12
f
g; v13
f
g; v15
f
g;
V^F2
2


;
V^F2
3


; v12; v13; v14; v15
f
g
	

;
C ^F2


¼
^e12
f
g; ^e14
f
g;
^E^F2
2


n ^e11;^e13
f
g;
^E^F2
3


; ^e11;^e12;^e13;^e14
f
g
	

:
At the same time, only 3 of the vertex separators listed above are non-degenerate,
i.e.
N DVS ^F2


¼
v12
f
g; v12; v14
f
g; v12; v15
f
g
f
g:
In closing this section, we should emphasize that the proposed concept of the
degenerate vertex separator plays a key role in expanding the overall space VS ^G
 
.
Such expansion, in turn, lays the groundwork for establishing the duality relation
between the vertex separators and cuts.
5
Concept of the Strict Line Hypergraph
The close inspection shows that the restriction of the edges cardinality serves as a
principal reason for the shortcomings of the L G
ð Þ graphs discussed in Sect. 3. Actually,
the vertices that ensure connecting n  2 edges are reﬂected in the structure of the line
graph by the collections containing C2
n edges. In turn, such edges are not accompanied
with any additional information allowing to indicate that they were inspired by the
same vertex of the original graph. On the contrary, the hyperedges of the ^C class
hypergraphs are exempted from the above-mentioned cardinality limitation. These
considerations emphasize the reasonableness of using the hypergraphs as the core of
the new mathematical structure formulated below.
Deﬁnition 5. The strict line hypergraph of the primal hypergraph ^G ¼ V; ^E


2 ^C is
denoted as ^L ^G
 
and given by a pair
V ^L; ^E^L


2 ^C including the sets speciﬁed as
follows:
Innovative Concept of the Strict Line Hypergraph
395

V
^L ¼
v
^L
i
^ei 2 ^E

n
o
; ^E
^L ¼
^e
^L
i
vi 2 V
j
n
o
;
^e
^L
i ¼
v
^L
k
vi 2 ^ek
ð
Þ ^ ^ek 2 ^E



n
o
:
In contrast to the L function, the transformation ^L : ^C ! ^C (speciﬁed implicitly as
the set of pairs
^G; ^L ^G
 


for all ^G 2 ^C) has both domain and codomain represented by
more general class ^C. Notice that each vertex vi 2 V of the primal hypergraph ^G is
reﬂected by only one hyperedge ^e^L
i 2 ^E^L in the corresponding strict line hypergraph
^L ^G
 
. This underlies the possibility to recover the argument ^G from the image ^L ^G
 
and constitutes the main difference of the proposed structure from the line graphs [15].
Theorem 1. The function ^L that maps each hypergraph ^G into the corresponding strict
line hypergraph ^L ^G
 
is an involution, i.e. ^L ^L ^G
 


¼ ^G.
▲Assume that the second application of the ^L function results in obtaining the
hypergraph ^L ^L ^G
 


¼
V ^L^L; ^E^L^L


. By taking into account that the hypergraph
^L ^G
 
¼
V ^L; ^E^L


serves as the argument at the second application of the ^L function,
we can specify the following expressions detailing the structure of V ^L^L and ^E^L^L:
V
^L^L ¼
v
^L^L
i
^e
^L
i 2 ^E
^L

n
o
; ^E
^L^L ¼
^e
^L^L
i
v
^L
i 2 V
^L

n
o
;
^e
^L^L
i
¼
v
^L^L
k
v
^L
i 2 ^e
^L
k


^ ^e
^L
k 2 ^E
^L



n
o
:
By putting the expressions for the collections V ^L and ^E^L into the obtained formulas for
V ^L^L and ^E^L^L, we would have
V
^L^L ¼
v
^L^L
i
^e
^L
i 2
^e
^L
t vt 2 V
j
n
o

n
o
;
^E
^L^L ¼
^e
^L^L
i
v
^L
i 2
v
^L
t ^et 2 ^E

n
o

n
o
:
Obviously, the condition ^e^L
i 2
^e^L
t vt 2 V
j
n
o
is satisﬁed if there exists such vertex
vt 2 V that t ¼ i. This allows to present the expression for the set V ^L^L in the following
compact form:
V
^L^L ¼
v
^L^L
i
vi 2 V
j
n
o
:
Therefore, the hypergraph ^L ^L ^G
 


holds the same vertex set as the initial
hypergraph ^G. In turn, since the condition v^L
i 2
v^L
t ^et 2 ^E

n
o
is satisﬁed at the presence
of the hyperedge ^ei 2 ^E, we can provide the following simpliﬁed form of the
expression for the ^E^L^L multiset:
396
A. Potebnia

^E
^L^L ¼
^e
^L^L
i
^ei 2 ^E

n
o
:
However, this result implies only that the hypergraphs ^G and ^L ^L ^G
 


hold an
equal number of hyperedges, i.e. ^E
  ¼ ^E^L^L

. In order to show that the hyperedges
belonging to the collections ^E and ^E^L^L have the identical structure, we need to perform
the transformations of the expression for the hyperedge ^e^L^L
i . In particular, using the
formulas for ^e^L
k and ^E^L, this expression could be rewritten as follows:
^e
^L^L
i
¼
v
^L^L
k
v
^L
i 2
v
^L
t
vk 2 ^et
ð
Þ ^ ^et 2 ^E



n
o



n
^
v
^L
t
vk 2 ^et
ð
Þ ^ ^et 2 ^E



n
o
2
^e
^L
h vh 2 V
j
n
o
n
o

o
:
Here the condition v^L
i 2
v^L
t
vk 2 ^et
ð
Þ ^ ^et 2 ^E



n
o
is satisﬁed only if vk 2 ^ei. By
considering this observation and inserting the formula for the ^e^L
h component, we would
have
^e
^L^L
i
¼
v
^L^L
k
vk 2 ^ei
ð
Þ ^
v
^L
t
vk 2 ^et
ð
Þ ^ ^et 2 ^E



n
o


n
2
v
^L
s
vh 2 ^es
ð
Þ ^ ^es 2 ^E



n
o
vh 2 V
j
n
oo
:
First of all, we need to prove that the second predicate of the conjunction is satisﬁed
for any vk 2 V. Note that the expression
v^L
t
vk 2 ^et
ð
Þ ^ ^et 2 ^E



n
o
represents the set
V ^L vk
ð
Þ composed of the vertices v^L
t 2 V ^L for which the hyperedges ^et 2 ^E are incident
to the vk node.
Conversely,
the
expression
v^L
s
vh 2 ^es
ð
Þ ^ ^es 2 ^E



n
o
vh 2 V
j
n
o
could
be
rewritten as
V ^L vh
ð
Þ vh 2 V
j
n
o
. Thus, the second predicate of the conjunction takes the
form of V ^L vk
ð
Þ 2
V ^L vh
ð
Þ vh 2 V
j
n
o
and is satisﬁed for any vk 2 V. In turn, as evident
from the ﬁrst predicate, each ^e^L^L
i
hyperedge of the hypergraph ^L ^L ^G
 


contains the
vertices v^L^L
k
such that vk 2 ^ei and, thereby, reproduces the structure of the corre-
sponding hyperedge ^ei of the hypergraph ^G.
▼
Figure 5 demonstrates the example of transforming the instance of the primal
hypergraph ^G into the instance of the strict line hypergraph ^L ^G
 
. The most natural
observation is that the nodes vi 2 V equipped with n neighbor hyperedges of the initial
hypergraph are reﬂected in the resulting structure by the corresponding hyperedges
^e^L
i 2 ^E^L that are incident to n vertices v^L 2 V ^L. Conversely, the hyperedges ^ei 2 ^E
covering m nodes are translated into the vertices v^L
i 2 V ^L with degrees d v^L
i


¼ m. In
particular, the loops (e.g. the hyperedges ^e1, ^e2, ^e4, and ^e7 in Fig. 5a) are transformed
Innovative Concept of the Strict Line Hypergraph
397

into the “leaf” vertices that are incident to only one hyperedge. The “leaf” nodes (such
as the vertices v2, v5, v7, and v8 in Fig. 5a), for their part, are converted into loops,
which is in conformity with Theorem 1. Notice that the isolated vertices vs 2 V are
reﬂected in the strict line hypergraph by the loose hyperedges ^e^L
s ¼ ;
f g 2 ^E^L that are
fully deprived of the incident nodes and allowed according to Deﬁnition 5. The vertex
v6 in Fig. 5a serves as an illustration of such case.
Clearly, the strict line hypergraph constructed for the disconnected primal hyper-
graph is represented by the disjoint union of the strict line hypergraphs that are sep-
arately formed for all its connected components. Another important property of the
function ^L is associated with the existence of the numerous ﬁxed points represented by
the hypergraphs whose structure remains unchanged after applying the function ^L.
Considering the hypergraph instance shown in Fig. 5a, the connected component
containing the vertex v7 equipped with the loop ^e7 serves as the simplest example of
such ﬁxed point. Notice that the hypergraphs whose maximum vertex degree is
bounded by 2 are mapped into the strict line hypergraphs belonging to the subclass of
undirected graphs C  ^C.
Fig. 5. Example of the primal hypergraph (a) and the result of its transformation into the
corresponding strict line hypergraph (b).
398
A. Potebnia

6
Construction of the Duality Relation Between the Vertex
Separators and Cuts
Theorem 2. Each vertex separator CV 2 VS ^G
 
in the primal hypergraph ^G ¼
V; ^E


2 ^C is mapped into the corresponding dual cut ^E^L CV
ð
Þ ¼
^e^L
i vi 2 CV
j
n
o
belonging to the space C ^L ^G
 


of the strict line hypergraph ^L ^G
 
¼
V ^L; ^E^L


.
▲Suppose the primal hypergraph ^G is connected and the set S  V is composed of the
vertices belonging to the non-degenerate connected component segregated after
excluding the nodes of the separator CV 2 VS ^G
 
from the overall set V, while
T ¼ Vn S S CV
f
g. In this context, we can divide the multiset of hyperedges ^E into the
disjoint allowed subcollections ^E S
ð Þ, ^E S; CV
ð
Þ, ^E CV
ð
Þ, ^E CV; T
ð
Þ, and ^E T
ð Þ. Assume
that their cardinality equals, respectively, t1, t2, t3, t4, and t5. Notice that each allowed
subcollection ^E 
ð Þ contains the hyperedges that are incident to at least one vertex from
each subset indicated in brackets and, at the same time, has no links with the nodes of
the other subsets. By contrast, the subcollections ^E S; T
ð
Þ and ^E S; CV; T
ð
Þ are forbidden
and contain no hyperedges. This follows directly from the fact that the presence of at
least one hyperedge that connects the S and T sets bypassing the nodes of CV is
completely inconsistent with considering the set CV as a vertex separator.
Without losing the generality, we can renumber all hyperedges of the hypergraph ^G
in such manner that
^E S
ð Þ ¼ ^e1; . . .;^er1
f
g; ^E S; CV
ð
Þ ¼ ^er1 þ 1; . . .;^er2
f
g;
^E CV
ð
Þ ¼ ^er2 þ 1; . . .;^er3
f
g; ^E CV; T
ð
Þ ¼ ^er3 þ 1; . . .;^er4
f
g;
^E T
ð Þ ¼ ^er4 þ 1; . . .;^em
f
g;
where ri ¼ Pi
j¼1 tj, while m  2 is the cardinality of ^E.
By Deﬁnition 5, each allowed subcollection ^E 
ð Þ is reﬂected in the structure of the
^L ^G
 
hypergraph by the corresponding subset of nodes V ^L 
ð Þ. In turn, the vertex sets
S, T, and CV are transformed respectively into the multisets of hyperedges and ^E^L S
ð Þ,
^E^L T
ð Þ, and ^E^L CV
ð
Þ. Thereby, the remainder of the proof is intended to demonstrate that
the multiset ^E^L CV
ð
Þ forms a cut in the dual hypergraph ^L ^G
 
.
Obviously, the vertices of the set CV could be covered only by the hyperedges
belonging to the union ^E S; CV
ð
Þ S ^E CV
ð
Þ S ^E CV; T
ð
Þ. This allows imposing the
restriction on the structure of each hyperedge ^e^L
i 2 ^E^L CV
ð
Þ taking the form of the
following superset:
^E
^L CV
ð
Þ ¼
^e
^L
i vi 2 CV
j
n
o
; ^e
^L
i  v
^L
k k ¼ r1 þ 1; . . .; r4
j
n
o
:
Conversely, the vertices of the S and T sets could be incident only to the hyperedges
belonging to the unions ^E S
ð Þ S ^E S; CV
ð
Þ and ^E CV; T
ð
Þ S ^E T
ð Þ, respectively. Taking
Innovative Concept of the Strict Line Hypergraph
399

this into consideration, we can provide the next expressions limiting the structure of the
hyperedges included in the collections ^E^L S
ð Þ and ^E^L T
ð Þ:
^E
^L S
ð Þ ¼
^e
^L
i vi 2 S
j
n
o
; ^e
^L
i  v
^L
k k ¼ 1; . . .; r2
j
n
o
;
^E^L T
ð Þ ¼
^e^L
i vi 2 T
j
n
o
; ^e^L
i  v^L
k k ¼ r3 þ 1; . . .; m
j
n
o
:
In the case of the non-degenerate vertex separator CV belonging to the space
N DVS ^G
 
, both S and T sets should be non-empty. Moreover, each of the multisets
^E S; CV
ð
Þ and ^E CV; T
ð
Þ should contain at least one hyperedge because the set CV
initiates the separation of the non-degenerate connected components and, thereby, is
required to be linked with their nodes. This observation produces the limitations t2  1
and t4  1 underlying the condition r4  r2  1. Such condition implies that the
multisets ^E^L S
ð Þ and ^E^L T
ð Þ are non-empty and contain the hyperedges that are incident
to the non-overlapping subsets of vertices. At the same time, since the connectivity of
hypergraphs is preserved under the ^L transformation, the hyperedges of the multiset
^E^L CV
ð
Þ play the “bridge” role. As a result, for any CV belonging to N DVS ^G
 
, the
collection ^E^L CV
ð
Þ represents a cut in the hypergraph ^L ^G
 
. The example illustrating
this case is given in Fig. 6.
In order to complete the proof, we need to take into consideration the case of the
degenerate vertex separators belonging to the set VS ^G
 
nN DVS ^G
 
. In particular,
regarding the vertex separator CV that segregates one non-degenerate connected
component, only the set S is non-empty, while all other vertices of VnS are included in
CV. As in the previous case, the multiset ^E S; CV
ð
Þ should be non-empty. At the same
time, the collection ^E CV
ð
Þ should contain one or more hyperedges underlying the
Fig. 6. Example of establishing the duality relation between the non-degenerate vertex separator
CV in the primal hypergraph (a) and the corresponding cut ^E^L CV
ð
Þ in the strict line hypergraph (b).
400
A. Potebnia

formation of the degenerate connected components after excluding all vertices of CV.
This contributes to the limitations t2  1, t3  1, and r3  r2  1 allowing to conclude
that the multiset ^E^L CV
ð
Þ constitutes a cut in ^L ^G
 
because the exclusion of its
hyperedges produces t3 isolated vertices and the connected component containing
t1 þ t2 nodes. Figure 7 illustrates the example of mapping the degenerate vertex sep-
arator into the corresponding cut segregating one isolated vertex and one connected
component containing three nodes.
Finally, the universal vertex separators containing all nodes of V and segregating
exclusively degenerate connected components are characterized by the empty S and
T sets, which leads to consolidating all hyperedges in the collection ^E CV
ð
Þ, i.e. t3  2.
Thereby, in this case, the multiset ^E^L CV
ð
Þ serves as the universal cut in ^L ^G
 
because
the removal of its hyperedges leads to the formation of t3 isolated vertices.
▼
Notice that in the case of the disconnected primal hypergraph, the considerations
given in the above proof should be followed for all its connected components.
Moreover, since the function ^L is an involution, the transformation formulated in
Theorem 2 could be performed also in the reverse order.
In summarizing the ﬁndings presented in this section, the non-degenerate vertex
separators in ^G are converted into cuts in ^L ^G
 
satisfying the restriction that the
exclusion of their hyperedges should produce at least two connected components
having more than one vertex. However, this conversation is non-exhaustive since it
does not put any structures in correspondence with cuts segregating less than two
connected components having at least two vertices and, thereby, provides only the
partial coverage of the space C ^L ^G
 


. The degenerate vertex separators ensure ﬁlling
this gap and serve as the dual structures for such cuts.
Fig. 7. Example of transforming the degenerate vertex separator CV in the primal hypergraph
(a) into the corresponding cut ^E^L CV
ð
Þ in the strict line hypergraph (b).
Innovative Concept of the Strict Line Hypergraph
401

7
Conclusions
As a corollary of Theorem 2, the established duality relationship is characterized by the
cardinality preservation, i.e. the vertex separators in the primal hypergraph ^G and their
dual cuts in the corresponding strict line hypergraph ^L ^G
 
are composed of an equal
number of elements. In particular, the articulation points in ^G are converted into the
“bridge” connections in ^L ^G
 
. This property underlies the opportunity of setting the
linkage between the combinatorial optimization problems whose solution spaces are
composed of the vertex separators and cuts. Most importantly, the problem of ﬁnding
the minimum vertex separator in ^G could be converted into the corresponding problem
of calculating the minimum cut in ^L ^G
 
. Consequently, solving both these problems
could be reduced to solving only one of them with the subsequent transformation of the
received solution into the solution of the linked problem.
At the same time, the exclusion of the degenerate form of the vertex separators from
the consideration affects both these problems as well as the linkage between them. In
particular, the problem of calculating the minimum non-degenerate vertex separator in
^G could be translated into the restricted problem of ﬁnding the minimum cut in ^L ^G
 
.
The feasible solutions of such restricted problem are represented by the cuts segre-
gating at least two connected components having more than one vertex.
Therefore, the duality relation formulated in this article is extremely crucial for
endowing the complex systems with the enhanced resistance to the structural attacks.
This relation allows selecting the most convenient way for searching the vulnerable
parts of the modeled system and their representing in terms of the vertex separators and
cuts. Such opportunity of the viewpoint switching could serve as the basis for the more
comprehensive investigation of the system structure, which contributes to the signiﬁ-
cance of the obtained results.
References
1. Goh, C.J., Yang, X.Q.: Duality in Optimization and Variational Inequalities. Taylor &
Francis, London (2002)
2. Potebnia, A.: Representation of the greedy algorithms applicability for solving the
combinatorial optimization problems based on the hypergraph mathematical structure. In:
Proceedings of 14th International Conference on the Experience of Designing and
Application of CAD Systems in Microelectronics (CADSM), pp. 328–332. IEEE, Lviv
(2017). https://doi.org/10.1109/cadsm.2017.7916145
3. Holme, P., Kim, B.J., Yoon, C.N., Han, S.K.: Attack vulnerability of complex networks.
Phys. Rev. E 65(5), 056109 (2002). https://doi.org/10.1103/physreve.65.056109
4. Potebnia, A.: Method for classiﬁcation of the computational problems on the basis of the
multifractal division of the complexity classes. In: Proceedings of 3rd International
Scientiﬁc-Practical Conference on Problems of Infocommunications, Science and Technol-
ogy (PIC S&T), pp. 1–4. IEEE, Kharkiv (2016). https://doi.org/10.1109/infocommst.2016.
7905318
402
A. Potebnia

5. Schaub, M.T., Lehmann, J., Yaliraki, S.N., Barahona, M.: Structure of complex networks:
quantifying edge-to-edge relations by failure-induced ﬂow redistribution. Netw. Sci. 2(1),
66–89 (2014). https://doi.org/10.1017/nws.2014.4
6. Ganesan, A.: Performance of sufﬁcient conditions for distributed quality-of-service support
in wireless networks. Wirel. Netw. 20(6), 1321–1334 (2014). https://doi.org/10.1007/
s11276-013-0680-z
7. Lu, Z., Li, X.: Attack vulnerability of network controllability. PLoS ONE 11(9), e0162289
(2016). https://doi.org/10.1371/journal.pone.0162289
8. Shen, Y., Nguyen, N.P., Xuan, Y., Thai, M.T.: On the discovery of critical links and nodes
for assessing network vulnerability. IEEE/ACM Trans. Netw. 21(3), 963–973 (2013).
https://doi.org/10.1109/tnet.2012.2215882
9. Ray, S.S.: Planar and dual graphs. In: Graph Theory with Algorithms and its Applications,
pp. 135–158. Springer, India (2013). https://doi.org/10.1007/978-81-322-0750-4_9
10. Ahuja, R.K., Magnanti, T.L., Orlin, J.B.: Network Flows: Theory, Algorithms, and
Applications. Prentice-Hall, New Jersey (1993)
11. Xicheng, L., Li, Q.: Line graphs of pseudographs. In: Proceedings of the International
Conference on Circuits and Systems, pp. 713–716. IEEE, Shenzhen (1991). https://doi.org/
10.1109/ciccas.1991.184458
12. Evans, T., Lambiotte, R.: Line graphs, link partitions, and overlapping communities. Phys.
Rev. E 80(1), 016105 (2009). https://doi.org/10.1103/physreve.80.016105
13. Huggett, S., Moffatt, I.: Bipartite partial duals and circuits in medial graphs. Combinatorica
33(2), 231–252 (2013). https://doi.org/10.1007/s00493-013-2850-0
14. Brylawski, T., Oxley, J.G.: The Tutte polynomial and its applications. In: White, N. (ed.)
Matroid Applications, pp. 123–225. Cambridge University Press, Cambridge (1992)
15. Potebnia, A.: Creation of the mathematical apparatus for establishing the duality relation
between the vertex separators and cuts in hypergraphs. In: Proceedings of 12th International
Scientiﬁc and Technical Conference on Computer Science and Information Technologies
(CSIT), pp. 236–239. IEEE, Lviv (2017)
Innovative Concept of the Strict Line Hypergraph
403

Involutory Parametric Orthogonal Transforms
of Cosine-Walsh Type with Application
to Data Encryption
Dariusz Puchala(&)
Institute of Information Technology,
Lodz University of Technology, Lodz, Poland
dariusz.puchala@p.lodz.pl
Abstract. In this chapter we introduce a novel scheme for constructing a class
of parametric involutory transforms that are described by the computational
lattice structure of fast cosine-Walsh type transform. Further on the practical
effectiveness of the proposed class of transforms is evaluated experimentally in
the task of data encryption. Finally, the selected aspects of mass-parallel real-
izations of the proposed class of involutory parametric transforms with usage of
modern graphics processing units are considered and discussed.
Keywords: Fast parametric linear transforms  Data encryption
1
Introduction
Modern applications of digital processing and analysis of one- and multidimensional
data require highly efﬁcient computational techniques. This is a direct consequence of
the systematic increase in the capabilities of data acquisition devices, in particular for
recording static images and video sequences. Naturally, this involves a proportional
increase in the size of data to be processed or analyzed. Classical methods that are
commonly used in practice utilize fast linear transforms which can be characterized by
high computational efﬁciency. Beyond any doubt among these the most popular are:
Walsha-Hadamard transform [1, 2], slant transform [3], fast Fourier transform [4], fast
trigonometric transforms of Fourier type, i.e. Hartley transform [5], cosine and sine
transforms in different variants [6], etc. Fast algorithms for such transforms are con-
structed using “divide and conquer” strategy and take the form of lattice structures with
planar rotation operations realized by ﬁxed and predetermined angles. The natural
extension of fast linear transforms are their parametric equivalents. Such transforms can
be characterized by high ﬂexibility while retaining the same computational efﬁciency.
The parameterization of a fast linear transforms consists in making the rotation angles
of all or only selected base operations dependent on the values of a set of parameters.
This allows for adaptation of the basis vectors to the speciﬁc aspects of the tasks being
performed, as well as the statistical characteristics of input signals. The high increase in
interest of researchers in parametric transforms was initiated by the chapter [7]. At the
present time, the most well-known classes of parametric transforms include: slant and
slant-Haar transforms [8, 9], reciprocal-orthogonal transforms [10, 11], Fourier and
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_29

Hartley transforms [12], involutory transforms [13], etc. Among these for special
attention deserve fast orthogonal parametric involutory transforms. Their additional
feature is the property of self-invertibility, which means that the inverse transform
matrix UT is the same as a forward one U, i.e. equality UT = U takes place. Such a
feature is very important primarily from the point of view of hardware dedicated
implementations. In such a case the same hardware layout (dedicated for forward
transform) is able to perform both forward and inverse transformations which translates
directly into the cost reduction.
The areas of application of fast parametric transforms are the same as of fast linear
ones. Hence, the examples may include the following tasks: signal denoising [8, 12,
14], encryption of data [11, 13, 15], joint compression and encryption of data [16] and
pattern recognition [17]. In this chapter we are interested in the task data encryption. It
should be noted that the idea of application of linear transforms to encrypt data is not
new and a considerable number of scientiﬁc chapters can be indicated were linear
parametric transforms are exploited for encryption or joint encryption and compression
of multimedia data [15, 18]. It is also obvious that data encryption methods based on
linear transformations can be easily cracked with plain text attacks. However, the
speciﬁcity of multimedia data transmissions (e.g. digital television broadcasts) which
can be shortly characterized as: transmission of huge amounts of data of low or
moderate value, makes it impossible or economically unproﬁtable to perform such
types of attacks. Moreover the simplicity of implementations both in software and
hardware (and also in parallel, mass-parallel, pipelined modes, etc.) combined with
high computational effectiveness and the adequate level of security of consumer market
data place linear transforms at the forefront of practically usable and desired methods of
data encryption.
In this chapter we propose a novel scheme for constructing a class of parametric
involutory transforms that are deﬁned by the aptly parametrized lattice structure of fast
algorithm for calculation of the discrete parametric cosine-Walsh transform (DCWT)
[7]. The necessary conditions imposed on the lattice structure in order to obtain
involutory transformations are formulated and given in the form of equations involving
all of the free parameters. Further on the usefulness of the proposed class of transforms
in the task of data encryption is investigated experimentally operating on model sig-
nals. Eventually, in the ﬁnal sections of the chapter we discuss the practical issues of
mass-parallel calculations of the considered variants of DCWT lattice structures with
aid of modern graphics processing units (GPUs). It should be noted that mass-parallel
realizations of several fast linear transforms, namely: discrete Fourier transform [19]
and discrete wavelet transform [20] have proved to be highly efﬁcient.
2
Discrete Parametric Cosine-Walsh Transform
The discrete parametric cosine-Walsh transform introduced in chapter [7] is a para-
metric transform deﬁned by the Cooley-Tuckey like fast computational lattice structure
of the well-known discrete Walsh-Hadamard transform (DWHT) [2] (see Fig. 1).
However, in the case of DCWT the base orthogonal operators are parametrized with
real-valued parameters representing angles of rotations which can take arbitrary values
Involutory Parametric Orthogonal Transforms of Cosine-Walsh Type
405

in contrast to DWHT where all base operators are invariable and represent rotations by
constant angle of p/4 radians. In Fig. 1 the lattice structure of DCWT for the case of
N = 8 element input data vectors is presented.
As the ﬁrst step the elements x(n) for n = 0, 1,…, N −1 of input vector must be
permuted in bit-reversal order (see [2]). Next the permuted vector is propagated for-
ward through i = 0, 1,…, log2N −1 consecutive layers each containing N/2 base
orthogonal operators Oi,j for j = 0, 1,…, N/2 −1 which are represented graphically as
“” and can be deﬁned in the matrix form as:
Oi;j ¼
cos ai;j
sin ai;j
sin ai;j
 cos ai;j


;
ð1Þ
where ai,j for i = 0, 1,…, log2N −1 and j = 0, 1,…, N/2 −1 are the free parameters in
a number of:
LPAR ¼ N
2 log2 N:
The computational lattice structure of DCWT transform is a fast one (understood in
the sense of fast linear transforms characterized by the order of O(Nlog2N) addi-
tion/multiplication operations) and can be described in the most general form by a
number of:
LADD ¼ N log2 N; LMUL ¼ 2N log2 N
ð2Þ
real-valued additions and multiplications respectively.
Fig. 1. The computational lattice structure of discrete cosine-Walsh transform for the case of
N = 8 point transformation.
406
D. Puchala

3
Involutory Discrete Cosine-Walsh Transform
The DCWT is an orthogonal transform and as so it can be described in a matrix form as
a product of sparse block-diagonal orthogonal matrices and permutation matrices
which describe base operations within stages and connections between neighboring
stages respectively. Let UN be a matrix of N-point forward DCWT. Then, according to
[21] it can be described as:
UN ¼ INðPlog2 N1Ulog2 N1PT
log2 N1Þ  . . .  ðP1U1PT
1ÞðP0U0PT
0ÞPbr
N ;
ð3Þ
where Pbr
N and IN are the permutation matrices of elements in input and output vectors,
IN is an identity N on N element matrix, Pbr
N is a bit-reversal permutation matrix, Ui for
i = 0, 1,…, log2N −1 are block-diagonal matrices Ui = diag{Oi,0, Oi,1,…, Oi,N/2–1}
describing operations within consecutive layers, and Pi for i = 0, 1,…, log2N −1 are
permutation matrices describing connections between neighboring layers of base
operators. Then taking into account the lattice structure of DCWT (c.f. Fig. 1) Pi
matrices for i = 0, 1,…, log2N −1 can be deﬁned as follows:
Pi ¼ ðI2log2 Ni1  Peo
2i þ 1Þ;
ð4Þ
where “” stands for the Kronecker product of matrices, and Peo
M are the permutation
matrices that group elements into the following order: M/2 elements with even indices,
M/2 elements with odd indices, i.e. Peo
M ¼ Pk;l

eo
M for k, l = 0, 1,…, M −1 and:
pk;l ¼
1
for 2k ¼ l and 0  l  M
2  1; or
for 2k þ 1 ¼ l  M
2 and
M
2  l  M  1;
0
in other cases:
8
<
:
Using introduced Pi permutation matrices the bit-reversal ordering matrix Pbr
N can
be described as a following product of matrices:
Pbr
N ¼
Y
log2 N1
i¼0
P2i þ 1
 
!
:
In Fig. 2 the lattice structure for calculation of N = 8 point DCWT in a fashion
resulting from the factorization described by formula (3) is presented, wherein the input
and output permutation matrices are deliberately omitted.
The involution property requires the transform matrix UN to be self-invertible, i.e.
the following equality U1
N
¼ UN must hold. By taking into account the orthogonality
of UN matrix it is straightforward to verify that the quoted equality would take the
following form UT
N ¼ UN. It is also obvious that then we would have UNUN = IN. But
Involutory Parametric Orthogonal Transforms of Cosine-Walsh Type
407

in order to obtain the desired property the additional dependencies between transform
parameters are required. In accordance with basic matrix algebra transformations we
can deﬁne on the basis of Eq. (3) the inverse matrix UT
N in the form of the following
factorization:
UT
N ¼ ðPbr
N ÞTðP0UT
0 PT
0ÞðP1UT
1 PT
1Þ  . . .  ðPlog2 N1UT
log2 N1PT
log2 N1ÞIN
ð5Þ
It should be also noted that Pbr
N

T¼ Pbr
N and in addition for i = 0, 1,…, log2N −1
we have UT
i ¼ Ui (c.f. Eq. (1)). Then taking into account the observations made we can
rewrite formula (5) as:
UT
N ¼ Pbr
N ðP0U0PT
0ÞðP1U1PT
1Þ  . . .  ðPlog2 N1Ulog2 N1PT
log2 N1ÞIN:
ð6Þ
Now comparing Eqs. (3) and (5) it can be conceived that one way to obtain the
equality UT
N ¼ UN leads by bringing the formula (5) to the form structurally identical to
the factorization described by (3). Here, by the structural identity we understand the
same structures of matrices used, i.e.: the same permutation matrices Pi at the same
positions and in both cases block-diagonal matrices representing base operations.
Let us start the process of refactorization of formula (6) by multiplying its right side
by Pbr
N
matrix twice in order to keep the equality undisturbed. We have then:
UT
N ¼ . . .
ð
Þ Pbr
N Pbr
N


, which is valid since: Pbr
N Pbr
N ¼ IN. Next the following transfor-
mation rule is proposed, i.e.:
ðPlog2 N1Ulog2 N1PT
log2 N1ÞPbr
N ¼ Pbr
N ðP0QNUlog2 N1QT
NPT
0Þ:
ð7Þ
From Eq. (7) emerges instantly the construction of QN which in matrix notation can
be written as: QN ¼ PT
0Pbr
N Plog2N1. Then introducing the proposed transformation rule
into formula (6) it can be rewritten as follows:
Fig. 2. The lattice structure of discrete cosine-Walsh transform for the case of N = 8 point
transformation resulting from matrix product factorization (3).
408
D. Puchala

UT
N ¼ Pbr
N ðP0U0PT
0ÞðP1U1PT
1Þ  . . .  Pbr
N ðP0 Ulog2 N1PT
0ÞPbr
N ;
ð8Þ
where Ulog2N1 ¼ QNUlog2N1QT
N is used to make notation more concise. Further on the
same transformation rule can be applied to the second stage which consists of matrices
ðPlog2N2Ulog2N2PT
log2N2Þ. Its application results in the following equality:
ðPlog2 N2Ulog2 N2PT
log2 N2ÞPbr
N ¼ Pbr
N ðP1QNUlog2 N2QT
NPT
1Þ:
ð9Þ
Then by solving Eq. (9) we would obtain QN ¼ PT
1Pbr
N Plog2N2: It can be veriﬁed
by direct check that: PT
1Pbr
N Plog2N2 ¼ PT
0Pbr
N Plog2N1: Further on this observation can
be extended to the following general form:
QN ¼ PT
i Pbr
N Plog2 Ni1
ð10Þ
for i = 0, 1,…, log2N −1 which is formulated here without proof. Equation (10)
allows to draw an additional conclusion that QN ¼ QT
N. Then on the basis of obser-
vation (10) it is clear that the proposed transformation rule can be applied to all
remaining layers resulting in the following factorization:
UT
N ¼ ðPlog2 N1 U0PT
log2 N1ÞðPlog2 N2 U1PT
log2 N2Þ  . . .
. . .  ðP1 Ulog2 N2PT
1ÞðP0 Ulog2 N1PT
0ÞPbr
N ;
ð11Þ
where Ui = QNUiQN for i = 0, 1,…, log2N −1. Further on it should be noted that QN
is a permutation matrix which places elements of input vector in bit-reversal order but
calculated for N/2 elements and applied to the pairs of adjacent elements in input
vector. For example for N = 8 we would obtain the following matrix:
Q8 ¼
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
1
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
1
2
66666666664
3
77777777775
and the following order of elements: [0, 1, 4, 5, 2, 3, 6, 7]. It is obvious that if such
permutation is applied to matrix Ui in the form: QNUiQN, then it would result in the
block-diagonalmatrix Ui with operations Oi,j for j = 0, 1,…, N/2 −1whichare permuted
in bit-reversal order. Thus, we would have Ui = diag{Oi,p(0), Oi,p(1),…, Oi,p(N/2−1)},
where p(j) for j = 0, 1,…, N/2 −1 stands for bit-reversal order calculated for N/2 ele-
ments. In Fig. 3 the lattice structure for calculation of inverse DCWT for N = 8 points is
presented in the form resulting from refactorization described by formula (11).
Involutory Parametric Orthogonal Transforms of Cosine-Walsh Type
409

Since both factorizations (3) and (11) for UN, UT
N transforms respectively are
structurally identical (c.f. Figs. 2 and 3) it is now elementary to derive the necessary
conditions required in order to achieve the equality UT
N ¼ UN. The mentioned condi-
tions can be written as: Ui ¼ Ulog2Ni1 for i = 0, 1,…, log2N −1, which further on
results in the following dependencies between speciﬁc transform parameters:
ai;pðjÞ ¼ alog2 Ni1;j
ð12Þ
for i = 0, 1,…, log2 N=2
d
e  1 and j = 0, 1,…, N/2 −1, where d e is a ceiling operator.
The resulting transform constructed in accordance with factorization (3) that takes into
account dependencies described by formula (12) must be an involutory parametric
orthogonal transform with fast computational lattice structure of cosine-Walsh trans-
form type. In such case a number of free parameters equals:
LPAR ¼
N
4 log2 N
for even values of log2 N;
N
4 log2 N þ 1
2
ﬃﬃ
N
2
p
for odd values of log2 N:
	
ð13Þ
In Fig. 4 we show the lattice structure of involutory parametric DCWT transform
for the case of N = 8 points. It is obvious that the computational complexity of
involutory variant of DCWT is the same as in the case of parametric DCWT and hence
it can be also described by formulas (2).
Fig. 3. The lattice structure of inverse discrete cosine-Walsh transform for the case of N = 8
point transformation in the form resulting from factorization (11).
410
D. Puchala

4
Experimental Study
As it was previously mentioned the experimental part of the chapter would be focused
on the issues of data encryption. The results of similar study but applied to different
types of lattice structures were published in chapter [22] (two-stage Beneś network)
and also in [23] (the comparison of two-stage Beneś network to Walsh-Hadamard like
structure and lattice structure for two-channel bank of orthogonal ﬁlters). In this
chapter, we adopt similar test procedure which involves the experimental calculation
of: (I) the probability distribution of relative mean square error of signal reconstruction
with randomly drawn private keys for the decryption step, (II) the expected value of
relative mean square error in the function of Hamming distance calculated between
private keys for the encryption and decryption steps. In Fig. 5 the ﬂow diagram for the
encryption and decryption steps is presented. In the ﬁrst place an input vector x is
encrypted with aid of orthogonal parametric transform U which is parametrized with
private key K. It is obvious that the construction of private key should allow to extract
the values of all parameters required by the parametric transform. After this step the
encrypted (cyphered) vector y is obtained. The decryption process requires knowledge
of the form of a private key and involves inverse parametric transform UT or U in the
case of involutory transformations. In Fig. 5 it is assumed that at both stages the same
private key K is known. Hence, theoretically (i.e. neglecting computational errors) the
input data vector x can be reconstructed without any distortion.
In paper [22] a simple mapping of individual bits of a private key to the values of
transform parameters ai,j for i = 0, 1,…, log2N −1 and j = 0, 1,…, N/2 −1 was pro-
posed. It assumes in the ﬁrst place that the interval [0, 2p) of parameters variation is
divided into a number of 2k subintervals of equal lengths Da = 2p/2k. Then the discrete
Fig. 4. The lattice structure of involutory parametric cosine-Walsh transform for the case of
N = 8 point transformation.
Involutory Parametric Orthogonal Transforms of Cosine-Walsh Type
411

values of parameters would be calculated as ai,j = ki,jDa, where ki,j denotes the decimal
integer number from an interval [0, 2k −1], which is encoded, e.g. with natural binary
code, with the subsequent bits of a private key assigned to this parameter. Then the size
of a private key K can be calculated as LK = kLPAR bits. It is clear that in the case of
involutory transforms the mapping of private key bits to the values of parameters must
take into account the necessary dependencies described by formula (12).
In order to verify the effectiveness of the proposed class of parametric transforms a
series of experiments involving model ﬁrst order Markov signals with variance r2 = 1
and the correlation coefﬁcient q = 0.9 was performed. The process of data encryption
and decryption proceeded according to the data ﬂow diagram depicted in Fig. 5. In
addition, we assumed k = 4 and considered the following lengths of input vectors, i.e.
N = 8, 16, 32 and 64 points, which correspond to the following lengths of private keys,
i.e.: LK = 28, 64, 168 and 384 bits. The obtained results averaged over a number of 105
trials are shown in Figs. 6 and 7.
The ﬁrst part of the study involves the experimental determination of the proba-
bility distribution of data reconstruction error during the simulation of brute force
attack. As a measure of reconstruction error we adopt the mean square error
(MSE) expressed as a percentage of mean signal energy. Such relative MSE can be
deﬁned as:
eMSE ¼ 100 
1
M
X
M1
i¼0
xi  zi
k
k2
xi
k k2
 
!
[% ],
ð14Þ
where xi and zi for i = 0, 1,…, M −1 stand for input and reconstructed vectors
respectively. In this experiment xi = x = const. for i = 0, 1,…, M −1 and output
vectors are calculated as: zi = UiUx, where U is an encrypting involutory DCWT
parametrized with private key K, and Ui are involutory decrypting DCWTs obtained for
randomly guessed keys Ki for i = 0, 1,…, M −1 in a number of M = 105 trials. The
results in the form of probability distribution of discrete values of relative MSE are
shown in Fig. 6 for all of the considered transform lengths. It should be noted that the
Fig. 5. The diagram of the steps of data encryption and decryption.
412
D. Puchala

relative MSE have the possible range of variation from 0% to 400%, where 0% means
z = x and 400% means z = −x. The obtained experimental results show that the most
probable value of relative MSE for all transform lengths is close to 200%. Taking into
account the shape of probability distribution function it means that the expected value
of relative MSE during brute-force attack is close to 200% of energy of input signal,
which corresponds to the scenario: zi ? x. In addition the standard deviation of relative
MSE tends to be smaller with an increase of transform size, which clearly results from
the increasing lengths of private keys.
The second part of experiments was concentrated on the experimental evaluation of
relative MSE in the function of Hamming distance calculated between private keys
used at the encryption and decryption stages. The relative MSE results averaged over a
number of M = 105 trials are presented in Fig. 7 for the following transform sizes:
N = 8, 16, 32 and 64 points. An analysis of experimental results allows to make the
following conclusions: (I) the mean value of Hamming distance between randomized
private keys equals one half of the key size LK, (II) the mean value of relative MSE
calculated over the possible interval of observation (distances lying outside that interval
were simply not observed due to very small probability of occurrence) is close to
200%, what is fully consistent with the results of the ﬁrst experiment, (III) the width of
the observation interval decreases with the growth of transform size which is a direct
Fig. 6. The probability distribution of relative MSE for the following transform sizes:
N = 8, 16, 32 and 64 points.
Involutory Parametric Orthogonal Transforms of Cosine-Walsh Type
413

consequence of the binomial Bernoulli distribution of the probability of random
drawing a pair of keys that differ by a given number of bits (see [22]).
Based on the series of experiments performed we conclude that the proposed class
of involutory transforms structurally deﬁned by the fast algorithms for calculation of
cosine-Walsh transform can be characterized by high practical effectiveness in the tasks
of data encryption, i.e. the high level of data concealment. It should be noted that the
considered encryption/decryption process is realized in the direct fashion, i.e. in signal
domain, and can be described by the data ﬂow diagram from Fig. 5. Moreover, if we
take into account high computational effectiveness and the identity of forward and
inverse transformations it can be stated that involutory DCWTs can be applied as
effective tools in practical tasks of data encryption, e.g. as a part of the scheme for joint
encryption and compression of visual data that was proposed in chapter [17].
5
Selected Aspects of Mass-Parallel Realizations
General purpose scientiﬁc and engineering computations realized in parallel mode
involving several hundreds or thousands of concurrently operating processors are
commonplace nowadays. This is possible by the availability of programming libraries
Fig. 7. The mean values of relative MSE in the function of Hamming distance between private
keys drawn randomly for the encryption and decryption steps.
414
D. Puchala

that enable convenient and fast way of implementation of computer programs that can
harness the potential of modern graphics processing units (GPUs). Due to the high
number of available processors such way of programming is referred to as the
mass-parallel programming. In the ﬁeld of linear transforms we can indicate a number
of research chapters (see [19, 20, 24, 25]) which deal with the problems of
mass-parallel realizations of well-known linear transforms.
In this section we analyze the selected aspects of DCWT calculation on modern
GPUs by taking into account two microarchitectures of NVIDIA cards: (I) G200
microarchitecture (NVS3100M GPU card) which can be treated as an old one to be
found in older computers, (II) Maxwell microarchitecture (GTX 960 GPU card), rela-
tively novel one, which establishes a today standard in consumer segment laptop and
desktop computers. Since it is well-known that older GPU microarchitectures suffered
from the loss of effectiveness while accessing data with improper alignment (coalescing)
we try to answer the following two questions: (I) in what measure a proper coalescing of
data accesses can increase the effectiveness of DCWT calculation algorithms on older
microarchitectures, (II) whether it is still needed to optimize data coalescing in the case
of modern GPUs. In order to answers these questions we perform a series of experiments
involving two computational structures for calculation of DCWTs. The ﬁrst structure
(Type I) is a standard one of Cooley-Tuckey type shown in Fig. 1. The second one
(Type II) is inspired by the lattice structure proposed in monograph [26] for fast cal-
culation of Fourier transformation (see Fig. 8)1. Next, regardless of the type of the lattice
structure considered we assume the following model of computations, i.e.: (I) we
operate on data stored in global memory, (II) a single computational thread is respon-
sible for calculations within a single butterﬂy operator, (III) calculations between
consecutive stages are synchronized (c.f. Fig. 9). Thus, it is clear that the structure of
type I can be characterized by a very poor data coalescing at the initial stages which is
gradually improving so as to achieve the optimum at the last stage (c.f. the Single
Instruction Multiply Threads (SIMT) model of instruction execution within warps of
threads [27]). In the case of the lattice structure of type II inputs of butterﬂy operator at
all stages are optimally aligned while outputs refer to the neighboring pairs of data. We
try to solve this problem by engaging shared memory for data mixing within warps and
thus allowing for improved alignment of data writes into global memory (c.f. Fig. 10).
Moreover for both types of structures an additional variant taking advantage of texture
memory to store the coefﬁcients of base operations is tested. It is well known that the
texture memory although residing in global memory area is provided with additional
cache memory which may result in a signiﬁcant increase of the bandwidth of data
transfer. Further on for the simplicity of calculation of indices which are required to
access the values of the coefﬁcients for butterﬂy operators (coefﬁcients are
pre-calculated and stored as the pairs of sine/cosine of operators angles) we implement
1 It should be noted that adaptation of lattice structure presented in monograph [26] for fast
calculations of Fourier transform to the ﬁeld of cosine-Walsh transformation requires proper
permutations of butterﬂy operators within consecutive stages. The required permutations can be
described as follows: (I) in the ﬁrst stage butterﬂy operators must be permuted in accordance to
bit-reversal order, (II) the permutations in the following stages are calculated as even/odd order of
operators taken from the directly preceding stage (c.f. Fig. 8).
Involutory Parametric Orthogonal Transforms of Cosine-Walsh Type
415

only the lattice structures for the general case of DCWTs. It is clear that involutory
DCWTs can be realized with aid of the same structures if only the proper dependencies
regarding the coefﬁcients would be taken into account. Hence, the values of the coef-
ﬁcients of butterﬂy operators can be always stored in the convenient way in global or
texture memory depending on the variant of the experiment. The results obtained with
G200 microarchitecture for the following variants of experiments, i.e.: (I) lattice
structure of type I, (II) lattice structure of type I using texture memory, (III) lattice
structure of type II, (IV) lattice structure of type II with texture memory and (V) lattice
structure of type II using texture memory and data mixing are collected in Tables 1 and
2. The representative results obtained for Maxwell microarchitecture can be found in
Table 3.
Fig. 8. The lattice structure of type II for fast calculation of DCWT with size of input data N = 8
points.
Fig. 9. The structure of kernel function written in pseudocode.
416
D. Puchala

Fig. 10. The applied scheme of data rearrangement using shared memory buffer for better
alignment of global memory accesses within warps of threads.
Table 1. The experimental results for DCWT calculated with lattice structure of the ﬁrst type for
G200 microarchitecture.
Basic implementation
n
10
11
12
13
14
15
t [ls] 56
99
208
340
646
1.2103
n
16
17
18
19
20
21
t [ls] 2.6103 5.3103 11103 23103 49103 102103
Implementation exploiting texture memory
n
10
11
12
13
14
15
t [ls] 53
90
198
345
635
1.1103
n
16
17
18
19
20
21
t [ls] 2.5103 5.1103 10103 22103 47103 98103
Involutory Parametric Orthogonal Transforms of Cosine-Walsh Type
417

An analysis of results collected in Tables 1 and 2 shows that the proper alignment
of data is of key importance in the case of older NVIDA GPU microarchitectures. By
comparing the results obtained with “type I & texture memory” and “type II & texture
memory & data mixing” variants of the structures we would see almost 45% decrease
of the time of calculations obtained with the structure of type II. It should be noted that
all of the considered improvements gave positive effects. For example an application of
the texture memory alone allowed to gain 4% to 18% acceleration. It gives an
immediate answer to the ﬁrst question.
In the case of modern Maxwell architecture the situation looks radically different.
Here, in the experiments involving all of the considered variants of lattice structures the
obtained results were identical if we only neglect the typical and minor deviations
resulting from the different initial states of the computational machine. This is the
reason why only one set of the representative results is given in Table 3. Hence, the
answer to the second question is: in the case of the considered variants of calculations
involving typical lattice structures for the fast computation of Fourier-like transforms
the way of data alignment is of marginal practical importance for modern GPU
microarchitectures.
Table 2. The experimental results for DCWT calculated with lattice structure of the second type
for G200 microarchitecture.
Basic implementation
n
10
11
12
13
14
15
t [ls] 27
61
114
263
498
994
n
16
17
18
19
20
21
t [ls] 2.0103 4.2103 8.9103 18103 39103 82103
Implementation exploiting texture memory
n
10
11
12
13
14
15
t [ls] 25
54
97
226
419
820
n
16
17
18
19
20
21
t [ls] 1.7103 3.5103 7.3103 15103 32103 67103
Impl. using texture memory & additional mixing within
warps
n
10
11
12
13
14
15
t [ls] 25
52
90
204
360
681
n
16
17
18
19
20
21
t [ls] 1.3103 2.8103 5.8103 12103 25103 54103
Table 3. The representative results of DCWT calculation for Maxwell microarchitecture.
Representative results
n
10 11
12
13
14
15
t [ls] 15 17
21
26
35
61
n
16 17
18
19
20
21
t [ls] 97 202 672 1417 3000 6257
418
D. Puchala

6
Summary
In this chapter a novel scheme for constructing fast orthogonal parametric transforms of
cosine-Walsh type with an additional involution property is proposed. The involution
property means that the inverse transform is obtained simply as: UT = U, which is a
desirable feature especially in the case of hardware implementations, where the same
electronic circuit can be used for calculation of both transforms, i.e. forward and
inverse transformations. In order to obtain involutory transforms in the form of lattice
structure of the fast cosine-Walsh type transform the necessary dependencies between
transform parameters are formulated and given in the chapter. Further on the proposed
class of involutory transforms is tested experimentally in terms of their applicability to
data encryption and, moreover, some aspects of their mass-parallel realizations with aid
of GPUs are considered and veriﬁed experimentally. The obtained experimental results
reveal good data concealment properties of the proposed class of transforms which
means that such transformations can be used successfully in the practical tasks, e.g. in
joint compression and encryption of visual data (see [17]). Moreover the practical tests
on mass-parallel realizations of the proposed transforms involving several variants of
lattice structures allow to draw the following important conclusions: (I) the proper care
of data alignment is crucial in the case of older GPU microarchitectures, (II) modern
microarchitectures deal with the problem of data misalignment in the measure which in
the case of performed type of calculations did not allow to notice any essential
differences.
References
1. Harmuth, H.F.: Transmission of Information by Orthogonal Functions. Springer, Heidelberg
(1972)
2. Ahmed, N., Rao, K.R.: Orthogonal Transforms for Digital Signal Processing. Springer,
Heidelberg (1975)
3. Fino, B.J., Algazi, V.R.: Slant Haar transform. Proc. IEEE 62, 653–654 (1974)
4. Cooley, J.W., Tukey, J.W.: An algorithm for the machine calculation of complex Fourier
series. Math. Comput. 19(90), 297–301 (1965)
5. Hartley, R.V.L.: A more symmetrical Fourier analysis applied to transmission problems.
Proc. IRE. 30(3), 144–150 (1942)
6. Rao, K., Yip, P.: Discrete Cosine Transform: Algorithms, Advantages, Applications.
Academic Press, New York (1990)
7. Morháč, M., Matoušek, V.: New adaptive cosine-Walsh transform and its application to
nuclear data compression. IEEE Trans. Sign. Proc. 48(9), 2693–2696 (2000)
8. Agaian, S., Tourshan, K., Noonan, J.P.: Parametric slant-Hadamard transforms with
applications. IEEE Sign. Process. Lett. 9(11), 375–377 (2002)
9. Agaian, S., Tourshan, K., Noonan, J.P.: Parameterisation of slant-Haar transforms. IEE Proc.
Vis. Image Sign. Process. 150(5), 306–311 (2003)
10. Bouguezel, S., Ahmad, O., Swamy, M.N.S.: A new class of reciprocal-orthogonal
parametric transforms. IEEE Trans. Circ. Syst. I Regul. Pap. 56(4), 795–805 (2009)
Involutory Parametric Orthogonal Transforms of Cosine-Walsh Type
419

11. Bouguezel,
S.,
Ahmad,
O.,
Swamy,
M.N.S.:
Image
encryption
using
the
reciprocal-orthogonal parametric transforms. In: IEEE Symposium on Circuits and Systems,
pp. 2542–2545 (2010)
12. Bouguezel, S., Ahmad, O., Swamy, M.N.S.: New parametric discrete Fourier and Hartley
transforms, and algorithms for fast computation. IEEE Trans. Circ. Syst. I Regul. Pap. 58(3),
562–575 (2011)
13. Bouguezel, S., Ahmad, O., Swamy, M.N.S.: A new involutory parametric transform and its
application to image encryption. In: IEEE Symposium on Circuits and System, pp. 2605–
2608 (2013)
14. Puchala, D., Yatsymirskyy, M.M.: Fast parametrized biorthogonal transforms. Electr. Rev.
4, 123–125 (2012)
15. Bhargava, B., Shi, C., Wang, S.Y.: MPEG video encryption algorithms. Multimedia Tools
Appl. 24(1), 57–79 (2004)
16. Puchala, D., Yatsymirskyy, M.M.: Fast parametrized biorthogonal transforms with
normalized basis vectors. Electr. Rev. R. 89(3a), 277–300 (2013)
17. Puchala, D., Yatsymirskyy, M.M.: Joint compression and encryption of visual data using
orthogonal parametric transforms. Bull. Polish Acad. Sci. Tech. Sci. 62(2), 373–382 (2016)
18. Tang, L.: Methods for encrypting and decrypting MPEG video data efﬁciently. In: ACM
Multimedia, pp. 219–229 (1996)
19. Puchala, D., Stokﬁszewski, K., Szczepaniak, B., Yatsymirskyy, M.M.: Effectiveness of fast
Fourier transform implementations on GPU and CPU. Electr. Rev. R.92(7), 69–71 (2016)
20. Yildrin, A., Ozdogan, C.: Parallel wavelet-based clustering algorithm on GPUs using
CUDA. Procedia Comput. Sci. 3, 296–400 (2011)
21. Minasyan, S., Astola, J., Guevorkian, D.: On uniﬁed architectures for synthesizing and
implementation of fast parametric transforms. In: 5th International Conference, Information
Communication and Signal Processing, pp. 710–714 (2005)
22. Puchala, D., Stokﬁszewski, K.: Parametrized orthogonal transforms for data encryption. In:
Computational Problems of Electrical Engineering, no. 2 (2012)
23. Puchala, D.: Comparison of fast orthogonal parametric transforms in data encryption.
J. Appl. Comput. Sci. 23(2), 55–68 (2015)
24. Moreland,
K.,
Angel,
E.:
The
FFT
on
a
GPU.
In:
Proceedings
of
ACM
SIGGRAPH/EUROGRAPHICS Conference on Graphics Hardware, pp. 112–119 (2003)
25. Spitzer, J.: Implementing a GPU-efﬁcient FFT. In: SIGGRAPH Course on Interactive
Geometric and Scientiﬁc Computations with Graphics Hardware (2003)
26. Yatsymirskyy, M.M.: Fast algorithms for orthogonal trigonometric transforms computations.
Lviv Academic Express (1997). (in Ukrainian)
27. NVIDIA’s Next Generation CUDATM Compute Architecture: FermiTM, NVIDIA.
Whitepaper (2009)
420
D. Puchala

On the Asymptotic Methods
of the Mathematical Models of Strongly
Nonlinear Physical Systems
Petro Pukach1, Volodymyr Il’kiv1, Zinovii Nytrebych1,
Myroslava Vovk1(&), and Pavlo Pukach2
1 Department of Mathematics,
Lviv Polytechnic National University, Lviv, Ukraine
ppukach@gmail.com, ilkivv@i.ua, znytrebych@gmail.com,
mira.i.kopych@gmail.com
2 Department of Applied Mathematics,
Lviv Polytechnic National University, Lviv, Ukraine
pavlopukach@gmail.com
Abstract. The approximation methods to investigate the dynamical processes
in the strong nonlinear single- or multi-degree physical systems are studied in
the chapter. The developed procedure of the investigation of the oscillations in
the nonlinear systems with the concentrated mass can solve not only the analysis
problems but the problems of the physical oscillation system synthesis on the
projective phase choosing such system characteristics enabled the resonance
phenomena as well. There is described the real application of the obtained
results to analyze the technical systems using to protect the equipment from the
vibration load.
Keywords: Mathematical model  Approximation method  Physical system
Resonance  Dynamical process
1
Introduction
The mathematical modeling is the important tool to solve the most of the scientiﬁc and
engineering problems. It also helps to understand the kernel of the processes that are
investigating in the modern physics, to interpret the known phenomena and to predict
the new ones.
The analysis and optimization of the physical, chemical dynamical processes in the
technical systems in the cases of high speed, high pressure and energy demands the
investigation of the complicated processes in the nonlinear oscillation theory.
The strong nonlinear oscillation physical systems are widely applied in the engi-
neering. They are well studied in the case while the ordinary linear or with the partial
derivatives differential equations are used as the mathematical models.
In the ﬁrst case these equations describe the processes in the systems with the
concentrated masses, in the second one in the system with the distributed parameters.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_30

Moreover, for the linear computing models of the oscillations physical systems
basing on the superposition principle are developed the analytical methods in virtue of
which one can state the reaction on the one- or multi-frequency periodical perturba-
tions. Quasilinear systems i.e. systems with the small maximum values of the nonlinear
forces comparing to the linear component of reducing force are similar to the linear
systems. The analytical investigating methods for the most of the classes of the
quasilinear systems are based on the different modiﬁcations of the perturbation methods
in particular asymptotic methods of the nonlinear mechanics. Therefore is possible to
describe and to state the special attributes of the oscillations in the quasilinear systems
that are not proper to the linear systems with the constant in time physical-mechanical
characteristics. One can speak ﬁrstly about resonance phenomena [1, 2] ﬁrstly (main
and combinational), process stability [3–5] secondly.
As to the more complicated systems, namely the strong nonlinear systems one can
use the analytical investigating methods only in some cases. The nonlinear systems
with the reducing force described by the power or close to power nonlinearity can be
treated to this case [6–8].
In reference to non-autonomous strong nonlinear systems, i.e. the systems under the
external periodic perturbation (even small on value), it is necessary to realize the additional
investigation. Their characteristics can be described solely. The general procedure of the
investigating some important classes of the nonlinear physical systems with strong non-
linearity, in particular, the procedure to study the resonance phenomena is developed. The
restoring forces can’t be linearized in such type systems. Single-degree and multi-degree
systems are considered. They are studied under perturbation. The perturbation is such that:
it can be described by the analytical periodic on time variable functions; and the maximum
perturbation value is small comparing to the maximum value of the reducing force.
Concerning to the unperturbed systems, the reducing force is described by close to the
power dependence and ensures the periodic processes. Such constraint may conﬁne the
classes of the physical (mechanic) systems that can be properly analyzed by this proce-
dure. However, for the most cases the nonlinear reducing forces of the autonomous type
with the sufﬁcient exactness degree can be approximated by the discussed dependence,
thus the procedure can be successfully used for the wide class of the nonlinear systems.
This approach is the feasible one (numerical-analytical) to solve the stated problem.
The main issue of the developed in the chapter procedure of the investigating the
resonance phenomena in the discussed class of the systems is based on idea to use the
special periodic Ateb-functions constructing the solutions of the unperturbed analogue
systems and to adopt the general ideas of the perturbation methods – in the case of the
non-autonomous systems.
2
The Mathematical Models of the Strong Nonlinear
Single-Degree Physical Systems
In this section there is described the procedure of the investigation of the oscillations in
the systems with one degree freedom while the reducing force is approximated close to
the power law. In this case the differential equation that describes such oscillations can
be written as
422
P. Pukach et al.

m€x þ cxm þ 1 ¼ ef x; _x; h
ð
Þ;
ð1Þ
where m is the mass of the material point; x is its coordinate at the arbitrary time
moment; FðxÞ ¼ cxm þ 1 is function, describing the main part of the reducing force; c,
v are constants, provided m þ 1 ¼ 2p þ 1
2q þ 1 (p; q ¼ 0; 1; 2; . . .); f x; _x; h
ð
Þ is analytical 2p –
periodic on h ¼ lt function, l is frequency of the external periodic perturbation acting
on the system; e is the small parameter indicating on the small value of the external
periodic perturbation and inconsiderable deviation of the restoring force from the
power law. The restrictions concerning the parameter value v are caused by the sym-
metry with respect to the coordinate origin of the function FðxÞ ¼ cxm þ 1, describing the
restoring force. More general representation of the reducing nonlinear force in the form
F x
ð Þ ¼ c xj jpx always with the necessary exactness power can be approximated by the
upper written dependence, thereby all obtained results, one can transfer also on the
systems with such type restoring force. Let’s notice, that function ef x; _x; h
ð
Þ can
describe the nonlinear forces with the maximum value being the small quantity com-
paring to the maximum value of the restoring force FðxÞ ¼ cxm þ 1, that is
F x
ð Þ  max ef x; _x; h
ð
Þ. The last inequality can use the general ideas of the perturba-
tion methods to construct the solution of the nonlinear Eq. (1). As per usual, it is
necessary to describe the dynamical process of the unperturbed (e ¼ 0) or generating
system, corresponding to (1). The next nonlinear equation is its analogue
m€x þ cxv þ 1 ¼ 0:
ð2Þ
In spite of the simplicity of the solution form, and the fact that the amplitude and
the frequency of vibration are exactly obtained, difference between the approximate
solution and the exact numerical one may be signiﬁcant. It was the reason that the exact
solution of (2) in the form of cosine Ateb periodic function being an inversion of the
incomplete Beta function [9] is introduced. The Ateb-functions are more complex than
the trigonometric functions and, at the same moment, are not familiar to engineers and
technicians who have to use them [10–14]. Usually, simple approximation of
Ateb-functions by elementary functions is proposed, in particular, in [9–14].
The solution of the Eq. (2) is represented via periodic Ateb-functions as
x ¼ aca v þ 1; 1; x tð Þ þ w
ð
Þ;
ð3Þ
Where a is the amplitude, x tð Þ þ w is the oscillation phase of the unperturbed motion,
xðaÞ is the oscillation frequency equals
x a
ð Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
c v þ 2
ð
Þ
2m
r
a
v
2:
ð4Þ
Hence, even for the unperturbed motion natural frequency of the dynamical process
depends on amplitude. This is the ﬁrst conceptual difference between the strong
nonlinear and quasilinear systems. Thus, in case of the unperturbed motion natural
frequency xðaÞ depends on the mass of the material body and on the proportionality
On the Asymptotic Methods of the Mathematical Models
423

constant in the restoring force (analogue constant inelasticity), and on the amplitude
and the nonlinear parameter v also. This is the main difference of the considered class
of the systems from the linear one. The issue is to construct the solutions of such strong
nonlinear systems, taking into account all upper dynamical peculiarities, and to answer
on the main questions of the investigating of the resonant oscillations.
Figure 1 demonstrates the dependence of the frequency xðaÞ on the parameters
deﬁning the dynamical process of the unperturbed motion, Fig. 2 demonstrates the ratio
of the periods of natural nonlinear to the linear v ¼ 0 oscillations on the amplitude and
on the parameter v, i.e. g ¼ P 1;v þ 1
ð
Þ
x a
ð Þ
. The parameters a and w for the unperturbed
motion are constants and are deﬁned from the initial conditions x 0
ð Þ ¼ x0, satisfying
the relationships
x0
a

v þ 2
þ
V0 v þ 2
ð
Þ
2ax a
ð Þ

2
¼ 1;
cta v þ 1; 1; w
ð
Þ ¼ 2x a
ð Þ
v þ 2 :
Figures 1 and 2 demonstrate:
(1) for the systems with small stiffness 1\m\0 the oscillation natural frequency
of the physical system for the greater amplitude’s values is less; (2) while the nonlinear
parameter v converges to zero process in the system is considered as similar to the
isochronous; (3) for the systems of the considerable stiffness m  0 and for the
oscillation amplitude values less than 1 for the greater values of nonlinear parameter
Fig. 1. Natural frequency’s dependence on the amplitude and the small nonlinear parameter v
424
P. Pukach et al.

v oscillation natural frequency is less; for the oscillation amplitude values greater than 1
for its greater values correspond the greater values of the natural frequencies; (4) at the
equal values of the parameters c and m the oscillation period of the strong nonlinear
system is greater than the oscillation period of the system’s linear analogue 1\m\0
and a\1 and less at m [ 0 i a\1. It is necessary to take into account all these
peculiarities in the resonance phenomena investigation.
3
Resonance Phenomena in the Strong Nonlinear Physical
Systems with the Discrete Structure
To develop the investigating methods for the reaction of the strong nonlinear system on
the periodic non-autonomous type perturbations it is necessary to study the resonance
conditions considering Eq. (2) as the model. The classic deﬁnition of the resonance
phenomena can be applied for the considered systems noting the difference, that
concept “frequency” treating as the meaning of the function xðaÞ, is wider, than in the
quasilinear systems. At ﬁrst, the oscillation natural frequency of the system is deﬁned
not only by the physical and mechanical characteristics (parameters m; c), but also by
the amplitude a; secondly, the periodic process of the unperturbed motion is described
by the periodic Ateb-functions. The period 2P for the last ones by the argument
w ¼ xðaÞt depends on the nonlinear parameter m and is deﬁned by the relationship
P ¼ P 1; m þ 1
ð
Þ ¼
ﬃﬃﬃp
p C
1
m þ 2


C
1
2 þ
1
m þ 2

 :
ð5Þ
Fig. 2. The ratio of the oscillation period of the nonlinear model to the oscillation period of the
linear model
On the Asymptotic Methods of the Mathematical Models
425

On account of this for the considered system class one can accept the derivative
from the classic meaning instead of the main resonance, in other words – the natural
oscillation period coincides with the perturbation period. Then the resonance condition
transforms as
p
l ¼ P 1; m þ 1
ð
Þ
xðaÞ
:
ð6Þ
Hereafter the parameter value l means the frequency of the external periodic
perturbation (force). Taking into account the function, describing “natural frequency”,
one can obtain the parameter value a, responsible for the resonance phenomena:
a ¼
P 1; m þ 1
ð
Þ
p
l
k
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2
m þ 2
r
 
!2
m
:
ð7Þ
where k ¼
ﬃﬃﬃc
m
p . Thus, the resonance phenomena in the investigated dynamical systems
appears when the oscillation amplitude is close to a. If the amplitude is greater than a,
then existing resistant forces in the real systems produce the decrease of the oscillation
amplitude to the value a, causing the resonance phenomena in the future.
This explains the second principle difference of the resonance phenomena in the
strong nonlinear systems. If there exist one or several stationary oscillation modes in
the perturbed strong nonlinear system, then the investigation of the physical system
reaction on the periodic perturbation must be considered separately. Figure 3 demon-
strates the dependence of the resonance amplitude on the parameters, that characterized
the physical and mechanical system properties and forced periodic perturbation
frequency.
In particular, from the obtained in the chapter plotted dependences follows, that for
the «soft» systems (1\m\0) the resonance phenomena is possible in the case, when
the natural frequency oscillation of the system linear analogue is less than the fre-
quency of the perturbing force; while the parameter m tends to 1 the resonance
amplitude a extremely increases. In the case of the «rigid» systems (m [ 0) the res-
onance phenomena is possible when the frequency of the linear analogue of the system
is greater than the frequency of the perturbing force; for the greater values of the
nonlinear system parameter and the frequency of the perturbing force the resonance
amplitude is greater. Moreover, the results of the chapter allow to state: if in the
physical system there are oscillations with the amplitude less than a, then the periodic
forces of the limited value and arbitrary frequency can’t cause the resonance oscilla-
tions value and arbitrary frequency can’t cause the resonance oscillations in it.
Simultaneously it is stated, that for the greater stiffness system the amplitude threshold
value is greater. Under the condition that the frequency of the external periodic per-
turbation l ¼ 8 s1 the increase of the nonlinear measure m from 0; 4 to 0; 8 produces
the increase of the amplitude threshold from 0; 012 m to 0; 04 m. For the greater
frequency values of the perturbation force (with all constant parameters) the amplitude
threshold value is greater. While the frequency of the external periodic perturbation
426
P. Pukach et al.

l ¼ 8 s1 and m ¼ 2
3 the amplitude threshold value equals 0; 04 m, while l ¼ 15 s1
and m ¼ 2
3 equals 0; 1 m.
4
The Mathematical Models and Resonance Phenomena
in Strong Nonlinear Physical Oscillation Multi-degree
Systems
The analytical investigation of the dynamical processes in the strong nonlinear
multi-degree systems is more complicated, than for their quasilinear analogues or even
for the strong nonlinear single-degree systems. It is explained by the fact that the
principle of the oscillation superposition can’t be realized, hence the analysis of the
dynamical process based on the partial solutions or on the application of the numerical
methods of the integration of the corresponding differential equations. Besides, the
main properties of the generating system and perturbations must be taken into account
independently from the investigating methods. As examples of the strong nonlinear
systems to applicate the analytical methods for the construction of the solutions cor-
responding mathematical models can be the systems where the potential energy is
deﬁned as follows
P x1; x2; . . .; xn
ð
Þ ¼
X
n
i;j¼0
cij xi  xj

m þ 2:
ð8Þ
In the relationship (8) x1; x2; . . .; xn are the generalized coordinates, cij, m are con-
stants, and m is deﬁned in the same form as upper. Thus, the potential energy in the both
Fig. 3. The dependence of the resonance amplitude on parameters m [ 0 and Z ¼ l
k
On the Asymptotic Methods of the Mathematical Models
427

cases is the homogenous function on the generalized coordinates. Taking into account
the fact, that the kinetic energy as the function of the generalized speeds, is deﬁned by
the relationship
T _x1; _x2; . . .; _xn
ð
Þ ¼ 1
2
X
n
i¼1
mi_x2
i ;
ð9Þ
where mi is the mass of i point, the differential equations describing the dynamical
process of the unperturbed motion are the next
mi€xi þ m þ 1
ð
Þ
X
n
j¼0
cij xi  xj

m þ 1 ¼ 0:
ð10Þ
In spite of the strong nonlinearity of the Eq. (10), the dynamical process in the
corresponding physical system can be represented via the periodic Ateb-functions.
That’s why formally suppose, that the phase coordinates are connected by the relations
xi ¼ bix1;
i ¼ 2; 3; . . .; n;
ð11Þ
where bi are the unknown constants. The physical explanation of the substitution (11)
is the next: in the strong nonlinear system there are oscillations that coincide by the
form with the ﬁrst generalized coordinate.
After the substitution of the variables (11) to describe the “normal” forms of the
oscillations one can get the differential equations
mi€xi þ m þ 1
ð
Þ
X
n
j¼0
cij bi  bj

m þ 1 ¼ 0; i ¼ 2; 3; . . .:; n;
ð12Þ
where the unknown coefﬁcients bi are deﬁned from the system of the algebraic
equations
bi
X
n
j¼0
cij 1  bj

m þ 1 ¼
X
n
j¼0
cij bi  bj

m þ 1:
ð13Þ
The solutions of the differential equations (12) can be written via the periodic
Ateb-functions in the form
x1 ¼ aca m þ 1; 1; xðaÞt þ h
ð
Þ;
xi ¼ abica m; 1; xðaÞt þ h
ð
Þ; i ¼ 2; 3; . . .:; n:
At the same time the unknown coefﬁcients bi are related by the system of the
nonlinear algebraic equations (13).
428
P. Pukach et al.

5
The Examples of the Application of the Developed Methods
for the Investigation of the Mathematical Models
of the Strong Nonlinear Vibro-Protection Systems.
The Numerical Simulation
A. The resonance phenomena in the quasi-zero stiffness vibro-protection
systems, that can be modeled by the ordinary differential equations
In the last decade there is very popular antivibrating procedure, using the systems,
known as quasi-zero stiffness vibro-protection systems [15]. In spite of the wide
spectrum of such type systems, their mathematical models are described by the ordi-
nary one-type differential equations with strong nonlinearity, exactly
m€x þ a_x þ bx þ cx3 ¼ f tð Þ:
ð14Þ
In the Eq. (14):
• m is the mass of the body under the action of the vibrating load, caused by the
motion of the base in accordance with the law g tð Þ. The function f tð Þ is
f ðtÞ ¼ m€gðtÞ;
• a is the constant of proportionality in the resistant force, is taken for the sake of
simplicity proportional to the motion speed of the body;
• b, c are constants deﬁned by stiffness of the system [8];
• the function gðtÞ and f ðtÞ too, are periodic on time and the maximum value of the
periodic perturbation is small comparing to the restoring force, i.e.
maxf tð Þ  max bx þ cx3


:
Let note, that the constant c can be the positive or negative, and can be equal to zero
also. The numerical integration and the respective analysis of the results based on it
was realized for some cases of the considered equation in [16]. To investigate the
dynamical processes of the considered vibro-protection system is effectually to use the
results of the Sects. 2 and 3 in this chapter. In accordance with them the ﬁrst-order
approximation of the non-resonant process is described by the relationship
x tð Þ ¼ aca 3; 1;
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2b
m at þ w
r
 
!
;
where the parameters a and w are determined from the system:
_a ¼ 
2a
mP 1; 3
ð
Þ
C 3
2
 
C 1
4
 
C 7
4
 
;
_w ¼
cﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2mb
p
C 3
4
 
ﬃﬃﬃp
p C 1
4
 
ﬃﬃﬃp
p C 3
4
 
C 5
4
 
a:
ð15Þ
On the Asymptotic Methods of the Mathematical Models
429

The resonance oscillations on the frequency l appear when the amplitude is close to
a ¼ 0:425l
ﬃﬃﬃ
m
b
q
and can be described by some system of the differential equations and
numerically found.
B. The resonance phenomena in the quasi-zero stiffness vibro-protection
systems modeled by the system of two ordinary differential equations
To protect the equipment from different type vibrations one can apply so-called
two-degree quasi-zero stiffness systems, their mathematical models are represented as
the differential equation system [15] in the form
€x1 þ c11x3
1 þ c12 x1  x2
ð
Þ3¼ c13x1  a1_x1;
€x2 þ c21x3
1 þ c22 x1  x2
ð
Þ3¼ c23x1  a2_x2:
ð16Þ
Let’s consider the case, when the both bodies are of the same mass, and the resistant
force and the previous deformations of the elastic elements are small. It possible to
state, that the maximum values in the right sides of the Eq. (16) are small, and therefore
for their investigation one can use the general ideas of the perturbation procedure. So,
ﬁrstly let’s describe the dynamical process of the generating systems as follows
€x1 þ c11x3
1 þ c12 x1  x2
ð
Þ3¼ 0;
€x2 þ c21x3
1 þ c22 x1  x2
ð
Þ3¼ 0:
ð17Þ
The aim is to ﬁnd the solutions (17) in the form
x1 ¼ aca 3; 1; x a
ð Þt þ h
ð
Þ;
x2 ¼ abca 3; 1; x a
ð Þt þ h
ð
Þ:
ð18Þ
To ﬁnd the unknown parameter b using the system (17), considering (18), let write the
next algebraic equation of the type (13)
b4 þ d1b3  d2b  1 ¼ 0;
ð19Þ
where d1; d2 are known constants expressed via the coefﬁcients of the system (17).
Using (18), one can get the solutions (17).
After deﬁning the normal oscillations forms of the unperturbed system (17), further
is proposed the ﬁrst-order approximation of the perturbed equations system (16).
Accordingly to the general method of perturbation theory for the nonlinear oscillation
systems, the relationship (18) also can be accepted as the solution of the system (16),
where the parameters a and h are the functions on time. To ﬁnd them there is the next
differential equation system
_a ¼
ea
2px a
ð Þ
Z2p
0
k 1 þ b
ð
Þca 3; 1; lw
ð
Þsa 1; 3; lw
ð
Þdw ¼ 0;
430
P. Pukach et al.

_h ¼
ea
2pxðaÞ
Z2p
0
kð1 þ bÞca2 3; 1; lw
ð
Þdw ¼ e0; 457
xðaÞ kð1 þ bÞ;
where k ¼ k1 ¼ k2, l ¼
C 1
4ð Þ
ﬃﬃp
p C 3
4ð Þ  1:67. The amplitudes of the normal modes of oscil-
lations in the considered system in the ﬁrst approximation of the asymptotic decom-
position stay the constants because of conservativity of the system. The frequencies of
the perturbed oscillations Xs (s ¼ 1; 2; 3; 4) depend on the amplitude and are deter-
mined by the relationship Xs ¼ x a
ð Þ þ e0:457
x a
ð Þ k 1 þ bs
ð
Þ, where
x a
ð Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
p
2 d3 þ d4 1 þ bs
ð
Þ3


a
r
;
d3; d4 are known constants, and bs are deﬁned according to (19).
Figure 4 demonstrates the dependence of the period of the normal form oscillations
on the amplitude at the different values of the system coefﬁcients (16).
From the obtained graphs it follows:
(1) while the amplitude increases the period of all normal forms oscillations of the
considered conservative system decreases;
(2) at the same amplitude values the oscillation period is the maximum for the case of
the motion in one phase.
Fig. 4. The graphs of the dependence of the normal form oscillation periods on the amplitude
On the Asymptotic Methods of the Mathematical Models
431

Figure 4 demonstrates the dependence of the period of the normal form oscillations
on the amplitude at the different values of the system coefﬁcients (16).
From the obtained graphs it follows:
(1) while the amplitude increases the period of all normal forms oscillations of the
considered conservative system decreases;
(2) at the same amplitude values the oscillation period is the maximum for the case of
the motion in one phase.
6
Conclusions
There is developed the procedure to investigate the dynamical processes in the strong
nonlinear oscillation single- and multi-degree physical systems. The attribute of the
considered systems is the next:
• the oscillation process of the unperturbed analogues can be described via the special
periodic Ateb-functions;
• the frequency (period) of this process depends on the amplitude.
The main difﬁculty of the investigation of the periodic forces action on the process
is tied with the last fact. That is why the asymptotic integration procedure (solution
construction) of the according strong nonlinear the second-order differential equations
of the non-autonomous type is developed. The standard equations describing the
changes laws of determining parameters of the dynamical process are obtained for the
both non-resonant and resonance cases based on the developed procedure. The next
results are obtained for the resonance case.
1. There are built the domains of the resonant (non-resonant) oscillations of the strong
nonlinear systems at the arbitrary frequency value of the periodic perturbation.
2. There is stated:
• in the case, when the resonance amplitude threshold is greater than its initial
value, but less than stationary amplitude value, then the system during some
time period would be in the resonance zone. The resonance process is very
short-time and after it the dynamical process approximates to the stationary
dynamical oscillation mode;
• in the case, when the resonance amplitude threshold is less than its initial value,
but greater or equal than stationary amplitude value, then the system during
some time period would be in the resonance zone. The resonance process is very
short-time After the exit from the resonance zone, due to the strong stability of
the stationary oscillation mode once more starts the resonance phenomena. The
process repeats until the periodic perturbation is acting.
3. There is developed the procedure of the investigation oscillations of the so-called
systems with quasi-zero stiffness, used to protect the equipment from the vibration
load. There are obtained the resonance conditions for such type systems and also are
described the laws of the resonant and non-resonant amplitude changes.
432
P. Pukach et al.

The proposed method of the investigating the oscillation processes of the strong
nonlinear physical discrete structure systems helps to solve not only the analysis
problems, but also the important problems of the oscillation systems synthesis on the
projective phase, and to choose such dynamical systems characteristics that make the
resonance phenomena impossible.
References
1. Bogolyubov, N.N., Mitropol’skii, Y.A.: Asymptotic Methods in the Theory of Nonlinear
Oscillations [Asimptoticheskiye metody v teorii nelineynykh kolebaniy]. Nauka, Moscow
(1974). (in Russian)
2. Grebennikov, E.A., Ryabov, Y.A.: Constructive Methods for the Analysis of Nonlinear
Systems [Konstruktivnyye metody analiza nelineynykh sistem]. Nauka, Moscow (1979). (in
Russian)
3. Bellman, R.: A Theory of the Stability of Solutions of Differential Equations [Teoriya
ustoychivosti resheniy differentsial’nykh uravneniy]. Foreign Literature Publishing House,
Moscow (1954). (in Russian)
4. Perestyuk, M.O., Chernihiv, O.S.: Theory of the Stability [Teoriya stiykosti]. Publishing
House “Kyiv”, Kyiv (2002). (in Ukrainian)
5. Kyrychenko, Y.O., Samusya, V.I., Kyrychenko, V.Y., Romanyukov, A.V.: Experimental
investigation of aero-hydroelastic instability parameters of the deepwater hydrohoist
pipeline. Middle East J. Sci. Res. 18(4), 530–534 (2013)
6. Nayfeh, A.H.: Perturbation Methods [Metody vozmushcheniy]. Mir, Moscow (1976). (in
Russian)
7. Pukach, P.Y., Kuzio, I.V.: Nonlinear transverse vibrations of semiinﬁnite cable with
consideration paid to resistance [Nelíníyní poperechní kolivannya napívneobmezhenogo
kanata z urakhuvannyam oporu]. Naukovyi Visnyk Natsionalnoho Hirnychoho Universitetu,
No. 3, pp. 82–86 (2013). (in Ukrainian)
8. Pukach, P.Y.: Investigation of bending vibrations in Voigt-Kelvin bars with regard for
nonlinear resistance forces. J. Math. Sci. 215(1), 71–78 (2016)
9. Cveticanin, L., Pogány, T.: Oscillator with a sum of noninteger-order nonlinearities. J. Appl.
Math. 2012, 1–20 (2012)
10. Andrianov, I., Awrejcewicz, J.: Asymptotic approaches to strongly non-linear dynamical
systems. Syst. Anal. Model. Simul. 43(3), 255–268 (2003)
11. Cveticanin, L.: Pure odd-order oscillators with constant excitation. J. Sound Vib. 330(5),
976–986 (2011)
12. Awrejcewicz, J., Andrianov, I.V.: Oscillations of non-linear system with restoring force
close to sign(x). J. Sound Vib. 252(5), 962–966 (2002)
13. Gendelman, O., Vakakis, A.F.: Transitions from localization to nonlocalization in strongly
nonlinear damped oscillators. Chaos, Solitons Fractals 11(10), 1535–1542 (2000)
14. Cveticanin, L., Kovacic, I., Rakaric, Z.: Asymptotic methods for vibrations of the pure
non-integer order oscillator. Comput. Math Appl. 60(9), 2626–2628 (2010)
15. Pukach, P.Y., Kuzio, I.V.: Resonance fenomena in quasi-zero stiffness vibration isolation
systems. Naukovyi Visnyk Natsionalnoho Hirnychoho Universitetu, No. 3, pp. 62–67
(2015)
16. Hayashi, T.: Nonlinear Oscillations in Physical Systems [Nelineynyye kolebaniya v
ﬁzicheskikh sistemakh]. Mir, Moscow (1968). (in Russian)
On the Asymptotic Methods of the Mathematical Models
433

Solution of the Discrete Ill-Posed Problem
on the Basis of Singular Value Decomposition
and Random Projection
Elena G. Revunova(&)
Department of Neural Information Processing Technologies,
International Research and Training Centre for Information
Technologies and Systems, Kiev, Ukraine
egrevunova@gmail.com
Abstract. A brief overview of our work on the solution of DIP is presented.
The stable solution of DIP we obtained, by truncated singular value decom-
position and by random projection methods. Analytic and experimental aver-
aging over random matrices for the evaluation of the error of true signal
recovery is carried out for the method of solving the discrete ill-posed problems
on the basis of random projection. Averaging over random matrices leads to
diagonalization of the matrix conditioning both components of the error (de-
terministic and stochastic). The values of the diagonal elements change mono-
tonically as a function of k. This in turn leads to the smoother characteristics and
reducing the number of local minima. The results of the experimental study
showed the connection of the elements of the diagonalized matrix with the
singular values of the original matrix. This provides the basis for investigating
the connection of the truncated singular value decomposition method and the
random projection method.
Keywords: Ill-posed problem  Singular value decomposition
Random projection  Averaging over random matrices
1
Introduction
Discrete ill-posed problems (DIP) often arise when the signals are reconstructed based
on the results of indirect measurements [1–3].
To overcome the instability and hence to improve the accuracy of solutions, reg-
ularization is used [3–5]. Regularization imposes some restrictions on the solution that
improve its stability - for example, the small l2-norm of the solution. The classic
method of regularization is Tikhonov regularization [4]. Defects inherent in the
methods of solving discrete ill-posed inverse problems based on Tikhonov regular-
ization, are the computational complexity and the difﬁculties in selecting the proper
regularization parameter on which largely depends the stability of solutions. Therefore,
alternative approaches to the inverse discrete incorrect problem are needed with the
accuracy on the level of Tikhonov regularization.
The stable solution of DIP can be obtained, for example, by truncated singular value
decomposition (SVD) [5–8] or by random projection (RP) [9–12] methods. RP is a
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_31

variety of methods for the formation of neural network distributed representations [13–
17]. Recently, there has been an increased interest in using RP as a regularizer [18, 19].
It is of interest to study and compare the error of the solution obtained by these two
methods. In this paper, the error averaging over an ensemble of random matrices is
performed analytically for the method based on random projection. The connection
between the error of the methods of truncated singular value decomposition and ran-
dom projection is investigated. A brief overview of our work on the solution of DIP is
also presented.
2
Solution of DIP Using the Methods of Truncated Singular
Value Decomposition and Random Projection
The problem of estimating a signal vector x 2 ℜn for the model y = Ax + e, where the
matrix A 2 ℜnn and the vector of measurements y 2 ℜn (y = y0 + e) are known, if
the singular values of the matrix A gradually decrease to zero, belongs to the class of
discrete ill-posed problems [5]. Stable solution of DIP can be obtained by using the
truncated SVD [5–8] or the RP method [9–12]. The vector of estimation x* based on
the truncated SVD is obtained by the following linear model [6]:
x
k SVD ¼
X
k
i¼1
vis1
i uT
i y;
ð1Þ
where ui 2 ℜn are the left singular vectors, vi 2 ℜn are the right singular vectors and si
are the singular values of the matrix A. Note that
Ak ¼
X
k
i¼1
uisivT
i :
ð2Þ
For the solution based on random projection both sides of the initial equation are
multiplied by a random matrix Rk 2 ℜkn: RkAx = Rky, and then the estimation vector
x* is obtained by the following linear model [10]:
x
k R ¼
X
k
i¼1
hirT
i y;
ð3Þ
where ri 2 ℜn is a column of the matrix Rk = [r1, …, rk], elements of this matrix are
realizations of a standard Gaussian random variable; hi 2 ℜn is a column of the matrix
(RkA)+ = [h1, …, hk].
The expected squared error of the x recovery is calculated as
eðkÞ ¼ Ef x  x
k

2g;
ð4Þ
Solution of the DIP on the Basis of Singular Value Decomposition
435

where E{} is the expectation with respect to the noise in vector of measurements y,
and xk
* is the recovered signal vector obtained by (1) or (3) with y containing the
respective noise realization.
In [8, 12], the expression for mean squared error of x recovery was obtained for the
SVD method as
eSVDðkÞ ¼
ðA þ
k Ak  IÞx

2 þ r2traceðA þ T
k
A þ
k Þ;
ð5Þ
and the error components are
eSVD dðkÞ ¼
ðA þ
k Ak  IÞx

2¼ xTx  xTVkVT
k x;
ð6Þ
eSVD sðkÞ ¼ r2traceðA þ T
k
A þ
k Þ ¼ r2traceðUkS2
k UT
k Þ;
ð7Þ
where eSVD d is the deterministic component of the error, eSVD s is the stochastic one,
and r2 is the variance of the noise.
In [9, 10], the expression for mean squared error of x recovery was obtained for RP:
eRðkÞ ¼
ððRkAÞ þ RkA  IÞx

2 þ r2traceððRkAÞ þ TðRkAÞ þ Þ;
ð8Þ
and its components are
eR dðkÞ ¼
ððRkAÞ þ RkA  IÞx

2; eR sðkÞ ¼ r2traceððRkAÞ þ TðRkAÞ þ Þ;
ð9Þ
where eR d is the deterministic component of the error and eR s is the stochastic one.
Experimental studies [10, 12] have shown that there exists an optimal number of
the components of linear models (1) and (3), that minimizes the error (4). The optimum
exists because the true signal recovery error can be represented as a sum of two
components, one of which (deterministic) decreases with the increasing number of
model components (the model dimensionality), and the other (stochastic) grows and is
proportional to the noise level in the measurement vector [9–12]:
eðkÞ ¼ edðkÞ þ esðkÞ ¼ edðkÞ þ r2egðkÞ;
ð10Þ
where ed(k) is the value of the deterministic error component for the model of
dimensionality k, es(k) is the value of the stochastic error component for the model of
dimensionality k, eg(k) = es(k)/r2. Thus, at a certain noise level, the global minimum of
the error can be achieved at 1  k  n.
The representation of the error in the form (10), the study of the error components
and the development of the model selection criteria (MSC) are the techniques used by
the inductive modeling approach [20–22] to ﬁnd the optimal solution. In practice, it is
impossible to calculate the recovery error ex(k) due to the lack of information about x,
therefore it is impossible to determine the optimal k directly. For the choice of k close to
the optimal, the MSC, that is, a function that would have an extremum at k close to or
equal to the optimal one [20, 23–25], is used.
436
E. G. Revunova

As the basis for the development of MSC for DIP we use the error of y0 recovery:
ey(k) = E{||y0 – yk
*||2}, where yk
* = Axk
*. Note that although y0 is unknown, it can be
estimated using a known measurement vector y.
When developing a MSC for the DIP solution, we have to:
• present the error in the form of a sum of two components;
• show the increase and decrease of the corresponding error components with the
increase of model dimensionality;
• show that the global minima of the dependencies of the x recovery error and the y0
recovery error on k coincide or at least are close to each other;
• obtain the expression for estimation of the recovery error of y0 using a known
measurement vector y.
A tool to show that the global minima of the dependence of x recovery error and y0
recovery error on k are the same or close is provided by the function
JðkÞ ¼ DedðkÞ
DegðkÞ :
ð11Þ
If, for e(k) represented in the form (10), the value of the noise variance is in the
range J(k + 1) < r2 < J(k), then the k dimensional model provides a minimum of the
recovery error. If J(k) for the x recovery error and the y0 recovery error are the same or
close, then the optimal k values for the recovery errors of x and y0 are also the same or
close.
For solution of DIP by means of truncated SVD, it is possible to obtain analytical
expressions for Ded(k) and Deg(k) and to demonstrate that Des(k) increases and
Ded(k) decreases both for ex(k) and for ey(k). A comparison of the positions of the
minima ex(k) and ey(k) with the use of J(k) then shows that the minima coincide. On the
basis of this fact, a MSC minimizing the error of the DIP solution can be constructed.
Similar criterion can be developed for the DIP solution by means of RP. Although in
this case the minima of ex(k) and ey(k) do not necessarily coincide, numerical experi-
ments show that they are close.
Development of the MSC for truncated SVD and RP methods of DIP solution is
further elaborated in the following sections.
3
The Model Selection Criterion for the DIP Solution Based
on Singular Value Decomposition
The behavior of the components of recovery error for the true signal x and the noiseless
model output y0 as a function of the number k of truncated singular value decompo-
sition components was investigated in [8, 12], and analytical expressions for Ded(k) and
Deg(k) were obtained.
For the deterministic error component ex d(k), the following expression holds:
Solution of the DIP on the Basis of Singular Value Decomposition
437

ex dðkÞ ¼ xTx 
X
k
i¼1
xTvivT
i x ¼ ex dðk  1Þ  xTvkvT
k x:
ð12Þ
Since xTvkvk
Tx > 0, the value of the deterministic component of the error ex d(k)
decreases with increasing k.
From the recursive expression for the stochastic error component:
ex sðkÞ ¼ ex sðk  1Þ þ r2s2
k traceðukuT
k Þ ¼ ex sðk  1Þ þ r2s2
k :
ð13Þ
it follows that its value increases with increasing k.
The expression for the deterministic component of the y0 recovery error has the
form:
ey dðkÞ ¼ ey dðk  1Þ  yT
0ukuT
k y0:
ð14Þ
Since y0
Tukuk
Ty0 > 0, the value ey d(k) decreases with increasing k.
The value of the stochastic component of the y0 recovery error is:
ey sðkÞ ¼ r2traceð
X
k
i¼1
vkvT
k Þ ¼ r2k
ð15Þ
and it increases with the increasing k.
Given that Dex d(k) = xTvkvk
Tx and Dex g(k) = sk
−2trace(ukuk
T) = sk
−2,
JxðkÞ ¼ Dex dðkÞ
Dex gðkÞ ¼ s2
kxTvkvT
k x:
ð16Þ
Let us calculate the ratio Jy(k) for the y0 recovery error. Since Dey
d(k) =
sk
2xTvkvk
Tx and Dey g(k) = trace(vkvk
T) = 1,
JyðkÞ ¼ Dey dðkÞ
Dey gðkÞ ¼ s2
kxTvkvT
k x:
ð17Þ
From the expressions (16) and (17) it is clear that Jx(k) and Jy(k) coincide, that is, for
the truncated singular value decomposition, the position of the minimum error in
recovery of the input x and the output y0 always coincide. Therefore, the dependence of
the y0 recovery error on the number of model components can be used to determine the
optimal number of said components.
In the work [12], the following expression that approximates the y0 recovery error
was obtained:
CRSVD ¼ E ðAkA þ
k  IÞy

2r2traceððAkA þ
k  IÞTðAkA þ
k  IÞÞ
þ r2traceððAkA þ
k ÞTAkA þ
k Þ:
ð18Þ
438
E. G. Revunova

This expression allows to determine the number of components of the truncated
singular value decomposition that is close to optimal. To estimate the noise variance r2,
known methods can be used [26, 27].
Figure 1 shows the plots of the dependence of CRSVD criterion values, average error
of the output signal recovery (e_y_mean), and average error of the true input signal
recovery (e_x_mean) on the number k of truncated singular value decomposition
components for Carasso problem [28]. It can be seen that the dependence curves for the
criterion CRSVD and the average recovery error of y0 are close, and the model selection
criterion CRSVD approximates the output recovery error well. The positions of the
minima of dependencies of x recovery error and y0 recovery error on k coincide.
Further experimental investigation of the developed criterion was carried out in [12].
4
The Model Selection Criterion for the DIP Solution Based
on Random Projection
For the DIP solution method based on RP, the increase of stochastic es(k) and decrease
of deterministic ed(k) error components with increasing k was shown analytically for
ex(k), and experimentally for ey(k). Experimental studies have shown the proximity (but
not complete coincidence) of the positions of the minima of ex(k) and ey(k). The
analytical expressions obtained for the random projection method for Ded(k), Deg(k),
Jx(k), Jy(k), however, do not allow to analytically investigate the closeness of the
positions of ex(k) and ey(k) minima.
The behavior of the dependence of error components on the number of rows k of
the matrix Rk was analytically investigated in [11]. The study is based on the repre-
sentation of the matrix Fk = Rk A as the sum of the original matrix and the perturbation
matrix. The pseudoinverse matrix Fk
+ = (Rk A)+ (that is used to estimate x) can be
represented as a perturbation of the pseudoinverse matrix through the perturbation of
the original matrix as proposed in [29].
1.E-06
1.E-05
1.E-04
1
5
9
13 17 21 25 29 33 37
k
1.E-02
1.E-01
1.E+00
1.E+01
e_y_mean
CR
e_x_mean
1.E-08
1.E-07
1.E-06
1.E-05
1
5
9
13 17 21 25 29 33 37
k
1.E-03
1.E-02
1.E-01
1.E+00
1.E+01
e_y_mean
CR
e_x_mean
Fig. 1. Dependencies of the CRSVD criterion values and the output recovery error on k. For the
noise level of 1e−4 (left) and the noise level of 1e−5 (right).
Solution of the DIP on the Basis of Singular Value Decomposition
439

Based on this representation, recursive expressions for the stochastic and deter-
ministic components of the input signal x recovery error were obtained. These
expressions provide a tool for the study of the behavior (that is, increase and decrease)
of the error components, depending on k. The matrix Fk = Rk A can be represented as
Fk ¼
Fk1
fk


;
ð19Þ
where Fk–1 2 ℜk−1N, row vector fk = rk A, rk is the k-th row of the matrix Rk = [r1,
…, rk]. As a perturbation of the matrix Fk–1, we consider a matrix Ek 2 ℜkN, con-
taining a single non-zero row fk that is added at the k-th step:
F0
k1 þ Ek ¼ Fk;
ð20Þ
Ek ¼
Ok1
fk


; F0
k1 ¼
Fk1
O1


;
ð21Þ
where Ok–1 is zero submatrix of the size (k – 1)  N.
In [11], the recursive expressions for stochastic and deterministic components of
the recovery error are formulated. For the stochastic component of the true signal
recovery error, the recursive expression has the form:
ex sðkÞ ¼ r2traceðF þ T
k
F þ
k Þ ¼ ex sðk  1Þ þ r2traceðMT
k1Mk1Þ þ r2dk;
ð22Þ
where Mk–1 = fk
+fk Fk–1
+ , dk = fk
+Tfk
+. If fk is nonzero, then dk > 0. For nonzero
Mk1; trace MT
k1Mk1


[ 0. Therefore, the value of the stochastic component of the
error increases with increasing k. For the deterministic component of the true signal
recovery error, the recursive expression has the form:
ex dðkÞ ¼ xTx  xTF þ
k Fkx ¼ ex dðk  1Þ  xTKkx;
ð23Þ
Kk ¼ ðI  F þ
k1Fk1ÞfT
k fkðI  F þ
k1Fk1Þ
fkðI  F þ
k1Fk1ÞðI  F þ
k1Fk1ÞTfT
k
¼ mT
k mk
mkmT
k
¼ mT
k mk
zk
:
ð24Þ
From (23) and (24) it follows that xTKkx is a positive number and the value of
deterministic component of the true signal recovery error decreases with increasing k.
From the recursive expressions (22) and (23) it follows that for the x recovery error
Dex dðkÞ ¼ xTf þ
k fkðI  F þ
k1Fk1Þx; Dex gðkÞ ¼ traceðMT
k1Mk1Þ þ dk:
ð25Þ
Respectively,
JxðkÞ ¼ Dex dðkÞ
Dex gðkÞ ¼ xTf þ
k fkðI  F þ
k1Fk1Þx
traceðMT
k1Mk1Þ þ dk
:
ð26Þ
440
E. G. Revunova

Unlike in the case of truncated SVD method, for the RP method the forms of Jx(k) and
Jy(k) are different, and the optimum k values for the input x and output y0 recovery
errors do not coincide. The numerical experiments however demonstrate that the
difference between Jx(k) and Jy(k) is small. Approximation of the output vector
recovery error, obtained as a result of averaging over multiple noise realizations, has
the form:
CRR ¼E ðAF þ
k RT
k  IÞy

2r2traceððAF þ
k RT
k  IÞTðAF þ
k RT
k  IÞÞ
þ r2traceðF þ T
k
ATAFkÞ:
ð27Þ
Figure 2 shows the plots of the dependence of CRR criterion values, average error
of the output recovery, and average error of the true signal recovery on k for the
Carasso problem.
1.E-06
1.E-05
1.E-04
1.E-03
1.E-02
1.E-01
1.E+00
1
5
9
13 17 21 25 29 33 37
1.E-02
1.E-01
1.E+00
1.E+01
e_y_mean
CR
e_x_mean (5*1e-4)
1.E-08
1.E-07
1.E-06
1.E-05
1.E-04
1.E-03
1.E-02
1
5
9
13 17 21 25 29 33 37
1.E-03
1.E-02
1.E-01
1.E+00
1.E+01
e_y_mean
CR
e_x_mean (5*1e-5)
Fig. 2. Dependencies of the CRR criterion values and the output recovery error on k for the noise
levels of 5e−4 and 5e−5.
Solution of the DIP on the Basis of Singular Value Decomposition
441

The dependence curves for the criterion CRR and the average recovery error of y0
are close; the model selection criterion CRR approximates the output recovery error
well. The positions of the minima of dependencies of x recovery error and y0 recovery
error on k are also close. We note that the experimental studies have shown that
averaging over random matrices leads to the smoothing of the characteristics ex(k),
ey(k), Jx(k), Jy(k) and, correspondingly, decrease in the number of local minima
(Figs. 3, 4, 5 and 6).
Therefore, analytical averaging over random matrices can lead to simpler expres-
sions for ex(k) and ey(k) and simplify the expressions for Ded(k), Deg(k), Jx(k), Jy(k). In
addition, numerous experimental studies of the truncated SVD and RP methods have
shown the closeness of the ex(k) dependences for these methods.
1.E-06
1.E-04
1.E-02
1.E+00
1
4
7
10 13 16 19 22 25 28 31 34 37
k
J x,  J y
J x 
J y 
nl=1e-3
Fig. 3. Dependence of Jx, Jy on k for the
Carasso problem.
1.E-02
1.E-01
1.E+00
1.E+01
1
4
7
10 13 16 19 22 25 28 31 34 37
k
e
e_x
Fig. 4. Dependence of the x recovery error on
k for the Carasso problem.
1.E-06
1.E-04
1.E-02
1.E+00
1
4
7
10 13 16 19 22 25 28 31 34 37
k
J x,  J y
J_x
J_y
nl=1e-3
Fig. 5. Dependence of Jx(k), Jy(k) on k for
the Carasso problem after averaging over
random matrices.
1.E-02
1.E-01
1.E+00
1.E+01
1
4
7
10 13 16 19 22 25 28 31 34 37
k
e
e_x
Fig. 6. Dependence of the x recovery error
on k for the Carasso problem after averaging
over random matrices.
442
E. G. Revunova

To study the connection of eSVD(k) and eR(k), we need to obtain an expression for
the ex(k) average over an ensemble of random matrices.
5
Error Averaging Over Random Matrices
We transform the expressions for the error components (9) as follows:
eR dðkÞ ¼
ððRkAÞ þ RkA  IÞx

2¼ xTx  xTATRT
k ðRkAATRT
k Þ þ RkAx;
ð28Þ
eR sðkÞ ¼ r2traceððRkAÞ þ TðRkAÞ þ Þ ¼ r2traceðRT
k ðRkAATRT
k Þ þ RkÞ:
ð29Þ
Let us average over the realizations of random matrices:
ERfeg ¼ ERfeR dg þ ERfeR sg ¼ xTx  xTATERfRT
k ðRkAATRT
k Þ þ RkgAx
þ r2traceERfRT
k ðRkAATRT
k Þ þ Rkg:
ð30Þ
Using A = USVT we obtain AAT = US2UT and
ERfRT
k ðRkAATRT
k Þ þ Rkg ¼ ERfRT
k ðRkUS2UTRT
k Þ þ Rkg:
ð31Þ
By the Lemma 30 in [30]
ERfRT
k ðRkUS2UTRT
k Þ þ Rkg ¼ UERfRT
k ðRkS2RT
k Þ þ RkgUT:
ð32Þ
In accordance with the expression (45) [30]
ERfRT
k ðRkS2RT
k Þ þ Rkg ¼ diagðk1; . . .; km; lInmÞ ¼ Dk;
ð33Þ
where
ki ¼
l
1 þ ls2
i
; l ¼ const; n  m  1
n
X
n
i¼1
1
1 þ ls2
i
:
ð34Þ
Consequently
ERfRT
k ðRkAATRT
k Þ þ Rkg ¼ UDkUT:
ð35Þ
The resulting expression for the error (8) after averaging over the matrices Rk takes
the form:
ERfeg ¼ xTx  xTATUDkUTAx þ r2traceðUDkUTÞ:
ð36Þ
Solution of the DIP on the Basis of Singular Value Decomposition
443

By converting xTATUDkUTAx = xTVSTDkSVTx = xTVS2DkVTx, we get:
ERfeg ¼ xTx  xTVS2DkVTx þ r2traceðUDkUTÞ:
ð37Þ
An experimental study of the fulﬁllment of expression (33) showed that the sequence
k1, …, km is not only upper bounded by the sequence 1/s1
2, …, 1/sm
2 , but also several
initial values of the diagonal of the matrix Dk approximate 1/si
2 with great accuracy. For
example, for the Carasso problem [28], the values of the diagonal elements of the matrix
Dk at k = {2, 3, 6, 9,13} are shown in Fig. 7, where the values of 1/si
2 and l are
also given.
If it is possible to show analytically that for some p < m we have di = 1/si
2, it will
allow us to investigate analytically the connection of the truncated SVD and RP as it is
sketched below.
6
Connection Between the Errors of the Truncated SVD
and RP Methods
Using (37), we write the expression for the deterministic error component for the RP
method:
eR d ¼ xTx  xT X
n
i¼1
ðvis2
i divT
i Þx;
ð38Þ
eR d ¼ xTx  ðxT X
p
i¼1
ðvis2
i
1
s2
i
vT
i Þx þ xT X
n
i¼p þ 1
ðvis2
i divT
i ÞxÞ:
ð39Þ
1.E+00
1.E+01
1.E+02
1.E+03
1.E+04
1.E+05
1
3
5
7
9
11 13 15 17 19 21 23 25 27 29 31
diag(D2) th
diag(D3) th
diag(D6) th
diag(D9) th
diag(D13) th
1/s^2
mu = 36
mu = 83
mu = 526
mu = 9300
mu = 2100
Fig. 7. Elements diag(Dk), values of l and 1/si
2 at k = {2, 3, 6, 9,13} for the Carasso problem.
444
E. G. Revunova

Let us transform this expression so as to isolate the deterministic component eSVD
d
of
the error for SVD:
eR d ¼ xTx  xT X
p
i¼1
ðvivT
i Þx  xT X
n
i¼p þ 1
ðvis2
i divT
i Þx ¼ eSVD
p d  xT X
n
i¼p þ 1
ðvis2
i divT
i Þx:
ð40Þ
In the expression for the stochastic error component for the RP method
eR s ¼ r2traceðUDkUTÞ ¼ r2traceðDkUTUÞ ¼ r2traceðDkÞ;
ð41Þ
isolate the stochastic component eSVD
s
of the error for SVD
eR s ¼ r2traceðDkÞ ¼ r2 X
p
i¼1
1
s2
i
þ r2 X
n
i¼p þ 1
di ¼ eSVD
p s þ r2 X
n
i¼p þ 1
di:
ð42Þ
Thus, the expression for the error of recovering the true signal by the RP method is:
ERfeg ¼ eSVD
p
 xT X
n
i¼p þ 1
ðvis2
i divT
i Þx þ r2 X
n
i¼p þ 1
di;
ð43Þ
where eSVD
p
is the error of recovering the true signal using the p components of SVD.
7
Experimental Study
For the problems of Philips [31] and Carasso [28], the following experiments were
carried out. The mathematical expectation ER RT
k RkS2RT
k

 þ Rk
n
o
was approximated
by averaging over 104 random matrices: 104 matrices Rki were generated for each k,
104 matrices Mki ¼ RT
ki RkiS2RT
ki

 þ Rki were calculated, and matrices
Dk ¼
X
104
i¼1
Mki=104;
ð44Þ
The values of the diagonal elements of the matrix obtained after averaging are
shown in Figs. 8 and 9. The theoretical values of the elements of the Dk matrix
diagonal calculated from (33) are shown in Figs. 8 and 9 with the label “th”.
For the DIP solution by the RP method, the error value and its components (8, 9)
were experimentally obtained with averaging over 104 matrices (Figs. 10 and 11). The
theoretical values of the error and its components calculated using the theoretical values
of diag(Dk) (33) are indicated with the label “th”.
Solution of the DIP on the Basis of Singular Value Decomposition
445

An example of these dependencies for the discrete ill-posed problems of Phillips (at
the noise levels of 1E−2, 1E−3, 1E−4) and Carasso (at the noise levels of 0.3E−3,
1E−4, 1E−5) is shown in Figs. 10 and 11. Here e, e1, e2 are the error and its com-
ponents with averaging over 104 matrices, eth, e1th, e2th are the error and its components
obtained by formula (36).
When k increases, the deterministic component decreases, and the stochastic
component increases, hence the total error norm has a minimum. With increasing noise
level the position of error minimum shifts to the lower values of k, and the error value
at the minimum increases.
1.E+00
1.E+01
1.E+02
1.E+03
1.E+04
1
3
5
7
9
11
13
15
17
19
diag(D2)
diag(D2) th
diag(D3)
diag(D3) th
diag(D6)
diag(D6) th
diag(D9)
diag(D9) th
diag(D13)
diag(D13) th
Fig. 8. Elements diag(Dk) obtained experi-
mentally and theoretically at k = {2, 3, 6,
9,13} for the Carasso problem.
1.E-02
1.E-01
1.E+00
1.E+01
1.E+02
1.E+03
1.E+04
1
3
5
7
9
11
13
15
17
19
21
diag(D4)
diag(D4) th
diag(D7)
diag(D7) th
diag(D10)
diag(D10) th
diag(D16)
diag(D16) th
diag(D23)
diag(D23) th
Fig. 9. Elements diag(Dk) obtained experi-
mentally and theoretically at k = {4, 7, 10,
16, 23} for the Phillips problem.
1.E-07
1.E-06
1.E-05
1.E-04
1.E-03
1.E-02
1.E-01
1.E+00
1.E+01
1.E+02
1
4
7
10
13
16
19
22
25
28
31
34
37
k
e
e_1
e_2
e
e_1_th
e_2_th
e_th
1.E-07
1.E-06
1.E-05
1.E-04
1.E-03
1.E-02
1.E-01
1.E+00
1.E+01
1
4
7
10 13 16 19 22 25 28 31 34 37
k
e
e_1
e_2
e
e_1_th
e_2_th
e_th
Fig. 10. The problem of Phillips. For the noise level of 1e−2 (left) and noise level of 1e−4
(right).
446
E. G. Revunova

8
Conclusions
Analytic and experimental averaging over random matrices for evaluation of the error
of the true signal recovery is carried out for the method of solving the discrete ill-posed
problems on the basis of random projection.
Comparison of the dependencies of the error and its components on model
dimensionality k obtained analytically and experimentally after averaging over an
ensemble of random matrices demonstrate a good agreement of the theoretical
dependences with the experimental ones.
Averaging over random matrices leads to diagonalization of the matrix condi-
tioning both components of the error (deterministic and stochastic). The values of the
diagonal elements change monotonically as a function of k. This in turn leads to the
smoother characteristics and reducing the number of local minima.
The results of the experimental study showed the connection of the elements of the
diagonalized matrix with the singular values of the original matrix. This provides the
basis for investigating the connection of the truncated singular value decomposition
method and the random projection method.
Acknowledgment. The author is grateful to Dmitri A. Rachkovskij for comprehensive help and
support.
References
1. Zabulonov, Y., Korostil, Y., Revunova, E.: Optimization of inverse problem solution to
obtain the distribution density function for surface contaminations. Model. Inf. Technol. 39,
77–83 (2006). (in Russian)
2. Rachkovskij, D., Revunova, E.: Intelligent Gamma-ray data processing for environmental
monitoring. In: Intelligent Data Analysis in Global Monitoring for Environment and
Security, pp. 124–145. ITHEA, Kiev-Soﬁa (2009)
1.E-04
1.E-03
1.E-02
1.E-01
1.E+00
1.E+01
1.E+02
1
4
7
10
13
16
19
22
25
28
31
34
e_1
e_2
e
e_1_th
e_2_th
e_th
1.E-04
1.E-03
1.E-02
1.E-01
1.E+00
1.E+01
1.E+02
1
4
7
10
13
16
19
22
25
28
31
34
e_1
e_2
e
e_1_th
e_2_th
e_th
Fig. 11. The problem of Carasso. For the noise level of 0.3e−3 and level of 1e−5.
Solution of the DIP on the Basis of Singular Value Decomposition
447

3. Starkov, V.: Constructive methods of computational physics in interpretation problems.
Naukova Dumka, Kyev (2002). (in Russian)
4. Tikhonov, A., Arsenin, V.: Solution of Ill-Posed Problems. V.H. Winston, Washington
(1977)
5. Hansen, P.: Rank-Deﬁcient and Discrete Ill-Posed Problems, Numerical Aspects of Linear
Inversion. SIAM, Philadelphia (1998)
6. Hansen, P.: The truncated SVD as a method for regularization. BIT 27, 534–553 (1987)
7. Revunova, E., Tyshchuk, A.: A model selection criterion for solution of discrete ill-posed
problems based on the singular value decomposition. In: 7th International Workshop on
Inductive Modelling, IWIM 2015, pp. 43–47. Kyiv-Zhukyn (2015)
8. Revunova, E., Tyshchuk, A.: Model selection criterion for the solution of discrete ill-posed
problems based on singular value decomposition. Control Syst. Comput. 6, 3–11 (2014)
9. Revunova, E., Rachkovskij, D.: Using randomized algorithms for solving discrete ill-posed
problems. Int. J. Inf. Theor. Appl. 2(16), 176–192 (2009)
10. Rachkovskij, D., Revunova, E.: Randomized method for solving discrete ill-posed problems.
Cybern. Syst. Anal. 48(4), 621–635 (2012)
11. Revunova, E.: Analytical study of the error components for the solution of discrete ill-posed
problems using random projections. Cybern. Syst. Anal. 51(6), 978–991 (2015)
12. Revunova, E.: Model selection criteria for a linear model to solve discrete ill-posed problems
on the basis of singular decomposition and random projection. Cybern. Syst. Anal. 52(4),
647–664 (2016)
13. Kussul, E., Baidyk, T., Lukovich, V., Rachkovskij, D.: Adaptive neural network classiﬁer
with multiﬂoat input coding. In: Proceedings Neuro-Nimes 1993, pp. 209–216 (1993)
14. Lukovich, V., Goltsev, A., Rachkovskij, D.: Neural network classiﬁers for micromechanical
equipment diagnostics and micromechanical product quality inspection. In: Proceed-
ings EUFIT 1997, vol. 1, pp. 534–536 (1997)
15. Kussul, E., Kasatkina, L., Rachkovskij, D., Wunsch, D.: Application of random threshold
neural networks for diagnostics of micro machine tool condition. In: Neural Networks
Proceedings, IEEE World Congress on Computational Intelligence, vol. 1, pp. 241–244
(1998)
16. Rachkovskij, D., Slipchenko, S., Kussul, E., Baidyk, T.: Properties of numeric codes for the
scheme of random subspaces RSC. Cybern. Syst. Anal. 41(4), 509–520 (2005)
17. Rachkovskij, D., Misuno, I., Slipchenko, S.: Randomized projective methods for construc-
tion of binary sparse vector representations. Cybern. Syst. Anal. 48(1), 146–156 (2012)
18. Durrant, R., Kaban, A.: Random projections as regularizers: learning a linear discriminant
from fewer observations than dimensions. Mach. Learn. 99(2), 257–286 (2015)
19. Wei,
Y.,
Xie,
P.,
Zhang,
L.:
Tikhonov
regularization
and
randomized
GSVD.
SIAM J. Matrix Anal. Appl. 37, 649–675 (2016)
20. Ivakhnenko, A., Stepashko, V.: Noise-Immunity of Modeling. Naukova dumka, Kiev
(1985). (In Russian)
21. Stepashko, V.: Theoretical aspects of GMDH as a method of inductive modeling. Control
Syst. Mach. 2, 31–38 (2003). (In Russian)
22. Stepashko, V.: Method of critical variances as analytical tool of theory of inductive
modeling. J. Autom. Inf. Sci. 40(3), 4–22 (2008)
23. Mallows, C.: Some comments on CP. Technometrics 15(4), 661–675 (1973)
24. Akaike, H.: A new look at the statistical model identiﬁcation. IEEE Trans. Autom. Control
19(6), 716–723 (1974)
25. Hansen, M., Yu, B.: Model selection and minimum description length principle. J. Am. Stat.
Assoc. 96, 746–774 (2001)
448
E. G. Revunova

26. Bayati, M., Erdogdu, M., Montanari, A.: Estimating LASSO risk and noise level. In:
Proceedings of Advances in Neural Information Processing Systems, NIPS (2013)
27. Fan, J., Guo, S., Hao, N.: Variance estimation using reﬁtted cross-validation in ultrahigh
dimensional regression. J. Roy. Stat. Soc. Ser. B (Stat. Methodol.) 74, 1467–9868 (2012)
28. Carasso, A.: Determining surface temperatures from interior observations. SIAM J. Appl.
Math. 42, 558–574 (1982)
29. Stewart, G.: On the perturbation of pseudo-inverses, projections and linear least squares
problems. SIAM Rev. 19(4), 634–662 (1977)
30. Marzetta, T., Tucci, G., Simon, S.: A random matrix-theoretic approach to handling singular
covariance estimates. IEEE Trans. Inf. Theor. 57(9), 6256–6271 (2011)
31. Phillips, D.L.: A technique for the numerical solution of integral equation of the ﬁrst kind.
J. ACM 9, 84–97 (1962)
Solution of the DIP on the Basis of Singular Value Decomposition
449

Methodology of Research the Library Information
Services: The Case of USA University Libraries
Antonii Rzheuskyi1(✉), Nataliia Kunanets1, and Vasil Kut2
1 Information Systems and Networks Department, Lviv Polytechnic National University,
Lviv, Ukraine
antonii.v.rzheuskyi@lpnu.ua, nek.lviv@gmail.com
2 Information Technology and Analytics Department,
Augustine Voloshin Carpathian University, Uzhgorod, Ukraine
kytnakryt@mail.ru
Abstract. The article presents the research methodology of the range of infor‐
mation services provided remotely by libraries of USA leading higher educational
establishments. The methodology consists of benchmarking method and pairwise
comparisons method on the basis of expert estimates. The authors reviewed the
existing approaches to carrying out benchmarking researches and proposed their
own concept. Innovation is that a comparative analysis of the results obtained in
the process of benchmarking studies and the pairwise comparisons method is held.
The library remote information services of higher educational establishments of
America are analyzed. The study is based on benchmarking and pairwise compar‐
isons methods that allow to identify the group of library-leaders in providing
quality information services.
Keywords: Benchmarking · Library · Consumers of information services · Cloud
services · Expert evaluation method
1
Introduction
Today, librarians have not solved the problem of developing a single integrated meth‐
odology for assessing the quality of information service, although some attempts in this
direction were made. In particular, with sociological studies the evaluation of some
library services by users was studied. But still the question remains to analyze the
parameters of this assessment. Conducted studies are irregular, and to improve the
libraries work, there is a demand on a toolkit for systematic analysis of information
services provided by libraries. In our previous studies we found out that such toolkit can
be considered a benchmarking methodology, which provides an opportunity to search,
evaluate and borrow experience from the best algorithms for the organization of eﬀective
work. Benchmarking is a part of systematic and structured approach for ﬁnding and
implementing the best examples of excellence. In librarianship benchmarking can be
used for comparative analysis of one service to others. The purpose of the paper is to
deﬁne USA libraries-leaders of providing high quality remote information service for
users.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_32

2
Background
Benchmarking is widely used in the economic ﬁeld. Particularly foreign researchers who
turn to this methodology should be called: Wann Jong-Wen, Lu Ta-Jung, Lozada Ina,
Cangahuala Guillermo [1], Dalalah Doraid, Al-Rawabdeh Wasﬁ [2], Franses Philip
Hans, de Bruijn Bert [3], Kwon He-Boong, Marvel Jon H., Roh James Jungbae [4].
Among the researchers who using benchmarking in the context of librarianship worth
noting: Vivas Salas Leonel Orangel, Briceno Sosa Maria Alejandra, Colls Ojeda Janeyra
del Carmen [5], Amos H., Hart S. [6].
Let’s consider the main advantages of using the benchmarking methodology in
library science:
• Firstly, it contributes to the objective analysis of the strengths and weaknesses of the
library work. Such monitoring of strengths and weaknesses is carried out in the
context of comparison with the standards.
• Secondly, it allows to study and implement new ideas in the organization of infor‐
mation and library work, marketing services and in the other areas of activity to
increase their eﬃciency.
• Thirdly, it ensures an analysis of the activity of library-leaders for the borrowing of
innovative technologies.
• Fourthly, it allows the strategic planning of the library institution activities with the
use of advanced technologies, implemented by leaders of the area.
• Fifthly, it helps to conduct regular researches that allows to keep up to date with
innovations in librarianship and actively implement them in own institution.
• Sixthly, it allows to overcome the conservative principle – to plan from the achieved,
based on the analysis of the libraries activity, which constitute a certain competi‐
tion [7].
By adapting the methodology of benchmarking to improve the management of social
institutions activity, research should be directed to the study, borrowing and implemen‐
tation of the best technologies, innovative processes and methods of work organization
in order to create and disseminate information products among own users.
The use of this method allowed us to develop algorithms for system analysis of the
activity of the investigated library and the library-leaders. The advantage of this method
is the ability to conduct studies to determine the variability of certain trends in a number
of parameters of the library activity for a certain time. The experience has shown that
using the method of data comparative analysis allows us to concentrate on studying of
one parameter at a certain time.
Taking into account the algorithm, deﬁned by R. Kemp [8], the benchmarking
comparisons in the study of the library should be carried out in several stages:
1. Identiﬁcation and deﬁnition of object reference comparisons.
2. Selection of standards and experts of benchmarking stage.
3. Identiﬁcation of suitable methods of information gathering and data collection.
4. Identiﬁcation of existing inconsistencies and gaps of the investigated library from
libraries-competitors for selected indicators.
Methodology of Research the Library Information Services
451

5. Establishing the desired results and levels of eﬀectiveness of library activity.
6. Informing representatives of all interested parties about the benchmarking results
and obtaining consent for their use.
7. Setting goals and objectives of increasing the eﬃciency of library activity.
8. The development of an action plan to achieve the objectives.
9. Carrying out planned events and analyzing their results. Integration of experience
in the work processes of one’s own institution.
10. Re-analysis of selected strategies with using benchmarking.
A simpliﬁed version of the use of benchmarking consists of four sequential steps:
1. Awareness and analysis of the details of one’s own activities
In this phase, problem areas of the library activity are set up, which should be inves‐
tigated with the help of benchmarking. This enables a critical attitude to the library
activity as a whole, individual processes or work of departments. At this stage, a
decision is made about the type of benchmarking.
As practice shows, due to the regular compilation of reports on the library work,
every section of its activity is thoroughly analyzed, that allows to identify the weak‐
nesses.
2. Analysis of the processes of other libraries
Having substantiated the goals, it is necessary actively to look for the best libraries
that will serve as standards. The potential standards should be the leaders, but also
convenient for the most simple comparability. Thus, the dynamics of speciﬁc work
indicators is analyzed.
3. Comparison of work results of investigated library with the activity results of the
library-leaders. At this stage, the collection of additional data of particular value, the
analysis of work content, processes or factors associated with their eﬀectiveness is
carried out. The analysis in this case means not only ﬁnding the similarities and
diﬀerences, but also to determine causal relationships.
4. Introduction of qualitative or quantitative changes to overcome the lag.
3
An Own Approach to the Benchmarking Research of Library
Information Services
In the research process of information library service, benchmarking methodology was
used to analyze the service provided with higher educational establishments of USA
library websites. We consider that benchmarking investigation of libraries should be
held by the following algorithm (Fig. 1).
1. To identify the maximum amount of libraries. For benchmark comparison, the
American higher educational establishments scientiﬁc libraries were covered. At this
stage the problem areas in libraries activity were setting that should be investigated
with the benchmarking method. This enables a critical attitude to the library activity
in general, individual processes or department units.
452
A. Rzheuskyi et al.

2. Determination of the maximum range of services provided by the library institutions.
The comparison of the investigated library processes with the results of library-
leaders. Substantiated goals it should actively seek the best libraries that act as
benchmarks. Potential benchmarks must be leaders, but inherent to the most simple
comparability. In this case the dynamic of speciﬁc indicators of work is analyzed.
3. At this stage, the collection of extra data, which are especially valuable, content
analysis of library processes or factors related to their eﬃciency is made. To get the
information the method of “competitive intelligence” is used. The application of
“competitive intelligence” is carrying out a quick search of necessary information
Fig. 1. The algorithm for studying the libraries of USA universities according to benchmarking
the methodology.
Methodology of Research the Library Information Services
453

and proper analysis, using legal methods of collecting and processing information,
based on open source to obtain it.
4. Due to reporting about the work of the library, thoroughly every area of its activity
has been analyzed, to determine weaknesses. Publicly available library reports serve
as a reliable source for the analysis and comparison of library processes.
5. However, not all libraries place free access reports. Besides, these documents contain
only statistics. So for a comparative analysis were used library websites as an open
source of providing remote service for users.
6. Based on the information services provided by libraries via a web-site, the library-
leaders as an example to follow of best practice were deﬁned. The obtained data will
overcome the lag in the information service.
7. The introduction of innovative services in the library, to which is used benchmarking
methodology, will contribute to the modernization of library information service.
The use of benchmarking methodology for eﬀective management of the library
should be systematic. Therefore, it is necessary to carry out continuous monitoring
of other libraries and information institutions to identify and implement innovative
technologies in the work of one’s own library.
It was found that USA higher educational establishments scientiﬁc libraries provide
the following range of remote services:
• electronic catalog;
• institutional repository;
• access to pre-paid databases;
• virtual reference;
• interactive communication services (skype, on-line assistant, etc.);
• electronic delivery of documents (EDD);
• multimedia resources;
• tools for help professionals;
• bibliographic managers;
• cloud services;
• recommended resources (guidance to public information resources in the Internet:
databases, on-line journals, web-services).
4
The Research of Remote Information Services of USA University
Libraries
Within the research, 19 library websites of higher educational establishments of the
United States of America were analyzed. For comparative analysis, the amount of the
libraries was divided into several groups. Binghamton University Library took the ﬁrst
place among the libraries in the group A (Fig. 2).
454
A. Rzheuskyi et al.

Fig. 2. The comparison of the libraries in the group A.
By the composition of library information services, the championship got Case
Western Reserve University, and Brown University of Washington (Fig. 3).
Fig. 3. The comparison of the libraries in the group B.
On this list of the libraries, the leader is Rutgers University Library (Fig. 4).
Benchmarking technology allowed to identify the library-leaders of the United States
of America that provide a wide range of the remote library services (Fig. 5).
Methodology of Research the Library Information Services
455

Fig. 5. The comparison of library-leaders.
5
Summarizing the Results of the Research with the Benchmarking
Methodology
It was found that USA universities libraries provide users with the ability to use the
system of electronic catalogs; repository, which stores theses, articles, abstracts of the
university employees; and a wide range of pre-paid and free of charge databases. The
decisive fact is that the USA library practice the individual approach to assist the
researchers – the curator of a particular area of knowledge, which is guided in an
Fig. 4. The comparison of the libraries in the group C.
456
A. Rzheuskyi et al.

appropriate range of information resources is used. The website provides supervisor
(subject guide) contact information: the phone number, e-mail and skype.
This provides powerful information support of scientiﬁc staﬀ of the university. The
users are suggested to use the open reference resources, on-line journals collections that
are available on the Internet. Cloud services include provision of access to full texts of
documents from the library collections via OverDrive service for a certain period of
time, which essentially plays the role of an electronic loan. It is proposed to use biblio‐
graphic management tools, access to which is provided from libraries websites to
improve the eﬃciency of information support of scientiﬁc researches.
The weak point is the lack of promotional library multimedia materials. However,
the audio and video documents are hosted in the library catalog collections. There is an
electronic document delivery service (EDD) – the user is given the opportunity to receive
the electronic documents on his own email. Every library website includes a virtual
reference service and chat – instant messaging users with a contact person in the library.
Thus, by selecting the library-standard, the detailed analysis of the services provided
to users was made. Benchmarking is not a blind imitation of the work of another insti‐
tution, its main task – to borrow best practices and adaptation in the studied library. It
was considered that not all innovations can facilitate the eﬃcient work of one’s own
library. Each proposal should be thoroughly considered, taking into account the realities
of one’s own library. Some ideas that at ﬁrst sight did not deserve of imitation, after
some rethinking and improvement had been revalued and their implementation contrib‐
uted to achieving signiﬁcant results.
6
Veriﬁcation of the Results Using the Saati Pairwise Comparisons
Method
The results of the research, conducted by benchmarking method were checked with an
expert evaluation method. Let’s consider the results of this test on the example of the
group C with the criterion “repository” (Table 1).
Table 1. The matrix of pairwise comparisons of alternatives for the criterion “repository”.
Alternatives
Univ. of
California
Rutgers
Univ.
Univ. of
Florida
Duke Univ.
Pennsylvania
State Univ.
Purdue
Univ.
Univ. of
California
1,00
1,00
1,00
3,00
3,00
5,00
Rutgers Univ.
1,00
1,00
1,00
3,00
3,00
5,00
Univ. of Florida
1,00
1,00
1,00
3,00
3,00
5,00
Duke Univ.
0,33
0,33
0,33
1,00
1,00
3,00
Pennsylvania
State Univ.
0,33
0,33
0,33
1,00
1,00
3,00
Purdue Univ.
0,20
0,20
0,20
0,33
0,33
1,00
According to this criterion this library service is compared for determination of the
providing of users with remote information resources.
Methodology of Research the Library Information Services
457

On the basis of an expert assessment method, it was determined the weight coeﬃ‐
cients for service “repository” and constructed a matrix.
The results of calculating the weights of alternatives for the criterion “repository”
are shown in the Table 1.
Elements of the table columns obtained with normalization of appropriate elements
(Table 2).
Table 2. The weight of alternatives according to the criterion “repository”.
Alternatives
Univ. of
California
Rutgers
Univ.
Univ.
of
Florida
Duke
Univ.
Pennsylvania
State Univ.
Purdue
Univ.
Weight of
alternative
Univ. of
California
0,259
0,259
0,259
0,265
0,265
0,227
0,2554
Rutgers Univ.
0,259
0,259
0,259
0,265
0,265
0,227
0,2554
Univ. of Florida
0,259
0,259
0,259
0,265
0,265
0,227
0,2554
Duke Univ.
0,086
0,086
0,086
0,088
0,088
0,136
0,0952
Pennsylvania
State Univ.
0,086
0,086
0,086
0,088
0,088
0,136
0,0952
Purdue Univ.
0,052
0,052
0,052
0,029
0,029
0,045
0,0432
Thus, by the criterion “repository” may be oﬀered University of California library,
Rutgers University library and University of Florida library (Fig. 6), because they have
the same and more than the other libraries the weight value – 0.2554.
For the matrix of pairwise comparisons, based on the criterion “repository” the
following parameters were calculated:
• evaluation of the largest eigenvalue, which is calculated according to the formula:
𝜆max =
n
∑
i=1
wisi
(1)
where wi – weight alternative with a number i, si – sum of the elements of column
number i matrix of pairwise comparisons, n – number of alternatives.
𝜆max = 0.2554 ⋅3.87 + 0.2554 ⋅3.87 + 0.2554 ⋅3.87
+ 0.0952 ⋅11.33 + 0.0952 ⋅11.33 + 0.0432 ⋅22 = 6.073;
(2)
• the index of consistency
CI = 𝜆max −n
n −1
= 6.073 −6
6 −1
= 0.0146;
(3)
• the index of the sequence of ratios
CR = CI
RI = 0.0146
1.24
= 0.0118
(4)
458
A. Rzheuskyi et al.

Here RI = 1, 24 – random index for n = 6, value of which is the same for all further
calculations of the weights of alternatives. As CR = 1.18 % < 10 % the matrix of
pairwise comparisons is considered consistent.
7
Conclusions
According to the results of our research, the use of benchmarking methods in the study
of libraries contributes to the improvement of the following directions of its activities:
• satisfying the information needs of the library users and the demand on document-
information resources of the library;
Fig. 6. The comparative analysis of the library information services in the group C.
Methodology of Research the Library Information Services
459

• socio-communicative relations of the library with the external environment;
• improving quality and expanding the range of services;
• the use of modern technologies;
• traditional library activities in an innovative mode;
• creating a positive image of the institution.
Thus, using the benchmarking and pairwise comparisons methods, among 19 USA
universities academic libraries were identiﬁed the leaders in providing the most compre‐
hensive list of library and information services. The results, obtained on the basis of the
benchmarking libraries, have been conﬁrmed with expert evaluation method, which
gives reason to keep using benchmarking method in librarianship studies.
References
1. Wann, J.-W., Lu, T.-J., Lozada, I., Cangahuala, G.: University-based incubators’ performance
evaluation: a benchmarking approach. Benchmarking Int. J. 24(1), 34–49 (2017)
2. Dalalah, D., Al-Rawabdeh, W.: Benchmarking the utility theory: a data envelopment approach.
Benchmarking Int. J. 24(2), 318–340 (2017)
3. Franses, P.H., Bruijn, B.: Benchmarking judgmentally adjusted forecasts. Int. J. Fin. Econ.
22(1), 3–11 (2017)
4. Kwon, H.-B., Marvel, J.H., Roh, J.J.: Three-stage performance modeling using DEA-BPNN
for better practice benchmarking. Expert Syst. Appl. 71(1), 429–441 (2017)
5. Salas, L.O.V., Sosa, M.A.B., del Carmen, J., Ojeda, C.: Benchmarking applied to catalog of
library services at the University of Los Andes. Visión Gerencial 1, 59–72 (2017)
6. Amos, H., Hart, S.: International collaboration for quality: a case study of fostering
collaboration across an international network of University Libraries through an activity based
benchmarking project. In: 34th Annual IATUL Conference Proceedings, Purdue e-Pubs (2013)
7. Rzheuskiy, A., Kunanets, N.: The concept of benchmarking in the librarianship. Coll. Sci.
Works Econ. Sci. 10, 17–24 (2014)
8. Camp, R.C.: Benchmarking. The Search for Industry Best Practices that Lead to Superior
Performance. ASQC Industry Press, Milwaukee (1989)
460
A. Rzheuskyi et al.

The Method of Big Data Processing for Distance
Educational System
Natalya Shakhovska
(✉), Olena Vovk, Roman Hasko, and Yuriy Kryvenchuk
Artiﬁcial Intelligence Department, Lviv Polytechnic National University,
12 S. Bandera Street, Lviv 79012, Ukraine
Nataliya.b.shakhovska@lpnu.ua
Abstract. The paper presents the main features of modern educational systems
such as huge amount of information and requirements for data processing. The Big
data definition and the main characteristics description The model of association
between entities and characteristics is constructed. The method of heterogeneous
data sharing and bringing to relational data model “entity-characteristic” was
created. The testing results of developed methods and algorithms are presented.
Keywords: Big data · XML · NoSQL · Relation database · Distance education
1
Introduction
The distance education is an innovative form of learning that helps overcome barriers
to obtaining the necessary knowledge and skills. The distance learning attracted the
attention of scientists as a theoretical and trainers, educators and practitioners. Distance
education is a promising way for the development of vocational education. Scientists
are exploring the development of distance education in the system of higher education
institutions. It should be noted that most of the researchers are devoted to a greater extent
analysis technology implementation processes of learning at a distance. And almost no
issue highlights the speciﬁc processes of distance learning of engineering students.
One of the modern forms of education, which was formed on the basis of active the
use of new information technologies for distance learning is based on the active use of
remote information systems training for engineering students as program-algorithmic
and technical complexes that implement a set of information technology for selection,
registration, transmission, storage, processing and presentation of educational informa‐
tion, given the speciﬁcity of the audience and laboratory work. With stipulates that
distance learning emerged as integration complex of modern information technology,
telecommunications, and modern educational methods and allows implementation
process of remote training of students using interactive educational tools and organize
communication students both within the group and with teachers, regardless of where
they are geographically. “Thin clients” (laptops, pocket PCs, smart phones, mobile
phones and so on) can provide presentation software and information components of
distance learning in the languages of HTML, XML, where the interface part provides a
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_33

web-browser, and signiﬁcantly improve mobility distance learning. The web technology
usage enables mobile organize an eﬀective job of distance learning, which should
provide ﬂexible expansion and shift functions to perform the kind of educational tasks
solved by remote education speciﬁed time. But there is no information about remote
laboratory preparation [9].
Modern education systems produce a huge amount of information. The size of indi‐
vidual databases is growing very fast and overcame barrier in PB. Most of the data
collected are not currently analyzed, or is only superﬁcial analysis [1].
The main problems that arise in the data processing in education system is the lack
of analytical methods suitable for use because of their diverse nature the need for human
resources to support the process of data analysis, high computational complexity of
existing algorithms for analysis and rapid growth of data collected. They lead to a
permanent increase in analysis time, even with regular updating of hardware servers and
also arise need to work with distributed database capabilities which most of the existing
methods of data analysis is not used eﬀectively. Thus, the challenge is the development
of eﬀective data analysis methods that can be applied to distributed databases in diﬀerent
domains. It is therefore advisable to develop methods and tools for data consolidation
and use them for analysis [6].
Big Data information technology is the set of methods and means of processing
diﬀerent types of structured and unstructured dynamic large amounts of data for their
analysis and use for decision support. There is an alternative to traditional database
management systems and solutions class Business Intelligence. This class attribute of
parallel data processing (NoSQL, algorithms MapReduce, Hadoop).
The main problems that arise in the data processing is the lack of analytical methods
suitable for use because of their diverse nature the need for human resources to support
the process of data analysis, high computational complexity of existing algorithms for
analysis and rapid growth of data collected. They lead to a permanent increase in analysis
time, even with regular updating of hardware servers and also arise need to work with
distributed database capabilities which most of the existing methods of data analysis is
not used eﬀectively. Thus, the challenge is the development of eﬀective data analysis
methods that can be applied to distributed databases in diﬀerent domains. It is therefore
advisable to develop methods and tools for data consolidation and use them for analysis.
This paper is extended version of [10].
2
Analysis of the Information Resources
Big data is a term used to identify data sets that we cannot cope with existing method‐
ologies and software tools because of their large size and complexity. many researchers
are trying to develop methods and software tools for data mining or information granules
of Big Data [1, 2].
Big Data features are:
• working with unstructured and structured information,
• orientation on the fast data processing
462
N. Shakhovska et al.

• leads to the fact that traditional query language is ineﬀective while working with
data.
The purpose of the article is to formally describe Big data model and carriers distin‐
guishing and sharing methods.
One of the adapting concepts not only of relational data is NoSQL. The followers of
the concept of NoSQL language emphasize that it is not a complete negation of SQL
and the relational model, but the project comes from the fact that SQL - is important and
very useful tool, that cannot be considered as universal. One problem that point for
classical relational database is a problem of dealing with huge data and projects with a
high load. The main objective approach is to extend the database if SQL ﬂexible enough,
and not displace it wherever it to perform its tasks.
The basis of ideas of the NoSQL are the following [7]:
• non-relational data model,
• distribution,
• open output code,
• good horizontal scalability.
As one of the methodological approach of NoSQL studies used a heuristic principle,
a theory known as CAP (Consistence, Availability, Partition tolerance - «consistency,
availability, resistance to division»), arguing that in a distributed system cannot simul‐
taneously provide consistency, availability and resistance to splitting distributed system
into isolated parts. Thus, if necessary, to achieve high availability and stability of the
division is expected not to focus on the means to ensure consistency of data provided
by traditional SQL-oriented database with transactional mechanisms on the principles
of ACID.
Non-strict proof of the CAP theory is based on a simple reﬂection. Let the distributed
system consisting of N servers, each of which handles the requests of a number of client
applications. While processing a request the server must ensure the relevance of the
information contained in the response to the request is sent, which previously required
synchronizing the contents of his own base with other servers. Thus, the server must
wait a full synchronization or generate a response based not synchronized data.
However, for any reasons synchronization is only part of the servers. In the ﬁrst case
the requirement of availability is not performed, in the second – consistency and in the
third - resistance to division.
The relation model is not suitable for parallel processing. That is why the method of
relation data transformation is proposed.
3
The Information Model of Big Data
Formally we can divide all the objects on the following categories [3]:
• e - entities;
•
f - characteristics;
• associations between e - entities and f –characteristics.
The Method of Big Data Processing for Distance Educational System
463

Therefore, the information model of Big data is triple [4]
BigD = ⟨e, f, a⟩,
where e ∈E is entity, f ∈F is characteristic, a →ne,f is association between entity e
and characteristic f.
From the relational model the model “entity-characteristic” is distinguished by the
association with a value ranging from 0 to 1. Opposite to proposed model the relational
model is a subspecies of the model “entity-characterization” association with a value
equal to 1 for each substance and characteristics associated with it.
For each entity e we calculate importance of association with f. It is necessary to
normalize the meaning of importance (weight):
V(e, f) =
(1 + log2
(ne,f
))) ⋅log2
( |E|
e(f)
)
√
∑((1 + log2
(ne,f
)) ⋅
( |E|
|e(f)|
))2
For each entity we calculate weight V(e, f). That’s is why r-statistics [8] between
objects E1 and E2, is distance between vectors (
V(e1, f), V(e2, f), …
).
3.1
The Method of Relational Data Conversion in to XML-Database Using Model
“Entity-Characteristic”
The main purpose of converting relational data expansion is nearly used Xml-description
in XML-enabled fuzzy membership functions. For fuzzy extension we propose to use
XML modern architecture MVVM (“View-Model”, “Model-View”) [5]. Its main diﬀer‐
ence from the well-known architectural software application MCV (Model-View-
Controller) is the lack of data binding requirements for their presentation. Under this
approach we will show the necessary components designed for automated conversion
of a relational database in XML model taking into account the features of the model
“entity value” (Fig. 1).
464
N. Shakhovska et al.

Fuzzy ХМL-markup
Fuzzy queries
Mining of XML, XSD,
DTD, XQEURY
 XML, XSD, DTD,
XQEURY processing
Syntax cheking of fuzzy
XML-tegs
Transformation to entity
and characteristic
Fig. 1. The scheme of conversion tables in relational XML-database incl model “entity-
characteriistic”
The architecture of the applications that are implemented with the structure as shown
in Fig. 1, there are several components that require further explanation. One of these
components - fuzzy XML-markup. This term means a Xml-description that allows
description of fuzzy variables and functions.
Implementation of fuzzy functions provided in the fuzzy XML- document is imple‐
mented by means of an external application that has the appropriate computing power.
For the convenience of universal description of fuzzy elements, we used Xsd
description, the type of which is given below. The membership function in this example
presents Minvalue limited value and Max Value.
The Method of Big Data Processing for Distance Educational System
465

<>:s : schema id="Fuzzyshema " elementformdefault="F qualified" xmlns 
:xs="http://www.w3.org/2001/XMLSchttp">
<xs:complextypename="fuzzy">
<xs:sequence>
<xs:elementname="fuzzy"minoccurs="1"maxoccurs="un-bounded"
type="functions7>
</xs:sequence>
</xs:complextype>
<xs:complextypename="functions">
<xs:sequence>
<xs:elementref="function"minoccurs="1"maxoccurs="unbounded"/>
</xs:sequence>
<xs:attributename="name"type="xs:string"/>
</xs:complextype>
<xs:elementname="function">
<xs:complextype>
<xs:simplecontent>
<xs:extensionbase="xs:string">
<xs:attributename="minvalue"type="xs:string"/>
<xs:attributename="maxvalue"type="xs:string"/>
</xs:extension>
</xs:simplecontent>
</xs:complextype>
</:ts:element>
</xs:schema>
The method of converting relational data model to “entity-characteristics” includes
the following steps.
Step 1. Create a root element of the XSD schema for the data model FDB.
Step 2. For each entity Efm (i) there is required to create separate element cxcmbi and
place it under the root element.
Step 3. For each attribute Attfm (j) for entity Efm (i) there is required to create attribute
xs: attribute R XSD, place it inside an appropriate description of the entity and
specify its type.
Step 4. For each attribute select and specify the data type and deﬁne thresholds.
In the real situation we must ﬁlter date. Figure 2 illustrates ﬁltering for data sources
in the form of Xml-ﬁles from ADM.
466
N. Shakhovska et al.

Fig. 2. The example of Dom-object ﬁltering Xml-data: a part of the dynamic model; b - Model
Xml-data sources; in - Xml-model data is loaded into the Dom-object
In the dynamic model in Fig. 2, the root condition sta: S Xml-deﬁnition sets Docu‐
ment doc: X1, continuing to ADM, and Element Dom-dom: D3, in which the document
provided for downloading doc: X1 ﬁltering. Data source src: X1 has an attribute
method = “cut”, which indicates that the source Xml-document will be “cut” subtree.
Additional attributes of «element», «ﬁeld» and «value» set ﬁltering condition. Attribute
«element» contains Xpath-expression that deﬁnes a plurality of nodes elements in Xml-
Tree, one of which is the root of the downloaded subtrees. Attribute «ﬁeld» contains
Xpath-expression that deﬁnes a subtree audited Xml-element or attribute. Attribute
«value» includes the required value. In an example we download Dom-object subtree
that begins in the element “e1”, attribute “k1” which has a value of “123”.
The model b in Fig. 2. Xml-Data doc: X1 with XML- root element «E0» can contain
child Xml-Elements «e1». Each of these elements contains attributes «k1» (ID) and
«a1». Thus, Xpath-expression «/ E0 / e1», speciﬁed in the attribute «element», has source
src: X1, but addresses all set Xml-items «e1»; attribute “ﬁeld” addresses the “E1” Xml-
Attribute “k1”, attribute “of value” asking for a desired value “123”.
During processing element dom: D3 interpreter refers to the source src: X1, takes
Xml-elements “e1” ﬁnding someone who Xml-Attribute “k1” is the desired value “123”,
and loads the appropriate subtree in Dom-Ob’ object. As a result, Dom-object «D3» will
be downloaded Xml-data corresponding to the model shown in Fig. 2c.
The streaming algorithm of elements processing of source document starts with
initialization tool streaming input Xmlreader.
Next step in the loop is executed streaming node reading from a document source.
We check whether the ﬁnding element is present in the source attribute, and in this case
checked is whether the processed node is desired. The loop is moving names from the
list given in the attribute wanted, compare with the processed node name in the case of
coincidence checked. At the end of the loop we check the box matches and, if unchecked,
the node processing is completed and we are directed to the next processing. If desired
processed node is recognized, it is checked for fulﬁllment of ﬁltering speciﬁed attribute
cond. These conditions require treatment to other components of the document input.
For this item processed (with contents) produced in the cache. Then in a series of “cond”
attribute this kind of conditions obtained and tested for processed node is in the cache.
If the processed node passed all inspections, he joined as a child element to the target
element machined parent document. Further puriﬁcation is carried captive subsidiary
The Method of Big Data Processing for Distance Educational System
467

element of internal elements whose names are given in the attribute ignore. Once
processed node source document veriﬁed attached to the target element and reﬁned, for
a worked out recursively internal data sources (Fig. 3).
Unstructured date
Semistructured date
Structured data
Smart-connectors
ETL for mediadata:
classification, indexing,
normalization, entity
finding
ETL for text and
Web-data:
clearing,
normalization,
structure mining
Stream
processing and
data recognition
in real-time mode
Input stream: preprocessing
Data types: database, knowledge base (ontology), index,
metadata
Queries: view, navigation, visualization, exact (context, sematnic)
finding, text extraction, semantic processing
Query constructor
Output stream
User interface
Fig. 3. Universal structure mining multistructural information
3.2
The Structure of the Remote Education Resource Center
The main task of information technology for remote education resource center (RERC)
is organizing and conducting a full educational process. Additionally, the system can be
used for remote control of knowledge and become a didactic tool for self-training.
The overall structure of the center is shown on Fig. 4.
468
N. Shakhovska et al.

Laboratory 1
Education
content
management
Education
content
database
WEB-
server
Remote Education
Resource Center
….
Access points
Laboratory n
Fig. 4. The overall structure
The system of distance training and consultation center performs the following
functions:
• creation of learning objects, which are available as part of educational materials;
• learning Content Management;
• remote student management;
• adaptive Learning Organization;
• testing and remote control of knowledge of the student;
• statistical data analysis.
The laboratory on the Fig. 4 has such technical and program features as:
• thin client as workplace;
• camera for tutor controlling;
• the system OrCAD PSpice for practical WORK.
The following learning modes of the RERC structure in Fig. 4 are available:
1. Testing students’ knowledge and learning. For this purpose there is used a database
of training materials, training and management server content and Web-server.
Student through collective access points (remotely or from the workplace at the
university) accesses training materials. The student’ testing can be used for assess‐
ment of his knowledge (of the interim evaluation) and for selfeducation.
2. Implementation of laboratory and practical work in the laboratories of other univer‐
sities. In this mode, there are involved laboratories with established hardware and
software simulation of analog electronic circuits, a database of training materials,
training and management server content and Web-server. With established in the
laboratory Web-cameras (one for 2-3 workgroups) teacher can observe the work of
the student. This mode of learning is close to the regime of passing certiﬁcation in
IT companies.
3. Implementation of laboratory and practical work in the laboratories of the Univer‐
sity. The diﬀerence from the previous mode of operation is that the teacher not only
oversees the work, but also can adjust it by removing the relevant messages in the
system.
The Method of Big Data Processing for Distance Educational System
469

3.3
The Results Analysis
The overall structure of the center (Fig. 4) implies the existence of heterogeneous
components:
• Relational database (Microsoft SQL Server Database Services, Oracle Database,
MySQL, PostgreSQL) 64-bit performance;
• Multidimensional database (Microsoft SQL Server Analysis Services or Hyperion
Essbase) 64-bit performance;
• “Hierarchical” database (MongoDB);
• The control system of federated data warehouse, which is a separate program
designed speciﬁcally to ensure that the data store, and includes ﬁle storage.
In order to develop a federal system management data warehouse platform used
Microsoft.Net, C# language and development environment Visual Studio. Class Library
that comes with .net and high-level language C#, and methodology RAD (rapid appli‐
cation development), which built Visual Studio development environment to quickly
build application-oriented database.
In the ﬁrst study the federated data sources carried. We analyze the relative number
of objects or documents available data sources to the total number of objects that hit the
federated repository. For the feasibility of diﬀerent sources, we must have data directory
(Data catalog), which is also posted on the Web server. Data catalog is a register of data
resources system that contains basic information about each of them: source name,
location, size, creation date and owner, etc. Product is infrastructure for most other
services RERC dataspace.
Data catalog structure is shown on Fig. 5.
Id
Resource name
Setting
Path
Recource id
Date create
Method of update
Refresh rate
Size
File count
Resource link
Resource metadata
Id
Metadata id
Name
Data type
Working type
Setting
Resource characteristic resource
Id
Surname
Post
Registering date
Profile
User metadata
Id
User id
Resource id
Access type
User right metadata
E-mail
Password
Id
User id
Resource id
Keyword
Resource find metadata
Fig. 5. Data catalog structure
It also indicated the data structuring and placement in storage (serial number ﬁeld
and the type of database).
470
N. Shakhovska et al.

The next step is design of query processing circuit in the system center (Fig. 6). It
consists of the following components:
• CEO server component (Main),
• authentication and authorization component (Auth),
• component to ensure quality of service (Qo),
• component, which provides safety information about the source (Catalog),
• management associations (Class),
• quality control data (Qo).
Application
Dataspace server
Class
accosiation
DB
DB server
DB
USERS
AUTH
DB
QoD
Web
Catalog
Main
Class
QoS
Web server
AUTH
Indexing
Invert index
Metadata
request
authentication
and authorization
request
DB request
Read/write
Dataspace
request
Dataspace
request
Fig. 6. The scheme of processing queries in Remote education resource center using Big data
technology
Table 1. Testing data
Source
Weight
Destination
MFUVUDB.t_aspnet_Roles
0
RDB
MFUVUDB.t_mfuvu_Settlements
1
RDB
MFUVUDB.t_vw_mfuvu_list_sti_correspond
0.666942
RDB, XML
MFUVUDB.t_vw_mfuvu_Users
0.401167
RDB, XML
TransBudgDB.t_VXXDDMMYY
0.961365
RDB
TransBudgDB.t_D_BUDG_LOCAL_DET
0.888913
RDB
TransBudgDB.t_D_ECON_CRED
0.9
RDB, XML
TransBudgDB.t_D_FIN
0.916667
RDB, XML
TransBudgDB.t_DMYYMM
0.533333
RDB, XML
TransBudgDB.t_D_INC_DET
0.903839
RDB
TransBudgDB.t_district
1
RDB
TransBudgDB.t_obl_region
1
RDB
TransBudgDB.t_VW_EXPENSES_DET
0.5
RDB, XML
FUPortalDB.t_People
0.227109
RDB, XML
FUPortalDB.t_Conclusions
1
RDB
FUPortalDB.t_aspnet_Applications
0.666667
RDB, XML
The Method of Big Data Processing for Distance Educational System
471

The next step is data source’s weight calculation (Table 1).
After that we analyze the quality of data processing method (Fig. 7).
Fig. 7. The quality of data processing
In the submitted work diagram analysis algorithm federal inquiry. The algorithm
compared to unmodiﬁed integration of the algorithm used in the Oracle Data Integrator.
Data in federated repository database come from various institutions known data struc‐
tures are unknown. The number of input records databases have to get into federated
data warehouse - 15000.
4
Conclusions
The paper solved important scientiﬁc task of organizing and integrating information
resources using Big data technologies for distance learning system.
The main deﬁnes of Big data are given and the main characteristics are described.
In the ﬁrst chapter there are analyzed mathematical means submission and processing
of Big data and deﬁne their limitations. There are posted software for working with large
data.
The formal description of Big data is deﬁned. There are posted patterns of associa‐
tions between entities and characteristics for various categories NoSQL databases.
However, the main operation is no consolidation of integration and federalization,
allowing capacitive reduce the complexity of requests.
The method of heterogeneous data sharing and bringing to the relational data model
“entity-characteristic” is developed. The software created for developed methods and
algorithms testing.
472
N. Shakhovska et al.

To achieve the desired goal, the development of a formal model of the Big Data
information technology is made and its structural elements are described.
References
1. Laney, D.: The Importance of ‘Big Data’: A Deﬁnition. Gartner (2012)
2. Beyer, M.: Gartner Says Solving ‘Big Data’ Challenge Involves More than Just Managing
Volumes of Data. Gartner, Archived from the original, 10 July 2011
3. Di Ciaccio, A., Coli, M., Ibanez, J.M.A. (eds.): Advanced Statistical Methods for the Analysis
of Large Data. Springer, Berlin (2012)
4. Veres, O., Shakhovska, N.: Elements of the formal model big date. In: 2015 XI International
Conference on Perspective Technologies and Methods in MEMS Design (MEMSTECH), pp.
81–83. Lviv (2015)
5. Shakhovska, N.: Consolidated processing for diﬀerential information products. In:
Perspective Technologies and Methods in MEMS Design, pp. 176–177. Polyana (2011)
6. Fang, L., Sarma, A.D., Yu, C., Bohannon, P.: Rex: explaining relationships between entity
pairs. Proc. VLDB Endowment 5(3), 241–252 (2011)
7. Chen, M., Mao, S., Liu, Y.: Big data: a survey. Mobile Netw. Appl. 19(2), 171–209 (2014)
8. Van der Waerden, B.L.: Mathematische Statistic. Springer-Verlag, Berlin (1957). English.
Translation of 2nd (1965) ed. Springer-Verlag, Berlin and New York (1969)
9. Educational Technology & Society 5(1), pp. 215–221 (2002). ISSN 1436-4522
10. Shakhovska, N.: The method of Big data processing. In: 2017 XII International Conference
on Computer sciences and information technlogies (CSIT), pp. 122–125. Lviv (2017)
The Method of Big Data Processing for Distance Educational System
473

Developments and Prospects of GMDH-Based
Inductive Modeling
Volodymyr Stepashko(&)
International Research and Training Centre for Information Technologies
and Systems of the NAS and MES of Ukraine, Kyiv 03680, Ukraine
stepashko@irtc.org.ua
Abstract. The article provides information on the historical development of the
scientiﬁc direction of inductive modeling, originated by Ukrainian scholar Pro-
fessor Oleksiy Ivakhnenko in 1968 with creation of his Group Method of Data
Handling, as well as characterizes the basic fundamental, applied and techno-
logical achievements. The term inductive modeling can be deﬁned as a self-
organizing process of evolutional transition from initial data to mathematical
models reﬂecting some patterns of functioning objects and systems implicitly
contained in available experimental, trial or statistical data.
The structured information is presented on the development of GMDH-based
inductive modeling in Ukraine and abroad, main fundamental, technological and
applied achievements are characterized, as well as the most prospective ways of
further research are formulated. The performed survey of the research state in
the ﬁeld of inductive modeling shows that GMDH is one of the most powerful
methods of data mining and a promising basis for creating modern information
technologies for discovering knowledge from observation data.
Keywords: Inductive modeling  Self-organization  GMDH  Data mining
Knowledge discovery  Survey
1
Introduction
The very ﬁrst article on the group method of data handling (GMDH) was published in
1968 by Professor Oleksiy Ivakhnenko in the Ukrainian journal «Avtomatyka». This
article was republished in the USA [1] and followed by next important publications [2,
3] abroad which manifested the creation of new scientiﬁc direction called now «in-
ductive modeling of complex systems and processes». Since then, the new modeling
method is being recognized by researchers around the world and various GMDH-based
techniques and technologies are intensively developing till nowadays.
The author called this scientiﬁc direction variously: «heuristic self-organization of
models» [4], «self-organization of models from experimental data» [5], «inductive
method for self-organizing models» [6], «inductive learning algorithms» [7]. Finally,
in 1998, when the Department for information technologies of inductive modeling
(ITIM) was established at the IRTC ITS of NAS of Ukraine, the direction obtained its
short name «Inductive Modeling» being now generally accepted for scientiﬁc forums,
articles and books in Ukraine and abroad.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_34

The term inductive modeling can be deﬁned as a self-organizing process of evo-
lutional transition from initial data to mathematical models reﬂecting some functioning
patterns of the modeled objects and systems implicitly contained in the available
experimental, trial or statistical information under the uncertainty conditions. The task
of inductive modeling consists in an automated construction of a mathematical model
approaching an unknown regularity of functioning the simulated object or process.
An enormous amount of articles has published during the 50-years period in the
world on the subject of GMDH-based inductive modeling; some of them are reﬂected
in surveys [8–11]. In this article, a structured information is presented on the devel-
opment of inductive modeling in Ukraine and abroad, including the ITIM department
as the follower of the Ivakhnenko’s school; basic fundamental, technological and
applied achievements are characterized, as well as the most prospective ways of further
research development are formulated.
2
Main Features of the Inductive Approach to Model
Building
Inductive modeling consists in construction of models based on empirical data, results
of measurements, observations, experiments, when this statistical material is used to
form mathematical models, plausibly describing the patterns displayed in the data. This
makes it possible to classify GMDH as an effective method of data mining and
computational intelligence, since it is aimed at automatic search and description of
regularities with the choice of structure and parameters of linear, nonlinear, difference
and other models based on data samples under condition of uncertainty and incom-
pleteness of information.
Instead of the traditional deductive way of solution of modeling tasks «from gen-
eral theory – to particular model», Ivakhnenko has formulated the principle of new
inductive approach based on the intensive use of computers, as way «from particular
data – to general model»: a researcher having observation data advances a hypothesis
on the class of functions, forms procedure of automatic generation of many thousand
variants of models in this class and sets the criterion of choice of the best model among
all of generated ones. It gives the most objective result along with maximum activation
of creative possibilities of the researcher.
The Ivakhnenko’s idea of models self-organization consisted in maximal automa-
tion of the whole process of model construction from observation data. The GMDH is
the basic instrument of the self-organizing modelling. This is a fundamentally new
method of modeling from experimental data.
GMDH differs from other data-driven modeling methods by active application of
the following principles:
(1) automatic generation of ever more complex structures of models,
(2) non-ﬁnal decisions (multiple choice of models) in every stage, and
(3) successive selection of best models by external criteria for the construction of
models of optimum complexity.
Developments and Prospects of GMDH-Based Inductive Modeling
475

It possesses an original network-type multilayered procedure of automatic gener-
ation of model structures simulating the process of biological selection with the pair-
wise account of successive features. For comparison and choice of best models,
external criteria based on dividing the sample into two parts are used, thus evaluating
the parameters and checking the models quality is executed on different subsamples.
The sample division into parts with similar statistical characteristics helps to auto-
matically take into account different types of uncertainties in the modeling process
providing a possibility to avoid burdensome a priory assumptions.
3
Brief Historical Information About the Development
of Inductive Modeling as a Scientiﬁc Area
In the 1970s, the method effectiveness was repeatedly conﬁrmed by solving speciﬁc
problems of modeling complex objects and processes in the areas of ecology, economy,
technology and hydrometeorology [4, 5]. The clearness of the method led to a rapid
growth of its popularity among researchers and practitioners around the world.
In the 1980s, intensive research continued on the application of GMDH to solving
practical problems of modeling economic and ecological processes and systems [6],
including that done by foreign authors [12, 13]. At the same time, research was evolved
on the development of algorithms for self-organization in the ﬁeld adjacent to mod-
eling, namely in problems of classiﬁcation and pattern recognition [6, 13].
In the early 1980s, A.G. Ivakhnenko established an organic analogy between the
problems of constructing a model from noisy experimental data and passing a signal
through a channel with noise [14]. This allowed building foundations of the theory of
noise-immune modeling [15]. The main result of the theory: the higher is the noise
level in the data, the simpler it should be the optimal model complexity. This means
that under noise conditions, a reduced “nonphysical” model can have smaller error than
the “physical” one.
In the same years, the so-called two-level predictive models were offered [15] for
modelling of dynamics of cyclic processes, in particular economic and ecological ones.
This was a new method of construction of systems of two-level difference models for
cyclicities of the “cycle-time” type. The variables of upper (cycle-averaged) and lower
(time-averaged) levels in their balance relationship are thus considered. A correspon-
dent algorithm and its applications were described in [16].
In 1994, the Ivakhnenko’s book [7] was published in the USA, containing the main
results of his school of the 1980th to early 1990th. These results include such algo-
rithms: objective system analysis (OSA [17]) intended for determination of input and
output variables among all measured ones and building the system of difference
equations of an object dynamics, and objective computer clusterization (OCC [18])
solving the tasks of clustering with the use of self-organization of the quantity and
composition of clusters. Besides that, new nonparametric prediction algorithms based
on correlation models, a reverse transformation of the transition probabilities matrix
into forecast [19] and the group analogs complexing [20] were introduced.
In 1990th, the classical multilayered GMDH algorithm has commenced to be called
as the “polynomial neural network” PNN [21]. The main result in these years was an
476
V. Stepashko

Ivakhnenko’s idea on the development of GMDH as a “neuronet with active neurons”
[22, 23]. The idea was to create a network of neurons in the form of GMDH algo-
rithms, the resulting structure turns into a network with heterogeneous neurons tuned
independently. In the same years, GMDH began to be classiﬁed as the successful
means of intelligent data mining [24].
It is worth to remember other constructive and efﬁcient ideas suggested and evolved
by Ivakhnenko regarding various new algorithms of model construction in the fol-
lowing original model classes: harmonic trend models of oscillatory processes in the
form of sum of harmonics with aliquant frequencies [25]; nonlinear additive-
multiplicative models with automatic determination of non-integer degrees of input
variables [26]; models of distributed processes dynamics in the form of difference
analogues of partial differential equations [27].
4
Main Results of GMDH Theoretical Studies
In the area of the GMDH theory, algorithms for generating variants of model structures
as well as criteria for selection of the best generated models are analyzed ﬁrst of all.
The main properties of the sorting-out GMDH algorithms are described in [15] while of
the iterative algorithms in [28].
When studying any of multilayer (iterative) GMDH algorithms, the question of
conditions for their convergence should be analyzed in the ﬁrst line [28]. Historically
this problem was examined in 1970–80th [29–31], the most comprehensive results was
obtained in [28] and represented in [7]. It should be noted that all the achieved results
was obtained only for the case of so-called “internal convergence” of the iterative
algorithms, when the sample division is not taken into account and the residual sum of
squares RSS is used as the “internal” selection criterion. The problem of “external
convergence” of the algorithms for the case of using external criteria based on the
sample division is still to be solved.
The task of analyzing the effectiveness of the model selection criteria was con-
sidered within the framework of the theory of noise-immune modeling [15]. Essential
results are related to analysis of properties of the so-called “ideal” J-criterion being
ﬁrstly introduced in [32] and investigated in [15, 33, 34]. Selective properties of the
predictions balance criterion used for construction of systems of two-level difference
models of cyclic processes was studied in [35].
The basic analytical apparatus of the theory of inductive modeling is the method of
critical variances for analyzing the regularities of model choice under uncertainty
conditions. This instrument, ﬁrstly proposed in [32], was applied in [15] and developed
in [36–38]. The method is presented systematically in [39].
Asymptotic properties of the external criteria for models selection were studied in
[40, 41] under assumption of inﬁnite growth of the sample length. The GMDH theory
foundations for objects with multidimensional output are also formed [42, 43].
Developments and Prospects of GMDH-Based Inductive Modeling
477

5
General Characteristics of the Inductive Modeling Task
If a sample W ¼ ½X y of n observations over m input variables X and one output y is
given, then the problem of constructing models from experimental data consists gen-
erally in ﬁnding the minimum of a given external criterion CRðX; y; f Þ on a discrete set
U of models of different structures of the form ^yf ¼ f ðX; ^hf Þ, where the parameter
estimation ^hf for each f 2 U is a solution of additional task of continuous minimization
of an internal error criterion QRðX; y; hf Þ, QR 6¼ CR.
In general, the process of solving the problem of structural and parametric identi-
ﬁcation includes the following main stages: (1) setting a data sample and a priori
information; (2) the choice or assignment of a class of basic functions and the respective
transformation of data; (3) generation of different model structures in this class;
(4) estimation of parameters of generated structures by an internal criterion QRðhf Þ and
formation of the set U; (5) minimization of a given external criterion CR(f) and selection
of the optimal model f*; (6) checking the adequacy of the model obtained; (7) com-
pletion of the process and/or use the model.
This means that each modeling method including GMDH can be described with
help of four main components: (a) model class, (b) structure generator, (c) method for
estimating parameters, and (d) criterion for model selection. Typical GMDH applica-
tions are modeling of nonlinear systems, forecasting complex processes, function
approximation, recognition, classiﬁcation, clustering and others.
6
Basic Components of the GMDH Algorithms
6.1
Model Classes Used in GMDH Algorithms
In the practice of modeling, the following main types of tasks are occurred: (1) building
regression models of static objects; (2) modeling of time series or processes; (3)
modeling of dynamic objects, processes and systems.
Models of Static Objects. The term “static object” unites all cases when it is necessary
to build a model of the form yk ¼ Pm
1 hjgj xk
ð
Þ, where k = 1, …, n is the number of the
observation point, hj are model parameters, g(x) is the given m-dimensional vector
function of mx input variables. The basic functions g(x) in GMDH algorithms are, ﬁrst
of all, polynomials, and each component of g(x) is a monomial.
Time Series Models. To model processes as time series, polynomial, trigonometric,
exponential, logarithmic and logistic functions of time are applied in GMDH algo-
rithms, as well as autoregressive models of the form yk ¼ Ply
1 aiyki, where ly is the
autoregression order.
Models of Dynamic Objects. The problems of modeling dynamics are solved ﬁrst of all
in the class of linear dynamic models with a vector of external actions x. Then the
general form of the linear dynamic model of a multidimensional system is represented
by the difference equation yk ¼ Ply
1 aT
ayka þ Plx
1 bT
v xkv þ 1, where ly, lx are the
478
V. Stepashko

numbers of the considered past values (delays, lags) for output and input variables,
aa; bm are vectors of unknown parameters.
In each GMDH algorithm there is a data conversion block in accordance with the
speciﬁed model class, and then the task of model y ¼ f ðx; ^hÞ ¼ xT^h synthesis is solved,
where the original task speciﬁcity is not taken explicitly into account.
6.2
Basic Generators of Model Structures
Most of the known GMDH generators of model structures are naturally divided into
two groups, sorting-out and iterative (like optimization methods) generators differing
by ways of forming structures and searching for a minimum of a given criterion.
Sorting-Out Algorithms [15, 44]. They are intended for solving the problem by
comparing models from a ﬁnite set U whose elements can be calculated independently
by estimating parameters of all models. Then the problem of discrete programming can
be solved by a complete or directed search.
The most known is an exhaustive search implementation in the form of the com-
binatorial
algorithm
COMBI
[45]
generating
all
possible
model
structures
yv ¼ Xv^hv; v ¼ 1; . . .; 2m. Here, the number of models being compared is 2m and the
exhaustive search is effective only up to approximately m = 25.
The purpose of directed search methods is to ﬁnd the global minimum of the
selection criterion, i.e. the result of an exhaustive search, by signiﬁcantly smaller cal-
culations. They are runnable with hundreds of arguments, e.g. the multistage algorithm
MULTI [46] which uses a special step-by-step evolving procedure of the type
^yl
s ¼ ðXi
s1jx j
sÞ^hs; s ¼ 1; m; i; l ¼ 1; Fs1, where s is the stage number (and the structure
complexity); Fs – number of the best structures (freedom of choice) at the stage s; j is the
index of a regressor vector being added to the matrix Xi
s1. In this case, a subset Us  U
is analyzed, with a high probability containing the result of a complete search. This
algorithm has the polynomial complexity of order m3.
The monograph [44] is devoted to the numerical research and practical application
issues of sorting-out GMDH algorithms. A comprehensive survey of the available
techniques of exhaustive and partial search of models in such algorithms and their
comparative analysis is presented in [47].
Iterative Algorithms [28, 48]. According to the principle of operation, the iterative
GMDH
algorithms
are
similar
to methods
of optimization using
successive
approaching, but the principle of indecisive solutions (freedom of choice F) is sub-
stantially used. Depending on the method of successive approaching, the iterative
GMDH algorithms may be divided into two groups: multilayered and relaxational ones.
Multilayered algorithms of the MIA GMDH type are constructed by the analogy
with the biological selection of living organisms: the complication of models from r-th
layer to (r + 1)-th occurs due to the “crossing” of all possible pairs from F best models
of the previous layer. Typically, the partial description of MIA is of the form
yr þ 1
l
¼ fl yr
i ; yr
j


; r ¼ 0; 1; . . .; i; j ¼ 1; F; l ¼ 1; C2
F, where second order polynomial
is usually used, but bilinear or even linear one may be applied. The iterative process of
model complication stops after the criterion CR starts to increase.
Developments and Prospects of GMDH-Based Inductive Modeling
479

In algorithms of relaxational type called now RIA GMDH, models are complicated
on each layer by “crossing” the best models of the previous layer with the initial
arguments. In the RIA case, a partial description of the following modiﬁcation yr þ 1
l
¼
flðyr
i ; xkÞ; i ¼ 1; F; k ¼ 1; m may be used with quadratic, bilinear or linear polynomial.
Such kind of descriptions was introduced at ﬁrst in [48–50] to avoid some known
drawbacks of the MIA GMDH: for instance, they help to exclude the possibility of
losing relevant arguments.
The monograph [51] is devoted to the description, research and application of
high-performance GMDH algorithms of the relaxational type with partial description
yr þ 1
l
¼ yr
i þ /ðxkÞ; i ¼ 1; F; k ¼ 1; m. Typical architectures of seven different itera-
tive algorithms are presented in the generalized algorithm GIA GMDH [52]. As it was
mentioned above, for iterative algorithms, in contrast to sorting-out ones, it becomes
necessary to prove the convergence of the iterative process. In general, iterative
methods are operable at m > 1000, and they allow constructing models even in
degenerate problems when n < m.
6.3
External Criteria of Models Quality
The criteria are based on dividing the sample into at least two nonoverlapping sub-
samples A and B, A [ B = W. They are called usually training A, checking (or testing)
B and learning (or working) W data sets. An additional subset C called examination (or
validation) one may be divided for veriﬁcation of a model adequacy.
Let us introduce the following denotations: the least-squares estimation of the
parameters at some subset G is ^hG;
G ¼ A; B; W; and the error on a subset Q of a
model with parameters estimated on G, is equal to DðQjGÞ ¼
yQ  XQ^hG


2
; Q = A,
B, W. Taking this into account, one can give the following computational formulae for
the main external criteria.
Accuracy Criteria. When Q = G, the value eG ¼ D G G
j
ð
Þ is equal to the residual sum
of squares RSS, and if G6¼Q, then it is so called regularity criterion: ARB ¼
DðBjAÞ; ARA ¼ DðAjBÞ: The symmetric form of the criterion: AD ¼ DðBjAÞ þ DðAjBÞ.
Conformity Criteria. The main one in this group is the consistency criterion (or un-
biasedness
criterion):
CB ¼
XW^hA  XW^hB


2
¼ ð^hA  ^hBÞTXT
WXWð^hA  ^hBÞ.
This
group includes also the so-called variability criterion which may be expressed as
difference of internal criteria: CV ¼ ð^hA  ^hWÞTXT
WXWð^hW  ^hBÞ ¼ ðeW  ðeA þ eBÞÞ:
For more details on the spectrum of various GMDH criteria apply to [15, 28].
7
Up-to-Date Trends in the Development of Algorithms
for Self-organization of Models
During the last two decades, the interest to the GMDH-based inductive modelling
algorithms is permanently growing which may be attributed both to their good per-
formance in ill-deﬁned modelling tasks and intensive development of new and
480
V. Stepashko

enhanced algorithms and software tools. These achievements are based on further
evolution of the initial Ivakhnenko’s ideas on models self-organization performed both
by the author with his disciples and a lot of researchers around the world.
GMDH as a Polynomial Neural Network. As it was mentioned above, the multilayered
iterative algorithm MIA GMDH is called now as Polynomial Neural Network
(PNN) [21, 24]. In this case, the main element of the algorithm, namely the partial
description, is considered as an elementary polynomial neuron. The originality and
efﬁciency of the network of such neurons lies in the speed of the process of local
training their weights and automatic global optimization (self-organization) of the
network structure (numbers of “hidden layers” and “nodes” in each layer). To the
explicit advantages of PNN GMDH we can refer the possibility to “fold” the adjusted
network directly into an explicit mathematical expression ready to be used for solving
tasks of simulation, prediction, control and decision making.
Neuronet with Active Neurons. A typical “GMDH neuron” in the form of quadratic
polynomial of two arguments can be called as “passive” because any of the neurons
have the same ﬁxed structure, i.e. the PNN GMDH is homogeneous net. In the 1990s,
Ivakhnenko proposed a new type of GMDH network with active neurons [22, 24] or a
heterogeneous network in which any of the neurons is in turn also a GMDH algorithm,
due to that the structure of the neuron is optimized. As a result, all neurons can get
different structures increasing the ﬂexibility of conﬁguring the network to a speciﬁc
task. Networks of such type are also called as “twice multilayered” ones [24].
GMDH-Like Feedback Neural Network. In [53], a GMDH-like neural network with
feedback was proposed: the neuron outputs of each layer are “crossing” with original
variables. In this algorithm, the network automatically selects one of three architectures
for each neuron: sigmoidal, radial or polynomial transition function. In this case, the
structural parameters, e.g. the number of layers, neurons in the hidden layers and input
variables, are selected automatically, i.e. self-organized.
Group of Adaptive Models Evolution. An evolutionary algorithm called GAME [54],
developed at the Czech Technical University in Prague, is based on the general GMDH
architecture. The main modiﬁcations of this GMDH-like system with active neurons:
the transfer function of a node can be linear, polynomial, logistic, RBF and others; the
network structure is heterogeneous; the number of node inputs increases with the depth
of the node in the network, and there are inter-row links; there are not all the locations
of nodes, but their random subsets; the original GMDH creates one optimal model, and
GAME does a group of locally optimal models.
Inductive Algorithms for Classiﬁcation, Recognition, Clusterization. Essential part of
real-world problems requires application of special methods of pattern recognition in
wide sense. Among current developments in the inductive modelling ﬁeld there are
many methods and tools dealing with tasks of such type, for instance [55–58].
Hybrid GMDH-Type Algorithms and Neural Networks. New and efﬁcient architectures
of neuronets are recently intensively developed on the basis of hybridization of GMDH
procedures and various approaches of computational intelligence and nature-inspired
solutions, e.g.: particle swarm optimization [59, 60], genetic selection and cloning [61],
Developments and Prospects of GMDH-Based Inductive Modeling
481

immune systems [62, 63] etc. A good deal of present hybridization variants is
implemented in the aggregated GMDH-based architecture GAME [54].
Fuzzy and Interval Approaches in Inductive Modeling. For real-world tasks with fuzzy
variables, there were developed corresponding algorithms: multi-layer hybrid fuzzy
PNN [64]; fuzzy GMDH [65] based on the classical structure MIA GMDH;
GMDH-like neo-fuzzy and cascade wavelet-neuro-fuzzy networks [66, 67] etc. For
case of assumptions on interval-given input data, methods [68, 69] were elaborated.
Algorithms Based on Paralleling Operations. There are several effective realizations
of both iterative [70] and combinatorial [71, 72] algorithms with implemented parallel
computations.
Automated Data Preprocessing and Ensembling-Based Prediction. An advanced idea
to build some kind of automatic means for data preprocessing enabling to enhance the
accuracy of classiﬁcation was suggested in [73]. A non-parametric ensembling pro-
cedure [74] for predicting processes is the core idea in the GAME method.
Software Tools Based on GMDH. There are several examples of computer tools for
modelling complex systems completely based on GMDH and/or GMDH-type algo-
rithms. The commercial software complex KnowledgeMiner for Mac [75] described in
[24] and its recent highly advanced realization [76] contains iterative and several
non-parametric GMDH algorithms for fully automatic building and analyzing fore-
casting models. Another variant of commercial software is GMDHshell [77] for PC
based on COMBI and MIA GMDH algorithms. The technology FAKE GAME [54] is
intended for constructing, investigation and application of inductive evolutionary
algorithms of different architecture.
More detailed information on the developed algorithms, tools and technologies,
their research and application can be found on the base site [78], on the site [79] of the
ITIM department, as well as on sites [76, 77].
8
Specialization and Results of ITIM Department
The studies of the ITIM department cover a complete cycle of scientiﬁc research:
methodology of the modeling from data samples; theory of inductive constructing
models of optimal complexity; algorithmization of high-performance modeling tools;
intellectualization of technologies for constructing models; computer experiments to
evaluate the effectiveness of the developed technologies; solving real-world problems
of modeling and forecast; testing and application of developed tools in monitoring,
control and decision support systems.
8.1
Main Scientiﬁc Results of ITIM Department
Starting from 1998, there was developed:
• the GMDH-based inductive modeling theory with the use of the method of critical
variances [39] which made it possible to explain the nature of the GMDH efﬁciency
482
V. Stepashko

as a method for constructing noise-immune models with minimum prediction error
variance; the problem of optimizing the data sample partitioning into two parts has
been solved [51].
• two-criteria method for extra-determining (additional determination) the model
choice using the new errors unbiasedness criterion [80] eliminating the ambiguity in
choosing the optimal model [81].
• principles of designing and implementing high-performance sorting-out GMDH
algorithms based on recurrent calculations [44], paralleling operations [72] and
sequential selection of informative variables [82], allowing to enhance the dimen-
sionality of the problems being solved.
• principles of constructing hybrid architectures of iterative GMDH algorithms as a
generalization of algorithmic structures of multilayered, relaxational and combi-
natorial types, based on which a generalized iterative algorithm GIA GMDH [52]
was developed as a neural network with active neurons in the form of the COMBI
algorithm for automatic adjustment of a neuron complexity.
• the generalized relaxational iterative algorithm GRIA GMDH based on the use of
high-speed recurrent computations and matrices of normal equations, which allows
solving inductive modeling problems from high-dimension data [51].
• theoretical foundations of a new class of sorting-out GMDH algorithms with the use
of recurrent-and-parallel computations on cluster systems [83] as a basis for
high-performance intelligent modeling technologies.
• principles of designing technologies for the intelligent modeling of complex sys-
tems based on the use of knowledge bases, inductive data analysis tools and an
intelligent user interface [84]. Such technologies should have three main instru-
mental levels: autonomous modeling from the available database; embedded
modeling as part of a real-time control system; combined modeling of a complex
system for identifying optimal operation modes and critical scenarios.
• theoretical principles and tools for predicting interrelated socio-economic processes
from statistical data in the class of discrete dynamic models of vector autoregres-
sion [85].
8.2
Technologies Developed in the Department
ASTRID methodology of designing GMDH-based computer technologies for building
models of complex systems from statistical data [86] is a base to develop technologies
for discovering regularities, identiﬁcation, prediction, aimed to informational support of
decision-making problems.
Package of software tools [44, 87] is developed for designing, researching and
applying modeling methods, conducting experiments on testing modeling methods and
their components (model classes, generators of model structures, methods for esti-
mating parameters and models selection criteria).
Integrated environment platform [88] is designed for storing and handling infor-
mation in tasks of inductive modelling for business intelligence systems.
Cross-platform software package with an advanced interface [89] is developed in
the Java language for inductive modeling and forecasting of complex objects and
Developments and Prospects of GMDH-Based Inductive Modeling
483

large-scale processes on the basis of the fast-operating GMDH algorithm with
sequential selection of informative and/or sifting of non-informative arguments [48].
The software package ASTRID-GIA [90] for inductive modeling of complex
systems based on various iterative GMDH algorithms makes it possible to use the
generalized algorithm GIA GMDH and all its special cases [52] in online access mode
both over the Internet and in the local network.
Computer system ASPIS [55] for building predictive models on the basis of the
high-speed generalized relaxational iteration algorithm GRIA GMDH [91]. It is
implemented in C++ and allows solving big data problems.
The management decisions informational support system MDISS allows to solve
problems of estimation, analysis and forecasting of the state of complex systems of
interrelated socio-economic processes with the purpose of making reasonable man-
agerial decisions [92].
8.3
Basic Applied Results
The tasks of modeling of such processes were solved:
• dynamics of the quantity of microorganisms in the soil, depending on environ-
mental factors and the dose of contamination with heavy metals [93];
• dynamics of interdependent indicators of the energy and investment areas in the
class of vector autoregression models for a short-term forecast [44, 85];
• dependence of the sputtering (disruption) coefﬁcient of a spacecraft surface under
action of ionized gas jets on the physical properties of the surface coating [44];
• analysis
of
the
ecological
consequences
of
trees
irrigation
with
treated
wastewater [94].
• quantitative assessment of the impact of sea water pollution with bitumoid sub-
stances on the total number of species of benthic organisms in the Sevastopol
bays [95].
• predicting the results of testing blood samples with medical products in order to
determine the most effective for a particular patient [51];
• classiﬁers construction for the differential diagnostics of blood diseases for reducing
the risks of misdiagnosis [51].
9
Prospects of the Research Development
The most promising areas of research development are:
The methodology of intelligent modeling: new paradigms of neural networks with
active neurons and hybrid GMDH-based architectures; theoretical study of modeling
efﬁciency under uncertainty; theory of building models for prediction, classiﬁcation,
recognition, and clustering, including ensemble-based approaches.
The theory and architectures of iterative (neuronet) GMDH algorithms with new
properties: increasing the accuracy of solving modeling problems on the basis of
484
V. Stepashko

generalization of structures of known algorithms; the development of new hybrid
architectures using evolutionary and multi-agent approaches; analysis of the “external”
convergence of iterative algorithms to the true model.
Novel structures of sorting-out GMDH algorithms: increasing the algorithms efﬁciency
on the basis of recurrent-and-parallel computations; development of effective schemes
for sequential selection of the best model variants; construction of hybrid algorithms
using the computational intelligence ideas [96].
New algorithms for inductive solving of classiﬁcation, recognition and clustering
problems: inductive construction of optimal classiﬁcation rules for big data prob-
lems; increasing the effectiveness of inductive solving the multiple-choice recognition
problems based on iterative GMDH algorithms; development of intelligent algorithms
for data analysis and automatic clustering to identify patterns.
The intelligent technologies for informational support of decision making based on
inductive modeling methods and tools: development of tools in the form of an algo-
rithm construction set as a shell for interactive synthesis of new modeling means;
development of intelligent interface means for efﬁcient support of user solutions in the
GMDH-based inductive modeling process; development of a set of software tools to
support the design of intelligent technologies for information support of decisions to
control of complex systems of various nature.
Application of the developed methods and tools in applied problems of intelligent
decisions support based on inductive modeling: support of operational decisions for
managing socio-economic processes at different levels; supporting the solution of
medical diagnosis problems and predicting the effectiveness of drugs and proce-
dures; intelligent modeling and control of technological processes and robotic sys-
tems; support solutions for control the ecological state of processes based on
environmental monitoring data; decisions support in problems of automatic search and
analysis of categories and structures of textual information [97]; identiﬁcation of
knowledge based on automated content analysis of messages in social networks in
order to support the administrative decision making at different levels.
10
Conclusion
The performed survey of the research state in the ﬁeld of inductive modeling shows
that GMDH is one of the most effective methods of computation intelligence and soft
computing as well as a promising basis for creating modern information technologies
for data mining, knowledge discovery, and business intelligence. They form a reliable
base for promoting intensive current and future development of methodology, theory,
algorithms and tools of inductive modelling of complex processes and systems in
applied decision making tasks.
During the whole period of GMDH evolution, this method and corresponding
software means demonstrated good performance when solving real-world modeling
problems of different nature in ﬁelds of environment, economy, ﬁnance, hydrology,
Developments and Prospects of GMDH-Based Inductive Modeling
485

technology, robotics, sociology, biology, medicine, and others. This method represents
the original and efﬁcient facility for solving a wide spectrum of artiﬁcial intelligence
problems including identiﬁcation and forecast, pattern recognition and clusterization,
data mining and search for regularities. This practically boundless area of knowledge is
worth of a special targeted survey.
Further development of the ideas of model self-organization assumes the
improvement of the inductive modeling theory, the development and implementation
of new high-performance algorithms for computer technologies, and the application of
these technologies to solve a wide range of real-world applications for tasks of mod-
eling, forecasting, control and decision-making in systems of different nature.
Other kinds of surveys are also expected to be prepared, both general and topical,
specialized, because the amount of available GMDH publications is enormous and
permanently growing with regard to both theoretical and applied aspects.
Some special attention seems to be done to the relationship between GMDH and
the task and methods of the “Deep learning in neural networks” because GMDH is
positioned as the very ﬁrst example of a deep learning architecture [98].
References
1. Ivakhnenko, A.G.: The group method of data handling – a rival of the method of stochastic
approximation. Sov. Autom. Control 1(3), 43–55 (1968)
2. Ivakhnenko, A.G.: Heuristic self-organization in problems of automatic control. Automatica
(IFAC) 3, 207–219 (1970)
3. Ivakhnenko, A.G.: Polynomial theory of complex systems. IEEE Trans. Syst. Man Cybern. 1
(4), 364–378 (1971)
4. Ivakhnenko, A.G.: Systems of Heuristic Self-Organization in Technical Cybernetics.
Technika, Kiev (1971). (in Russian)
5. Ivakhnenko, A.G.: Long-Term Forecasting and Control of Complex Systems. Technika,
Kiev (1975). (in Russian)
6. Ivakhnenko, A.G.: Inductive Method for Self-Organizing Models of Complex Systems.
Naukova Dumka, Kiev (1982). (in Russian)
7. Madala, H.R., Ivakhnenko, A.G.: Inductive Learning Algorithms for Complex Systems
Modeling. CRC Press, New York (1994)
8. Ivakhnenko, A.G., Müller, J.-A.: Recent developments of self-organising modeling in
prediction and analysis of stock market. Microelectron. Reliab. 37, 1053–1072 (1997)
9. Anastasakis, L., Mort, N.: The development of self-organization techniques in modelling: a
review of the Group Method of Data Handling (GMDH). ACSE research report, vol. 813,
39 p. The University of Shefﬁeld (2001)
10. Snorek, M., Kordik, P.: Inductive modelling world wide the state of the art. In: Proceedings
of 2nd International Workshop on Inductive Modelling, pp. 302–304. CTU, Prague (2007)
11. Stepashko, V.: Ideas of Academician O.H. Ivakhnenko in the inductive modelling ﬁeld from
historical perspective. In: Proceedings of the 4th International Conference on Inductive
Modelling, ICIM-2013, pp. 30–37. IRTC ITS NASU, Kyiv (2013)
12. Farlow, S.J. (ed.): Self-Organizing Methods in Modeling: GMDH Type Algorithms. Marcel
Decker Inc., New York, Basel (1984)
13. Iwachnenko, A.G., Müller, J.-A.: Selbstorganisation von Vorhersagemodellen. VEB Verlag
Technik, Berlin (1984)
486
V. Stepashko

14. Ivakhnenko, A.G., Karpinsky, A.M.: Computer-aided self-organization of models in terms
of the general communication theory (information theory). Sov. Autom. Control 15(4), 7–15
(1982)
15. Ivakhnenko, A.G., Stepashko, V.S.: Noise Immunity of Modelling. Naukova Dumka, Kiev
(1985). (in Russian)
16. Stepashko, V.S., Kostenko, Y.: A GMDH algorithm for two-level modeling of multidi-
mensional cyclic processes. Sov. Autom. Control 20(4), 49–57 (1987)
17. Ivakhnenko, A.G., Kostenko, Y.: System analysis and long-term prediction on the basis of
model self-organisation (OSA algorithm). Sov. Autom. Control 15(3), 11–17 (1982)
18. Ivakhnenko, A.G.: Objective computer clasterization based on self-organisation theory. Sov.
Autom. Control 20(6), 1–7 (1987)
19. Ivakhnenko, A.G., Osipenko, V.V., Strokova, T.I.: Prediction of two-dimensional physical
ﬁelds using inverse transition matrix transformation. Sov. Autom. Control 16(4), 10–15
(1983)
20. Ivakhnenko, A.G.: Inductive sorting method for the forecasting of multidimensional random
processes and events with the help of analogs forecast complexing. Pattern Recogn. Image
Anal. 1(1), 99–108 (1991)
21. Oh, S.K., Pedrycz, W.: The design of self-organizing polynomial neural networks. Inf. Sci.
141, 237–258 (2002)
22. Ivakhnenko, A.G., Ivakhnenko, G.A., Mueller, J.-A.: Self-organization of neuronets with
active neurons. Pattern Recogn. Image Anal. 4(4), 177–188 (1994)
23. Ivakhnenko, A.G., Wunsh, D., Ivakhnenko, G.A.: Inductive sorting-out GMDH algorithms
with polynomial complexity for active neurons of neural networks. In: Proceedings of the
International Joint Conference on Neural Networks, pp. 1169–1173. IEEE, Piscataway
(1999)
24. Muller, J.-A., Lemke, F.: Self-Organizing Data Mining: An Intelligent Approach to Extract
Knowledge from Data. Trafford Publishing Press, Berlin (1999)
25. Vysotskiy, V.N., Ivakhnenko, A.G., Cheberkus, V.I.: Long term prediction of oscillatory
processes by ﬁnding a harmonic trend of optimum complexity by the balance-of-variables
criterion. Sov. Autom. Control 8(1), 18–24 (1975)
26. Ivakhnenko, A.G., Krotov, G.I.: A multiplicative-additive nonlinear GMDH algorithm with
optimization of the power of factors. Sov. Autom. Control 17(3), 10–15 (1984)
27. Ivakhnenko, A.G., Peka, PYu., Vostrov, N.P.: Combined Method for Modeling Water and
Oil Fields. Naukova dumka, Kiev (1984). (In Russian)
28. Ivakhnenko, A.G., Yurachkovsky, Y.: Complex Systems Modeling after Experimental Data,
Radio and Communication. Radio i Swiaz, Moscow (1987). (in Russian)
29. Ivakhnenko, A.G., Kovalchuk, P.I., Todua, M.M., Shelud’ko, O.I., Dubrovin, O.F.: Unique
construction of regression curve using a small number of points. Sov. Autom. Control 6(5),
29–41 (1973)
30. Yurachkovsky, Y.: Convergence of multilayer algorithms of the group method of data
handling. Sov. Autom. Control 14(3), 29–34 (1981)
31. Kovalchuk, P.L.: Internal convergence of GMDH algorithms. Sov. Autom. Control 16(2),
88–91 (1983)
32. Stepashko, V.S.: Potential noise stability of modelling using a combinatorial GMDH
algorithm without information regarding the noise. Sov. Autom. Control 16(3), 15–25
(1983)
33. Kocherga, Y.L.: J-optimal reduction of model structure in the Gauss-Markov scheme. Sov.
J. Autom. Inf. Sci. 21(4), 21–23 (1988)
34. Aksenova, T.I., Yurachkovsky, Y.P.: Characterization of unbiased structure and condition of
its J-optimality. Sov. J. Autom. Inf. Sci. 21(4), 24–32 (1988)
Developments and Prospects of GMDH-Based Inductive Modeling
487

35. Stepashko, V.S.: Noise immunity of choice of model using the criterion of balance of
predictions. Sov. Autom. Control 17(5), 27–36 (1984)
36. Stepashko, V.S.: Investigation of the predicting properties of a recurrent structural-parametric
identiﬁer. Sov. J. Autom. Inf. Sci. 24(3), 31–40 (1991)
37. Stepashko, V.S.: Structural identiﬁcation of predicting models for planned experiment.
J. Autom. Inf. Sci. 25(1), 23–31 (1992)
38. Stepashko, V.S.: Analysis of criteria effectiveness for structural identiﬁcation of forecasting
models. J. Autom. Inf. Sci. 27(3–4), 13–20 (1994)
39. Stepashko, V.S.: Method of critical variances as analytical tool of theory of inductive
modeling. J. Autom. Inf. Sci. 40(3), 4–22 (2008)
40. Stepashko, V.S.: Asymptotic Properties of External Criteria for Model Selection. Sov.
J. Autom. Inf. Sci. 21(6), 84–92 (1988)
41. Aksenova, T.I.: Sufﬁcient conditions and convergence rate using different criteria for model
selection. Syst. Anal. Model. Simul. 20(1–2), 69–78 (1995)
42. Sarychev, A.P.: System criterion of regularity in the group method of data handling.
J. Autom. Inf. Sci. 38(1), 22–35 (2006)
43. Sarychev, A.P.: Modelling in the class of regression equations systems in conditions of
structural uncertainty. In: Proceedings of 2nd International Workshop on Inductive
Modelling, IWIM-2007, pp. 193–203. Czech Technical University, Prague (2007)
44. Stepashko, V.S., Yeﬁmenko, S.M., Savchenko, Y.A.: Computerized Experiment in
Inductive Modeling. Naukova Dumka, Kyiv (2014). (in Ukrainian)
45. Stepashko, V.S.: A combinatorial algorithm of the group method of data handling with
optimal model scanning scheme. Sov. Autom. Control 14(3), 24–28 (1981)
46. Stepashko, V.S.: A ﬁnite selection procedure for pruning an exhaustive search of models.
Sov. Autom. Control 16(4), 84–88 (1983)
47. Moroz, O.G., Stepashko, V.S.: Comparative analysis of model structures generators in
sorting-out GMDH algorithm. Inductive Model. Complex Syst. 8, 173–191 (2016).
IRTC ITS NASU, Kyiv (in Ukrainian)
48. Sheludko, O.I.: GMDH algorithm with orthogonalized complete description for synthesis of
models by the results of a planned experiment. Sov. Autom. Control 7(5), 24–33 (1974)
49. Tamura, H., Kondo, T.: Large-spatial pattern identiﬁcation of air pollution by a combined
model of source-receptor matrix and revised GMDH. In: Proceedings of IFAC Symposium
on Environmental Systems Planning, Design and Control, pp. 373–380. Elsevier, Oxford
(1977)
50. Yurachkovsky, Y.: Restoration of polynomial dependencies using self-organization. Sov.
Autom. Control 14(4), 17–22 (1981)
51. Pavlov, A.V., Stepashko, V.S., Kondrashova, N.V.: Effective Methods of Models
Self-Organization. Akademperiodika, Kyiv (2014). (in Russian)
52. Stepashko, V., Bulgakova, O.: Generalized iterative algorithm GIA GMDH. In: Proceedings
of the 4th International Conference on Inductive Modelling, ICIM-2013, Kyiv, Ukraine,
September 2013, pp. 119–123. IRTC ITS NASU, Kyiv
53. Kondo, T., Ueno, J.: Feedback GMDH-type neural network self-selecting optimum neural
network architecture and its application to 3-dimensional medical image recognition of the
lungs. In: Proceedings of II International Workshop on Inductive Modelling, September
2007, pp. 63–70. Czech Technical University, Prague. ISBN 978-80-01-03881-9
54. Kordik, P.: Fully automated knowledge extraction using group of adaptive model evolution.
Ph.D. thesis, Department of Computer Science and Computers, FEE, CTU in Prague, 150
p. (2006)
488
V. Stepashko

55. Sarychev, A.P.: The solution of the discriminant analysis task in conditions of structural
uncertainty on basis of the group method of data handling. J. Autom. Inf. Sci. 40(6), 27–40
(2008)
56. Sarycheva, L.: Quality criteria for GMDH-based clustering. In: Proceedings of the II
International Conference on Inductive Modelling, ICIM-2008, pp. 84–90. IRTC ITS NASU,
Kyiv (2008)
57. Borisova, I.A.: Calculation of FRiS-function over mixed dataset in the task of generalized
classiﬁcation. In: Proceedings of the III International Conference on Inductive Modelling,
ICIM-2010, pp. 44–50. KNTU, Kherson, Ukraine (2010)
58. Cepek, M., Snorek, M., Chudacek, V.: ECG signal classiﬁcation using GAME neural
network and its comparison to other classiﬁers. In: Proceedings of International Conference
on Artiﬁcial Neural Networks, ICANN 2008, pp. 768–777. Springer, Heidelberg (2008)
59. Voss, M.S., Feng, X.: A new methodology for emergent system identiﬁcation using particle
swarm optimization (PSO) and the group method of data handling (GMDH). In: Proceedings
of the Genetic and Evolutionary Computation Conference, pp. 1227–1232. Morgan
Kaufmann Publishers, New York (2002)
60. Onwubolu, G., Sharma, A., Dayal A. et al.: Hybrid particle swarm optimization and group
method of data handling for inductive modeling. In: Proceedings of 2nd International
Conference on Inductive Modelling, pp. 95–103. IRTC ITS NASU, Kyiv (2008)
61. Jirina, M., Jirina Jr., M.: Genetic selection and cloning in GMDH MIA method. In:
Proceedings of the II International Workshop on Inductive Modelling, IWIM 2007, pp. 165–
171. CTU, Prague (2007)
62. Lytvynenko, V., Bidyuk, P., Myrgorod, V.: Application of the method and combined
algorithm on the basis of immune network and negative selection for identiﬁcation of turbine
engine surging. In: Proceedings of the II International Conference on Inductive Modelling,
ICIM-2008, pp. 116–123. IRTC ITS NASU, Kyiv (2008)
63. Lytvynenko, V.: Hybrid GMDH cooperative immune network for time series forecasting. In:
Proceedigns of the 4th International Conference on Inductive Modelling, pp. 179–187.
IRTC ITS NASU, Kyiv (2013)
64. Oh, S.K., Pedrycz, W., Park, H.S.: Multi-layer hybrid fuzzy polynomial neural networks: a
design in the framework of computational intelligence. Neurocomputing 64, 397–431 (2005)
65. Zaychenko, Y.: The investigations of fuzzy group method of data handling with fuzzy inputs
in the problem of forecasting in ﬁnancial sphere. In: Proceedings of the II International
Conference on Inductive Modelling, ICIM-2008, pp. 129–133. IRTC ITS NASU, Kyiv
(2008)
66. Bodyanskiy, Y.V., Zaychenko, Y.P., Pavlikovskaya, E.: The neo-fuzzy neural network
structure optimization using the GMDH for the solving forecasting and classiﬁcation
problems. In: Proceedings of the 3rd International Workshop on Inductive Modelling,
IWIM-2009, Krynica, Poland, pp. 10–17. Czech Technical University, Prague (2009)
67. Bodyanskiy, Y., Vynokurova, O., Teslenko, N.: Cascade GMDH-wavelet-neuro-fuzzy
network. In: Proceedings of the IV International Workshop on Inductive Modelling,
IWIM-2011, pp. 16–21. IRTC ITS NASU, Kyiv (2011)
68. Dyvak, M., Manzhula, V., Pukas, A., Stakhiv, P.: Structural identiﬁcation of interval models
of the static systems. In: Proceedings of 2nd International Workshop on Inductive
Modelling, pp. 132–139. Czech Technical University, Prague (2007)
69. Voytyuk, I., Dyvak, M., Spilchuk, V.: The method of structure identiﬁcation of
macromodels as difference operators based on the analysis of interval data and genetic
algorithm. In: Proceedings of the IV International Workshop on Inductive Modelling,
IWIM-2011, pp. 114–118. IRTC ITS NASU, Kyiv (2011)
Developments and Prospects of GMDH-Based Inductive Modeling
489

70. Lemke, F.: Parallel self-organizing modeling. In: Proceedings of the II International
Conference on Inductive Modelling, ICIM-2008, pp. 176–183. IRTC ITS NASU, Kyiv
(2008)
71. Koshulko, O.A., Koshulko, A.I.: Multistage combinatorial GMDH algorithm for parallel
processing of high-dimensional data. In: Proceedings of III International Workshop on
Inductive Modelling, IWIM-2009, pp. 114–116. CTU, Prague (2009)
72. Stepashko, V., Yeﬁmenko, S.: Parallel algorithms for solving combinatorial macromodelling
problems. Przegląd Elektrotechniczny (Electr. Rev.) 85(4), 98–99 (2009)
73. Čepek, M., Kordík, P., Šnorek, M.: The effect of modelling method to the inductive
preprocessing algorithm. In: Proceedings of the III International Conference on Inductive
Modelling, ICIM-2010, pp. 131–138. KNTU, Kherson (2010)
74. Kordík, P., Černý, J.: Advanced ensemble strategies for polynomial models. In: Proceedings
of the III International Conference on Inductive Modelling, ICIM-2010, pp. 77–82. KNTU,
Kherson (2010)
75. http://knowledgeminer.eu. Accessed 17 May 2017
76. Lemke, F.: Insights v.2.0, self-organizing knowledge mining and forecasting tool (2013).
http://www.knowledgeminer.eu. Accessed 21 May 2017
77. https://www.gmdhshell.com. Accessed 12 Apr 2017
78. www.gmdh.net. Accessed 15 Apr 2017
79. www.mgua.irtc.org.ua. Accessed 11 May 2017
80. Ivakhnenko, A.G., Ivakhnenko, G.A., Savchenko, E.A.: GMDH algorithm for optimal model
choice by the external error criterion with the extension of deﬁnition by model bias and its
applications to the committees and neural networks. Pattern Recogn. Image Anal. 12(4),
347–353 (2002)
81. Ivakhnenko, A.G., Savchenko, E.A.: Investigation of efﬁciency of additional determination
method of the model selection in the modeling problems by application of GMDH algorithm.
J. Autom. Inf. Sci. 40(3), 47–58 (2008)
82. Samoilenko, O., Stepashko, V.: A method of successive elimination of spurious arguments
for effective solution of the search-based modelling tasks. In: Proceedings of the II
International Conference on Inductive Modelling, pp. 36–39. IRTC ITS NASU, Kyiv (2008)
83. Yeﬁmenko, S., Stepashko, V.: Intelligent recurrent-and-parallel computing for solving
inductive modeling problems. In: Proceedings of 16th International Conference on Com-
putational Problems of Electrical Engineering, pp. 236–238. LNPU, Lviv, Ukraine (2015)
84. Stepashko, V.S.: Conceptual fundamentals of intelligent modeling. Control Syst. Mach.
(USiM) 4, 3–15 (2016). (in Russian)
85. Yeﬁmenko, S.N.: Construction of systems of predictive models for multidimensional
interrelated processes. Control Syst. Mach. (USiM) 4, 80–86 (2016). (in Russian)
86. Stepashko, V.S.: GMDH algorithms as basis of modeling process automation after
experimental data. Sov. J. Autom. Inf. Sci. 21(4), 33–41 (1988)
87. Yeﬁmenko, S., Stepashko, V.: Technologies of numerical investigation and applying of
data-based modeling methods. In: Proceedings of the II International Conference on
Inductive Modelling, ICIM-2008, pp. 236–240. IRTC ITS NASU, Kyiv (2008)
88. Shcherbakova, N., Stepashko, V.: Integrated environment for storing and handling
information in tasks of inductive modelling for business intelligence systems. In: Setlak,
G., Alexandrov, M., Markov, K. (eds.) Artiﬁcial Intelligence Methods and Techniques for
Business and Engineering Applications, pp. 210–219. ITHEA, Rzeszow, Poland, Soﬁa,
Bulgaria (2012)
89. Samoilenko, O.A.: Designing new GMDH algorithms as basic components of a modeling
subsystem. Inductive Model. Complex Syst. 3, 191–208. IRTC ITS NASU, Kyiv (2011). (in
Ukrainian)
490
V. Stepashko

90. Bulgakova, O., Zosimov, V., Stepashko, V.: Software package for modeling of complex
systems based on iterative GMDH algorithms with the network access capability. Syst. Res.
Inf. Technol. 1, 43–55 (2014). (in Ukrainian)
91. Pavlov, A.: Design patterns of automated structure-parametric identiﬁcation system. In:
Proceedings of 6th International Workshop on Inductive Modelling, pp. 31–35. IRTC ITS
NASU, Kyiv (2015)
92. Stepashko, V., Samoilenko, O., Voloschuk, R:. Informational support of managerial
decisions as a new kind of business intelligence systems. In: Setlak, G., Markov, K. (eds.)
Computational Models for Business and Engineering Domains, pp. 269–279. ITHEA,
Rzeszow, Poland, Soﬁa, Bulgaria (2014)
93. Iutynska, G., Stepashko, V.: Mathematical modeling in the microbial monitoring of heavy
metals polluted soils. In: Book of Proceedings of IX ESA Congress, Part 2, pp. 659–660.
Institute of Soil Science and Plant Cultivation, Warsaw (2006)
94. Kalavrouziotis, I.K., Vissikirsky, V.A., Stepashko, V.S., Koukoulakis, P.H.: Application of
qualitative analysis techniques to the environmental modeling of plant species cultivation.
Glob. NEST J. 12(2), 161–174 (2010)
95. Alyomov, S.V., Bulgakova, O.S., Stepashko, V.S.: Modeling of the Black Sea pollution
impact on the total number of benthic organisms species. Collected Art. SNUNE&I
Sevastopol 3(39), 54–62 (2011). (in Ukrainian)
96. Stepashko, V., Moroz, O.: Hybrid searching GMDH-GA algorithm for solving inductive
modeling tasks. In: IEEE International Conference on Data Stream Mining & Processing,
pp. 350–355, Lviv, Ukraine (2016)
97. Zosimov, V., Stepashko, V. Bulgakova, O.: Inductive building of search results ranking
models to enhance the relevance of the text information retrieval. In: Spies, M., et al. (ed.)
Proceedings of the 26th International Workshop on Database and Expert Systems
Applications, Valencia, Spain, pp. 291–295. IEEE Computer Society, Los Alamitos (2015)
98. Schmidhuber, J.: Deep learning in neural networks: an overview. Neural Netw. 61, 85–117
(2015)
Developments and Prospects of GMDH-Based Inductive Modeling
491

Construction and Research of the Generalized
Iterative GMDH Algorithm
with Active Neurons
Volodymyr Stepashko1, Oleksandra Bulgakova2(&),
and Viacheslav Zosimov2
1 Department for Information Technologies of Inductive Modeling, International
Research and Training Centre for Information Technologies and Systems,
Akademik Glushkov Prospect 40, Kiev 03680, Ukraine
stepashko@irtc.org.ua
2 Department for Applied Mathematics and Information Computer
Technologies, V.O. Sukhomlynsky Mykolaiv National University,
Nikolska Street, 24, Mykolaiv 54030, Ukraine
sashabulgakova2@gmail.com, zosimovvvv@gmail.com
Abstract. Architecture of the generalized iterative algorithm GIA GMDH with
active neurons is presented based on hybridization of iterative and combinatorial
search schemes and the use of interactive technologies. The architecture com-
prises six standard variants of typical GMDH algorithms. Experiments show that
the proposed modiﬁcations improve considerably the practical performance of
the multilayered GMDH algorithm and accuracy of the simulation results.
Solution results are given for modeling Ukraine’s Black Sea economic region
GRP as dependent from socio-economic indicators that describe the state region
development using generalized iterative algorithm GMDH.
Keywords: Inductive modeling  GMDH  Multilayered algorithm
Combinatorial algorithm  Generalized iterative algorithm  Active neuron
1
Introduction
Problems of complex systems modeling can be solved using both the logical deductive
(theory-driven) and empirical inductive (data-driven) methods. Deductive methods
have advantages in the case of simple modeling tasks, if there is known the theory of an
object being modeled and therefore it is possible to build a model based on physical
principles using knowledge of processes in an object. But these methods cannot give
satisfactory results for complex systems. In this case, an approach of knowledge
extraction directly from the data based on statistical or experimental measurements has
an advantage. Such inductive approach is useful when a priori information about the
properties of such objects may be only minimal or even absent.
The world-wide known Group Method of Data Handling (GMDH) [1–3] is one of
the most successful methods of inductive modeling. It is used in various tasks of data
analysis and knowledge discovery, forecasting and systems modeling, classiﬁcation
and pattern recognition.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_35

Main GMDH advantages are as follows [3]: the model structure and its parameters
are found automatically; optimal complexity of the model structure is found adequately
to the noise level in data sample.
– The method uses information directly from a data sample and minimizes the impact
of a priori assumptions of an author on the modeling results;
– Relationships in data are found and informative input variables are selected;
– The model structure and its parameters are found automatically;
– Optimal complexity of the model structure is found adequately to the noise level in
the data sample;
– Any non-linear functions or factors that could inﬂuence the output variable are used
as input variables (arguments).
As of today, a great variety of GMDH algorithms of the sorting-out and iteration
types was developed and explored [4, 5]. The sorting-out algorithms are effective as the
tool for structural identiﬁcation but only for limited number of arguments. Iterative
algorithms are capable of working with a lot of arguments but the speciﬁc of their
architecture do not guarantee constructing the true model structure. Until recently, these
two classes of algorithms were developed without combining their strengths.
This chapter considers in some detail the basic structural elements of typical iter-
ative GMDH algorithms, analyzes its advantages and shortcomings, describes the
historical ways to overcome these shortcomings, discusses recent trends in GMDH
methodology, proposes a new type of a generalized hybrid GMDH algorithm “with
active neurons”, investigates comparative effectiveness of various algorithms using
numerical experiments with artiﬁcially generated data, and demonstrates results of
applying diverse algorithms to modeling of a real economic process.
2
GMDH as an Inductive Method of Model Building
The development of GMDH theory contributed to appearance of the wide range of
algorithms having its own advantages and disadvantages. GMDH algorithms may vary
against the type of elementary functions, the way of a model structure formation, the
form of external criteria and so on. The choice of an algorithm depends on the level of
noise in the data and their sufﬁciency.
Any GMDH algorithm solves a discrete optimization task to construct the model of
optimal complexity by the minimum of a given external criterion based on the data
sample division:
f  ¼ arg min
f 2U
CRðf Þ;
ð1Þ
where CR is a selection criterion as a measure of the quality of a model f 2 U. A model
selection criterion is called “external” if it is based on additional information that is not
contained in the data used for calculation of model parameters.
The set U of models being compared can be formed by various generators of model
structures of diverse complexities. All structure generators developed within the
Generalized Iterative GMDH Algorithm with Active Neurons
493

GMDH framework may be divided into two main groups, sorting-out and iterative ones
which differ by techniques of variants generation and organization of search of a given
criterion minimum.
To realize the principle of external supplement [1], the parameter estimations
(usually by the least squares method LSM) and criteria values are calculated on dif-
ferent parts of the sample W ¼ ½X..
.
y, where X and y are matrix and vector of n mea-
surements of m inputs (arguments) and one output respectively.
The simplest case is the sample splitting into two subsets: W ¼ ½WT
A WT
B T, con-
sequently X ¼ ½XT
AXT
BT; y ¼ ½yT
AyT
BT, where A, B, and W ¼ A [ B; n ¼ nA þ nB, are
training, checking and learning sets, respectively. The most commonly used among
GMDH criteria is the regularity criterion calculated for a model f 2 U as follows:
ARBjAðf Þ ¼
yB  ^yBjAðf Þ

2¼
yB  XBf ^hAf


2
;
ð2Þ
where designation ARB A
j ðf Þ means “error on B of a model f with parameters obtained
on A”, and XAf, XBf are submatrices of the matrix X containing columns that correspond
to a partial model f 2 U being considered.
Another type of external criteria is represented by so called unbiasedness criterion
reﬂecting the requirement that models obtained on A and B should differ minimally:
CB ¼ CBW A; B
j
ðf Þ ¼
^yWf A
ð Þ  ^yWf B
ð Þ

2¼
XWf ^hAf  XWf ^hBf


2
;
ð3Þ
where ^hAf and ^hBf are LSM parameter estimations on A and B of a model f.
Accuracy (2) and bias (3) are different, not interchangeable, measures of model
quality especially when noisy data are used. When the noise is absent, all criteria
(internal or external) are equivalent because they all specify the physical model [3]. For
more information on the GMDH criteria see [6].
3
Main Historical Types and Modiﬁcations of GMDH
Iterative Algorithms
Iterative GMDH algorithms solve the task (1) by successive approaching to the cri-
terion minimum using a network-type procedure based on the analogy with the bio-
logical selection of living organisms: the complication of models on a layer occurs due
to the pairwise “crossing” F best models from the previous layer. The process of
complication stops after the criterion starts to increase.
Presently, the classical multilayered iterative algorithm MIA GMDH [1–3] is the
most widely known, Fig. 1. However it has some substantial drawbacks: (1) possibility
of loss of informative arguments if they were eliminated at the beginning of the
selection procedure; (2) inclusion of spurious arguments to the ﬁnal model if they were
included at the beginning of the selection procedure; (3) exponential growth of the
polynomial degree (1, 2, 4, 8, …) due to using the non-linear (quadratic) partial
494
V. Stepashko et al.

description, and others. To enhance the efﬁciency of the iterative GMDH algorithm,
various modiﬁcations of MIA have been proposed in different periods of its evolution.
Main variants of them are presented below.
For example, the mentioned disadvantages of MIA have been partly removed in the
following algorithms. The ﬁrst drawback was eliminated in algorithms with including
initial arguments to the selection process at any layer [7]. The second one was partly
precluded by adding some kind of sorting variants of partial description structures at
any layer [8–10]. The last one may be prevented due to using the second-order
polynomial on the ﬁrst layer and only linear forms for next layers [11] or, on the
contrary, by using ﬁrstly linear partial descriptions and then, starting from the second
layer, quadratic descriptions [12]. In what follows, some of such modiﬁed algorithms
proposed in 1970–80th are characterized in more details.
So called “Simpliﬁed multilayered GMDH algorithm” was developed in 1974
where pairs were formed only from the intermediate and initial arguments to prevent
the loss of informative ones [7]. The algorithm was attributed later to the relaxational
type of GMDH algorithms; its disadvantage is absence of freedom of choice, F = 1.
Structurally similar algorithms were proposed in 1981, also providing the inclusion
of initial arguments to the selection process on subsequent layers of the iterative
procedure [10]. First one of this kind was CML, multilayered algorithm with linear
partial descriptions. This is the only algorithm for which the so called internal con-
vergence was proved. Here the following partial models are considered on a layer r:
yr
l ¼ ar1yr1
i
þ br1zr1
j
;
ð4Þ
where both the intermediate and initial arguments may be used as variable z. Polyno-
mial algorithm with the Gödel numbering GN in which partial models are considered
on a layer r based on not two but three inputs:
Fig. 1. Classical algorithm MIA GMDH
Generalized Iterative GMDH Algorithm with Active Neurons
495

yr
l ¼ ar1yr1
i
þ br1zr1
j
zr1
k
;
ð5Þ
where similarly the intermediate and/or initial arguments may be used as variable z.
Obviously, the previous algorithm CML is a particular case of GN (with yk = 1 or
yj = 1). Algorithm of GN type is further developed as GMDH-PNN [13].
Algorithms described above eliminate only the ﬁrst shortcoming, namely the
possibility of informative arguments loss if they were removed or not included at the
beginning of the selection procedure. To eliminate the second and third shortcomings
(ﬁxing uninformative arguments and growth the degree of polynomial), the use of the
full sorting-out of partial model variants was proposed [8, 9].
For example, in so called multilayered-combinatorial algorithm [9] a procedure of
multilayered selection was implemented using combinatorial sorting-out of partial
models applied to the following partial description: y ¼ a1 þ a2xi þ a3xj þ a4xixj. Main
steps of the algorithm are as follows:
1. Implementation of multilayered selection of the partial descriptions: generating all
possible pairs of arguments for current selection layer.
2. Combinatorial sorting-out of all variants of partial descriptions for a given pair of
arguments. Selection of the best partial models by the given criterion.
3. Checking the stop rule and ending the process or switching to the next layer.
But it is obvious that this algorithm, like classical multilayered one, does not still
eliminate the ﬁrst of the disadvantages mentioned above.
4
Current Trends of GMDH Development
At present, many domestic and foreign researchers in Ukraine, Czech Republic, Japan
and other countries are actively developing GMDH-like systems based on the classical
multilayered algorithm.
GMDH as a polynomial neural network. At early stages of the GMDH theory
development, the similarity between neural networks and multilayered GMDH algo-
rithm was observed. O. Ivakhnenko in one of the articles stated that it is acceptable to
call GMDH-systems as “perceptron-like systems”.
Starting from 1990th, GMDH algorithm is often called among professionals as
Polynomial Neural Network (PNN) [14]. This is because one of the main elements of
iterative GMDH algorithms, namely the polynomial partial description, can be con-
sidered as an elementary neuron of the GMDH neural network. The network originality
with such neurons consists in high speed of the process of local adjustment of neuron
weights and automatic global optimization of the network structure (number of units
and iterations or hidden layers).
The idea of neural networks with active neurons. In all kinds of standard (passive)
neurons, any mechanisms of optimization of the set of input variables are not used,
only parametric optimization is performed. The mechanisms are realized in the com-
plex process of self-organization of the whole system of many neurons in general.
496
V. Stepashko et al.

A combined method was proposed by prof. Ivakhnenko in [15] extending the
theory of self-organization from ﬁxed passive structures to active neural networks. An
algorithm known as “neural network with active neurons” is used in the GMDH
architecture. Both multilayered and combinatorial GMDH algorithm can be used as
active neurons; that leads to increasing the accuracy and reducing the calculation time.
The advantage of GMDH neural networks with active neurons as compared to the
conventional neural network with uniform neurons consists in that self-organizing of
the network is simpliﬁed: each neuron ﬁnds necessary connections and its own
structure in the process of self-organization. The idea of active neurons served also as
the basis for generalization of the previous modiﬁcations in order to signiﬁcantly
improve the efﬁciency of iterative GMDH algorithms.
GMDH-type neural network with feedback. A new multilayered GMDH-type
neural network with feedback was proposed in [16] which may automatically choose
the optimum neural network architecture using the idea of self-organization. In this
algorithm outputs of neurons are connected with the system inputs (feedback) for
further calculation. Thus, the complexity of the neural network increases gradually.
This GMDH-type neural network algorithm has an ability of self-selecting optimum
neural network architecture from three variants such as sigmoid function, radial basis
function (RBF) and polynomial neural network. In addition, structural parameters such
as number of neurons in hidden layers and input variables are selected automatically by
minimizing the Akaike criterion, that is without data division similarly as in [8].
Group of Adaptive Models Evolution. An evolutionary algorithm based on GMDH
and called GAME (group of adaptive models evolution) was developed in [17]. Major
modiﬁcations of this GMDH-type system are [9]: the transfer function of the unit is of
several types (linear, polynomial, logistic, etc.); each type of unit has its own learning
algorithm for coefﬁcients estimation; choice of the type of units that form a network is
determined by the given criterion.
This is so-called heterogeneous network structure: the number of unit inputs
increases together with the depth of the unit in the network; there exist interlayer
connections in the network; the network construction process does not search all
possible layouts of units, ıt searches just the random subset of these layouts.
The modiﬁed GMDH generates a group of models on a single training data set that
are locally optimal. Weights and coefﬁcients of units are randomly initialized. Transfer
functions of many units types are deﬁned pseudo-randomly when the unit is initialized.
Inputs for units are selected pseudo-randomly as well. It results in the fact that the
topology of models developed on the same training data set may differ.
Hybrid of Differential Evolution and GMDH for Inductive Modeling. The GMDH
and differential evolution (DE) population-based algorithm are two well-known non-
linear methods of mathematical modeling. A new design methodology which is a
hybrid of GMDH and DE was proposed in [18]. The method constructs a GMDH
network model of a population of promising DE solutions.
The modeling methods have many common features, but, unlike the GMDH, DE
does not follow a pre-determined path for input data generation. The same input data
elements can be included or excluded at any stage in the evolutionary process by virtue
of the stochastic nature of the selection process. A DE algorithm can thus be seen as
implicitly having the capacity to learn and adapt in the search space and thus allow
Generalized Iterative GMDH Algorithm with Active Neurons
497

previously bad elements to be included if they become beneﬁcial in the later stages of
the search process.
5
Constructing the Generalized Hybrid GMDH Algorithm
The above analytical results make it possible to advance the following basic ideas in
developing new hybrid iterative algorithms methodology for removing all the three
disadvantages of MIA GMDH:
– Enabling the use of the initial arguments in each layer to prevent losing the relevant
ones in the multilayer self-organizing process;
– Using the idea of active neurons by performing the optimization of every partial
model structure by combinatorial algorithm to avoid inclusion of spurious argu-
ments in the ﬁnal model and overﬁtting the model complexity;
– Using various partial descriptions in the form of linear, bilinear or nonlinear
functions on different layers;
– Enabling to a user applying different modes of the modeling process.
These modiﬁcations were applied and studied in [19–21] to construct the so called
Generalized Iterative Algorithm GIA GMDH which makes it possible to get the fol-
lowing basic variants of the GMDH algorithms of iterative type:
(1) Classical MIA GMDH algorithm with the use of pairwise combinations of only
intermediate arguments;
(2) An algorithm with adding to intermediate arguments only initial ones;
(3) An algorithm with equal usage of both the intermediate and initial arguments in
the partial descriptions;
(4) In any of the three variants, the optimization of every partial model structure by
combinatorial algorithm may or may not be used;
(5) In the case of using the optimization, this architecture becomes a typical hybrid
algorithm with active neurons.
Due to these features the developed hybrid algorithm with quadratic partial
description can build linear, bilinear and nonlinear models of complex systems and
processes.
5.1
GIA GMDH Algorithm Description
Formally, in general case, a layer of the GIA GMDH may be deﬁned as follows [20]:
(1) The input matrix is Xr þ 1 ¼ ðyr
1; . . .; yr
F; x1; . . .; xmÞ for a layer r + 1, where
x1; . . .; xm are the initial arguments and yr
1; . . .; yr
F are the intermediate ones of the
layer r;
(2) The operators of the kind
498
V. Stepashko et al.

yr þ 1
l
¼ f ðyr
i ; yr
j Þ; l ¼ 1; 2; . . .; C2
F;
i; j ¼ 1; F;
yr þ 1
l
¼ f ðyr
i ; xjÞ; l ¼ 1; 2; . . .; Fm; i ¼ 1; F; j ¼ 1; m
ð6Þ
may be applied on the layer r + 1 to construct linear, bilinear and quadratic partial
descriptions:
z ¼ f ðu; vÞ ¼ a0 þ a1u þ a2v ;
z ¼ f ðu; vÞ ¼ a0 þ a1u þ a2v þ a3uv ;
z ¼ f ðu; vÞ ¼ a0 þ a1u þ a2v þ a3uv þ a4u2 þ a5v2 :
ð7Þ
(3) For any description, the optimal structure is searched by combinatorial algorithm;
e.g., for the linear partial description the expression holds:
f ðu; vÞ ¼ a0d1 þ a1d2u þ a2d3v;
ð8Þ
where dk; k ¼ 1; 2; 3, dk ¼ f0; 1g are elements of the binary structural vector
d ¼ ðd1 d2 d3Þ where values 1 or 0 mean inclusion or not a relevant argument.
Then the best model will be described as f ðu; v; doptÞ, where
dopt ¼ arg min
l¼1;q
CRl; q ¼ 2p  1; foptðu; vÞ ¼ f ðu; v; doptÞ:
ð9Þ
(4) The algorithm stops when the condition CRr [ CRr1 is checked, where
CRr; CRr1 are criterion values for the best models of (r–1)-th and r-th layers
respectively. If the condition holds, then stop, otherwise jump to the next layer.
The GIA structure is schematically represented in the Fig. 2.
5.2
Main Particular Cases of the GIA GMDH Architecture
The following six typical algorithms can be generated as particular cases of the gen-
eralized architecture:
(a) Iterative GMDH algorithms:
(1) Multilayered Iterative Algorithm MIA in which pairs are only between the
intermediate arguments;
(2) Relaxation Iterative Algorithm RIA in which pairs are between the interme-
diate and initial arguments;
(3) Combined Iterative Algorithm CIA in which pairs are possible between both
the intermediate and initial arguments;
(b) Iterative-combinatorial GMDH algorithms with combinatorial optimization of
partial descriptions:
(4) Multilayered Iterative-Combinational MICA;
(5) Relaxation Iterative-Combinational RICA;
Generalized Iterative GMDH Algorithm with Active Neurons
499

(6) Combined Iterative-Combinational CICA (being practically the same as
GIA).
Any of these variants may be formed and used with the help of the specialized
modeling software complex [21] based on the GIA GMDH with implementation of the
following features: automatic and interactive options for organization of user interface;
administration through the web interface; ensuring multi-access. Best constructed
models are presented by system for the graphic and semantic analysis as well as
selection of the most informative arguments.
5.3
The Ordered Coding of the Iterative GMDH Algorithms Structure
Let us deﬁne the GIA GMDH as a set of iterative and iterative-combinatorial algo-
rithms described by the vector of the following three elements: DM (Dialogue Mode),
IC (Iterative-Combinatorial), MR (Multilayer-Relaxation).
This means that any iterative algorithm can be deﬁned as a special case of the
generalized one of general description GIA(DM,IC,MR). In this case, DM takes three
values: 1 – standard automatic mode, 2 – planned automatic mode, 3 – interactive
mode; IC takes two values: 1 – iterative, 2 – Iterative-combinatorial algorithms; MR
takes three values: 1 – classic iterative, 2 – relaxation, 3 – combined algorithms.
This vector deﬁnes any of typical iterative algorithms mentioned in the item 5.2: if
DM = 1, we have three standard variants of iterative algorithms MIA = GIA(1,1,1),
RIA = GIA(1,1,2), CIA = GIA(1,1,3), as well as three iterative-combinatorial ones:
Fig. 2. The generalized architecture of GIA GMDH
500
V. Stepashko et al.

MICA = GIA(1,2,1); RICA = GIA(1,2,2), CICA = GIA(1,2,3). For DM equal 2 or 3,
some new versions of these algorithms can be formed.
A hierarchical representation of the developed GIA GMDH architecture is dis-
played on Fig. 3, where white blocks correspond to known algorithms and gray ones
indicate newly designed structures as special cases of GIA.
6
Experimental Research of the Algorithms Effectiveness
in the Task of True Model Structure Construction
6.1
Integrated Technique for Analysis of the Effectiveness of Iterative
GMDH Algorithms Using Computational Experiments
The aim of computational experiments is investigating the effectiveness of the gener-
alized hybrid algorithm which combines the ideas of initial arguments selection and
optimization of the partial models complexity. It also allows performing comparison of
various iterative GMDH algorithms as special cases of the generalized one.
Any of the iterative algorithms is characterized by its input (control) parameters as
well as intermediate and output indicators of the model building process. The structure
of a resulting data table is presented in Table 1.
The input parameters of any algorithm: the number of initial arguments m; number
of points n in the sample; procedure to partition the dataset W into the training A and
testing B subsets; selection criteria; freedom of choice value F.
The intermediate indicators: plot of minimum and maximum values of the criterion
changing by layers; changing of the best model arguments by layers.
Fig. 3. The hierarchy of typical iterative algorithms as special cases of the GIA GMDH
Generalized Iterative GMDH Algorithm with Active Neurons
501

The output indicators: the best model and its parameters; values of the selection
criteria; composition of arguments of the best model and its proximity to the true one;
program run-time (work time of central processor).
Besides studying the impact of key control parameters, in the experiments it is
necessary to investigate the effectiveness of the true structure detection. Therefore, to
study restoring of the structure by different algorithms we need to generate experiments
in which it is necessary to:
– Restore the true structure in case of many redundant arguments from data without
noise;
– Restore the true model structure hidden in data and investigate the algorithms
effectiveness in tasks with noise. The noise can be generated, for example, as
follows:
^y ¼y þ að2c  1Þymax ymin
200
ð2Þ
where y is the true signal, a is noise percentage, c is a random value uniformly
distributed in the interval [0, 1], ymax, ymin are maximum and minimum of the output
value respectively.
– Restore the true structure in case of a small number of arguments but with a
complex nonlinear dependence;
– Examine the developed algorithms for the existence of so-called “internal conver-
gence” of the iterative process to the true model by structure and parameters.
Based on the proposed methodology of comparative analysis of algorithms effec-
tiveness, one can investigate the properties of iterative GMDH algorithms and the effect
of key parameters on modeling performance. It should be noted that in all cases the
effect of freedom of models choice F on performance indicators of algorithms was
investigated especially as the main of the control parameters.
Experiments described below were based on artiﬁcially generated data and carried
out to study the effectiveness of various iterative algorithms and the regularity criterion
was used in all the experiments. The goal was to compare all the six algorithms
introduced above but ﬁrst of all the three typical ones, namely classical multilayered
MIA, advanced combined CIA and the generalized GIA with active neurons.
Table 1. Data table structure
No X [nm]
y [n1]
x1
x2
… xm
1
x11 x12 … x1m y1
2
x21 x22 … x2m y2
…
…
…
… …
…
n
xn1 xn2 … xnm yn
502
V. Stepashko et al.

The comparison was done under various conditions: linear and nonlinear depen-
dences in data; presence of additive noise of different level; testing the internal con-
vergence of the algorithms. In all cases the regularity criterion AR (2) was used.
6.2
Effectiveness of Iterative Algorithms in Detecting True Model,
the Case of Quadratic Dependence
The experiment goal is to compare the ability of different GMDH algorithms to restore
structure of the true nonlinear model hidden in data.
Data characteristics: artiﬁcial data, random numbers in the range of [0, 5]; argu-
ments number m = 200; sample length n = 243; sample division nA = 163, nB = 80;
freedom of choice F = 40.
Algorithms being compared: (1) multilayered MIA, (2) relaxation RIA, (3) com-
bined iterative CIA, (4) multilayered-combinatorial MICA, (5) relaxation-combinatorial
RICA; (6) generalized GIA.
The quadratic dependence is represented by the following formula (196 redundant
arguments):
y ¼ 3x1  2x2x3 þ x2
25 þ x25x1 þ 12
ð3Þ
Table 2 shows that in this case the true structure is restoring by algorithms RICA
and GIA but the latter (generalized) enables to recover the true model most precisely by
structure and parameters.
6.3
Effectiveness of Iterative Algorithms Under Conditions of Noisy Data
The experiment goal: to restore the true model structure hidden in the data using
various algorithms GMDH and compare effectiveness of them in the case of noisy data.
Data characteristics: artiﬁcial data, random numbers in the range of [0, 1]; m = 40,
n = 60, nA = 40, nB = 20; freedom of choice F = 40.
Algorithms being compared: (1) multilayered MIA, (2) combined iterative CIA,
(3) generalized GIA.
Table 2. Comparison of iterative algorithms effectiveness
Algorithm AR(B)
Model
1
11.309 ^y ¼ 3x1  1:231x2x3 þ 2:711x25 þ 0:998x32 þ x37  2:001x45 þ 12
2
3.031
^y ¼ 3x1  1:231x2 þ 6:700x25  2:001x45 þ 11:99
3
3.011
^y ¼ 3x1  1:231x2x3 þ 6:711x25  2:001x45 þ 12
4
0,462
^y ¼ 2:0001x1x3 þ x2
25 þ x1x25  2:999x45 þ 12
5
4*10−7 ^y ¼ 3:000x1  2:000x2x3 þ x2
25 þ 0:985x25x1 þ 12:000
6
3*10−8 ^y ¼ 3:000x1  2:000x2x3 þ x2
25 þ x25x1 þ 12:000
Generalized Iterative GMDH Algorithm with Active Neurons
503

The true dependence is represented by the following formula:
y ¼ 0; 5  1; 2x2 þ 5x10  3; 4x25:
ð4Þ
The executed experiments show that when the output variable is eddied by uniform
noise with noise/signal ratio of 10% and 30% then GIA remains to be the best algo-
rithm detecting the true model structure, see Fig. 4.
6.4
Effectiveness Investigation of Constructing an Essentially Nonlinear
Dependence in Case of a Small Number of Arguments
The experiment goal: to study the inﬂuence of the freedom of choice F of the best
solutions as the key GMDH parameter on the comparative efﬁciency of algorithms in
nonlinear tasks.
The sample parameters: m = 3 arguments and n = 100 points, the division into
sub-samples nA = 65, nB = 35. The effect of the freedom of choice F on the perfor-
mance of algorithms is investigated.
The true model is the following nonlinear function of three arguments:
y ¼ 7  6x1 þ 5x3  4x2
2 þ 3x1x3  2x2
1x2 þ x3
3:
ð13Þ
Figure 5 shows the dependence of minimum values of the criterion AR for different
algorithms on the value of freedom of choice of the best F models. The lowest value of
the criterion for all F has been achieved for the GIA which detected the true model
structure. As it is obvious from the ﬁgure, the freedom of choice value F = 20 is
sufﬁcient for all algorithms.
6.5
Veriﬁcation of Internal Convergence of Iterative GMDH Algorithms
It was experimentally veriﬁed the existence of so-called internal convergence of iter-
ative algorithms theoretically proved earlier in [10].
Fig. 4. Comparison of the iterative algorithms effectiveness under conditions of noisy data
504
V. Stepashko et al.

An iterative algorithm holds the internal convergence if as a result of iteration
process with the selection criterion RSS being the residual sum of squares on the entire
sample W (without partition), the estimations of optimal model parameters are con-
verging to the estimates of parameters of the true model by the Least Squares Method.
Data characteristics in this case: m = 10, n = 50. The internal convergence is
studied for the following three typical iterative GMDH algorithms: classical MIA,
combined CIA, and generalized GIA [12].
The true model is the following linear function of 10 arguments:
y ¼ 3  2x1 þ 5x2  x3 þ 7x4  2x5 þ 4x6  3x7 þ 2; 5x8  1; 8x9 þ x10:
ð14Þ
Fig. 5. Changing the selection criterion values while increasing the freedom of models choice F
Fig. 6. Change of RSS values for the three iterative GMDH algorithms, F = 20 (logarithmic
scale)
Generalized Iterative GMDH Algorithm with Active Neurons
505

Figure 6 shows the change of RSS values as dependent on the layer number r of
these algorithms with F = 20.
The constructed models are of form [12]:
yMIA ¼ 22:387 þ 5:334x2  0:535x3 þ 5:266x4 þ 0:812x5 ;
þ 4:726x6 þ 1:278x8  2:643x9 þ 0:437x10
ð15Þ
yCIA ¼ 2:913  1:978x1 þ 4:999x2  1:015x3 þ 7:004x4
;
 1:988x5 þ 3:997x6  2:994x7 þ 2:514x8  1:808x9 þ 1:003x10
ð16Þ
yGIA ¼ 3:001  1:999x1 þ 4:999x2  1:002x3 þ 7:004x4
:
 1:998x5 þ 4:000x6  2:999x7 þ 2:500x8  1:801x9 þ 1:003x10
ð17Þ
In the classical multilayered algorithm MIA, the convergence is observed only by
criterion but there is no internal convergence with respect to the structure and
parameters in view of the loss of true arguments x1 and x7, compare (14) and (15).
The Eqs. (16) and (17) generally conﬁrm the internal convergence of the two
iterative algorithms because values of the estimated parameters are close to true ones,
see (14). The Fig. 6 shows that the generalized algorithm reaches its minimum on the
9th layer and the combined one on the 12th, hence the convergence rate of GIA is much
higher than CIA. The accuracy of the obtained models also differs in favor of the
generalized: for GIA RSS = 0.001, for CIA RSS = 0.006. This demonstrates the
effectiveness of the idea of partial models optimization.
The results obtained above illustrate, ﬁrst, that the developed architecture of the
generalized GMDH algorithm allows investigating various versions of iterative algo-
rithms, and, second, this algorithm demonstrates highest performance in all versions of
test problems.
The effectiveness comparison of the generalized hybrid GMDH algorithm with
other iterative GMDH algorithms have been made based on study of the effect of key
parameters of algorithms on performance indicators of the model construction process.
Executed experiments showed the best performance of the generalized algorithm.
7
Modeling the Ukraine Black Sea Economic Region GRP
Dependence on Socio-Economic Indicators
The Ukraine’s Black Sea economic region includes the territory of Odessa, Mykolaiv,
and Kherson oblasts located in the southern and south-western parts of Ukraine.
For this economic region, the gross regional product (GRP) dependence on
socio-economic indicators was modeled using the generalized iterative algorithm
GMDH. The goal was to identify the dependence of GRP growth (mln. UAH) on the
following socio-economic indicators:
506
V. Stepashko et al.

Industry: x1 is index of industrial production, % to the previous year;
Production of major animal products: production of meat x2, milk x3, wool x4, and
eggs x5 in all categories of farms;
Production of main agricultural crops: grains and legumes x6; sunﬂower seeds x7;
potatoes x8; vegetables x9; fruits and berries x10;
Fishing industry: ﬁshery and other aquatic resources x11;
Investment and construction activities: commissioning housing x12;
Transport: transportation of cargo x13 and passengers x14;
Foreign trade: export x15 and import x16 of goods and services;
Internal trade: retail turnover x17;
Job Market: unemployment rate x18; average monthly salary x19; wage arrears x20.
Data were taken at the Ukrainian State Statistics Committee.
Statistical sample of quarterly data for Ukraine’s Black Sea economic region for the
period from 2004 (1st quarter) to 2011 (4th quarter) contains totally 20 variables and 32
data points which are divided into three parts: training A (20 points), testing B (8
points), and examination C (4 points, 2011 year) sub-samples.
Table 3 and Figs. 7, 8 and 9 show the modeling results of the GRP values.
The obtained dependences demonstrate that:
– For Mykolaiv region, only 8 socio-economic indicators among all 20 ones have the
most signiﬁcant effect on GDP: milk production; sunﬂower seeds; ﬁshing extraction
and other aquatic resources; commissioning housing; departure (transportation) of
cargo; import goods and services; retail turnover; average monthly salary;
– For Kherson region, only 4 indicators among all 20 ones have signiﬁcant effect on
GDP: wool production; grains and legumes; the average monthly salary; sunﬂower
seeds;
– For Odessa region, only 8 indicators among all 20 ones have the most signiﬁcant
effect on GDP: sunﬂower seeds; vegetables; export goods and services; average
monthly salary; fruits and berries.
Table 3. Result of modeling the GRP values
Region
AR
Model
accuracy,
%
Model
Mykolaiv
region
0.787 99
y ¼ 1697:10 þ 0:0306x3  0:01315x7
 0:00549x11 þ 1:815x12  0:00023x13 þ 0:000027x16
þ 0:00026x17 þ 9:284x19 þ þ 0:022x2
12 þ 24  1010x2
17
Kherson
region
202.01
87
y ¼ 538:59 þ 1:1376x4 þ 0:063x6 þ 8:228x19 þ 0:000037x2
7
Odessa
region
189.23
90
y ¼ 18548:99  0:00407x7 þ 0:541x9 þ 17:83x15
þ 35:868x19  0:00014x15x9  0:0093x15x19  0:0006x2
10
Generalized Iterative GMDH Algorithm with Active Neurons
507

Fig. 7. Result of modeling of the Mykolaiv region GRP values, on years
Fig. 8. Result of modeling of the GRP Kherson region values, on years
Fig. 9. Result of modeling of the Odessa region GRP values, on years
508
V. Stepashko et al.

The obtained modeling results show that the priority sector of the Black Sea
Economic Region is the food industry and processing of agricultural products. These
conclusions may serve as a basis for further study of reasons of the uneven level and
rate of economic development of the region to mitigate the current regional
differentiation.
8
Conclusion
Results presented above specify ways to solving the problem of how to improve the
efﬁciency of complex systems modeling based on hybridization of architectures of
iterative and combinatorial GMDH algorithms. Effectiveness of the developed
improvements has been conﬁrmed by both numerical experiments on artiﬁcial exam-
ples and solving a modeling problem of a real economic process.
Comparative analysis of advantages and drawbacks of existing iterative GMDH
algorithms has resulted in some reasonable ways to increase effectiveness of solving
the model building problems. The Generalized Iterative Algorithm GIA GMDH with
Active Neurons based on the combinatorial optimization of partial descriptions was
introduced and implemented.
The developed computational online-technology for automated solving the mod-
eling tasks on the basis of the generalized GMDH algorithm makes it possible to carry
out computations via Internet and local net in automatic and interactive modes for
building models of complex system in the polynomial class.
GIA GMDH algorithm is a neural network with active neurons being optimized
using COMBI, i.e. it is a generalized hybrid algorithm entirely based on the GMDH
architectures. In all the executed numerical experiments, the GIA algorithm was the most
effective. The fact of internal convergence of the GIA GMDH to the true polynomial
model with respect to the structure and parameters was experimentally conﬁrmed.
The modeling task of the Ukraine Black Sea economic region GRP dependence on
socio-economic indicators was solved using the GIA GMDH. Informative indicators of
the socio-economic development inﬂuencing the dynamics of the Kherson, Mykolaiv
and Odessa region’s GRP were determined.
The above results illustrate, ﬁrst, that the developed architecture of the generalized
GMDH algorithm allows investigating various versions of iterative algorithms, and,
second, this algorithm demonstrates highest performance both in all versions of test
tasks and when solving real-world problems.
References
1. Ivakhnenko, A.G.: Group method of data handling as a rival of the stochastic approximation
method. Soviet Autom. Control 3, 58–72 (1968)
2. Farlow, S.J. (ed.): Self-Organizing Methods in Modeling: GMDH Type Algorithms. Marcel
Decker Inc., New York, Basel (1984)
3. Madala, H.R.: Inductive Learning Algorithms for Complex Systems Modeling. CRC Press
Inc., Boca Raton (1994)
Generalized Iterative GMDH Algorithm with Active Neurons
509

4. Ivakhnenko, A.G., Müller, J.-A.: Recent Developments of Self-Organizing Modeling in
Prediction and Analysis of Stock Market. Microelectron. Reliab. 37, 1053–1072 (1997)
5. Stepashko, V.: Ideas of Academician O.H. Ivakhnenko in the Inductive Modelling Field
from Historical Perspective. In: Proceedings of the 4th International Conference on Inductive
Modelling ICIM 2013, IRTC ITS NASU, Kyiv, Ukraine, pp. 30–37 (2013)
6. Stepashko, V.S., Kocherga, Y.L.: Classiﬁcation and analysis of the noise immunity of
external criteria for model selection. Soviet Autom. Control 17(3), 36–47 (1984)
7. Sheludko, O.I.: GMDH algorithm with orthogonalized complete description for synthesis of
models after results of a planned experiment. Soviet Autom. Control 7(5), 46–57 (1974)
8. Tamura H., Kondo T.: Large-Spatial pattern identiﬁcation of air pollution by a combined
model of source-receptor matrix and revised GMDH. In: Proceedings of IFAC Symposium
on Environmental Systems Planning, Design and Control, pp. 373–380. Elsevier, Oxford
(1977)
9. Ivakhnenko, N.A., Marchev, A.A.: Self-organization of a mathematical model for long-term
planning of construction and installation works. Soviet Autom. Control 11(3), 10–16 (1978)
10. Yurachkovskiy, Y.P.: Recovery of polynomial dependences using self-organization. Soviet
Autom. Control 14(4), 14–19 (1981)
11. Parker R.G.J., Tummala M.: Identiﬁcation of Volterra systems with a polynomial neural
network. In: Proceedings of the 1992 International Conference on Acoustics, Speech and
Signal Processing ICASSP 1992, San Francisco, vol. 4, pp. 561–564. IEEE (1992)
12. Hara, K., Yamamoto, T., Terada, K.: Improved dual mode GMDH with automatic switch.
Int. J. Syst. Sci. 21(8), 1553–1565 (1990)
13. Aksyonova, T.I., Volkovich, V.V., Tetko, I.V.: Robust polynomial neural network in
quantative-structure activity relationship studies. Syst. Anal. Model. Simul. 43(10), 1331–
1341 (2003)
14. Oh, S.K., Pedrycz, W.: The design of self-organizing polynomial neural networks. Inf. Sci.
141, 237–258 (2002)
15. Ivakhnenko, A.G., Wunsh, D., Ivakhnenko, G.A.: Inductive sorting-out GMDH algorithms
with polynomial complexity for active neurons of neural networks. In: Proceedings of the
International Joint Conference on Neural Networks, pp. 1169–1173. IEEE, Piscataway,
New Jersey (1999)
16. Kondo T.: GMDH neural network algorithm using the heuristic self-organization method
and its application to the pattern identiﬁcation. In: Proceedings of the 37th SICE Annual
Conference SICE 1998, pp. 1143–1148. IEEE, Piscataway, New Jersey (1998)
17. Kordík, P., Náplava, Šnorek, M., Genyk-Berezovskij, P.: The modiﬁed GMDH method
applied to model complex systems. In: Proceedings of International Conference on Inductive
Modelling ICIM 2002, Lviv, Ukraine, pp. 134–138. SRDIII (2002)
18. Onwubolu, G.C.: Design of hybrid differential evolution and group method of data handling
for inductive modeling. In: Proceedings of the IInd International Workshop on Inductive
Modelling IWIM 2007, CTU in Prague, Czech Republic, pp. 87–95 (2007)
19. Stepashko, V., Bulgakova, O., Zosimov, V.: Experimental veriﬁcation of internal
convergence of iterative GMDH algorithms. In: Proceedings of the Vth International
Workshop on Inductive Modelling IWIM 2012, IRTC ITS NASU, Kyiv, pp. 53–56 (2012)
20. Stepashko, V., Bulgakova, O.: Generalized iterative algorithm GIA GMDH. In: Proceedings
of the 4th International Conference on Inductive Modelling ICIM 2013, IRTC ITS NASU,
Kyiv, pp. 119–123 (2013)
21. Zosimov, V.V., Bulgakova, O.S., Stepashko, V.S.: Software package for complex systems
modelling on the basis of iterative GMDH algorithms with the possibility of network access.
Syst. Res. Inf. Technol. 1, 43–55 (2014). (In Ukrainian)
510
V. Stepashko et al.

The Fast Fourier Transform Partitioning Scheme
for GPU’s Computation Eﬀectiveness Improvement
Kamil Stokﬁszewski
(✉), Kamil Wieloch, and Mykhaylo Yatsymirskyy
Lodz University of Technology University, Lodz, Poland
{kamil.stokfiszewski,mykhaylo.yatsymirskyy}@p.lodz.pl,
kamil.wieloch@dokt.p.lodz.pl
Abstract. In this paper authors present the Fast Fourier Transform (FFT) parti‐
tioning scheme aimed at improvement of the eﬀectiveness of the considered
transform computation on graphics processing units (i.e. the GPUs). The FFT
radix-2 decimation in time (DIT) algorithm is chosen as the base procedure for
the FFT calculation which is then partitioned into subtransform blocks of arbitrary
sizes enabling for diﬀerent GPU resources distribution during its computational
process and thus resulting in the potential improvement of the overall FFT execu‐
tion time for chosen consumer segment GPU models. The conducted experiments
show that for a chosen GPU architectures running in the single instruction
multiple thread (SIMT) mode of operation partitioning of the FFT into 4-point
and 8-point subtransforms calculated sequentially within an individual thread,
instead of its calculation using standard 2-point butterﬂy operations, signiﬁcantly
reduces the FFT’s computation time. The presented scheme is general and can be
used for the partitioning of the FFT into arbitrary size subtransform blocks aimed
at the scheme’s time eﬀectiveness ﬁne-tuning to the chosen, particular GPU
architectures.
Keywords: Fast Fourier Transform · FFT · GPU programming · Parallel
computations · Computational eﬀectiveness
1
Introduction
Parallel computations with the aid of graphics processing units, i.e. GPUs, has in recent
years been attracting an increasing attention of the scientiﬁc community across a broad
spectrum of computational research domains [1]. This comes from fact that the GPUs
have become an attractive tool for general-purpose industrial and research computa‐
tional applications, oﬀering a cost-eﬀective hardware solutions, especially within a
consumer segment GPUs, coupled with relatively high computational performance, see
e.g. [1, 2]. This, on the other hand, has caused an intense interest among researchers in
redesigning the implementations of many well-known classical algorithms to meet the
requirements of parallel computations, and thus enabling them to build their time-eﬀec‐
tive variants that are suitable for the GPU realizations - see e.g. [3, 4]. A particular
interest has been paid to the discrete Fourier transform (DFT), since it plays a key role
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_36

in many areas of engineering such as digital signal processing and signal ﬁltering, data
compression and encryption, image and pattern and recognition, as well as in a variety
of many other data and signal processing applications, e.g. [5, 6] or [7]. A well-known
class of DFT computation algorithms is Cooley-Tukey’s Fast Fourier Transform algo‐
rithm [8, 9], often shortly referenced to as the FFT, which is especially convenient for
parallel implementations due to its well-structured computational form. Various FFT
realizations were analyzed in literature in terms of the improvement of their parallel
GPU implementations’ eﬀectiveness, c.f. [10–12].
In this paper the authors analyze the issue of enhancement of time and resource
allocation eﬃciency of parallel GPU FFT realizations by introducing a general FFT
partitioning scheme that can be adopted for implementations on diﬀerent, selected
consumer segment GPU models. The scheme makes use of the decimation in time (DIT)
FFT algorithm’s variant which is partitioned into subtransform blocks of arbitrary sizes
for which the computations are performed in a sequential manner. Experimental study
reveals that such partitioning ultimately improves the DIT variant of the FFT calculation
algorithm’s time eﬀectiveness, which is tested for the two selected diﬀerent GPU archi‐
tectures. Furthermore the generality of the scheme enables it to be adopted to various,
chosen consumer segment GPU architectures performing their computations in the
single instruction multiple thread (SIMT) manner.
2
Decimation in Time Fast Fourier Transform Algorithm
The discrete Fourier transform, also often referred to as the DFT, is one most commonly
used mathematical tools in the areas of digital signal and data processing and also in
many other important engineering ﬁelds. For the sake of clarity we’ll now introduce its
deﬁnition. For k = 0, 1, … , N −1 the k-th coeﬃcient of the N-point DFT of the complex
sequence x(n), n = 0, 1, … , N −1 is deﬁned as follows
X(k) = DFTN{x(n)} =
N−1
∑
n =0
x(n) ⋅e−j2𝜋kn∕N,
(1)
where j is the complex imaginary unit. It can be easily deducted form deﬁnition formula
(1) that the computational complexity of the DFT calculation of all of its N coeﬃcients
is equal to O(N2). The reduction of the DFT’s computational complexity is crucial for
practical applications. The historically ﬁrst class of the computational algorithms which
were aimed to achieve the mentioned goal were introduced by Cooley-Tuckey in [8]. In
their work Cooley-Tuckey proposed a class of fast computational procedures for the
DFT calculation, i.e. Fast Fourier Transforms (FFTs), which require performing only
O(N log2N) basic arithmetic operations in order to calculate all of the DFT coeﬃcients,
thus reducing its original computational complexity by one order of magnitude, see e.g.
[9]. Cooley-Tuckey’s original algorithm comes in many possible variants. In our work
we have chosen decimation in time (DIT) FFT’s version as our departure point. In the
DIT FFT approach to DFT calculation the deﬁnition formula (1) can be expressed in the
following recursive form
512
K. Stokﬁszewski et al.

X(k) = DFTN∕2{x(2n)} + Wk
NDFTN∕2{x(2n + 1)},
X(k + N∕2) = DFTN∕2{x(2n)} −Wk
NDFTN∕2{x(2n + 1)},
(2)
where n = 0, 1, … , N −1, k = 0, 1, … , N∕2 −1 for N = 2p, p ∈𝐍 and Wk
N = e−j2𝜋kn∕N.
Figure 1 presents the data ﬂow diagram of the 16-point DIT FFT obtained from the
successive, recursive application of decomposition Eq. (2).
Fig. 1. Data ﬂow diagram of 16-point decimation in time FFT calculation algorithm.
FFT’s base operations, so called “butterﬂy” operations, are deﬁned below in Fig. 2.
Fig. 2. Base operations of the Cooley-Tuckey FFT calculation algorithm form Fig. 1.
From Eq. (2) it can be easily veriﬁed that the computational complexity of FFT
calculation algorithm is of O(N log2N) and it’s though justiﬁed to call the analyzed
procedure a fast one with relation to the original DFT deﬁnition (1).
Presented in (2) and depicted in Figs. 1 and 2, DIT FFT algorithm is chosen by the
authors as a departure point for the construction of the partitioning scheme for time-
eﬀective GPU FFT calculation.
3
Partitioning Scheme for the Eﬀective GPU FFT Computation
Parallel computing with the use of GPUs became very popular in recent years, see e.g.
[10–12]. There are many research papers related to this subject, but the majority is still
focused on comparing CPU performance against the GPU, see e.g. [11]. The most
The Fast Fourier Transform Partitioning Scheme
513

popular approach in such type of research is based on the use of the maximum possible
number of threads working in parallel and the largest defragmentation of given problem
in case of GPUs’ realizations against standard sequential implementations. During
execution of such calculation procedures, basic operations conducted on the CPU and
the GPU are the same, so algorithms remain unchanged from the logical point of view.
In this paper our aim is concentrated at attempting to increase the performance of
parallel implementation of the FFT algorithm. In our case we we’re not interested in
comparing CPU and GPU processing times. As mentioned earlier, such study has al-
ready been undertaken in many research papers and it is well known that the use of
graphics cards may signiﬁcantly reduce the FFT’s execution time, c.f. [13, 14]. In this
article we are focusing on the opportunities arising from the possibility of diﬀerent task-
defragmentation strategies of the FFT implementations on chosen GPU architectures to
achieve optimal FFT computation time-eﬀectiveness. Such approach demands
comparing diﬀerent parallel implementations of FFT to identify the fastest one in terms
of execution times on chosen GPU models. In our research we have used the DIT FFT
[15] as the base procedure for further considerations. The most commonly used in prac‐
tical parallel implementations on GPU devices is the radix-2 DIT FFT algorithm which
is schematically illustrated in Fig. 3.
Fig. 3. Graphical interpretation of standard 16-point DIT FFT parallel GPU implementation.
514
K. Stokﬁszewski et al.

In this implementation a single thread processes only one of the basic butterﬂy oper‐
ations depicted in Fig. 2. The advantage of such a solution is the large defragmentation
of the problem, which is beneﬁcial for many of the consumer segment GPU devices.
Unfortunately, the considered algorithm has also a signiﬁcant disadvantage which is the
necessity of performing synchronizations after each stage to ensure data integrity. Such
defragmentation is clearly not the only one possible to conduct. The very structure of
the original DIT FFT algorithm [8] allows for the transform’s division into customizable
blocks of larger sizes than the considered 2-point subtransforms, as depicted in Fig. 3.
In Fig. 4 one can see how the original radix-2 DIT FFT algorithm structure could be
topologically interpreted using defragmentation into 4-point subtransform blocks.
Fig. 4. 4-point subtransform block visualization on 16-point DIT FFT diagram.
It is easy to notice that the scheme depicted in Fig. 4 is based on successive repetition
of the same basic operations’ graphs in each of its stages. The only diﬀerence, which
has to be taken into consideration are the locations of input data elements, but from the
logical point of view both algorithms are identical.
Fig. 5. 16-point FFT realization with the use of 4-point subtransform blocks.
The Fast Fourier Transform Partitioning Scheme
515

Examples of diﬀerent defragmentations are presented in Figs. 5 and 6. The following
diagrams show two examples of 16-point FFT defragmentation into 4-point and 8-point
subtransforms blocks.
Fig. 6. 16-point FFT realization with the use of 8-point subtransform blocks.
In the case illustrated in Fig. 4, the amount of required synchronizations could be
reduced from 4 to 2, with respect to the original FFT algorithm, and every single thread
is computing a 4-point subtransform at each computational stage. In case illustrated in
Fig. 5 the amount of required synchronizations might also be reduced from 4 to 2 because
of the need of computation of 2-point transformations in the algorithm’s last stage. Here
every single thread is computing 8-point transform in the ﬁrst stage, and single butterﬂy
(2-point transform) in its ﬁnal stage. Operations performed by single thread have been
marked with the dashed lines in Figs. 4, 5 and 6.
Such grouping, originally proposed in [15], potentially induces signiﬁcant beneﬁts
with respect to parallel GPU implementations of the FFT algorithm see e.g. [3, 14] or
[16]. Firstly, the increase in the subtransform size computed by a single thread decreases
the overall number of threads in a single stage and, at the same time, provides potential
for better memory coalescing for each of the GPU’s physical cores. Secondly, the
increase in the subtransform size computed by a single thread enables also for the
decrease in the number of explicit synchronizations, which are often relatively time
consuming operations for many GPU devices. E.g. for an input signal of length 2N, the
classic approach with 2-point transformations partitioning requires N explicit synchro‐
nizations, while the same transformation realized with 4-point subtransforms reduces
this amount by half, like it’s demonstrated in Figs. 3 and 5. Another aspect is the temporal
order of memory access operations which, due to suitable subdivision, can be organized
more eﬀectively. Unfortunately for speciﬁc cases, e.g. like the one presented in Fig. 4,
computation of the complete transformation becomes more complicated because it
requires the additional maximum subdivision step in its last stage. This becomes
obvious, e.g. when looking at the structure presented in Fig. 6, that the calculation of
the entire scheme might not be possible with a single decomposition of the FFT to
subtransforms of a certain size. Scheme that contains an odd number of stages, cannot
516
K. Stokﬁszewski et al.

be fully realized with the 4-point subtransforms because they them-selves consist of two
computational stages.
In order to verify the outlined considerations we have developed a parallel imple‐
mentations of the discussed FFT algorithms. The research implementation was prepared
that is able to compute the FFT of arbitrary input signal size (of the power of 2) providing
at the same time possibility to select any size of transformation processed by a single
GPU thread with the only limitation that size of the transformation computed by single
thread could not be larger than the size of the input vector itself. To create a general
solution to the considered problem, it was necessary to divide the FFT algorithm into
several elementary steps, so its ﬁnal structure is as follows:
1. Copy the raw input data to GPU memory.
2. Determine of the number of GPU blocks and threads, taking into account the chosen
FFT scheme’s defragmentation.
3. Perform parallel calculation of the coeﬃcient dictionary table based on “butterﬂy”
operations depicted in Fig. 2.
4. Perform parallel bit-inverse operation for input array, see e.g. [12].
5. Perform FFT radix-2 diagram realization with the chosen defragmentation strategy:
• Determine of the number of layers and points processed by a single thread during
a single computational stage
• Launch the appropriate number of parallel threads
• Determine the number of remaining layers and select the appropriate subdivision
strategy
• Redeﬁne of the number of layers and points processed by a single thread for the
remaining stages
• Launch the appropriate number of parallel threads
6. Copy the data from graphics card memory to RAM
7. Determine the results of time performance statistics
Fig. 7. Pseudocode illustrating the operation performed by a single parallel thread.
The Fast Fourier Transform Partitioning Scheme
517

The above sequence of operations calculates the FFT for a selected defragmentation
strategy and for a single input data N-vector. For the purposes of the experiments, the
considered sequence was repeated for input data sizes ranging from 24 up to 225 elements
and defragmentation strategies of subtransform blocks of no greater size than 128-point
subtransforms computed by a single thread. Above, in Fig. 7, we also present pseudocode
illustrating the operation performed by a single parallel thread used in our implemen‐
tation. Next section presents the results of the conducted experiments.
4
Results of the Experimental Research
Time eﬀectiveness comparisons of the proposed implementations were performed with
input vectors containing random data and also with arbitrarily prepared signals. The
hardware used to obtain and collect the results was equipped with Nvidia GeForce GTX
1060 graphics card, built in Pascal architecture with CUDA 6.1 compute capability and
4 GB DRAM memory. Furthermore, to conﬁrm the obtained results, exactly the same
computations were performed also on a diﬀerent architecture graphics card, namely the
Nvidia’s GeForce GTX 860M, built in Maxwell architecture with compute capability
CUDA 5.2, and also equipped with 4 GB DRAM memory. In both cases, the host oper‐
ating system was Windows 10 with the latest available GPU drivers, and Nvidia CUDA
Tookit’s 8.0. The research application was written in CUDA C programming language,
with the use of the latest available NVCC compiler.
Execution time comparisons of the considered radix-2 FFT GPU implementations
with diﬀerent subtransform block size partitioning are presented in Tables 1 and 2.
Table 1. Time comparison results for GPU model GTX 1060.
Input vector
size – 2N/time
in ms
Subtransform block size
2
4
8
16
32
4
0.015360
0.012288
0.015360
0.023552
–
5
0.019456
0.017408
0.017408
0.026624
0.052224
6
0.021504
0.018432
0.023552
0.029696
0.056320
7
0.027616
0.021504
0.027648
0.036864
0.059392
8
0.029696
0.023552
0.033792
0.056320
0.068608
9
0.032768
0.028672
0.041920
0.070656
0.093184
10
0.038810
0.032704
0.047104
0.072704
0.156672
11
0.047104
0.043008
0.053216
0.083968
0.158720
12
0.050144
0.080896
0.106496
0.136192
0.186368
The above results include only a part of the overall of the performed tests. Every
execution included FFT transform sizes of 2p, p = 4, 5, … , 25 points, while Tables 1 and
2 present only the results for FFT sizes of 2p, p = 4, 5, … , 12, since the standard
approach involving 2-point subtransform block sizes turned out to be the best in terms
of time eﬀectiveness for all cases in which p > 11. For all the tested input data, FFT’s
518
K. Stokﬁszewski et al.

computations with 2, 4, 8, 16, 32, 64, and 128 subtransform block sizes calculations
were performed. The results gathered in Tables 1 and 2 also don’t include all the tested
cases for the same reason. Table 1 gathers signiﬁcant tests for GTX 1060.
Table 2 gathers exactly the same tests, but this time performed with the GTX 860 M
GPU model.
It turns out that the time-eﬃciency for subtransform block sizes larger than 16 starts
to worsen signiﬁcantly for both considered GPUs.
In order to illustrate the dependence between the execution times of the FFT’s of the
selected sizes with respect to their block size partitionings we have prepared three graphs
which are presented below.
Figure 8 presents execution times for the FFT of input signal length of 128 points,
implemented with partitioning strategies of 2b, b = 1, 2, … , 7 subtransform blocks sizes.
Table 2. Time comparison results for GPU model GTX 860M.
Input vector
size – 2N/time
in ms
Subtransform block size
2
4
8
16
32
4
0.022112
0.016096
0.018848
0.026720
–
5
0.026880
0.021216
0.021376
0.032768
0.058368
6
0.033248
0.024416
0.028608
0.037120
0.066240
7
0.038720
0.030624
0.036480
0.045056
0.073536
8
0.044416
0.034464
0.044192
0.071104
0.085664
9
0.050624
0.039552
0.053568
0.090144
0.115712
10
0.054912
0.044672
0.062848
0.094912
0.198784
11
0.072384
0.067296
0.076512
0.105376
0.204576
12
0.079200
0.119168
0.156576
0.197152
0.272704
Fig. 8. Execution times for the 128-point FFT with various block sizes defragmentation.
The Fast Fourier Transform Partitioning Scheme
519

It can be observed that defragmentation into 2-point, 4-point, and 8-point subtransforms
performs very similar, although increasing further the size of the subtransform has a
negative eﬀect, and this behavior is repeated also for all the other tests.
Figure 9 depicts the same execution times as shown in Fig. 8 but this time of a input
signal length of 1024 points and partitioning strategies of 2b, b = 1, 2, … , 9 subtransform
blocks sizes.
Fig. 9. Execution times for the 1024-point FFT with various block sizes defragmentation.
In Fig. 10 the acceleration ratios between the second fastest, standard 2-point and
the most time-eﬀective amongst all of the tested implementations is shown. It can be
observed that for transform sizes up to 211 the acceleration of the 4-point subtransform
Fig. 10. 4-point subtransforms to 2-point subtransforms defragmentation ratio
520
K. Stokﬁszewski et al.

implementation with respect to a standard 2-point one achieves levels between 10% up
to an almost 30% in particular test cases.
5
Conclusions
In this article authors present a general FFT partitioning scheme that can be adopted for
implementations on diﬀerent GPU architectures whose aim is to lower the overall FFT’s
the execution time by optimizing computational resources’ allocation on the selected
GPU models. The scheme utilizes decimation in time (DIT) FFT algorithm’s imple‐
mentation in which the FFT is partitioned into subtransform blocks of selected size and
within the subtransforms the computations are performed in a sequential manner. Exper‐
imental studies have shown that the maximum ﬁne-grained division of the FFT data
ﬂow diagrams into standard 2-point butterﬂy operations is not always the best choice.
In all cases where the input data size is fairly not to large (up to 2048 elements), sequen‐
tial execution of 4-point transformations by each of single threads comprising the DIT
FFT procedure reduces its total processing time even by up to 30% relative to the
standard solution of calculating 2-point transformations by each of single GPU threads.
However, for larger arrays, more eﬃcient approach turns out to be the smallest defrag‐
mentation int standard 2-point butterﬂy operations. We may conclude that the memory
access performed by each GPU core during the FFT calculation does not have a signif‐
icant eﬀect on its performance. On the other hand synchronization of the GPU threads
has signiﬁcant inﬂuence on the execution times, especially for relatively smaller data
sizes. If the calculation times are relatively large, even the need for increasing the number
of eﬀective synchronizations does not aﬀect the overall execution time results and then
the ﬁne-grained FFT division strategy becomes ones again the most beneﬁcial one. This
study undoubtedly conﬁrms that defragmentation strategies of the GPU implementations
of FFT algorithms might have direct and signiﬁcant impact on the overall FFTs compu‐
tational time.
References
1. Owens, J.D., Luebke, D., Govindaraju, N., Harris, M., Kruger, J., Lefohn, A.E., Purcell, T.:
A survey of general purpose computation on graphics hardware. Comput. Graph. Forum
26(1), 80–113 (2007)
2. Govindaraju, N.K., Lloyd, B., Dotsenko, Y., Smith, B., Manferdelli, J.: High performance
discrete Fourier transforms on graphics processors. In: Proceedings of the ACM/IEEE
Conference on Supercomputing. IEEE Press, Piscataway (2008)
3. Cheng, J., Grossman, M., McKercher, T.: Professional CUDA® C Programming. Wiley,
Indianapolis (2014)
4. Hillis, W.D., Steele, G.L.: Data parallel algorithms. Commun. ACM 29(12), 1170–1183
(1986)
5. Ahmed, U.N., Rao, K.R.: Orthogonal Transforms for Digital Signal Processing. Springer-
Verlag New York Inc., Secaucus (1975)
6. Puchala, D., Stokﬁszewski, K.: Parametrized orthogonal transforms for data encryption.
Comput. Probl. Electr. Eng. J. 3(1), 93–97 (2013)
The Fast Fourier Transform Partitioning Scheme
521

7. Yatsymirskyy, M., Stokﬁszewski, K., Szczepaniak, P.S.: Image compression using fast
transforms realized through neural network learning. Model. Comput. Sci. Technol.
Ukrainian Nat. Acad. Sci. 23, 95–99 (2003)
8. Cooley, J.W., Tukey, J.W.: An algorithm for the machine calculation of complex Fourier
series. Math. Comput. 19(90), 297–301 (1965)
9. Nussbaumer, H.J.: Fast Fourier Transform and Convolution Algorithms. Springer,
Heidelberg (1982)
10. Moreland, K., Angel, E.: The FFT on a GPU. In: Proceedings of the ACM Siggraph/
Eurographics Conference on Graphics Hardware, pp. 112–119. Eurographics Association
Aire-la-Ville, San Diego (2003)
11. Spitzer, J.: Implementing a GPU-eﬃcient FFT. In: Siggraph Course on Interactive Geometric
and Scientiﬁc Computing with Graphics Hardware (2003)
12. Puchala, D., Stokﬁszewski, K., Szczepaniak, B., Yatsymirskyy, M.: Eﬀectiveness of Fast
Fourier Transform implementations on GPU and CPU. Przegląd Elektrotechniczny 92(7),
69–71 (2016)
13. Ambuluri, S.: Implementations of the FFT algorithm on GPU. MSc. thesis, Linköping
University, Department of Electrical Engineering (2012)
14. Dotsenko, Y., Baghsorkhi, S.S., Lloyd, B., Govindaraju, N.K.: Auto-tuning of Fast Fourier
Transform on Graphics Processors. In: Proceedings of the 16-th ACM Symposium on
Principles and Practice of Parallel Programming, San Antonio, TX, USA, pp. 257–266 (2011)
15. Yatsymirskyy, M.: Fast Algorithms for Orthogonal Trigonometric Transforms’
Computations (in Ukrainian). Lviv Acad. Express, Lviv (1997)
16. CUDA® Programming Guide. http://docs.nvidia.com/cuda/cuda-c-programming-guide/
index.html. Accessed 20 Aug 2016
522
K. Stokﬁszewski et al.

Consolidation of Virtual Machines Using
Stochastic Local Search
Sergii Telenyk
, Eduard Zharikov(&)
, and Oleksandr Rolik
Department of Automation and Control in Technical Systems,
National Technical University of Ukraine “Igor Sikorsky Kyiv
Polytechnic Institute”, Kiev, Ukraine
telenik@acts.kiev.ua, zharikov.eduard@acts.kpi.ua
Abstract. A modern cloud data center is represented as a complex system
where virtual machine consolidation and scheduling inﬂuence directly the cloud
cost and performance. The virtual machine consolidation is the subject to many
constraints originating from multiple domains, such as the resource require-
ments, user Service Level Agreement (SLA) compliance, security requirements,
availability requirements, and other. Properly deﬁned resource management
methods and algorithms allow to achieve execution efﬁciency, SLA compliance,
utilization of resources, energy saving, and the increasing proﬁt of cloud pro-
viders. In this paper, the authors propose two versions of the Optimization using
Simulated Annealing (OSA) algorithm to solve dynamic virtual machine con-
solidation problem. The virtual machine consolidation problem is considered as
a multi-dimensional vector bin-packing problem. The authors take into account
that the properties of items can be changed, new items may be requested to be
deploy, and existing items may need to be reassigned to bins. Other constraints
should be taken into consideration to solve virtual machine consolidation
problem such as balanced load of resources of each physical machine, the
limitation on maximum number of simultaneous migrations per physical
machine, hardware constraints and other. The conﬁguration of the system, the
function for obtaining new conﬁguration, the objective function for the opti-
mization problem are determined for the proposed algorithms. The evaluation
results show, that using OSA algorithms the simulated data center consumes
almost the same amount of energy as while using a not optimized algorithm. On
the other hand, the OSA algorithm with constraints allows to decrease overall
performance degradation by virtual machines due to migrations, as a result, SLA
violation is decreased. Furthermore, both OSA algorithms allow to reserve some
resources of physical machine to react to increasing random resource demands
in the nearest future.
Keywords: Virtual machine placement  Cloud computing
Energy efﬁciency  Simulation
1
Introduction
A modern cloud data center is represented as a complex system using virtualization
technologies, different cloud service models (IaaS, PaaS, SaaS), software-deﬁned
technologies and a large amount of heterogenous resources. In the cloud data center
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_37

environment, a service provider faces with different problems caused by the expanding
set of customer requirements and regulations. On one hand, the cloud service providers
should ensure service QoS parameters (such as response time, latency, throughput,
space) improving resource utilization and energy efﬁciency while complying the SLA.
On the other hand, there are resource provision, resource allocation, resource mapping,
resource scaling, resource estimation, and resource brokering tasks to be performed to
achieve a high-quality performance of application services with a minimal amount of
resources involved.
The resource allocation problem is an important and an urgent problem in modern
cloud data centers now. The IaaS cloud service model enables customers to request
dynamically the needed number of virtual machines (VM) based on their business
requirements. One of the main tasks of managing resources in IaaS is the VM allo-
cation on the physical machine (PM) of the cloud data center. The VM allocation
process must be performed with the minimal number of PMs involved and with
decreasing energy consumption. The process of selecting, which VM should be allo-
cated at each PM of a cloud data center, is known as VM consolidation or VM
placement problem. The VM placement problem has been extensively studied [1].
The VM placement is the subject to many constraints originating from multiple
domains, such as the VM resource requirements, security requirements, availability
requirements, and other. Many of the constraints may be deﬁned in an SLA associated
with each customer’s VM [2].
The virtual machine consolidation problem is considered as a multi-dimensional
vector bin-packing problem taking to account that the properties of items can be
changed, new items may be requested to be deploy, and existing items may need to be
reassigned to bins. The workload of multiple services in a cloud data center can change
over time when a set of clients requesting services or jobs spans one or multiple VMs.
This requires to solve periodically an optimization problem to deploy new and real-
locate existing VMs in the data center. Reassignment of new and existing VMs should
be performed periodically in asynchronous mode as needed. Stochastic local search
approach is one of the widely used technique to solve such hard combinatorial opti-
mization problem. In this paper, simulated annealing technique is adopted to dynamic
VM consolidation problem with uniform loading of CPU, RAM and network interface
to achieve maximal productivity and fully utilize data center resources. As shown in the
literature [3–5], stochastic local search methods can be applied to cloud management
problems ﬁnding acceptable solutions in admissible amount of time and resources.
Initial VM allocation calculated by using heuristics such as First Fit Decreasing
(FFD) or Best Fit Decreasing (BFD). In this paper, the BFD heuristic with sorting all
VMs in decreasing order of their current CPU utilization is used. At the same time,
initial VM allocation does not account the usage of each PM’s RAM, storage, and
network interface utilization. To obtain near optimal VM allocation map with more
balanced loading of CPU, RAM and network interface of each PM the optimization
process employs simulated annealing technique.
The simulation environments are widely used to conduct repeatable large-scale
experiments on different data center resource management algorithms [6]. Many
research studies are based on simulation methods, since it is very expensive to conduct
experiments on a production data center infrastructure. Thus, the use of simulation
524
S. Telenyk et al.

tools and frameworks has become a useful and powerful approach in a cloud com-
puting research community. In [7], a CloudSim toolkit is proposed to support system
design and modeling of cloud components such as data centers, virtual machines,
physical machines and provisioning strategies that can be customized according to
researcher’s demand.
Thus, it is important to design and develop algorithms and approaches to solve the
dynamic VM consolidation problem as a part of the cloud data center management
problem.
The remainder of the paper is organized as follows: in Sect. 2 the related work
review is presented, Sect. 3 describes the system model of the VM placement problem,
in Sect. 4 the problem of VM placement is formulated taking into account new VM
deployments and migrations of existing VMs, Sect. 5 describes the modeling process
using simulated annealing technique, in Sect. 6 the conﬁguration of simulated envi-
ronment and modeling results are presented, and Sect. 7 concludes the paper.
2
Related Work
In the recent years, different approaches have been proposed to solve the VM place-
ment problem in the cloud environment [1, 8, 9]. The VM placement problem is an
optimization problem with different objective functions. Several researchers have
highlighted this problem and proposed various solutions to ﬁnd the most optimal
placement plan for VMs with the objective to minimize number of PM used, with
speciﬁed SLA. Most of them make use of the ordering algorithms and prediction
techniques to generate the VM allocation map.
To solve VM placement problem the meta-heuristics are widely used, such as tabu
search [10], evolutionary algorithm [4], ant colony optimization [3], neighborhood
search [11], simulated annealing [5] and other. The reason to use the local search
algorithms is to improve the results obtained by such heuristics as FFD, BFD and their
modiﬁcations. There are several objective functions used for VM placement problem
including the energy consumption minimization [12], number of PMs minimization
[13], network trafﬁc minimization [14], Availability Maximization [15], resource uti-
lization maximization [12] and others.
During the last decade, many approaches to various VM placement problems have
been proposed. In [16], the authors propose an efﬁcient multi-start iterated local search
metaheuristic for multi-capacity bin packing problem and machine reassignment
problem. The proposed solution approach relies on such parameters as the thresholds
restart, shaking restart, the size of the shaking operator, and the pruning criteria. The
proposed approach produces high-quality solutions in a reasonable computational time
for large-scale instances of both problems. In most cases more than 99% of the possible
improvement from the initial solution have been achieved by reorganizing the pro-
cesses with proposed method. However, there are two main drawbacks of the proposed
approach. First, all processes migrate (VM migrations performed) simultaneously
without any time-processing delay, and second, the proposed approach does not
account the limitation on maximum number of simultaneous migrations per physical
machine.
Consolidation of Virtual Machines Using Stochastic Local Search
525

To solve the VM consolidation problem, the authors in [10] have proposed two
phases VM consolidation algorithm which minimizes the number of required PMs,
while also guaranteeing that the migrations performed in the transition to the new
mapping to be completed in a speciﬁed maximum time. The ﬁrst phase aims at ﬁnding
a feasible mapping of VMs to PMs that minimizes the maximum migration time of all
virtual machines. The second phase employs tabu search metaheuristic to produce
solutions using a smaller number of physical servers, but also respecting a maximum
migration time threshold. However, the authors have not determined how to detect
overloaded PMs and have not considered other PM resources.
A multi-objective optimization mechanism for VM consolidation have designed
and implemented in [11]. The authors also proposed mechanism that allows data center
operators to add other optimization objectives. The proposed mechanism allows to
reduce computational overheads by classifying PMs into a relatively small number of
equivalent sets on the basis of the status of each one. It also realizes neighborhood
search to ﬁnd the lowest cost VM placement to serve VM deployment request from a
user. But the proposed mechanism does not consider existing VMs and constraints on
the number of VM migrations per PM. Other limitation is that the VM placement is not
adapted at run-time.
In [3], the authors propose a multi-objective ant colony system algorithm for the
virtual machine placement problem to efﬁciently obtain a set of non-dominated solu-
tions that simultaneously minimize total resource wastage and power consumption.
However, the constraints on the number of VM migrations per PM are not considered
and to characterize a VM and a PM only CPU and memory resources are used. The
proposed VM placement algorithm also does not allow dynamically consolidate
existing VMs.
In this paper, the approach to solving VM consolidation problem is based on Power
Aware Best Fit Decreasing (PABFD) heuristic [17] but, at the same time, balanced load
of each physical machine resources is optimized using simulated annealing technique
while taking into account the limitation on maximum number of simultaneous
migrations per physical machine.
3
The System Model
The system model is shown in Fig. 1 [18]. The structure of the control object is
represented by a set of PMs, each of which is characterized by the heterogeneous
hardware and operating platform. Besides, each PM has a ﬁxed capacity of resources.
A PM enables a multiple virtual machine allocation. The PMs are connected using
some form of topology and constitute the data center. In production data center, the
number of VMs always changes. It is necessary to consider that in dynamic environ-
ment some VMs determined for migration may cease to exist during control.
The Global Manager (GM) is the main module of data center centralized man-
agement and is implemented as a failover and high-availability cluster of the PMs [19].
The GM coordinates data center virtual and physical resource management and allows
to select a variety of resource and virtual machine management policies in order to
adapt to the impact of external factors. The GM performs VM consolidation, PM and
526
S. Telenyk et al.

VM state management, VM scheduling, and new VM placement. The GM decides in
which physical machines should VMs be allocated.
On the application layer, there is a set of clients requesting services or jobs
spanning one or multiple VMs. Each VM is associated with speciﬁc performance goals
speciﬁed in the SLA. The admission control module is responsible for handling user
requests so that SLAs of admitted requests are met. A single VM can process one task
at a time.
4
Problem Formulation
In a cloud data center, within a certain time there are M PMs, with heterogeneous
conﬁguration. Each PM is represented by multi-dimensional resources such as CPU,
memory, storage and network interface bandwidth. All VMs on a physical machine
share CPU, memory, storage and network bandwidth resources provided by the PM.
The VM placement problem arises when one needs to determine which VM should be
placed on which PM. This problem is to be solved.
Virtual machine placement problem consists of two parts. The ﬁrst part of VM
placement problem is an initial allocation of new N VMs with different resource
demand to appropriate physical machines with speciﬁc resource capacity. The second
part of VM placement problem is a reallocation of some VMs to other PMs due to
demand and constant workload changes. Both parts of VM placement problem can be
viewed as an extension of the bin-packing problem, where N numbers of items with
different sizes (properties) to be placed on M numbers of bins, with an objective to
minimize the number of bins used. The VM placement problem differ from classical
bin-packing problem, as the properties of items can be changed, and there are other
constraints while mapping virtual machines to physical machines [20]. For example, to
provide VM live migration without downtime it is recommended to limit maximum
Fig. 1. The system model.
Consolidation of Virtual Machines Using Stochastic Local Search
527

number of concurrent migrations per PM. The VM placement problem can be con-
sidered also as a multi-dimensional vector bin-packing problem because each item has
more than two properties.
It is important to note that to provide a near 100% guarantee of no SLA violation
during VM migrations hypervisors limit maximum number of simultaneous migrations
per PM, per network resource, and per datastore [21]. On some workloads that wildly
use RAM or on older hardware, the big number of simultaneous migrations (more than
recommended number or practically determined number) could saturate a system
resource and result in downtime.
The problem of dynamic VM consolidation consists of four parts [17]: (1) deter-
mining an overloaded PM and the migration of one or more VMs from this PM is
required; (2) determining an underloaded PM and the migration of all VMs from this
PM is required to switch it to the sleep mode; (3) determining VMs that should be
migrated from an overloaded PM; and (4) ﬁnding a new PMs for the VMs selected for
migration from the overloaded and underloaded PMs. As a result of solving part (3) of
the problem, GM determines a list of VMs from overloaded and underloaded PMs to be
placed.
As shown in [22], cloud data centers continuously receive an unpredictable number
of VM deploy requests over time. In [23], to account admission of new requests for
VM provisioning the high-level algorithm of new and existing VMs placement is
proposed. According to the high-level algorithm [23], Global Manager runs a separate
instance of the control process for each request from the PM or for a new VM pro-
visioning. It is proposed initially to serve a request from the PM overloaded, after that
the request for new VMs creation, and then the request from underloaded PM. As a
result of high-level algorithm, the GM obtains a set GVM of virtual machines to be
placed in the data center. GM also determines a set GPM. The GPM is the set of suitable
PMs as expectants for hosting new and migrating VMs.
Thus, it is necessary to develop an algorithm of allocation VMs from the GVM set
using minimum PMs with uniform loading of CPU, RAM and network interface to
achieve maximal productivity and fully utilize data center resources.
5
VM Placement Modeling
According to the problem formulation, the dynamic allocation of VMs based on
simulated annealing algorithm is presented in this section. The problem of VM con-
solidation can be seen as a bin packing problem with variable bin sizes and prices. To
solve it a modiﬁcation of the simulated annealing algorithm is applied.
Simulated annealing is a general purpose combinatorial optimization technique [24].
It uses the Metropolis Criterion [25] and Boltzmann distribution to accept a new
solution. Using this technique, the solution space of the VM consolidation problem is
optimized. It is explored in a controlled fashion using a temperature T as a control
parameter so that allocation map of VMs with successively better measures for the
objective function is obtained. To adopt simulated annealing, it is necessary to deter-
mine a conﬁguration of the system, a function for obtaining new conﬁguration, the
528
S. Telenyk et al.

objective function for the optimization problem, and maximum number of allowed
consecutive iterations that yield no improvement relatively to current conﬁguration.
The allocation map of VMs MAPbest, the set of VMs GVM, and the set of PMs GPM
constitute the conﬁguration of the system for simulated annealing. The aim of the
algorithm is to obtain better VM allocation map in term of objective function. At every
evolution step of the algorithm, the allocation map of VMs is changed into a new
neighborhood state by using search function. Each new allocation map of VMs is
estimated by comparison of its acceptance indicator with the acceptance indicator of
current allocation map of VMs to take a decision whether or not to accept a new
allocation map as a current allocation map. In this paper, for VM placement problem
the maximum number of allowed consecutive iterations, that yield no improvement
relatively to current conﬁguration, is accepted being equal to 100.
The pseudo-code for the algorithm of Optimization using Simulated Annealing is
presented in Algorithm 1. As a result of Algorithm 1 close-to-optimal solutions of VM
placement can be obtained. The primary objective of the OSA algorithm is to calculate
VM allocation map and pass it to GM for migration process. Next run of the OSA
algorithm is performed after completing of all VM migrations prescribed by previous
run. The solution has to be an allocation map with minimum PMs used and with
uniform CPU, RAM and network interface utilization of each PM to achieve maximal
productivity and utilize different resources fully.
The new neighborhood state is determined by using of two types of search func-
tions. First, the simple search function (simple OSA) is developed and estimated.
A simple strategy is to swap two different randomly picking VMs changing their
assignment even if the VM was rearranged at previous steps of the algorithm. Second
strategy (OSA with constrains) is to limit number of migrations per PM. In this case
there are two constraints taking into account: (1) particular VM can be reassigned only
once during the optimization and (2) the number of VMs migrating to and from each
PM is limited.
As an input the algorithm gets the set of VMs GVM to be allocated, the set of PMs
GPM, temperature T, and maximum number of allowed consecutive iterations that yield
no improvement MaxCount. The allocation map MAP stores not only mappings of
VMs to PMs calculated by Algorithm 1 but also a state of VMs and PMs. For the OSA
with constrains, each PM can be limited to perform VM migrations, so the limit on
maximum number of concurrent migrations per PM can be reached. In lines 2 and 3 the
initial state of each PM (MigrationCount) is set to zero. Each VM can be in migration
state (VM.isInMigrationSatate = true) or not (VM.isInMigrationSatate = false). In
lines 4 and 5 the initial state of each VM is set to false.
Consolidation of Virtual Machines Using Stochastic Local Search
529

The function getInitialSolution() returns initial VM allocation calculated by using
heuristics such as BFD or FFD. In our experiments, the BFD heuristic with sorting all
VMs in decreasing order of their current CPU utilization is used. The initial VM
allocation is calculated according to one of the deﬁned strategy. For the OSA with
constrains the maximum number of concurrent migrations per PM is constrained. At
the same time, initial VM allocation does not account the usage of each PM’s RAM
and network interface utilization.
530
S. Telenyk et al.

To ﬁnd more balanced and productive solution the function swap() is called. The
pseudo-code of the function swap() for the simple OSA algorithm is presented in
Algorithm 2. The pseudo-code for the algorithm of the function swap() with migration
constraints is presented in Algorithm 3. As a result of swap() function, a new VM
allocation map is obtained. A new VM allocation map is acquired by picking two
different VMs and changing their PMs randomly by swapping their assignment. This
rearrangement of VMs results in creation of new conﬁguration and reﬂects the basic
operations of doing bin-packing to load a PM more uniformly. The main beneﬁt of
Algorithm 2 is its simplicity for obtaining new allocation map. As a result, more states
can be explored during the optimization process. However, the main drawback of
Algorithm 2 is that it does not account production requirements and generates too many
VM migrations for each optimization cycle.
As the Algorithm 1 visits more solutions, its temperature T drops. In high tem-
perature states, the algorithm is allowed to choose a new allocation map even if it is
worse than the one currently existing.
In this paper, the objective function being optimized is being minimized, and when
we say that some allocation map of VMs results in a better objective function than
some other allocation map, what we mean is, that allocation map of VMs has a smaller
value for the objective function. The function getEstimation() is used to calculate the
objective function.
The objective function IB for VM allocation optimization can be represented as
follows (1):
IB ¼
X
K
i¼1
IBi
ð1Þ
IBi ¼
1
CPUiRAMiNETi
ð2Þ
where IBi is an imbalance indicator of i-th PM, CPUi, RAMi, NETi are utilization of
CPU, memory and network interface of i-th PM respectively, K is a number of PMs
used in initial solution.
High values of IBi refer to unbalanced utilization of CPU, RAM and network
interface while a value close to 1 indicates a good level of balancing. The values very
close to 1 are not recommended because resource demand in the nearest future can
increase, so it is preferable to reserve some resources to prevent SLA violations. Three
kinds of resources (CPU, RAM, and network interface) are considered in the objective
function. However, the approach can be extended to any number of resources sup-
ported by simulation environment.
Consolidation of Virtual Machines Using Stochastic Local Search
531

532
S. Telenyk et al.

The authors also consider the case, when there are no overloaded PMs detected. In
some cases, there are only underloaded PMs during some long period of time and there
are no new VM deployed. If the number of unused resources on a PM is greater than
the threshold, then that PM is added to a consolidation list GPM. The algorithm restarts
if the total number of unused resources on the PMs, included in the consolidation list,
exceeds the resources of one PM. All migrations initiated by the OSA at the previous
stage must be completed. The selection of the threshold value is not considered in this
paper.
6
Evaluation
To conduct repeatable large-scale experiments to evaluate the proposed algorithms on a
data center infrastructure is very expensive. To evaluate the proposed OSA algorithms,
a CloudSim toolkit [7] is used. It is a modular and extensible open source toolkit which
has built-in capability to implement and compare management algorithms for different
cloud environments and workloads. The extended version of CloudSim is used [26] to
enable energy-aware simulations.
A data center that comprises 800 heterogeneous PMs with two cores was used for
simulation [17]. One half of PMs are HP ProLiant ML110 G4 servers with 1860
Million Instructions Per Second (MIPS), and the other half consists of HP ProLiant
ML110 G5 servers with 2660 MIPS. Each PM is modeled to have 1 GB/s network
bandwidth. VMs characteristics correspond to Amazon EC2 instance types but with
one core because of the nature of workload data. The workload traces from a real
system were used as a workload for simulations. The workload traces are obtained as a
part of the CoMon project, a monitoring infrastructure for PlanetLab. The interval of
utilization measurements is 5 min [27].
The amount of RAM for VMs is as follows: High-CPU Medium Instance (2500
MIPS, 0.85 GB); Extra Large Instance (2000 MIPS, 3.75 GB); Small Instance (1000
MIPS, 1.7 GB); and Micro Instance (500 MIPS, 613 MB). Initially the VMs are
allocated according to the resource requirements deﬁned by the VM types [12].
Following methods [26] are modiﬁed in order to realize OSA algorithm:
getNewVmPlacement, getNewVmPlacementFromUnderUtilizedHost, and ﬁndHost-
ForVm from the package org.cloudbus.cloudsim.power.
The simulation results have been obtained from a set of experiments. To compare
the efﬁciency of the proposed algorithm with CloudSim built-in ones the metrics from
[17] are used as follows. In [17], two metrics were proposed for measuring the level of
SLA violations in an IaaS cloud: the percentage of time, during which active hosts have
experienced the CPU utilization of 100%, SLA violation Time per Active Host
(SLATAH) (3); and the overall performance degradation by VMs due to migrations,
Performance Degradation due to Migrations (PDM) (4). The authors in [17] concluded
that if a PM serving applications is experiencing 100% utilization, the performance of
the hosted applications is bounded by the PM capacity, therefore, VMs experience
performance degradation.
Consolidation of Virtual Machines Using Stochastic Local Search
533

SLATAH ¼ 1
M
X
M
i¼1
TSi
Tai
ð3Þ
PDM ¼ 1
N
X
N
j¼1
Cdj
Crj
ð4Þ
where M is the number of PMs; TSi is the total time during which the host i has
experienced the utilization of 100% leading to an SLA violation; Tai is the total time of
the host i being in the active state; N is the number of VMs; Cdj is the estimate of the
performance degradation of the VM j caused by migrations; Crj is the total CPU
capacity requested by the VM j during its lifetime. Cdj is estimated as 10% of the CPU
utilization in MIPS during all migrations of the VM j [17].
Another metric proposed in [17] is a combined metric SLA Violation (SLAV) that
encompasses both performance degradation due to PM overloading and due to VM
migrations (5). The combined metric that captures both energy consumption and the
level of SLA violations is denoted Energy and SLA Violations (ESV) (6).
SLAV ¼ SLATAH  PDM
ð5Þ
ESV ¼ E  SLAV
ð6Þ
Each experiment has been repeated ten times for each day of the workload traces.
The ﬁnal results are averaged over these experiments. Simulation results for THR, IRQ,
MAD, LRR, and LR algorithms are obtained from [17]. The metrics of both OSA
algorithms (3)–(6) are calculated and presented in Table 1.
The objective of the experimental phase has been to evaluate the quality indicators
of OSA algorithm under different load conditions and to compare them with results
obtained in [17]. Results shown in Table 1 indicate that using both OSA algorithms
Table 1. Simulation results of the algorithms in [17] and both OSA algorithms (median values).
Algorithm
ESV
(10−3)
E
(kWh)
SLAV
(10−5)
SLATAH
%
PDM
%
THR-MMT-0.8
4.19
89.92
4.57
4.61
0.10
IQR-MMT-1.5
4.00
90.13
4.51
4.64
0.10
MAD-MMT-2.5
3.94
87.67
4.48
4.65
0.10
LRR-MMT-1.2
2.43
87.93
2.77
3.98
0.07
LR-MMT-1.2
1.98
88.17
2.33
3.63
0.06
OSA algorithm (simple)
1.81
88.24
2.05
3.41
0.06
OSA algorithm (with
constraints)
1.71
88.4
1.93
3.85
0.05
534
S. Telenyk et al.

simulated data center consumes almost the same amount of energy as LR-MMT-1.2
because the OSA algorithm uses LR and MMT policies but only performs VM
placement according to simulated annealing optimization technique. Using simple OSA
algorithm the PDM metric is not changed comparing to LR-MMT-1.2 algorithm
because more uniform PM loading does not inﬂuence the number of VM migrations
directly. As a result of both OSA algorithms, it should be noted that the more uniform
PM loading allows to decrease SLATAH metric. Using the OSA algorithm with
migration constraints it is possible to decrease the number of migrations and, as a
result, to decrease PDM metric. The uniform PM loading has the effect which reduces
SLA violations due to reservation of some PM resources to react to increasing random
resource demands in the nearest future.
7
Conclusion
To meet cloud data center management requirements, in this paper, the virtual machine
consolidation problem is solved with uniform loading of CPU, RAM and network
interface of PM. That has been done to achieve maximal productivity and fully utilize
data center resources.
The VM consolidation problem is considered as a multi-dimensional vector
bin-packing problem taking to account that the properties of items can be changed, and
there are other constraints while mapping virtual machines to physical machines.
The authors propose two variants of Optimization using Simulated Annealing
algorithms to solve virtual machine consolidation problem for hosting new and already
deployed VMs. The conﬁguration of the system, the function for obtaining new con-
ﬁguration, the objective function for the optimization problem are determined for the
proposed simulated annealing algorithm.
To decrease the number of SLA violation during VM migrations the constraint on
the number of simultaneous migrations per PM is considered in the OSA algorithm
with constraints. The evaluation results show, that using the OSA algorithms simulated
data center consumes almost the same amount of energy as while using a not optimized
algorithm. On the other hand, the OSA algorithm with constraints allows to decrease
overall performance degradation by VMs due to migrations, as a result, SLA violation
is decreased. Furthermore, both OSA algorithms allow to reserve some resources of
physical machine to react to increasing random resource demands in the nearest future.
References
1. Pires, F.L., Barán, B.: A virtual machine placement taxonomy. In: 15th IEEE/ACM
International Symposium on Cluster, Cloud and Grid Computing (CCGrid), pp. 159–168
(2015)
2. Calcavecchia, N., Biran, O., Hadad, E., Moatti, Y.: VM placement strategies for cloud
scenarios. In: 5th IEEE International Conference on Cloud Computing CLOUD, pp. 852–
859 (2012)
Consolidation of Virtual Machines Using Stochastic Local Search
535

3. Gao, Y., Guan, H., Qi, Z., Hou, Y., Liu, L.: A multi-objective ant colony system algorithm
for virtual machine placement in cloud computing. J. Comput. Syst. Sci. 79(8), 1230–1242
(2013)
4. Mark, C.C., Niyato, D., Chen-Khong, T.: Evolutionary optimal virtual machine placement
and demand forecaster for cloud computing. In: IEEE International Conference on Advanced
Information Networking and Applications (AINA), pp. 348–355 (2011)
5. Wu, Y., Tang, M., Fraser, W.: A simulated annealing algorithm for energy efﬁcient virtual
machine placement. In: IEEE International Conference on Systems, Man, and Cybernetics
(SMC), pp. 1245–1250 (2012)
6. Kaleem, M.A., Khan, P.M.: Commonly used simulation tools for cloud computing research.
In: 2nd International Conference on Computing for Sustainable Global Development
(INDIACom), pp. 1104–1111 (2015)
7. Calheiros, R.N., Ranjan, R., Beloglazov, A., De Rose, C.A., Buyya, R.: CloudSim: a toolkit
for modeling and simulation of cloud computing environments and evaluation of resource
provisioning algorithms. Softw. Pract. Experience 41(1), 23–50 (2011)
8. Salimian, L., Saﬁ, F.: Survey of energy efﬁcient data centers in cloud computing. In: 2013
IEEE/ACM 6th International Conference on Utility and Cloud Computing, pp. 369–374.
IEEE Computer Society (2013)
9. Mills, K., Filliben, J., Dabrowski, C.: Comparing VM-placement algorithms for on-demand
clouds. In: IEEE Third International Conference on Cloud Computing Technology and
Science (CloudCom), pp. 91–98 (2011)
10. Ferreto, T., De Rose, C., Heiss, H.U.: Maximum migration time guarantees in dynamic
server consolidation for virtualized data centers. In: Euro-Par 2011 Parallel Processing,
pp. 443–454. Springer (2011)
11. Shigeta, S., Yamashima, H., Doi, T., Kawai, T., Fukui, K.: Design and implementation of a
multi-objective optimization mechanism for virtual machine placement in cloud computing
data center. In: Cloud Computing, pp. 21–31. Springer (2013)
12. Cao, Z., Dong, S.: An energy-aware heuristic framework for virtual machine consolidation
in cloud computing. J. Supercomput. 69, 1–23 (2014)
13. Sun, M., Gu, W., Zhang, X., Shi, H., Zhang, W.: A matrix transformation algorithm for
virtual machine placement in cloud. In: 12th IEEE International Conference on Trust,
Security and Privacy in Computing and Communications (TrustCom), pp. 1778–1783
(2013)
14. Pires, F.L., Barán, B.: Multi-objective virtual machine placement with service level
agreement: a memetic algorithm approach. In: 2013 IEEE/ACM 6th International
Conference on Utility and Cloud Computing, pp. 203–210. IEEE Computer Society (2013)
15. Wang, W., Chen, H., Chen, X.: An availability-aware virtual machine placement approach
for dynamic scaling of cloud applications. In: 9th International Conference on Ubiquitous
Intelligence and Computing and 9th International Conference on Autonomic and Trusted
Computing (UIC/ATC), pp. 509–516 (2012)
16. Masson, R., Vidal, T., Michallet, J., Penna, P.H.V., Petrucci, V., Subramanian, A.,
Dubedout, H.: An iterated local search heuristic for multi-capacity bin packing and machine
reassignment problems. Expert Syst. Appl. 40(13), 5266–5275 (2013)
17. Beloglazov, A., Buyya, R.: Optimal online deterministic algorithms and adaptive heuristics
for energy and performance efﬁcient dynamic consolidation of virtual machines in cloud data
centers. Concurrency Comput. Pract. Experience 24(13), 1397–1420 (2012)
18. Telenyk, S., Zharikov, E., Rolik, O.: An approach to software deﬁned cloud infrastructure
management. In: XI International Scientiﬁc and Technical Conference on Computer Science
and Information Technologies Congress on Information Technology (CSIT 2016), pp. 21–26
(2016)
536
S. Telenyk et al.

19. Telenyk, S., Zharikov, E., Rolik, O.: Architecture and conceptual bases of cloud IT
infrastructure management. In: Advances in Intelligent Systems and Computing, vol. 512,
pp. 41–62. Springer (2017)
20. Li, X., Qian, Z., Chi, R., Zhang, B., Lu, S.: Balancing resource utilization for continuous
virtual machine requests in clouds. In: 6th IEEE International Conference on Innovative
Mobile and Internet Services in Ubiquitous Computing, IMIS, pp. 266–273 (2012)
21. Limits on Simultaneous Migrations. https://docs.vmware.com/en/VMware-vSphere/6.0/com.
vmware.vsphere.vcenterhost.doc/GUID-25EA5833-03B5-4EDD-A167-87578B8009B3.
html. Accessed 10 July 2017
22. Amazon Usage Estimates. http://blog.rightscale.com/2009/10/05/amazon-usage-estimates/.
Accessed 10 July 2017
23. Telenyk, S., Zharikov, E., Rolik, O.: An approach to virtual machine placement in cloud data
centers.
In: International
Conference
Radio
Electronics
and Info
Communications
(UkrMiCo), pp. 1–6 (2016)
24. Kirkpatrick, S., Gelatt, C.D., Vecchi, M.P.: Optimization by simulated annealing. Science
220, 671–680 (1983)
25. Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., Teller, E.: Equation of
state calculations by fast computing machines. J. Chem. Phys. 21, 1087–1092 (1953)
26. CloudSim: A Framework For Modeling And Simulation Of Cloud Computing Infrastruc-
tures And Services. https://github.com/Cloudslab/cloudsim. Accessed 10 July 2017
27. Park, K., Pai, V.S.: CoMon: a mostly-scalable monitoring system for PlanetLab.
ACM SIGOPS Oper. Syst. Rev. 40(1), 65–74 (2006)
Consolidation of Virtual Machines Using Stochastic Local Search
537

Architecture and Models for System-Level
Computer-Aided Design of the Management
System of Energy Efﬁciency of Technological
Processes at the Enterprise
Taras Teslyuk(&), Ivan Tsmots(&), Vasyl Teslyuk(&),
Mykola Medykovskyy(&), and Yuriy Opotyak(&)
Department of Automated Control Systems, Lviv Polytechnic National
University, S. Bandera Street, 12, Lviv, Ukraine
taras.teslyuk@gmail.com, ivan.tsmots@gmail.com,
vasyl.m.teslyuk@lpnu.ua, medyk@lp.edu.ua,
yuvoua@ukr.net
Abstract. Multilevel system architecture of energy efﬁciency management in
the region is developed in this work. The architecture model consists of three
levels, namely, level of data collection and management of executive mecha-
nisms, level of control and management of technological process, and level of
operator control and formation of administrative decisions. Hardware and
software of each hierarchical level are developed. Furthermore, models of
system-level computer-aided design, which are based on a Petri net theory and
allow the dynamics of the system operation to be examined, are developed. The
results of a research of the system using constructed models are represented by a
reachability graph for states of the system.
Keywords: Multi-level management system  Energy efﬁciency
Microcontrollers  Architecture  Hardware and software  Petri net models
Reachability graph for states of the system
1
Introduction
The development of enterprises characterized with the intensive introduction of
information technologies that provide automation of processes and the accumulation of
large amounts of information about their dynamics. Energy efﬁciency management at
the level of technological processes requires the creation of a common information
space with accurate, complete and operative information about ﬂow of technological
processes at the enterprise. Approaching of processing means (microcontroller systems)
to the sources of information (sensors) and executive mechanisms is one of the ways to
reduce the volume of information. Accumulated data are processed using computa-
tional intelligence technologies. The received results are used to form management
solutions. Enhancement of the management of energy efﬁciency of technological
processes at the enterprise can be achieved by developing multi-level management
system of energy efﬁciency of technological processes (MMSEETP), which should
integrate all the functions of monitoring and management in a single system. In
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_38

developing of MMSEETP it is advisable to focus on the widespread use of telecom-
munication and Web technology, database tools for data collection, evaluation, oper-
ative analytical and intellectual data In developing of MMSEETP it is advisable to
focus on the widespread use of telecommunication and Web technology, database tools
for data collection, evaluation, operative analytical and intellectual data visualization of
their processing and decision making, visualization of processing results and decision
making.
In works [1, 2] the hardware and software means of MMSEETP are analyzed,
modern information technology and expediency of their use for the development of
automated process control systems are considered. The industrial control systems,
sensors and executive mechanisms are also considered in details.
In the next works [1] the modern microcontrollers and micro controlled systems,
programmable logic controllers, sensors, wired and wireless communication facilities
are considered. The issues of collection, storage and processing of data are reviewed in
details [3–7]. According to the analysis of publications [3] the formation of effective
administrative decisions in MMSEETP is based on the results of the analytical and
intellectual processing of collected data.
However, it is paid insufﬁcient attention to an integrated approach to the devel-
opment of software and hardware means and synthesis MMSEETP in the above works.
The aim is to develop architecture MMSEETP focused on maximum use of
ready-made components and Internet technologies.
To achieve this goal, it is necessary to solve the following tasks: to determine a list
of problems to solve MMSEETP; to form MMSEETP requirements; to MMSEETP
design principles; to elaborate MMSEETP structure; to identify problems and tools
designed for every level of the system.
2
Model of MMSEETP System Architecture
2.1
Tasks of MMSEETP
To ensure management of energy efﬁciency on the level of technological processes
MMSEETP should provide resolving of the following tasks:
• real-time collection, storage and pre-processing of data on the manufacturing
processes;
• forecasting, management of process control and executive mechanisms;
• setting parameters of technical means depending on environmental conditions;
• drafting and analysis of the energy balances of production and consumption of
energy carriers;
• generation of reports of energy efﬁciency and the creation of their templates;
• integration of various data with databases and web servers;
• data protection in the system from unauthorized access;
• modeling of processes and intelligent processing of collected data;
• analysis of collected data and determining ways of reducing technical and
non-productive losses of energy resources;
Architecture and Models for System-Level Computer-Aided Design
539

• visualization of multidimensional data on energy efﬁciency and presenting the
results of processing data in graphs and charts;
• formation and control of administrative decisions.
2.2
Requirements and Design Principles of MMSEETP
Structure of MMSEETP should focus on collecting data, creating a database with
operational, reliable, comprehensive information, processing it, preparation of man-
agement solutions for operators and formation, if necessary, executive mechanisms
control signals. The peculiarity of MMSEETP work should be: high ﬂexibility of
conﬁguration, support of remote control and using wired and wireless interfaces for
communication between system components.
Computer components used for the synthesis MMSEETP should ensure solving
problems in real time. In particular, this concerns problem of ﬁltering the previous data
processing and control actuators. Application of computer components directly from
the sensors and executive mechanisms imposes severe restrictions on their weight and
size characteristics. At the same time, the following components are put forward strict
requirements for power consumption, affecting the power supply size and heat dissi-
pation means. In addition, these components meet high requirements for survivability
and reliability [8]. Computer Components of MMSEETP must provide veriﬁcation of
performance and rapid localization problems.
It’s proposed to develop MMSEETP with using a systematic approach that covers
all levels of process integration, hardware and software. Synthesis of MMSEETP is
based on the ﬁnished hardware and software, because the development and production
of new ones demands considerable time and money. When selecting components it is
necessary to consider set of factors, namely: information about ready hardware and
software components and their speciﬁcations, compliance with standards interfaces, the
possibility of their purchase and more.
2.3
Development of MMSEETP Structure
Modern MMSEETP are a handy tool to support decision-making at all levels of
government. Managers who use such systems do not focus on the problems of inte-
gration and data processing, and they focus directly to the energy problems by creating
effective management decisions.
To ensure these requirements MMSEETP designed structure, which is given in
Fig. 1, where PC - personal computer; PLC - programmable logic controller; MK -
microcontroller; EM - executive mechanism, SCADA - (Supervisory Control And Data
Acquisition) supervisory control and data collection, PLC - (Programmable logic
controller) DCS - (Distributed Control System) distributed control system.
Developed MMSEETP basic structure consists of three levels:
• Data collection and control executive mechanisms;
• monitoring and process control;
• operator control and formation of management decisions.
540
T. Teslyuk et al.

The speciﬁcs of each level process control system are deﬁned by hardware and
software components [4]. At each hierarchical level management the tasks the
appropriate level of difﬁculty are solved.
An operation algorithm for each level of the system being designed is shown in
Fig. 2 through Fig. 4. In particular, the system at the ﬁrst level carries out the following
functions: data collection from sensors; impact on the state of actuators; independent
operation; communication with higher levels; execution of higher-level commands;
data preprocessing, etc.
The ﬁrst-level elements are usually single threaded and run a main program in an
inﬁnite loop. The main tasks of the ﬁrst level of the system to be designed is to execute
commands from outside and implement internal logic. For this purpose, it is necessary
to consider the ability to switch between the major commands.
That is why the invented algorithm includes the next features such as a cyclical
inquiry about available input commands and their execution, or execution of an internal
cycle of the system’s operation.
Step 1: While the system is started, the initialization begins, which involves reading
performance conﬁguration, initialization of ports and communication channels, etc.
Step 2: Upon system initialization, input communication channels are scanned so as
to make sure that external commands are received. If the input command is
received, advance to step 10. If no input command is received, proceed to step 3.
Step 3: The device must perform commands according to the internal logic of the
system. For the analysis of the environment state, it is necessary to consistently
enquire its state and data received from the sensors.
Fig. 1. Basic structure of MMSEETP
Architecture and Models for System-Level Computer-Aided Design
541

Step 4: Based on data from the sensors, a preliminary analysis and transformation of
data is carried out, which may presuppose normalization, scaling and other simple
manipulations.
Step 5: Changes in the state of environment should be veriﬁed. The comparison
with previously recorded values of the state of the system is made for this. If the
state undergoes a change – go to step 6. If no changes occur – advance to step 7.
Step 6: A change in the state has to be notiﬁed to the higher levels. For this an
incoming message is generated and sent to the level of TP.
Step 7: As ﬁrst-level elements are able to perform simple logical operations and
operate automatically, it is necessary to check the correspondence between internal
Fig. 2. Block diagram of the ﬁrst-level operation of the designed system
542
T. Teslyuk et al.

scenarios of the operation of the system and the current state. If the actuator needs to
be changed, go to step 8, otherwise choose step 2.
Step 8: Transmit control signals to change the state of actuators following the
internal scenarios.
Step 9: Report TP on changes occurred in the state of actuators, then go to step 2.
Step 10: Upon receiving input commands, check and analyse a string obtained.
A type of command as well as support command options needs also to be received.
Step 11: If this command involves obtaining data on the state of the sensor, proceed
to step 12. Otherwise, choose step 14.
Step 12: The state and current results have to be read from a sensor which is
described by the command.
Step 13: Carry out a preanalysis and transformation of data from the sensor, proceed
to step 17.
Step 14: If this command is to change the actuator’s state, go to step 15. Otherwise,
proceed to step 16.
Step 15: Transmit control signals to change the state of actuators which are
described by the input command. Proceed to step 17.
Step 16: Execute another external command according to the advanced serial
communications protocols.
Step 17: Make and send a report about execution of the input command, then
advance to step 2.
At the second level the system comprises: storage of data on the current state of the
TP system; analysis of lower-level input data; checking a TP system state; generating
control commands for the stabilization of the TP state; transmitting and enquiring
devices of lower levels; control of connection with the lower-level elements, com-
munication with the mentioned ones.
At the second level of design, one can implement more complex logic, which may
comprise execution of periodic tasks, multithreading and more complex communica-
tion between levels.
Step 1: While the system is started, the initialization begins, which involves reading
performance conﬁguration, initialization of ports and communication channels,
kernel initialization.
Step 2: Upon system initialization, input communication channels are scanned so as
to make sure that external commands are received. If an input command is received,
go to step 15. If there isn’t any input command, proceed to step 3.
Step 3: The status of periodic tasks has to be checked. If it is time for accomplishing
the task, go to step 4. Otherwise, proceed to step 8.
Step 4: Each periodic task is assigned a list of subtasks for execution. The list of
subtasks is being made.
Step 5: The subtasks are transformed into the commands for the system and the
elements of the lower level in order to read data, change the state of actuators, etc.
Step 6: A queue of commands is formed to send them to the lower-level elements.
Step 7: The commands are consistently sent to the lower-level elements, proceed to
step 2.
Architecture and Models for System-Level Computer-Aided Design
543

Step 8: Launch an internal logic core. Update the value of environment state and
running actuators.
Step 9: Check the performance of the system following internal logic of the TP
operation. It is necessary to verify whether the system parameters are not beyond
the limits.
Step 10: Report about changes in the state of TP to the level of operator control
(OC).
Step 11: If the output of the system parameters is detected, go to step 12. Otherwise,
go back to step 2.
Step 12: Generate a list of necessary tasks for the stabilization of the system.
Step 13: Transform the tasks intended to stabilize the state of the system into the
commands for the elements of microcontroller (MC).
Step 14: Form a queue of commands for the lower-level elements. Transfer the
commands consistently, then go back to step 2.
Step 15: Check the recipient of the command. If the command was received from
OK, move to step 16. Otherwise, go to step 18.
Step 16: Check and analyze the command given from OC. Receiving a type of
command as well as support command options is also necessary.
Step 17: Change the operating parameters of TP following the command from OC.
Report about the command execution, go back to step 8.
Step 18: Check and analyze the command or report from the MC. A type of
message and its support parameters have to be obtained.
Step 19: Update the value of the system state, go back to step 8.
At the third level of design, the following functions are fulﬁlled: storage of a large
amount of data about the system’s state over time; an in-depth analysis of data about
the system; presentation of the system’s state in a more understandable form to the
operator
(SCADA
systems);
user
interactions;
generating
commands
for
the
lower-level elements; control of connection with the elements of the lower levels.
At the third level of design, communication with an operator should be imple-
mented. It is therefore necessary to consider the task showing the state and a two-way
interaction with the operator.
Step 1: While the system is started, the initialization begins, which involves reading
performance conﬁguration, the initialization of ports and communication channels,
the kernel initialization and initialization of a user interface.
Step 2: Upon system initialization, input communication channels are scanned so as
to make sure that external commands are given from TP. If an input command is
received, advance to step 10. If there isn’t any input command, proceed to step 3.
Step 3: If the command is received form the operator, go to step 4. Otherwise, go
back to step 2.
Step 4: Check the type of input command given from the operator. If this is the
command that forecasts the state of TP, go to step 5. Otherwise, move to step 7.
Step 5: Launch the kernel of data analysis of the system. Conﬁgure the kernel,
specify search parameters and make prediction. Make calculations of predictions.
Step 6: Representing the prediction results for the user. Return to step 2.
544
T. Teslyuk et al.

Step 7: If this is a command that changes the state of the TP, go to step 8.
Otherwise, go back to step 2.
Step 8: Generate a list of necessary changes and transfer these tasks to a set of
commands for TP.
Step 9: Send commands to the level of TP. Return to step 2.
Step 10: Analyze the incoming report given from TP. Selecting and excluding the
type of report and a set of input data.
Step 11: Update the value of database system according to the report obtained.
Step 12: Display the changes in the state of TP on the user interface. Return to
step 2.
3
Levels Design of MMSEETP
3.1
The Level of Data Collection and Management of Executive
Mechanisms
At this level, the primary information that is pre-processed, collected and comes to the
controls is formed. With the use of the information generated signals to manage
executive mechanisms and process are formed. Keep in mind that the companies built
in recent years are provided with means of collecting technological information, and
the selection and integration process of collecting data is already in the design phase. In
this case, it’s used staff means of mentioned systems, to solve problems of MMSEETP
(Fig. 3).
A more difﬁcult situation is observed in companies that are already in operation. In
these companies, especially for small and medium businesses deployment of systems
for collecting and processing are usually not planned due to their high cost in the past.
The performed analysis of typical manufacturing processes of current production
makes it possible to formulate a number of features that must be considered when
creating MMSEETP:
• monitoring of process parameters is desirable to use the technologies for efﬁciency
and simplicity of deployment;
• the number of points of measurement parameters can be changed, and therefore, the
system should have an open architecture with the possibility of scaling;
• hardware system should be developed on the base of modern standard solutions to
ensure simplicity and low cost;
• a broad application of basic telecommunications protocols is necessary.
To pre-processing of data from sensors and management of executive mechanisms
it is appropriate to use the most simple hardware in the form of ready industrial
components. To create this level of MMSEETP today it’s necessary to focus on
single-chip microcontrollers, System on Chip (SoC) and Programmable logic inte-
grated circuits (PLICs) [9].
The class of single-chip microcontrollers, that are made on the basis of available
modules for a wide range of systems, is presented today by means of companies ST
Microelectronics, Atmel, STCmicro, Microchip, TI and others. It is usually 8 bit
Architecture and Models for System-Level Computer-Aided Design
545

programmable microcontroller with architecture RISC or CISC and clock speed of tens
MHz, built-in RAM, Flash-memory, Eeprom and peripherals that supports interfaces
UART, SPI, CAN, I2C. The amount of internal memory for storing data and programs
is small and ensures the simple functions of measurement data to the appropriate
interface and basic management. The advantages of these microcontrollers include,
above all, low cost of ﬁnished industrial modules based on them, debugging boards,
programmer and debugger.
Recently, 32 bit microcontrollers and ready modules based on them, for example,
based on ARM architecture company ST Microelectronics appeared on the market [10].
Clock frequency of hundreds of MHz RAM at hundreds kB, embedded Flash-memory
Fig. 3. Block diagram of the second-level operation of the designed system
546
T. Teslyuk et al.

and interfaces UART, SPI, I2C, CAN enable the creation of means of data collection
and management to ensure implementation rather complex processing algorithms.
From a practical point of view, using of such means in the ﬁrst place we should pay
attention to a single address space, which hosted Flash-memory, RAM, Eeprom, reg-
isters and peripherals. It greatly simpliﬁes code writing and codes of different stacks
and libraries are easily converted as primarily designed for von Neumann architecture
(in meaning of address space). However, tires for access to different types of memory
are divided, indicating the presence of the Harvard architecture.
Considered 8 and 32 bit microcontrollers provide an easy integration into the
system of various sensors and controls, because they contain the main industrial
interfaces.
To develop microprograms languages C, C++ and assembler are typically used. For
some microcontrollers developed free libraries peripheral drivers and software frame-
work (software framework), which provides substantial ease software development.
Recently, means of data transferring at the enterprises create technology-based
Ethernet and Wi-Fi, that is caused by the availability and a wide nomenclature of cable
systems, active and passive network equipment. Wireless technology of Wi-Fi net-
works are most comfortable in a production environment where requirements include
portability, ease of installation and use (of course, subject to consideration of elec-
tromagnetic compatibility with existing manufacturing equipment and the absence of
interference from its side and the impact on the functioning of the network). The
advantage is the ﬂexibility of network architecture with the ability to change dynam-
ically the network topology, speed, design and implementation, that is critical with
strict requirements for the duration of network, no need in cabling, support for stack of
telecommunication protocol TCP/IP.
To transfer data over wireless networks it’s necessary to have a special radiomodem
to ensure the physical and channel layers of the OSI model in wireless network and to
support the senior levels of the OSI model on a high performance microprocessor is
need. Today specialized devices - modules RS232-Wi-Fi are produced, but their cost
may be comparable to the cost of basic microcontroller system [11].
However, in recent years, a new class of specialized microcontrollers appeared,
which provides full support for Wi-Fi and protocol stack TCP/IP. In addition, a rather
powerful processor, RAM and peripherals rather developed with support protocols SPI,
I2C and UART are placed on the chip. On the basis of this device, you can create a
complete wireless device that can receive data from the sensor, perform pre-processing
and transmit the information via telecommunication protocols on a local or remote
server. These tools gave a new boost to the development of Internet technology things.
In addition, on the basis of these specialized microcontrollers ready industrial modules
are available that can be used in the implementation of tasks MMSEETP lower level.
Among these tools should indicate microcontroller NL6621 of Nufront ﬁrm, RTL8710
of Realtek ﬁrm, ESP8266 of company Espressif. For example ESP8266 provides a full
support of Wi-Fi functions (host mode and access points with providing authentication
protocols WEP and WPA/WPA2) and protocol stack TCP/IP. ESP8266 has an inte-
grated 32 bit RISC processor Tensilica Xtensa LX106 with a clock speed of 80 MHz,
64 KB RAM and 96 KB command RAM data supports external Flash-memory storage
Architecture and Models for System-Level Computer-Aided Design
547

of ﬁrmware from 512 KB to 16 MB. The chip provides support interfaces UART, SPI,
I2C and includes single-channel 10-bit ADC [12].
To create software, software framework, libraries of a wide range of sensors sup-
porting are accessible. For development C language can be used, in some cases
development can be simpliﬁed because of use by the use LUA interpreter.
To implement on the lower level complex data processing algorithms single-board
computers based on specialized processor SoC can be used. Single-board microcom-
puter is a promising platform for automation systems, and it has an open architecture,
low price and uses the operating system Linux. One of the ﬁrst in this class, the
Raspberry Pi microcomputer appeared in 2012 and built on the system (SoC) Broad-
com BCM2835, which includes ARM processor with a clock speed of 700 MHz GPU
VideoCore IV, and 512 or 256 MB RAM, SD-card is used as an additional memory
card. Microcomputer is produced in several versions: junior (A) (700 MHts clock
Fig. 4. Block diagram of the third-level operation of the designed system
548
T. Teslyuk et al.

speed, 256 MB RAM, one USB port), older (B) (up to 1.2 GHz clock speed with
Ethernet, 1 GB of RAM, up to 4 USB ports), recently appeared cheaper version - Zero
(up to 1 GHz clock speed, 512 MB RAM, one microUSB port). Raspberry Pi has
GPIO ports, supporting interfaces UART, SPI, I2C, which you can use to connect
sensors and manage [13, 14].
Recently single-board computer on SoC signiﬁcantly expanded, for example,
BeagleBone Black processor AM3359 ARM Cortex-A8 ﬁrms Texas Instruments; Intel
Edison processor Intel Atom; pcDuino A10 ARM processor company AllWinner;
Cubieboard2 A20 A20 with ARM processor, a line OrangePI processor H3, H2 + of
company AllWinner. These single-board computers have developed periphery, they
can connect a variety of sensors and provide work in a wireless network.
From the point of view of development, they are full-ﬂedged Linux systems. To
implement the algorithms languages C, C++, Python, sometimes - assembler are used.
The problem may be the lack of speciﬁc software framework for a single-board
computer, however, project Armbian can help to solve the problem, which develops
Linux distributions on basis of Debian for SoC based on ARM, and each computer has
already installed and has ready to use development tools.
For direct management of executive mechanisms software DCS and PLC are used.
At this level of MMSEETP it’s advisable to use FPGA as ﬁnished industrial units
FPGA/CPLD ﬁrm Altera - MAXII EPM240, Altera Cyslone ll EP2C5T144, Altera
Cyclone IV EP4CE6, ﬁrm Xilinx - CoolRunner-II FPGA CPLD XC2C64A, FPGA
Spartan-3E XC3S250E and other more productive to solve problems that require high
ﬂow performance. Developing for entry-level FPGA can be done using free versions of
the software.
3.2
The Level of Control and Management of Technological Process
This level of management is assumed to be sufﬁciently autonomous, that can work
independently for a long time without loss of information in the absence of commu-
nication with the upper level. At this level an above-mentioned single-board computer
on the basis of SoC, and, for example, programmable logic controllers Mitsubishi
Melsec FX3U, the standard means of visual inspection and process control can be used.
Mitsubishi
FX3U
programmable
controllers
are
the
most
powerful
and
high-performance controllers in the line of controllers MELSECFX (FX1S, FX3S,
FX1 N, FX3G). Architecture of controller is two-tired that increases its capacity.
Mitsubishi programmable controller provides connectivity expansion modules as the
previous generation and new generation units of FX3U series. When connecting
modules of FX3U controller automatically switches to its high switching bus mode and
data exchange takes place at high speed. Modules FX0 N, FX2 N controller works at
normal speed. Thus, the use of expansion modules provides increased number of
inputs/outputs up to 256 (with direct address), and a station of the decentralized
input/output to 384.
In addition, Mitsubishi programmable controller provides high-speed connectivity
adapter modules FX3U-XXX-ADP, that extend the capabilities of the controller when
dealing with analog signals and increase the number of additional communication
interfaces (RS232/422/485).
Architecture and Models for System-Level Computer-Aided Design
549

Over the last decade there have been several PLC programming languages, for
which the standard IEC 61131 is developed. This standard speciﬁes changes in the area
of programming languages for the process control systems. It covers requirements for
hardware, installation, testing, documentation, communication and PLC programming.
The standard deﬁnes ﬁve programming languages, which are divided into textual and
graphical. The textual PLC programming languages include the Instruction List (IL),
the language of instructions; Structured Text (ST), the language of structured text.
The IL language is a typical assembler of batteries and transition tags. A set of
instructions is standardized, and it is independent from a speciﬁc target platform. This
language provides work with any type of data; calls functions and function blocks
implemented in other languages. With IL the algorithms of any complexity are easily
implemented.
The ST language is a high-level language, which is syntactically similar to Pascal.
Instead of Pascal procedures, in ST the IEC-deﬁned software components are used.
The graphical PLC languages include: Sequential Function Chart (SFC) is a lan-
guage of consistent function charts; Function Block Diagram (FBD) means a language
of functional and block diagrams; Ladder Diagrams (LD) denotes a language of
relay-switching circuits.
The SFC language is a high-level graphical tool, in which a graphic chart consists
of steps and transitions between them. Allowance for the transition is determined by the
condition while with the step speciﬁc actions are associated.
The FBD language is oriented towards a description of schematic circuits of a
chip-based electronic device. The conductors in FBD are used for transmission of any
type of signals (logic, analog, time signal, etc.). The signals from the outputs of the
blocks can be issued to the inputs of other units or directly to the PLC outputs. The
blocks themselves, shown in the diagram as “black boxes”, can fulﬁl any function. By
using FBD, it is provided a description of interconnections between the inputs and
outputs of the diagram. If the algorithm is well described in terms of signals, then its
FBD representation is always more vivid than in textual languages.
The LD language is designed to implement the structure of electric circuits.
Graphically the LD chart is shown as two vertical power rails. Between them are
located the circuits formed by coupling the contacts. A relay serves as the load on each
circuit. Each relay has contacts that can be used in other circuits. Sequential (I), parallel
(OR) connection of contacts and inversion (NOT) constitute a Boolean basis. As a
result, LD is ideal not only for developing relay-controlled devices, but also for soft-
ware implementation of combinational logic circuits. Due to the ability of LD to use
functions and functional blocks written in other languages, the sphere of its application
is increasing.
The languages of IEC 61131 are developed on the basis of the most popular
programming languages for modern controllers. The programs written for modern
controllers can be transferred into the environment IEC 61131-3 at minimum expense.
The feature of IEC 61131 is that there is an ability to create hardware-independent
libraries for the implementation of regulators, ﬁlters, motor control, fuzzy logic mod-
ules, etc.
For programming controllers with the IEC 61131-3 deﬁned languages, it is
developed a programming environment CoDeSys (Controllers Development System)
550
T. Teslyuk et al.

whose editors and debugging tools are based on the principles of a popular professional
programming environment (Visual C++, etc.). The feature of the programming envi-
ronment CoDeSys [https://www.codesys.com/] is that it’s not restricted to a speciﬁc
hardware platform. A modiﬁcation is made to different PLC programming environ-
ments by adapting the program to low-level resources such as memory allocation,
communication interfaces, and input-output interfaces. The CoDeSys environment
provides: direct machine code generation; the full implementation of standardized IEC
61131-3 languages; language intelligent editors that correct mistakes common to the
beginners; an embedded controller emulator that allows debugging a project without
additional hardware; built-in visualization elements providing a creation of a model of
a control object and debugging a project without making simulation tools; the use of
ready-made libraries and service functions.
3.3
The Level of Operator Control and Forming of Management
Decisions
This level is represented by an automated operator workplace. Hardware that can be
used at this level is determined by the intensity of ﬂow data, processing algorithms
complexity and reliability requirements that apply to the control system [16, 17]. As
hardware can be used for operator workstations RISC- or Intel-platforms and industrial
computers such as Mitsubishi Electric, BECKHOFF, Eaton, AXIOMTEK, that are
corresponding to industrial conditions with high requirements for durability and reli-
ability. Tasks at this level are: collecting data from peripheral controllers and micro-
controller systems; data storage; data processing; processing video streams, image
recognition and scenes in vision systems; synchronization of distributed subsystems;
visualization and mapping of the implementation process; formation of management
decisions.
To solve such problems the system SCADA is used, its main function is to create
an operator interface and the data collection process. To control and manage techno-
logical process, tool software is used, which is a combination of hardware channels and
algorithmic and software means.
Features of the tasks solved at the level of operator control and formation of
management decisions are:
• large volume and diversity of data;
• inconsistency and incompleteness of data;
• consistency and high intensity of incoming data;
• a large amount of computation with a predominance of logical computing opera-
tions in processing of video streams, image recognition and scenes in vision
systems;
• constant complication of algorithms and increasing requirements for accuracy of
results;
• a possibility of parallelization of data processing both in time and in space.
In consideration of the widespread development and implementation of wireless
means of data exchange and the increasing integration of different systems from the
Internet in recent years, there is the task of creating a universal platform that would
Architecture and Models for System-Level Computer-Aided Design
551

support as a function ensure guaranteed data exchange with peripherals and functions
to create universal secure storage, access, in accordance with their authority in any part
of the world. Thus, with the implementation of MMSEETP arises another task (level) -
the formation of an integrated system of sharing and storing information from dis-
tributed systems.
Herewith, the aim of this system is to provide an association between sensors,
microcontroller systems and executive mechanisms with leading, wireless channels,
and arrangement of coordination with the Internet. An open platform of data inter-
change Device Hive can be used for this. This technology is ﬂexible, scalable and easy
to use and provides communication between hardware components on the basis of
M2M. Application of Device Hive provides the formation of the communication
environment, program control and the use of multiplatform libraries for development of
remote management and monitoring, telemetry, remote management and monitoring.
With technology Device Hive it’s able to work using a wide range of technologies,
such as “embedded Linux”, Python, libraries of C++, the protocol JSON, or connect
the AVR, Microchip microcontrollers [10, 15]. The peculiarity of working with Device
Hive is an organization above all access to the network, and then - programming of
speciﬁc applications, reducing design time transfer methods.
The solutions of implementation of distributed data exchange microcontroller
devices and systems are offered by leading companies as Google, Oracle, Amazon, and
a number of lesser-known providers of this service.
4
Development of Petri Net Models
For the research and analysis of operation in a system-level design of MMSEETP, a
Petri net model is employed [18]. Accordingly, the Fig. 5 shows a developed structured
model based on the theory of Petri nets, which is designed to analyse the dynamics of
MMSEETP’s operation and generate a reachability graph for states of the system.
The model based on the Petri nets can be described using the following expression:
Pn ¼ fPS; TrS; ArcS; BBPoSMarkg;
ð1Þ
where PS ¼ p1; p2; . . .; pn
f
g – a set of position (states); TrS ¼ t1; t2; . . .; tn
f
g – a set
of transitions; ArcS – a set of curves, input and output arcs according to the transitions;
BBPoSMark – set, which sets the initial marking of Petri nets.
An example of segments of the reachability graph for parts of system are shown in
Figs. 6 and 7. These results make it possible to argue that the system is alive, the
required states are reachable and deadlocks are absent.
The models based on the theory of coloured Petri nets offer wider possibilities for
analyzing the operation of a system designed. In this case a model can be described
using the following expression [19]:
Pn col ¼ fPS; TrS; ArcS; BBPoSMark; TpS; PTpS; ArcStpS; CnDg;
ð2Þ
552
T. Teslyuk et al.

Fig. 5. Block diagram model of a Petri net system
Architecture and Models for System-Level Computer-Aided Design
553

Fig. 6. Research results in a form of the reachability graph for states of the system designed
554
T. Teslyuk et al.

Fig. 7. Research results in a form of the reachability graph for states of the system designed
Architecture and Models for System-Level Computer-Aided Design
555

where TpS – a set of types; PTpS – a set that reﬂects the set of available positions in the
network; ArcSTpS – a set of the markers that stimulate the transition, or indicate what
types of tokens need to be generated by the transition; CnD – a set of the markers that
need to be stimulated by the transition, or indicate what types of tokens need to be
generated; m – conversions in colored Petri nets.
5
Conclusions
1. The architecture of BSUETP, which is based on the principles such as system
integration, use of pre-built components and basic design decisions, modularity,
openness and interoperability, has been developed.
2. The computer components of BSUETP should provide data processing in real time,
considering limitations on size, power consumption and cost.
3. The high-speed, small-sized and with low power consumption hardware compo-
nents of BSUETP are developed on the basis of the ﬁxed-point microcontrollers, a
small word length, a truncated interface and a shortened system of commands.
4. For programming microcontrollers, it is advisable to use the programming envi-
ronment CoDeSys (Controllers Development System) whose editors and debugging
tools are based on the principles of professional programming environment (Visual
C++, etc).
5. It is suggested to combine sensors, microcontroller systems and actuators with each
other through wired and wireless channels and connect them to the Internet via the
Device Hive technology.
6. The algorithm for the system operation has been developed and the models based
on the Petri net theory have been built, which allows the dynamics of the system
operation to be examined in the system level design. The results that are acquired
from examining the dynamics of the system appear to argue that the system operates
properly and correctly.
References
1. Medykovskyi, M.O., Tsmots, I.G., Tsymbal, Y.V.: Information analytical system for energy
efﬁciency management at enterprises in the city of Lviv (Ukraine). Actual Probl. Ekonomics
1(175), 379–384 (2016)
2. Medykovskyi, M.O., Tsmots, I.G., Tsymbal, Y.V.: Intelligent data processing tools in the
systems of energy efﬁciency management for regional economy. Actual Probl. Ekonomics
12(150), 271–277 (2013)
3. Apaкeлoв, B.E., Кpeмep, A.И.: Meтoдичecкиe вoпpocы экoнoмии энepгopecypcoв. M:
Энepгoaтoмиздaт. 188 c (1990)
4. Гaнжa, B.Л.: Ocнoвы эффeктивнoгo иcпoльзoвaния энepгopecypcoв. Mн.: Бeлopycкaя
нayкa. 452 c (2007)
5. Mинaeв,
И.Г.,
Caмoйлeнкo,
B.B.:
Пpoгpaммиpyeмыe
лoгичecкиe
кoнтpoллepы:
пpaктичecкoe pyкoвoдcтвo для нaчинaющeгo инжeнepa. Cтaвpoпoль: AГPУC, 100 c
(2009)
556
T. Teslyuk et al.

6. Якoвлeв, O.Г.: Пpoгpaммиpoвaниe ПЛК. Mocквa (2001)
7. Пpoгpaммиpyeмыe лoгичecкиe кoнтpoллepы Mitsubishi Electric. http://www.sovras.com/
fx.php
8. Sydor, A.R., Teslyuk, V.M., Denysyuk, P.Y.: Recurrent expressions for reliability indicators
of compound electropower systems. Tekhnichna elektrodynamika 4, 47–49 (2014)
9. Patti, R.S.: Three-dimensional integrated circuits and the future of system-on-chip designs.
Proc. IEEE 94(6), 1214–1224 (2006)
10. Teslyuk, T., Denysyuk, P., Kernytskyy, A., Teslyuk, V.: Automated control system for
arduino and android based intelligent greenhouse. In: Proceeding of the XIth International
Conference Perspective Technologies and Methods in MEMS Design, Polyana-Lviv,
Ukraine, pp. 7–10 (2015)
11. Parkhomenko, A., Gladkova, O., Kurson, S., Sokolyanskii, A., Ivanov, E.: Internet-based
technologies for design of embedded systems. In: Proceedings of 13th International
Conference: The Experience of Designing and Application of CAD Systems in Microelec-
tronics, CADSM (2015)
12. Thaker, T.: ESP8266 based implementation of wireless sensor network with Linux based
web-server. In: Symposium on Colossal Data Analysis and Networking (CDAN) (2016)
13. Kochlan, M., Hodon, M., Cechovic, L., Kapitulik, J., Jurecka, M.: WSN for trafﬁc
monitoring using Raspberry Pi board. In: Federated Conference on Computer Science and
Information Systems (FedCSIS), 7–10 September 2014, pp. 1023–1026 (2014)
14. Leccese, F., Cagnetti, M., Trinca, D.: A smart city application: a fully controlled street
lighting system isle based on Raspberry-Pi Card, ZigBee sensor network and WiMAX.
Sensors 14(12), 24408–24424 (2014). Special Issue on “Sensors and Smart Cities”
15. Jidin, A.Z., Yusof, N.M., Sutikno, T.: Arduino based paperless queue management system.
TELKOMNIKA (Telecommun. Comput. Electron. Contr.) 14(3), 839–845 (2016)
16. Stefanovych, T., Shcherbovskykh, S., Droździel, P.: The reliability model for failure cause
analysis of pressure vessel protective ﬁttings with taking into account load-sharing effect
between valves. Diagnostyka 16(4), 17–24 (2015)
17. Mulyak, A., Yakovyna, V., Volochiy, B.: Inﬂuence of software reliability models on
reliability measures of software and hardware. East. Eur. J. Enterp. Technol. 4, 9(76), 53–57
(2015)
18. Teslyuk, V.M., Beregovskyi, V.V., Pukach, A.I.: Development of smart house system model
based on colored Petri nets. In: Proceedings of International Seminar/Workshop on Direct
and Inverse Problems of Electromagnetic and Acoustic Wave Theory, DIPED 2013, Lviv,
Ukraine, pp. 205–208, September 2013
19. Jensen, K., Kristensen, L.M.: Coloured Petri Nets: Modelling and Validation of Concurrent
Systems, 1st edn. Springer, Heidelberg (2009). 395
Architecture and Models for System-Level Computer-Aided Design
557

Basic Components of Neuronetworks
with Parallel Vertical Group Data Real-Time
Processing
Ivan Tsmots1(&), Vasyl Teslyuk1(&), Taras Teslyuk1(&),
and Ihor Ihnatyev2(&)
1 Lviv Polytechnic National University, Lviv, Ukraine
ivan.tsmots@gmail.com, vasyl.m.teslyuk@lpnu.ua,
taras.teslyuk@gmail.com
2 Ternopil National Economic Universiity, Ternopil, Ukraine
iiv@tneu.edu.ua
Abstract. Neuroalgorithms and neuronetwork structures were analyzed, basic
components of neuronetworks were deﬁned and the principles of their devel-
opment were chosen. It was shown that using the method of parallel vertical
group data processing for the implementation of the neuronetworks basic
components provides speed increase, reduce of hardware costs and increasing of
the equipment use efﬁciency. Parallel vertical group codes converter, which
provides time alignment of data receipt processes and bit sections formation,
was developed. The methods and the structures of the components with parallel
vertical group data processing for deﬁnition of maximum and minimum num-
bers in the arrays, calculation of the sum of differences squares and scalar
product, which due to parallel processing of bit sections groups, provide speed
increase, were developed. It was shown that use of the developed basic com-
ponents for neuronetworks synthesis will provide reduction of time and devel-
opment cost.
Keywords: Neuroalgorithms  Real-time processing
Neural oriented computer system  An integrated approach
Principles  Structure  Specialized module algorithm
1
Formulation of the Problem
When we use real-time neurotechnologies in industry [1] (control of technological
processes and complex objects), electric power industry [2] (load optimization in the
electric networks), military industry [3, 4] (technical vision, motion control of mobile
robot), transport (motion and engine control), medicine [5–7] (diagnostics of diseases)
and instrument making [8, 9] (image recognition and management optimization) we
require processing of intensive data streams with neuronetwork, which meet limitations
concerning dimensions, weight, energy consumption etc. In order to provide a wide
range of applications, such neuronetwork means should simply adapt to the require-
ments, and costs and time of thir development should be minimal. Existing hardware
neuronetwork means can’t meet such requirements, since thay have structural
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_39

organization of the universal type, which functionally and structurally is redundant and
does not take into account the requirements of speciﬁc applications.
Development of neuronetwork hardware, which meets listed limitations, requires
wide use of modern elements base (half-ordered and ordered super integrated circuits
(VLSI), single-crystal neuroprocessors) and development of new methods, algorithms
and specialized VLSI-structures for the implementation of basic neurooperations.
In this regard, the problem of basic components (ПE) development of real-time
hardware neuronetworks, which simply adapt to the requirements of speciﬁc applica-
tions, provide high efﬁciency of the equipment use and are oriented on synthesis of
wide range of neuronetworks, becomes especially actual.
2
Analysis of Recent Research and Publications
Analysis of tasks and ﬁelds of application of real-time neuronetworks shows that they
have the following features [10–16]:
• high intensity and stability of input data streams;
• permanent complication of processing algorithms and increasing the requirements
for the accuracy results;
• possibility of parallel processing both in time and in space;
• ability for generalization and abstraction;
• ability for learning, self learning and self-organization under the inﬂuence of the
environment.
Existing neuronetworks and means of their implementation were analyzed in the
works [10–12]. The analysis shows that the majority of neuronetworks are imple-
mented using software. Such neuronetworks have relatively low performance and need
signiﬁcant hardware costs for their implementation.
It becomes evident from the analysis of works [10, 11, 14–16], that in order to
ensure high performance of real-time neuronetworks, it is necessary to use hardware
implementation with conveyorization and spatial parallelism.
The analysis of neuronetwork algorithms shows that their operational basis consists
of three groups of basic operations: (1) pre-processing; (2) processor operations;
(3) calculation of transfer functions.
The group of pre-processing operations provides conversion of initial data for their
better perception by neuronetwork. This group includes the following operations:
• input data normalization, during which input data are reduced to one range;
• quantization is performed with continuous quantities, for which ﬁnite set of discrete
values is deﬁned;
• ﬁltration is performed for «noisy» data and is designed to discard values, which,
most likely, are incorrect.
Normalization of input data is the procedure of input data preprocessing (educa-
tional, test and work samples), during which the parameter, forming input vector, is
reduced to some given range. After such normalization all values of input parameters
will be reduced to some narrow range (usually, [0, 1] or [−1, 1].
Basic Components of Neuronetworks
559

There are many methods of input parameters normalization. The simplest one, but
in most cases effective, is linear normalization. If initial data should be reduced to the
range [0, 1], it is executed in this way:
X
j ¼ Xj  Xmin
Xmaxmin
;
ð1Þ
where j = 1,…,N, N – input data number.
In order to reduce initial data to the range [−1, 1], linear normalization is carried
out as follows:
X
j ¼ Xj
Xmax
:
ð2Þ
It is necessary to develop the methods and structures for calculating such operations
for hardware implementation of such normalization:
• deﬁnition of maximum number from the group of numbers;
• dividing.
After data normalization, depending of the network type, other procedures of data
pre-processing can be used. In particular, it is necessary to perform calculation of the
Euclidean distance from each input vector to all others after data normalization in RBF-
and GRNN-networks. Basic operation for calculation of the sum of differences squares
is used for this calculation:
Y ¼ jjXe
j  Xb
j jj2 ¼ Xe
1  Xb
1

2 þ Xe
1  Xb
2

2 þ . . . þ Xe
N  Xb
N

2:
ð3Þ
Another types of data pre-processing can be used for other types of neuronetworks.
For instance, for neuro- indeﬁnite networks input data transformations using indeﬁnite
sets and the rules of fuzzy logic, having good approximation capabilities, are carried
out. Wavelet-transformations, for instance, Haar wavelet, or Daubeşi wavelets [17–20]
are used for the analysis of different frequency components of input data in wavelet
neural networks.
The group of processor multi-operand operations consists of such operations:
• calculation of scalar product;
• group adding.
The group of operations, realizing transfer functions. This group of operations uses
mainly such functions: rigid threshold function, sigmoid or hyperbolic tangent.
In threshold function, for neuron output deﬁnition, total sum is compared with
some threshold. If the sum is larger than the threshold parameter, processing element
generates a signal, otherwise the signal is not generated or braking signal is generated.
Sigmoid or S-shaped curve approximates minimum and maximum value in
asymptotes within the range [0, 1]. Hyperbolic tangent approximates minimum and
maximum value within the range [−1, 1]. The important feature of these curves is
continuity of functions and their derivatives.
560
I. Tsmots et al.

Comparison schemes are used for hardware implementation of threshold function,
and tabular realization is used for calculation of sigmoid and hyperbolic tangent [21].
Disadvantage of existing hardware for implementation of basic neurooperations is
complexity of provision of data input intensity receipts with computational intensity of
such hardware.
3
Purpose and Tasks of the Research
The purpose of work – development of methods and structures for hardware imple-
mentation of basic components of real-time neuronetworks with parallel vertical group
processing of data.
In order to achieve the set goal, it is necessary to solve the following tasks:
• to formulate requirements and to choose the principles of basic components
development of real-time neuronetworks with parallel vertical group processing of
data;
• to develop parallel vertical group converter of codes;
• to develop method and structure of the component of maximum and minimum
values calculation;
• to develop method and structure of the component for calculation of the sum of
differences squares;
• to develop method and structure of the component for calculation of scalar product.
4
The Main Results of the Research
The Requirements and Principles of Development of Hardware Neuronetworks
Basic Components. The structure of hardware neuronetworks basic components
should provide implementation of such basic operations: data pre-processing, processor
operations and calculation of transfer functions. Hardware basic components of
real-time neuronetworks should meet such requirements:
• high efﬁciency of equipment use;
• efﬁcient use of opportunities and beneﬁts of VLSI-technologies;
• small terms of development and small price;
• coordination of data input intensity with calculation intensity of basic components;
• taking into account the requirements for speciﬁc applications;
• reduction of number of interface output and interneural connections;
• real-time work.
In oder to decrease the number of interface outputs and hardware costs, it is offered that
data should come parallel, by bit sections groups (vertically), a processing should be
performed by with parallel vertical group method using multi-operand approach.
Parallel vertical group method of data processing provides that weighting coefﬁcients
Wj and input data Xj come parallel by sections from k bits, and for that they are written
as follows:
Basic Components of Neuronetworks
561

Wj ¼
X
n
i¼1
2 i1
ð
Þwji ¼
X
m
g¼1
2 g1
ð
Þk wj g1
ð
Þk þ 1
½
 þ 21wj g  1
ð
Þk þ 2
½
 þ . . . þ 2 k1
ð
Þwj g1
ð
Þk þ k
½



; ð4Þ
Xj ¼
X
n
i¼1
2ði1Þxji ¼
X
m
g¼1
2ðg1Þkðxj½ðg1Þk þ 1 þ 21xj½ðg1Þk þ 2 þ . . . þ 2ðk1Þxj½ðg1Þk þ kÞ ;
ð5Þ
where wji, xji – parameter of i-bits of weighting coefﬁcients and input data; n – bit
capacity of weighting coefﬁcients and input data, m ¼
n
k
 
, k - bits number in the group,
in which weighting coefﬁcients Wj and input data Xj are separated.
The following principles are chosen to develop basic components of hardware
real-time neuronetworks with parallel vertical group processing method:
• use of the basis of elementary arithmetic operations for implementation of algo-
rithms of performed basic neurooperations;
• spatial and time parallelization of calculation algorithms of basic neurooperations;
• implementation of algorithms of basic neurooperations calculation as a single
macrooperation;
• ensuring of regularity, modularity and wide use of standard elements;
• localization and reduction of connections number between the elements of basic
components;
• specialization and adaptation of basic components to the processing algorithms
structure;
• programmability of architecture through use of reprogrammed logic integrated
circuits.
Development of Basic Components. The main purpose of development of basic com-
ponents of hardware real-time neuronetworks is obtaining high performance, modular and
regular VLSI-structure with calculation intensity, coordinated with data input intensity.
Output information for development of highly efﬁcient basic components are:
• algorithms of basic neurooperations implementation;
• quantity and number of operands;
• intensity of receipt of input data
, where Fd, m i nк – respectively, the
frequency of data receipt, number and bit capacity of channels;
• requirements to the interface of basic components;
• calculation accuracy;
• technical and economic requirements and limitations.
Structural organization of basic components is deﬁned with the set of features, the
main of which are the following: number of operands, which are processed simulta-
neously; operating modes; the way of organization of connections between processor
elements (PE). Basic components can be devided into synchronous and asynchronous
according to the operating modes. In the last case such basic components are called
single-cycle, since input data processing is performed without intermediate memo-
rization. Speed of single-cycle basic components is deﬁned with response time of PE,
located on the longest path of data passing. Single-cycle basic components are con-
sequential in terms of implementation of data processing algorithm. It is expedient to
562
I. Tsmots et al.

use synchronous basic components, processing of which is conducted according to the
conveyor principle, for processing of intensive data streams. Conveyor basic compo-
nents are devided into steps with the buffer memory. Basic components should
implement as simple as possible operations with approximately the same execution
time to provide high speed and efﬁciency of using the equipment of ПE steps.
Recording of the results of operations fulﬁllment in the buffer memory is conducted
during timing pulses application in the conveyor. The frequency of such timing pulses
FTI is equal to:
ð6Þ
where
– time of record into buffer memory;
– time of operation fulﬁllment.
In general, the task of development of basic components of hardware real-time
neuronetworks can be formulated as follows:
• to deﬁne composition, properties and quantity of each PE type;
• to deﬁne buffer memory parameters;
• to deﬁne necessary connections between PE and exchange methods;
• to synthesize control devices;
• to evaluate main parameters of basic components.
The process of synthesis development of basic components of hardware real-time
neuronetworks can be conducted using the next steps:
• development of parallel vertical-group algorithm of basic neurooperations fulﬁll-
ment and its presentation as speciﬁc coordinated stream graph;
• synthesis of basic components structure, connections topology and synchronization
functions of change between PE;
• development of basic components interface;
• synthesis of PE structure due to given set of operations considering the element base
features and manufacturing technology.
The ﬁnal results of the ﬁrst three design stages are set of structures, adequate to
algorithm, which consist of buffer memory, limited set of PE, combined with a
switching system and provide technical requirements towards basic components.
High efﬁciency of equipment use is achieved with hardware costs minimization at
real time provision of hardware real-time neuronetworks in the basic components.
Transition from the algorithm of basic operation implementation to the basic compo-
nents structure formally presumes minimization of hardware costs
ð7Þ
Basic Components of Neuronetworks
563

providing such condition:
ð8Þ
where
- hardware costs for basic components, control block,
interface circuits, buffer memory, processor element, si – number of
of i-type, N–
number of input data, n – bit rate of input data,
– time of calculation of basic
operation accordingly.
The main ways of costs minimization of the equipment at development of basic
components of hardware real-time neuronetworks are choosing of algorithms of basic
neurooperations fulﬁllment and coordination of intensity of data receipt with calcula-
tion intensity of basic components, through change of conveyor time period
or number m and bit capacity nк of data entry channels.
In order to select a structure option of basic components of hardware real-time
neuronetworks, we use the criterion of efﬁciency of equipment use E, taking into
account the number of interface outputs, structure homogeneity, number and location
of connections, connect efﬁciency with equipment costs and evaluate the elements by
the efﬁciency. Quantitative value of efﬁciency of equipment use for basic components
is deﬁned as follows:
ð9Þ
where R - complexity of the algorithm of basic neuroperation implementation; tк -
conveyor time;
– equipment costs for the implementation of PEi, si – number of i-
type PE, k1 – coefﬁcient, taking into account homogeneity k1 = f(s), k2 – coefﬁcient,
taking into account connections regularity k2 = f(Dj), Dj - distance between commu-
nication lines, Q – number of communication lines, k3 – coefﬁcient, taking into
accountof number of communication interface outputs k3 = f(Y), Y - number of com-
munication interface outputs.
5
Methods and Structures of Basic Components
Implementation
Parallel Vertical Group Converter of Codes. Converter of codes should provide
parallel consecutive group transformation, that is forming of group bit sections. Parallel
consecutive group transformation is carried out by consecutive reception of N numbers
with further forming of bit sections from k bits for all N numbers in each cycle. Such
transformation is used for download of hardware neuronetworks component with
564
I. Tsmots et al.

parallel vertical group processing of input data. At processing of data streams in each g-
work cycle on the input of the component it is necessary to form such bit sections with
k bits for all N numbers. In order to provide processing of continuous data streams, a
parallel vertical group converter of codes, the structure of which is shown on Fig. 1,
was developed, where
– registers block,
–register, V – operation mode of
converter, Dj – input data.
Parallel vertical group converter of codes consists of two groups of register blocks, one
of which works for parallel record of input data, and another one for bitwise output of
previously accumulated data. The groups of registers work in two operating modes:
V = 0 - parallel record; V = 1 – bitwise data output. At parallel recording sequential
outputs of registers are in the third state.
6
Calculation Component of Maximum and Minimum
Numbers
Parallel vertical group method of calculation maximum Dmax i minimum Dmin num-
bers in one-dimensional Dk
f
gN
h¼1 and two-dimensional
Dkj

	N;M
h¼1;j¼1 arrays provides in
each g–cycle (g = 1,…,m where m ¼
n
k
 
, k – bits number in the group, n - bit rate
numbers), parallel receipt of N numbers with older bits before section with k bits [12].
Calculation of maximum Dmax i minimum Dmin numbers in one-dimensional array
Dk
f
gN
h¼1 by this method is based on the implementation for each r-bit section (r = 1,…,
k) of the same type basic мaкpooperations, which are performed on the basis of three
simple operations. At calculation of maximum number Dmax in one-dimensional array
Dk
f
gN
h¼1 such operations are used:
1
1
2
Nk
Рг 11
Рг 1k
1
1
m
m
1
k
n-k
n
БлРг1
1
k
11
1m
k1
km
БлРгN
1
k
Рг 11
Рг 1k
1
1
m
m
1
k
n-k
n
БлРг1
1
k
11
1m
k1
km
БлРгN
1
k
...
...
Dj
V
Fig. 1. Developed structure of parallel vertical group converter of codes
Basic Components of Neuronetworks
565

(1) forming of value of r-bit section Pr by the formula:
Pr ¼ _
N
h¼1 Drh ^ yrh;
ð10Þ
where Drh – the value of r-cycle of h-number of array, yrh – the value of h –cycle of
r-word
of
control,
the
value
of
1-word
of
control
is
equal
to
y11 = y12 = … = y1 N = 1;
(2) deﬁnition of r-cycle of maximum number Dmaxr by the expression:
when
when
ð11Þ
(3) forming of h bits (r + 1)-word of control by the formula:
when
when
when
ð12Þ
At calculation of minimum number of Dmin in one-dimensional array
Dk
f
gN
h¼1
basic macrooperation is realized on such operations:
(1) forming the value r-o of bit section Pr, which is performed by the formula:
Pr ¼ _
N
h¼1
Drh ^ yrh;
ð13Þ
where Drh – inverse value of r- cycle of h-number array, yrh – the value of h-cycle of
r-word of control, the value of 1-word of control is equal to y11 = y12 = … = y1 N = 1;
(2) deﬁnition of r-cycle minimum number Dminr by the expression:
when
when
ð14Þ
566
I. Tsmots et al.

(3) forming of h bits (r + 1)-word of control, which is performed by the expression:
when
when
when
ð15Þ
The peculiarity of the considered parallel vertical-group method of maximum
(minimum) number calculation is that in each g-work cycle k bits of maximum Dmax
(minimum Dmin) number are deﬁned.
Deﬁnition of maximum Dmax and minimum Dmin numbers in a two-dimensional
array
Dhj

	N;M
h¼1;j¼1 is founded on basic macrooperations, which are performed by the
formulas accordingly (10)–(12) and (13)–(15). Difference of deﬁnition of maximum
Dmax (minimum Dmin) number in a two-dimensional array
Dkj

	N;M
k¼1;j¼1 is that after
execution of each m cycles with one-dimensional array from (N + 1) numbers maxi-
mum (minimum) number of new j- one-dimensional arrays is used in calculation and
units record in all bits of control register is conducted. Calculation maximum (mini-
mum) number in a two-dimensional array
Dkj

	N;M
k¼1;j¼1 requires fulﬁllment of MЧm
basic operations.
The peculiarity of the parallel vertical-group method of maximum and minimum
numbers calculation of arrays numbers is:
• use of one basic macro-operation;
• ability to use parallelization and calculation conveyorization;
• ability of simultaneous processing of N- bits sections;
• time of calculation is mainly deﬁned with bits number in the group k, as well as with
bit capacity of numbers n, and not with their number N.
Structure of the component calculation of maximum and minimum numbers. Cal-
culation of the component of maximum and minimum numbers is realized on the basis
of the same PE type. Each PE mechanically conducts k basic macrooperations of
maximum and minimum numbers calculation.
Built structure of the component of maximum and minimum numbers calculation in
one-dimensional array Dk
f
gN
h¼1 with parallel vertical group method is shown on Fig. 2,
where TI – timing pulses,
– initial setting, Tг – trigger, Pг – register, Dh1 – Dhk - h-
input of the group with k bits, Dh1min – Dhkmin and Dh1max – Dhkmax Dimin – output from
the groups with k bits of maximum and minimum numbers respectively.
Number of PE, connected with the common bus of results, at simultaneous cal-
culation of maximum and minimum numbers for one-dimensional array
Dk
f
gN
k¼1 is
deﬁned according to its size. Use of common buses of the results provides paral-
lelization of the process of bit section processing, processing time of which deﬁnes
work cycle of the device. Calculation of maximum and minimum numbers with parallel
Basic Components of Neuronetworks
567

vertical group method in such device is made during the period, which is deﬁned as
follows:
ð16Þ
where
response time trigger and logical elements of the type OR, AND,
AND-NOT accordingly, k – bits number in the group.
Calculation Component of the Sum of Differences Squares. Parallel vertical group
method of the sum of differences squares calculation requires, that each operand should
be presented in the form of groups with k bits. As such the operands are recorded as
follows:
Xj ¼
X
n
i¼1
2ði1Þxji ¼
X
m
g¼1
2ðg1Þkðxj½ðg1Þk þ 1 þ 21xj½ðg1Þk þ 2 þ . . . þ 2ðk1Þxj½ðg1Þk þ kÞ; ð17Þ
where xji –value of i-cycle of j- operand; n – bit rate of operand, m - number of groups,
into which the operand is devided.
Squaring is the main operation of calculation of differences squares sum. We’ll use
a vertical algorithm for execution of such operation:
X2 ¼ ð0:01Þ ^ x1 þ 21ð0:x101Þ ^ x2 þ 22ð0:x1x201Þ
^ x3 þ . . . þ 2ðn1Þð0:x1x2. . .xn101Þ ^ xn
¼
X
n
i¼1
2ði1ÞRi;
ð18Þ
&
1
&
&
1
&
&
1
&
&
1
&
Ргh
1
k
1
k
1
k
1
k
Тг
С
D
R
Тг
С
D
R
...
...
1h
y
h
k
y
)
1
( +
1h
y
h
k
y
)
1
( +
Dh1
Dhk
ТІ
ПУ
ПЕ 1
ПЕ h
ПЕ N
D11
D1k
DN1
DNk
max
1
D
max
k
D
min
1
D
min
k
D
Fig. 2. Structure of the calculation component of maximum and minimum numbers with
parallel vertical group method
568
I. Tsmots et al.

where Ri – partial result of squaring, which is deﬁned as follows:
Ri ¼ ð0:x1x2. . .xi101Þ ^ xi:
ð19Þ
Development of the considered algorithm is forming of macropartial result of
squaring for the group with k bits RMg:
RMg ¼ Rg1 þ 21Rg2 þ . . . þ 2
k1
ð
ÞRgk ¼
X
k
r¼1
2 r1
ð
ÞRgr;
ð20Þ
where Rgr - partial result of squaring.
Algorithm of squaring, using forming of macropartial results RMg, is written as
follows:
X2 ¼
X
m
g¼1
2 g1
ð
ÞkRMg:
ð21Þ
We will calculate differences squares sum on the basis of multi-operand approach,
which consists of simultaneous processing of all operands and forming of macropartial
result of the sum of differences squares. We will perform calculation of sum of dif-
ferences squares, using parallel vertical group method, which is written as follows:
y ¼ Xe
1  Xb
1

2 þ Xe
2  Xb
2

2 þ . . . þ Xe
N  Xb
N

2¼ DX2
2 þ . . . þ DX2
N
¼ P
m
g¼1
2 g1
ð
ÞkR1Mg þ . . . þ P
m
g¼1
2 g1
ð
ÞkRNMg ¼ P
N
j¼1
P
m
g¼1
2 g1
ð
ÞkRjMg ¼ P
m
g¼1
2 g1
ð
ÞkRjmg ¼ P
N
j¼1
RjMg ¼ P
m
g¼1
2 g1
ð
ÞkpMg
; ð22Þ
where PMg – g-macropartial result of the sum of differences squares.
The main stages of parallel vertical-group method of calculation of the sum of
differences squares is:
• simultaneous consecutive group input of operands Xe
i ; Xb
i and calculation of module
DXi;
• forming of macropartial results of squaring RMg;
• forming of macropartial result of the sum of differences squares PMg through adding
of macropartial results of squaring RMg;
• obtaining the result of sum of differences squares using adding of macropartial
results of calculation PMg with the right shift on k bits.
Structure of the compinent for parallel vertical group calculation of differences
squares sum. Dependning on the method of forming and adding of macropartial results
of the sum of differences squares PMg, the following variants of implementation of the
component of differences squares sum calculation are possible:
• with sequential forming and adding of PMg;
• with parallel forming and sequential adding of PMg;
• with parallel forming and adding of PMg.
Basic Components of Neuronetworks
569

Developed structure of the component of calculation of differences squares sum
with parallel forming and sequential adding of PMg is shown on Fig. 3, where Pг –
register,
– multichannel adder, Cм – adder, PE – processor element.
The main elements of this structure are: PEj, designed for forming of macropartial
results of squaring RjMg;
, which with the help of parallel adding of RjMg forms
macropartial result of sum of differences squares PMg; Pг PMg, CмY and PгY, which
provide sequential calculation of the sum of differences squares by the formula:
Yg ¼ 2kZg1 þ PMg;
ð23Þ
where Y0 = 0.
Built structure of PEj is shown on Fig. 4, where
– deductor, Tг – trigger,
–
converter of parallel code into vertical group, DXj




 - shaper of difference module, Rrg –
shaper of partial result of squaring.
Fig. 4. Developed
structure
Rk
БСмRМg
РгY
СмY
РгРМg
ПЕ 1
ПЕ j
ПЕ 1
ПЕ N
e
jg
x
1
e
jgk
x
b
jg
x
1
b
jgk
x
РМg
R1Мg
RjМg
RNМg
Y
Fig. 3. Structure of the component of calculation of differences squares sum with parallel
forming and sequential adding
570
I. Tsmots et al.

The operands Xe
j nad Xb
j entry to the input
is carried out sequentially by the
groups with k bits, beginning with younger bits. In each
with the help of deductor
during m cycles the difference DXj is calculated and recorded in the registers Pг1,
…,Pгm. The calculated difference DXi comes to the inputs of shaper DXj




, on the
output of which we receive its module | DXj |. In the next work cycles we receive partial
results of squaring on the outputs of shapers Rrg. Forming of partial results of squaring
Rrg is realized beginning with older bits of the module | DXj | according to the formula
(19). Formed k partial results of squaring Rrg come with the right shift on (r – 1) bit
inputs of multichannel adder
, where they are added. The sum, obtained on the
output of multichannel adder
is a macropartial result of squaring RjMg.
Maкpopartial results of squaring R1Mg,…, RNMg are added with the help of multi-
channel adder БCм RMg. Received sum, which is a macropartial result of the sum of
differences squares PMg, is recorded in the register PгPMg. Adding of data from the
output of the register PгPMg to the previously accumulated amount from the register
PгY with the right shift on k bits is performed on the adder CмY in each cycle.
The feature of work of this component is time alignment of data input processes of
one array and calculation of another one. Using such combination of the sum of
differences squares calculation in this component will be made for m cycles.
Calculation Component of Scalar Product. Parallel vertical group method of cal-
culation of scalar product is based on elementary arithmetic operations, focused on
HBIC-implementation, and provides decrease of work cycles number, and calculation
time accordingly. Calculation of scalar product by this method presumes forming and
adding of partial multiplication product in accordance with the following formula:
Z ¼
X
N
j¼1
WjXj ¼
X
N
j¼1
X
m
g¼1
2 g1
ð
Þk WjXj g1
ð
Þk þ 1
½
 þ 21WjXj g1
ð
Þk þ 2
½
 þ . . . þ 2ðk1ÞWjXj g1
ð
Þk þ k
½



¼
X
N
j¼1
X
m
g¼1
2 g1
ð
ÞkPjg;
ð24Þ
where Pjg - group partial multiplication result,
Pjg ¼ WjXj g1
ð
Þk þ 1
½
 þ 21WjXj g1
ð
Þk þ 2
½
 þ . . . þ 2 k1
ð
ÞWjXj g1
ð
Þk þ k
½
.
After making all necessary changes in the formula (24), calculation of scalar
product can be written as follows:
Z ¼
X
m
g¼1
2 g1
ð
Þk X
N
j¼1
Pjg ¼
X
m
g¼1
2 g1
ð
ÞkRg;
ð25Þ
where Rg - partial scalar product, Rg ¼ P
N
j¼1
Pjg, which can be calculated both parallel
and sequential. Sequential calculation of scalar product Rg is performed by the formula:
Basic Components of Neuronetworks
571

Rg ¼
X
m
g¼1
2 g1k
ð
Þ X
N
j¼1
wj g1
ð
Þk þ 1
½
xj g1
ð
Þk þ 1
½
 þ 21wj g1
ð
Þk þ 2
½
xj g1
ð
Þk þ 2
½


þ . . . þ 2k1wj g1
ð
Þk þ k
½
xj g1
ð
Þk þ k
½
ÞÞ:
ð26Þ
It becomes evident from the formula (25), that in roder to calculate scalar product it
is necessary to perform m cycles, and each of them includes such operations:
• forming of partial multiplication product in accordance with the formula
Pj g1
ð
Þk þ r
½
 ¼ WjXjð g1
ð
Þk þ r
½
, where r = 1,…, for each j- couple of operands k;
• calculation of group partial multiplication product Pjg for j-couple of operands in
accordance with the formula Pjg ¼ P
k
r¼1
2 r1
ð
ÞWjXj g1
ð
Þk þ r
½
;
• calculation of partial result of scalar product Rg using adding of гpyпoвиx partial
multiplication product Pjg in accordance with the formula Rg ¼ P
N
j¼1
Pjg;
• adding of partial results of scalar product Rg according to the expression
Zg ¼ 2kZg1 þ Rg, where Z0 = 0.
The structure of the component parallel vertical group calculation of scalar
product. Depending on the ways of operands coming, number of used buses for
operands entering, forming and adding of partial ﬁnal result of scalar product Rg,
conducting of entering processes and calculation of scalar product in time, the fol-
lowing variants of implementation of the component of scalar product calculation are
possible:
• with parallel vertical group input of data by younger (older) bits forward;
• with individual or multiplexed buses for input of data Xj and weighting coefﬁcients Wj;
• with parallel or sequential forming of partial result of scalar product;
• with parallel or sequential adding of м partial results scalar product;
• with separation or combination of input processes and calculation of scalar product.
Rk
ФЧР
Рг Z
См Z
РгRg
R g
Z
1
1 /
j
j
x
w
jk
jk
x
w
/
Fig. 5. The structure of the component with parallel vertical group calculation of scalar product
572
I. Tsmots et al.

The most efﬁcient structure for VLSI-implementation is the following: with data
input by younger bits forward; use of multiplexed buses for operands entry; parallel
forming of partial result of scalar product Rg; sequential adding of partial results of
scalar product Rg; combining of input processes and scalar product calculation. Taкa
Structure is shown on Fig. 5, where ФЧP – shaper of partial results of scalar product.
The main element of this structure is
, which can perform both parallel and
sequential forming of partial result of scalar product Rg. The structure of
with
parallel forming of partial result of scalar product Rg is shown on the Fig. 6, where
wj1=xj1. . .wjk=xjk multiplexed single-bit information inputs; Pг – register,
– con-
verter of bits sequential groups into parallel code; kCм, NCм – N i k input adders.
Parallel vertical group calculation of scalar product in this component can be
devided into two stages, and each of them is performed for m cycles.
k bits of multiplier Wj enter to the j- information entry on the ﬁrst stage in each g-
cycle, beginning with younger bits. Accumulation of sequential groups bits of multi-
plied Wj and their transformation into parallel code is performed in
.
On the second stage in each g in the work cycle in ПEj for the group of multiplier
bits Xjg1Xjg2. . .Xjgk k partial multiplication product in accordance with the formula
Pjgr ¼ WjXjgr is formed. Formed partial multiplication products come to the input of
k-input adder, at that r-
(r = 1,…,k) partial multiplication product WjXjgr is shifted
Fig. 6. The structure of the shaper of partial results of scalar product
Basic Components of Neuronetworks
573

with regard to (r-1)- partial multiplication product WjXjg r1
ð
Þ in one bit to the right.
Using adding of partial multiplication product on the output of k-input adder, we obtain
a group with partial multiplication result Pjg, which is recorded in the register PгPjg.
Group partial multiplication products Pjg from outputs
come to the inputs of adder
NCм, where they are added. On the output of adder NCм we obtain g-partial result of
scalar product Rg, which is recorded in the register PгRg. Adding of partial results of
scalar product Rg, according to the expression Zg ¼ 2kZg1 þ Rg, where Z0 = 0, is
realized on the adder CмZ.
Developed component for scalar product calculation works by the conveyor prin-
ciple and is oriented for processing of continuous data streams. Conveyor work cycle of
such device is deﬁned as follows:
where
– response time of register, tNCм – time of N numbers adding. Calculation of
scalar product is realized for m conveyor cycles.
7
Conclusions
1. Neuroalgorithms and structures of neuronetworks were analyzed, the followwing
basic components were deﬁned: converter of codes, calculation of maximum and
minimum numbers, calculation of the sum of differences squares, calculation of
scalar products.
2. It was offered to conduct implementation of basic components according to such
principles: modularity, coordination of intensity of data receipt with computing
power of the components, conveyorization and spatial parallelism, localization and
simplifying of the links between the elements, specialization and adaptation of
hardware and software to the requirements of speciﬁc application.
3. Use of parallel vertical group method of data processing and basis of elementary
arithmetic operations for hardware implementation of neuronetworks basic opera-
tions provides performance increase, decrease of hardware costs and their orien-
tation for VLSI implementation.
4. Parallel vertical group converter of codes, which due to time alignment of
sequential recording processes of bits groups and forming of bit section groups for
data array provides faster performance, was developed, using dual memory.
5. Parallel vertical group method of calculation of maximum and minimum numbers in
arrays, which due to parallel section processing from the group of bits of all
numbers provides time reduction of calculation, was developed.
6. Parallel vertical group method of calculation of the sum of differences squares,
which is founded on the calculation operations of differences modules, forming of
partial results of squaring and group adding, was developed. This method, due to
parallel processing of section from the group of bits of all numbers, provides faster
performance.
574
I. Tsmots et al.

7. Parallel vertical group method for scalar product calculation, which in comparison
with wellknown, due to increasing of bits channels of multipliers input and number
analysis of bits for forming partial multiplication result, provides time reduction for
calculation of scalar product, was developed.
References
1. Mohamad, H.: Hassoun Fundamentals of Artiﬁcial Neural Networks, p. 511. MIT Press,
Cambridge (1995)
2. Teich, T., Roessler, F., Kretz, D., Frank, S.: Design of a prototype neural network for smart
homes and energy efﬁciency. In: Proceedings of 24th DAAAM International Symposium on
Intelligent Manufacturing and Automation, pp. 603–608. Zwickau, Germany (2013)
3. Цюй Дyньюэ. Упpaвлeниe мoбильным poбoтoм нa ocнoвe нeчeткиx мoдeлeй / Цюй
Дyньюэ // Coвpeмeнныe пpoблeмы нayки и oбpaзoвaния. – №6. – C, pp. 115–121 (2012)
4. Matviichuk, K., Teslyuk, V., Teslyuk, T.: Vision system model for mobile robotic systems.
In: Proceeding of the XIIh International Conference “Perspective Technologies and Methods
in MEMS Design”, MEMSTECH 2016, 20–24 April 2016, pp. 104–106. Polyana, Lviv,
Ukraine (2016)
5. Cagnoni, S., Coppini, G., Rucci, M., et al.: Neural network segmentation of magnetic
resonance spin echo images of the brain. J. Biomed. Eng. 15(5), 355–362 (1993)
6. Fujita, H., Katafuchi, T., Uehara, T., et al.: Application of artiﬁcial neural network to
computer-aided diagnosis of coronary artery disease in myocardial SPECT bull’s-eye
images. J. Nucl. Med. 33(2), 272–276 (1992)
7. Astion, M.L., Wener, M.H., Thomas, R.G., Hunder, G.G., Bloch, D.A.: Application of
neural networks to the classiﬁcation of giant cell arteritis. Arthritis Reum. 37(5), 760–770
(1994)
8. Peleshko, D., Ivanov, Y., Sharov, B., Izonin, I., Borzov, Y.: Design and implementation of
visitors queue density analysis and registration method for retail videosurveillance purposes.
In: 2016 IEEE First International Conference on Data Stream Mining & Processing (DSMP),
pp. 159–162. Lviv, Ukraine (2016)
9. Badlani, A., Bhano, S.: Smart home system design based on artiﬁcial neural networks. In:
Proceedings of the World Congress on Engineering and Computer Science 2011, pp. 106–
111. San Francisco, USA, 19–21 October 2011
10. Pukach, A.I., Teslyuk, V.M., Tkachenko, R.O., Ivantsiv, R.-A.D.: Implementation of neural
networks for fuzzy and semistructured data. In: Proceedings of 11-th International
Conference on the Experience of Designing and Application of CAD Systems in
Microelectronics, CADSM 2011, pp. 350–352. Lviv, Polyana, Ukraine, 23–25 February
2011
11. Ocoвcкий C. Heйpoнныe ceти для oбpaбoтки инфopмaции / Пep. c пoльcкoгo. – M.:
Финaнcы и cтaтиcтикa, 344 c (2009)
12. Пaтeнт №101922 Укpaїнa, G06F 7/38. Пpиcтpiй для oбчиcлeння cкaляpнoгo дoбyткy/
Цмoць I.Г., Cкopoxoдa O.B., Tecлюк B.M. Бюл. №9 (2013)
13. Пaтeнт №110187 Укpaїнa, G06F 7/38. Пpиcтpiй для визнaчeння мaкcимaльнoгo чиcлa з
гpyпи чиceл/ Цмoць I.Г., Cкopoxoдa O.B., Meдикoвcький M.O., Aнтoнiв B.Я. Бюл. №22
(2015)
Basic Components of Neuronetworks
575

14. Цмoць I.Г. Moдифiкoвaний мeтoд тa HBIC - cтpyктypa пpиcтpoю гpyпoвoгo
пiдcyмoвyвaння для нeйpoeлeмeнтa. / I.Г. Цмoць, O.B. Cкopoxoдa, Б.I. Бaлич // Bicник
HУ « Львiвcькa пoлiтexнiкa » – Львiв. – №732: « Кoмп’ютepнi нayки тa iнфopмaцiйнi
тexнoлoгiї » . – C, pp. 51–57 (2012)
15. Tsmots, I., Skorokhoda, O., Rabyk, V.: Structure software model of a parallel-vertical
multi-input adder for FPGA implementation. In: Proceedings of XIth International Scientiﬁc
and Technical Conference CSIT 2016, pp. 158–160. Lviv, Ukraine, 6–10 September 2016
16. Цмoць I.Г., Cкopoxoдa O.B., Iгнaтєв I.B. Cинтeз кoмпoнeнтiв пapaлeльниx нeйpoмepeж
вepтикaльнo-гpyпoвoгo типy. Bicник HУ “Львiвcькa пoлiтexнiкa” “Кoмп’ютepнi нayки
тa iнфopмaцiйнi тexнoлoгiї” №826 Львiв. C. pp. 69–79 (2015)
17. Bodyanskiy, Y., Dolotov, A., Vynokurova, O.: Evolving spiking wavelet-neuro-fuzzy
self-learning system. Appl. Soft Comput. 14, 252–258 (2014)
18. Bodyanskiy, Y., Vynokurova, O., Pliss, I., Peleshko, D., Rashkevych, Y.: Hybrid
generalized additive wavelet-neuro-fuzzy-system and its adaptive learning. In: Zamojski,
W., Mazurkiewicz, J., Sugier, J., Walkowiak, T., Kacprzyk, J. (eds.) Dependability
Engineering and Complex Systems: Proceedings of the Eleventh International Conference
on Dependability and Complex Systems DepCoS-RELCOMEX. 27 June 2016–1 July 2016,
pp. 51–61. Brunow, Poland (2016)
19. Haar, A.: Zur Theorie der orthogonalen Funktionensysteme. Math. Ann. 69(3), 331–371
(1910)
20. Daubechies, I.: Ten Lectures on Wavelets, SIAM, p. 194 (1992)
21. Galushkin, A.I.: Neurocomputers. Book 3 – M.: IPRZR, p. 528 (2000)
576
I. Tsmots et al.

Recommendation Systems as an Information
and Technology Tool for Virtual Research Teams
Nataliia Veretennikova
(✉) and Nataliia Kunanets
(✉)
Information Systems and Networks Department,
Lviv Polytechnic National University, Lviv, Ukraine
Nataver19@gmail.com, nek.lviv@gmail.com
Abstract. The paper analyzes options of concept interpretation of “e-Science”
and its author’s deﬁnition as a systemically integrated complex of computer
information telecommunication and social and communicative technologies,
which ensure the fulﬁllment of functions and the solution of the actual tasks of
science in the information society. The basic principles of the modern concept of
a “virtual scientiﬁc team” are considered. It is analyzed the creation problems of
eﬀective information and technological tools for the comfortable and qualitative
implementation of information and communication processes in the social and
communication environments of virtual scientiﬁc teams that conduct research on
the platform of e-Science. The main tendencies of using recommender systems
are highlighted for an eﬀective implementation of information and communica‐
tion processes in virtual scientiﬁc teams. A description of the functional capabil‐
ities and the basic algorithms of work of the recommender system “Information
assistant of a scientist” is presented, which supports the implementation of
processes for eﬀective information support of researchers in the virtual scientiﬁc
teams.
Keywords: E-science · Information support · Virtual research team ·
Consolidated resources · Recommender system
1
Іntroduction
In the information society, the development of e-science involves the development of a
large-scale distributed computing, communication and information infrastructure,
middleware and the formation of virtual research teams. This concept is related to the
emergence of inter-organizational and interdisciplinary research, based on innovative
approaches in the dissemination, use and analysis of data.
A successful solution to such problems requires the creation of eﬀective tools for
the implementation of information and communication processes among such creative
scientiﬁc communities.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_40

2
Analysis of Recent Researches and Publications
The methodological principles of the construction of e-science system is devoted the
work by J. Taylor [1], the questions of planning scientiﬁc tasks in the ﬁeld of e-science
are researched in the work of E. Deelman, D. Gannon, M. Shields [2], the problems of
management of scientiﬁc projects are analyzed by D. Spencer, A Zimmerman,
D. Abramson [3], A. Warr and others [4]. The combination of WEB- and GRID-tech‐
nologies in the formation of the infrastructure of e-science is described in the works of
M.Z. Zgurovskyi and A.I. Petrenko [5].
The problems of developing a new innovative scientiﬁc and technological area,
which is e-science, are discussed in the articles “Communication and Collaboration in
e-Science Projects” [6], “Supporting Scientiﬁc Collaboration: Methods, Tools and
Concepts” [7] and “Collaborative e-Science Experiments and Scientiﬁc Workﬂows” [8].
D. Sonnenwald analyzes the concepts of transformations of organizational structures in
the form of virtual organizations [9] and the peculiarities of information exchange tech‐
nologies in them [10]. A number of publications analyze the ways to establish commu‐
nication in the environment of participants in a particular scientiﬁc project [11]. At the
same time, we should note that there are practically no works in which the tasks of
developing a comprehensive technological tool for providing information and commu‐
nication processes of virtual research teams are analyzed.
The problem of formation of consolidated information resources that is relevant to
the proﬁle of researches of virtual creative teams remains not completely examined, as
well as their technological support with the use of conceptual approaches of data ware‐
houses, data spaces and large data. A separate actual informational and technological
problem in the ﬁeld of e-science is the lack of tools for providing the user with person‐
alized advice taking into account the personal information needs of each member of the
virtual research team in the context of implementing a joint project.
3
E-Science and Virtual Scientiﬁc Teams
The actual scientiﬁc research in many technologically advanced countries undergoes
dramatic changes in connection with the introduction of an innovative approach called
e-science. This term was ﬁrst used by John Taylor, a chairman of the UK Department
of Education and Technology in 1999. According to Taylor, the term e-Science means
global collaboration in key areas of science and innovative infrastructure development
that will enable such collaboration [12]. It should be noted that the researcher did not
emphasize the need for the development of information and communication infrastruc‐
ture, which became especially relevant. M. Riedel deﬁnes e-Science as a new area of
research that is based on collaboration in key areas of science and uses innovative infra‐
structure to expand the capacity of scientiﬁc calculations [13]. A. Bosin, N. Dessi, B.
Pes understand e-Science as a certain type of large-scale interaction within the typical
scenario of an experiment that requires multiple use of data (preliminary processing,
calculations, post processing and data storing) [14]. The above deﬁnitions are based on
the assertions that many of the complex and topical scientiﬁc problems of our time are
578
N. Veretennikova and N. Kunanets

solved by large groups of globally distributed researchers who need an access to modern
computer infrastructure for the complete, timely and qualitative support them by neces‐
sary data, computing and network resources. Today, the key areas of the use of e-Science
concept, according to leading experts, are physics, astronomy, biology, medicine,
computer science, etc.
In its turn, a number of domestic scientists deﬁne e-Science as a system-integrated
complex of computer information, telecommunication and social and communicative
technologies, which ensure the fulﬁllment of functions and the solution of the actual
tasks of science in the information society [15].
There are several deﬁnitions of the concept of a virtual scientiﬁc team, which were
formed as a result of durable professional discussions [16, 17]:
(1) The virtual team is a temporarily formed group of representatives from various
scientiﬁc institutions that through the systematic use of modern information and
communication technologies virtually unite for joint research and implementation
of complex scientiﬁc projects. Typically, they integrate vertically in order to
combine the key competencies of individual participants in such virtual geograph‐
ically distributed team and organize work while solving problems as a single organ‐
izational research unit.
(2) Virtual team is an identiﬁed group of people or organizations that, based on the
widespread use of information and communication technologies, integrate into joint
business or research.
The virtual team usually creates a separate virtual organization to achieve a certain
goal. A virtual organization is a continuously evolving entity, redeﬁnes the direction of
its work to achieve the practical goals of a particular business, combining geographically
distant employees [16].
The main task of a virtual team (organization) is to provide system-integrated,
purposeful management of its functioning, which is aimed at achieving the goal. The
directions of activity of virtual organizations can be the most diverse: production,
marketing, research and development, etc. In this paper, a virtual organization is consid‐
ered that deals with scientiﬁc research, and it requires for organizing its work to create
a qualitative social and communicative environment based on information, communi‐
cation and computer resources that are geographically distributed and have a diverse
organizational and administrative subordination.
One of the essential components of the successful implementation of projects based
on the e-Science infrastructure and the achievement of the set goals in the integrated
scientiﬁc projects is an eﬀective management of e-science teams and the quality imple‐
mentation of communication processes in the research teams. The task of the leader of
the e-Science project is to support synchronous work of a creative team, which is to
provide, develop, implement and use large-scale technological systems and relevant
infrastructures, which include numerous teams of highly specialized researchers, long-
term cooperation in a dynamic environment [3], and ensure an eﬀective implementation
of large-scale and correct information and communication processes.
The daily interaction of all participants in large, distributed multidisciplinary projects
implemented on the e-Science platform faces with a number of problems, the main
Recommendation Systems as an Information and Technology Tool
579

reason is the lack of eﬀective means and tools that would ensure communication in the
team and quick access to geographically distributed expensive information, communi‐
cation and computer resources. One of the possible tools that would implement this
feature is a recommender system developed by the authors.
4
Information Assistant of a Scientist
4.1
Recommender Systems
Recommender system is an intelligent information system to form recommendations for
the sequence and the list of possible user actions in the process of solving a particular
problem [18].
These systems provide a solution to a number of problems such as:
– ensuring quick access to information resources and information and analytical serv‐
ices;
– support and formation of thematic databases.
The main task of recommender system is to provide users with personalized infor‐
mation online products or online recommendation services from the growing informa‐
tion ﬂow and the improvement of data management. The researchers acknowledge that
the recommender systems are inquiring in the ﬁelds of business, administration, educa‐
tion and others. This is conﬁrmed by a number of recent successful recommender
systems that are used in a wide range of real-world tasks.
“Information assistant of a scientist” [19], that belongs to the class of recommender
systems, allows participants in the virtual scientiﬁc team without resorting to additional
programming to perform the following tasks: editing the page content, including add or
delete graphics; adding new pages; changing the structure of the document and various
data; setting up registration forms; managing the surveys, polls and forums; statistics of
visits; distributing the human resource management among users. A system with such
functional capabilities is best suited for providing information support of a virtual crea‐
tive team.
The main functionality of the recommender system can be formulated as the
following:
– to create user proﬁle in the database;
– to ﬁll the consolidated database with documents that are relevant to the subject area
of a project;
– to form metadata and abstracts;
– to search documents in the database according to diﬀerent criteria;
– to generate recommendations according to the rating of information needs.
4.2
Consolidated Information Resources of the Scientiﬁc Project
The ﬁrst step in the functioning of a recommender system of a virtual creative team is
the formation of a problem-oriented consolidated information resource. Members of
580
N. Veretennikova and N. Kunanets

virtual scientiﬁc teams need quick information obtaining in a convenient form of
submission and place where the researcher is staying at the moment. This functionality
can be provided by ensuring each researcher with on-line access to large amounts of
data that must be properly processed in advance, which is achieved through the formation
of consolidated information resources.
According to the deﬁnition, the concept of “consolidated information” is interpreted
as received from diﬀerent sources and systematically integrated multi-type information
resources, which together have features of completeness, integrity, consistency and
constitute an adequate information model of the problem area for its analysis, processing
and eﬀective use in the decision-making processes [20]. The formation of a consolidated
information resource for a virtual research team is the basis for eﬀective research. The
formation of a consolidated information resource has several stages: the search and data
collection; preliminary processing and structuring of data (data transformation into
information); information analysis and synthesis – transforming it into knowledge;
consolidation of information resources; creation of a series of information products to
meet the information needs of potential consumers.
The information arrays proposed to a user-scientist should be accompanied by a
system of user search and provide an interactive study of ontologies to facilitate reﬁne‐
ment of search queries.
Storing the required information, including scientiﬁc articles on relevant topics
conducted by a system administrator or registered users (Fig. 1). Each user can ﬁll up
the database with publications and sources that are relevant to this subject area. The
eﬃciency of this system is determined by saving not own materials but their attributes.
Fig. 1. List of articles
Recommendation Systems as an Information and Technology Tool
581

The system provides the formation of a database that stores document metadata: title,
authors, abstract, keywords, and hyperlinks to the full text document in free access. It
ensures minimization of the resources needed for the system work, and enables the
avoidance of misunderstandings in reference to copyright represented in search results
of articles. It ensures minimization of the resources required for the system work, and
enables the avoidance of potential misunderstandings in reference to copyright in the
articles included in the lists generated by the recommender system. It facilitates the
processing of multilingual sources of use of diverse dictionaries.
In addition to the database, which stores articles that are relevant to the users queries,
the system contains an interpretative (in Ukrainian and English) dictionary of the terms
in the e-science ﬁeld, the deﬁnitions of them are automatically provided to the user when
the term is included in the keyword ﬁeld to search. Users are also given the opportunity
to supplement the system with new terms with author’s interpretation.
4.3
Formation of User Proﬁles
The next step in the functioning of the recommender system involves the formation of
user proﬁles.
The rapid growth of volumes and speed of formation of information ﬂows set new
requirements for technological instruments that provide the implementation of infor‐
mation communication among members of virtual scientiﬁc teams. The wide techno‐
logical capabilities of the worldwide web along with the implementation of functions
for the real transformation of information resources into a digital form, the rapid devel‐
opment of intellectual capabilities of search engines, aﬀect in general the essential nature
of the information needs of scientists and the level of technology for their satisfaction.
In the recommender systems, the member of a virtual team will register by entering an
email address and a convenient password (Fig. 2). The registration procedure involves
submitting information about the user that will be used during generating one or another
recommendation. One of the key elements of the registration procedure is to identify
Fig. 2. Interface for user registration
582
N. Veretennikova and N. Kunanets

the scope of the interests of a scientist and the subject area of his research, the system
of concepts and the relevant keywords. This ensures to avoid the problem of “cold start”
by a recommender system, as it guarantees at least one rating of the document for the
formation of system recommendations the rated lists [21]. In this case, the authors take
into account the possibility of data processing in situations that are referred to as the
appearance of an “unusual user”. Its essence is based on the user inherent of interdisci‐
plinary information needs that do not allow unambiguously classifying his needs in
generating relevant recommendations [22].
4.4
Generation of Recommendations
The next step is to generate recommendations, which experts often call the step of simi‐
larity determination.
The system proceeds in its actions on the similarity (distinction) of the information
needs of users and the consolidated information resources available in the database of
metadata documents. In the system, they diﬀer in the proﬁles of information needs of
users. Search of information resources at the user request involves paralleling the process
of searching for metadata and full-text documents. The process for generating recom‐
mendations for user queries is shown in Fig. 3. The ranking and prioritization of the
generation of the information presentation sequence on a user’s request is determined
by the data contained in the user’s information proﬁle. In this case, the search processes
in the database and the generation of recommendations do not aﬀect the procedures for
updating the database.
The recommender system developed by the authors ensures the implementation of
the function of information ﬁltering and building ratings of information objects oﬀered
to the user. The system generates a database containing one-way metadata, obtained
from some members of the research team and forms recommendations on its basis to
other members of the team using the so-called graph of their interests. Unlike search
algorithms, the recommender system generates personalized metadata lists that are rele‐
vant to information requests.
The recommender system “Information assistant of a scientist” provides collabora‐
tive ﬁltration, in particular in accordance with the classical model of user-item, based
on the user’s behavior, embodied in his information proﬁle.
The search follows the key words in the abstract, the article title and the article text
presented in open sources and based on the database of “Information Assistant of a
Scientist”. Search results are displayed in the program window. The recommendations
generated by the system are sent to the email address of registered user.
Recommendation Systems as an Information and Technology Tool
583

4.5
The Main Characteristics of the System
It is used an object oriented programming language C # to develop an “Information
assistant of a scientist”. It is chosen an integrated development environment of software
systems – MS Visual Studio to implement the software solution, as this product allows
through the use of web applications with a graphical interface to create the convenient
user interface. The software system is developed using three-tier client-server architec‐
ture (Fig. 4). It is used the technology WCF to implement the service part. When
designing the user interface technology, it is used Windows Forms. To work with the
databases, it is used MS SQL Server and language T-SQL.
Fig. 3. UML diagram of guidance from recommendation generation
584
N. Veretennikova and N. Kunanets

Fig. 4. System architecture
The developed software is designed for eﬃcient search and information storage for
the purpose of information support of members in the virtual creative team that conducts
research on the platform of e-science.
The main advantages of “information assistant of a scientist” are:
• simplicity and convenience of use;
• system specialization, focused on storage and information retrieval for members of
virtual creative team;
• speed, which is provided by search using attributes described above;
• avoiding the misunderstanding of copyright by preserving only links to documents
in the system;
• simple and accessible interface, aimed at users with diﬀerent levels of technological
skills;
• the possibility of using by a wide range of users maintaining a number of dictionaries;
• improving performance and reliability through the development of common infra‐
structure that is provided by technology WCF.
5
Conclusion
The paper presents the main problem tasks concerning the information communications
of the participants of scientiﬁc virtual teams that conduct research on the platform of e-
science. An analysis of the use of recommender systems in the formation of information
and communication processes in virtual creative teams is carried out. The main
approaches to the formation of recommendations are investigated as well as the use of
the collaborative ﬁltration method for the generation of recommendations by the intel‐
lectual system. It is developed the recommender system “Information Assistant of a
Scientist” which implements processes of eﬀective information support of members in
Recommendation Systems as an Information and Technology Tool
585

virtual scientiﬁc teams. The system functionality is analyzed, the algorithms of its work
and the processes of recommendation generation are given. The recommender system
“Information Assistant of a Scientist” is implemented in the social and communicative
environment of an active virtual research laboratory.
References
1. Taylor, I.J., Deelman, E., Gannon, D.B., Shields, M.: Workﬂows for e-Science, p. 526.
Springer-Verlag, London (2007)
2. Deelman, E.: Pegasus: a framework for mapping complex scientiﬁc workﬂows onto
distributed systems. Sci. Program. J. 13(3), 219–237 (2005)
3. Spencer, D., Zimmerman, A., Abramson, D.: Special theme: project management in e-
science: challenges and opportunities. Comput. Support. Coop. Work 20(3), 155–163 (2011)
4. Warr, A.: Project Management in e-Science. A report from the ‘Embedding e-Science
Applications: Designing and Managing for Usability’ project (EPSRC Grant No: EP/
D049733/1). https://www.oerc.ox.ac.uk/sites/default/ﬁles/uploads/ProjectFiles/FLESSR/Hi
PerDNO/embedding/Project%20Management%20Report.pdf
5. Zhurovskii, M., Petrenko, A.: E-science towards a semantic grid. Part 1: Combining WEB
and GRID technologies. System research and information technologies, no. 1, pp. 26–38
(2010)
6. Darch, P., Turilli, M., Lloyd, S., Jirotka, M., de la Flor, G.: Communication and Collaboration
in e-Science Projects. https://www.oerc.ox.ac.uk/sites/default/ﬁles/uploads/ProjectFiles/
FLESSR/HiPerDNO/Comm_-_collaboration%2030%20June.pdf
7. Jirotka, M., Lee, C.P., Olson, G.M.: Supporting scientiﬁc collaboration: methods, tools and
concepts. Comput. Support. Coop. Work 22(4–6), 667–715 (2013)
8. Belloum, A.: Collaborative e-science experiments and scientiﬁc workﬂows. IEEE Internet
Comput. 15(4), 39–47 (2011)
9. Sonnenwald, D.H., Whitton, M.C., Maglaughlin, K.L.: Evaluating a scientiﬁc collaboratory:
results of a controlled experiment. ACM Trans. Comput. Human Interact. (TOCHI) 10(2),
150–176 (2003)
10. Sonnenwald, D.H., Whitton, M.C., Maglaughlin, K.L.: Designing to support situation
awareness across distances: an example from a scientiﬁc collaboratory. Inf. Proc. Manag.
40(6), 989–1011 (2004)
11. Darch, P.: Shared Understandings in e-Science Projects. Technical report, Oxford e-Research
Centre, Oxford University. https://www.oerc.ox.ac.uk/sites/default/ﬁles/uploads/Project
Files/FLESSR/HiPerDNO/embedding/Shared_Understanding%2030%20June.pdf
12. Deﬁning e-Science. http://www.nesc.ac.uk/nesc/deﬁne.html
13. Riedel, M.: Classiﬁcation of diﬀerent approaches for e-science applications in next generation
computing infrastructures. In: Proceedings of the e-Science Conference, Indianapolis,
Indiana, USA (2008)
14. Bosin, A., Dessì, N., Pes, B.: Extending the SOA paradigm to e-Science environments. Future
Gener. Comput. Syst. 27, 20–31 (2011)
15. Veretennikova, N., Pasichnyk, V., Kunanets, N., Gats, B.: E-Science: new paradigms, system
integration and scientiﬁc research organization. In: Computer Science and Information
Technologies: Proceedings of the Xth International Scientiﬁc and technical Conference CSIT,
pp. 76–81. Lviv Polytechnic Publishing House, Lviv (2015)
586
N. Veretennikova and N. Kunanets

16. Serrano-Guerrero, J., Herrera-Viedma, E., Olivas, J.A., Cerezo, A., Romero, F.P.: A google
wave-based fuzzy recommender system to disseminate information in university digital
libraries 2.0. Inf. Sci. 181, 1503–1516 (2011)
17. Creamer, A.T., Morales, M.E., Kafel, D., Crespo, J., Martin, E.R.: A sample of research data
curation and management courses. J. eSci. Librarianship (2012). http://escholarship.
umassmed.edu/jeslib/vol1/iss2/4/
18. Ricci, F., Rokach, L., Shapira, B., Kantor, P.B.: Recommender Systems Handbook, p. 875.
Springer, Boston (2011)
19. Certiﬁcate of registration of copyright № 68261. Computer program “Information Assistant
of a scientist” (in Ukrainian)
20. Kunanets, N.E., Pasichnyk, V.V.: Introduction to the specialty: “Consolidated Information”,
p. 196. Lviv (2010)
21. Schein, A.I., Popescul, A., Ungar, L.H., Pennock, D.M.: Methods and metrics for cold-start
recommendations. In: Proceedings of the 25-th Annual International ACM SIGIR Conference
on Research and Development in Information Retrieval, pp. 253–260. ACM Press, New York,
Finland (2002)
22. Vozalis, E., Margaritis, K.G.: Analysis of Recommender Systems (2003). http://lsa-svd-
applicationfor-analysis.googlecode.com/svnhistry/r72/trunk/LSA/Other/LsaToRead/hercma
2003.pdf
Recommendation Systems as an Information and Technology Tool
587

Analytical Model for Availability Assessment of IoT
Service Data Transmission Subsystem
Bogdan Volochiy
(✉), Vitaliy Yakovyna
(✉), and Oleksandr Mulyak
(✉)
Lviv Polytechnic National University, Lviv, Ukraine
bvolochiy@ukr.net, vitaliy.s.yakovyna@lpnu.ua,
mulyak.oleksandr@gmail.com
Abstract. This paper discusses the role of safety, availability, and dependability
of Internet of Things (IoT) data services dedicated to the monitoring and control
of objects in a physical world. These services are designed to be available to
devices and users on request at any time and at any location. The Internet of Things
diﬀers from today’s global Internet in a number of ways. For instance, the
networks are typically unmanaged, most of IoT services are safety critical, the
link layers are optimized for low power usage, and most nodes have to be imple‐
mentable in a lightweight manner.
The design of IoT data services should take into account that this type of
system has huge scalability (hundreds, thousands of clients). On the one hand this
leads to signiﬁcant ﬁnancial costs for maintenance the IoT infrastructure, on the
other hand, is necessary to provide the appropriated availability and safety level.
In addition, today most of IoT services are operated in a critical system such as
oil and gas industry, smart cities, medicine, the ﬁnancial sector. According to this
point an actual question of availability and safety estimations IoT services
appears, because the fault of IoT system part can lead to huge ﬁnancial losses or
to observed objects death.
In this paper, the model of IoT services as queue network is proposed that
allows estimating the availability and safety of these systems.
Keywords: Internet of Things (IoT) · Queue networks · Discrete-continuous
stochastic system · Reliability behavior · Availability · Safety
1
Introduction
Nowadays the developments of IoT services [1–6] are a part of automotive, medical,
smart cities, energy and other critical systems. One of the main tasks is to provide
requirements of reliability, availability and functional safety. Thus the two types of
possible risks related to the assessment of risk, and to ensure their safety and security.
The next wave in the era of computing will be outside the realm of the traditional
desktop. In the Internet of Things (IoT) paradigm, many of the objects that surround us
will be on the network in one form or another. Generally speaking, IoT refers to the
networked interconnection of everyday objects, which are often equipped with ubiqui‐
tous intelligence. The term Internet of Things was ﬁrst coined by Kevin Ashton in 1999
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_41

in the context of supply chain management [7]. However, in the past decade, the deﬁ‐
nition has been more inclusive covering a wide range of applications like healthcare,
utilities, transport, etc. [8]. Although the deﬁnition of ‘Things’ has changed as tech‐
nology evolved, the main goal of making a computer sense information without the aid
of human intervention remains the same. IoT increases the ubiquity of the Internet by
integrating every object for interaction via embedded systems, which leads to a highly
distributed network of devices communicating with human beings as well as other
devices. Thanks to rapid advances in underlying technologies, IoT is opening tremen‐
dous opportunities for a large number of novel applications that promise to improve the
quality of our lives. In recent years, IoT has gained much attention from researchers and
practitioners from around the world [9, 10]. Actually, many challenging issues still need
to be addressed and both technological, as well as social knots, have to be untied before
the IoT idea being widely accepted. Central issues are making a full interoperability of
interconnected devices possible, providing them with an always higher degree of smart‐
ness by enabling their adaptation and autonomous behavior, while guaranteeing trust,
privacy, and security [10].
The goal of this paper is to suggest a technique to develop a Markovian chain for
critical IoT system using the proposed formal procedure and tool. The main idea is to
decrease risks of errors during development of Markovian chain (MC) for systems with
very large (tens and hundreds) number of states. We propose a special notation which
allows supporting development chain step by step and designing ﬁnal MC using software
tools. The paper is structured in the following way. The aim of this research is calculating
the availability function of critical IoT system. To achieve this goal we propose a newly
designed reliability model of critical IoT system. As an example, a critical IoT system
is researched (Fig. 1).
Fig. 1. Typical structure of IoT service with n-clients
Analytical Model for Availability Assessment
589

2
Typical IoT Systems and Their Usage
The Internet of Things is a concept in which the virtual world of information technology
integrates seamlessly with the real world of things. The real world becomes more acces‐
sible through computers and networked devices in business as well as everyday
scenarios. With access to ﬁne-grained information, management can start to move freely
from macro to micro levels and will be able to measure, plan and act accordingly.
However, the Internet of Things is more than a business tool for managing business
processes more eﬃciently and more eﬀectively – it will also enable a more convenient
way of life [11]. Last years the IoT systems market is greatly growing and used in many
sectors of people life.
IoT for manufacturing: Predictive maintenance brings Sandvik to the cutting edge of
digital manufacturing [12], Predictive analytics help optimize Rolls-Royce airplane
engine performance [13], IoT solutions fuel oil and gas industry success at Rockwell
Automation [14], Jabil uses Microsoft IoT to create a digital, intelligent, and predictive
factory [15].
IoT for smart cities: Johnson Controls creates connected buildings for a greener planet
[16]; BCA partners Microsoft to leverage IoT, data analytics and the cloud for next-
generation Green Mark buildings [17]; Ecolab solves global water challenges with the
help of Microsoft IoT [18].
IoT for transportation: Fathom’s IoT-enabled Weather Cloud enhances driver safety
during inclement weather [19]; Revolutionizing air-traﬃc control at NAV CANADA
using IoT [20].
IoT for retail: Coop Italia develops the supermarket of the future with Microsoft IoT
[21]; Mondelez builds an intelligent, interactive snack food experience [22]; Hershey
creates IoT solutions for sweeter success [23].
IoT for health care: Weka builds a smart fridge to help save lives [24]; Advancing
hospital hand hygiene practices through IoT solutions [25]; Liebherr unveils intelligent
pharmaceutical fridges with IoT technology [26].
This list of usage the IoT system conﬁrms that these systems are critical and have a
huge impact on people living and perform more critical and responsible functions. All
of this is increase requirements to IoT system dependability and safety. Here we provide
the typical structure (Fig. 1) of researched IoT system, that lets to manage and monitoring
the numbers of clients on the one platform. In general is possible to divide IoT system
to few levels, such as:
Level 1. It is the lowest level which includes the devices (sensors, actuators, and other).
These devices are operated in autonomous mode and receiving the information about
the condition of monitoring object. All devices which operated in this level are hardware/
software system that usually is unmaintained and without software updates. However
is possible devices restart to return it for operational conditions after temporary software
590
B. Volochiy et al.

failure which instigated by hardware failure. In some cases is possible to replace devices
but this lead to increase the total system cost, therefore this device designed in a fault-
tolerant manner to achieve huge reliability level. Usually, devices on this level operated
in 24/7 mode and communicate with gateway by Wi-Fi channel.
Level 2. Hardware/software gateway level which provides the communications of all
ﬁrst levels devices on the one network, managements (hub function) and communicates
the ﬁrst levels devices with a cloud server. Communications with cloud server can occur
by mobile (2G, 3G, 4G, 5G), DSL and other connections. This gateway is usually hard‐
ware/software devices with the possibility of software restart and updates. In addition,
the gateway should process the incoming data stream from ﬁrst levels devices and reli‐
able and safety transfer this data to the cloud server.
Level 3. Cloud Server which provides receiving, processing and manages data from all
clients. In the design of IoT system should consider that system would operate when all
clients would have a reliable and stable connection to the cloud server. To ensure a high
level of reliability in some cases is required to use two or more cloud servers. The cloud
server is a combination of hardware, software parts, and communications network. At
this point of reliability, the cloud servers have structural redundancy which allows a
high level of reliability provided. The software in cloud servers has a large variety:
networks ﬁrmware, operational systems, and services software with the possibility of
version redundancy. In such systems is possible to repair elements of services without
it operational losses through redundancy, also software restart and updates.
Level 4. End user level (Desktop and mobile app.). This level is hardware/software
platform with a weak spot in software. This software is constantly changing according
to each IoT services and client requirements.
One of based technology which was used in solutions of this class is M2M (Machine-
to-Machine) [27]. M2M, the acronym for Machine-to-Machine Applications (or more
aptly, Mobile to Machine and Machine to Mobile communications) is an emerging area
in the ﬁeld of telecom technologies. This technology allows simple, reliable and prof‐
itable the data transfer between diﬀerent devices provides. M2M allows a wide variety
of machines to become nodes of personal wireless networks and provide to develop
monitoring and remote control applications. This will decrease costs for involved human
resources and will make machines more intelligent and autonomous. The M2M can
operate by stationary and mobile mode. The stationary mode was used for manage the
technology process, security monitoring, payment terminals, meters, vending machines
and other. Mobile mode of M2M lets you moving objects management (vehicle),
onboard devices, navigations, diagnostics, positions, and protection. Using such system
allows collecting data about monitoring objects, conduct an analysis of this data and
provide business recommendations regarding operation or modernization of objects.
Analytical Model for Availability Assessment
591

3
An Approach to Development of the Availability Model for IoT
Services
To solve this problem one should develop a mathematical representation of IoT system
that will allow evaluating the safety, reliability and determining the requirements of each
of its components at the design phase.
As was mentioned in the description of the typical IoT system structure, important
tasks that arise when addressing the design and analysis of safety and reliability of IoT
system are:
1. Reliability synthesis of the system and its components using structural-automaton
model (SAM).
2. Considering the performance reliability of software development at SAM.
3. Considering the processes of reloading and updating software in the reliable behavior
of IoT system. Herewith, to improve the authenticity of reliability indicators, in
reliability model you should take into account the phenomenon of an imperfect ﬁx
of software defects after replacing it with the new version.
4. In the evaluation of the availability and safety of IoT systems, you should take into
account the ability of routers and cloud services to handle the ﬂow of applications
from the device of the IoT ﬁrst level architecture (sensors and actuators). To solve
this problem, you can use the formalism of the queuing theory.
5. Evaluation of the safety and availability of IoT systems cannot be authentic without
consideration of communication channels reliability.
Figure 2 shows a model of the overall IoT structure as a queuing system, with
multiple queues and system maintenance. Speciﬁcally, the inputs of service 1 … n
(Service Facility 1 … n) (software and hardware router) via a wireless communication
channel receives messages from ﬁrst level devices with the intensity of Arrival rate 1
… m. In service systems, the adaptive service discipline with assigning priorities to
critical messages for a maximum speed of transmission is implemented. The next step
Fig. 2. Queue networks of the IoT service
592
B. Volochiy et al.

is forming queues messages that represent communication links between routers and
cloud server. These message queues with the intensity of Arrival rate 1.1…n.m are
transmitted via the communication channel (Mobile (2G, 3G, 4G), DSL) to the service
system 1.1 (Service Facility 1.1) (cloud service), in which they are pretreated. The user
through the service system 1.1.1 … 1.2.z gets the access to the cloud service. This access
provides sophisticated telecommunications network.
Suggested mathematical model of the IoT service should provide answers to the
following questions:
1. What is the availability coeﬃcient of the system? The cost of maintenance and
development of the system will depend on the required level of availability.
2. Which frequency of messages should be from the ﬁrst level devices, to not to “over‐
whelm” a communication channel with the router and at the same time to react in
time on an event that actually leads to the need to solve the optimization problem?
3. Which communication channel (in the case of mobile communication, what traﬃc
is optimal?) has to be chosen between the second and third levels of the system to
provide the reliable (lossless) and timely data transmission.
4. How should the cloud server system be built in order to provide a given level of
reliability?
5. What should be the permissible intensity of software failures (defects) on the fourth
level of service to ensure the required level of reliability of the system as a whole?
Thus, further research will be focused on building reliability models of the IoT
system and its components. In these models, the following aspects should be considered:
software reliability indices, processes of its restart and update, the data ﬂow processing
by routers and cloud services, the reliability of data transmission channel. The aim of
this research is the evaluation of safety and availability level of the IoT system.
4
Forecasting of Software Failure and Failure Rate for IoT
Transmission Subsystem
All IoT systems consist of two major components: hardware and software (ﬁrmware).
Software reliability is diﬀerent from hardware reliability in a sense that software does
not wear out or burn out. The software itself does not fail unless ﬂaws within the software
result in a failure in its dependent system. The greatest problem facing the industry today
is how to assess quantitatively software reliability characteristics (see e.g. [29, 30] and
others). This is also of high importance to the future networks, in particular to both cloud-
based services and the software running on IoT devices. Research on software reliability
engineering has been conducted during the past three decades and numerous statistical
models have been proposed for estimating software reliability. Most existing models for
predicting software reliability are based purely on the observation of software product
failures where they require a considerable amount of failure data to obtain an accurate
reliability prediction. Some other research eﬀorts recently have developed reliability
models addressing fault coverage, testing coverage, and imperfect debugging processes
[28–30].
Analytical Model for Availability Assessment
593

This paper outlines the software that is operated on IoT devices (sensors, actuators)
and gateways. These two types of software have the diﬀerence between functionality
and structural complexity, and software updates capabilities.
The IoT devices ﬁrmware is less complicated than gateways ﬁrmware and usually
is not updated. The main task of estimating the IoT devices ﬁrmware is to calculate the
residual numbers of ﬁrmware bugs based on testing results.
For solving this task, we propose to use the nonhomogeneous Poisson process
(NHPP) in a software reliability growth model with an index of complexity [31].
The group of models based on NHPP have analytical results for descriptions of the
software failure behaviors during the testing. The main problem of this sort of software
reliability models is the determining of software failures mean value function. A lot of
models belonging to this group have been developed for the latest four decades including
exponential Musa model [32], Goel and Okumoto NHPP reliability growth model [33],
Yamada and Ohba S-shaped reliability growth model [34] etc. The nonhomogeneous
Poisson distribution works well in diﬀerent areas where the subject of interest is the
quantities of independent random events [35].
The main assumption of the NHPP software reliability growth models is that M(t),
failures count during the time interval (0, t], belongs to the Poisson distribution, and
{M(t), t ≥0} corresponds to nonhomogeneous Poisson process. Let 𝜇(t) = E[M(t)] is a
mathematical expectation of M(t) or a mean value of Poisson process. Diﬀerent models
of this group diﬀers by the 𝜇(t) function expression. Wide NHPP software reliability
models exploitation, among others, is caused by the ability to calculate the expected
amount of software failures at any time point due to explicit deﬁnition of the mean value
function; ease to obtain point estimations of model parameters using maximum likeli‐
hood or mean squares methods.
The NHPP reliability models with the following software failure rate form were
developed in [31]:
𝜆(t) = 𝛼𝛽s+1ts exp (−𝛽t),
(1)
here 𝛼 is the coeﬃcient, which deﬁnes the expected number of failures to be observed
eventually, 𝛽 is the fault detection rate per fault (𝛽> 0), s is the dynamic software
complexity index.
With given failure rate function (1) the expected number of failures observed by time
t has the following form:
𝜇(t) = ∫
t
o
𝜆(𝜏)d𝜏= 𝛼[−𝛽stte−𝛽t + sΓ𝛽t(s)
],
(2)
here, Γz(p) = ∫
Z
0 tp−1e−tdt, (Re p > 0), is the incomplete gamma-function.
The expected number of faults to be eventually detected is deﬁned as the expected
number of failures to be observed when t →∞, thus it has the form:
𝜇(∞) = 𝛼sΓ(s),
(3)
594
B. Volochiy et al.

where Γ(s) is the gamma-function.
To use the model for the reliability behavior description of concrete software package
using Eqs. (1) and (2) one has to obtain the point estimations of the model parameters.
Maximum likelihood method was used in [31] to perform this task.
Using the described model for software reliability indices assessment allows one
signiﬁcantly improve the accuracy of the software failures rate evaluation comparing to
other widely used models keeping the expected number of faults value evaluation prac‐
tically identical [36].
Thus, based on software failures count at the testing stage, using the Eqs. (1) and (3)
one can estimate the values of software failures rate and the number of residual faults
within the sensor or actuator ﬁrmware, which is used in IoT system studied.
The software part of IoT system gateways unlike the device’s software usually is
more complex in a functional sense, possesses higher values of complexity metrics, and
usually is updated by the vendor e.g. by ﬁrmware over the air update. After an update,
new ﬁrmware version will have diﬀerent values of failure rate and a number of residual
defects. The reliability model of the IoT system should include the mentioned facts that
in turn will aﬀect estimated risk value and functional model of the system.
Implementing the software complexity index s into the software reliability model
(1) and estimation of its value ranges [37] allows to apply this model for gateways
ﬁrmware failure rate and residual faults assessment. This results from the fact that the
typical gateway ﬁrmware complexity does not exceed the limits of the model adequacy,
and furthermore it is usually no need to consider software structure and diﬀerent cases
usage in such devices as IoT gateways. The latter would result in the need to apply an
architecture software reliability model [38] instead.
5
Availability Model for IoT Services
To ﬁnd an answer to the ﬁrst questions: what is the availability coeﬃcient of the system,
we proposed the availability model of IoT system. The method of automated develop‐
ment of the Availability Model in Markovian chain form of the researched IoT systems
is described in the monograph [39]. It involves a formalized representation of the study
object as a “structural-automated model”. To develop this model of the IoT systems one
needs to perform the following tasks: develop a verbal description of the research object
(Fig. 1); deﬁne the basic events; deﬁne the components of vector states that can be
described as a state of random time; deﬁne the parameters for the object of research that
should be in the model; and shape the tree of the modiﬁcation of the rules and component
of the vector of states.
5.1
A Set of Events for the IoT Services
As a result of structure analysis, ten basic events, in particular, were determined: Event
1 – “Hardware/Firmware failure of sensor/actuator/device”; Event 2 – “Hardware/
Firmware failure of gateway”; Event 3 – “Hardware failure of cloud server”; Event 4
– “Software failure of cloud server”; Event 5 – “Hardware failure of user’s device”;
Analytical Model for Availability Assessment
595

Event 6 – “Repair of sensor, actuator or device”; Event 7 – “Repair of gateway”; Event
8 – “Repair of hardware failure on cloud server”; Event 9 – “Fix software failure of
cloud server”; Event 10 – “Repair hardware failure of user’s device.
5.2
Components of the State’s Vector for the IoT Services
To describe the state of the system, nine components are used: V1 – displays the current
number of sensors/actuators/devices (the initial value of components V1 is equal to n);
V2 – displays the current number of gateways (the initial value of components V2 is
equal to m); V3 – displays the number of user’s devices (the initial value of components
V3 is equal to k); V4 – displays the number of cloud servers (the initial value of compo‐
nents V4 is equal to 1); V5 – displays HW fault on cloud server; V6 – displays SW fault
on cloud server; V7 – displays the number of non-operational units due to fault of sensor,
actuator or device; V8 – displays the number of non-operational units due to fault of
gateway; V9 – displays the number of non-operational units due to HW fault of user’s
device.
5.3
Sets of Parameters Which Describe the IoT Services
Developing the Markov model of the IoT Services, its composition and separate compo‐
nents should be set to relevant parameters in particular: n – number of sensors/actuators/
devices; m – number of gateways; k – number of user’s devices; λhw0 – the failure rate
of sensor, actuator or device; λﬁrmware0 – the ﬁrmware failure rate of sensor, actuator or
device; λhw1 – the failure rate of gateway; λﬁrmwarer1 – the ﬁrmware failure rate of gateway;
λhw2 – the failure rate of cloud server hardware; λhw3 – the failure rate of user’s device
hardware; λsw – the failure rate of cloud server software; Trep0 – the duration of sensor,
actuator or device repair; Trep1 – the duration of gateway repair; Trep2 – the duration of
cloud server hardware repair; Trep3 – the duration of user’s device hardware repair; Tsw
– the duration of software failure ﬁx.
5.4
Structural-Automated Model of the IoT Services for the Markovian Chain
Development
According to the technology of analytical modeling, the discrete-continuous stochastic
systems [39] based on certain events using the component vector state and the parameters
that describe IoT Services, and model of the IoT Services for the automated development
of the Markovian chains are presented in Table 1.
Proposed structural-automated model is the input sets for cutting-age problem-
oriented software tool ASNA. Using this technology we automatically build Markovian
Chain for a diﬀerent structure of IoT system [38, 39]. We can easily change the number
of sensors, gateways, users devices and maintenance parameters. Based on a Markovian
chain we shape the mathematical model in Chapmen-Kolmogorov diﬀerential equation
form. A solution of this equation gives answers to the following questions: What is the
Probability of Faultless? What is the mean time between failures? What is the mean time
between repairs? Which fault-tolerance conﬁgurations of cloud server are better?
596
B. Volochiy et al.

This mathematical model gives us the opportunity to solve tasks of parametric
synthesis of IoT system components.
Table 1. Structural-automated model of the IoT System for the automated development of the
Markovian chains
Terms and conditions
Formula used for the intensity of
the events
Rule of modiﬁcation component
for the state vector
Event 1. Hardware/Firmware failure of sensor/actuator/device
(V1 > 0) AND (V2 = 1) AND
(V4 = 1) AND (V3 = 1) AND
(V7 < n)
V1·λhw0·λﬁrmware0
V1 := V1 − 1; V2 := 1; V3 := 1;
V4 := 1; V7 := V7 + 1
Event 2. Hardware/Firmware failure of gateway
(V2 = 1) AND (V1 > 0) AND
(V4 = 1) AND (V3 = 1) AND
(V8 = 0)
V2·λhw1
V2 := 0; V1 := V1; V3 := 1;
V4 := 1; V8 := 1
Event 3. Hardware failure of cloud server
(V4 = 1) AND (V1 > 0) AND
(V2 = 1) AND (V3 = 1) AND
(V5 = 0)
V4·λhw2
V4 := 0; V1 := 0; V2 := 0; V3 = 0;
V5 := 1; V7 := n; V8 := 1; V9 := 1
Event 4. Software failure of cloud server
(V4 = 1) AND (V1 > 0) AND
(V2 = 1) AND (V3 = 1) AND
(V6 = 0)
V4·λsw
V4 := 0; V1 := 0; V2 := 0; V3 = 0;
V5 := 1; V6 := 1; V7 := n; V8 := 1;
V9 := 1
Event 5. Hardware failure of user’s device
(V3 = 1) AND (V1 > 0) AND
(V2 = 1) AND (V4 = 1) AND
(V9 = 0)
V3·λhw3
V3 := 0; V1 := V1; V2 := 1;
V4 := 1; V9 := 1
Event 6. Repair of sensor, actuator or device
(V1 < n) AND (V2 = 1) AND
(V4 = 1) AND (V3 = 1) AND
(V7 > 0)
1/Trep0
V1 := V1 + 1; V2 := 1; V3 := 1;
V4 := 1; V7 := V7 − 1
Event 7. Repair of gateway
(V2 = 0) AND (V1 > 0) AND
(V4 = 1) AND (V3 = 1) AND
(V8 = 1)
1/Trep1
V2 := 1; V1 := V1; V3 := 1;
V4 := 1; V8 := 0
Event 8. Repair of hardware failure on cloud server
(V4 = 0) AND (V1 = 0) AND
(V2 = 0) AND (V3 = 0) AND
(V5 = 1) AND (V7 = n) AND
(V8 = 1) AND (V9 = 1)
1/Trep2
V4 := 1; V1 := n; V2 := 1; V3 := 1;
V5 := 0; V7 := 0; V8 := 0; V9 := 0
Event 9. Fix software failure of cloud server
(V4 = 0) AND (V1 = 0) AND
(V2 = 0) AND (V3 = 0) AND
(V5 = 1) AND (V6 = 1) AND
(V7 = n) AND (V8 = 1) AND
(V9 = 1)
1/Tsw
V4 := 1; V1 := n; V2 := 1; V3 := 1;
V5 := 0; V6 := 0; V7 := 0; V8 := 0;
V9 := 0
Event 10. Repair hardware failure of user’s device
(V3 = 0) AND (V1 > 0) AND
(V2 = 1) AND (V4 = 1) AND
(V9 = 1)
1/Trep3
V3 := 1; V1 := V1; V2 := 1;
V4 := 1; V9 := 0
Analytical Model for Availability Assessment
597

6
Conclusion
This paper presents a motivation to research the availability and safety of IoT service.
The queue networks for estimating the availability and safety of IoT service and tasks
for next research were presented. The availability model was developed using the struc‐
tural-automated model of studying the discrete-continuous stochastic systems. The
technique of IoT systems software reliability indices assessment has been described that
allows increasing the adequacy of IoT system reliability assessment. The reliability
model of IoT system in the form of SAM was developed. The developed model allows
carrying out the parametric synthesis of IoT system components, select an optimal
maintenance strategy depending on given exploitation environments.
References
1. Atzori, L., Iera, A., Morabito, G.: The Internet of Things: a survey. Comput. Netw. 54, 2787–
2805 (2010)
2. Ma, H.-D.: Internet of things: objectives and scientiﬁc challenge. J. Comput. Sci. Technol.
26, 919–924 (2011)
3. Miorandi, D., Sicari, S., Pellegrini, F.D., Chlamtac, I.: Internet of things: vision applications
and research challenges. Ad Hoc Netw. 10, 1497–1516 (2012)
4. Roman, R., Alcaraz, C., Lopez, J., Sklavos, N.: Key management systems for sensor networks
in the context of the Internet of Things. Comput. Electr. Eng. 37, 147–159 (2011)
5. Silva, M.D., Leandro, R., Batista, D.M., Guedes, L.A.: A dependability evaluation tool for
the Internet of Things. Comput. Electr. Eng. 39, 2005–2018 (2013)
6. Zin, T.T., Tin, P., Hama, H.: Reliability and availability measures for Internet of Things
consumer world perspectives. In: 5th IEEE Global Conference on Consumer Electronics, pp.
1–2 (2016)
7. Ashton, K.: That ‘Internet of Things’ thing. RFID J., 1–2 (2009)
8. Sundmaeker, H., Guillemin, P., Friess, P., Woelﬄé, S.: Vision and challenges for realising
the Internet of Things. In: Cluster of European Research Projects on the Internet of Things
—CERP IoT (2010)
9. Xia, F., Yang, L.T., Wang, L., Vinel, A.: Internet of Things. Int. J. Commun Syst 25, 1101–
1102 (2012)
10. Gubbi, J., Buyya, R., Marusic, S., Palaniswami, M.: Internet of Things (IoT): a vision,
architectural elements, and future directions. Future Gener. Comput. Syst. 29, 1645–1660
(2013)
11. Uckelmann, D., Harrison, M., Michahelles, F.: An architectural approach towards the future
Internet of Things. In: Uckelmann, D., Harrison, M., Michahelles, F. (eds.) Architecting the
Internet of Things, pp. 1–24. Springer, Heidelberg (2011)
12. Azure IoT Suite helps Sandvik Coromant stay on cutting edge within “digital manufacturing”.
https://blogs.microsoft.com/iot/2016/09/12/azure-iot-suite-helps-sandvik-coromant-stay-
on-cutting-edge-within-digital-manufacturing/#08ckShkjVuPGPOBP.99
13. Rolls-Royce and Microsoft collaborate to create new digital capabilities. https://
customers.microsoft.com/en-US/story/rollsroycestory
14. Fueling the oil and gas industry with IoT. https://customers.microsoft.com/en-US/story/
fueling-the-oil-and-gas-industry-with-iot-1
598
B. Volochiy et al.

15. How manufacturers are creating the digital, intelligent and predictive factory. https://
blogs.microsoft.com/transform/2016/04/24/how-manufacturers-are-creating-the-digital-
intelligent-and-predictive-factory/#sm.0000ns1xd9hpqcolwy71tyhiubvkz
16. Connecting Buildings to the Cloud for a Greener Planet. https://customers.microsoft.com/en-
US/story/connecting-buildings-to-the-cloud-for-a-greener-planet
17. BCA partners Microsoft to leverage IoT, data analytics and the cloud for next-generation
Green 
Mark 
buildings. 
https://news.microsoft.com/en-sg/2016/09/07/bca-partners-
microsoft-to-leverage-iot-data-analytics-and-the-cloud-for-next-generation-green-mark-
buildings/#jwEJJSbDCDrtL6fK.99
18. Ecolab uses cloud computing to save fresh water on the ground. https://blogs.microsoft.com/iot/
2016/04/05/ecolab-uses-cloud-computing-to-save-freshwater-on-the-ground/
#XMQgmhjSRmdtDQ7B.99
19. Fathym’s IoT-enabled Weather Cloud enhances driver safety during inclement weather.
https://blogs.microsoft.com/iot/2016/12/09/fathyms-iot-enabled-weathercloud-enhances-
driver-safety-during-inclement-weather/#wpbM9QQvdxROoDx8.99
20. Azure IoT Technology helps NAV CANADA revolutionize air-traﬃc control. https://
blogs.microsoft.com/iot/2016/03/17/azure-iot-technology-helps-nav-canada-revolutionize-
air-traﬃc-control/#JvuzE3WFYvjuqU6h.99
21. Italian grocery co-op develops supermarket of the future. https://blogs.microsoft.com/iot/
2016/04/08/italian-grocery-co-op-develops-supermarket-of-the-future/#4vi5fAJ5xT87wAjZ.
99/
22. Immersive, interactive, intelligent: New retail experiences on display at NRF. https://
blogs.microsoft.com/iot/2016/01/19/immersive-interactive-intelligent-new-retail-experiences-
on-display-at-nrf/
23. Hershey enhances global brands and productivity with cloud technology. https://
customers.microsoft.com/en-US/story/hershey-oﬃce365
24. IoT-enabled 
Smart 
Fridge 
helps 
manage 
vaccines 
and 
saves 
lives. 
https://
blogs.microsoft.com/iot/2016/08/16/iot-enabled-smart-fridge-helps-manage-vaccines-and-
saves-lives/
25. Advancing 
hospital 
hand 
hygiene 
practices 
through 
IoT 
solutions. 
https://
www.microsoft.com/en-us/internet-of-things/customer-stories#healthcare&gojoindustries
26. Liebherr Domestic Appliances collaborates with Microsoft to build new smart fridge for
medicine. https://blogs.microsoft.com/transform/2016/04/24/liebherr-domestic-appliances-
collaborates-with-microsoft-to-build-new-smart-fridge-for-medicine/#sm.
0000ns1xd9hpqcolwy71tyhiubvkz
27. Krishnan, V., Braswar, S.: M2M Technology: Challenges and Opportunities. Tech Mahindra
(2010)
28. Pham, H.: System Software Reliability. Springer Series in Reliability Engineering. Springer,
London (2006)
29. Trivedi, K.S., Bobbio, A., Muppala, J.K.: Greenbook: Reliability and Availability
Engineering: Modeling, Analysis, and Applications. Cambridge University. (2017, in Press)
30. Maevsky, D.A.: A new approach to software reliability. In: Gorbenko, A., Romanovsky, A.,
Kharchenko, V. (eds.) SERENE 2013. LNCS, vol. 8166, pp. 156–168. Springer, Heidelberg
(2013)
31. Chabanyuk, Y.M., Yakovyna, V.S., Fedasyuk, D.V., Seniv, M.M., Khimka, U.T.:
Development and study the software reliability model with project size index. Software Eng.,
24–29 (2010). (in Ukrainian)
32. Musa, J.D.: A theory of software reliability and its application. IEEE Trans. Software Eng.
SE-1(3), 312–327 (1975)
Analytical Model for Availability Assessment
599

33. Goel, A.L., Okumoto, K.: Time-dependent error-detection rate model for software and other
performance measures. IEEE Trans. Reliab. R-28, 206–211 (1979)
34. Yamada, S., Ohba, M., Osaki, S.: S-shaped reliability growth modeling for software error
detection. IEEE Trans. Reliab. R-32, 475–478 (1983)
35. Goel, A.L.: Software reliability models: assumptions, limitations, and applicability. IEEE
Trans. Software Eng. SE-11, 1411–1423 (1985)
36. Mulyak, A., Yakovyna, V., Volochiy, B.: Inﬂuence of software reliability models on
reliability measures of software and hardware systems. Eastern Eur. J. Enterp. Technol. 4,
53–57 (2015)
37. Bobalo, Yu., Volochiy, B., Lozynskyi, O., Mandziy, B., Ozirkovskii, L., Fedasyuk, D.,
Shcherbovskyh, S., Yakovyna, V.: Mathematical Models and Methods for Reliability
Analysis of Electrical, Electronics and Software Systems. Lviv Polytechnic Publishing
House, Lviv (2013). (in Ukrainian)
38. Yakovyna, V., Nytrebych, O.: Discrete and continuous time high-order Markov models for
software reliability assessment. In 11th International Conference ICTERI 2015, Lviv,
Ukraine, 14–16 May 2015. CEUR-WS.org, CEUR-WS.org/Vol-1356/paper_62.pdf
39. Volochiy, B.: Technology of Modelling of Algorithms of Behavior of Information Systems.
Lviv Polytechnic Publishing House, Lviv (2004). (in Ukrainian)
600
B. Volochiy et al.

Building Vector Autoregressive Models
Using COMBI GMDH
with Recurrent-and-Parallel Computations
Serhiy Yeﬁmenko(&)
Department for Information Technologies of Inductive Modelling,
International Research and Training Center for Information Technologies
and Systems, Ave Glushkov, 40, Kyiv 03680, Ukraine
syeﬁm@ukr.net
Abstract. The paper presents theoretical grounds of recurrent-and-parallel
computing applying for modelling and prediction of complex multidimensional
interrelated processes in the class of vector autoregressive models. The com-
binatorial GMDH algorithm is used for vector autoregressive (VAR) modelling
by exhaustive search of all possible variants and ﬁnding the best model for every
time series. The algorithm with selection a few best models for every process is
used. The procedure of structural and parametric identiﬁcation of VAR models
is proposed. It is applied the combining all possible variants of systems from the
selected best models and choosing the best system model according to an
additional criterion. The test experiment on solving the problem of structural and
parametric identiﬁcation for experimental testing of algorithm efﬁciency was
carried out. The effectiveness of constructed algorithm is demonstrated by
prediction of interrelated processes in the ﬁeld of Ukraine energy sphere with the
purpose of effective managerial decision making.
Keywords: Multidimensional time series  VAR  Inductive modelling
GMDH  Combinatorial algorithm  Recurrent-and-parallel computing
1
Introduction
The problem of mathematical modelling and prediction of complex multidimensional
interrelated time series ﬁnds the application foremost in economic, ecological, socio-
logical spheres [1–4]. In the case of prediction of a vector process presented as a set of
time series (or multidimensional time series), it is advisable to use such class of models
as vector autoregression [5, 6]. Granger causality has been applied to reveal interde-
pendence structure in such kind of time series [7–9].
An approach to the problem of structural and parametric identiﬁcation of this
process, when parameters for every model are estimated independently, is considered.
The drawback of the approach is that the parameters of separate models for interrelated
processes are interdependent. The paper uses an algorithm with selection not one but a
few best models for every process to eliminate the shortcoming. It is applied the
combining all possible variants of systems from the selected best models and choosing
the best system model according to an additional criterion.
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_42

We use the Group Method of Data Handling (GMDH) [10] as one of the most
effective inductive modelling methods. It is widely used for solving broad spectrum
problems of artiﬁcial intelligence such as identiﬁcation, forecasting, recognition,
clustering, and macromodelling.
When solving the problem of vector autoregressive modelling with the use of
inductive modelling methods, the run-time is one of the most important criteria of
efﬁciency. The most effective ways to achieve the high performance are recurrent
parameter estimation and parallel computing.
High-performance algorithms and software tools on the basis of both recurrent and
parallel computing were already developed and proved their efﬁciency [11–13].
The purpose of this work is to apply the technology, combining both these
approaches, for effective solving the problem of vector autoregressive modelling using
GMDH combinatorial algorithm.
2
Vector Autoregressive Modelling
The vector autoregressive (VAR) model generalizes the model of autoregression to
multidimensional case. It is built by the stationary time series. It is the system of
equations, in which every variable (component of multidimensional time series) is
linear combination of all variables in the previous time points. The order of such model
is determined by the order of the lags. For the simplest case of two time series with one
lag the model of VAR will be as follows:
x1ðtÞ ¼ h11x1ðt  1Þ þ h12x2ðt  1Þ;
x2ðtÞ ¼ h21x1ðt  1Þ þ h22x2ðt  1Þ;
ð1Þ
where hij; i; j ¼ 1; 2 – model parameters.
In the general case for m time series and k lags the model will be the system of
m equations:
x1 tð Þ ¼ h11x1 t  1
ð
Þ þ . . . þ h1kx1 t  k
ð
Þ þ h1;k þ 1x2
 t  1
ð
Þ þ . . . þ h1;2kx2 t  k
ð
Þ þ . . . þ h1;mkxm t  k
ð
Þ;
. . .
xm tð Þ ¼ hm1x1 t  1
ð
Þ þ . . . þ hmkx1 t  k
ð
Þ þ hm;k þ 1x2
 t  1
ð
Þ þ    þ hm;2kx2 t  k
ð
Þ þ . . . þ hm;mkxm t  k
ð
Þ;
ð2Þ
or alternatively in matrix form:
XðtÞ ¼
X
k
j¼1
HjXðt  jÞ;
ð3Þ
where Hj; j ¼ 1; k – matrices of model parameters (3) of size m  m.
602
S. Yeﬁmenko

3
Procedure of Structural and Parametric Identiﬁcation
of VAR Models
The general models structure in the form of a system of m difference equations is
determined as a result of the sequence of such operations initially developed in [14]:
1. Data array of m  k arguments is composed under the number of interrelated pro-
cesses m and lags к.
2. Maximal complexity for restricted search is deﬁned [14]. COMBI algorithm with
sequentially complicated structures of models on the basis of recurrent-and-parallel
computing is used for modelling (see next section). For every time series the best
F (by the value of the regularity criterion [10]) models are selected. Overall
F  m models are passed to the next step.
3. The sorting of G ¼ Fm possibilities of systems of models is carried out. The best
model (by the value of the systemic integral criterion of vector models quality) is
selected. The value of the criterion is calculated on the given part of initial data set
in the prediction mode of the process for the given steps number nS:
B ¼
X
nS
i¼1
X
m
j¼1
ðxij  x
ijÞ2;
ð4Þ
where x
ij is the result of step-by-step integration (recursive computations) of the system
of m equations.
Acceptable modelling time should be taken into account when assignment the value
of degree of freedom F (number of the best models being passed to the next step).
Figure 1 shows experimental results for the test problem with m = 11 time series and
k = 2 lags. The time of sorting of systems (and computing the value of the criterion (4))
grows exponentially.
0,02
305
0,2
265
5,3
167
62,3
140
0
50
100
150
200
250
300
350
2
3
4
5
Degree of freedom
Modelling time, sec
Criterion (4) value
Fig. 1. Dependence of modelling time and criterion (4) value on degree of freedom
Building Vector Autoregressive Models
603

4
Recurrent-and-Parallel Computing
The exponential dependence of number of system models G on the number of inter-
related processes m results in huge amount possibilities sorting and requires
improvement of modelling aids. Therefore both paralleling of computing and recurrent
algorithm of parameters estimation are used here. Recurrent computing increase
modelling efﬁciency (in terms of modelling time) nearly ﬁve times [16].
According to theoretical study, efﬁciency of paralleling the COMBI algorithm
computations is approximate to maximum possible 100% (i.e., paralleling on l pro-
cessors reduces the modelling time nearly l times). The results of testing the efﬁciency
of parallel computing are given below (Sect. 8).
Figure 2 shows approximate modeling time of exhaustive search for 40 arguments
with the use of COMBI algorithm for 4 cases:
• without recurrent-and-parallel computing (I);
• with recurrent parameters estimation (II);
• with parallel computing, 100 processors (III);
• with recurrent-and-parallel computing, 100 processors (IV).
5
Combınatorial GMDH Algorithm with Recurrent
Parameters Estimation
The combinatorial algorithm is used for VAR modelling by exhaustive search of all
possible variants and ﬁnding the best model for every time series containing the most
informative subset of input arguments (regressors). It consists of such main units:
• data conversion according to a basic class of models (linear in parameters);
• forming models of different complexity;
• calculation values of external quality criteria for all models being formed;
• selection of the best models.
291
58
2,9
0,6
0
100
200
300
Time, 
hours
I
II
III
IV
Fig. 2. Approximate modeling time of exhaustive search
604
S. Yeﬁmenko

For linear object with m input, all possible models are compared in the process of
exhaustive search. Total quantity of all generated models of the type
y
_
v ¼ Xvh
_
v; v ¼ 1; . . .; 2m  1
ð5Þ
is 2m–1. Decimal number m corresponds to binary number dm in (5). Unit elements of dm
indicate inclusion regressors with corresponding numbers in the model, whereas zero
elements signify exclusion.
Due to the exponential growth of 2m as function of arguments amount, it is
advisable to use algorithms recurrent in the number of parameters in structural iden-
tiﬁcation problems for the parameters estimation of model structures being sequentially
complicated.
Efﬁcient recurrent modiﬁcations of classic Gauss and Gramm-Schmidt algorithms
were offered in [12]. As far as the recurrent variant of Gauss method is useful for
combinatorial algorithm paralleling, its short-form description is done below.
The modiﬁcation, in a nutshell, is as follows. The matrix Hs ¼ XT
s Xs of the size
s  s
is
reduced
to
superdiagonal
form
by
computing
only
elements
hs
i s;
i ¼ 2; s  1, hs
s i;
i ¼ 2; s, and gs ¼ XT
s y at every step s; s ¼ 1; m during the
direct motion. The elements of the nested matrix Hs–1 of size (s–1)  (s–1) (reduced to
superdiagonal form on the previous step) remain changeless. So only “bordering ele-
ments” (bold fonts) are computed on step s:
6
Paralleling of COMBI Algorithm with Standard Binary
Counter
The scheme of COMBI paralleling based on the modiﬁed recurrent Gauss algorithm
with standard binary counter is described in [15].
The sequence of all possible combinations for models comprising e.g. m = 3
arguments will be as follows (with corresponding binary structural vector):
Building Vector Autoregressive Models
605

y1 ¼ a1x1
1; 0; 0
f
g
y2 ¼ a2x2
0; 1; 0
f
g
y3 ¼ a1x1 þ a2x2
1; 1; 0
f
g
y4 ¼ a3x3
0; 0; 1
f
g
y5 ¼ a1x1 þ a3x3
1; 0; 1
f
g
y6 ¼ a2x2 þ a3x3
0; 1; 1
f
g
y7 ¼ a1x1 þ a2x2 þ a3x3
1; 1; 1
f
g
Table 1 represents approximate dependence of modelling time on arguments
number and used processors for constructed algorithm. Already for more than 50
arguments, an exhaustive search (in acceptable modelling time) becomes impossible
even for cluster system containing one hundred processors. Any effective reducing of
exhaustive search is impossible due to the feature of the standard binary generator:
complexity of structural vectors changes inconsequentially.
Another scheme of COMBI paralleling is used for this case.
7
Paralleling of COMBI Algorithm with Sequential Binary
Counter
This scheme uses such sequence of binary numbers generation when all combinations
with one unit in structural vector appears ﬁrst of all (totally C1
m ¼ m possible variants is
generating), then with two units (C2
m ¼ mðm1Þ
2
possible variants), and so on to complete
model Cm
m ¼ 1


comprising all arguments.
Table 1. Approximate time of exhaustive search
Arguments Models
Time
1 processor 100 processors
20
1048575
1 s
0,01 s
21
2097151
2 s
0,02 s
…
…
…
…
40
1,1E + 12 *12 days
*3 h
…
…
…
…
50
1,1E + 15 *12 years
*124 days
606
S. Yeﬁmenko

The sequence of all possible combinations for models comprising three arguments
will be the following:
y1 ¼ a1x1
1; 0; 0
f
g
y2 ¼ a2x2
0; 1; 0
f
g
y3 ¼ a3x3
0; 0; 1
f
g
y4 ¼ a1x1 þ a2x2
1; 1; 0
f
g
y5 ¼ a1x1 þ a3x3
1; 0; 1
f
g
y6 ¼ a2x2 þ a3x3
0; 1; 1
f
g
y7 ¼ a1x1 þ a2x2 þ a3x3
1; 1; 1
f
g
The scheme can be easily used for COMBI paralleling on the given amount of
processors [14].
It allows to partially solve the problem of exhaustive search when arguments
number exceeds capability of the algorithm with a standard binary generator. In this
case it is advisable to execute an exhaustive search not among all possible models but
only for models of the restricted complexity.
8
Testing the Efﬁciency of the Algorithm Paralleling
The test experiment on solving the problem of structural and parametric identiﬁcation
for experimental testing of algorithm efﬁciency was carried out. The system model for
m = 11 time series with k = 2 lags was built. The computing was distributed on l = 5
threads and carried out sequentially on PC with processor Intel Pentium M (CPU clock
1.73 GHz). Hence we have a result close to theoretical due to avoiding interprocessor
interaction.
The best F = 5 models were selected for every time series by the value of the
regularity criterion. The run-time of every thread and run-time of the whole program
(without paralleling) were measured and compared. Figure 3 represents result of the
experiment as a run-time diagram.
0
20
40
60
80
100
120
140
W/o
paral.
1
2
3
4
5
Time, 
s
Thread
no.
Fig. 3. Program run-time diagram
Building Vector Autoregressive Models
607

The results of the experiment may be used for calculation the efﬁciency of
paralleling:
E ¼
T1
l  Tlmax
 100%
ð6Þ
and the uniformity of the computer load:
P ¼
1  Tlmax  Tlmin
Tlmax


 100%;
ð7Þ
where T1 is the program run-time for 1 thread (i.e. without paralleling), Tlmax is the
run-time for l = 5 threads (maximal run-time from 5 threads), Tlmin is the minimal
run-time.
Table 2 shows numerical values of efﬁciency indices for this experiment.
Thus, according to the test experiment, paralleling the COMBI algorithm compu-
tations on l processors reduces the modelling time nearly l times.
9
Modelling and Prediction of Indices of Ukraine Energy
Sphere
As an example of applying the constructed technology, an economic problem is solved
with the use of vector autoregressive modelling.
In total, 4 algorithms (the classical COMBI algorithm, its recurrent modiﬁcation,
parallel implementation, and recurrent-and-parallel algorithm as their synergism) are
compared. The main criterion is the modelling time. And these variants (in represented
order) reduce the criterion value.
Ukrainian Ministry of Economy data for 11 energy sphere indices for years 1996 to
2005 (n = 10 records) are used for modelling. In addition it is known value of ﬁrst 9
indices in 2006 year. The indicators are as follows:
x1 – share of domestic supply in energy balance, %;
x2 – share of dominant fuel resources in energy balance, %;
x3 – share of fuel import from one country (company) in its total volume, %;
x4 – tearing of ﬁxed assets of fuel-energy complex (FEC) enterprises, %;
x5 – ratio of investments amount in FEC enterprises to GDP;
x6 – energy intensity of GDP;
x7 – volume of coal production, million tons;
Table 2. Efﬁciency indices
Efﬁciency of paralleling, % 97
Uniformity of the load, %
96
608
S. Yeﬁmenko

x8 – oil transit, million tons;
x9 – gas transit, billion cubic metres;
x10 – natural gas production, billion cubic metres;
x11 – oil and gas condensate production, million tons.
Two approaches is used for modelling: (1) traditional, when every index (i.e. single
time series) is modelling independently in the class of autoregressive models;
(2) modelling the vector of indices (i.e. interdependent time series) in the class of
discrete dynamic VAR models.
Autoregressive modelling. Figures 4 and 5 presents modelling results and real values
for x3 and x8. All other results have similar character. It may be concluded that
autonomous autoregressive models are inappropriate for the description of experi-
mental data (have a character of a simple shifting the curve) and cannot be used for
reliable prediction.
Vector autoregressive modelling. The best 5 models (system of linear difference
equations) were selected for every index to build VAR model. Such value of the degree
of freedom was chosen to have acceptable modelling time.
50
60
70
80
1996
1998
2000
2002
2004
2006
Real values
Model
Fig. 4. Modelling results for index x3
20
30
40
50
1996
1998
2000
2002
2004
2006
Real values
Model
Fig. 5. Modelling results for index x8
Building Vector Autoregressive Models
609

After the selection of 55 best models, sorting of G = 511 possible systems of
models were carried out. The best system model was selected by the value of systemic
integral criterion (5). It was calculated in the prediction mode of the process for 8 steps.
A built VAR model is as follows:
x1ðtÞ ¼ 0:74x3ðt  2Þ þ 42:93x6ðt  2Þ  0:06x9ðt  2Þ þ 7:96x11ðt  1Þ  15:16x11ðt  2Þ
x2ðtÞ ¼ 0:27x2ðt  1Þ  0:93x3ðt  2Þ  0:59x9ðt  1Þ  0:44x9ðt  2Þ þ 57:04x11ðt  2Þ
x3ðtÞ ¼ 5x1ðt  1Þ þ 4:28x1ðt  2Þ  0:29x7ðt  2Þ  0:5x9ðt  2Þ þ 9:98x10ðt  2Þ
x4ðtÞ ¼ 1:46x4ðt  2Þ þ 0:08x7ðt  2Þ þ 0:14x8ðt  2Þ  0:04x9ðt  2Þ  8:68x11ðt  2Þ
x5ðtÞ ¼ 0:03x2ðt  2Þ  0:003x7ðt  1Þ  0:05x8ðt  1Þ þ 0:01x9ðt  1Þ þ 0:01x9ðt  2Þ
x6ðtÞ ¼ 0:02x1ðt  1Þ  0:03x4ðt  1Þ þ 0:75x6ðt  2Þ þ 0:004x10ðt  1Þ þ 0:15x11ðt  1Þ
x7ðtÞ ¼ 1:04x2ðt  1Þ þ 0:78x4ðt  2Þ þ 288:53x6ðt  1Þ  249:19x6ðt  2Þ þ 9:33x11ðt  2Þ
x8ðtÞ ¼ 1:81x2ðt  1Þ  4:71x5ðt  1Þ  1:96x9ðt  1Þ  1:86x9ðt  2Þ þ 118:35x11ðt  2Þ
x9ðtÞ ¼ 7:38x1ðt  2Þ  13:94x4ðt  2Þ  1:46x8ðt  2Þ  0:6x9ðt  2Þ þ 41:94x10ðt  2Þ
x10ðtÞ ¼ 0:32x4ðt  2Þ  11:34x6ðt  2Þ þ 0:04x8ðt  2Þ  0:04x9ðt  2Þ þ 3:35x11ðt  2Þ
x11ðtÞ ¼ 0:08x4ðt  2Þ  0:21x5ðt  1Þ  2:06x6ðt  2Þ þ 0:01x7ðt  1Þ þ 0:27x11ðt  1Þ
Modelling time for this enough complex problem was about 5 min.
Table 3 shows ratio errors for ﬁrst 9 indicators.
Figures 6 and 7 present modelling results and real values for indicators x3 and x8
with the worst and the best model accuracy on the testing data set, respectively.
Table 3. Model accuracy
x1
x2
x3
x4
x5
x6
x7
x8
x9
Ratio error, % 9.3 11 38.9 5.7 6.2 4.3 22 4.3 30.6
50
60
70
80
1996
1998
2000
2002
2004
2006
Real values
Model
Fig. 6. Modelling results for index x3
610
S. Yeﬁmenko

Figures 8 and 9 show modelling results for indices x10 and x11 with pure prediction
on 2006 year. It is signiﬁcant to note that modelling values for every index were
calculated by integrating the process from initial conditions (ﬁrst two records).
Constructed VAR model is accurate on experimental data for all indices of Ukraine
energy sphere. And it has acceptable prediction capabilities for most of them. Diver-
gence between real and model values for some indices may be caused by too short data
set. Another possible reason consists in neglecting some economic or other factors.
25
35
45
55
1996
1998
2000
2002
2004
2006
Real values
Model
Fig. 7. Modelling results for index x8
17
18
19
20
21
22
1996
1998
2000
2002
2004
2006
Real values
Model
Fig. 8. Modelling results for index x10
3,5
4
4,5
1996
1998
2000
2002
2004
2006
Real values
Model
Fig. 9. Modelling results for index x11
Building Vector Autoregressive Models
611

Hence, the constructed system model can be used for intermediate-term forecasting
– integration of the optimal system has high accuracy on 8 points (and for most of
indicators – even on 9 points).
10
Conclusion
The technique of recurrent-and-parallel computing in COMBI GMDH algorithm for
building discrete prediction models of complex multidimensional interrelated processes
is proposed.
Software tools for modelling and prediction of complex multidimensional inter-
related processes on the basis of high-performance combinatorial algorithm in the class
of discrete dynamic VAR models are developed.
The tools were used for modelling and prediction of interrelated processes of
Ukraine energy sphere with the purpose of effective managerial decision making in this
ﬁeld.
References
1. Neusser, K.: Time Series Econometrics. Springer International Publishing, Switzerland
(2016)
2. Lui, G.C.S., Li, W.K.: Modelling algal blooms using vector autoregressive model with
exogenous variables and long memory ﬁlter. Ecol. Model. 200, 130–138 (2007)
3. Primiceri, G.E.: Time varying structural vector autoregressions and monetary policy. Rev.
Econ. Stud. 72, 821–852 (2005)
4. Brandt, P.T., Williams, J.T.: Multiple Time Series Models. Sage (2007)
5. Canova, F.: VAR models: speciﬁcation, estimation, inference and forecasting. In: Pesaran,
H., Wickens, V. (eds.) Handbook of Applied Econometrics. Blackwell, Basil (1994)
6. Enders, W.: Applied Econometric Time Series, New York (1992)
7. Granger, J.: Investigating causal relations by econometric models and cross-spectral
methods. Acta Phys. Pol. B 37, 424–438 (1969)
8. Lutkepohl, H.: New Introduction to Multiple Time Series Analysis. Springer, Heidelberg
(2005)
9. Hoover, K.: Causality in Economics and Econometrics. The New Palgrave Dictionary of
Economics, 2nd edn. Palgrave Macmillan, London (2008)
10. Madala, H.R., Ivakhnenko, A.G.: Inductive Learning Algorithms for Complex Systems
Modeling. CRC Press, New York (1994)
11. Stepashko, V.S.: A combinatorial algorithm of the group method of data handling with
optimal model scanning scheme. Soviet Autom. Control 14(3), 24–28 (1981)
12. Stepashko, V.S., Eﬁmenko, S.N.: Sequential estimation of the parameters of regression
model. Cybern. Syst. Anal. 41(4), 631–634 (2005)
13. Stepashko, V., Yeﬁmenko, S.: Parallel algorithms for solving combinatorial macromodelling
problems. Przegląd Elektrotechniczny (Electrical Review) 85(4), 98–99 (2009)
14. Yeﬁmenko, S., Stepashko ,V.: Intelligent recurrent-and-parallel computing for solving
inductive modeling problems. In: 16th IEEE International Conference on Computational
Problems of Electrical Engineering (CPEE), pp. 236–238. LNPU, Lviv, Ukraine (2015)
612
S. Yeﬁmenko

15. Stepashko, V.S., Kostenko, Y.V.: A GMDH algorithm for two-level modeling of
multidimensional cyclic processes. Sov. J. Autom. Inform. Sci. 20(4), 49–57 (1987)
16. Yeﬁmenko, S.: Comparative effectiveness of parallel and recurrent calculations in
combinatorial algorithms of inductive modelling. In: Proceedings of the 4th International
Conference on Inductive Modelling ICIM 2013, pp. 231–234. IRTC ITS NASU, Kyiv
(2013)
Building Vector Autoregressive Models
613

Adaptive Enhancement of Monochrome
Images with Low Contrast and Non-uniform
Illumination
Elena Yelmanova1(&)
and Yuriy Romanyshyn1,2
1 Lviv Polytechnic National University, Lviv, Ukraine
yelmanova@lp.edu.ua, yuriy.romanyshyn1@gmail.com
2 University of Warmia and Mazury, Olsztyn, Poland
Abstract. The problem of adaptive enhancement for complex images with low
contrast and non-uniform illumination is considered. The histogram-based
method of image contrast enhancement in automatic mode on the basis of the
estimations of parameters of distribution of brightness values at the boundaries
of objects and background for the various known deﬁnitions of contrast kernels
is proposed. The research of the effectiveness of the proposed and well-known
methods of image contrast enhancement is carried out using the known
no-reference metrics of generalized contrast of image.
Keywords: Image processing  Contrast enhancement  Low-contrast
Non-uniform illumination  Small-sized objects  Contrast kernel
1
Introduction
Operative (in real-time) images quality enhancement in automatic mode is extremely
relevant for the vast majority of practical applications in imaging, image processing and
analysis [1].
Wide applying of modern technologies of imaging and image processing makes the
automatic quality enhancement for the formed images more relevant than ever [1].
The widespread use of various types of mobile gadgets (mobile phones, tablets,
laptops, car dash cams, etc.) equipped with video image sensors (high-resolution photo
and video cameras), of the systems of remote sensing, imagery intelligence, surveil-
lance and reconnaissance of various destination and of basing, of the remotely piloted
vehicles and remote-controlled robotic systems (UAVs, copters, robotic cars), etc.,
requires addressing the problem of real-time images enhancement in automatic mode.
The real-time image enhancement in automatic mode is currently one of the most
pressing and difﬁcult problems in imaging and image pre-processing [2].
The objective quality of digital images is characterized by several basic para-
meters [3]. Contrast is the main quantitative characteristic that determines the objective
quality of the image [3].
Therefore, the development of new effective techniques for real-time image contrast
enhancement in automatic mode (with acceptable level of computational costs) is
especially relevant at present [1]. Particularly relevant is the addressing of the problem
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_43

of effective contrast enhancement of complex images with low-contrast and small-sized
objects and non-uniform illumination.
The problem of developing of histogram-based methods of contrast enhancement in
automatic mode for complex low-contrast images with small-sized objects and
non-uniform illumination is considered (Sect. 2). The object of study is the process of
improving the image quality. The problem of contrast enhancement in automatic mode
for complex monochrome images with low-contrast and small-sized objects and
non-uniform illumination is considered. The purpose of the work is to improve the
effectiveness of contrast enhancement of small-sized low-contrast objects on the
complex multi-element monochrome images with non-uniform illumination. The
subject of the study is histogram-based methods of adaptive contrast enhancement of
complex monochrome images in automatic mode.
In this paper the new histogram-based method for adaptive enhancement in auto-
matic mode of the contrast of monochrome low-contrast images with small-sized
objects and non-uniform illumination on the basis of the estimation of parameters of
contrast distribution at the boundaries of image elements (objects and background) is
proposed (Sect. 3).
The proposed method of adaptive contrast enhancement provides an efﬁcient
redistribution of the contrast of image objects by maximizing the contrast for poorly
distinguishable low-contrast objects with the proviso that the contrast of high-contrast
objects is guaranteed to remain at a level which ensures their reliable detection and
recognition.
Research
of
the
effectiveness
of
the
proposed
and
well-known
histogram-based methods of image contrast enhancement in automatic mode was
carried out using known no-reference metrics of generalized contrast for four groups of
low-contrast monochrome images with a complex structure and non-uniform illumi-
nation (Sects. 4 and 5).
2
Image Contrast Enhancement
Various approaches for improving image quality are known at present [1, 2].
One of the most effective methods of quality improvement for image is enhance-
ment of its contrast (of its generalized contrast).
Generalized contrast of image is the most important quantitative characteristic,
which to a large extent determines the objective quality of the image, as well as the
efﬁciency, reliability and accuracy of its subsequent analysis and interpretation [2, 4].
The generalized contrast of complex (multi-element) subject images is usually
determined on the basis of the analysis of contrast values for all individual pairs of
image elements (objects and background) [5]. The contrast of a pair of image elements
characterizes the quantitative difference between the brightness values of two image
areas (of two objects or an object and a background) [5]. Increasing the generalized
contrast allows to effectively improve the quality of the initial image [3].
There are various approaches to enhance the contrast of monochrome images in
automatic mode. The known techniques of image contrast enhancement in a spatial
domain are most often subdivided into two main classes: global and local methods.
Adaptive Enhancement of Monochrome Images
615

Currently, the histogram-based techniques of image enhancement in the spatial
domain, such as [1, 2, 6, 7]:
– adaptive linear stretching,
– methods of nonlinear stretching (logarithmic, exponential and power transforma-
tions and their modiﬁcations, etc.),
– histogram speciﬁcation, histogram alignment and its modiﬁcations, and others, are
of the greatest interest for images contrast enhancement in real time.
A main shortcoming for the majority of known histogram-based techniques of
contrast enhancement is a possible reduction of contrast and the disappearance of
small-sized objects with low contrast which is unacceptable to image processing in
automatic mode [2].
The effective contrast enhancement of the small-size objects on the complex image
is possible by the analysis the contrast distribution at the boundaries of image elements
(objects and background).
To address these shortcomings, the histograms-based method of image contrast
enhancement on the basis of the estimations of contrast values at the boundaries of
objects and background is proposed.
3
Proposed Method
In this paper we propose the histogram-based method of contrast enhancement of
monochrome images on the basis of the estimations of parameters of distribution of
brightness values at the boundaries of objects and background for the various deﬁni-
tions of contrast kernels.
The proposed method is designed to enhance the contrast for complex low-contrast
images with small-sized objects and non-uniform illumination in automatic mode.
The proposed contrast enhancement method is designed to redistribute the contrast
of objects on the image for maximizing the contrast of poorly discernible low-contrast
objects, with the proviso that the contrast of high-contrast objects is guaranteed to
remain at a level which ensured their reliable detection and recognition.
To process the initial image, a known transformation is used [1]:
L
i ¼ L
min þ L
max  L
min


 ftr Li
ð Þ;
ð1Þ
where L
i - brightness value of i-th pixel of the transformed image, L
min and L
max -
minimum and maximum brightness values of the transformed image, Li - brightness
value of i-th pixel of the initial image, ftr Li
ð Þ - transformation function of brightness of
initial image.
As a rule, the transformation function ftr Li
ð Þ is a monotonically increasing function
and is normalized to the range [0, 1].
In the proposed method, the transformation function ftr Li
ð Þ for the initial image is
deﬁned as:
616
E. Yelmanova and Y. Romanyshyn

ftr Li
ð Þ ¼ M 
ZLi
0
lcon Lj
 
dLj;
ð2Þ
M ¼
Z1
0
lcon Lk
ð
Þ dLk
2
4
3
5
1
;
ð3Þ
where lcon Lj
 
- distribution function of contrast for the initial image, M - normalizing
coefﬁcient, constant for current image.
The distribution function lcon Li
ð Þ of contrast is the assessment of averaged contrast
of all image elements (objects and background) relative to the value Li of preset
adaptation level.
In this case, the distribution function lcon Li
ð Þ of contrast for the initial image is
deﬁned as:
lcon Li
ð Þ ¼
Z1
0
u C Li; Lj




 qbou Li; Lj


dLj;
ð4Þ
where C Li; Lj


- contrast value for two image elements i and j, u 
ð Þ - correction
function for contrast values of image elements, qbou Li; Lj


- two-dimensional distri-
bution of brightness on the boundaries of image elements.
Expressions (1)–(4) describe the proposed method of image contrast enhancement.
However, it should be noted that for the practical implementation of the proposed
method, it is necessary to solve a number of rather complicated problems.
In particular, it is necessary to solve the problems of choosing the contrast deﬁ-
nition for two image elements (the contrast kernel), of choosing of the weighting
function for contrast values and also of choosing of the method to estimate the
parameters of the two-dimensional distribution of brightness values at the boundaries
of image elements.
To demonstrate the possibilities and limitations of the proposed method of image
enhancement, let us consider a concrete example of its implementation.
In expression (4), the contrast of a pair of image elements (object and background)
was deﬁned in a generalized form.
Various approaches to measuring the contrast value for the two image elements are
known at present.
To demonstrate the implementation of the proposed method, it is proposed to
consider the three most well-known contrast deﬁnitions for two elements of the image,
namely:
(1) the weighted contrast [1]:
Adaptive Enhancement of Monochrome Images
617

Cwei Li; Lj


¼ Li  Lj
Li þ Lj
;
ð5Þ
(2) the relative contrast [5]:
Crel Li; Lj


¼
Li  Lj
max Li; Lj

 ;
ð6Þ
(3) the absolute contrast, invariant to linear transformations [8]:
Cabs Li; Lj


¼
Li  Lj
Lmax  Lmin
;
ð7Þ
where Lmin; Lmax - minimum and maximum brightness value of the initial image.
The deﬁnitions (5)–(7) of the contrast of the image elements are called the contrast
kernels [5] and are the basis for calculating the distribution function of image contrast.
The correction function u C
ð Þ provides a redistribution of contrast between
low-contrast and high-contrast objects in the image by increasing of contrast values for
low-contrast objects and limiting the contrast values for objects with excessively high
contrast.
The correction function u C
ð Þ provides a predominant increasing of contrast for
low-contrast objects as compared to objects with high contrast when forming a dis-
tribution function of image contrast lcon(L).
To demonstrate the capabilities of the proposed method, the correction function of
contrast values u C
ð Þ can be deﬁned as:
u C Li; Lj




¼ min C Li; Lj



c; d


;
ð8Þ
where c - exponent, parameter; d - threshold value, parameter.
To estimate the distribution parameters of brightness on the boundaries of image
elements it is necessary to solve the problem of ﬁnding the boundaries of contiguous
image elements, which in itself is quite a complex and resource intensive task.
For the case where image elements are independent events in relation to each other,
it can be suggested that the distribution of brightness values at the boundaries of image
elements has the form:
qbou Li; Lj


¼ N  p Li
ð Þbp Lj
 b;
ð9Þ
N ¼
Z1
0
p Lk
ð
ÞbdLk
0
@
1
A
2
;
ð10Þ
618
E. Yelmanova and Y. Romanyshyn

where p (L) - probability density function of brightness of initial image, b - exponent,
parameter, N - normalizing coefﬁcient, constant for current image.
The proposed method of image contrast enhancement is deﬁned in accordance with
(1)–(4).
Research of the effectiveness of the proposed method was carried out through
comparison with well-known histogram-based methods of contrast enhancement.
Research of the effectiveness of the proposed and known methods were carried out by
measuring the contrast using known no-reference metrics of generalized contrast for
four groups of low-contrast monochrome images with a complex structure and
non-uniform illumination.
Comparative analysis of the effectiveness of contrast enhancement for the
well-known and proposed histogram-based method of contrast enhancement was car-
ried out in Sects. 4 and 5.
4
Research
Research was carried out through a comparative analysis of the proposed method and
four well-known histogram-based methods of contrast enhancement, namely:
(1) method of linear stretching, a = 0.01 [1];
(2) method of global histogram equalization (GHE) [2];
(3) BBHE method [6];
(4) DSIHE method [7];
(5) proposed method using weighted contrast (1)–(5), (8) and (9) with c = 1.0,
d = 1.0, b = 0.5;
(6) proposed method using relative contrast (1)–(4), (6), (8) and (9) with c = 1.0,
d = 1.0, b = 0.5.
Research of the effectiveness of the proposed and well-known methods of contrast
enhancement was carried out by measuring of contrast using known no-reference
histogram-based metrics of generalized contrast.
To measure the contrast of the processed image by its histogram, the known
no-reference metrics of generalized contrast were used, namely:
(1) generalized contrast on the basis of deﬁnition of weighted contrast [5]:
Cwei
gen ¼
Z1
0
L  L
j
j
L þ L  p L
ð Þ dL;
ð11Þ
where L - mean value of image brightness.
(2) generalized contrast on the basis of deﬁnition of relative contrast [5]:
Adaptive Enhancement of Monochrome Images
619

Crel
gen ¼
Z1
0
L  L
j
j
max L; L
ð
Þ  p L
ð Þ dL;
ð12Þ
(3) generalized contrast on the basis of deﬁnition of absolute contrast [5]:
Cabs
gen ¼
Z1
0
ðL  LÞ
LMAX þ 1
2  ðL  LÞ
LMAX  1
2



  pðLÞdL;
ð13Þ
where LMAX - maximum possible brightness value,
(4) averaged contrast on the basis of deﬁnition of weighted contrast [8]:
Cwei
ave ¼
Z1
0
Z1
0
Li  Lj


Li  Lj
 p Li
ð Þ  p Lj
 
dLidLj;
ð14Þ
(5) averaged contrast on the basis of deﬁnition of relative contrast [8]:
Crel
ave ¼
Z1
0
Z1
0
Li  Lj


max Li; Lj

  p Li
ð Þ  p Lj
 
dLidLj;
ð15Þ
(6) averaged contrast on the basis of deﬁnition of absolute contrast [8]:
Cabs
ave ¼
Z1
0
Z1
0
Li  Lj


LMAX  p Li
ð Þ  p Lj
 
dLidLj:
ð16Þ
Research of the effectiveness of the proposed and known methods were carried out
for four groups of test monochrome images with a complex structure and non-uniform
illumination.
Four complex monochrome images with small-sized objects and non-uniform
illumination were used as the initial images.
Appearance of the four initial images and their histograms is shown in Figs. 1, 2,
3 and 4.
Each group of test images consisted of the initial image and of the results of its
processing with the use of various (the most well-known and proposed) contrast
enhancement methods by histogram transformation.
The results of processing the four initial images (Figs. 1, 2, 3 and 4) with using
earlier considered methods are shown in Figs. 5, 6, 7 and 8.
620
E. Yelmanova and Y. Romanyshyn

Fig. 1. The
ﬁrst
test
image
and
its
histogram
Fig. 2. The second test image and its histogram
Fig. 4. The fourth test image and its histogram
Fig. 3. The third test image and its histogram
Adaptive Enhancement of Monochrome Images
621

Fig. 5. The results of processing for the ﬁrst initial image (Fig. 1)
622
E. Yelmanova and Y. Romanyshyn

Fig. 6. The results of processing for the second initial image (Fig. 2)
Adaptive Enhancement of Monochrome Images
623

Fig. 7. The results of processing for the third initial image (Fig. 3)
624
E. Yelmanova and Y. Romanyshyn

Fig. 8. The results of processing for the fourth initial image (Fig. 4)
Adaptive Enhancement of Monochrome Images
625

The results of contrast measurements for four groups of test images using known
no-reference metrics of generalized contrast (11)–(16) are shown in Table 1.
The results of the contrast measurements (Table 1) for each of the four groups of
test images are also shown in the form of graphs in Figs. 9, 10, 11 and 12.
Table 1. The results of measurements of generalized contrast for four groups of test images
Cwei
gen
Crel
gen
Cabs
gen
Cwei
ave
Crel
ave
Cabs
ave
1
0.247 0.370 0.103 0.264 0.371 0.074
5.a 0.345 0.471 0.128 0.383 0.492 0.095
5.b 0.288 0.403 0.500 0.387 0.500 0.334
5.d 0.496 0.636 0.267 0.469 0.574 0.176
5.e 0.560 0.675 0.518 0.578 0.669 0.328
5.f
0.193 0.294 0.235 0.273 0.391 0.173
5.g 0.202 0.305 0.242 0.284 0.403 0.178
2
0.231 0.350 0.118 0.271 0.386 0.083
6.a 0.309 0.434 0.197 0.369 0.487 0.143
6.b 0.288 0.404 0.502 0.387 0.500 0.334
6.d 0.505 0.645 0.358 0.501 0.607 0.232
6.e 0.523 0.646 0.517 0.555 0.654 0.329
6.f
0.214 0.323 0.275 0.293 0.415 0.195
6.g 0.224 0.336 0.284 0.305 0.428 0.201
3
0.284 0.414 0.216 0.328 0.451 0.156
7.a 0.388 0.515 0.226 0.457 0.576 0.165
7.b 0.289 0.405 0.502 0.388 0.502 0.335
7.d 0.432 0.568 0.388 0.484 0.597 0.259
7.e 0.469 0.600 0.517 0.527 0.634 0.333
7.f
0.240 0.354 0.327 0.326 0.450 0.233
7.g 0.253 0.369 0.337 0.339 0.464 0.239
4
0.383 0.533 0.223 0.395 0.511 0.137
8.a 0.517 0.646 0.430 0.542 0.644 0.267
8.b 0.289 0.405 0.502 0.388 0.500 0.334
8.d 0.473 0.613 0.442 0.500 0.609 0.285
8.e 0.535 0.656 0.525 0.563 0.659 0.334
8.f
0.291 0.423 0.407 0.363 0.488 0.265
8.g 0.308 0.442 0.420 0.378 0.503 0.272
626
E. Yelmanova and Y. Romanyshyn

Fig. 9. The values of contrast for the ﬁrst group of test images (Figs. 1 and 5)
Fig. 10. The values of contrast for the second group of test images (Figs. 2 and 6)
Adaptive Enhancement of Monochrome Images
627

Fig. 11. The values of contrast for the third group of test images (Figs. 3 and 7)
Fig. 12. The values of contrast for the fourth group of test images (Figs. 4 and 8)
628
E. Yelmanova and Y. Romanyshyn

5
Discussion
Analysis of the results of research shows that the methods of image processing in
automatic mode by modiﬁcation their global histogram allow essentially enhance the
contrast of low-contrast monochrome images with small-sized objects and non-uniform
illumination with acceptable level of computational costs. However, it should be noted
that the considered metrics (11) and (12), (14) and (15) using kernels of weighted (5)
and relative (6) contrast are substantially dependent on the average brightness level of
the image and give greatly overstated values of contrast (Figs. 9, 10, 11 and 12).
The results of the research show that the image processing techniques on the basis
of the technology of histogram equalization and its modiﬁcations (GHE, BBHE and
DSIHE methods) ensure the maximum increase of the generalized contrast of the initial
image. A main shortcoming for the majority of known histogram-based techniques of
contrast enhancement is a possible reduction of contrast and the disappearance of
small-sized objects [2]. The results of the experimental research have shown that
methods on the basis of technology of the histogram equalization and its modiﬁcations
(GHE, BBHE and DSIHE methods) in some cases can lead to a signiﬁcant decrease of
contrast or to the disappearance of small-size objects in image (Figs. 5(b), (d), (e), 6(d),
(e), 7(b), (d), 8(b), (d) and (e). Possible signiﬁcant reduction of contrast of small-sized
objects and excessive increasing of contrast for large-sized objects in image signiﬁ-
cantly limits the use the technologies of histogram equalization (in particular, GHE,
BBHE and DSIHE methods) for processing of complex images in automatic mode. The
effectiveness of methods of linear stretching and its modiﬁcations essentially depends
on the dynamic range of brightness of initial image and is very low for the complex
images with non-uniform illumination (Figs. 5(a), 6(a), 7(a) and 8(a). The results of the
experimental research have shown that proposed method pro-vides the increase of
efﬁciency of contrast enhancement in automatic mode (by an average of 51%–98%) for
all initial images without reducing the contrast of small-sized objects on image
(Figs. 5(f), (g), 6(f), (g), 7(f), (g), 8(f) and (g)).
6
Conclusion
Operative images quality enhancement in automatic mode is extremely relevant for the
vast majority of practical applications in imaging, image processing and analysis.
The development of new effective techniques for real-time image contrast
enhancement in automatic mode (with acceptable level of computational costs) is
especially relevant at present. Particularly relevant is the addressing of the problem of
effective contrast enhancement of complex images with low-contrast and small-sized
objects and non-uniform illumination.
In this paper, the problem of development of histogram-based methods of contrast
enhancement in automatic mode for complex monochrome images with small-sized
objects and non-uniform illumination was considered. In this paper it is shown that
histogram-based methods for contrast enhancement allow signiﬁcantly increase the
contrast of low-contrast monochrome images with small-sized objects and non-uniform
illumination with an acceptable level of computational costs.
Adaptive Enhancement of Monochrome Images
629

However, the known technologies for image contrast enhancement by histogram
equalization have several signiﬁcant disadvantages, which signiﬁcantly limit their
practical use. A main shortcoming for the majority of known histogram-based tech-
niques of contrast enhancement is a possible reduction of contrast and the disappear-
ance of small-sized objects with low contrast which is unacceptable to image
processing in automatic mode.
The histogram-based method of image contrast enhancement in automatic mode on
the basis of the estimations of parameters of distribution of brightness values at the
boundaries of objects and background for the various deﬁnitions of contrast kernels
was proposed to address these shortcomings. The proposed histogram-based method of
adaptive contrast enhancement provides an efﬁcient redistribution of the contrast of
image objects by maximizing the contrast for poorly distinguishable low-contrast
objects with the proviso that the contrast of high-contrast objects is guaranteed to
remain at a level which ensures their reliable detection and recognition.
Research
of
the
effectiveness
of
the
proposed
and
several
well-known
histogram-based methods of contrast enhancement was carried out using the known
no-reference metrics of generalized contrast for four groups of complex monochrome
images with small-sized objects and non-uniform illumination.
The results of research conﬁrm the effectiveness of the proposed method for con-
trast enhancement in automatic mode for low-contrast monochrome images with
small-sized objects and non-uniform illumination.
References
1. Gonzalez, R.C., Woods, R.E.: Digital Image Processing, 2nd edn. Prentice Hall, New Jersey
(2002)
2. Pratt, W.K.: Digital Image Processing: PIKS Inside, 3rd edn. Wiley, New York (2001)
3. Wang, Z., Bovik, A.C.: Modern image quality assessment. In: Synthesis Lectures on Image,
Video, and Multimedia Processing, Vol. 2, no. 1, pp. 1–156. Morgan and Claypool
Publishers, New York (2006). https://doi.org/10.2200/S00010ED1V01Y200508IVM003
4. Kosarevych, R.J., Rusyn, B.P., Korniy, V.V., Kerod, T.I.: image segmentation based on the
evaluation of the tendency of image elements to form clusters with the help of point ﬁeld
characteristics. Cybern. Syst. Anal. 1, 704–713 (2015)
5. Vorobel, R.A.: Logarithmic Image Processing. Naukova Dumka, Kyiv, Ukraine (2012). (In
Ukrainian)
6. Kim, Y.T.: Contrast enhancement using brightness preserving bi-histogram equalization.
IEEE Trans. Consum. Electron. 43(1), 1–8 (1997)
7. Wang, Y., Chen, Q., Zhang, B.: Image enhancement based on equal area dualistic sub-image
histogram equalization method. IEEE Trans. Consum. Electron. 45(1), 68–75 (1999)
8. Yelmanova, E., Romanyshyn, Y.: No-reference contrast assessment by image histogram. In:
Proceedings of 14th International Conference CADSM 2017, pp. 148–152. IEEE, Lviv,
Ukraine (2017)
630
E. Yelmanova and Y. Romanyshyn

Secure Routing in Reliable Networks:
Proactive and Reactive Approach
Oleksandra Yeremenko(&)
, Oleksandr Lemeshko
,
and Anatoliy Persikov
Kharkiv National University of Radio Electronics, 14 Nauka Ave.,
Kharkiv, Ukraine
oleksandra.yeremenko.ua@ieee.org,
oleksandr.lemeshko@nure.ua,
persikovanatoliy@gmail.com
Abstract. In this paper, the approach to providing a given level of information
security for multipath routing of conﬁdential messages in a network is considered.
A method for providing secure routing over overlapping paths is developed and
belongs to the class of proactive solutions for ensuring a given level of infor-
mation security. The analysis has shown that using the proposed method within
the presented calculated examples can improve the probability of compromising
transmitted messages at average from 5–10% to 25–50% due to the possibility of
using composite paths that are one of the subclasses of overlapping paths.
A method of Secure Fast ReRouting (S-FRR) of messages in the network has been
synthesized, the novelty of which lies in the fact that it focuses on the imple-
mentation of both proactive and reactive secure routing conﬁdential messages. In
this case, the proactive nature of the solutions is conditioned by the calculation of
the set of primary composite paths forming the primary multipath, along which
parts of the conﬁdential message are transmitted. However, in the case of vio-
lation of the information security requirements in the network caused by the
increased probability of compromising one or multiple composite paths con-
stituent the primary multipath, the messages will be transmitted over the calcu-
lated set of the backup composite paths determining the backup multipath. Within
the framework of the proposed S-FRR method, it is possible to protect both the
primary multipath as a whole and one or several precomputed composite paths
included in this primary multipath. The developed methods of secure routing can
be used as the basis for new network protocols for routing and fast rerouting for
multipath transmission of parts of a conﬁdential message with speciﬁed
requirements regarding the probability of its compromise in the network.
Keywords: Information security  Reliable network  Secure routing
Probability of compromise  Link  Multipath  Composite path
Secure fast rerouting
1
Introduction
As shown by the analysis, one of the most important tasks, which is regulated by
standards of next generation networks (NGN) construction, is the problem of the
implementing information security functions. In accordance with the requirements of
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_44

the International Telecommunication Union (ITU) standards, provision of information
security is carried out within three levels: security of infrastructure, security of services
and security of applications [1]. At the same time, the effectiveness of the top two
levels is entirely determined by the efﬁciency of functioning of the infrastructure level
security means, the main tasks of which are: ensuring security at the level of network
elements (switches, routers, servers), links, and routes consisting of links in general.
As a rule, the security level of network elements is evaluated using such an important
indicator as the probability of compromise, where compromising means the fact of
unauthorized access to protected information, as well as the implementation of a sus-
picion of such access. At the same time, the security services should be provided by
appropriate levels of OSI model protocols [1–3]. In turn, the security at the network level
should be maintained and provided by routing protocols. With the help of routing tools,
as shown in [4–8], secure delivery of various conﬁdential information, in particular
session keys, information on authentication, critical user data, etc., can be carried out.
To ensure a given level of information security in practice, depending on the time
of reaction to a possible compromise of communication links and network fragments,
both proactive and reactive means should be complementarily applied, including those
related to routing solutions. Proactive means are used, as a rule, at the stage of pre-
venting the compromise of transmitted messages or minimizing the probability of its
occurrence [9]. Reactive means are used in those cases when the security of the
transmitted data is violated and, for example, it is important for the means of routing to
quickly restore the required level of security.
An example of a proactive approach can be a solution related to the provision of a
given level of security. In this case, transmission of messages divided into parts according
to the Shamir’s scheme from the source to the destination is organized using the multipath
routing with balancing the number of parts over non-overlapping paths [10]. When
changing the network state that causes a violation of the security level of the transmitted
messages, it is important to determine the operational order of changing the set of paths
used to transfer parts of conﬁdential messages. Therefore, fast re-routing solutions can be
considered as elements of the reactive approach while ensuring secure routing.
It is known that the effectiveness of protocol solutions is largely determined by the
quality of the mathematical models and methods for calculating the desired paths,
which they are based. The use of theoretically grounded solutions allows to signiﬁ-
cantly enhance the functionality of the routing protocol in relation to the network tasks
assigned to it. Therefore, we will propose theoretical solutions presented by mathe-
matical models and methods of secure routing and combine the capabilities of both
proactive and reactive approaches when implementing secure routing of messages in
the network.
2
Method of Secure Routing of Messages Using
Non-overlapping Paths: Proactive Approach
As the conducted analysis has shown [10–12], the possibility of calculating probability
of compromising the message being transmitted in the network is mostly determined by
the features of the structural network construction and types of paths used. It is
632
O. Yeremenko et al.

common knowledge that a set of paths in the network could be divided into two
subsets: a subset of non-overlapping paths and a subset that allows nodal or link
overlapping [13]. Moreover, non-overlapping paths mean only those routes with
common source-destination nodes. If the paths contain at least one common node
and/or link, then they are called overlapping paths. In addition, if the paths have
common nodes, they are called as the paths that are overlapping over nodes. If they
have common links, they are called as the paths overlapping over links.
One of the directions for ensuring a given level of information security in com-
munication networks is the implementation of a mechanism based on multipath routing
of the transmitted message previously divided into parts in accordance with the Sha-
mir’s scheme [10]. As a result of using such a scheme, it is possible to reduce the
probability of compromising the transmitted message, because an attacker in order to
compromise the message, must compromise all paths, usually non-overlapping, over
which parts of the divided message are transmitted.
Currently there are known analytical expressions for calculating the probability of
compromising the message transmitted in parts over a set of non-overlapping paths
[10–12]. In addition, it is assumed that the following initial data are known:
Secure Routing in Reliable Networks
633

In this case, during the multipath routing and balancing the number of message
parts over paths, it is necessary to ensure a speciﬁed level of information security,
provided by, for example, probability of compromising the message Pmsg being
transmitted:
Pmsg  cP:
ð1Þ
The following considerations assume that the source and the destination are secure,
i.e. the probabilities of compromising the source node and the destination node are
equal to zero. It is assumed in [10, 12] that if an element (node, link) of the path has
been compromised, then all the fragments sent over this element will also be com-
promised. Then the probability of compromising the i th path consisting of Mi elements
can be calculated using the expression
pi ¼ 1  ð1  p1
i Þð1  p2
i Þ. . .ð1  pMi
i Þ ¼ 1 
Y
Mi
j¼1
ð1  p j
i Þ:
ð2Þ
Besides, during the calculation of the control variables ni (i ¼ 1; M) regulating the
allocation of the message parts over the non-overlapping paths the following condition
[10, 12] must be met:
X
M
i¼1
ni ¼ N:
ð3Þ
In the case of Shamir’s scheme with redundancy when T\N the condition below
must be satisﬁed
N  ni \ T;
i ¼ 1; M


;
ð4Þ
while when T ¼ N the following conditions must be met in the non-redundant sharing
scheme
1  ni  T  1;
i ¼ 1; M


:
ð5Þ
The condition (4) ensures that in the case of compromising all the paths except the i
th path, an adversary cannot reconstruct the whole message.
One of the main conditions that must be fulﬁlled in secure routing is that the
probability of message compromising, when it is transmitted over the network, should
not exceed the set allowable value (1). Then, for instance, the probability of com-
promising the message divided into N parts according the Shamir’s scheme ðN; NÞ and
transmitted over M paths, is determined by the expression [10].
Pmsg ¼
Y
M
i¼1
pi:
ð6Þ
634
O. Yeremenko et al.

The advantages of the described method include the fact that using a set of
non-overlapping paths when transferring parts of a conﬁdential message greatly sim-
pliﬁes the process of calculating the probability of its compromising in the network
using expressions (2)–(6). In this case, the condition (1) is completely determined by
the parameters of the non-overlapping paths used, and the task of balancing the parts of
the message along these paths is already secondary and lies in fulﬁlling the conditions
(3)–(5). Therefore, if the use of an accessible set of non-overlapping paths does not
allow satisfying the requirement (1), then the task of ensuring the given level of
information security remains unsolved.
It can be intuitively assumed that the use of overlapping paths on the same network
topology could lead to an improvement in the probability of compromising the
transmitted message and, as a result, to the successful solution of the task. However, as
the analysis has shown, in the case of using overlapping paths in the network, the
procedure for calculating the probability of compromising a message is signiﬁcantly
complicated, and sometimes becomes impossible (in the analytical form) [10–12]. In
this regard, the relevant task seems to be ﬁnding a trade-off solution concerned with the
deﬁnition of such a class of overlapping routes, for which it is possible to analytically
calculate and hence to control the probability of compromising the message being
transmitted.
3
Method of Secure Routing of Messages Using Overlapping
Paths: Proactive Approach
The use of expressions (2) and (6) yields adequate results for the case of nodal
overlapping of paths, but only if there is a true hypothesis that the probabilities of
compromising all network nodes are equal to zero, i.e. only links can be compromised.
This is true for some class of wireless networks. In a more general case, any network
node subject to network attacks and other impacts can also be always represented by a
conditional communication link, where the probability of its compromise is equal to the
probability of compromising the simulated node. The method of calculating paths with
a nodal overlapping is proposed, for example, in [13].
In the paper [14], an attempt has been made to expand the class of overlapping
paths, in which it is still possible to analytically estimate the probability of compro-
mising the transmitted message. This will allow to create conditions for monitoring
compliance with the requirements for the level of information security (1) under
implementation of overlapping paths.
In this context, we must additionally enter two more types of paths: simple and
composite. A simple path is always formed by the series connection of communication
links of the network, and the probability of its compromising is calculated using the
expression (2). In turn, composite paths represent more complex structural forms
including the overlapping of simple paths. In this regard, we reﬁne the previously
introduced notations and introduce some additional ones:
Secure Routing in Reliable Networks
635

In order to provide the formulation of the expression for calculating the probability
of compromising a composite path in the analytical form during secure routing, the
path must contain two types of fragments consisting of a series (Fig. 1(a)) or parallel
(Fig. 1(b)) connection of links. Figure 1(c) presents an example of a composite path
with a series connection of two network fragments. The ﬁrst fragment is represented by
the parallel connection of links, and the second is a series one.
Figure 1(c) shows the structure of the network containing one composite path,
which includes links of two overlapping simple paths. The ﬁrst simple path is repre-
sented by the nodes 1 ! 2 ! 3 ! 4, the second one – by the nodes 1 ! 3 ! 4. On
the other hand, this composite path consists of two subsequently connected fragments.
The ﬁrst fragment includes a parallel connected link 1 ! 3 and a sequence of links
1 ! 2 and 2 ! 3, whereas the second fragment is represented by the link 3 ! 4.
Then the probability of compromise of the composite path (Fig. 1(c)) is calculated
according to the expression
636
O. Yeremenko et al.

~p1 ¼ 1  ð1  ~p1
1Þð1  ~p2
1Þ;
ð7Þ
where the probabilities of compromising the ﬁrst and second fragments are determined
via the probabilities of compromise of their links:
~p1
1 ¼ 1  ð1  p1
1Þð1  p2
1Þ


p3
1; ~p2
1 ¼ p4
1:
Thus, in general, the probability of compromising the i th composite path consisting
of ~Mi fragments can be calculated according to the following expression:
~pi ¼ 1 
Y
~Mi
j¼1
ð1  ~p j
i Þ:
ð8Þ
In case if a single composite path is used to deliver the message, the probability of
compromising this message is determined by the probability of compromising this
composite path. In a more general case, when parts of a message are transmitted over a
set of non-overlapping composite paths, the following expression must be used to
calculate the probability of compromising a message:
Fig. 1. The examples of fragment types and composite path: (a) series, (b) parallel, and
(c) composite path.
Secure Routing in Reliable Networks
637

~Pmsg ¼
Y
~M
i¼1
~pi;
ð9Þ
which is a modiﬁcation of the formula (6).
The example of such a case is given in Fig. 2, when in order to transmit message
parts two non-overlapping paths are used:
• the ﬁrst path is composite and it includes the following communication links
1 ! 2, 2 ! 3, 2 ! 4, 3 ! 5, 4 ! 5, 5 ! 7;
• the second path is simple and it contains the communication links 1 ! 6, 6 ! 7.
In its turn, the ﬁrst (composite) path includes three series-connected network
fragments:
• the ﬁrst fragment is presented by the link 1 ! 2;
• the second fragment is based on parallel connection of the links 2 ! 3, 3 ! 5 and
2 ! 4, 4 ! 5;
• the third fragment is presented by the link 5 ! 7.
Then for the considered network structure (Fig. 2), the probability of compromising
the message when using two different above-described paths will be determined as
follows:
Pmsg ¼ ~p1  ~p2;
ð10Þ
Fig. 2. The example of using two paths: the composite and the simple ones.
638
O. Yeremenko et al.

where the probabilities of compromising composite and simple paths 1 and 2 respec-
tively are expressed through the corresponding probabilities of compromising their
fragments and links as
~p1 ¼1  1  ~p1
1


1  ~p2
1


1  ~p3
1


¼1  1  p1
1


1  1  1  p2
1


1  p3
1




 1  1  p4
1


1  p5
1






1  p6
1


; ð11Þ
~p2 ¼ 1  ð1  ~p1
2Þð1  ~p2
2Þ ¼ 1  ð1  p1
2Þð1  p2
2Þ:
ð12Þ
In the general case, one composite path may comprise several series-connected
fragments with parallel connection of communication links. Let us denote the maxi-
mum number of parallel connected links hi over all fragments of the i th composite
path. Then condition (5) takes the form
hi  ni  T  1;
i ¼ 1; ~M


;
ð13Þ
and its fulﬁllment will thus allow to distribute parts of the message over parallel links
of network fragments of composite paths so that in each of them a nonzero number of
such parts of the message is transmitted and expressions (8) and (9) are valid.
In addition, the condition (4) taking into account the composite nature of the used
paths will take the form:
N  ni\T;
i ¼ 1; ~M


:
ð14Þ
In this regard, the basis of the proposed method of secure routing for the parts of the
transmitted message over a set of overlapping paths can be the solution of the opti-
mization problem associated with the use of the optimality criterion
min
ni
Y
~M
i¼1
~piðniÞ;
ð15Þ
which ensures the minimization of the probability of compromising the transmitted
message. In addition, the constraints (8), (13) or (14) are imposed on the control
variables depending on the Shamir’s scheme used, as well as the analog of condition
(3) represented by the equality
X
~M
i¼1
ni ¼ N:
ð16Þ
The formulated optimization problem belongs to the class of Nonlinear Integer
Programming problems. The variables ni to be calculated are integer, and the optimality
criterion (15) is nonlinear.
Secure Routing in Reliable Networks
639

The proposed method for secure routing of messages across multiple overlapping
paths is a tool for the Proactive Approach to improve the level of information security.
This is determined by the fact that on the basis of constant analysis of the network state,
its structure and parameters of security of the communication links, and also during the
optimal balancing of the conﬁdential message parts over the overlapping paths, all
available capabilities are realized in order to minimize the probability of compromising
transmitted data.
4
Numerical Analysis of the Proposed Method for Secure
Routing of Messages Using Overlapping Paths
4.1
Numerical Study of Using the Unique Composite Path
Using the proposed approach, we will analyze the effect of the security parameters of
individual links and network fragments inﬂuence on the probability of compromising
the message. In addition, we will estimate the gain on compromise probability obtained
using the approach proposed in Sect. 3 in comparison to the known one described in
Sect. 2. First, the speciﬁc features of calculating the probability of compromising a
message will be demonstrated for the network structure shown in Fig. 1(c). The values
given in Table 1 have been used as the initial data. The last row in Table 1 shows the
result of the solution of the stated optimization problem associated with minimizing the
expression (15) under the constraints (8), (13) or (14), (16) when implementing the
Shamir’s scheme (10, 10) and h1 ¼ 2.
When transmitting the message from the ﬁrst to the fourth node, its parts are
directed along the two overlapping routes: 1 ! 2 ! 3 ! 4 and 1 ! 3 ! 4, i.e. the
3 ! 4 link has been common to them. In the course of the study, it was assumed that
the probabilities of compromising the ﬁrst and second links have been ﬁxed and
amounted to 0.1 and 0.2, respectively; the probabilities of compromising the third and
fourth links varied from 0 to 1.
The probability of compromise has been calculated for two cases:
• in the ﬁrst case, to calculate the probability of message compromising (~Pmsg) we
used the approach proposed in Sect. 3 based on expressions (7)–(9), (14);
• in the second case, for calculations we used the approach (2)–(6) given in Sect. 2,
which assumes using of only non-overlapping paths. In respect to Fig. 1(c) this
assumes using either the path 1 ! 3 ! 4, to which the probability of compromise
P1
msg corresponded to, or the path 1 ! 2 ! 3 ! 4, the compromise of which was
estimated by the probability P2
msg.
Table 1. Initial research data for the case of using the unique composite path.
Link
1 ! 2 2 ! 3 1 ! 3 3 ! 4
Link number in path
1
2
3
4
Probability of compromising the link 0.1
0.2
0  1
0  1
Number of message parts
5
5
5
10
640
O. Yeremenko et al.

Fig. 3. Dependence of the probability of compromising the message transmitted along different
types of paths for the network structure, as shown in Fig. 1(c).
Secure Routing in Reliable Networks
641

Then, Fig. 3 shows the nature of the dependence of the probability of compro-
mising a message transmitted along different types of paths for the network structure
given in Fig. 1(c) on the values of the probability of compromising the fourth link p4
1
(plotted along the abscissa axis). Each of the set of lines corresponded to its value of the
probability of compromising the third link (p3
1). As shown in Fig. 3, with increasing p3
1
and p4
1 the probability of compromising the transmitted message using the composite
path and simple path 1 ! 3 ! 4 has always increased, but the nature of the depen-
dence when using paths of different types (overlapping and non-overlapping) was
signiﬁcantly different. Given that the simple path 1 ! 2 ! 3 ! 4 did not contain the
third link (Fig. 1(c)), the probability of its compromise depended only on p4
1 and did
not depend on p3
1 (Fig. 3(c)).
To quantify the gains from the probability of compromising messages due to the
implementation of the proposed method based on the use of overlapping paths, in
comparison with earlier known solutions, the following expressions were used:
D1 ¼
P1
msg  ~Pmsg
P1
msg
 100%;
ð17Þ
and
D2 ¼
P2
msg  ~Pmsg
P2
msg
 100%:
ð18Þ
In accordance with these expressions, the graphs presented in Fig. 4 have been
obtained.
Based on the results presented in Fig. 4, we can conclude that the use of the
proposed method for secure routing of message parts along two overlapping simple
paths, united in a single composite path, has led to an improvement in the probability of
compromising the transmitted message:
• in comparison to implementation of one simple path 1 ! 3 ! 4 at average by 20–
55% under p4
1 ¼ 0:1  0:3; and by 5–20% under p4
1 ¼ 0:5  0:9 (Fig. 4(a));
• in comparison to implementation of one simple path 1 ! 2 ! 3 ! 4 at average
by 5–50% under p4
1 ¼ 0:1  0:3; and by 3–15% under p4
1 ¼ 0:5  0:9 (Fig. 4(b)).
The gain on the probability of compromise of the transmitted message decreased at
p4
1 ! 1 because all the considered routes, both simple and composite, passed through
this link.
4.2
Numerical Study of Using the Two Different Types
of Non-overlapping Paths
Similarly, we perform a comparative analysis of the effectiveness of the proposed
method (see Sect. 3) and the previously known solutions (see Sect. 2) for the network
structure presented in Fig. 2. Using the method suggested in the paper, the probability
642
O. Yeremenko et al.

of message compromise (~Pmsg) transmitted using all available communication links
included into one composite and one simple path was estimated.
Using the previously known method (2)–(6), the probability of compromising the
message was estimated, the parts of which were transmitted using two non-overlapping
simple paths. Two possible cases of a combination of the choice of such paths were
considered. In the ﬁrst case, the paths 1 ! 2 ! 4 ! 5 ! 7 and 1 ! 6 ! 7 were
used, to which the probability of compromising the message P1
msg corresponded. In the
second
case,
parts
of
the
message
were
transmitted
over
another
pair
of
non-overlapping paths: 1 ! 2 ! 3 ! 5 ! 7 and 1 ! 6 ! 7, to which the proba-
bility of compromise P2
msg corresponded.
Fig. 4. The dependence of the gain on the probability of compromise from the use of the
proposed method in comparison with the previously known solutions for the network structure
given in Fig. 1(c).
Secure Routing in Reliable Networks
643

The indicator of the effectiveness of secure routing was again the probability of
compromising transmitted message. In the course of the research, the impact of the
probabilities of compromising the links 2 ! 4 and 1 ! 6, varying from 0 to 1, on the
effectiveness were analyzed. The link 2 ! 4 was a part of the composite path under the
number four, and the link 1–6 had the ﬁrst number in the structure of the simple path
(Table 2). Table 2 also shows the probabilities of compromising all the links included
in these two paths.
The last line in Table 2 shows the result of the solution for the stated optimization
problem associated with minimizing the expression (15) under the constraints (8), (13)
or (14), (16) in the Shamir’s scheme (10, 10) and h1 ¼ 2, h2 ¼ 1. On the ﬁrst (com-
posite) path and the second (simple) path, 5 parts of the original message were
transmitted, i.e. n1 ¼ n2 ¼ 5.
Figure 5 shows the nature of the dependence of the probability of compromising
the message transmitted along different types of paths for the network structure, as
shown in Fig. 2, on the values of the probability of compromising the fourth link in the
composite path p4
1 (plotted along the abscissa axis). Each of the set of lines in Fig. 5
corresponded to its value of the probability of compromising the ﬁrst link of the simple
path (p1
2). As shown in Fig. 5(a) and (b), with increasing p1
2 and p4
1 the probability of
compromising the transmitted message using the composite path and the simple path
1 ! 2 ! 4 ! 5 ! 7 has always increased. Given that the simple paths 1 ! 2 ! 3
! 5 ! 7 and 1 ! 6 ! 7 did not contain the link 2 ! 4 (Fig. 2), the probability of
their compromising depended only on p1
2 and did not depend on p4
1 (Fig. 5(c)).
A quantitative analysis of the gain in the probability of compromising messages
(Fig. 6) from the implementation of the proposed method based on the use of over-
lapping paths is performed in comparison with known solutions using expressions (17)
and (18).
As shown in Fig. 6, the gain according to the formulas (17) and (18) on the
probability of compromising the message transmitted along the paths of various types
depends only on the security parameters of the links entering the composite path. In this
case, this is the link 2 ! 4, which is the fourth link of the ﬁrst (composite) path with
the probability of compromise p4
1. The gain (17) and (18) did not depend (Fig. 6) on the
value of the probability of compromising the link 1 ! 6 (p1
2 ¼ 0:1  0:9), which is the
ﬁrst link of the second (simple) path.
Table 2. Initial research data for the case of using two different types of non-overlapping paths.
Path number
1 (Composite)
2 (Simple)
Link
1 ! 2 2 ! 3 3 ! 5 2 ! 4 4 ! 5 5 ! 7 1 ! 6 6 ! 7
Link number in path
1
2
3
4
5
6
1
2
Probability of
compromising the link
0.2
0.1
0.1
0  1
0.1
0.2
0  1
0.2
Number of message parts
5
3
3
2
2
5
5
5
644
O. Yeremenko et al.

Fig. 5. Dependence of the probability of compromising the message transmitted along different
types of paths for the network structure, as shown in Fig. 2.
Secure Routing in Reliable Networks
645

Based on the results presented in Fig. 6, it can also be concluded that the use of the
proposed method of secure routing has led to an improvement in the probability of
compromising the transmitted message:
• in comparison to implementation of two non-overlapping simple paths 1 ! 2
4 ! 5 ! 7 and 1 ! 6 ! 7 at average by 20−33% under p4
1 ¼ 0:1  0:3; and by
40–50% under p4
1 ¼ 0:5  0:9 (Fig. 6(a));
• in comparison to implementation of two non-overlapping simple paths 1 ! 2
3 ! 5 ! 7 and 1 ! 6 ! 7 at average by 16–20% under p4
1 ¼ 0:1  0:3; and by
3–12% under p4
1 ¼ 0:5  0:9 (Fig. 6(b)).
Fig. 6. The dependence of the gain on the probability of compromise from the use of the
proposed method in comparison with the previously known solutions for the network structure
given in Fig. 2.
646
O. Yeremenko et al.

Thus, with increasing probability of compromising the link included in the com-
posite path, i.e. at p4
1 ! 1, the gain on the probability of compromising the transmitted
message increased in comparison with the use of simple paths containing the same link
(Fig. 6(a)). After all, under p4
1 ¼ 1 the whole simple path 1 ! 2 ! 4 ! 5 ! 7 will
be compromised, and the use of the composite path including the network fragment
with parallel connection of links allows to avoid this phenomenon.
On the other hand, if the link 2 ! 4 was compromised, i.e. p4
1 ¼ 1, then the
composite path actually lost its advantage, turning de facto into the simple path
1 ! 2 ! 3 ! 5 ! 7. This led to a decrease in the gain on the probability of com-
promising the message from the application of the proposed method in comparison
with the method of secure routing over non-overlapping paths (Fig. 6(b)).
5
Method of Secure Fast ReRouting of Messages Over
Composite Paths: Proactive and Reactive Approaches
In order to extend the functionality of the means of secure routing, it is important for
the proposed method to implement the principles of not only the proactive but also the
reactive approach. In other words, in the structure of the method of secure routing it is
important to provide procedures for prompt response to possible violations of the
information security level. Currently, routing protocols react to possible changes in the
network state in the time scale of tens of seconds, which is not always acceptable both
in terms of the required level of quality of service and information security.
Therefore, methods and protocols of fast rerouting are increasingly used in practice,
during which two types of paths are precomputed: primary and backup. In this case, the
use of each type separately should lead to satisfaction of the requirements regarding the
level of information security. Then, if the main path fails, the transmitted data will be
routed almost instantaneously (with a delay of tens of milliseconds) using backup
routes. Of course, the primary and backup routes should not overlap on the failed
network elements (routers, communication links or routes in general) [15–17]. The
causes of denial of service can be both overload and breakdown of network equipment,
and the consequences of network attacks and the impact of malicious software
(viruses).
Then, within the framework of Secure Fast ReRouting (S-FRR), the use of multiple
primary paths refers to Proactive Approach solutions for providing a given level of
information security, and the application of backup paths meets the requirements of
Reactive Approach. At the same time, in the framework of the proposed method, the
calculation of the set of primary and backup paths should be carried out as consistently
as possible in order to improve the efﬁciency of the ﬁnal solutions.
Division of paths into primary and backup means that parts of the message will not
be transmitted over all accessible composite and simple paths, only in their limited
number, but with the fulﬁllment of the requirements for the probability of compromise
(1).
Considering the fact that in order to increase the level of information security of
transmitted messages, it is necessary to implement multipath routing of their parts,
Secure Routing in Reliable Networks
647

primary and backup paths will be represented by not individual composite or simple
paths, but by the multipaths formed by them. In this case, the composition of both the
primary and backup multipath can include several composite and (or) simple paths.
In the calculation of the backup multipath, it is proposed to implement the fol-
lowing two protection schemes for the primary multipath:
• protection scheme for the primary multipath as a whole, in which the primary and
backup multipaths do not overlap either by nodes or by communication links;
• protection scheme for a single path (composite or simple) of the primary multipath
where the backup multipath should not contain the protected path of the primary
multipath.
The implementation of each of the protection schemes is aimed at restoring a given
level of information security by eliminating the primary multipath and moving to the
use of a backup multipath. In this regard, we reﬁne the previously set notations and
introduce some additional ones:
In accordance with the above notations, in order to calculate the probability of
compromising the message transmitted in parts over a set of composite paths, it is
necessary, by analogy with formulas (1) and (9), to use expressions
648
O. Yeremenko et al.

~Ppr
msg ¼
Y
~M
i¼1
~ppr
i
and ~Pb
msg ¼
Y
~M
i¼1
~pb
i :
ð19Þ
It should be noted that the probabilities of compromising network fragments ~ppr
i
and ~pb
i are the function of the number of message parts transmitted in them, i.e. from ni
and ni. Then, taking into account (8), we have the conditions
~ppr
i ¼
1  Q
~Mi
j¼1
ð1  ~p j
i Þ; ni [ 0;
1; ni ¼ 0;
8
<
:
and ~pb
i ¼
1  Q
~Mi
j¼1
ð1  ~p j
i Þ; ni [ 0;
1; ni ¼ 0:
8
<
:
ð20Þ
The systems (20) can be rewritten as follows:
~ppr
i ¼ 1  H0ðniÞ
Y
~Mi
j¼1
ð1  ~p j
i Þ and ~pb
i ¼ 1  H0ðniÞ
Y
~Mi
j¼1
ð1  ~p j
i Þ;
ð21Þ
where H0 is the Heaviside function, which, taking into account expression (20), is
calculated as follows
H0ðnÞ ¼
0; n ¼ 0;
1; n [ 0:

The conditions (16) due to realization of S-FRR are added by the expression
X
~M
i¼1
ni ¼ N:
ð22Þ
In turn, to protect the main multipath, by analogy with [15, 16], it is necessary to
ensure the following condition:
X
~M
i¼1
ni ni ¼ 0:
ð23Þ
If it is necessary to protect the individual i th composite path, it is important to
ensure fulﬁllment of the condition
ni ni ¼ 0;
ð24Þ
which is also nonlinear (bilinear).
In order to meet the requirements regarding the probability of compromising the
messages transmitted using both the primary and backup multipath, the following
conditions are introduced by analogy with (1):
Secure Routing in Reliable Networks
649

Ppr
msg  cP and Pb
msg  cP:
ð25Þ
Therefore, the basis of the developed S-FRR method can be the solution of the
optimization problem of Nonlinear Integer Programming with the optimality criterion
J ¼
X
~M
i¼1
~pini þ
X
~M
i¼1
~pini;
ð26Þ
and the constraints represented by the conditions (13), (14), (16), (22)–(25). In this
case, the constraints (23)–(25) are nonlinear, and the calculated variables ni and ni are
integer. In the criterion (26), the values ~pi calculated in accordance with expressions (8)
are the cost weight coefﬁcients. This ensures secure routing over the network when the
maximum number of message parts will be sent over the path with the minimum
probability of compromise. Conversely, the minimum number of message parts will be
transmitted over the path with the highest probability of compromise or none of them
will be transmitted.
6
Numerical Study of Secure Fast ReRouting
Let us demonstrate the features of the proposed mechanism of Secure Fast ReRouting.
The initial structure of the network is shown in Fig. 7, and the corresponding proba-
bilities of compromising the communication links are indicated in Table 3.
The source of the message is the ﬁrst node, and the destination is the seventeenth
node. The solid lines in Fig. 7 show the communication links used to form the primary
and backup multipaths for message transmission.
Suppose that with the secure fast rerouting, the Shamir’s scheme (10, 10) is realized
and according to the structure of the paths shown in Fig. 7, h1 ¼ 1, h2 ¼ 2, h3 ¼ 2 and
h4 ¼ 1, and the allowable value of the probability of compromising the transmitted
message, determined by the parameter cP, is 0.3. Then, in the course of the study, two
cases were considered demonstrating the features of the implementation of the pro-
tection schemes described in Sect. 5:
• the ﬁrst case is associated with the implementation of the protection scheme of the
second (composite) path;
• the second case describes the protection scheme for the primary multipath as a
whole.
Let us consider Case 1 in more detail. Thus, according to the data in Table 3, based
on the calculation method proposed in Sect. 5, the primary multipath includes two
composite paths: the second and the third, with the smallest probabilities of compro-
mising: 0.5339 and 0.4061, respectively. The parameters of these paths and the order of
balancing the parts of the transmitted message by the paths are presented in Table 4.
On the third (composite) path, 8 parts of the message were transmitted, because the
probability of compromising this path is minimum and equal to 0.4061. On the second
(composite) path two parts of the message were transmitted as its probability of
650
O. Yeremenko et al.

compromising was already 0.5339, but the lower threshold n2 under the conditions (13)
was h2 ¼ 2. The use of these two paths as the primary multipath in accordance with
expression (19) provides the probability of the message compromise equal to 0.2168,
which satisﬁes the requirements (0.3).
Fig. 7. Initial network structure.
Table 3. Initial research data for the Secure Fast ReRouting.
Secure Routing in Reliable Networks
651

When protecting the second (composite) path of the primary multipath, the ﬁrst
(simple) and third (composite) paths, the parameters of which are presented in Table 5,
will already be a part of the calculated backup multipath. The third (composite) path
will transmit 9 parts of the message, and the ﬁrst one (simple) will transmit only 1 part,
because the probability of compromising the ﬁrst path was 0.5428 at h1 ¼ 1. The use of
a backup multipath also allowed to satisfy the requirements for the level of information
security, because the probability of compromising the transmitted message was 0.2204.
Let us consider Case 2 in more detail, in which it was necessary to protect the
primary multipath as a whole. The application of the proposed S-FRR method has left
the basic multipath (Table 4) unchanged. Then, according to the initial data in Table 3,
the backup multipath includes two simple paths: the ﬁrst and the fourth (Table 6).
The probability of compromising the fourth (simple) path was 0.5483. Then the use
of a backup multipath allowed to ensure the probability of compromising the trans-
mitted message to 0.2977 under the requirements cP ¼ 0:3. In the ﬁrst path, 9 parts of
Table 4. Parameters of the primary multipath.
Table 5. Parameters of the backup multipath (Case 1).
652
O. Yeremenko et al.

the message were transmitted, because the probability of compromise was lower than
that of the fourth path, over which one part of the message was transmitted under
h4 ¼ 1.
7
Conclusion
1. The paper considers the approach to providing a given level of information security
by means of multipath routing of conﬁdential messages in the network. The pro-
posed solutions are the further development of secure routing methods for
non-overlapping paths proposed in [10–12], when the probability of compromising
transmitted messages was the main indicator of information security.
2. The method has been developed for the secure routing of messages over overlap-
ping paths, which belongs to the class of proactive solutions to ensure a given level
of information security. The novelty of the method is that it is fair, also when using
a special class of overlapping paths, which form the basis of the so-called composite
paths containing network fragments with series and/or parallel connection of
communication links of the network. The method is based on optimizing the pro-
cess of selecting a set of composite paths and balancing the parts of the transmitted
message along them with the provision of speciﬁed values of its probability of
compromise. The formulated optimization problem belonged to the class of Non-
linear Integer Programming problems. This is due to the fact that the set of variables
to be calculated characterizing the number of transmitted message parts in the
composite path were integer, and the constraints (1), (9) as well as the criterion (15)
associated with calculating the probability of compromising the message were
nonlinear.
3. As the analysis has shown, the use of the proposed method within the framework of
the presented calculated examples (Sect. 4) makes it possible to improve the
probability of compromise of transmitted messages at average from 5–10% to 25–
50% (Figs. 4 and 6) due to the possibility of using composite paths that are one of
the subclasses of overlapping paths.
Table 6. Parameters of the backup multipath (Case 2).
Secure Routing in Reliable Networks
653

4. The method of Secure Fast ReRouting of messages in the network has been syn-
thesized. The novelty of the method is that it focuses on the implementation of both
proactive and reactive secure routing of conﬁdential messages. In this case, the
proactive nature of the solutions is conditioned by the calculation of the set of
primary composite paths that form the primary multipath, along which parts of the
conﬁdential message are transmitted. In the case of violation of the information
security requirements in the network caused by the increased probability of com-
promising one or multiple composite paths entering the primary multipath, the
messages will be transmitted according to the precomputed set of backup composite
paths determining the backup multipath. Based on the use of backup paths, such a
solution refers to the reactive approach while ensuring a given level of information
security.
5. Moreover, within the proposed S-FRR method, the possibility of protecting both the
primary multipath as a whole and one or several precomputed composite paths
included in this primary multipath is provided. The application of the developed
S-FRR method allows real-time provision of speciﬁed values of such an important
information security indicator as the probability of compromising transmitted
messages even in conditions of dynamic network state change (probability of link
and path compromise) due to the calculation and operative switching to the backup
composite paths (backup multipath) in the multipath transmission of parts of the
conﬁdential message.
6. The developed methods of secure routing can be used as a basis for new network
routing protocols and fast rerouting for multipath transmission of parts of a conﬁ-
dential message with speciﬁed requirements regarding the probability of its com-
promise in the network.
References
1. ITU-T X-805. Security architecture for systems providing end-to-end communications
(2003)
2. ISO 7498–2:1989 Information processing systems – Open Systems Interconnection – Basic
Reference Model – Part 2: Security Architecture (1989)
3. ITU-T X-800. Security architecture for Open Systems Interconnection for CCITT
applications (1991)
4. Stallings, W.: Cryptography and Network Security: Principles and Practice, 7th edn.
Pearson, London (2016)
5. Schneier, B.: Data and Goliath: The Hidden Battles to Collect Your Data and Control Your
World, 1st edn. WW Norton & Company, New York (2015)
6. Cisco Networking Academy (ed.): Routing Protocols Companion Guide, 1st edn. Cisco
Press (2014)
7. Santos, O., Kampanakis, P., Woland, A.: Cisco Next-Generation Security Solutions:
All-in-one Cisco ASA Firepower Services, NGIPS, and AMP, 1st edn. Cisco Press (2016)
8. Wang, M., Liu, J., Mao, J., Cheng, H., Chen, J.: NSV-GUARD: constructing secure routing
paths in software deﬁned networking. In: Proceedings of the 2016 IEEE International
Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and
Networking (SocialCom), Sustainable Computing and Communications (SustainCom)
(BDCloud-SocialCom-SustainCom), pp. 293–300 (2016)
654
O. Yeremenko et al.

9. Almerhag, I.A., Almarimi, A.A., Goweder, A.M., Elbekai, A.A.: Network security for QoS
routing metrics. In: Proceedings of the 2010 International Conference on Computer and
Communication Engineering (ICCCE), pp. 1–6 (2010)
10. Lou, W., Liu, W., Zhang, Y., Fang, Y.: SPREAD: improving network security by multipath
routing in mobile ad hoc networks. Wirel. Netw. 15(3), 279–294 (2009)
11. Alouneh, S., Agarwal, A., En-Nouaary, A.: A novel path protection scheme for MPLS
networks using multi-path routing. Comput. Netw. 53(9), 1530–1545 (2009)
12. Yeremenko, O.S., Ali, A.S.: Secure multipath routing algorithm with optimal balancing
message fragments in MANET. Radioelectron. Inform. 1(68), 26–29 (2015)
13. Yeremenko, O.: Enhanced ﬂow-based model of multipath routing with overlapping by nodes
paths. In: Proceedings of the 2015 Second International Scientiﬁc-Practical Conference
Problems of Infocommunications Science and Technology (PIC S&T), pp. 42–45 (2015)
14. Yeremenko, O., Lemeshko, O., Persikov, A.: Enhanced method of calculating the
probability of message compromising using overlapping routes in communication network.
In: Proceedings of the 2017 XIIth International Scientiﬁc and Technical Conference
Computer Sciences and Information Technologies (CSIT), pp. 87–90 (2017)
15. Lemeshko, O., Romanyuk, A., Kozlova, H.: Design schemes for MPLS fast reroute. In:
Proceedings of the 2013 12th International Conference on the Experience of Designing and
Application of CAD Systems in Microelectronics (CADSM), pp. 202–203 (2013)
16. Lemeshko, O.V., Yeremenko, O.S., Tariki, N., Hailan, A.M.: Fault-tolerance improvement
for core and edge of IP network. In: Proceedings of the 2016 XIth International Scientiﬁc
and Technical Conference Computer Sciences and Information Technologies (CSIT),
pp. 161–164 (2016)
17. Lemeshko, O., Yeremenko, O., Nevzorova, O.: Hierarchical method of inter-area fast
rerouting. Transp. Telecommun. J. 18(2), 155–167 (2017)
Secure Routing in Reliable Networks
655

Linguistic Comparison Quality Evaluation
of Web-Site Content with Tourism
Documentation Objects
Pavlo Zhezhnych1(&) and Oksana Markiv2
1 Department of Social Communication and Information Activities,
Lviv Polytechnic National University, Lviv, Ukraine
pavlo.i.zhezhnych@lpnu.ua
2 Department of Information Systems and Networks,
Lviv Polytechnic National University, Lviv, Ukraine
oksanasoprunyuk@gmail.com
Abstract. Information support for tourism service consumers is the important
element of tourism activity. Open websites are the main sources of such support.
Widespread approach to the analysis of tourism services quality is the analysis
of feedbacks from consumers based on the results of tourism services provision.
In this work, it is proposed to evaluate the information provision of tourism
services consumer formed in the form of tourism documentation. This approach
allows even before the provision of tourism services to inﬂuence the choice of
the consumer and satisfaction with tourism services. The developed method for
evaluating the quality of tourism documentation generated on the basis of the
website content is based on ISO/IEC-25010 quality indices that are suitable for
assessing the quality of the tourism documentation from the point of view of the
consumer of its content. These indices include such sub-characteristics of
Operability, as Appropriateness recognizability, Helpfulness, Attractiveness.
Keywords: Tourism  Information quality  Documentation  Web-site
Content
1
Introduction
Nowadays activities in providing a wide range of tourism services (TS) are practically
impossible without proper information support through the World Wide Web (WWW).
Obviously, the main source of information for tourism services support are open web
sites. According to Maurice de Kunder and his colleagues calculations, about 4.66
billion web pages are available in the WWW via search engines (as of March 2016)
[18]. In modern conditions, the content of a large number of web pages is generated by
users of web sites (bloggers). According to the blogging industry, about 11% of blogs
cover such topics as “Travel” and “Food & Beverage” [14], that is, they directly
concern tourism problems. Therefore, even on the basis of the data on the subject of
blogs it is easy to predict that millions of web pages that are related to the tourism
industry are accessible in the WWW. Qualitative processing, consolidation and
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1_45

presentation of information from such an array is one of the important tasks of effective
tourism activity.
This paper dwells upon the presentation of information for tourism services pro-
vision in the form of relevant documentation. That is, tourism documentation (TD) is
technical documentation for TS, which allows to provide the consumer with the nec-
essary information about these TS. Consumer orientation allows to consider TD as
information product, on the quality of which depends the satisfaction of the TD con-
sumers needs. Inadequate TD may result in the poor provision of tourism services
because of misinformation and dissatisfaction of tourists, whose negative experience
extends and weakens the image of tourism institutions. The construction of high quality
TD requires constant monitoring of its quality, its analysis and formation of modern
quality requirements, so that TD will be available for use and as informative as pos-
sible. Formation of quality TD is related to the achievement of such goals in the
interests of the consumer:
• increase of the TS attractiveness in the eyes of the consumer (potential tourist), that
can increase the competitiveness of TS;
• support of the information that is needed in order to understand the features of TS
provision, that allows the TS generator to be protected from possible claims that
may arise during inappropriate consumption of TS.
The basis of the TD formation is the linguistic analysis of the information content
of websites with open access in the WWW. The application of the appropriate method
of TD formation allows automated processing of large amounts of information from
open web resources, as well as its consolidation, classiﬁcation, structuring and further
use.
The main result of this work is the development of method for assessing the quality
of TD, formed on the basis of websites content. This method is based on quality
indicators of ISO/IEC-25010 [15], which are suitable for assessing the quality of the
TD from the point of view of the consumer of its content.
2
State-of-the-Art
The construction of various information systems to support tourism activity is a
widespread trend in scientiﬁc research. The main emphasis of these studies is on the
effective presentation of tourism information for the end user. In particular, [2, 13]
discuss general approaches to the construction of tourism support systems and services,
and [6] highlights the problems of effective selection of information for its collection in
such systems. Personalization of tourism information depending on the needs of the TS
consumer is important for planning travel through e-Tourist services [3]. In this case,
not only the causes that inﬂuence the choice of travel, but also the consequences of this
choice are important [16]. An important consequence of the choice of travel is the
ability to predict the happiness of the consumer of tourism services, which allows
adapting tourism services to a particular consumer [11]. An important element of
information systems of tourism activity support is the visualization of information
related to geodata [8]. In this case, useful information source of such information is the
Linguistic Comparison Quality Evaluation of Web-Site Content
657

user generated content [9], which is often associated with a GPS location [7]. User
generated content is also used to generate new knowledge based on the common
tourism key phrases and text corpus [12].
Key element of tourism services provision is the assessment of their quality, since it
directly affects the frequency and duration of tourist services consumption [10]. The
main method of such evaluation is consumer feedback analysis [5]. At the same time,
typical methods of detecting information about the impressions of tourism services
consumer are based on the use of fuzzy evaluation [21] with regard to the level of trust
to the consumer [4].
This paper dwells upon the issues of assessing the quality of tourism services not
from the point of view of their provision, but from the point of view of their infor-
mation support. Actually, the initial information provided to the consumer largely
inﬂuences his choice, as well as his satisfaction with tourism services and feedback.
Therefore, this information needs to be properly structured in the form of tourism
documentation.
The quality of tourism documentation is a category that reﬂects the degree of
dependence of its characteristics and properties conditioned or pre-seen needs and
requirements of the tourism product consumer [20]. TD in terms of quality should have
the following main properties:
• provide a set of structural elements that are ﬁlled with information about tourism
activities;
• be clear to the consumer;
• simple and easy to use;
• contain no gaps in the ﬁlling of structural elements;
• contain actual information.
To assess the quality of TD, the ISO/IEC-25010 quality standard is acceptable. In
this case, among all the indicators of quality applicable are those sub-characteristics
that
meet
the
requirements
of
TD
construction.
These
are
the
following
sub-characteristics of Operability [16]:
• Appropriateness recognizability;
• Helpfulness;
• Attractiveness.
3
Tourism Documentation Structure
3.1
General Structure of Tourism Documentation
General structure of tourism documentation is an extension of simpliﬁed tourism
documentation structure described in [19]. The tourism documentation consists of the
set of TS and sets of TS consumer experiences (Fig. 1). The structure of the TS is
similar to that of TS consumer experiences (TE) and consists of the set of tourism
objects (TO) and tourism objects actions (TOA). The structure of the TO is similar to
the structure of TOA and consists of the sets of facts and events. Facts and events are
658
P. Zhezhnych and O. Markiv

elementary (indivisible) informational units in the structure of TD. They describe TO
and TOA both in the structure of TS and in the structure of TE.
The same TO and TOA can occur in different TS and TE. That is, in fact, TD has
the form of network, and not a tree, where each TO and TOA should be associated with
all TS and TE. However, such more complex structure of TD is needed ﬁrst of all for
the construction of a speciﬁc IS intended for the formation, processing and use of TD.
The simpliﬁcation of the network model to the model in the form of a tree is used in the
methods of reduction of graphs [17], and this approach is used in the construction of a
formal TD model, which is sufﬁcient for the application of linguistic approaches to the
automated formation of TD [19].
3.2
Eligibility of Tourism Documentation Content
The eligibility of the content of TD depends on the requirements that are necessary to
meet the needs of the user. In particular, the following requirements are included:
• Lack of content gaps;
• Information authenticity;
• Information actuality.
Gaps in TD content arise because of the lack of information on the facts and events
associated with certain structural elements of TD as TO and TOA. In general, the
Fig. 1. General structure of tourism documentation.
Linguistic Comparison Quality Evaluation of Web-Site Content
659

presence of content gaps shows that for the structural element of the TD there are not
all necessary facts and events associated with it.
If Elementi is TO or TOA, ElementFact TD
ð
Þ Elementi
ð
Þ is a set of all facts of this TO
or TOA in TD, ElementFact Expected
ð
Þ Elementi
ð
Þ is an expected set of all facts of TO or
TOA, then there is a fact content gap, if:
ElementFact Gap
ð
Þ Elementi
ð
Þ
¼ ElementFact Expected
ð
Þ Elementi
ð
ÞnElementFact TD
ð
Þ Elementi
ð
Þ 6¼ £
ð1Þ
If ElementEvent TD
ð
Þ Elementi
ð
Þ is a set of all events of TO or TOA Elementi in TD,
ElementEvent Expected
ð
Þ Elementi
ð
Þ – expected set of all events of TO or TOA, then there
is event content gap, if:
ElementEvent Gap
ð
Þ Elementi
ð
Þ
¼ ElementEvent Expected
ð
Þ Elementi
ð
ÞnElementEvent TD
ð
Þ Elementi
ð
Þ 6¼ £
ð2Þ
In general, the authenticity of the information provided on the website depends
directly on the level of trust to its author [1]. The credibility of the author, in particular,
is determined by the adequacy of the data provided in the relevant user proﬁle of the
website [4]. Each fact or event contained in TD must be associated with separate author
or related website. If the author of the information submitted on the website is
unknown, then the owner of this website should be considered as the author of such
information. Thus, the authenticity of the facts and events of TD is entirely determined
by the level of trust to the author of the relevant information from the website.
The actuality of the information is determined by the time of its post publishing. If
T0 – the starting time point of the actuality index (that is, information that was pub-
lished earlier then T0 is considered irrelevant), Tnow – the current time moment. Then
the actuality of the information in Post, which was published at the time
PostTime Post
ð
Þ, is:
Actuality Post
ð
Þ ¼
Tnow  T0
ð
Þ  Tnow  PostTime Post
ð
Þ
ð
Þ
Tnow  T0
PostTime Post
ð
Þ  T0
0
PostTime Post
ð
Þ\T0
8
<
:
ð3Þ
4
Tourism Documentation Quality Indices
4.1
Appropriateness Recognisability
Appropriateness recognisability is a measure of the user’s ability to recognize whether
the TD meets his/her needs. Appropriateness recognizability will is determined by the
full description of the elements of TD:
660
P. Zhezhnych and O. Markiv

Quality O:AR
ð
Þ ¼ A O:AR
ð
Þ
B O:AR
ð
Þ
ð4Þ
A O:AR
ð
Þ displays the number of described elements of TD, B O:AR
ð
Þ – the total
quantity of TD elements. If Element Pr esent
ð
Þ is a set of available structural elements of
TD, Element ContentGap
ð
Þ is a set of structural elements of TD, which contains content
gaps in the content deﬁned by (1) or (2), then for all structural elements of TD (TO and
TOA) associated with facts and events:
• A O:AR
ð
Þ ¼ Element Pr esent
ð
Þ

  Element ContentGap
ð
Þ

; B O:AR
ð
Þ ¼ Element Pr esent
ð
Þ


Thus, the appropriateness recognisability in accordance with the recognition indi-
cates the level of presence of content gaps in the TD according to TO, TOA.
Also, this quality index is modiﬁed to display content gaps in the authenticity and
actuality of facts and events (the facts of deﬁnition are not considered here).
Element TrustGap
ð
Þ – set of elements, containing facts or events whose authors have level
of conﬁdence lower than admitted, Element ActualityGap
ð
Þ – set of elements containing
non-actual facts or events in accordance with (3). Then the quality of the TD in the
fullness of authentic and actual descriptions is determined by (5), if:
• A O:AR
ð
Þ ¼ Element Pr esent
ð
Þ

  Element TrustGap
ð
Þ

, B O:AR
ð
Þ ¼ Element Pr esent
ð
Þ

 – for
authenticity;
• A O:AR
ð
Þ ¼ Element Pr esent
ð
Þ

  Element ActualityGap
ð
Þ

,
B O:AR
ð
Þ ¼ Element Pr esent
ð
Þ


–
for actuality.
The authenticity varies in time, since the credibility of the post author, on the basis
of which the fact or event are generated in TD, may change depending on its behavior
in time.
4.2
Helpfulness
Helpfulness is a measure of user support by the help through the use of information
from TD. Helpfulness on the availability of assistance is deﬁned as follows:
Quality O:H
ð
Þ ¼ A O:H
ð
Þ
B O:H
ð
Þ
ð5Þ
A O:H
ð
Þ – quantity of TO or TOA, that have TE in TD, B O:H
ð
Þ – general quantity of
TO or TOA. If Element Experience
ð
Þ is a set of TO or TOA, for which TE are present,
Element ExperienceContent
ð
Þ is TO or TOA, for which TE are present and which do not have
content gaps, according to (1) and (2), then:
• A O:H
ð
Þ ¼ Element ExperienceContent
ð
Þ

; B O:H
ð
Þ ¼ Element Experience
ð
Þ

.
Thus, helpfullness according to trust level shows the proportion of TO or TOA that
contain content in the form of facts and events that cause interest of TS consumers
displayed by their feedbacks.
Linguistic Comparison Quality Evaluation of Web-Site Content
661

4.3
Attractiveness
Attractiveness is the level of inﬂuence (impressions) of the TD on the user. Attrac-
tiveness is deﬁned as follows:
Quality O:A
ð
Þ ¼ A O:A
ð
Þ
B O:A
ð
Þ
ð6Þ
A O:A
ð
Þ – quantity of TO and TOA, which are in description part of TD and for which
TE are present, B O:A
ð
Þ – general quantity of TO and TOA, which are in description part
of TD. If Element Present
ð
Þ is a set of presented TO or TOA in TD and Element Experience
ð
Þ
is a set of TO or TOA, for which TE are present, then:
• A O:A
ð
Þ ¼ Element Experience
ð
Þ

; B O:A
ð
Þ ¼ Element Present
ð
Þ

.
In this way, the suitability for use for attractiveness shows the proportion of TO or
TOA that have raised interest in TS consumers as reﬂected in their feedbacks.
5
Tourism Documentation Quality Monitoring
Monitoring of the TD quality on the indicators of suitability for use is made during the
formation of TD on two tourism companies.
The ﬁrst tourism company 1 provides services of the organization of tourism trips
and recreation. The formation of the company TD through linguistic comparison of the
website content with tourism documentation objects has begun in September 2014 and
lasted 25 months. At the end of the experimental term, TD contained:
• 235 tourism services (dynamics of TS quantity change is presented in Fig. 2);
• 1545 tourism objects (dynamics of TO quantity change is presented in Fig. 3).
Fig. 2. Dynamics of TS quantity change in TD of the company 1.
662
P. Zhezhnych and O. Markiv

Quality monitoring of TD has been made by the appropriateness recognisability
indices according to the fullness of TO description (Fig. 4).
Figure 4 shows that in the company 1 after the ﬁrst year of formation and support of
TD in the current state it is possible to fully stabilize the level of TD quality at the
sufﬁciently high level.
Tourism Company 2 provides services for organizing tours in the city of Lviv,
Ukraine and abroad. The formation of the company TD through the linguistic com-
parison of the website content with tourism documentation objects has begun in July
2015 and lasted 15 months. At the end of the experimental term, TD contained:
• 210 tourism services (dynamics of TS quantity change is presented in Fig. 5);
• 210 tourism experiences (dynamics of TE quantity change is presented in Fig. 6);
Fig. 3. Dynamics of TO quantity change in TD of the company 1.
Fig. 4. Dynamics of quality indices change of TD of the company 1.
Linguistic Comparison Quality Evaluation of Web-Site Content
663

• 1248 tourism objects (dynamics of TO quantity change is presented in Fig. 7);
• 2053 tourism objects actions (dynamics of TOA quantity change is presented in
Fig. 8).
Quality monitoring of TD has been made by such indices (Fig. 9):
• appropriateness recognisability by the fullness of TO and TOA description;
• level of helpfulness by availability of TO and TOA;
• attractiveness of TO and TOA.
Figure 9 shows that in the company 2 after the ﬁrst year of formation and support of
TD in the current state it is possible to fully stabilize the level of TD quality at a
sufﬁciently high level.
Fig. 5. Dynamics of TS quantity change in TD of the company 2.
Fig. 6. Dynamics of TE quantity change in TD of the company 2.
664
P. Zhezhnych and O. Markiv

Fig. 7. Dynamics of TO quantity change in TD of the company 2.
Fig. 8. Dynamics of TOA quantity change in TD of the company 2.
Fig. 9. Dynamics of quality indices change of TD of the company 2.
Linguistic Comparison Quality Evaluation of Web-Site Content
665

6
Conclusions
This paper dwells upon the method of assessing the TD quality, formed by linguistic
analysis of the open websites content in the WWW. The basis of this method is
ISO/IEC-25010 quality standards, which, unlike the common methods of analysis of
feedbacks from tourism services users, is oriented towards the assessment of primary
information collected in the form of tourism documentation and is provided to the
consumer. This approach allows even before tourism services provision to inﬂuence the
choice of the consumer and satisfaction with tourism services and feedbacks.
The developed method is based on the sub-characteristics of Operability of the
ISO/IEC-25010 quality standard such as Appropriateness Recognition, Helpfulness,
Attractiveness. These indices allow to control the ability of the TD to meet the needs of
the TS consumer in terms of providing by the necessary information.
The monitoring of the TD quality, which is automatically formed with the help of
information systems, was carried out in two travel companies within 25 and 15 months.
Quality indices have been determined on a monthly basis. The dynamics of TD quality
indices changes of these companies has showed that after the ﬁrst year of formation and
support of the TD in the current state, it was possible to fully stabilize the level of TD
quality at the level of 0,6–0,9.
References
1. Berezko, O.: Using Web badges as personifying inserts in web-oriented public actions. In:
Proceedings of the 5th International Scientiﬁc and Technical Conference “Computer science
and information Technology” (CSIT 2010), Lviv, Publishing house “Vezha and Co”, p. 55
(2010)
2. Chu, Y., Wang, H., Zheng, L., Wang, Z., Tan, K.-L.: TRSO: a tourism recommender system
based on ontology. In: Lehner, F., Fteimi, N. (eds.) KSEM 2016. LNCS, vol. 9983,
pp. 567–579. Springer, Cham (2016)
3. Cvetkovic, B., et al.: e-Turist: an intelligent personalised trip guide. Informatica J. Comput.
Inform. 40(4), 447–455 (2016)
4. Fedushko, S., Syerov, Y., Peleschyshyn, A., Korzh, R.: Determination of the account
personal data adequacy of web-community member. Int. J. Comput. Sci. Bus. Inform. 15(1),
1–12 (2015)
5. Fux, M., Noti, M., Myrach, T.: Quality of feedback to E-Mail requests - an explorative study
in alpine tourism destinations. In: Hitz, M., Sigala, M., Murphy, J. (eds.) Information and
Communication Technologies in Tourism, p. 370. Springer, Vienna (2006)
6. Gregorash, B.J.: Restaurant revenue management: apply reservation management. Inform.
Technol. Tourism 16(4), 331–346 (2016). Springer, Berlin, Heidelberg
7. Hendrikx, J., Johnson, J., Shelly, C.: Using GPS tracking to explore terrain preferences of
heli-ski guides. J. Outdoor Recreation Tourism-Research Plann. Manage. 13, 34–43 (2016)
8. Hirakawa, G., Nagatsuji, R., Shibata, Y.: A Collection and delivery method of contents in
tourism with location information. In: Proceedings of 2016 19th International Conference on
Network-Based Information Systems, pp. 393–396 (2016)
666
P. Zhezhnych and O. Markiv

9. Li, Q.S., Wu, Y.D., Wang, S., Lin, M.S., Feng, X.M., Wang, H.Y.: VisTravel: visualizing
tourism network opinion from the user generated content. J. Vis. 19(3), 489–502 (2016)
10. Neal, J.D., Sirgy, M.J., Uysal, M.: Measuring the effect of tourism services on travelers’
quality of life: further validation. Soc. Indic. Res. 69(3), 243–277 (2004)
11. Neidhardt, J., Rummele, N., Werthner, H.: Predicting happiness: user interactions and
sentiment analysis in an online travel forum. Inform. Technol. Tourism 17(1), 101–119
(2017). Springer, Berlin, Heidelberg
12. Paramonov, I., Lagutina, K., Mamedov, E., Lagutina, N.: Thesaurus-based method of
increasing text-via-keyphrase graph connectivity during keyphrase extraction for e-Tourism
applications. In: Ngonga Ngomo, A.-C., Křemen, P. (eds.) KESW 2016. Communications in
Computer and Information Science, vol. 649, pp. 129–141. Springer, Cham (2016)
13. Smirnov, A., Ponomarev, A., Kashevnik, A.: Tourist attraction recommendation service: an
approach, architecture and case study. In: Proceedings of the 18th International Conference
on Enterprise Information Systems, vol 2, pp. 251–261 (2016)
14. State of
the Blogging Industry,
ConvertKit.
https://convertkit.com/reports/blogging/.
Accessed 30 June 2017
15. Systems and software engineering – Systems and software Quality Requirements and
Evaluation (SQuaRE) – System and software quality models, ISO/IEC 25010.2:2008. http://
sa.inceptum.eu/sites/sa.inceptum.eu/ﬁles/Content/ISO_25010.pdf. Accessed 21 June 2017
16. Tanti, A., Buhalis, D.: The inﬂuences and consequences of being digitally connected and/or
disconnected to travelers. Inform. Technol. Tourism 17(1), 121–141 (2017). Springer,
Berlin, Heidelberg
17. Tkachenko, S., Soprunyuk, O., Tkachenko, V., Solomko, I.: Efﬁciency enhancement of
optimal reduction method by strengthening parallelism of structural models formation.
Mach. Dyn. Res. Poland 3(2), 85–90 (2013)
18. Van den Bosch, A., Bogers, T., de Kunder, M.: Estimating search engine index size
variability: a 9-year longitudinal study. Scientometrics, An International Journal for all
Quantitative Aspects of the Science of Science, Communication in Science and Science
Policy, vol. 106, #2 (2016). http://www.dekunder.nl/Media/10.1007_s11192-016-1863-z.pdf
19. Zhezhnych, P., Markiv, O.: A linguistic method of web-site content comparison with tourism
documentation objects. In: Proceedings of 12th International Scientiﬁc and Technical
Conference Computer Science and Information Technologies (CSIT 2017), Lviv, Ukraine
(2017)
20. Zhezhnych, P., Soprunyuk, O.: Analysis of the tourism documentation quality improvement.
In: Proceedings of the 7th International Scientiﬁc and Technical Conference “Computer
science and information technology” (CSIT 2012), pp. 34–36. Lviv, Publishing house
“Vezha and Co” (2012)
21. Zhou, R., Lin, S., Lin, M.: Design of evaluation system of residents’ tourism quality of
tourism area based on fuzzy evaluation. In: Cao, B.-Y., Ma, S.-Q., Cao, H.-h. (eds.)
Ecosystem Assessment and Fuzzy Systems Management. AISC, vol. 254, pp. 189–197.
Springer, Cham (2014)
Linguistic Comparison Quality Evaluation of Web-Site Content
667

Author Index
A
Alienin, Oleg, 243
Andrakhanov, Anatoliy, 1
B
Babichev, Sergii, 21
Balcerzak, Bartlomiej, 40
Basyuk, Taras, 54
Batyuk, Anatoliy, 361
Belyaev, Alexander, 1
Bidyuk, Peter, 66
Bodyanskiy, Yevgeniy V., 186
Borowska, Bożena, 79
Buchok, Viktor, 361
Bulgakova, Oleksandra, 492
Bun, Rostyslav, 217
Burov, Yevhen, 310
C
Chyrun, Lyubomyr, 204
Czajka, Agnieszka, 40
D
Danylo, Olha, 217
Davydov, Maksym, 89
Dilai, Marianna, 320
Dyvak, Mykola, 101
F
Faisal, Muhammad, 124
Fiser, Jiri, 21
Frýda, Tomáš, 257
G
Gaida, Anatoly J., 113
Gayda, Anatoliy Y., 146
Geche, Fedir, 361
Gordienko, Yuri, 243
Gozhyj, Aleksandr, 66, 204
Gozhyj, Victor, 66
Grigorian, Tigran G., 146
H
Hasko, Roman, 461
Hovorushchenko, Tetiana, 166
Hu, Zhengbing, 186
I
Ihnatyev, Ihor, 558
Il’kiv, Volodymyr, 421
K
Kalinina, Irina, 66
Kanishcheva, Olga, 204
Kinakh, Vitalii, 217
Kirichenko, Lyudmyla, 230
Kochura, Yuriy, 243
Kopec, Wieslaw, 40
Kordík, Pavel, 257
Koshkin, Konstantin V., 113
Koshkin, Vladimir K., 146
Kravchyshyn, Vitalii, 320
Kryvenchuk, Yuriy, 461
Kułakowski, Andrzej, 270
Kunanets, Nataliia, 450, 577
Kut, Vasil, 450
L
Lange, Tatjana, 280
Lemeshko, Oleksandr, 631
Liubchenko, Vira, 299
Lozynska, Olga, 89
Lytvyn, Vasyl, 310
Lytvynenko, Volodymyr, 21
M
Markiv, Oksana, 656
Maslyak, Yurii, 101
© Springer International Publishing AG 2018
N. Shakhovska and V. Stepashko (eds.), Advances in Intelligent Systems
and Computing II, Advances in Intelligent Systems and Computing 689,
https://doi.org/10.1007/978-3-319-70581-1
669

Medykovskyy, Mykola, 320, 538
Melnyk, Roman, 320
Montenegro, Sergio, 124
Moroz, Olha, 346
Mulesa, Oksana, 361
Mulyak, Oleksandr, 588
N
Nataliia, Melnykova, 334
Nielek, Radoslaw, 40
Novotarskiy, Michail, 243
Nytrebych, Zinovii, 421
O
Opotyak, Yuriy, 538
P
Persikov, Anatoliy, 631
Popova, Svetlana, 374
Porplytsya, Natalia, 101
Potebnia, Artem, 386
Puchala, Dariusz, 404
Pukach, Pavlo, 421
Pukach, Petro, 421
R
Radivilova, Tamara, 230
Revunova, Elena G., 434
Rishnyak, Ihor, 310
Rogala, Bartosz, 270
Rolik, Oleksandr, 523
Romanyshyn, Yuriy, 614
Rzheuskyi, Antonii, 450
S
Shakhovska, Natalya, 461
Shynkaryk, Mykola, 101
Skitalinskaya, Gabriella, 374
Skvor, Jiri, 21
Stepashko, Volodymyr, 346, 474, 492
Stirenko, Sergii, 243
Stokﬁszewski, Kamil, 511
T
Telenyk, Sergii, 523
Teslyuk, Taras, 538, 558
Teslyuk, Vasyl, 538, 558
Titov, Sergey D., 146
Tsmots, Ivan, 538, 558
Tyshchenko, Oleksii K., 186
V
Veres, Oleh, 310
Veretennikova, Nataliia, 577
Volochiy, Bogdan, 588
Vovk, Myroslava, 421
Vovk, Olena, 461
Vysotska, Victoria, 204, 310
W
Warpechowski, Kamil, 40
Wieloch, Kamil, 511
Y
Yakovyna, Vitaliy, 588
Yatsymirskyy, Mykhaylo, 511
Yeﬁmenko, Serhiy, 601
Yelmanova, Elena, 614
Yeremenko, Oleksandra, 631
Z
Zarichuk, Elena A., 113
Zharikov, Eduard, 523
Zhezhnych, Pavlo, 656
Zinkevich, Illya, 230
Zosimov, Viacheslav, 492
670
Author Index


Hybrid Intelligence for Image Analysis
and Understanding

Hybrid Intelligence for Image Analysis
and Understanding
Edited by
Siddhartha Bhattacharyya
RCC Institute of Information Technology
India
Indrajit Pan
RCC Institute of Information Technology
India
Anirban Mukherjee
RCC Institute of Information Technology
India
Paramartha Dutta
Visva-Bharati University
India

This edition Ô¨Årst published 2017
¬© 2017 John Wiley & Sons Ltd
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or
transmitted, in any form or by any means, electronic, mechanical, photocopying, recording or otherwise,
except as permitted by law. Advice on how to obtain permission to reuse material from this title is available
at http://www.wiley.com/go/permissions.
The right of Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta to be
identiÔ¨Åed as the authors of the editorial material in this work has been asserted in accordance with law.
Registered OÔ¨Éce(s)
John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030, USA
John Wiley & Sons Ltd, The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, UK
Editorial OÔ¨Éce
The Atrium, Southern Gate, Chichester, West Sussex, PO19 8SQ, UK
For details of our global editorial oÔ¨Éces, customer services, and more information about Wiley products
visit us at www.wiley.com.
Wiley also publishes its books in a variety of electronic formats and by print-on-demand. Some content that
appears in standard print versions of this book may not be available in other formats.
Limit of Liability/Disclaimer of Warranty
While the publisher and authors have used their best eÔ¨Äorts in preparing this work, they make no
representations or warranties with respect to the accuracy or completeness of the contents of this work and
speciÔ¨Åcally disclaim all warranties, including without limitation any implied warranties of merchantability or
Ô¨Åtness for a particular purpose. No warranty may be created or extended by sales representatives, written
sales materials or promotional statements for this work. The fact that an organization, website, or product is
referred to in this work as a citation and/or potential source of further information does not mean that the
publisher and authors endorse the information or services the organization, website, or product may provide
or recommendations it may make. This work is sold with the understanding that the publisher is not engaged
in rendering professional services. The advice and strategies contained herein may not be suitable for your
situation. You should consult with a specialist where appropriate. Further, readers should be aware that
websites listed in this work may have changed or disappeared between when this work was written and when
it is read. Neither the publisher nor authors shall be liable for any loss of proÔ¨Åt or any other commercial
damages, including but not limited to special, incidental, consequential, or other damages.
Library of Congress Cataloging-in-Publication Data:
Names: Bhattacharyya, Siddhartha, 1975- editor. | Pan, Indrajit, 1983-
editor. | Mukherjee, Anirban, 1972- editor. | Dutta, Paramartha, editor.
Title: Hybrid intelligence for image analysis and understanding / edited by
Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, Paramartha
Dutta.
Description: Hoboken, NJ : John Wiley & Sons, 2017. | Includes index. |
IdentiÔ¨Åers: LCCN 2017011673 (print) | LCCN 2017027868 (ebook) | ISBN
9781119242932 (pdf) | ISBN 9781119242956 (epub) | ISBN 9781119242925
(cloth)
Subjects: LCSH: Image analysis. | Computational intelligence.
ClassiÔ¨Åcation: LCC TA1637 (ebook) | LCC TA1637 .H93 2017 (print) | DDC
621.36/7028563‚Äìdc23
LC record available at https://lccn.loc.gov/2017011673
Cover design by Wiley
Cover images: (Background) ¬© Yakobchuk/Gettyimages; (From left to right)
¬© zmeel/Gettyimages; ¬© tkemot/Shutterstock; ¬© Semnic/Shutterstock;
¬© Callista Images/Gettyimages; ¬© Karl Ammann/Gettyimages
Set in 10/12pt Warnock by SPi Global, Chennai, India
10 9 8 7 6 5 4 3 2 1

v
Dedication
Dedicated to my parents, the late Ajit Kumar Bhattacharyya and the late Hashi
Bhattacharyya; my beloved wife, Rashni; my elder sisters, Tamali, Sheuli, and Barnali;
my cousin sisters, Sutapa, Mousumi, and Soma; and all my students, who have made
this journey enjoyable.
Dr. Siddhartha Bhattacharyya
Dedicated to all my students.
Dr. Indrajit Pan
Dedicated to my respected teachers.
Dr. Anirban Mukherjee
Dedicated to my parents, the late Arun Kanti Dutta and Mrs. Bandana Dutta.
Dr. Paramartha Dutta

vii
Contents
Editor Biographies
xvii
List of Contributors
xxi
Foreword
xxvii
Preface
xxxi
About the Companion website
xxxv
1
Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm
(MfGA)-based Fuzzy C-Means
1
Sourav De, Sunanda Das, Siddhartha Bhattacharyya, and Paramartha Dutta
1.1
Introduction
1
1.2
Fuzzy C-Means Algorithm
5
1.3
ModiÔ¨Åed Genetic Algorithms
6
1.4
Quality Evaluation Metrics for Image Segmentation
8
1.4.1
Correlation CoeÔ¨Écient (ùúå)
8
1.4.2
Empirical Measure Q(I)
8
1.5
MfGA-Based FCM Algorithm
9
1.6
Experimental Results and Discussion
11
1.7
Conclusion
22
References
22
2
Character Recognition Using Entropy-Based Fuzzy C-Means
Clustering
25
B. Kondalarao, S. Sahoo, and D.K. Pratihar
2.1
Introduction
25
2.2
Tools and Techniques Used
27
2.2.1
Fuzzy Clustering Algorithms
27
2.2.1.1
Fuzzy C-means Algorithm
28
2.2.1.2
Entropy-based Fuzzy Clustering
29
2.2.1.3
Entropy-based Fuzzy C-Means Algorithm
29
2.2.2
Sammon‚Äôs Nonlinear Mapping
30
2.3
Methodology
31
2.3.1
Data Collection
31
2.3.2
Preprocessing
31
2.3.3
Feature Extraction
32

viii
Contents
2.3.4
ClassiÔ¨Åcation and Recognition
34
2.4
Results and Discussion
34
2.5
Conclusion and Future Scope of Work
38
References
39
Appendix
41
3
A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
47
Pawan Kumar Singh, Supratim Das, Ram Sarkar, and Mita Nasipuri
3.1
Introduction
47
3.2
Review of Related Work
48
3.3
Properties of Scripts Used in the Present Work
51
3.4
Proposed Work
52
3.4.1
Discrete Wavelet Transform
53
3.4.1.1
Haar Wavelet Transform
55
3.4.2
Radon Transform (RT)
57
3.5
Experimental Results and Discussion
63
3.5.1
Evaluation of the Present Technique
65
3.5.1.1
Statistical SigniÔ¨Åcance Tests
66
3.5.2
Statistical Performance Analysis of SVM ClassiÔ¨Åer
68
3.5.3
Comparison with Other Related Works
71
3.5.4
Error Analysis
73
3.6
Conclusion
74
Acknowledgments
75
References
75
4
Feature Extraction and Segmentation Techniques in a Static Hand
Gesture Recognition System
79
Subhamoy Chatterjee, Piyush Bhandari, and Mahesh Kumar Kolekar
4.1
Introduction
79
4.2
Segmentation Techniques
81
4.2.1
Otsu Method for Gesture Segmentation
81
4.2.2
Color Space‚ÄìBased Models for Hand Gesture Segmentation
82
4.2.2.1
RGB Color Space‚ÄìBased Segmentation
82
4.2.2.2
HSI Color Space‚ÄìBased Segmentation
83
4.2.2.3
YCbCr Color Space‚ÄìBased Segmentation
83
4.2.2.4
YIQ Color Space‚ÄìBased Segmentation
83
4.2.3
Robust Skin Color Region Detection Using K-Means Clustering and
Mahalanobish Distance
84
4.2.3.1
Rotation Normalization
85
4.2.3.2
Illumination Normalization
85
4.2.3.3
Morphological Filtering
85
4.3
Feature Extraction Techniques
86
4.3.1
Theory of Moment Features
86
4.3.2
Contour-Based Features
88
4.4
State of the Art of Static Hand Gesture Recognition Techniques
89
4.4.1
Zoning Methods
90
4.4.2
F-Ratio-Based Weighted Feature Extraction
90

Contents
ix
4.4.3
Feature Fusion Techniques
91
4.5
Results and Discussion
92
4.5.1
Segmentation Result
93
4.5.2
Feature Extraction Result
94
4.6
Conclusion
97
4.6.1
Future Work
99
Acknowledgment
99
References
99
5
SVM Combination for an Enhanced Prediction of Writers‚Äô Soft
Biometrics
103
Nesrine Bouadjenek, Hassiba Nemmour, and Youcef Chibani
5.1
Introduction
103
5.2
Soft Biometrics and Handwriting Over Time
104
5.3
Soft Biometrics Prediction System
106
5.3.1
Feature Extraction
107
5.3.1.1
Local Binary Patterns
107
5.3.1.2
Histogram of Oriented Gradients
108
5.3.1.3
Gradient Local Binary Patterns
108
5.3.2
ClassiÔ¨Åcation
109
5.3.3
Fuzzy Integrals‚ÄìBased Combination ClassiÔ¨Åer
111
5.3.3.1
gùúÜFuzzy Measure
111
5.3.3.2
Sugeno‚Äôs Fuzzy Integral
113
5.3.3.3
Fuzzy Min-Max
113
5.4
Experimental Evaluation
113
5.4.1
Data Sets
113
5.4.1.1
IAM Data Set
113
5.4.1.2
KHATT Data Set
114
5.4.2
Experimental Setting
114
5.4.3
Gender Prediction Results
117
5.4.4
Handedness Prediction Results
117
5.4.5
Age Prediction Results
118
5.5
Discussion and Performance Comparison
118
5.6
Conclusion
120
References
121
6
Brain-Inspired Machine Intelligence for Image Analysis: Convolutional
Neural Networks
127
Siddharth Srivastava and Brejesh Lall
6.1
Introduction
127
6.2
Convolutional Neural Networks
129
6.2.1
Building Blocks
130
6.2.1.1
Perceptron
134
6.2.2
Learning
135
6.2.2.1
Gradient Descent
136
6.2.2.2
Back-Propagation
136
6.2.3
Convolution
139

x
Contents
6.2.4
Convolutional Neural Networks: The Architecture
141
6.2.4.1
Convolution Layer
142
6.2.4.2
Pooling Layer
145
6.2.4.3
Dense or Fully Connected Layer
146
6.2.5
Considerations in Implementation of CNNs
146
6.2.6
CNN in Action
147
6.2.7
Tools for Convolutional Neural Networks
148
6.2.8
CNN Coding Examples
148
6.2.8.1
MatConvNet
148
6.2.8.2
Visualizing a CNN
149
6.2.8.3
Image Category ClassiÔ¨Åcation Using Deep Learning
153
6.3
Toward Understanding the Brain, CNNs, and Images
157
6.3.1
Applications
157
6.3.2
Case Studies
158
6.4
Conclusion
159
References
159
7
Human Behavioral Analysis Using Evolutionary Algorithms and Deep
Learning
165
Earnest Paul Ijjina and Chalavadi Krishna Mohan
7.1
Introduction
165
7.2
Human Action Recognition Using Evolutionary Algorithms and Deep
Learning
167
7.2.1
Evolutionary Algorithms for Search Optimization
168
7.2.2
Action Bank Representation for Action Recognition
168
7.2.3
Deep Convolutional Neural Network for Human Action
Recognition
169
7.2.4
CNN ClassiÔ¨Åer Optimized Using Evolutionary Algorithms
170
7.3
Experimental Study
170
7.3.1
Evaluation on the UCF50 Data Set
170
7.3.2
Evaluation on the KTH Video Data Set
172
7.3.3
Analysis and Discussion
176
7.3.4
Experimental Setup and Parameter Optimization
177
7.3.5
Computational Complexity
182
7.4
Conclusions and Future Work
183
References
183
8
Feature-Based Robust Description and Monocular Detection: An
Application to Vehicle Tracking
187
Ramazan Y√≠ld√≠z and Tankut Acarman
8.1
Introduction
187
8.2
Extraction of Local Features by SIFT and SURF
188
8.3
Global Features: Real-Time Detection and Vehicle Tracking
190
8.4
Vehicle Detection and Validation
194
8.4.1
X-Analysis
194
8.4.2
Horizontal Prominent Line Frequency Analysis
195
8.4.3
Detection History
196

Contents
xi
8.5
Experimental Study
197
8.5.1
Local Features Assessment
197
8.5.2
Global Features Assessment
197
8.5.3
Local versus Global Features Assessment
201
8.6
Conclusions
201
References
202
9
A GIS Anchored Technique for Social Utility Hotspot Detection
205
Anirban Chakraborty, J.K. Mandal, Arnab Patra, and Jayatra Majumdar
9.1
Introduction
205
9.2
The Technique
207
9.3
Case Study
209
9.4
Implementation and Results
221
9.5
Analysis and Comparisons
224
9.6
Conclusions
229
Acknowledgments
229
References
230
10
Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and
Target IdentiÔ¨Åcation
233
Vaibhav Lodhi, Debashish Chakravarty, and Pabitra Mitra
10.1
Introduction
233
10.2
Background and Hyperspectral Imaging System
234
10.3
Overview of Hyperspectral Image Processing
236
10.3.1
Image Acquisition
237
10.3.2
Calibration
237
10.3.3
Spatial and Spectral preprocessing
238
10.3.4
Dimension Reduction
239
10.3.4.1 Transformation-Based Approaches
239
10.3.4.2 Selection-Based Approaches
239
10.3.5
postprocessing
240
10.4
Spectral Unmixing
240
10.4.1
Unmixing Processing Chain
240
10.4.2
Mixing Model
241
10.4.2.1 Linear Mixing Model (LMM)
242
10.4.2.2 Nonlinear Mixing Model
242
10.4.3
Geometrical-Based Approaches to Linear Spectral Unmixing
243
10.4.3.1 Pure Pixel-Based Techniques
243
10.4.3.2 Minimum Volume-Based Techniques
244
10.4.4
Statistics-Based Approaches
244
10.4.5
Sparse Regression-Based Approach
245
10.4.5.1 Moore‚ÄìPenrose Pseudoinverse (MPP)
245
10.4.5.2 Orthogonal Matching Pursuit (OMP)
246
10.4.5.3 Iterative Spectral Mixture Analysis (ISMA)
246
10.4.6
Hybrid Techniques
246
10.5
ClassiÔ¨Åcation
247
10.5.1
Feature Mining
247

xii
Contents
10.5.1.1 Feature Selection (FS)
248
10.5.1.2 Feature Extraction
248
10.5.2
Supervised ClassiÔ¨Åcation
248
10.5.2.1 Minimum Distance ClassiÔ¨Åer
249
10.5.2.2 Maximum Likelihood ClassiÔ¨Åer (MLC)
250
10.5.2.3 Support Vector Machines (SVMs)
250
10.5.3
Hybrid Techniques
250
10.6
Target Detection
251
10.6.1
Anomaly Detection
251
10.6.1.1 RX Anomaly Detection
252
10.6.1.2 Subspace-Based Anomaly Detection
253
10.6.2
Signature-Based Target Detection
253
10.6.2.1 Euclidean distance
254
10.6.2.2 Spectral Angle Mapper (SAM)
254
10.6.2.3 Spectral Matched Vilter (SMF)
254
10.6.2.4 Matched Subspace Detector (MSD)
255
10.6.3
Hybrid Techniques
255
10.7
Conclusions
256
References
256
11
A Hybrid Approach for Band Selection of Hyperspectral Images
263
Aditi Roy Chowdhury, Joydev Hazra, and Paramartha Dutta
11.1
Introduction
263
11.2
Relevant Concept Revisit
266
11.2.1
Feature Extraction
266
11.2.2
Feature Selection Using 2D PCA
266
11.2.3
Immune Clonal System
267
11.2.4
Fuzzy KNN
268
11.3
Proposed Algorithm
271
11.4
Experiment and Result
271
11.4.1
Description of the Data Set
272
11.4.2
Experimental Details
274
11.4.3
Analysis of Results
275
11.5
Conclusion
278
References
279
12
Uncertainty-Based Clustering Algorithms for Medical Image
Analysis
283
Deepthi P. Hudedagaddi and B.K. Tripathy
12.1
Introduction
283
12.2
Uncertainty-Based Clustering Algorithms
283
12.2.1
Fuzzy C-Means
284
12.2.2
Rough Fuzzy C-Means
285
12.2.3
Intuitionistic Fuzzy C-Means
285
12.2.4
Rough Intuitionistic Fuzzy C-Means
286
12.3
Image Processing
286

Contents
xiii
12.4
Medical Image Analysis with Uncertainty-Based Clustering
Algorithms
287
12.4.1
FCM with Spatial Information for Image Segmentation
287
12.4.2
Fast and Robust FCM Incorporating Local Information for Image
Segmentation
290
12.4.3
Image Segmentation Using Spatial IFCM
291
12.4.3.1 Applications of Spatial FCM and Spatial IFCM on Leukemia Images
292
12.5
Conclusions
293
References
293
13
An Optimized Breast Cancer Diagnosis System Using a Cuckoo Search
Algorithm and Support Vector Machine ClassiÔ¨Åer
297
Manoharan Prabukumar, Loganathan Agilandeeswari, and Arun Kumar Sangaiah
13.1
Introduction
297
13.2
Technical Background
301
13.2.1
Morphological Segmentation
301
13.2.2
Cuckoo Search Optimization Algorithm
302
13.2.3
Support Vector Machines
303
13.3
Proposed Breast Cancer Diagnosis System
303
13.3.1
Preprocessing of Breast Cancer Image
303
13.3.2
Feature Extraction
304
13.3.2.1 Geometric Features
304
13.3.2.2 Texture Features
305
13.3.2.3 Statistical Features
306
13.3.3
Features Selection
306
13.3.4
Features ClassiÔ¨Åcation
307
13.4
Results and Discussions
307
13.5
Conclusion
310
13.6
Future Work
310
References
310
14
Analysis of Hand Vein Images Using Hybrid Techniques
315
R. Sudhakar, S. Bharathi, and V. Gurunathan
14.1
Introduction
315
14.2
Analysis of Vein Images in the Spatial Domain
318
14.2.1
Preprocessing
318
14.2.2
Feature Extraction
319
14.2.3
Feature-Level Fusion
320
14.2.4
Score Level Fusion
320
14.2.5
Results and Discussion
322
14.2.5.1 Evaluation Metrics
323
14.3
Analysis of Vein Images in the Frequency Domain
326
14.3.1
Preprocessing
326
14.3.2
Feature Extraction
326
14.3.3
Feature-Level Fusion
330
14.3.4
Support Vector Machine ClassiÔ¨Åer
331
14.3.5
Results and Discussion
331

xiv
Contents
14.4
Comparative Analysis of Spatial and Frequency Domain Systems
332
14.5
Conclusion
335
References
335
15
IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using
Statistical Decision Making
339
Indra Kanta Maitra and Samir Kumar Bandyopadhyay
15.1
Introduction
339
15.1.1
Breast Cancer
339
15.1.2
Computer-Aided Detection/Diagnosis (CAD)
340
15.1.3
Segmentation
340
15.2
Previous Works
341
15.3
Proposed Method
343
15.3.1
Preparation
343
15.3.2
Preprocessing
345
15.3.2.1 Image Enhancement and Edge Detection
346
15.3.2.2 Isolation and Suppression of Pectoral Muscle
348
15.3.2.3 Breast Contour Detection
351
15.3.2.4 Anatomical Segmentation
353
15.3.3
IdentiÔ¨Åcation of Abnormal Region(s)
354
15.3.3.1 Coloring of Regions
354
15.3.3.2 Statistical Decision Making
355
15.4
Experimental Result
358
15.4.1
Case Study with Normal Mammogram
358
15.4.2
Case Study with Abnormalities Embedded in Fatty Tissues
358
15.4.3
Case Study with Abnormalities Embedded in Fatty-Fibro-Glandular
Tissues
359
15.4.4
Case Study with Abnormalities Embedded in Dense-Fibro-Glandular
Tissues
359
15.5
Result Evaluation
360
15.5.1
Statistical Analysis
361
15.5.2
ROC Analysis
361
15.5.3
Accuracy Estimation
365
15.6
Comparative Analysis
366
15.7
Conclusion
366
Acknowledgments
366
References
367
16
Automatic Detection of Coronary Artery Stenosis Using Bayesian
ClassiÔ¨Åcation and Gaussian Filters Based on DiÔ¨Äerential
Evolution
369
Ivan Cruz-Aceves, Fernando Cervantes-Sanchez, and Arturo Hernandez-Aguirre
16.1
Introduction
369
16.2
Background
370
16.2.1
Gaussian Matched Filters
371
16.2.2
DiÔ¨Äerential Evolution
371
16.2.2.1 Example: Global Optimization of the Ackley Function
373

Contents
xv
16.2.3
Bayesian ClassiÔ¨Åcation
375
16.2.3.1 Example: ClassiÔ¨Åcation Problem
375
16.3
Proposed Method
377
16.3.1
Optimal Parameter Selection of GMF Using DiÔ¨Äerential Evolution
377
16.3.2
Thresholding of the Gaussian Filter Response
378
16.3.3
Stenosis Detection Using Second-Order Derivatives
378
16.3.4
Stenosis Detection Using Bayesian ClassiÔ¨Åcation
379
16.4
Computational Experiments
381
16.4.1
Results of Vessel Detection
382
16.4.2
Results of Vessel Segmentation
382
16.4.3
Evaluation of Detection of Coronary Artery Stenosis
384
16.5
Concluding Remarks
386
Acknowledgment
388
References
388
17
Evaluating the EÔ¨Écacy of Multi-resolution Texture Features for
Prediction of Breast Density Using Mammographic Images
391
Kriti, Harleen Kaur, and Jitendra Virmani
17.1
Introduction
391
17.1.1
Comparison of Related Methods with the Proposed Method
397
17.2
Materials and Methods
398
17.2.1
Description of Database
398
17.2.2
ROI Extraction Protocol
398
17.2.3
WorkÔ¨Çow for CAD System Design
398
17.2.3.1 Feature Extraction
400
17.2.3.2 ClassiÔ¨Åcation
407
17.3
Results
410
17.3.1
Results Based on ClassiÔ¨Åcation Performance of the ClassiÔ¨Åers (ClassiÔ¨Åcation
Accuracy and Sensitivity) for Each Class
411
17.3.1.1 Experiment I: To Determine the Performance of DiÔ¨Äerent FDVs Using SVM
ClassiÔ¨Åer
411
17.3.1.2 Experiment II: To Determine the Performance of DiÔ¨Äerent FDVs Using SSVM
ClassiÔ¨Åer
412
17.3.2
Results Based on Computational EÔ¨Éciency of ClassiÔ¨Åers for Predicting 161
Instances of Testing Dataset
412
17.4
Conclusion and Future Scope
413
References
415
Index
423

xvii
Editor Biographies
Dr Siddhartha Bhattacharyya earned his bachelor‚Äôs in Physics, bachelor‚Äôs in Optics
and Optoelectronics, and master‚Äôs in Optics and Optoelectronics from University of
Calcutta, India, in 1995, 1998, and 2000, respectively. He completed a PhD in computer
science and engineering from Jadavpur University, India, in 2008. He is the recipient
of the University Gold Medal from the University of Calcutta for his master‚Äôs in 2012.
He is also the recipient of the coveted ADARSH VIDYA SARASWATI RASHTRIYA
PURASKAR for excellence in education and research in 2016. He is the recipient of the
Distinguished HoD Award and Distinguished Professor Award conferred by Com-
puter Society of India, Mumbai Chapter, India in 2017. He is also the recipient of the
coveted Bhartiya Shiksha Ratan Award conferred by Economic Growth Foundation,
New Delhi in 2017.
He is currently the Principal of RCC Institute of Information Technology, Kolkata,
India. In addition, he is serving as the Dean of Research and Development of the insti-
tute from November 2013. Prior to this, he was the Professor and Head of Informa-
tion Technology of RCC Institute of Information Technology, Kolkata, India, from 2014
to 2017. Before this, he was an Associate Professor of Information Technology in the
same institute, from 2011 to 2014. Before that, he served as an Assistant Professor in
Computer Science and Information Technology of University Institute of Technology,
The University of Burdwan, India, from 2005 to 2011. He was a Lecturer in Informa-
tion Technology of Kalyani Government Engineering College, India, during 2001‚Äì2005.
He is a coauthor of four books and the coeditor of eight books, and has more than
175 research publications in international journals and conference proceedings to his
credit. He has got a patent on intelligent colorimeter technology. He was the convener
of the AICTE-IEEE National Conference on Computing and Communication Systems
(CoCoSys-09) in 2009. He was the member of the Young Researchers‚Äô Committee of the
WSC 2008 Online World Conference on Soft Computing in Industrial Applications. He
has been the member of the organizing and technical program committees of several
national and international conferences. He served as the Editor-in-Chief of Interna-
tional Journal of Ambient Computing and Intelligence (IJACI) published by IGI Global
(Hershey, PA, USA) from July 17, 2014, to November 6, 2014. He was the General Chair
of the IEEE International Conference on Computational Intelligence and Communica-
tion Networks (ICCICN 2014) organized by the Department of Information Technol-
ogy, RCC Institute of Information Technology, Kolkata, in association with Machine
Intelligence Research Labs, Gwalior, and IEEE Young Professionals, Kolkata Section; it
was held at Kolkata, India, in 2014. He is the Associate Editor of International Journal

xviii
Editor Biographies
of Pattern Recognition Research. He is the member of the editorial board of Interna-
tional Journal of Engineering, Science and Technology and ACCENTS Transactions on
Information Security (ATIS). He is also the member of the editorial advisory board of
HETC Journal of Computer Engineering and Applications. He has been the Associate
Editor of the International Journal of BioInfo Soft Computing since 2013. He is the Lead
Guest Editor of the Special Issue on Hybrid Intelligent Techniques for Image Analysis and
Understanding of Applied Soft Computing (Elsevier, Amsterdam). He was the General
Chair of the 2015 IEEE International Conference on Research in Computational Intel-
ligence and Communication Networks (ICRCICN 2015) organized by the Department
of Information Technology, RCC Institute of Information Technology, Kolkata, in asso-
ciation with IEEE Young Professionals, Kolkata Section and held at Kolkata, India, in
2015. He is the Lead Guest Editor of the Special Issue on Computational Intelligence and
Communications in International Journal of Computers and Applications (IJCA) (Tay-
lor & Francis, London) in 2016. He has been the Issue Editor of International Journal of
Pattern Recognition Research since January 2016. He was the General Chair of the 2016
International Conference on Wireless Communications, Network Security and Signal
Processing (WCNSSP2016) held during June 26‚Äì27, 2016, at Chiang Mai, Thailand. He
is the member of the editorial board of Applied Soft Computing (Elsevier, Amsterdam).
He has visited several leading universities in several countries like China, Thailand,
and Japan for delivering invited lectures. His research interests include soft comput-
ing, pattern recognition, multimedia data processing, hybrid intelligence, and quantum
computing. Dr Bhattacharyya is a Fellow of Institute of Electronics and Telecommunica-
tion Engineers (IETE), India. He is a senior member of Institute of Electrical and Elec-
tronics Engineers (IEEE), USA; Association for Computing Machinery (ACM), USA;
and International Engineering and Technology Institute (IETI), Hong Kong. He is a
member of International Rough Set Society, International Association for Engineers
(IAENG), Hong Kong; Computer Science Teachers Association (CSTA), USA; Interna-
tional Association of Academicians, Scholars, Scientists and Engineers (IAASSE), USA;
Institution of Engineering and Technology (IET), UK; and Institute of Doctors Engineers
and Scientists (IDES), India. He is a life member of Computer Society of India, Opti-
cal Society of India, Indian Society for Technical Education, and Center for Education
Growth and Research, India.
Dr Indrajit Pan has done his BE in Computer Science and Engineering with Honors
from The University of Burdwan in 2005; M.Tech. in Information Technology from Ben-
gal Engineering and Science University, Shibpur, in 2009; and PhD (Engg.) from Indian
Institute of Engineering Science and Technology, Shibpur, in 2015. He is the recipient
of BESU, University Medal for securing Ô¨Årst rank in M.Tech. (IT). He has a couple of
national- and international-level research publications and book chapters to his credit.
He has attended several international conferences, national-level faculty development
programs, workshops, and symposiums.
In this Institute, his primary responsibility is teaching and project guidance at UG
(B.Tech.) and PG (M.Tech. and MCA) levels as Assistant Professor of Information Tech-
nology (erstwhile Lecturer since joining in February 2006). Apart from this, he has car-
ried out additional responsibility of Single Point of Contact (SPoC) for Infosys Campus
Connect Initiative in 2009‚Äì2011, and Coordinator of Institute-level UG Project Com-
mittee in 2008‚Äì2010. At present, his additional responsibility includes Nodal OÔ¨Écer
of Institutional Reforms for TEQIP‚ÄìII Project since 2011 and Member Secretary of

Editor Biographies
xix
Academic Council since 2013. Apart from these, he has actively served as an orga-
nizing member of several Faculty Development Programs and International Confer-
ences (ICCICN 2014) for RCCIIT. He has also acted as the Session Chair in an Inter-
national Conference (ICACCI 2013) and Member of Technical Committee for FICTA
2014. Before joining RCCIIT, Indrajit served Siliguri Institute of Technology, Darjeeling,
as a Lecturer in CSE/IT from 2005 to 2006.
Indrajit is a Member of Institute of Electrical and Electronics Engineers (IEEE), USA;
and Association for Computing Machinery (ACM), USA.
Dr Anirban Mukherjee did his bachelor‚Äôs in Civil Engineering in 1994 from Jadavpur
University, Kolkata. While in service, he achieved a professional Diploma in Opera-
tions Management (PGDOM) in 1998 and completed his PhD on Automatic Diagram
Drawing based on Natural Language Text Understanding from Indian Institute of Engi-
neering, Science and Technology (IIEST), Shibpur, in 2014. Serving RCC Institute of
Information Technology (RCCIIT), Kolkata, since its inception (in 1999), he is cur-
rently an Associate Professor and Head of the Department of Engineering Science &
Management at RCCIIT. Before joining RCCIIT, he served as an Engineer in the Sci-
entiÔ¨Åc & Technical Application Group in erstwhile RCC, Calcutta, for six years. His
research interest includes computer graphics, computational intelligence, optimization,
and assistive technology. He has coauthored two UG engineering textbooks: one on
Computer Graphics and Multimedia and another on Engineering Mechanics. He has
also coauthored more than 18 books on Computer Graphics/Multimedia for distance
learning courses He holds BBA/MBA/BCA/MCA/B.Sc (Comp.Sc.)/M.Sc (IT) of diÔ¨Äer-
ent universities of India. He has a few international journal articles, book chapters, and
conference papers to his credit. He is in the editorial board of International Journal of
Ambient Computing and Intelligence (IJACI).
Dr Paramartha Dutta, born 1966, did his bachelor‚Äôs and master‚Äôs in Statistics from
the Indian Statistical Institute, Calcutta, in the years 1988 and 1990, respectively. He
afterward completed his master‚Äôs degree of technology in Computer Science from the
same Institute in 1993, and PhD in engineering from the Bengal Engineering and Sci-
ence University, Shibpur, in 2005, respectively. He has served in the capacity of research
personnel in various projects funded by Government of India, which include DRDO,
CSIR, Indian Statistical Institute, Calcutta, and others. Dr Dutta is now a Professor in
the Department of Computer and System Sciences of the Visva Bharati University, West
Bengal, India. Prior to this, he served Kalyani Government Engineering College and Col-
lege of Engineering in West Bengal as a full-time faculty member. Dr Dutta remained
associated as Visiting/Guest Faculty of several universities and institutes, such as West
Bengal University of Technology, Kalyani University, and Tripura University. He has
coauthored eight books and has also Ô¨Åve edited books to his credit. He has published
about 185 papers in various journals and conference proceedings, both international
and national; as well as several book chapters in edited volumes of reputed interna-
tional publishing houses like Elsevier, Springer-Verlag, CRC Press, and John Wiley, to
name a few.
Dr Dutta has guided three scholars who already had been awarded their PhD.
Presently, he is supervising six scholars for their PhD program. Dr Dutta has served
as editor of special volumes of several international journals published by publishers
of international repute such as Springer. Dr Dutta, as investigator, could imple-
ment successfully projects funded by AICTE, DST of the Government of India.

xx
Editor Biographies
Prof. Dutta has served/serves in the capacity of external member of boards of studies
of relevant departments of various universities encompassing West Bengal University
of Technology, Kalyani University, Tripura University, Assam University, and Silchar, to
name a few. He had the opportunity to serve as the expert of several interview boards
conducted by West Bengal Public Service Commission, Assam University, Silchar,
National Institute of Technology, Arunachal Pradesh, Sambalpur University, and so on.
Dr Dutta is a Life Fellow of the Optical Society of India (OSI); Computer Society of
India (CSI); Indian Science Congress Association (ISCA); Indian Society for Technical
Education (ISTE); Indian Unit of Pattern Recognition and ArtiÔ¨Åcial Intelligence
(IUPRAI) ‚Äì the Indian aÔ¨Éliate of the International Association for Pattern Recogni-
tion (IAPR); and senior member of Associated Computing Machinery (ACM); IEEE
Computer Society, USA; and IACSIT.

xxi
List of Contributors
Tankut Acarman
Computer Engineering Department
Galatasaray University
Istanbul
Turkey
Loganathan Agilandeeswari
School of Information
Technology & Engineering
VIT University
Vellore
Tamil Nadu
India
Samir Kumar Bandyopadhyay
Department of Computer
Science and Engineering
University of Calcutta
Salt Lake Campus
Kolkata
West Bengal
India
Piyush Bhandari
Indian Institute of Technology Patna
Bihta, Patna
Bihar, India
S. Bharathi
Department of Electronics and
Communication Engineering
Dr. Mahalingam College of Engineering
and Technology, Coimbatore
Tamil Nadu, India
Sangita Bhattacharjee
Department of Computer
Science and Engineering
University of Calcutta
Salt Lake Campus
Kolkata
West Bengal
India
Siddhartha Bhattacharyya
Department of Information Technology
RCC Institute of Information Technology
Kolkata
West Bengal
India
Nesrine Bouadjenek
Laboratoire d‚ÄôIng√©nierie des Syst√®mes
Intelligents et Communicants (LISIC)
Faculty of Electronics and
Computer Sciences
University of Sciences and Technology
Houari Boumediene (USTHB), Algiers
Algeria
Fernando Cervantes-Sanchez
Centro de Investigaci√≥n
en Matem√°ticas (CIMAT)
A.C., Jalisco S/N
Col. Valenciana
Guanajuato
M√©xico

xxii
List of Contributors
Anirban Chakraborty
Department of Computer Science
Barrackpore Rastraguru
Surendranath College
Barrackpore
Kolkata
West Bengal
India
Debashish Chakravarty
Indian Institute of Technology
Kharagpur
West Bengal
India
Subhamoy Chatterjee
Indian Institute of Technology Patna
Bihta, Patna
Bihar, India
Youcef Chibani
Laboratoire d‚ÄôIng√©nierie des Syst√®mes
Intelligents et Communicants (LISIC)
Faculty of Electronics and
Computer Sciences
University of Sciences and Technology
Houari Boumediene (USTHB), Algiers
Algeria
Aditi Roy Chowdhury
Department of Computer
Science and Technology
Women‚Äôs Polytechnic
Jodhpur Park
Kolkata
West Bengal
India
Ivan Cruz-Aceves
CONACYT ‚Äì Centro de Investigaci√≥n
en Matem√°ticas (CIMAT)
A.C., Jalisco S/N
Col. Valenciana
Guanajuato
M√©xico
Sunanda Das
Department of Computer
Science & Engineering
University Institute of Technology
The University of Burdwan
Burdwan
West Bengal
India
Supratim Das
Department of Computer
Science and Engineering
Jadavpur University
Kolkata
West Bengal
India
Sourav De
Department of Computer
Science & Engineering
Cooch Behar Government
Engineering College
Cooch Behar
West Bengal
India
Paramartha Dutta
Department of Computer Science
and System Sciences
Visva-Bharati University
Santiniketan
West Bengal
India
V. Gurunathan
Department of Electronics and
Communication Engineering
Dr. Mahalingam College of
Engineering and Technology
Tamil Nadu
India

List of Contributors
xxiii
Joydev Hazra
Department of Information Technology
Heritage Institute of Technology
Kolkata
West Bengal
India
Arturo Hernandez-Aguirre
Centro de Investigaci√≥n
en Matem√°ticas (CIMAT)
A.C., Jalisco S/N
Col. Valenciana
Guanajuato
M√©xico
Deepthi P. Hudedagaddi
School of Computer Science and
Engineering (SCOPE)
VIT University
Vellore
Tamil Nadu
India
Earnest Paul Ijjina
Visual Learning and
Intelligence Group (VIGIL)
Department of Computer
Science and Engineering
Indian Institute of Technology
Hyderabad (IITH)
Hyderabad
Telangana
India
Harleen Kaur
Electrical and Instrumentation
Engineering Department
Thapar University
Patiala
Punjab
India
Mahesh Kumar Kolekar
Indian Institute of Technology Patna
Bihta, Patna
Bihar, India
B. Kondalarao
Department of Mechanical Engineering
Indian Institute of Technology
Kharagpur
West Bengal
India
Kriti
Electrical and Instrumentation
Engineering Department
Thapar University
Patiala
Punjab
India
Brejesh Lall
Department of Electrical Engineering
Indian Institute of Technology Delhi
India
Vaibhav Lodhi
Indian Institute of Technology
Kharagpur
West Bengal
India
Indra Kanta Maitra
Department of Information Technology
B.P. Poddar Institute of Management and
Technology
Kolkata
West Bengal
India
Jayatra Majumdar
Department of Computer Science
Barrackpore Rastraguru Surendranath
College
Barrackpore
Kolkata
West Bengal
India

xxiv
List of Contributors
J.K. Mandal
Department of Computer
Science & Engineering
University of Kalyani
Kalyani
Nadia
West Bengal
India
Pabitra Mitra
Indian Institute of Technology
Kharagpur
West Bengal
India
Chalavadi Krishna Mohan
Visual Learning and
Intelligence Group (VIGIL)
Department of Computer
Science and Engineering
Indian Institute of Technology
Hyderabad (IITH)
Hyderabad
Telangana
India
Mita Nasipuri
Department of Computer Science and
Engineering
Jadavpur University
Kolkata
West Bengal
India
Hassiba Nemmour
Laboratoire d‚ÄôIng√©nierie des Syst√®mes
Intelligents et Communicants (LISIC)
Faculty of Electronics and
Computer Sciences
University of Sciences and Technology
Houari Boumediene (USTHB) Algiers
Algeria
Arnab Patra
Department of Computer Science
Barrackpore Rastraguru
Surendranath College
Barrackpore
Kolkata
West Bengal
India
Manoharan Prabukumar
School of Information
Technology & Engineering
VIT University
Vellore
Tamil Nadu
India
D.K. Pratihar
Department of Mechanical Engineering
Indian Institute of Technology
Kharagpur
West Bengal
India
S. Sahoo
Department of Mechanical Engineering
Indian Institute of Technology
Kharagpur
West Bengal
India
Arun Kumar Sangaiah
School of Computing
Science & Engineering
VIT University
Vellore
Tamil Nadu
India
Ram Sarkar
Department of Computer
Science and Engineering
Jadavpur University
Kolkata
West Bengal
India

List of Contributors
xxv
Pawan Kumar Singh
Department of Computer Science and
Engineering
Jadavpur University
Kolkata
West Bengal
India
Siddharth Srivastava
Department of Electrical Engineering
Indian Institute of Technology Delhi
India
R. Sudhakar
Department of Electronics and
Communication Engineering
Dr. Mahalingam College of
Engineering and Technology
Coimbatore
Tamil Nadu
India
B.K. Tripathy
School of Computer Science and
Engineering (SCOPE)
VIT University
Vellore
Tamil Nadu
India
Jitendra Virmani
CSIR-CSIO
Sector-30C
Chandigarh
India
Ramazan Y√≠ld√≠z
Computer Engineering Department
Galatasaray University
Istanbul
Turkey

xxvii
Foreword
Image analysis and understanding have been daunting tasks in computer vision given the
high level of uncertainty involved therein. At the same time, a proper analysis of images
plays a key role in many real-life applications. Examples of applications include image
processing, image mining, image inpainting, video surveillance, and intelligent trans-
portation systems, to name a few. Albeit there exists a plethora of classical techniques for
addressing the problem of image analysis, which include Ô¨Åltering, hierarchical morpho-
logic algorithms, 2D histograms, mean shift clustering, and graph-based segmentation,
most of these techniques often fall short owing to their incapability in handling inherent
real-life uncertainties. In past decades, researchers have been able to address diÔ¨Äerent
types of uncertainties prevalent in real-world images, thanks to the evolving state of the
art of intelligent tools and techniques such as convolutional neural networks (CNNs)
and deep learning. In this direction, computational intelligence techniques deserve spe-
cial mention owing to their Ô¨Çexibility, application-free usability, and adaptability. Of
late, hybridization of diÔ¨Äerent computational intelligence techniques has come up with
promising avenues in that these are more robust and oÔ¨Äer more eÔ¨Écient solutions in
real time.
This book aims to introduce the readers with the basics of image analysis and
understanding, with recourse to image thresholding, image segmentation, and image
and multimedia data analysis. The book also focuses on the foundations of hybrid
intelligence as it applies to image analysis and understanding. As a sequel to this,
diÔ¨Äerent state-of-the-art hybrid intelligent techniques for addressing the problem of
image analysis will be illustrated to enlighten the readers of upcoming research trends.
As an example of the recent trends in image analysis and understanding, albeit aging
mitigates the glamor in human beings, wrinkles in face images can often be used for
estimation of age progression in human beings. This can be further utilized for trac-
ing unknown or missing persons. Images exhibit varied uncertainty and ambiguity of
information, and hence understanding an image scene is far from being a general pro-
cedure. The situation becomes even graver when the images become corrupt with noise
artifacts.
In this book, the editors have attempted to deliver some of the recent trends in hybrid
intelligence as it applies to image analysis and understanding. The book contains 17
well-versed chapters illustrating diversiÔ¨Åed areas of application of image analysis using
hybrid intelligence. These include multilevel image segmentation, character recogni-
tion, image analysis, video image processing, hyperspectral image analysis, and medical
image analysis.

xxviii
Foreword
The Ô¨Årst chapter deals with multilevel image segmentation. The authors propose
a modiÔ¨Åed genetic algorithm (MfGA) to generate the optimized class levels of the
multilevel images, and those class levels are employed as the initial input in the
fuzzy c-means (FCM) algorithm. A performance comparison is depicted between the
MfGA-based FCM algorithm, the conventional genetic algorithm (GA)-based FCM
algorithm, and the well-known FCM algorithm with the help of three real-life multilevel
grayscale images. The comparison revealed the superiority of the proposed method
over the other two image segmentation algorithms.
Chapters 2 to 5 address the issue of character recognition and soft biometrics.
Chapter 2 shows pros and cons of an entropy-based FCM clustering technique to
classify huge training data for English character recognition. In chapter 3, the authors
propose a two-stage word-level script identiÔ¨Åcation technique for eight handwritten
popular scripts, namely, Bangla, Devanagari, Gurumukhi, Oriya, Malayalam, Telugu,
Urdu, and Roman. Firstly, discrete wavelet transform (DWT) is applied on the input
word images to extract the most representative information, whereas in the second
stage, radon transform (RT) is applied to the output of the Ô¨Årst stage to compute a set
of 48 statistical features from each word image. Chapter 4 presents a skin color region
segmentation method based on K-means clustering and Mahalanobis distance for static
hand gesture recognition. The Ô¨Ånal chapter in this group deals with soft-biometrics
prediction. There, three prediction systems are developed using a support vector
machine (SVM) classiÔ¨Åer associated to various gradient and textural features. Since
diÔ¨Äerent features yield diÔ¨Äerent aspects of characterization, the authors investigate
classiÔ¨Åer combination in order to improve the prediction accuracy. As a matter of fact,
the fuzzy integral is used to produce a robust soft-biometrics prediction.
Chapters 6 and 7 focus on image analysis applications. More speciÔ¨Åcally, in chapter 6,
the authors draw an analogy and comparison between the working principle of CNNs
and the human brain. In chapter 7, the authors propose a framework for human action
recognition that is trained using evolutionary algorithms and deep learning. A CNN
classiÔ¨Åer designed to recognize human actions from action bank features is initialized
by evolutionary algorithms and trained using back-propagation algorithms.
Chapter 8 is targeted to video image processing applications. The authors propose a
technique using Haar-like simple features to describe object models in chapter 8. This
technique is applied with Adaboost classiÔ¨Åer for object detection on the video records.
The tracking method is described and illustrated by fusing global and local features.
Chapter 9 deals with a GIS-based application. The proposed GIS-anchored system
extends a helping hand toward common and innocent people and exposes the trails for
releasing themselves from the clutches of criminals. The chief intent of the proposed
work is to not only check out the hot-spot areas (the crime-prone areas), but also give
a glance to the Ô¨Çourishing of criminal activities in future. The process of determination
of hot-spots is carried out by associating rank (an integer value) to each ward/block on
the digitized map. The process of hooking up rank to a speciÔ¨Åc region is carried out on
the basis of criminal activity at that particular region.
Chapters 10 and 11 deal with hyperspectral image analysis. Chapter 10 covers the
hyperspectral data analysis and processing algorithms organized into three topics:
spectral unmixing, classiÔ¨Åcation, and target identiÔ¨Åcation. In chapter 11, the authors
deal with the band selection problem. They use the fuzzy k-nearest neighbors (KNN)

Foreword
xxix
technique to calculate the aÔ¨Énity of the band combination based on features extracted
by 2D principal component analysis (PCA).
The remaining chapters deal with medical image analysis using diÔ¨Äerent hybrid intel-
ligent techniques. Chapter 12 aims to introduce several uncertainty-based hybrid clus-
tering algorithms and their applications in medical image analysis. In chapter 13, the
authors propose a diagnosis system for early detection of breast cancer tissues from
digital mammographic breast images using a Cuckoo search optimization algorithm and
SVM classiÔ¨Åers. In general, the complete diagnosis process involves various stages, such
as preprocessing of images, segmentation of the breast cancer region from its surround-
ings, extracting tissues of interest and then determining the associated features that may
be vital, and, Ô¨Ånally, classifying the tissue into either benign or malignant. Chapter 14
proposes a new approach for biometric recognition, using dorsal, palm, Ô¨Ånger, and wrist
veins of the hand. The analysis of these vein modalities is done in both the spatial domain
and frequency domain. In the spatial domain, a modiÔ¨Åed 2D Gabor Ô¨Ålter is used for fea-
ture extraction, and these features are fused at both the feature level and score level for
further analysis. Similarly in the frequency domain, a contourlet transform is used for
feature extraction, a multiresolution singular value decomposition technique is utilized
for fusing these features, and further classiÔ¨Åcation is done with the help of an SVM clas-
siÔ¨Åer. An automated segmentation technique has been proposed in chapter 15 for dig-
ital mammograms to detect abnormal mass/masses [i.e., tumor(s)]. The accuracy of an
automatic segmentation algorithm requires standardization (i.e., preparation of images
and preprocessing of medical images that are mandatory, distinct, and sequential). The
detection method is based on a modiÔ¨Åed seeded region growing algorithm (SRGA)
followed by a step-by-step statistical elimination method. Finally a decision-making sys-
tem is proposed to isolate mass/masses in mammograms. Chapter 16 presents a novel
method for the automatic detection of coronary stenosis in X-ray angiograms. In the
Ô¨Årst stage, Gaussian matched Ô¨Ålters (GMFs) are applied over the input angiograms for
the detection of coronary arteries. The GMF method is tuned in a training step apply-
ing diÔ¨Äerential evolution (DE) for the optimization process, which is evaluated using
the area under the receiver operating characteristic (ROC) curve. In the second stage,
an iterative thresholding method is applied to extract vessel-like structures from the
background of the Gaussian Ô¨Ålter response. The authors of chapter 17 introduce an
eÔ¨Écient CAD system, based on multiresolution texture descriptors using 2D wavelet
transform, which has been implemented using a smooth SVM (SSVM) classiÔ¨Åer. The
standard Mammographic Image Analysis Society (MIAS) dataset has been used for clas-
sifying the breast tissue into one of three classes, namely, fatty (F), fatty-glandular (FG),
and dense-glandular (DG). The performance of the SSVM-based CAD system has been
compared with SVM-based CAD system design.
From the varied nature of the case studies treated within the book, I am fully con-
Ô¨Ådent and can state that the editors have done a splendid job in bringing forward the
facets of hybrid intelligence to the scientiÔ¨Åc community. This book will, hence, not only
serve as a good reference for senior researchers, but also stand in good stead for novice
researchers and practitioners who have started working in this Ô¨Åeld. Moreover, the book
will also serve as a reference book for some parts of the curriculum for postgraduate and
undergraduate disciplines in computer science and engineering, electronics and com-
munication engineering, and information technology.

xxx
Foreword
I invite the readers to enjoy the book and take advantage of its beneÔ¨Åts. One could
join the team of computational intelligence designers and bring new insights into this
developing and challenging enterprise.
Italy, November 2016
Cesare Alippi

xxxi
Preface
Image processing happens to be a wide subject encompassing various problems therein.
The very understanding of diÔ¨Äerent image-processing applications is not an easy task
considering the wide variety of contexts they represent. Naturally, the better the under-
standing, the better the analysis thereof is expected to be. The uncertainty and/or impre-
cision associated inherently in this Ô¨Åeld make the problem even more challenging. The
last two decades have witnessed several soft computing techniques for addressing such
issues. Each such soft computing method has strengths as well as shortcomings. As a
result, no individual soft computing technique oÔ¨Äers a solution to such problems in
a comprehensive sense that is uniformly applicable. The concept of hybridization per-
vades from this situation, where more than one soft computing technique are subjected
to coupling with the hope that the weakness of one may be overcome by the other.
Present-time reporting is more oriented toward that. In fact, the present volume is also
aimed at inviting the contributions toward adequate justiÔ¨Åcation for hybrid methods in
diÔ¨Äerent aspects of image processing.
The authors of chapter 1 try to demonstrate the signiÔ¨Åcance of using modiÔ¨Åed genetic
algorithms in obtaining optimized class levels, which when subsequently applied as ini-
tial input in the fuzzy c-means (FCM) algorithm overcome the limitation of possible
premature convergence in FCM-based clustering.
The authors justify through extensive experimentation in chapter 2 as to how
FCM-based clustering may be eÔ¨Äectively hybridized with entropy-based clustering
for English character recognition. The main uniqueness of the contribution of chapter
3 is the development of a two-stage system capable of recognizing texts containing
multilingual handwritten scripts comprising eight Indian languages. The authors
substantiate the eÔ¨Äectiveness of their method by evaluating an extensive database of
16,000 handwritten words with a reported achievement of 97.69%.
In chapter 4, the authors propose a robust technique based on K-means clustering
using a Mahalanobis distance metric to achieve static hand gesture recognition. In their
proposed method, they consider zoning-based shape feature extraction to overcome the
problem of misclassiÔ¨Åcation associated to other techniques for this purpose.
The use of a support vector machine (SVM) for classiÔ¨Åcation and eÔ¨Äective
soft-biometric prediction on the basis of three gradient and texture features, and
how these features are combined to attain higher prediction accuracy, are the car-
dinal contributions of chapter 5. The authors validate their results by extensive
experimentation.

xxxii
Preface
In chapter 6, the authors attempt to draw an eÔ¨Äective comparison of the functioning
of a convolutional neural network (CNN) with that of the human brain by highlighting
the fundamental explanation for the functioning of CNNs.
In chapter 7, the authors demonstrate how hybridization of evolutionary optimized
deep learning may be eÔ¨Äective in human behavior analysis.
The authors in chapter 8 explore how eÔ¨Äectively various feature extraction techniques
may be used for vehicle tracking purposes with requisite validation by experimental
results.
Chapter 9 reports a GIS-anchored system that oÔ¨Äers a helping hand toward common
and innocent people by identifying crime hotspot regions and thereby alerting them to
criminals‚Äô possible whereabouts.
In chapter 10, authors oÔ¨Äer hyper-spectral data analysis and processing algorithms
consisting of spectral unmixing, classiÔ¨Åcation, and target identiÔ¨Åcation. The eÔ¨Äective-
ness is validated by adequate test results.
In chapter 11, the authors demonstrate the eÔ¨Äectiveness of artiÔ¨Åcial immune systems
(AISs) in identifying important bands in hyperspectral images. The aÔ¨Énity of band com-
bination, derived based on 2D principal component analysis (2DPCA), is computed by
fuzzy k-nearest neighbor (KNN), reporting convincing results.
In chapter 12, the authors aim to study several hybrid clustering algorithms capable
of tackling uncertainties and their applications in medical image analysis.
In chapter 13, the authors hybridize Otsu thresholding and morphological segmenta-
tion algorithms to achieve accurate segmentation of breast cancer images. Subsequently,
relevant features are optimized using Cuckoo search, which in turn equips SVMs to clas-
sify breast images into benign or malignant. Their Ô¨Ånding has been substantiated by the
accuracy reported.
Chapter 14 proposes a new approach for biometric recognition of a person, using dor-
sal, palm, Ô¨Ånger, and wrist veins of the hand. The analysis of these vein modalities is done
in both the spatial domain and frequency domain with encouraging results reported.
In chapter 15, the authors are able to achieve a convincing performance in terms of
metrics like accuracy, sensitivity, and speciÔ¨Åcity of their proposed method, based on
computer-aided automated segmentation of mammograms comprising a seeded region
growing algorithm followed by statistical elimination and eventually decision making.
In chapter 16, authors provide a novel method for the automatic detection of coronary
stenosis in X-ray angiograms, comprising three stages: (1) diÔ¨Äerential evolution-based
optimization of Gaussian matching Ô¨Ålters (GMFs), (2) application of iterative threshold-
ing of the Gaussian Ô¨Ålter response, followed by (3) naive Bayes classiÔ¨Åcation for deter-
mining vessel stenosis. Their technique is justiÔ¨Åed by encouraging experimental results.
In the work reported in chapter 17, the authors substantiate the eÔ¨Äectiveness of an
eÔ¨Écient CAD system based on multiresolution texture descriptors (derived from ten
diÔ¨Äerent compact support wavelet Ô¨Ålters) using 2D wavelet transform using a smooth
SVM (SSVM) classiÔ¨Åer. The proposed technique is validated with adequate experimen-
tation reporting very encouraging results in the context of classiÔ¨Åcation of mammogram
images.
The chapters appearing in this volume show a wide spectrum of applications pertain-
ing to image processing through which the use of hybrid techniques is vindicated. It is
needless to mention that the reporting of various chapters in the present scope indi-
cates very limited applicability of hybridization in the Ô¨Åeld of various image-processing

Preface
xxxiii
problems. There are, of course, many more to come in days ahead. From that point
of view, the editors of the present volume want to avail this opportunity to express
their gratitude to the contributors of diÔ¨Äerent chapters, without whose participation the
present treatise would not have arrived in the present form. The editors feel encouraged
to expect many more contributions from the present authors in this Ô¨Åeld in their future
endeavours also. This is just a limited eÔ¨Äort rendered by the editors to accommodate
what the relevant research community is thinking today. In fact, from that perspective,
it is just the beginning of such an endeavour and deÔ¨Ånitely not the end. Last but not the
least, the editors also thank Wiley for all the cooperation they rendered as a professional
publishing house toward making the present eÔ¨Äort a success.
Siddhartha Bhattacharyya
India, November 2016
Indrajit Pan
Anirban Mukherjee
Paramartha Dutta

xxxv
About the Companion website
Don‚Äôt forget to visit the companion website for this book:
www.wiley.com/go/bhattacharyya/hybridintelligence
There you will Ô¨Ånd valuable material designed to enhance your learning, including:
‚Ä¢ Videos
Scan this QR code to visit the companion website.

1
1
Multilevel Image Segmentation Using ModiÔ¨Åed Genetic
Algorithm (MfGA)-based Fuzzy C-Means
Sourav De1, Sunanda Das2, Siddhartha Bhattacharyya3, and Paramartha Dutta4
1Department of Computer Science and Engineering, Cooch Behar Government Engineering College, Cooch Behar,
West Bengal, India
2Department of Computer Science and Engineering, University Institute of Technology, The University of Burdwan,
Burdwan, West Bengal, India
3Department of Information Technology, RCC Institute of Information Technology, Kolkata, West Bengal, India
4Department of Computer Science and System Sciences, Visva-Bharati University, Santiniketan, West Bengal, India
1.1
Introduction
Image segmentation plays a key role in image analysis and pattern recognition and also
has other application areas, like machine vision, biometric measurements, medical
imaging, and so on for the purposes of detecting, recognzing or tracking an object. The
objective of image segmentation is to segregate an image into a set of disjoint regions
on the basis of uniform and homogeneous attributes such as intensity, color, texture,
and so on. The attributes in the diÔ¨Äerent regions are heterogeneous to each other,
but the attributes in the same region are homogeneous to each other. On the basis of
the inherent features of an image and some a priori knowledge and/or presumptions
about the image, the pixels of that image can be classiÔ¨Åed successfully. Let an image be
represented by I, and that image can be partitioned into n number of subimages (i.e.,
I1, I2,..In), such that:
‚Ä¢ ‚ãÉK
i=1 Ii = I;
‚Ä¢ Ii is a connected segment;
‚Ä¢ Ii ‚â†‚àÖfor i = 1, ¬∑ ¬∑ ¬∑ , K; and
‚Ä¢ Ii ‚à©Ij = ‚àÖfor i = 1, ¬∑ ¬∑ ¬∑ , K, j = 1, ¬∑ ¬∑ ¬∑ , K, and i ‚â†j.
DiÔ¨Äerent image segmentation techniques have been developed on the basis of the dis-
continuity and similarity of the intensity levels of an image. Multilevel grayscale and
color image segmentation became a perennial research area due to the diversity of the
grayscale and color-intensity gamuts. In diÔ¨Äerent research articles, the image segmenta-
tion problem is handled by diÔ¨Äerent types of classical and nonclassical image-processing
algorithms.
Among diÔ¨Äerent types of classical image segmentation techniques [1‚Äì3], edge detec-
tion and region growing, thresholding, normalized cut, and others are well-known
image segmentation techniques to segment the multilevel grayscale images. The image
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

2
Hybrid Intelligence for Image Analysis and Understanding
segmentation by the edge detection techniques are done by Ô¨Ånding out the boundaries
of the objects in that image. But an incorrect segmentation may happen with edge
detection algorithms as these processes are not assistive for segmenting any compli-
cated images or the blur images. Region-growing techniques are not eÔ¨Éciently applied
for multilevel image segmentation, as the diÔ¨Äerent regions of an image are not fully
separated. The image segmentation using thresholding techniques are fully dependent
on the histogram of that image. The images that have the distinctive background and
objects can be segmented eÔ¨Éciently by the thresholding techniques. This process may
not work if the distribution of the pixels in the image is very complex in nature.
To solve the clustering problems, the k-means [4] and the fuzzy c-means [5] are two
very renowned clustering algorithms. The common feature of these two algorithms is
that both start with a Ô¨Åxed number of predeÔ¨Åned cluster centroids. The meaning of k
in the k-means algorithm is the same as that of c in the FCM algorithm, and both k
and c signify the number of clusters. At the initial stage of the k-means algorithm, the
k number of cluster centroids are generated randomly. The clusters are formed with
the patterns on the basis of the minimum distance between the pattern and the clus-
ter centroids. The cluster centriods update their positions iteratively by minimizing the
sum of the squared error in between the corresponding members within the clusters.
This clustering algorithm is considered as hard clustering as it is assumed that each pat-
tern is a member of a single cluster. In FCM, the belongingness of a pattern within a
cluster is deÔ¨Åned by the degree of membership value of that pattern. The well-known
least-squares objective function is used to minimize, and the degree of membership of
the pattern will be updated to optimize the clustering problem. That is why FCM is
considered a soft clustering algorithm.
Quality improvement of the k-means algorithm has been reported in diÔ¨Äerent
research articles. Luo et al. [6] proposed a spatially constrained k-means approach
for image segmentation. Initially, the k-means algorithm is applied in feature space,
and after that, the spatial constraints are considered in the image plane. Khan and
Ahmad [7] tried to improve the k-means algorithm by modifying the initialization
of the cluster centers. The high-resolution images are segmented by the k-means
algorithm after optimization by the Pillar algorithm [8]. The k-means algorithm along
with the improved watershed segmentation algorithm are employed to segment the
medical images [9]. The noisy images can be segmented eÔ¨Éciently by the modiÔ¨Åed FCM
algorithm, which is proposed by Ahmed et al. [10, 11]. In this process, the objective
function for the standard FCM algorithm has been modiÔ¨Åed to allow the labels in the
immediate neighborhood of a pixel to inÔ¨Çuence its labeling [10]. The spatial information
of the image is incorporated in the traditional FCM algorithm to segment the noisy
grayscale images [12]. In this method, the spatial function is considered as the sum of all
the membership functions within the neighborhood of the pixel under consideration.
Noisy medical images can be segmented by FCM [13]. In this approach, the input
images are converted into multiscale images by smoothing them in diÔ¨Äerent scales. The
scaling operation, from high scale to low scale, is performed by FCM. The image with
the highest scale is processed with the FCM, and after that, the membership of the
current scale is determined by the cluster centers of the previous scale. Noordam et al.
[14] presented a geometrically guided FCM (GG-FCM) algorithm in which geometrical
information is added with the FCM algorithm during clustering. In this process, the
clustering is guided by the condition of each pixel, which is decided by the local

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
3
neighborhood of that pixel. DiÔ¨Äerent types of modiÔ¨Åed objective functions are applied
to segment the corrupted images to derive a lower error rate [10, 15]. A modiÔ¨Åed FCM
algorithm, considering spatial neighborhood information, is presented by Chen et al.
[16]. In this method, a simple and eÔ¨Äective 2D distance metric function is projected by
considering the relevance of the current pixel to its neighbor pixels, and an objective
function is also formed to update the cluster centers simultaneously in two dimensions
of the pixel value and its neighboring value.
However, the k-means algorithm and fuzzy c-means algorithm have some common
disadvantages. Both the algorithms need a predeÔ¨Åned number of clusters, and the cen-
troid initialization is also a problem. It may not be feasible to know the exact number of
clusters beforehand in a large data set. It has been observed in diÔ¨Äerent research articles
that diÔ¨Äerent initial numbers of clusters with diÔ¨Äerent initializations had been applied
to determine the exact number of clusters in a large data set [17]. Moreover, it may not
be eÔ¨Äective if the algorithm starts with a limited number of center of clusters. This type
of problem is known as the number of clusters dependency problem [18]. Ultimately, it
may happen that the solutions may be stuck in local minima instead of obtaining a global
optimal solution.
In this real-world scenario, most of the problems can be considered as optimization
problems. The diÔ¨Äerentiable functions are solved by the traditional heuristic algorithms,
but the real-world optimization problems are nondiÔ¨Äerentiable. It is rare that the non-
diÔ¨Äerentiable optimization functions are properly solved by a heuristic algorithm. In
many research articles, it has been found that nondiÔ¨Äerentiable optimization functions
are solved by diÔ¨Äerent types of metaheuristic approaches, and these algorithms are now
becoming more popular in the research arena. Applying stochastic principles, evolu-
tionary algorithms are now becoming an alternative way to solve optimization prob-
lems as well as clustering problems. These types of algorithms work on the basis of
probabilistic rules to search for a near-optimal solution from a global search space. By
inspiring the principle of natural genetics, genetic algorithms (GAs), diÔ¨Äerential evolu-
tion, particle swarm optimization, and so on are some of the examples of evolutionary
algorithms.
GAs [19], as randomized search and optimization techniques, are applied eÔ¨Éciently
to solve diÔ¨Äerent types of image segmentation problems. Inspired by the principles of
evolution and natural genetics, GAs apply three operators (viz., selection, crossover, and
mutation over a number of generations) to derive potentially better solutions. Without
having any a priori knowledge about the probable solutions to the problem or diÔ¨Éculties
to formulate the problem, GAs can be the solution to solve those types of problems. Due
to the generality characteristic, GAs demand a segmentation quality measurement to
evaluate the segmentation technique. Another important characteristic of GAs is that
they can derive the optimal or near-optimal solutions for their balanced global and local
search capability.
The GA in combination with wavelet transformation is applied to segment multilevel
images [20]. At Ô¨Årst, the wavelet transform is applied to reduce the size of the original
histogram, and after that, the number of thresholds and the threshold values are resolved
with the help of a GA on the basis of the lower resolution version of the histogram.
Fu et al. [21] presented an image segmentation method using a multilevel threshold of
gray-level and gradient-magnitude entropy based on GAs. A hybrid GA is proposed
by Hauschild et al. [22] for image segmentation on the basis of the q-state Potts spin

4
Hybrid Intelligence for Image Analysis and Understanding
glass model to a grayscale image. In this approach, a set of weights for a q-state spin
glass is formed from the test image, and after that, the candidate segmented images are
generated using the GA until a suitable candidate solution is detected. The hierarchi-
cal local search is applied to speed up the convergence to an adequate solution [22].
The GA in combination with the multilayer self-organizing neural network (MLSONN)
architecture are employed to segment the multilevel grayscale images [11]. In [11], it is
claimed that the MLSONN architecture, with the help of multilevel sigmoidal (MUSIG)
activation function, is not capable of incorporating the image heterogeneity property in
the multilevel grayscale image segmentation process. To induce the image content het-
erogeneity in the segmentation process, the authors employed a GA to generate the opti-
mized class levels for designing the optimized MUSIG (OptiMUSIG) activation func-
tion [11]. Now, the OptiMUSIG activation function based MLSONN architecture is
capable of segmenting the multilevel grayscale images. The variable threshold‚Äìbased
OptiMUSIG activation function is also eÔ¨Écient for grayscale image segmentation [23].
To decrease the eÔ¨Äect of isolated points on the k-means algorithm, a GA is applied to
enhance the k-means clustering algorithm [24].
The performance and the shortcomings of FCM algorithms can be improved by diÔ¨Äer-
ent types of evolutionary algorithms. Biju et al. [25] proposed a genetic algorithm‚Äìbased
fuzzy c-means (GAFCM) technique to segment spots of complementary DNA (cDNA)
microarray images for Ô¨Ånding gene expression. An improved version of the GA is pre-
sented by Wei et al. [26]. In this approach, the genetic operators are modiÔ¨Åed to enhance
the global searching capability of GAs. To improve the convergence speed, the impro-
vised GA applies FCM optimization after each generation of genetic operation [26]. The
GA inspired with the FCM algorithm is capable of segmenting the grayscale images [27].
In this approach, the population of GAs is generated with the help of FCM. Jansi and
Subashini [28] proposed a GA integrated with FCM for medical image segmentation.
The resultant best chromosome, derived by the GA, is applied as the input in the FCM.
The drawback of the FCM algorithm (i.e., convergence to the local optima solution,) is
overcome in this approach.
The main concern of this chapter is to overcome the shortcomings of the FCM algo-
rithm as this algorithm is generally stuck to a local minima point. To eliminate this
drawback, an evolutionary algorithm is the better choice to deal with these types of
problems. A GA has the capability to Ô¨Ånd the global optimal or near‚Äìglobal optimal
solutions in a large search space. That is why the authors considered the GA to handle
this problem. In this chapter, a modiÔ¨Åed genetic algorithm (MfGA)-based FCM algo-
rithm is proposed. For that reason, some modiÔ¨Åcations are made in the traditional GA.
A weighted mean approach is introduced for the chromosome generation in the popu-
lation initialization stage. Like the traditional GA, the crossover probability is not Ô¨Åxed
throughout the generations in the MfGA. As crossover probability plays a vital role
in GAs, the value of the crossover probability decreases as the number of generations
increases. The resultant class levels, derived by the MfGA, are applied as the input of the
FCM algorithm. The proposed MfGA-based FCM algorithm is compared with the tradi-
tional FCM algorithm [5] and the GA-based FCM algorithm [28]. The above-mentioned
algorithms are employed on three benchmark images to determine the performance of
those algorithms. Two standard eÔ¨Éciency measures, the correlation coeÔ¨Écient (ùúå) [11]
and the empirical measure Q [29], are applied as the evaluation functions to measure
the quality of the segmented images. The experimental results show that the proposed

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
5
MfGA-based FCM algorithm outperforms the other two algorithms to segment the mul-
tilevel grayscale images.
This chapter is organized in the following ways. Section 1.2 discusses the traditional
FCM algorithm. Two quality evaluation metrics for image segmentation, the correlation
coeÔ¨Écient (ùúå) and the empirical measure (Q), are narrated in Section 1.4. Before that,
in Section 1.3, the proposed MfGA is illustrated. After that, the MfGA-based FCM is
discussed in Section 1.5. The experimental results and the comparison with the FCM
algorithm [5] and the GA-based FCM [28] algorithm are included in Section 1.6. Finally,
Section 1.7 concludes the chapter.
1.2
Fuzzy C-Means Algorithm
The FCM, introduced by Bezdek [5], is the most widely applied fuzzy clustering method.
Basically, this algorithm is an extension of the hard c-means clustering method and is
based on the concept of fuzzy c-partition [30]. Being an unsupervised clustering algo-
rithm, FCM is associated with the Ô¨Åelds of clustering, feature analysis, and classiÔ¨Åer
design. The wide application areas of FCM are in astronomy, geology, chemistry, image
processing, medical diagnosis, target recognition, and others.
The main objectives of the FCM algorithm are to update the cluster centers and to
calculate the membership degree. The clusters are formed on the basis of the distance
calculation between data points, and the cluster centers are formed for each cluster. The
membership degree is applied to show the belongingness of each data point with each
cluster, and the cluster centers are also updated with this information. This means that
every data point in the data set is associated with every cluster, and among them, a data
point may be considered in a cluster when it has high membership value with that cluster
and low membership value with the rest of the clusters in that data set.
Let X = x1, x2, ‚Ä¶ , xN, xk ‚ààRn be a set of unlabeled patterns, where N is the number
of patterns and each pattern has n number of features. This algorithm tries to mini-
mize the value of an objective function that calculates the quality of the partitioning
that divides the data set into C clusters. The distance between a data point xk to a clus-
ter center Ti is calculated by the well-known Euclidean distance, and it is represented
as [5]:
D2
ik =
n
‚àë
j=1
(xkj ‚àíTij)2,
1 ‚â§k ‚â§N, 1 ‚â§i ‚â§C
(1.1)
where the squared Euclidian distance in n-dimensional space is denoted as D2
ik. The
membership degree is calculated as [5]:
Uik =
1
C‚àë
j=1
( Dik
Djk )
2
(m‚àí1)
,
1 ‚â§k ‚â§N, 1 ‚â§i ‚â§C
(1.2)
where Uik represents the degree of membership of xk in the ith cluster. The degree of
fuzziness is controlled by the parameter m > 1. It signiÔ¨Åes that every data point has a
degree of membership in every cluster.

6
Hybrid Intelligence for Image Analysis and Understanding
The centroids are updated according to the following equation [5]:
Tij =
N‚àë
k=1
(Uik)mxk
N‚àë
k=1
(Uik)m
,
1 ‚â§i ‚â§C
(1.3)
Ultimately, the membership degree of each point is measured using equation (1.2) with
the help of new centroid values.
1.3
ModiÔ¨Åed Genetic Algorithms
Conventional GAs [19] are well-known, eÔ¨Écient, adaptive heuristic search and global
optimization algorithms. The basic idea of GAs took from the evolutionary ideas of nat-
ural selection and genetics. A GA provides a near-optimal solution in a large, complex,
and multimodal problem space. It contains a Ô¨Åxed population size over search space.
In GAs, the performance of the entire population can be upgraded instead of improv-
ing the performance of the individual members in the population-based optimization
procedure. The evolution starts by generating the population through random creation
of individuals. These individual solutions are known as chromosomes. The quality or
goodness of each chromosome or individual is assessed by its Ô¨Åtness value. The poten-
tially better solutions are evolved after applying three well-known genetically inspired
operators like selection, crossover, and mutation.
In this chapter, a MfGA is proposed to improve the performance of the conventional
GA. In the chromosome representation of the conventional GA, each chromosome con-
tains N number of centroids to cluster a data set into N number of clusters. The initial
values of the chromosomes are selected randomly. The clusters are formed between
the cluster centroids and the individual data points on the basis of some criteria. In
most of the cases, the distance between the data points is the selection criterion. A data
point is included in a cluster when the distance between that data point and the par-
ticular cluster centroid is minimum rather than the same with other cluster centroids.
It may happen that a cluster may contain very few data points compared to other clus-
ters in that data set. The relevance of that small cluster may decrease in that data set.
At the same time, the spatial information of the data points in the data set is also not
considered. To overcome that, N+1 number of cluster centroids are selected in the ini-
tial stage to generate the N number of cluster centroids. Steps of MfGA are discussed
here:
‚Ä¢ Population initialization: In MfGA, the population size is Ô¨Åxed, and the lengths
of individual chromosomes are Ô¨Åxed. At the initial stage, N+1 number of cluster
centroids are generated randomly within the range of minimum and maximum
values of the data set, and the cluster centroids are denoted as R1, R2, R3,..,Ri, .., Rn+1.
These cluster centroids are temporary, and they are used to generate the actual
cluster centroids. The weighted mean between the temporary cluster centroids of Ri
and Ri+1 is applied to generate the actual cluster centroids Li, and it is represented

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
7
as:
Li =
Ri+1
‚àë
I=Ri
fI √ó I
Ri+1
‚àë
I=Ri
fI
(1.4)
where I represents the value of the data point and fI denotes the frequency of the Ith
data point. For example, the cluster centroid L1 is generated after taking the weighted
mean between the temporary cluster centroids R1 and R2. The cluster centroid Li,
having the most frequency value in between the cluster centroids Ri and Ri+1, will be
selected by this process, and this will make a good chromosome for future genera-
tion. Ultimately, the spatial information is taken into consideration at the population
initialization time.
‚Ä¢ Selection: After population initialization, the eÔ¨Äectiveness of the individual chromo-
somes is determined by a proper Ô¨Åtness function so that better chromosomes are
taken for further usage. This is done in the perception that the better chromosomes
may develop by transmitting the superior genetic information to new generations,
and they will persist and generate oÔ¨Äspring. In the selection step, the individuals are
selected for mating on the basis of the Ô¨Åtness value of each individual. This Ô¨Åtness
value is used to associate a probability of selection with each individual chromosome.
Some of the well-known selection methods are roulette wheel selection, stochastic
universal selection, Boltzmann selection, rank selection, and so on [19].
‚Ä¢ Crossover: The randomly selected parent chromosomes exchange their information
by changing parts of their genetic information. Two chromosomes, having the same
probability of crossover rate, are selected to generate oÔ¨Äspring for the next generation.
The crossover probability plays a vital role in this stage, and it is used to show a ratio
of how many parents will be picked for mating. In conventional GA, the crossover
probability is same throughout the process. It may happen that a good chromosome
may be mated with a bad chromosome, and that good chromosome is not stored
in the next stage. To overcome this drawback, it is suggested in this process that the
crossover probability rate will decrease as the number of iterations will increase. Here,
the crossover probability is inversely proportional to the number of iterations so that
the better chromosomes will remain unchanged and will go to the next generation of
population. Mathematically, it is represented as:
Cp = Cmax ‚àí
( Cmax ‚àíCmin
ITmax ‚àíITcur
)
(1.5)
where Cp is the current crossover probability, and Cmax and Cmin are the maximum
and minimum crossover probability, respectively. The maximum number of iterations
and the current number of iterations are represented as ITmax and ITcur, respectively.
‚Ä¢ Mutation: The sole objective of the mutation stage is to introduce the genetic diver-
sity into the population. Being a divergence operation, the frequency of the mutation
operation is much less, and so the members of the population are aÔ¨Äected much less.
Mutation probability is taken as a very small value for this reason. Generally, a value
in the randomly selected position in the chromosome is Ô¨Çipped.

8
Hybrid Intelligence for Image Analysis and Understanding
1.4
Quality Evaluation Metrics for Image Segmentation
It is absolutely necessary to measure the quality of the Ô¨Ånal segmented images after seg-
menting the images by diÔ¨Äerent types of segmentation algorithms. Usually, diÔ¨Äerent sta-
tistical mathematical functions are employed to evaluate the results of the segmentation
algorithms. Here, two unsupervised approaches are provided to measure the goodness
of the segmented images in Sections 1.4.1 and 1.4.2.
1.4.1
Correlation CoeÔ¨Écient (ùùÜ)
The degree of the similarity between the original and segmented images can be mea-
sured by using the standard measure of correlation coeÔ¨Écient (ùúå) [11], and it is repre-
sented as [11]:
ùúå=
1
n2
n‚àë
i=1
n‚àë
j=1
(Rij ‚àíR)(Gij ‚àíG)
‚àö
1
n2
n‚àë
i=1
n‚àë
j=1
(Rij ‚àíR)2
‚àö
1
n2
n‚àë
i=1
n‚àë
j=1
(Gij ‚àíG)2
(1.6)
where Rij, 1 ‚â§i, j ‚â§n and Gij, 1 ‚â§i, j ‚â§n are the original and the segmented images,
respectively, each of dimensions n √ó n. The respective mean intensity values of Rij and
Gij are denoted as R and G, respectively. The higher value of ùúåsigniÔ¨Åes the better quality
of image segmentation [11].
1.4.2
Empirical Measure Q(I)
A empirical measure, Q(I), is used to evaluate the goodness of the segmented images,
and it is suggested by Borsotti et al. [29, 31]. It is denoted as [29]:
Q(I) =
1
1000.SI
‚àö
N
N
‚àë
g=1
[
e2
g
1 + log Sg
+
(N(Sg)
Sg
)2]
(1.7)
where the area of the gth region of an image (I) is denoted as Sg, and the number of
regions having an area Sg is signiÔ¨Åed by N(Sg). SI is the area of an image (I) to be seg-
mented. The squared color error of region g, e2
g, is noted as [29, 31]:
e2
g =
‚àë
ùë£‚àà(r,g,b)
‚àë
pl‚ààRIg
(Cùë£( pl) ‚àíÃÇCùë£(RIg))2
(1.8)
The number of pixels in region g is presented as RIg. The average value of feature ùë£
(red, green, or blue) of a pixel pl in region g is referred to as [29, 31]:
ÃÇCùë£(RIg) =
‚àë
pl‚ààRIg
Cùë£( pl)
Sg
(1.9)
where Cùë£( pl) signiÔ¨Åes the value of component ùë£for pixel pl.
A smaller value of Q implicates better quality of segmentation [29]. In this chapter,
the quality of the segmented images is evaluated by these measures, which are applied
as diÔ¨Äerent objective functions.

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
9
1.5
MfGA-Based FCM Algorithm
FCM has three major drawbacks: there must be a priori information of the number of
classes to be segmented, this algorithm can only be applied to hyperspherical-shaped
clusters, and it often converges to local minima [6]. In this chapter, we have tried to
overcome the drawback about the convergence to the local minima. In many cases, it
has been observed that a FCM algorithm will easily converge to the local minimum
point and the clustering will be aÔ¨Äected if the initial centroid values are not initialized
correctly. The spatial relative information is not considered in the process of the FCM
algorithm. The selection of the initial cluster centers and/or the initial membership value
plays a vital role in the performance of the FCM algorithm. The quick convergence and
drastic reduction of processing time can be possible in FCM if the selection of the initial
cluster center is very close to the actual Ô¨Ånal cluster center.
The drawback of FCM can be overcome with the help of the proposed MfGA
algorithm. The optimized class levels, obtained by the proposed MfGA algorithm,
are applied as the initial class levels in the FCM algorithm. Another advantage of this
proposed method is that the image content heterogeneity is also considered. The pixels,
having most occurrence in the image, have the higher probability for being selected as
the class levels in the initial stage. The Ô¨Çowchart in Figure 1.1 shows the steps of the
MfGA-based FCM algorithm.
The pseudo-code of the GA-based FCM is as follows:
1. Pop ‚ÜêGenerate P number of feasible solutions randomly with N number of class
levels.
2. Calculate Ô¨Åtness (P).
3. For i=1 to itr, do the following:
A. Apply selection on Pop.
B. Apply crossover operation with crossover probability Cp.
C. Apply mutation operation with mutation probability ùúám.
D. End for.
4. Return the best chromosome with the class levels.
5. Initialize cluster centers of the FCM algorithm with the best solution derived from
GA.
6. For i=0 to itr2, do the following:
A. Update the membership matrix (Uik) using equation (1.2).
B. Update the centroids [equation (1.3)].
7. Return the ultimate cluster centroids/class levels.
The pseudo-code of the MfGA-based FCM is as follows:
1. Generate M number of chromosomes randomly with N + 1 number of temporary
cluster centroids individually.
2. Using weighted mean equation (1.4), generate M number of chromosomes with N
number of actual cluster centroids individually.
3. Calculate the Ô¨Åtness computation of population using a relevant Ô¨Åtness function.
4. Repeat (for a predeÔ¨Åned number of iterations or until a certain condition is
satisÔ¨Åed).
A. Select parents from population, on the basis of Ô¨Åtness values.

10
Hybrid Intelligence for Image Analysis and Understanding
Generate (N+1) number of cluster centroids for each chromosome randomly
Population
Initialization
Start
Using weighted mean, generate N number of cluster centroids for each chromosome
Compute fitness values of each chromosome
Generate new mating pool using selection method
Apply crossover in consideration with crossover probability
Apply mutation to get new population
Compute fitness value of new population
Apply the optimized cluster centroids as the initial cluster centroids in FCM algorithm
Initialize the membership matrix (Uik)
Compute the new cluster center matrix (Tij)
Compute the new membership matrix (Uik)
Stop and return the ultimate cluster centroids for the dataset
End
Criteria satisfied?
Termination criteria
attained
YES
NO
YES
NO
Figure 1.1 Flowchart of MfGA-based FCM algorithm.
B. Execute crossover and mutation to create a new population [crossover probability
is applied using equation (1.5)].
C. Compute Ô¨Åtness of new population.
5. Return the best chromosome with the class levels.

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
11
6. Initialize cluster centers of the FCM algorithm with the best solution derived from
the GA.
7. For i=0 to itr2, do the following:
A. Update the membership matrix (Uik) using equation (1.2).
B. Update the centroids [equation (1.3)].
8. Return the ultimate cluster centroids/class levels.
1.6
Experimental Results and Discussion
Multilevel grayscale image segmentation with the MfGA-based FCM algorithm was
demonstrated with three real-life grayscale images (viz., Lena, baboon, and peppers)
of dimensions 256√ó256. Experimental results in quantitative and qualitative terms are
reported in this section. The multilevel images are segmented in K = {6, 8, 10} classes,
but the results are reported for K = {6, 8} classes. Results are also presented for the
segmentation of the multilevel grayscale images with the GA-based FCM algorithm [5]
and also with the FCM algorithm [28]. To measure the eÔ¨Éciency of the diÔ¨Äerent algo-
rithms qualitatively, two evaluation functions (ùúå, Q), presented in equations (1.6) and
(1.7), respectively, have been used in this chapter.
In the initial stage, the class levels are generated by the proposed MfGA algorithm,
and after that the obtained class levels are supplied as the input in the FCM algorithm.
For that, the pixel intensity levels of the multilevel grayscale images and the number of
classes (K) to be segmented are supplied as inputs to this process. The randomly gener-
ated real numbers within the range of the minimum and maximum intensity values of
the input image are applied to create the chromosomes for this process. These compo-
nents of the chromosomes are treated as the class levels or class boundaries to segment
the input image. For example, to segment an image into eight segments, the chromo-
somes with nine class levels are generated at the starting point. The process of generating
the chromosomes and creating the chromosome pool in the GA-based FCM method is
the same as the proposed method. A population size of 50 has been used for the pro-
posed and GA-based FCM method. At the initial stage, the class levels are also generated
randomly in the FCM method.
The GA operators (viz., selection, crossover, and mutation) are used in the proposed
and GA-based FCM [28] approaches. The roulette wheel approach is employed for the
selection phase for both the GA-based approaches. Afterward, the crossover and muta-
tion operators are applied to generate the new population. Though the crossover prob-
ability value is Ô¨Åxed in the GA-based FCM approach, the crossover probability value in
the MfGA-based FCM method is dependent on the number of iterations, and that works
according to equation (1.5). The crossover probability value is 0.9 for all the stages in the
GA-based FCM algorithm. The maximum crossover probability value (Cmax) and min-
imum crossover probability value (Cmin) are applied as 0.9 and 0.5, respectively, in the
proposed MfGA-based FCM method. In both GA-based approaches, the single-point
crossover operation is used in the crossover stage, and 0.01 is considered as the mutation
probability. The new population is formed after the mutation operation.
The class levels generated by the proposed algorithm on the basis of the two Ô¨Åtness
functions (ùúåand Q) for diÔ¨Äerent numbers of classes are tabulated in Tables 1.1 and 1.2
for Lena images. In this type of tables, the number of segments (# segments), the name

12
Hybrid Intelligence for Image Analysis and Understanding
Table 1.1 Class boundaries and evaluated segmentation quality measures ùúåby diÔ¨Äerent
algorithms for diÔ¨Äerent classes of the Lena image
# Segments
Algorithm
#
Class levels
Fitness value
6
FCM
1
7, 61, 107, 143, 179, 229
0.9133
2
6, 64, 109, 144, 177, 229
0.9213
3
9, 64, 107, 143, 179, 229
0.9194
4
7, 64, 110, 145, 180, 227
0.9227
GA-based FCM
1
47, 76, 105, 135, 161, 203
0.9823
2
50, 93, 124, 148, 173, 206
0.9824
3
47, 78, 105, 134, 161, 203
0.9823
4
47, 77, 105, 135, 161, 203
0.9823
Proposed
1
47, 76, 105, 135, 161, 203
0.9828
2
50, 92, 123, 148, 173, 206
0.9824
3
50, 94, 125, 149, 173, 204
0.9824
4
47, 76, 105, 135, 161, 203
0.9828
8
FCM
1
4, 37, 71, 105, 131, 155, 188, 231
0.9559
2
5, 37, 70, 105, 131, 155, 188, 231
0.9273
3
4, 37, 71, 105, 131, 154, 188, 230
0.9251
4
6, 37, 71, 105, 131, 155, 188, 231
0.9266
GA-based FCM
1
42, 58, 84, 104, 130, 152, 178, 207
0.9893
2
42, 58, 83, 105, 131, 152, 176, 207
0.9894
3
46, 72, 99, 122, 140, 156, 179, 208
0.9893
4
42, 58, 83, 105, 130, 152, 176, 207
0.9894
Proposed
1
46, 71, 98, 122, 140, 157, 179, 208
0.9897
2
45, 72, 99, 124, 141, 158, 179, 208
0.9898
3
46, 72, 99, 123, 140, 156, 177, 207
0.9896
4
46, 72, 98, 122, 142, 157, 179, 208
0.9898
of the algorithm (Algorithm), the serial number (#), class levels, and the Ô¨Åtness value (Ô¨Åt
val) are accounted in the columns. The class levels evaluated by the FCM algorithm and
the GA-based FCM algorithm are also reported in these tables. Each type of experiments
is performed 50 times, but only four good results are tabulated in these tables in respect
to the number of segments and in respect to the individual algorithms. The best results
obtained by any process for each number of segments are highlighted by boldface type.
The mean and standard deviations are evaluated for diÔ¨Äerent algorithm-based Ô¨Åtness
values using diÔ¨Äerent types of Ô¨Åtness functions, and these results for Lena images are
reported in Table 1.3. The time taken by diÔ¨Äerent algorithms is also preserved, and the
mean of the time taken by the diÔ¨Äerent algorithms is also mentioned in this table. The
good results are marked in bold.
For segmenting the Lena image, the proposed algorithm outperforms the other two
algorithms. In Table 1.1, the ùúåvalues, derived by the MfGA-based FCM algorithm, are
better than the other two algorithms for a diÔ¨Äerent number of segments. The same
observation can be obtained if anyone goes through the reported results in Table 1.2. The
Q-based Ô¨Åtness values, obtained by the proposed algorithm, are much better than the
results derived by the other two processes. In Table 1.3, the reported mean and standard

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
13
Table 1.2 Class boundaries and evaluated segmentation quality measures Q by diÔ¨Äerent
algorithms for diÔ¨Äerent classes of the Lena image
# Segments
Algorithm
#
Class levels
Fitness value
6
FCM
1
7, 62, 107, 144, 179, 228
24169.019
2
8, 64, 107, 143, 177, 229
19229.788
3
7, 64, 106, 140, 179, 231
24169.018
4
5, 66, 110, 142, 180, 229
15017.659
GA-based FCM
1
47, 77, 105, 135, 161, 203
10777.503
2
50, 93, 124, 148, 173, 206
16017.944
3
47, 74, 105, 135, 161, 203
12194.313
4
47, 76, 108, 134, 161, 203
10909.195
Proposed
1
47, 78, 105, 135, 161, 203
7117.421
2
47, 77, 105, 134, 161, 204
7020.245
3
50, 93, 124, 148, 173, 206
13146.389
4
50, 94, 126, 158, 179, 204
10577.836
8
FCM
1
4, 38, 71, 105, 131, 155, 188, 229
64095.622
2
5, 37, 69, 105, 132, 155, 188, 231
83743.507
3
6, 37, 71, 105, 129, 155, 187, 230
108324.494
4
4, 37, 71, 105, 131, 155, 184, 231
48229.038
GA-based FCM
1
46, 72, 99, 122, 140, 157, 179, 208
49382.388
2
46, 73, 100, 127, 149, 169, 193, 212
90707.0511
3
46, 72, 99, 122, 140, 157, 179, 208
47419.664
4
46, 73, 99, 122, 140, 158, 180, 208
46906.552
Proposed
1
42, 58, 83, 105, 130, 152, 176, 207
33531.256
2
42, 59, 84, 107, 129, 152, 175, 207
32630.511
3
42, 58, 83, 105, 131, 152, 176, 207
32496.323
4
46, 72, 99, 122, 140, 157, 179, 208
47429.683
Table 1.3 DiÔ¨Äerent algorithm-based means and standard deviations using diÔ¨Äerent types of
Ô¨Åtness functions and mean of time taken by diÔ¨Äerent algorithms for the Lena image
Fitness function
# Segments
Algorithm
Mean ¬± standard deviation
Mean time
ùúå
6
FCM
0.9311 ¬± 0.02033
31 sec
GA-based FCM
0.9623 ¬± 0.00003
23 sec
Proposed
0.9824 ¬± 0.00002
20 sec
8
FCM
0.9506 ¬± 0.01814
41 sec
GA-based FCM
0.9817 ¬± 0.00021
39 sec
Proposed
0.9898 ¬± 2.34E-16
37 sec
Q
6
FCM
13119.427 ¬± 7046.408
39 sec
GA-based FCM
12018.495 ¬± 2149.968
25 sec
Proposed
8966.715 ¬± 2635.109
22 sec
8
FCM
47943.902 ¬± 29441.925
49 sec
GA-based FCM
47417.776 ¬± 16557.246
47 sec
Proposed
43233.689 ¬± 7179.615
43 sec

14
Hybrid Intelligence for Image Analysis and Understanding
Table 1.4 Class boundaries and evaluated segmentation quality measures ùúåby diÔ¨Äerent
algorithms for diÔ¨Äerent classes of the peppers image
# Segments
Algorithm
#
Class levels
Fitness value
6
FCM
1
28, 76, 108, 140, 178, 201
0.9291
2
26, 75, 103, 144, 169, 191
0.9312
3
23, 69, 98, 148, 171, 201
0.7860
4
25, 71, 107, 139, 170, 204
0.8916
GA-based FCM
1
29, 76, 108, 144, 173, 201
0.9829
2
28, 75, 108, 144, 170, 201
0.9828
3
29, 76, 108, 145, 173, 203
0.9828
4
30, 76, 110, 144, 173, 201
0.9827
Proposed
1
29, 78, 108, 144, 173, 203
0.9830
2
28, 76, 110, 144, 172, 203
0.9830
3
29, 78, 110, 146, 173, 202
0.9830
4
28, 75, 108, 144, 172, 201
0.9831
8
FCM
1
26, 69, 94, 111, 136, 160, 181, 203
0.9268
2
17, 46, 79, 106, 128, 156, 178, 203
0.9317
3
40, 79, 104, 121, 143, 160, 176, 199
0.9585
4
17, 46, 79, 106, 127, 155, 178, 202
0.9309
GA-based FCM
1
27, 71, 98, 115, 146, 170, 193, 213
0.9887
2
17, 45, 79, 106, 128, 156, 178, 203
0.9890
3
27, 71, 98, 114, 146, 170, 193, 213
0.9887
4
18, 47, 81, 108, 140, 191, 167, 211
0.9889
Proposed
1
18, 47, 81, 108, 140, 167, 191, 211
0.9890
2
27, 71, 98, 114, 146, 170, 193, 213
0.9890
3
17, 46, 79, 106, 128, 156, 179, 203
0.9891
4
17, 46, 78, 106, 129, 156, 179, 203
0.9892
deviation of both Ô¨Åtness values (ùúåand Q) are best for the proposed algorithm compared
to the same reported by the other two algorithms. It is also observed that the proposed
method has taken less time than the other two approaches.
The same experiment is repeated for the peppers image. The ùúå-based and Q-based
class boundaries and the Ô¨Åtness values are reported in Table 1.4 and Table 1.5, respec-
tively. The better results are highlighted in bold. From these tables, it is detected that
the Ô¨Åtness values derived by the proposed algorithm are better than the same results
obtained by the other two algorithms. This observation will remain unchanged for the
peppers image whether it is segmented in six segments or eight segments.
The peppers image is segmented by the proposed approach 50 times for each Ô¨Åtness
function. The mean and standard deviation of those diÔ¨Äerent function-based Ô¨Åtness
values are reported in Table 1.6. The mean of the time taken by the proposed algorithm is
also tabulated in this table. The same results are also saved for the other two algorithms.
The mean and standard deviation of the Ô¨Åtness values and the mean time taken by the
other two algorithms are also reported in Table 1.6. The MfGA-based FCM algorithm is
much better than the other two algorithms if anyone considers the accounted results in

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
15
Table 1.5 Class boundaries and evaluated segmentation quality measures Q by diÔ¨Äerent
algorithms for diÔ¨Äerent classes of the peppers image
# Segments
Algorithm
#
Class levels
Fitness value
6
FCM
1
28, 76, 108, 145, 173, 201
42696.252
2
29, 76, 110, 144, 175, 202
37371.362
3
27, 75, 105, 143, 170, 201
31326.604
4
26, 71, 108, 144, 173, 201
9813.211
GA-based FCM
1
28, 78, 106, 144, 169, 203
17130.923
2
29, 76, 108, 140, 173, 201
9130.923
3
26, 75, 109, 144, 174, 202
15065.869
4
27, 76, 108, 144, 171, 203
14531.759
Proposed
1
29, 76, 108, 144, 173, 201
5693.583
2
29, 78, 110, 156, 174, 194
4907.482
3
29, 76, 107, 144, 172, 201
5693.583
4
29, 76, 108, 145, 173, 201
4892.024
8
FCM
1
18, 47, 81, 108, 140, 167, 191, 211
181883.131
2
27, 71, 98, 115, 146, 170, 193, 213
102247.881
3
27, 71, 98, 115, 146, 172, 198, 211
198840.268
4
17, 46, 79, 106, 128, 156, 179, 203
283016.901
GA-based FCM
1
27, 71, 98, 115, 146, 170, 193, 213
53784.935
2
27, 76, 97, 118, 140, 167, 191, 211
44311.485
3
28, 71, 98, 115, 146, 170, 193, 213
53908.007
4
27, 71, 98, 115, 147, 175, 193, 212
27230.946
Proposed
1
18, 47, 81, 108, 125, 142, 170, 199
20251.096
2
17, 46, 79, 106, 128, 156, 178, 202
21406.658
3
27, 71, 99, 115, 147, 171, 199, 213
24926.826
4
17, 46, 79, 106, 128, 156, 178, 202
19632.432
Table 1.6 DiÔ¨Äerent algorithm-based mean and standard deviation using diÔ¨Äerent types of Ô¨Åtness
functions and mean of time taken by diÔ¨Äerent algorithms for the peppers image
Fitness function
# Segments
Algorithm
Mean ¬± standard deviation
Mean time
ùúå
6
FCM
0.8890 ¬± 0.06096
36 sec
GA-based FCM
0.9727 ¬± 2.686E-06
28 sec
Proposed
0.9831 ¬± 3.789E-07
24 sec
8
FCM
0.9472 ¬± 0.01815
58 sec
GA-based FCM
0.9824 ¬± 0.00021
56 sec
Proposed
0.9890 ¬± 0.00016
54 sec
Q
6
FCM
14739.441 ¬± 15846.203
30 sec
GA-based FCM
14681.419 ¬± 2967.542
26 sec
Proposed
5534.818 ¬± 334.728
24 sec
8
FCM
111303.905 ¬± 91628.397
59 sec
GA-based FCM
40479.148 ¬± 15736.887
56 sec
Proposed
32567.705 ¬± 15566.113
55 sec

16
Hybrid Intelligence for Image Analysis and Understanding
Table 1.7 Class boundaries and evaluated segmentation quality measures ùúåby diÔ¨Äerent
algorithms for diÔ¨Äerent classes of the baboon image
# Segments
Algorithm
#
Class levels
Fitness value
6
FCM
1
37, 70, 95, 121, 140, 173
0.9204
2
37, 69, 95, 118, 144, 175
0.9154
3
37, 69, 89, 116, 142, 174
0.9099
4
35, 68, 98, 118, 145, 170
0.9188
GA-based FCM
1
37, 69, 96, 119, 146, 173
0.9785
2
36, 69, 95, 118, 144, 175
0.9783
3
37, 69, 95, 120, 144, 173
0.9784
4
37, 68, 94, 118, 145, 175
0.9783
Proposed
1
38, 70, 96, 120, 154, 173
0.9784
2
43, 79, 110, 133, 158, 174
0.9785
3
37, 69, 98, 120, 145, 173
0.9786
4
38, 68, 96, 118, 144, 174
0.9785
8
FCM
1
36, 66, 82, 101, 125, 141, 159, 180
0.9514
2
31, 60, 86, 96, 118, 130, 159, 179
0.9183
3
29, 58, 81, 100, 116, 128, 161, 181
0.9204
4
30, 59, 79, 99, 119, 138, 159, 178
0.9498
GA-based FCM
1
32, 61, 81, 101, 118, 137, 160, 178
0.9869
2
32, 59, 81, 100, 119, 137, 159, 177
0.9870
3
33, 59, 82, 99, 118, 135, 158, 178
0.9869
4
31, 60, 81, 101, 120, 137, 158, 177
0.9861
Proposed
1
33, 59, 81, 99, 121, 137, 159, 178
0.9869
2
32, 59, 81, 101, 117, 138, 159, 178
0.9870
3
33, 59, 81, 100, 118, 137, 158, 178
0.9871
4
32, 59, 81, 101, 118, 137, 159, 178
0.9870
Table 1.6. The Ô¨Åtness values and the execution time of the proposed algorithm are best
compared to the same for the other two algorithms.
The class boundaries and the ùúå-based Ô¨Åtness values for segmenting the baboon image
are reported in Table 1.7. In Table 1.8, the class levels for segmenting the same image
and the Q-based Ô¨Åtness values are presented. The best results derived by the individual
algorithm are highlighted in bold.
The proposed approach is applied 50 times for segmenting the baboon image on the
basis of two Ô¨Åtness functions separately. The mean and standard deviation of those
diÔ¨Äerent function-based Ô¨Åtness values are tabulated in Table 1.9. The mean of the time
taken by the proposed algorithm is also reported in this table. The same thing is also
done for the other two algorithms. The mean and standard deviation of the Ô¨Åtness
values and the mean time taken by the other two algorithms are also accounted in
Table 1.9. If anyone goes through the reported results in Table 1.9, they can observe
that the other two algorithm-based results are far behind the results derived by the
MfGA-based FCM algorithm.
The segmented grayscale output images obtained for the K= {6, 8} classes, with the
proposed approach vis-√†-vis with the FCM [5] and the GA-based FCM [28] algorithms,

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
17
Table 1.8 Class boundaries and evaluated segmentation quality measures Q by diÔ¨Äerent
algorithms for diÔ¨Äerent classes of the baboon image
# Segments
Algorithm
#
Class levels
Fitness value
6
FCM
1
38, 71, 95, 116, 140, 173
16413.570
2
37, 69, 95, 114, 134, 165
13422.376
3
36, 69, 92, 114, 141, 170
18223.109
4
37, 68, 95, 112, 143, 176
16808.168
GA-based FCM
1
37, 69, 98, 120, 144, 173
7933.554
2
36, 69, 95, 119, 146, 173
15368.872
3
38, 71, 98, 118, 144, 174
5532.344
4
37, 73, 95, 118, 145, 173
15368.872
Proposed
1
38, 69, 95, 120, 146, 173
4376.556
2
38, 70, 94, 118, 144, 173
4571.008
3
43, 79, 110, 133, 158, 174
5053.403
4
37, 68, 94, 118, 144, 173
4985.565
8
FCM
1
32, 49, 80, 99, 118, 131, 159, 178
95888.688
2
33, 62, 81, 101, 118, 137, 152, 178
73697.780
3
32, 60, 81, 100, 113, 137, 159, 179
102621.172
4
29, 59, 81, 97, 118, 137, 159, 178
70320.873
GA-based FCM
1
32, 59, 82, 101, 118, 137, 159, 178
27848.183
2
33, 60, 82, 102, 119, 138, 160, 179
31323.896
3
34, 60, 82, 101, 118, 138, 159, 178
32329.115
4
33, 59, 81, 101, 118, 137, 161, 178
28488.563
Proposed
1
33, 61, 84, 104, 121, 147, 164, 175
21661.820
2
32, 59, 80, 99, 117, 135, 156, 177
24137.965
3
34, 61, 84, 104, 121, 143, 160, 172
21198.127
4
33, 60, 81, 101, 118, 137, 158, 178
26805.747
Table 1.9 DiÔ¨Äerent algorithm-based mean and standard deviation using diÔ¨Äerent types of Ô¨Åtness
functions and mean of time taken by diÔ¨Äerent algorithms for the baboon image
Fitness function
# Segments
Algorithm
Mean ¬± standard deviation
Mean time
ùúå
6
FCM
0.9211 ¬± 0.01114
25 sec
GA-based FCM
0.9680¬± 8.198E-07
22 sec
Proposed
0.9788 ¬± 7.971E-07
14 sec
8
FCM
0.9489 ¬± 0.01751
59 sec
GA-based FCM
0.9762 ¬± 2.135E-05
56 sec
Proposed
0.9870 ¬± 2.519E-05
54 sec
Q
6
FCM
9777.519 ¬± 5695.687
42 sec
GA-based FCM
9671.766 ¬± 4961.474
40 sec
Proposed
4894.895 ¬± 628.569
32 sec
8
FCM
52408.164 ¬± 30991.141
53 sec
GA-based FCM
28137.835 ¬± 2594.486
51 sec
Proposed
24343.203 ¬± 8091.355
43 sec

18
Hybrid Intelligence for Image Analysis and Understanding
are demonstrated afterward. The segmented grayscale test images with K = 8 segments
are presented in this chapter. The segmented multilevel grayscale test images derived
by the FCM algorithm and the GA-based FCM algorithm are depicted in the Ô¨Årst and
second rows of each Ô¨Ågure. In the third row of each Ô¨Ågure, the segmented multilevel
grayscale test images by the proposed MfGA-based FCM algorithm are shown. For easy
recognition in each Ô¨Ågure, the GA-based FCM and the MfGA-based FCM algorithms
are noted as GA FCM and MfGA FCM, respectively. The segmented images by the
FCM algorithm are presented in (a‚Äìd), the segmented images by the GA-based FCM
algorithm are shown in (e‚Äìh), and the MfGA-based FCM-based multilevel segmented
images are pictured in (i‚Äìl) of each Ô¨Ågure, respectively. In Figure 1.2, the class levels
(K=8) of Table 1.1, obtained by the proposed approach and other two algorithms, are
employed to get the segmented output image of the Lena image. The Ô¨Åtness function,
ùúå, is applied in this case. Figure 1.3 is generated with the class levels obtained by each
algorithm. These images are segmented into eight segments, and Q is applied as the
evaluation function in this case.
It is observed from Figures 1.2 and 1.3 that the proposed approach gives better seg-
mented outputs than the same derived by the other two approaches.
The multilevel segmented outputs of the peppers image are shown in Figures 1.4
and 1.5. The four class levels (K=8) of Table 1.4, obtained by the proposed algorithm as
(a)
FCM
GA FCM
MfGA FCM
(b)
(c)
(d)
(i)
(j)
(k)
(l)
(e)
(f)
(g)
(h)
Figure 1.2 8-class segmented 256√ó256 grayscale Lena image with the class levels obtained by (a‚Äìd)
FCM, (e‚Äìh) GA-based FCM, and (i‚Äìl) MfGA-based FCM algorithms of four results of Table 1.1, with ùúåas
the quality measure.

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
19
(a)
FCM
GA FCM
MfGA FCM
(b)
(c)
(d)
(i)
(j)
(k)
(l)
(e)
(f)
(g)
(h)
Figure 1.3 8-class segmented 256√ó256 grayscale Lena image with the class levels obtained by (a‚Äìd)
FCM, (e‚Äìh) GA-based FCM, and (i‚Äìl) MfGA-based FCM algorithms of four results of Table 1.2, with Q as
the quality measure.
(a)
FCM
GA FCM
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 1.4 8-class segmented 256√ó256 grayscale peppers image with the class levels obtained by
(a‚Äìd) FCM, (e‚Äìh) GA-based FCM, and (i‚Äìl) MfGA-based FCM algorithms of four results of Table 1.4, with
ùúåas the quality measure.

20
Hybrid Intelligence for Image Analysis and Understanding
MfGA FCM
(i)
(j)
(k)
(l)
Figure 1.4 (Continued)
(a)
FCM
GA FCM
MfGA FCM
(b)
(c)
(d)
(i)
(j)
(k)
(l)
(e)
(f)
(g)
(h)
Figure 1.5 8-class segmented 256√ó256 grayscale peppers image with the class levels obtained by
(a‚Äìd) FCM, (e‚Äìh) GA-based FCM, and (i‚Äìl) MfGA-based FCM algorithms of four results of Table 1.5, with
Q as the quality measure.
well as other two approaches, are employed to derive the segmented output images of
the peppers image in Figure 1.4. In this case, ùúåis employed as the evaluation function.
In Figure 1.5, the multilevel segmented outputs of the peppers image are yielded using
the Q Ô¨Åtness function based on four results from Table 1.5 with K=8 class levels. The
multilevel segmented peppers images by the proposed MfGA-based FCM algorithm
are segmented in a better way than the segmented images deduced by the other two
approaches, and this is clear from Figures 1.4 and 1.5.
The ùúåis applied as the Ô¨Åtness function to generate the segmented baboon image, which
is shown in Figure 1.6 using K=8 class levels of Table 1.7. The class levels (K=8) of

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
21
(a)
FCM
GA FCM
MfGA FCM
(b)
(c)
(d)
(i)
(j)
(k)
(l)
(e)
(f)
(g)
(h)
Figure 1.6 8-class segmented 256√ó256 grayscale baboon image with the class levels obtained by
(a‚Äìd) FCM, (e‚Äìh) GA-based FCM, and (i‚Äìl) MfGA-based FCM algorithms of four results of Table 1.7, with
ùúåas the quality measure.
Table 1.8 are employed to generate the segmented baboon image that is depicted in
Figure 1.7. In this case, the empirical measure Q is applied as the quality measure.
From Figures 1.6 and 1.7, it can be said that the segmented multilevel baboon images
are better segmented by the proposed algorithm than by the FCM and GA-based FCM
algorithms.
At the end, it can be concluded that the proposed MfGA algorithm overwhelms the
FCM [5] and GA-based FCM [28] algorithms quantitatively and qualitatively.
(a)
FCM
(b)
(c)
(d)
Figure 1.7 8-class segmented 256√ó256 grayscale baboon image with the class levels obtained by
(a‚Äìd) FCM, (e‚Äìh) GA-based FCM, and (i‚Äìl) MfGA-based FCM algorithms of four results of Table 1.8, with
Q as the quality measure.

22
Hybrid Intelligence for Image Analysis and Understanding
GA FCM
MfGA FCM
(i)
(j)
(k)
(l)
(e)
(f)
(g)
(h)
Figure 1.7 (Continued)
1.7
Conclusion
In this chapter, diÔ¨Äerent types of multilevel grayscale image segmentation techniques
are considered. In this regard, a modiÔ¨Åed version of the genetic algorithm (MfGA)-based
FCM algorithm is proposed to segment the multilevel grayscale images. The FCM and
the GA-based FCM algorithms are also recounted brieÔ¨Çy, and they are also used to seg-
ment the same multilevel grayscale images. The drawback of the original FCM algorithm
is also pointed out in this chapter quite eÔ¨Éciently. The way to get rid of the drawback
of the FCM algorithm is also discussed elaborately by proposing the proposed algo-
rithm. The solutions derived by the MfGA-based FCM algorithm are globally optimized
solutions. To derive the optimized class levels in this procedure, diÔ¨Äerent image segmen-
tation quality measures are used. The performance of the proposed MfGA-based FCM
algorithm for real-life multilevel grayscale image segmentation is superior in most of the
cases as compared to the other two segmentation algorithms.
References
1 Zaitoun, N.M. and Aqel, M.J. (2015) Survey on image segmentation techniques,
in Proceedings of International Conference on Communications, management, and
Information technology (ICCMIT‚Äô2015), vol. 65, Procedia Computer Science, vol. 65,
pp. 797‚Äì806.
2 Khan, W. (2013) Image segmentation techniques: a survey. Journal of Image and
Graphics, 1 (4), 166‚Äì170.
3 Chauhan, A.S., Silakari, S., and Dixit, M. (2014) Image segmentation methods: a
survey approach, in Proceedings of 2014 Fourth International Conference on Commu-
nication Systems and Network Technologies (CSNT), pp. 929‚Äì933.

Multilevel Image Segmentation Using ModiÔ¨Åed Genetic Algorithm (MfGA)-based Fuzzy C-Means
23
4 MacQueen, J. (1967) Some methods for classiÔ¨Åcation and analysis of multivariate
observations, in Fifth Berkeley Symposium on Mathematics, Statistics and Probabil-
ity, pp. 281‚Äì297.
5 Bezdek, J. (1981) Pattern Recognition with Fuzzy Objective Function Algorithms,
Plenum Press, New York.
6 Luo, M., Ma, Y.F., and Zhang, H.J. (2003) A spatial constrained K-means approach to
image segmentation, in Proceedings of the 2003 Joint Conference of the Fourth Inter-
national Conference on Information, Communications and Signal Processing, 2003
and Fourth PaciÔ¨Åc Rim Conference on Multimedia, vol. 2, pp. 738‚Äì742.
7 Khan, S.S. and Ahamed, A. (2004) Cluster center initialization algorithm for
K-means clustering. Pattern Recognition Letters, 25 (11), 1293‚Äì1302.
8 Barakbah, A.R. and Kiyoki, Y. (2009) A new approach for image segmentation using
Pillar-kmeans algorithm. World Academy of Science, Engineering and Technology, 59,
23‚Äì28.
9 Ng, H.P., Ong, S.H., Foong, K.W.C., Goh, P.S., and Nowinski, W.L. (2006) Medical
image segmentation using k-means clustering and improved watershed algorithm, in
2006 IEEE Southwest Symposium on Image Analysis and Interpretation, pp. 61‚Äì65.
10 Ahmed, M.N., Yamany, S.M., Mohamed, N., Farag, A.A., and Moriarty, T. (2002) A
modiÔ¨Åed fuzzy c-means algorithm for bias Ô¨Åeld estimation and segmentation of MRI
data. IEEE Transactions on Medical Imaging, 21 (3), 193‚Äì199.
11 De, S., Bhattacharyya, S., and Dutta, P. (2010) EÔ¨Écient grey-level image segmen-
tation using an optimised MUSIG (OptiMUSIG) activation function. International
Journal of Parallel, Emergent and Distributed Systems, 26 (1), 1‚Äì39.
12 Tripathy, B.K., Basu, A., and Govel, S. (2014) Image segmentation using spatial
intuitionistic fuzzy C means clustering, in 2014 IEEE International Conference on
Computational Intelligence and Computing Research (ICCIC), pp. 1‚Äì5.
13 Balafar, M.A., Ramli, A.R., Saripan, M.I., Mahmud, R., Mashohor, S., and Balafar,
M. (2008) New multi-scale medical image segmentation based on fuzzy c-mean
(FCM), in 2008 IEEE Conference on Innovative Technologies in Intelligent Systems
and Industrial Applications, pp. 66‚Äì70.
14 Noordam, J.C., van den Broek, W.H.A.M., and Buydens, L.M.C. (2000) Geometrically
guided fuzzy c-means clustering for multivariate image segmentation, in Proceedings
of 15th International Conference on Pattern Recognition, pp. 462‚Äì465.
15 Pham, D.L. and Prince, J.L. (1999) Adaptive fuzzy segmentation of magnetic reso-
nance images. IEEE Transastions on Medical Imaging, 18 (9), 737‚Äì752.
16 Chen, L., Cui, B., Han, Y., Guan, Y., and Luo, Y. (2015) Two-dimensional fuzzy clus-
tering algorithm (2DFCM) for metallographic image segmentation based on spatial
information, in 2nd International Conference on Information Science and Control
Engineering, pp. 519‚Äì521.
17 Hruschka, E.R., Campello, R.J.G.B., Freitas, A.A., and Leon, P. (2009) A survey of
evolutionary algorithms for clustering. IEEE Transactions on System Man Cybernat-
ics, C: Applcations and Reviews, 39, 133‚Äì155.
18 Ripon, K.S.N., Tsang, C.H., and Kwong, S. (2006) Multi-objective data clustering
using variable-length real jumping genes genetic algorithm and local search method,
in IEEE International Joint Conference on Neural Networks, pp. 3609‚Äì3616.
19 Goldberg, D.E. (1989) Genetic Algorithms: Search, Optimization and Machine Learn-
ing, Addison-Wesley, New York.

24
Hybrid Intelligence for Image Analysis and Understanding
20 Hammouche, K., Diaf, M., and Siarry, P. (2008) A multilevel automatic threshold-
ing method based on a genetic algorithm for a fast image segmentation. Computer
Vision and Image Understanding, 109 (2), 163‚Äì175.
21 Fu, Z., He, J.F., Cui, R., Xiang, Y., Yi, S.L., Cao, S.J., Bao, Y.Y., Du, K.K., Zhang, H.,
and Ren, J.X. (2015) Image segmentation with multilevel threshold of gray-level &
gradient-magnitude entropy based on genetic algorithm, in International Conference
on ArtiÔ¨Åcial Intelligence and Industrial Engineering, pp. 539‚Äì542.
22 Hauschild, M., Bhatia, S., and Pelikan, M. (2012) Image segmentation using a genetic
algorithm and hierarchical local search, in Proceedings of the 14th Annual Conference
on Genetic and Evolutionary Computation, pp. 633‚Äì640.
23 De, S., Bhattacharyya, S., and Dutta, P. (2009) Multilevel image segmentation using
OptiMUSIG activation function with Ô¨Åxed and variable thresholding: a comparative
study, in Applications of Soft Computing: From Theory to Praxis, Advances in Intelli-
gent and Soft Computing (J. Mehnen, M. Koppen, A. Saad, and A. Tiwari), Springer-
Verlag, Berlin, pp. 53‚Äì62.
24 Min, W. and Siqing, Y. (2010) Improved k-means clustering based on genetic algo-
rithm, in International Conference on Computer Application and System Modeling
(ICCASM), pp. 636‚Äì639.
25 Biju, V.G. and Mythili, P. (2012) A genetic algorithm based fuzzy c-mean cluster-
ing model for segmenting microarray images. International Journal of Computer
Applications, 52 (11), 42‚Äì48.
26 Wei, C., Tingjin, L., Jizheng, W., and Yanqing, Z. (2010) An improved genetic FCM
clustering algorithm, in 2nd International Conference on Future Computer and
Communication, vol. 1, pp. 45‚Äì48.
27 Halder, A., Pramanik, S., and Kar, A. (2011) Dynamic image segmentation using
fuzzy c-means based genetic algorithm. International Journal of Computer Applica-
tions, 28 (6), 15‚Äì20.
28 Jansi, S. and Subashini, P. (2014) ModiÔ¨Åed FCM using genetic algorithm for seg-
mentation of MRI brain images, in IEEE International Conference on Computational
Intelligence and Computing Research, pp. 1‚Äì5.
29 Borsotti, M., Campadelli, P., and Schettini, R. (1998) Quantitative evaluation of color
image segmentation results. Pattern Recognition Letters, 19 (8), 741‚Äì747.
30 Ruspini, E. (1969) A new approach to clustering. Information and Control, 15,
22‚Äì32.
31 Zhang, H., Fritts, J., and Goldman, S. (2004) An entropy-based objective evalua-
tion method for image segmentation, in Proceedings of SPIE Storage and Retrieval
Methods and Applications for Multimedia, pp. 38‚Äì49.

25
2
Character Recognition Using Entropy-Based Fuzzy C-Means
Clustering
B. Kondalarao, S. Sahoo, and D.K. Pratihar
Department of Mechanical Engineering, Indian Institute of Technology, Kharagpur, West Bengal, India
2.1
Introduction
Recognition is a unique ability of the human brain. People and animals eÔ¨Äortlessly
identify the objects around them even when the number of possibilities is vast. A lot
of research had been carried out for recognition of diÔ¨Äerent characters like Bengali,
English, Hindi, Tamil, Chinese, and so on [1]. The challenging area of research in recog-
nition is handwritten characters, but most of the studies have been on multi-font and
Ô¨Åxed-font reading. The automatic recognition of handwritten characters is a diÔ¨Écult
task, because of the fact that it has noise problem and large variety of handwriting
styles. This noise problem could arise due to the quality of paper or pressure applied
while writing.
Clustering technique for character recognition had been used for a long time.
Necognitron network [2] was speciÔ¨Åcally designed to recognize handwritten charac-
ters. Two other unsupervised methods have become popular in recent literature of
character recognition; those are the adaptive reasoning theory classiÔ¨Åer [3] and the
nearest-neighbor clustering algorithm [4]. To overcome the problem that is caused
due to the noise present in the image, in the above-mentioned methods, a Fourier
coeÔ¨Écient as a feature and adaptive fuzzy leader clustering as classiÔ¨Åer were used [5].
However, because of more changes in the word arrangement with respect to spacing,
size, and orientation of the component, character recognition of complete words,
using Fourier coeÔ¨Écient as a feature, is not successful. Adaptive clustering had been
used [6] to improve the performance of the recognition system further for overlapping
characters.
Extensively used classiÔ¨Åers for character recognition are neural network, statistical
classiÔ¨Åers, template matching, support vector machine, and so on. Template matching
is the simplest way of character recognition, where a template is previously stored and
the similarity between the stored data and the template gives the degree of recogni-
tion. Michael Ryan et al. [7] used a template matching-technique for character recog-
nition on ID cards with proper preprocessing technique and segmentation. Many other
researchers also worked on template matching [8, 9], but the technique was more sensi-
tive to noise and had a poor recognition eÔ¨Éciency. Many statistical methods like nearest
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

26
Hybrid Intelligence for Image Analysis and Understanding
neighbor [10], Bayes‚Äô classiÔ¨Åer [11], and hidden Markov model [12] were also used for
character recognition. Neural network is an eÔ¨Écient soft computing technique that is
extensively used for recognition by training a massively parallel network. The prob-
lem with neural network is its computational complexity and it requires a large data
set for training. The problem with individual classiÔ¨Åcation techniques is that none of
these methods is independently suÔ¨Écient for a good classiÔ¨Åcation; that‚Äôs why hybrid
techniques came into the picture. Hybrid soft computing is a combination of two or
more soft computing tools, where advantages of both the tools are utilized to eliminate
their limitations for betterment of the classiÔ¨Åcation. A combined neuro-fuzzy technique
showed better results with respect to the neural network itself [13].
Fuzzy clustering techniques had also been used by many researchers for character
recognition. A lot of fuzzy clustering algorithms had been proposed that dealt with the
classiÔ¨Åcation of a large data set. The logic behind diÔ¨Äerentiating the large data set into
clusters is Ô¨Ånding the cluster centers of a data set formed by the samples of similar char-
acteristic. Still, there are diÔ¨Äerent approaches where the clusters are formed using the
membership function approach [14]. Zadeh et al. [15] used a diÔ¨Äerent fuzzy clustering
technique called FANNY [16] for recognizing Twitter hashtags. The renowned cluster-
ing techniques are fuzzy ISODATA [17], Fuzzy c-means (FCM) [18], fuzzy k-nearest
neighbors algorithm [19], and entropy-based clustering [20]. Among the fuzzy cluster-
ing tools described above, FCM is the most popular clustering method used for diÔ¨Äerent
applications. Sometimes, the traditional FCM clustering technique is unable to form
distinct clusters. Several modiÔ¨Åed versions of FCM had been developed to encounter
diÔ¨Äerent drawbacks; a few of them are Conditional FCM [21], quadratic function-based
FCM [22], and Modied FCM [23]. These techniques are data dependent; each method is
proposed for solving a speciÔ¨Åc problem. To deal with diÔ¨Éculties like uneven distribution
of centers, local minima problem, and dependency on the random values of membership
(ùúá) taken initially, entropy-based clustering technique was proposed. The entropy-based
fuzzy clustering technique can automatically detect the number of clusters depending
upon the user-deÔ¨Åned threshold value of similarity (ùõΩ). But the compactness of the clus-
ters formed by entropy-based clustering is not good enough, and the number of clusters
formed by this technique is highly sensitive to ùõΩvalue. Pratihar et al. [24] proposed a
combined technique called entropy-based FCM algorithm for obtaining distinct and
compact clusters. The working principle of this algorithm will be discussed in the next
section of this chapter.
Preprocessing and feature extraction are important parts of classiÔ¨Åcation, used
for character recognition. The features of an image can be divided into three broad
categories: global features, statistical features, and geometrical and topological
features [25]. Global features include Fourier transform, wavelet transform, Gabor
transform, moments, and so on. Global features do not change with transforms or
rotation of the original image. Major statistical features like zoning, crossing, and
projections are used for reducing the dimensionality of the image data to improve the
speed of recognition in case of online processing. There are various geometrical features
available that give some local or global characteristic of an image. A combination of
two or more features might help the recognition process. Preprocessing helps the
classiÔ¨Åcation process by improving the quality of an image or by reducing the amount
of noise incorporated in the image. Preprocessing followed by feature extraction is
always preferable for character recognition. The popular preprocessing techniques,

Character Recognition Using Entropy-Based Fuzzy C-Means Clustering
27
such as smoothening, thinning or broadening, and size normalization, are generally
used. As the test image can be taken from any imaging system, distortion of the image
is obvious for such cases. Smoothening generally deals with any random change in
the image. Thinning or broadening helps to bring all images under observation into
a standard shape that helps the classiÔ¨Åcation process. Size normalization is a method
of converting all the input images to a standard matrix (pixel value) format, such as
64 √ó 64, to reduce the variation of size of diÔ¨Äerent images collected from diÔ¨Äerent
sources. Some popular features like zoning [26], projection proÔ¨Åle [27], chain codes [28],
and longest run [29] are frequently used by the researchers for character recognition.
In zoning, the image is divided into some Ô¨Ånite number of sections or zones, and the
average image intensity value of individual zones is calculated and used as the training
data for classiÔ¨Åcation. For a binary image, the projection proÔ¨Åle is the sum of either
dark or white (depends upon the user) pixels along the rows or columns. The chain
code or boundary descriptor is an important feature for character recognition that
gives the information about the shape of the character. Four-directional chain codes
and eight-directional chain codes are the two types of chain codes available to deÔ¨Åne
complex boundaries of an image. Most of the studies had been done on Devnagari
[25], Bengali [29], Malayalam [30] Tamil [31], Chinese [32], and Arabic [33] letter
recognition. Due to its extensive variety, these letters are comparatively easy to classify
with respect to English letters. For example, English letters like ‚ÄòA‚Äô and ‚ÄòV‚Äô, ‚ÄòE‚Äô and ‚ÄòB‚Äô,
and ‚ÄòE‚Äô and ‚ÄòF‚Äô give almost the same feature values, so the classiÔ¨Åcation algorithm often
confuses to clearly classify the cluster centers. Thus, selection of good features is an
important task for proper character recognition. For template matching, preprocessing
and segmentation are important, and feature extraction is not required in this case. But
template matching does not provide good recognition results as mentioned earlier, and
it also has several other disadvantages.
This study made an approach to introduce the entropy-based fuzzy C-means (EFCM)
technique for the recognition of printed English alphabets. For training, Ô¨Åve diÔ¨Äerent
types of letters (A to Z) of diÔ¨Äerent fonts are created in Microsoft Word, and the
extracted features are fed to the classiÔ¨Åer for training. Fonts used here are the popular
letters used in newspapers, ID cards, and the internet. Three diÔ¨Äerent fuzzy classiÔ¨Åers
are used to draw a comparison and for validating the results of EFCM to classify and
recognize English alphabets correctly. The study is speciÔ¨Åcally aimed at Ô¨Ånding the suc-
cess rate of the EFCM technique for classiÔ¨Åcation of English letters and its applications
like a fully automated system for ID card detection that can be used in government
oÔ¨Éces, airports, and libraries. With some improvements, this methodology can be
implemented for helping visually impaired people to read newspapers, books, and
so on.
2.2
Tools and Techniques Used
2.2.1
Fuzzy Clustering Algorithms
Clustering is one of the methods of data mining that is used to extract useful informa-
tion from a set of data. Clusters are generally of two types in nature. The clusters that
have well-deÔ¨Åned and Ô¨Åxed boundaries are said to be crisp clusters, whereas those with

28
Hybrid Intelligence for Image Analysis and Understanding
vague boundaries are said to be fuzzy clusters [34]. In this study, recognition of English
alphabets is done using some of the algorithms of fuzzy clustering, namely the FCM
algorithm, entropy-based fuzzy clustering (EFC) algorithm, and EFCM algorithm.
2.2.1.1
Fuzzy C-means Algorithm
FCM is one of the methods of fuzzy clustering algorithms. Two or more clusters may
contain the same data, so a membership value is used to deÔ¨Åne the belongingness of
a data point to the clusters. This means that each data point of a data set belongs to
diÔ¨Äerent clusters with diÔ¨Äerent membership values. It is an iterative algorithm, in which
a dissimilarity measure is going to be minimized by updating centers of the clusters and
data point membership values with the clusters. The dissimilarity values are measured
in terms of Euclidean distance between data points and predeÔ¨Åned cluster centers. The
data points residing in the same cluster have high similarity value.
The FCM algorithm has the following steps, considering N data points represented by
(x1, x2, x3 ‚Ä¶ xN), each of which has L dimensions.
Step 1: Let C be the number of clusters to be created, where 2 ‚â§C ‚â§N.
Step 2: Assume a right value of cluster fuzziness (g ‚â•1).
Step 3: Assign random values to the N √ó C sized membership matrix [ùúá], such that
ùúá‚àà[0.0 1.0] and
C
‚àë
j=1
ùúáij = 1.0 for each i.
Step 4: Compute the kth dimension of the jth cluster center CCjk using the following
expression:
CCjk =
N
‚àë
i=1
ùúág
ijxik
N
‚àë
i=1
ùúág
ij
Step 5: The Euclidean distance is computed as follows between the ith data point and
jth cluster center:
dij = ||CCj ‚àíxi||
Step 6: Update membership matrix [ùúá] using the following expression according to dij.
If dij ‚â•0, then
ùúá=
1
C
‚àë
m=1
( dij
dim
)
2
g‚àí1
Step 7: Repeat Step 4 through Step 6 until the changes in [ùúá] becomes less than some
prespeciÔ¨Åed values.
Step 8: Form the clusters using the similarities in membership values of data points.
Step 9: If the cluster contains more than ùõæ% of the total data points, then only the clus-
ter is said to be valid.

Character Recognition Using Entropy-Based Fuzzy C-Means Clustering
29
2.2.1.2
Entropy-based Fuzzy Clustering
In this algorithm, the entropy value of each data point is determined based on a similarity
measure. This similarity measure value depends on the Euclidean distance between two
data points. Generally, a data point with low entropy value is selected as the center of a
cluster. The cluster thus formed is considered as valid, if and only if the number of data
points contained in a cluster is found to be more than ùõæ% of total data points.
EFC consists of the following steps, considering an input data set matrix [T], which
consists of N data points having L dimensions.
Step 1: Calculate the Euclidean distance between points i and j as follows:
dij =
‚àö
‚àö
‚àö
‚àö
‚àö
L
‚àë
k=1
(xik ‚àíxjk)2
Step 2: Determine the similarity Sij between two data points i and j, as given below:
Sij = e‚àíùõºdij,
where ùõºis a constant and similarity Sij varies in the range of (0.0, 1.0).
Step 3: Compute the entropy value of a data point xi with reference to all other points
as follows:
Ei = ‚àí
j‚â†1
‚àë
j‚ààx
(Sijlog2Sij + (1 ‚àíSij)log2(1 ‚àíSij))
Step 4: The data point with the minimum entropy value is selected as the cluster center.
Step 5: The data points having similarity with the selected cluster center greater than a
prespeciÔ¨Åed threshold value (ùõΩ) will be put in the cluster, and the same will be
removed from [T].
Step 6: If [T] is empty, terminate the program; otherwise, go to Steps 4 and 5.
2.2.1.3
Entropy-based Fuzzy C-Means Algorithm
Both the above-discussed algorithms have their inherent limitations to generate the
clusters, which have good distinctness as well as compactness. In this algorithm, both
the above-discussed algorithms are combined to get both the distinct as well as compact
clusters simultaneously.
The EFCM algorithm consists of the following steps. Consider N data points with L
dimensions each.
Step 1: Compute the Euclidean distance between points i and j, as follows:
dij =
‚àö
‚àö
‚àö
‚àö
‚àö
L
‚àë
k=1
(xik ‚àíxjk)2
Step 2: Compute the similarity value Sij between the data points i and j, as follows:
Sij = e‚àíùõºdij,
where ùõºis a constant that represents the relation between distance and similar-
ity of two data points.

30
Hybrid Intelligence for Image Analysis and Understanding
Step 3: Compute entropy of the ith data point with respect to all other data points, as
given here:
Ei = ‚àí
j‚â†1
‚àë
j‚ààx
(Sijlog2Sij + (1 ‚àíSij)log2(1 ‚àíSij))
Step 4: The data point that has the minimum entropy value is selected as the cluster
center.
Step 5: The data points having similarity with the selected cluster center greater than
a prespeciÔ¨Åed threshold value (ùõΩ) will be put in the cluster, and the same will
be removed from [T]. The cluster is declared as invalid if the number of data
points present in that cluster is found to be less than a prespeciÔ¨Åed fraction of
total data points. If the data set becomes empty, then go to Step 6; otherwise,
go to Step 3.
Step 6: Compute the Euclidean distance between ith and jth cluster centers using the
following expression:
dij = ||CCj ‚àíxi||
Step 7: Update the fuzzy membership matrix as follows:
ùúá=
1
C
‚àë
m=1
( dij
dim
)
2
g‚àí1
Step 8: Update the coordinates of the cluster centers using the information of the fuzzy
membership matrix as follows:
CCjk =
N
‚àë
i=1
ùúág
ijxik
N
‚àë
i=1
ùúág
ij
Step 9: Put the data points into the clusters obtained in Step 5, depending on the simi-
larity of their membership values.
2.2.2
Sammon‚Äôs Nonlinear Mapping
This is used to map the data from a higher dimension to lower dimension for the purpose
of visualization. Let us consider N vectors having L-dimensions each, which are denoted
by Xi where i=1,2, ‚Ä¶, N. Our aim is to map these N vectors from L-dimensional space
to 2D space. Let us assume that the 2D data is denoted by Yi, where i = 1,2, ‚Ä¶, N. The
mapping process is as follows:
Step 1: Generate N vectors in the 2D plane at random.
Step 2: The following condition is to be satisÔ¨Åed for perfect mapping:
d‚àó
ij = dij,
where d‚àó
ij is the Euclidean distance between two vectors Xi and Xj in higher
dimensional space; and dij is the distance between the corresponding two
mapped points Yi and Yj in a 2D plane.

Character Recognition Using Entropy-Based Fuzzy C-Means Clustering
31
Step 3: The mapping error is calculated using the following expression:
E(m) = 1
C
N
‚àë
i=1
N
‚àë
j=1(i<j)
[d‚àó
ij ‚àídij(m)]2
d‚àó
ij
,
where C =
N
‚àë
i=1
N
‚àë
j=1(i<j)
d‚àó
ij and dij(m) =
‚àö
‚àö
‚àö
‚àö
‚àö
D
‚àë
k=1
[yik(m) ‚àíyjk(m)]2.
Step 4: The steepest descent method is used to minimize the mapping error, and the
relationship between mth and (m + 1)th iterations is as follows:
ypq(m + 1) = ypq(m) ‚àí(MF)Œîpq(m),
where MF is a magic factor, which varies from 0.0 to 1.0, and
Œîpq(m) =
ùúïE(m)
ùúïypq(m)
ùúï2E(m)
ùúïy2
pq(m)
Now,
ùúïE(m)
ùúïypq(m) = ‚àí2
C
N
‚àë
j=1,j‚â†p
[d‚àó
pj ‚àídpj
dpjd‚àó
pj
]
(ypq ‚àíyjq)
ùúï2E(m)
ùúïy2
pq(m) = ‚àí2
C
N
‚àë
j=1,j‚â†p
1
dpjd‚àó
pj
[
(d‚àó
pj ‚àídpj) ‚àí
(ypq ‚àíyjq)2
dpj
(
1 +
d‚àó
pj ‚àídpj
dpj
)]
.
2.3
Methodology
2.3.1
Data Collection
The experiment has been conducted on printed English alphabets of diÔ¨Äerent fonts. Five
diÔ¨Äerent fonts of 26 alphabets are considered for the experiments. The Ô¨Åve diÔ¨Äerent fonts
consist of Arial, Bardely, Calibri, Cambria, and Times New Roman, which are shown in
Figure 2.1. These were stored as BMP Ô¨Åle format. These Ô¨Åve fonts are chosen due to
their variations in shape, style, and orientation. As these image Ô¨Åles of diÔ¨Äerent letters
are created manually, some preprocessing has been performed before feature extraction.
2.3.2
Preprocessing
Preprocessing is a technique for enhancing the image quality after accruing the image
from diÔ¨Äerent sources to eliminate the noise. It also helps to reduce the unnecessary data
Figure 2.1 Letters used for training the algorithm:
(a) Arial, (b) Bardely, (c) Calibri, (d) Cambria, and
(e) Times New Roman.
(a)
(b)
(c)
(d)
(e)

32
Hybrid Intelligence for Image Analysis and Understanding
in the image of interest, which is incorporated by human inaccuracy while creating or
capturing the test image. In this study, for simplicity, the printed letters are created from
Microsoft Word, so no image enhancing is required, but one may use the enhancing
technique if the image is scanned or captured from a newspaper or handwritten script.
Here, the preprocessing technique is used for helping the feature extraction and also
to eliminate the variations created while preparing the samples. The raw data is an RGB
image. As color intensity information of letters is unnecessary for recognition, it is better
to convert the RGB image to a binary image. First, the RGB image is converted to a
grayscale image and then to a binary image. A binary image only contains 0 and 1, where
0 signiÔ¨Åes the black pixel and 1 denotes the white pixel. Size normalization is applied to
the image to convert all images into a standard size, such as 64 √ó 64 contour, to deal with
the variable sizes of the collected data. Then, the boundary of the image is extracted to
support the feature extraction data to get better clusters. Figure 2.2 shows the output
image of diÔ¨Äerent stages of preprocessing. The Ô¨Ånal output (Figure 2.2c) which deÔ¨Ånes
the boundary of an image, has been used for extracting the feature. Figure 2.3 shows the
complete steps involved in the image preprocessing with the size variation information
of the image in diÔ¨Äerent steps.
2.3.3
Feature Extraction
For getting good clusters, the features should be unique for each and every character.
The beauty of feature extraction lies in the fact that the feature values should be similar
for the same character of diÔ¨Äerent fonts and diÔ¨Äerent for diÔ¨Äerent characters. This makes
feature extraction an important part of any classiÔ¨Åcation method. In this proposal, we
concentrate on the longest run feature. A longest run feature of a binary image is the
(a)
(b)
(c)
Figure 2.2 (a) Raw image, (b) converted binary image, and (c) boundary of the image.
Printed RGB
image (68√ó56√ó3)
Grayscale image
(68√ó56)
Binary image
(68√ó56)
Boundary of an
image (64√ó64)
Normalized
image (64√ó64)
Complement of the
image (68√ó56)
Figure 2.3 Flow diagram of preprocessing technique.

Character Recognition Using Entropy-Based Fuzzy C-Means Clustering
33
0
0
13
13
3
3
3
3
3
3
13
13
13
13
0
0
0
15
15
15
3
3
3
3
3
3
3
3
3
3
0
0
0
0
0
0
0
12
12
12
Figure 2.4 Longest run feature row-wise and column-wise for a 20 √ó 20 image.
sum of continuous white pixels along each row or column. As diÔ¨Äerent characters have
diÔ¨Äerent shapes, the longest run across rows or columns gives diÔ¨Äerent values. Figure 2.4
describes the concept of the longest run feature along the rows and columns.
The longest run feature gives the information on a local scale. If the size of the normal-
ized image is 64 √ó 64, the lengths of the row-wise longest run vector across the row and
column-wise longest run vector across the column are 64. The elements of the longest
run vector along the rows and columns are diÔ¨Äerent. The longest run vector along the
row provides the information about the horizontal alignment of the image, and that
across the column provides the information about the vertical alignment of the image
in a miniature scale. But the problem is that the longest run values are sensitive to the
shape and alignment of an image. It gives diÔ¨Äerent results if the same letter of a diÔ¨Äer-
ent font is slightly thick or tilted. It also gives diÔ¨Äerent results for the same letter with
diÔ¨Äerent fonts; for example, ‚ÄòE‚Äô of Times New Roman and ‚ÄòE‚Äô of Calibri font will give dif-
ferent values, which is not accepted. To deal with this problem, an averaged longest run
vector is used for constructing the main feature vector. The longest run values for 8 or
16 consecutive rows or columns are averaged to get the averaged longest run vector. As
the image contour has 64 rows and 64 columns, the length of the feature vector is 16 in
the case of a 8-point averaged longest run vector or 8 in the case of a 16-point averaged
longest run vector. The feature vector contains elements of the row-wise longest run vec-
tor and column-wise longest vector. Both the 8-point and 16-point averaged longest run
features are tested using the three classiÔ¨Åcation algorithms. For all the algorithms, the
16-point averaged longest run feature gives better recognition results compared to the
8-point averaged longest run feature in terms of recognition percentage. The main algo-
rithm is designed using the 16-point averaged longest run vector as the main feature.

34
Hybrid Intelligence for Image Analysis and Understanding
This recognition result depends on the size of the normalized image and the number
of points taken for average. As in the proposal, the normalized size of the test image is
64 √ó 64; the 16-point averaged longest run feature gives better recognition results, but
it may diÔ¨Äer if the normalized image is of diÔ¨Äerent size. One can use zero padding for
constructing the averaged longest run vector, if the number of rows or columns of the
image contour is not exactly divisible by the number of points taken for average.
2.3.4
ClassiÔ¨Åcation and Recognition
Both the FCM and EFC algorithms have their inherent limitations. FCM iteratively
updates the clusters, and EFC determines the clusters uniquely but does not improve
clusters iteratively. EFC can determine more distinct clusters, whereas FCM can deter-
mine more compact clusters. An EFCM clustering algorithm captures the advantages of
both these algorithms.
ClassiÔ¨Åcation of huge input data using EFCM clustering has been carried out based on
similarity in data. Also, the same has been done using FCM and EFC algorithms. Then,
the comparison of results obtained by three algorithms is done.
Figure 2.5 shows the Ô¨Çowchart of developed algorithms. Initially, clustering is carried
out on the input data separately using the above three algorithms. The number of clus-
ters formed in all three algorithms is 26, as there are 26 alphabets. After forming the
desired number of clusters, Euclidean distances are calculated between each test data
and the cluster centers. Out of all Euclidean distances calculated, the minimum value
will decide the belongingness of the test data to a predeÔ¨Åned cluster. Now, this prede-
Ô¨Åned cluster will decide the alphabet for recognition.
2.4
Results and Discussion
The experiment has been conducted on a wide range of characters to see how well the
system reacts to the variations. Five diÔ¨Äerent formats of 26 alphabets are considered for
the experiments. Five diÔ¨Äerent formats, namely Arial, Bardely, Calibri, Cambria, and
Times New Roman, are used (refer to Tables A.2.1 through A.2.5 of the Appendix). These
are stored as BMP Ô¨Åle format. The shape, orientation, thickness, and style of writing of
diÔ¨Äerent character sets are not similar.
Input data 
Classification of input data using fuzzy
clustering algorithms
Calculation of Euclidean distance
between test data and cluster
center
Recognition
Testing data
Figure 2.5 Flowchart of the proposed methodology for classiÔ¨Åcation and recognition.

Character Recognition Using Entropy-Based Fuzzy C-Means Clustering
35
Compactness is an intracluster distance of the elements. The members of each cluster
should be as close to each other as possible to make a cluster more compact. Variance
(ùúé2) is the common measure of compactness, which is given as follows:
ùúé2 = 1
N
N
‚àë
i=1
(xi ‚àíùúá)2
Distinctness is the intercluster distance. The distance between cluster centers should
be as high as possible to form clusters that can be easily distinguishable from others.
Euclidean distance is the common measure of distinctness, and it is calculated as follows:
dij =
‚àö
‚àö
‚àö
‚àö
‚àö
L
‚àë
k=1
(xik ‚àíxjk)2
It can be observed (see Table 2.1) that the compactness and distinctness values are well
balanced in EFCM, where as in the case of FCM, the value of distinctness is poor, and
for EFC, the value of compactness is poor. FCM and EFCM give more compact clusters,
as they update the membership values and the cluster centers iteratively. The EFCM
algorithm is found to yield a good distinct cluster as the EFC algorithm does. Both EFCM
and EFC algorithms determine the clusters uniquely by calculating the entropy value of
each point. So, from the above points, it is clear that the EFCM has a good balance
of compactness and distinctness in forming the clusters. However, the performance of
these fuzzy clustering algorithms may be data dependent.
The cluster centers obtained using all the algorithms need to be visualized in order
to conceive their special positions. The data of cluster centers, in this experiment, is of
higher dimensions. This high-dimensional data is to be mapped to either 2D or 3D for
proper visualization. Sammon‚Äôs mapping technique is used to visualize the data. Here,
the role of Sammon‚Äôs mapping is just to visualize the higher dimensional data by map-
ping it into a lower dimension. Figure 2.6 shows the 2D view of the cluster centers of
input data, as achieved by all the three algorithms. Note that two axes of this Ô¨Ågure
represent dimensionless numbers. The Ô¨Ågures obtained using EFC and EFCM (refer to
Figure 2.6b and 2.6c) are seen to be more distinct compared to that obtained by the FCM
algorithm (refer to Figure 2.6a). The EFCM algorithm is able to form clusters, which are
as compact as the FCM developed clsuters, and also their distinctness is comparable
with that obtained by the EFC algorithm.
Table 2.2 displays the result of recognition using all three said algorithms. The suc-
cessful recognition rate of EFCM is found to be more than that of the remaining two. It
is also observed that the letters that have similarity are misidentiÔ¨Åed more. However, the
recognition rate can be improved using appropriate feature extraction techniques with
Table 2.1 Output of diÔ¨Äerent clustering
algorithms for input data
Algorithm
Compactness
Distinctness
FCM
0.7289
5.0032
EFC
0.170
5.6281
EFCM
0.5312
5.3643

36
Hybrid Intelligence for Image Analysis and Understanding
‚Äì6
‚Äì6
‚Äì4
‚Äì2
0
2
4
6
‚Äì4
‚Äì2
0
(a)
2
4
6
‚Äì6
‚Äì6
‚Äì4
‚Äì2
0
2
4
6
‚Äì4
‚Äì2
0
(b)
2
4
6
‚Äì6
‚Äì8
‚Äì6
‚Äì4
‚Äì2
0
2
4
6
‚Äì4
‚Äì2
0
(c)
2
4
6
Figure 2.6 Visualization of cluster centers of input data obtained using the (a) FCM algorithm, (b) EFC
algorithm, and (c) EFCM algorithm.

Character Recognition Using Entropy-Based Fuzzy C-Means Clustering
37
Table 2.2 Results of recognition of alphabets with three algorithms for
Times New Roman
Input alphabets
Output of FCM
Output of EFC
Output of EFCM
A
A
A
A
B
?
B
B
C
C
?
C
D
?
D
?
E
E
E
E
F
F
F
F
G
?
?
?
H
H
?
H
I
?
I
I
J
?
J
?
K
K
K
K
L
L
L
L
M
M
M
M
N
N
N
N
O
O
?
O
P
P
?
P
Q
Q
Q
Q
R
?
R
R
S
S
?
S
T
T
T
T
U
U
U
U
V
V
V
V
W
W
W
W
X
X
?
?
Y
Y
Y
Y
Z
Z
Z
Z
% of recognition
76.92
73.07
84.61
the clustering techniques. Results of Table 2.2 have been obtained using Times New
Roman font as testing data and all the remaining four fonts as training data.
Table 2.3 shows the percentage of recognition accuracy of each font by using all three
algorithms. From Table 2.3, it can be observed that the EFCM has a higher percentage
of recognition accuracy compared to FCM and EFC except for the Bardely font, as the
shape of this font is diÔ¨Äerent compared to the others. EFCM has maintained a better per-
centage of recognition accuracy compared to both FCM and EFC. It could be because of
the balance in compactness and distinctness values, which shows the quality of clusters
formed. Moreover, the results of EFCM are found to be more consistent than those of
the remaining two algorithms.
The CPU time values of the FCM, EFC, and EFCM are found to be equal to 0.176206,
0.180281, and 0.20401 seconds, respectively. Out of all the algorithms, EFCM is taking

38
Hybrid Intelligence for Image Analysis and Understanding
Table 2.3 Recognition accuracy with respect to each font
Test Font
FCM (%)
EFC (%)
EFCM (%)
Arial
73.08
73.07
76.92
Bardely
57.69
76.92
61.54
Calibri
84.62
73.07
84.62
Cambria
70.37
73.07
73.08
Times New Roman
76.92
73.07
84.61
more time because it is a combination of both FCM and EFC. In other words, EFCM
initially creates the cluster centers using entropy values of the data points like the EFC
algorithm, and then updates the created cluster centers iteratively, as it is done in FCM.
2.5
Conclusion and Future Scope of Work
EFCM is a hybrid clustering technique, and the main advantage of EFCM over FCM and
EFC is that it can produce distinct as well as compact clusters, as discussed earlier in this
chapter. The results shown in Table 2.2 support the statement that the compactness and
distinctness values are well balanced in EFCM, whereas in the case of FCM, the value of
distinctness is poor, and for EFC, the value of compactness is poor. This study is meant
to show the eÔ¨Éciency of EFCM for English character recognition, compared to the exist-
ing clustering techniques. By observing the results shown in Tables 2.2 and 2.3 and the
distributions of clusters in Figure 2.6, it is clear that the EFCM algorithm gives better
cluster formation and recognition accuracy than the EFC and FCM do. The method of
averaging the longest run feature gives better results than using the longest run feature
itself; also, averaging the longest run feature reduces the size of the feature vector or the
database. If we use only the longest run feature, the size of the feature vector for each
font would be 128 √ó 26, but in our case, it is 8 √ó 26. Reduction of length of the feature
vector also reduces the execution time of the proposed algorithm. Less execution time
will help for the implementation of the letter recognition technique in real-time appli-
cations, like identity card detection, reading the number plates of vehicles, and so on.
The English alphabet does not have much variation, unlike Chinese, Hindi, and Bengali
alphabets, which makes it diÔ¨Écult to distinguish. It is a challenge for the researchers
to obtain the same accuracy for recognizing English characters is obtained for Chinese,
Bengali, and Hindi characters.
The accuracy obtained in this study with a comparatively small data set is not up to
the mark. In future, attempts will be made to improve the accuracy of the result by
incorporating some new features or a combination of diÔ¨Äerent features. For recognition
technique, we have used the Euclidean distance between the test data and the cluster
center; researchers may think of a better recognition technique to improve the accuracy.
The next approach will be to run the proposed EFCM-based technique on handwritten
characters and to get an accuracy level obtained by the current classical and soft com-
puting techniques. A statistical analysis like Z-test or McNemar‚Äôs test will be carried
out in future to get a clear idea about the diÔ¨Äerences between the mentioned clustering
techniques.

Character Recognition Using Entropy-Based Fuzzy C-Means Clustering
39
References
1 Suen, C.Y., Berthod, M., and Mori, S. (1980) Automatic recognition of handprinted
characters‚Äîthe state of the art. Proceedings of the IEEE, 68 (4), 469‚Äì487.
2 Fukushima, K. (1988) Neocognitron: A hierarchical neural network capable of visual
pattern recognition. Neural networks, 1 (2), 119‚Äì130.
3 Gan, K.W. and Lua, K.T. (1992) Chinese character classiÔ¨Åcation using an adaptive
resonance network. Pattern Recognition, 25 (8), 877‚Äì882.
4 Sharan, A. and Mitra, S. (1994) Handwritten character recognition by an adap-
tive fuzzy clustering algorithm, in Fuzzy Systems, 1994. IEEE World Congress on
Computational Intelligence., Proceedings of the Third IEEE Conference on, IEEE,
pp. 1820‚Äì1824.
5 O‚ÄôHair, M.A. (1990) A whole word and number reading machine based on two
dimensional low frequency Fourier transforms, Tech. Rep., DTIC Document.
6 Ashir, A.M. and Shehu, G.S. (2015) Adaptive clustering algorithm for optical charac-
ter recognition, in Electronics, Computers and ArtiÔ¨Åcial Intelligence (ECAI), 2015 7th
International Conference on, IEEE, SG‚Äì13.
7 Ryan, M. and HanaÔ¨Åah, N. (2015) An examination of character recognition on ID
card using template matching approach. Procedia Computer Science, 59, 520‚Äì529.
8 Hu, J. and Pavlidis, T. (1996) A hierarchical approach to eÔ¨Écient curvilinear object
searching. Computer vision and image understanding, 63 (2), 208‚Äì220.
9 Ramteke, R. and Mehrotra, S. (2008) Recognition of handwritten Devanagari numer-
als. International Journal of Computer Processing of Oriental Languages, Chinese
Language Computer Society & World ScientiÔ¨Åc Publishing Company.
10 Arora, S., Bhattacharjee, D., Nasipuri, M., Malik, L., Kundu, M., and Basu, D.K.
(2010) Performance comparison of svm and ann for handwritten devnagari character
recognition. arXiv preprint arXiv:1006.5902.
11 Namboodiri, A.M. and Jain, A.K. (2004) Online handwritten script recognition.
Pattern Analysis and Machine Intelligence, IEEE Transactions on, 26 (1), 124‚Äì130.
12 Bhowmik, T.K., Parui, S.K., and Roy, U. (2008) Discriminative HMM training with
ga for handwritten word recognition, in Pattern Recognition, 2008. ICPR 2008. 19th
International Conference on, IEEE, pp. 1‚Äì4.
13 Cho, S.B. (2002) Fusion of neural networks with fuzzy logic and genetic algorithm.
Integrated Computer-Aided Engineering, 9 (4), 363‚Äì372.
14 Baraldi, A. and Blonda, P. (1999) A survey of fuzzy clustering algorithms for pattern
recognition. i. Systems, Man, and Cybernetics, Part B: Cybernetics, IEEE Transactions
on, 29 (6), 778‚Äì785.
15 Zadeh, L.A., Abbasov, A.M., and Shahbazova, S.N. (2015) Analysis of Twitter hash-
tags: Fuzzy clustering approach, in Fuzzy Information Processing Society (NAFIPS)
held jointly with 2015 5th World Conference on Soft Computing (WConSC), 2015
Annual Conference of the North American, IEEE, pp. 1‚Äì6.
16 Kaufman, L. and Rousseeuw, P.J. (2009) Finding groups in data: an introduction to
cluster analysis, vol. 344, John Wiley & Sons.
17 Dunn, J.C. (1973) A fuzzy relative of the isodata process and its use in detecting
compact well-separated clusters. Cybernetics and Systems, 3 (3), 32‚Äì57.
18 Bezdek, J.C. (1973) Fuzzy mathematics in pattern classiÔ¨Åcation. PhD thesis, Cornell
University, Ithaca, NY.

40
Hybrid Intelligence for Image Analysis and Understanding
19 Keller, J.M., Gray, M.R., and Givens, J.A. (1985) A fuzzy k-nearest neighbor algo-
rithm. Systems, Man and Cybernetics, IEEE Transactions on, (4), 580‚Äì585.
20 Yao, J., Dash, M., Tan, S., and Liu, H. (2000) Entropy-based fuzzy clustering and
fuzzy modeling. Fuzzy Sets and Systems, 113 (3), 381‚Äì388.
21 Zigkolis, C.N. and Laskaris, N.A. (2009) Using conditional FCM to mine
event-related brain dynamics. Computers in Biology and Medicine, 39 (4), 346‚Äì354.
22 Ichihashi, H., Honda, K., Notsu, A., and Hattori, T. (2007) Aggregation of standard
and entropy based fuzzy c-means clustering by a modiÔ¨Åed objective function, in
Foundations of Computational Intelligence, 2007. FOCI 2007. IEEE Symposium on,
IEEE, pp. 447‚Äì453.
23 Han, S., Lee, I., and Pedrycz, W. (2009) ModiÔ¨Åed fuzzy c-means and Bayesian equal-
izer for nonlinear blind channel. Applied Soft Computing, 9 (3), 1090‚Äì1096.
24 Dey, V., Pratihar, D.K., and Datta, G.L. (2011) Genetic algorithm-tuned
entropy-based fuzzy c-means algorithm for obtaining distinct and compact clusters.
Fuzzy Optimization and Decision Making, 10 (2), 153‚Äì166.
25 Dongre, V.J. and Mankar, V.H. (2011) A review of research on Devnagari character
recognition. arXiv preprint arXiv:1101.2491.
26 Rajashekararadhya, S. and Ranjan, P.V. (2009) A novel zone based feature extrac-
tion algorithm for handwritten numeral recognition of four Indian scripts. Digital
Technology Journal, 2 (1).
27 Arora, S., Bhattacharjee, D., Nasipuri, M., Basu, D.K., and Kundu, M. (2010) Recog-
nition of non-compound handwritten Devnagari characters using a combination of
MLP and minimum edit distance. arXiv preprint arXiv:1006.5908.
28 Malik, L. and Deshpande, P. (2009) Recognition of printed Devnagari characters with
regular expression in Ô¨Ånite state models, in International workshop on machine intel-
ligence research.
29 Das, N., Sarkar, R., Basu, S., Saha, P.K., Kundu, M., and Nasipuri, M. (2015) Hand-
written Bangla character recognition using a soft computing paradigm embedded in
two pass approach. Pattern Recognition, 48 (6), 2054‚Äì2071.
30 Chacko, A.M.M. and Dhanya, P. (2015) A comparative study of diÔ¨Äerent feature
extraction techniques for oÔ¨Ñine Malayalam character recognition, in Computational
Intelligence in Data Mining-Volume 2, Springer, pp. 9‚Äì18.
31 Kannan, R.J. and Subramanian, S. (2015) An adaptive approach of Tamil character
recognition using deep learning with big data: a survey, in Emerging ICT for Bridging
the Future-Proceedings of the 49th Annual Convention of the Computer Society of
India (CSI) Volume 1, Springer, pp. 557‚Äì567.
32 Kimura, F., Takashina, K., Tsuruoka, S., and Miyake, Y. (1987) ModiÔ¨Åed quadratic
discriminant functions and the application to Chinese character recognition. Pattern
Analysis and Machine Intelligence, IEEE Transactions on, (1), 149‚Äì153.
33 Alginahi, Y.M. (2013) A survey on Arabic character segmentation. International Jour-
nal on Document Analysis and Recognition (IJDAR), 16 (2), 105‚Äì126.
34 Pratihar, D.K. (2014) Soft computing: Fundamentals and Applications, Narosa Pub-
lishing House, New Delhi.

Character Recognition Using Entropy-Based Fuzzy C-Means Clustering
41
Appendix
Table A.2.1 Database for Arial font
16 average row-wise longest run feature
16 average column-wise longest run feature
Letter
1.5
1
3.8125
1.4375
2.1875
2.75
2.75
2.125
A
5.625
2.875
3.125
6
6.125
1
1.125
2.6875
B
3.5
1.4375
1.5
3.3125
3.6875
1
1
1.6875
C
5.0625
1
1
5.3125
7.375
1
1.0625
3.3125
D
7.375
3.75
3.75
7.6875
5.875
1
1
2.0625
E
7.625
3.5625
3.5625
1.5625
6.3125
1
1
1.6875
F
3.25
1.375
3.8125
3.5
3.3125
1
1.375
3.5
G
1.5
3.25
3.375
1.5625
6.5625
1
1
6.5
H
2.75
1
1
2.8125
0
4.5
1
3.75
I
1.6875
1
1.625
3.6875
1.5625
1.1875
4
3.875
J
1.875
1.4375
1.375
1.75
6.1875
1.125
1.375
1.3125
K
1.6875
1
1
7.5625
7.875
1
1
1.375
L
1.75
1
1
1.5
8.9375
2.8125
2.9375
8.4375
M
1.625
1.1875
1.1875
1.6875
7.75
1.6875
1.4375
7.8125
N
3.3125
1
1
3.25
3.4375
1
1
3.3125
O
5.6875
2.875
3.25
1.4375
5.9375
1
1
1.9375
P
3.375
1
1.4375
3.6875
3.1875
1
1.25
3.5
Q
5.625
3
2.5
1.5625
6.25
1
1.4375
2.1875
R
3.6875
3.25
2.875
4
2.5
1.0625
1
2.5
S
5.875
1
1
1.4375
1.0625
4.25
4.1875
1.25
T
1.4375
1
1
3.4375
6.625
1
1.0625
6.5
U
1.4375
1
1
1.5625
2.125
2.6875
2.5625
2.375
V
1.3125
1
1
1.375
4.875
4.375
4.25
5.0625
W
1.75
1.3125
1.4375
1.75
1.625
1.625
1.625
1.5625
X
1.875
1.3125
1
1.5625
1.5
3.3125
2.9375
1.625
Y
7.125
1.5
1.4375
8.1875
1.9375
1.25
1.1875
1.9375
Z

42
Hybrid Intelligence for Image Analysis and Understanding
Table A.2.2 Database for Bardely font
16 average row-wise longest run feature
16 average column-wise longest run feature
Letter
1.0625
1.4375
3.5
1.8125
2.1875
2.25
3.25
2.0625
A
4.1875
2.4375
4.0625
4.1875
1.75
3.625
1.875
1.9375
B
2.0625
1.0625
1.3125
4.25
3.1875
1.125
2
2
C
3
1.4375
1.25
4.1875
3.875
1.0625
1
2.1875
D
6
5.625
1.1875
3.75
3.5625
1
1.125
1.9375
E
4.5625
4.8125
1.0625
1.25
3.875
2
1.0625
1.75
F
3.375
1.5625
4.4375
1.1875
2
1
1.625
2.5
G
0.9375
4.125
1
1.1875
4.5625
1
2.875
3.25
H
4.625
2.375
2.625
4.3125
2.5
1.5
2.5625
1.4375
I
1
1
1.0625
3.9375
1.625
1.9375
1.375
3.1875
J
1.25
2.6875
1.75
2.8125
3.5625
1.1875
1.75
1.6875
K
0.875
1.0625
1
5.125
4.3125
1
1.25
1.6875
L
1.3125
1.8125
1.9375
1.3125
4.4375
3.125
2.125
5
M
1.3125
1.4375
2.375
1.875
3.9375
1.375
1.25
5
N
3.5
1.25
1.25
3.125
3.3125
1.25
1.4375
2.3125
O
3.9375
1.8125
3.3125
1.25
2.625
2.75
1.1875
1.625
P
3.8125
1.75
2.5
3.5
3.75
2.5625
2.3125
3.75
Q
3.875
3.1875
2
2.75
3.25
1.8125
1.75
1.8125
R
3.0625
2.125
2.5
3.25
1.8125
1.4375
1.0625
2.0625
S
4.5
1
1
1.4375
1.3125
4.5625
1.125
1.9375
T
1.1875
1.25
1.5625
2.3125
4
1.4375
3.5625
1.4375
U
1.125
1
1.0625
1.125
2.0625
3.125
2
1.9375
V
1.375
1.9375
2
1.5625
5.8125
2.5
5.4375
4.75
W
1.5
1.375
1.3125
2.1875
1.625
1.8125
2.125
1.375
X
1.75
1.25
1
1.125
1.125
3.8125
1.6875
1
Y
2.875
1.25
1.1875
3.875
1.625
1.5625
1.6875
1.625
Z

Character Recognition Using Entropy-Based Fuzzy C-Means Clustering
43
Table A.2.3 Database for Calibri font
16 average row-wise longest run feature
16 average column-wise longest run feature
Letter
1.6875
1.1875
4.375
1.5625
2.375
2.625
2.625
2.5625
A
4.875
2.5625
2.75
5.375
5.875
1
1.5625
2.8125
B
4.4375
1
1.0625
4
3.4375
1.1875
1
1.3125
C
4.875
1.125
1.0625
5
7.25
1
1.0625
3.4375
D
7
3.25
3.25
7.0625
4.3125
2.25
1
1.5625
E
6.9375
3.5
3.5
1.6875
4.4375
2.625
1
1.4375
F
3.9375
2.625
2.0625
3.75
2.9375
1.0625
1.25
3.5
G
1.375
3.1875
3.1875
1.375
6
1
1
5.9375
H
3.0625
1
1
3.125
3.9375
1
1
3.9375
I
1.9375
1
1
5
1.0625
1
4.125
4.125
J
2.125
1.8125
1.625
2.125
6.375
1.375
1.625
1.3125
K
1.875
1
1
6.8125
4.375
4.375
1
1.25
L
1.4375
1
1
1.25
8.375
2.875
3.0625
8.25
M
1.9375
1.25
1.1875
1.75
8
1.9375
1.875
8
N
3.5625
1.0625
1
3.4375
3.5625
1
1.0625
3.5
O
5.0625
1.125
4.375
1.6875
5.8125
1
1.125
2.125
P
3.4375
1
1.125
3.5
3.4375
1
2.0625
2.875
Q
4.9375
2.5625
2.1875
1.6875
6
1
1.75
2.125
R
4.75
2.3125
2.375
5.625
2.0625
1
1.0625
2.75
S
6.125
1
1
1.5
1.3125
4.375
4.3125
1.1875
T
1.5625
1
1.125
3.625
6.8125
1
1
6.875
U
1.5
1
1
1.6875
2.4375
2.6875
2.6875
2.3125
V
1.1875
1
1
1.375
4.5625
4.5
4.25
4.6875
W
1.75
1.4375
1.4375
1.875
1.5
1.875
1.9375
1.5625
X
1.8125
1.25
1
1.5625
1.5
3.375
3.4375
1.5
Y
7.125
1.25
1.3125
7.125
1.8125
1.25
1.375
1.5625
Z

44
Hybrid Intelligence for Image Analysis and Understanding
Table A.2.4 Database for Cambria font
16 average row-wise longest run feature
16 average column-wise longest run feature
Letter
1.4375
1
3.5
2.375
1.5625
3
3.25
2.1875
A
5.1875
2.5
2.6875
5.375
3.9375
2.5
1
2.75
B
4.125
1.0625
1
4.3125
3.6875
1.3125
1
1.75
C
4.8125
1.0625
1.125
4.875
4.1875
4.25
1.25
3.625
D
6.8125
2.5
2.5
6.9375
4.25
2.5
1.1875
3.1875
E
6.5625
2.375
2.375
2.8125
3.9375
2.5
1.125
2.8125
F
3.875
1
2.625
3.3125
3.625
1.125
1.1875
4.125
G
2.375
4.375
1
2.4375
4.0625
2.5625
2.6875
4.0625
H
5.8125
1
1
5.8125
1.125
4
3.875
1.25
I
4.375
1
1
3.0625
0.8125
3.875
3.8125
0.75
J
2.5
1.5625
1.5
2.5
4.0625
2.6875
1.75
1.1875
K
2.8125
1
1.3125
6.3125
3.9375
4.125
1
2.3125
L
1.9375
1
1
2.5625
7.4375
3
3.0625
7.1875
M
2.3125
1.25
1.25
2.3125
7
1.625
1.6875
7.125
N
3.25
1
1.0625
3.25
3.8125
1.125
1.0625
3.875
O
5.0625
1.125
3.8125
3
4.0625
2.875
1.0625
2.125
P
3.0625
1
1.4375
3.75
3.125
1.0625
1.5625
3
Q
4.5
1.1875
2.3125
2.6875
4.125
2.875
1.875
1.9375
R
4.4375
2.125
1.9375
4.9375
2.3125
1.375
1.125
2.6875
S
5.625
1.3125
1
2.75
2
4.25
4.25
2.125
T
2.625
1
1
3.25
3.875
3.9375
1.375
6.25
U
2.5625
1.0625
1
1.4375
1.6875
3.3125
2.5625
2.4375
V
1.8125
1
1
1.375
4.5
4.375
5.4375
3.5
W
2.9375
1.25
1.4375
2.8125
1.625
1.8125
2.0625
1.3125
X
2.5625
1.25
1
2.6875
1.375
2.9375
3.0625
1.3125
Y
6.4375
1.625
1.6875
6.625
2.25
1.5625
1.5625
2.4375
Z

Character Recognition Using Entropy-Based Fuzzy C-Means Clustering
45
Table A.2.5 Database for Times New Roman font
16 average row-wise longest run feature
16 average column-wise longest run feature
Letter
1.125
1
3.5
2.3125
2.0625
2.6875
2.8125
1.75
A
4.4375
2.1875
2.25
4.75
4.0625
3.125
1.375
3.25
B
3.25
1.1875
1.0625
3.75
3.4375
1.1875
1.0625
2.625
C
4.625
1
1
4.5
4.25
4.1875
1
3.25
D
6.25
3.9375
1.375
6.25
4.3125
2.5625
1.125
3.5625
E
6.5
2.5
2.5
3.125
4.25
3.0625
1.75
3.25
F
3.0625
2.5
1
3.1875
3.125
1.1875
1.0625
5.375
G
2.5625
2.75
2.75
2.625
4.25
3.5
3.375
4.3125
H
5.1875
1
1
5.1875
1.25
4.25
4.125
1.25
I
4.0625
1
1
3
1.0625
4.5
3.9375
1.0625
J
2.8125
1.125
1.0625
3.1875
5.75
1.1875
1.6875
1.5625
K
2.875
1
1.125
6.5
4.25
4.3125
1
1.8125
L
1.6875
1
1
2.125
7.6875
2.5625
2.6875
7.4375
M
2.125
1.1875
1.125
2.5625
6.5625
1.5625
1.25
7.3125
N
2.875
1
1
2.75
3.75
1.125
1.0625
3.625
O
4.6875
1.375
2.8125
3.0625
4.375
2.6875
1.25
1.9375
P
2.9375
1
1.5625
3.5
3.0625
1.4375
1.4375
3.125
Q
3.9375
1.5625
1.9375
2.875
4.375
3
2
1.875
R
3.4375
2.4375
2.0625
3.625
3.1875
1
1
3.4375
S
5.625
1
1
2.75
1.75
4.4375
4.375
1.6875
T
2.4375
1
1
3.3125
6.625
1.125
1
6.5
U
2.5
1
1
1.0625
2.125
2.375
2.5
2.0625
V
2
1
1.0625
1.125
3.0625
4.125
4.375
3.1875
W
2.9375
1.0625
1.1875
2.9375
1.5625
1.8125
1.9375
1.4375
X
2.5625
1
1.1875
2.625
1.375
3.125
2.9375
1.75
Y
6.75
1.25
1.375
7.4375
2.5625
1.4375
1.3125
2
Z

47
3
A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
Pawan Kumar Singh, Supratim Das, Ram Sarkar, and Mita Nasipuri
Department of Computer Science and Engineering, Jadavpur University, Kolkata, West Bengal, India
3.1
Introduction
There is an enormous diversity among the scripts used in the world. Almost every coun-
try has its own language/script(s), which can be distinguished from the others in dif-
ferent facades. Script identiÔ¨Åcation is deÔ¨Åned as a process of recognizing the scripts,
written or hand-printed, in any multilingual, multiscript environment. Script identiÔ¨Å-
cation is stimulated by optical character recognition (OCR) research, which is used to
recognize the characters written in a particular script of the underlying document. The
tool OCR is generally described as the method of reading the optically scanned text
by the electronic device [1]. Until the last few decades, researchers have paid almost
no attention to the problem of script/language. But, as the world is getting progres-
sively more interlinked, there is a real need for automatic script identiÔ¨Åcation because
the growing requirement for processing of documents in our daily life causes people
to frequently face situations where the diversity of scripts/languages makes such man-
ual text processing impractical. Also, in a multilingual country like India, where a wide
variety of scripts and languages are prevalent, researchers of the OCR community have
always felt the lack of a workable solution for the script identiÔ¨Åcation module when
they deal with multilingual documents. In general, any OCR engine is made in such a
way that it can process documents written in a particular script (or language) within
its knowledge. OCR in a multilingual environment can be made using either of the
subsequent two choices: (1) designing a comprehensive OCR system that can identify
each character of the text words of the input scripts that may prevail in the document
pages, and (2) designing a script identiÔ¨Åcation system to recognize diÔ¨Äerent script words
present in the document pages and then running individual OCR available for each
script. For the Ô¨Årst option, it is generally not feasible to recognize characters that are
written in diÔ¨Äerent scripts using a single OCR module. This is due to the fact that the
structural properties, style, and nature of writing are deterministic features for character
recognition that mostly diÔ¨Äer from one script to another [2]. For instance, the features
that are used to recognize Devanagari script might not be appropriate for recognizing
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

48
Hybrid Intelligence for Image Analysis and Understanding
other Indic scripts. An alternative option for identifying scripts in a multiscript envi-
ronment is to apply a pool of OCRs corresponding to diÔ¨Äerent scripts. The characters
present in an input script document can then be identiÔ¨Åed reliably by selecting the
appropriate OCR system from the said repository. Consequently, the vast majority of
the OCR algorithms used in these applications are selected based upon a priori knowl-
edge of the script and/or language of the documents under analysis. Unfortunately, this
information may not be readily available. Here appears the issue of script identiÔ¨Åcation.
Almost all existing works on OCR systems make an important implicit assumption that
the script type of the document to be processed is known beforehand. This assump-
tion requires human intervention to select the appropriate OCR algorithm, limiting
the possibility of completely automating the analysis process, especially in a multilin-
gual environment. Hence, in this regard, there is a requirement to develop a pre-OCR
script identiÔ¨Åcation system to identify the script type of the document, so that a speciÔ¨Åc
OCR tool can be selected. Automatic script identiÔ¨Åcation facilitates sorting, search-
ing, indexing, and retrieving multilingual documents. Script recognition also helps in
text area identiÔ¨Åcation, video indexing, and retrieval when dealing with a multiscript
environment. So, the manifold applicability along with the challenges of the script iden-
tiÔ¨Åcation problem are now inspiring the researchers a lot. However, research in script
identiÔ¨Åcation is still in its early stage, and, hence, not much literature is available in this
Ô¨Åeld.
The problem of script identiÔ¨Åcation is generally conceived on any of the following
three levels: (1) page level, (2) text line level, and (3) word level. Performing script recog-
nition at the word level is a much more challenging task than at the other two, higher
levels. The reason for this is the information gathered from a few characters present in a
word image sometimes may not be enough to identify the script truthfully. In addition
to this, recognition of scripts at word level also facilitates automation of grouping words
belonging to a speciÔ¨Åc script/language and also separates interlaced words pertaining
to other scripts. In contrast, the computational complexity of feature extraction at page
level and text line level can be sometimes too laborious and time-consuming. Therefore,
script identiÔ¨Åcation at word level is always an apparent choice.
The remainder of the chapter is organized as follows: Section 3.2 presents a brief
review of some of the previous approaches to handwritten script identiÔ¨Åcation, and
some basic information related to the scripts used in the present chapter is described
in Section 3.3. The proposed methodology is presented in Section 3.4. Experimental
results and their analyses are given in Section 3.5. Section 3.6 concludes the work and
lists some future directions.
3.2
Review of Related Work
Script identiÔ¨Åcation depends on the fact that the character set of each script has
unique spatial distribution and visual attributes that make it possible to distinguish
it from other scripts. So, the primary task included in recognition of a script is to
formulate a technique to determine these features or attributes from a given document
and then classify the document‚Äôs script accordingly. The reported works on script
identiÔ¨Åcation have acknowledged multiple approaches and features, which may be
grouped into two broad categories ‚Äì (1) structure-based methods and (2) visual

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
49
appearance-based methods. Abundant work [1‚Äì13] has been reported in literature by
means of structure-based methods. A.L. Spitz [1] proposed a technique to distinguish
Han-based script (Chinese, Korean, and Japanese) from script based on Latin (German,
Russian, English, and French) by examining the upward concavities of the connected
components. A two-stage classiÔ¨Åer is also designed for the classiÔ¨Åcation purpose.
Initially, the separation between Han-based scripts and Latin-based scripts was done
by computing variances of their upward concavity distributions. Furthermore, the
analysis of the distribution of optical density in the script image helped to classify
within Han-based scripts. A projection proÔ¨Åle technique to classify Korean, Chinese,
Roman, Arabic, and Russian script characters was proposed by S. Wood et al. in [2].
The reason for applying this technique was that the projection proÔ¨Åles of document
images were suÔ¨Écient to identify diÔ¨Äerent scripts. However, no suggestion was
provided regarding how these projection proÔ¨Åles can be examined automatically for
script recognition without human intervention, and also no recognition accuracy
was claimed to substantiate their argument. J. Hochberg et al. [3] described a script
identiÔ¨Åcation system for recognizing six diÔ¨Äerent scripts, namely, Arabic, Cyrillic,
Chinese, Devanagari, Japanese, and Roman. A Ô¨Åve-element feature set (i.e., aspect
ratio, number of holes, relative horizontal centroid, relative vertical centroid, and
sphericity) was extracted from all the connected components assuming 8-connectivity.
A set of Fisher linear discriminants (FLDs), one FLD for each pair of script classes,
was utilized for categorization. The classiÔ¨Åcation of the document to their respective
script classes was Ô¨Ånally done based on where it is classiÔ¨Åed most often. An automatic
script identiÔ¨Åcation scheme from documents printed in Chinese, Roman, Arabic,
Devanagari, and Bangla scripts was designed by U. Pal et al. in [4]. The features were
extracted, combining statistical features and a water overÔ¨Çow analogy from a reservoir.
B.B. Chaudhuri et al. in [5] reported a dual procedure based on interdependency
between text-line and interline gap for recognition of six diÔ¨Äerent handwritten Indic
scripts, namely, Oriya, Hindi, English, Gujarati, Bangla, and Malayalam. A script
identiÔ¨Åcation model from a trilingual document printed in Devanagari, English, and
Telugu scripts was introduced by M.C. Padma et al. in [6]. The classiÔ¨Åcation was done
at the text line level using the concept of both top and bottom proÔ¨Åle-based features.
M. Hangarge et al. in [7] examined texture as a tool for determining the script of
handwritten document images of the three Indic scripts, namely, English, Devanagari,
and Urdu. The identiÔ¨Åcation of scripts was based on the observation that each script
text possesses a distinct visual texture. A set of 13 spatial spread features were extracted
using morphological Ô¨Ålters. S.K. Sangame et al. in [8] proposed a script identiÔ¨Åcation
model to classify text words written in English and Kannada scripts from a handwritten
bilingual document. A nine-element feature set (comprising a top-horizontal line, ver-
tical line, bottom-component, top-holes, bottom-holes, top-down curves, bottom-up
curves, right curve, and left curve) was estimated from both the scripts based on the
vertical projection proÔ¨Åles of the word images. K. Roy et al. in [9] described a method
for word-level identiÔ¨Åcation of handwritten Oriya and Roman scripts from Indian
postal documents by using the concept of a water reservoir analogy, as explained
in [4]. A method for wordwise handwritten script identiÔ¨Åcation for Indian postal
automation was also proposed in [10] by using the run length smoothing algorithm
(RLSA). A 25-element feature set based on fractal dimension, Matra/Shirorekha, a
water reservoir, and other topologies was extracted to identify handwritten Bangla

50
Hybrid Intelligence for Image Analysis and Understanding
and English script words. A system that separated the scripts of handwritten words
from a document, written in Bangla or Devanagari mixed with Roman scripts using
a multilayer perceptron (MLP) classiÔ¨Åer, was presented by R. Sarkar et al. in [11]. A
set of eight diÔ¨Äerent word-level holistic features was extracted from the word images.
An intelligent feature-based technique, which recognized the scripts of handwritten
words from a document page, written in Devanagari script mixed with Roman script
using a MLP classiÔ¨Åer, was reported by P.K. Singh et al. in [12]. A 39-element feature
set was designed, of which eight features were topological (as described in [11]) and the
remaining 31 were based on the convex hull of each word image. In an extended version
described in [13], P.K. Singh et al. introduced statistical signiÔ¨Åcance tests for evaluating
the performances of multiple classiÔ¨Åers with the set of designed features, as described
in [12], on numerous subsets of the data sets. A printed script identiÔ¨Åcation scheme for
Kashmiri, Roman, Devanagari, and Urdu document scripts at the word level was pre-
sented in [14]. The feature extraction was based on statistical features calculated using
density of four mentioned scripts, and the classiÔ¨Åcation was done using a tree classiÔ¨Åer.
S.M. Obaidullah et al. [15] proposed a two-stage approach for printed script identiÔ¨Å-
cation at the page level for six oÔ¨Écial languages of India, namely, Bangla, Devanagari,
Malayalam, Urdu, Oriya, and Roman scripts, using some abstract/mathematical,
structure-based, and script-dependent features. A. Saidani et al. in [16] introduced
structural features that were intrinsic features for word-level identiÔ¨Åcation of both
printed and handwritten Arabic and Latin scripts. In structure-based methods, scripts
were identiÔ¨Åed based on the features extracted from either text line [4‚Äì7] or word
[8‚Äì13, 16].
These above methods generally extracted features based on connected component
analysis for determining the script of the text. Unfortunately, the paradox inherent in
using such features was achieved only after Ô¨Åne segmentation the underlying docu-
ment image. Consequently, the accuracy of script identiÔ¨Åcation in turn is dependent
on the accuracy of the intermediate steps, namely, text line and word segmentation,
which are, in general, script dependent. But it is not easy to locate a common seg-
mentation process, as diÔ¨Äerent script classes show script-biased behavior. In addition
to this, the presence of noise, skewness or slant, nonuniform gap between interwords
(or intrawords), and other relevant degradations signiÔ¨Åcantly aÔ¨Äect the connected com-
ponent analysis procedure, thus making these approaches ineÔ¨Écient. Due to these lim-
itations, structure-based methods might not be a good choice to be a comprehensive
methodology.
On the other hand, Ô¨Åne segmentation of the original document image into its
corresponding text lines and words is not needed in case of visual appearance-based
methods. As a result, the visual appearance-based methods make the task of script
recognition much easier and less computationally expensive than the structure-based
methods. But the literature survey depicts that only a few works have been pro-
posed using visual appearance-based methods [17‚Äì19]. These methods utilize the
texture-based features. These features can also be extracted from a segment of a text
section of a script document image. A rotation-invariant texture feature extraction
method for automatic script identiÔ¨Åcation for six languages (viz., Chinese, Greek,
English, Russian, Persian, and Malayalam) was described by T.N. Tan in [17]. In the
initial stage, a uniform text block was produced from the input document image, and
texture features were then extracted from this text block using a 16-channel Gabor

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
51
Ô¨Ålter. In order to achieve invariance to rotation, Fourier coeÔ¨Écients for this set of
16 channel outputs were calculated. One drawback of this method was that the text
blocks extracted from the input documents did not necessarily have uniform character
spacing. A. Busch et al. in [18] investigated the application of texture as a device in order
to determine the document image script, based on the observation that diÔ¨Äerent script
text possesses a diÔ¨Äerent visual texture. Experimentation with a number of texture
features was evaluated on eight printed scripts, namely, Farsi, Chinese, Japanese,
Greek, Cyrillic, Hebrew, Sanskrit, and Latin. A method for automatic identiÔ¨Åcation of
scripts/languages from document images was proposed by G.S. Peake et al. in [19].
The feature extraction is done using multiple-channel Gabor Ô¨Ålters and gray-level
co-occurrence matrices for seven printed languages: Chinese, English, Greek, Korean,
Malayalam, Persian, and Russian. They extended work described in [1], where they
applied some preprocessing techniques to acquire homogeneous text blocks from the
input script document images. These consist of text-line localization, outsized text-line
elimination, padding, and spacing normalization. Documents were also skew-corrected
so that it was not necessary to generate rotation-invariant features. Script identiÔ¨Åcation
was then performed by using KNN (k-nearest neighbor) classiÔ¨Åer. One of the critical
problems encountered in using Gabor Ô¨Ålters was the large computational cost due to
the repeated Ô¨Åltering of input images.
Prevailing techniques for Indic script identiÔ¨Åcation use the texture features such
as wavelet-based co-occurrence histograms [20], Gabor Ô¨Ålters [21, 22], wavelet
packet-based features [23], histogram of oriented gradients (HOG) [24], and a
combination of discrete cosine transform (DCT), moment invariants [25], among
others. However, visual appearance-based methods are more suitable for a widespread
approach to the problem of script identiÔ¨Åcation. But hardly any researcher showed
attention toward word-level handwritten script identiÔ¨Åcation from Indic documents
in the literature. This motivates us to propose a handwritten word-level script iden-
tiÔ¨Åcation technique based on texture features. The experiment is conducted on eight
popular oÔ¨Écial scripts used in India, namely, Bangla, Devanagari, Gurumukhi, Oriya,
Malayalam, Telugu, Urdu, and Roman.
3.3
Properties of Scripts Used in the Present Work
India is a multilingual country where 23 constitutionally recognized languages are
written using 12 major scripts. The oÔ¨Écially recognized languages [26] are as follows:
Assamese, Bengali, Bodo, Gujarati, Hindi, Marathi, Nepali, Oriya, Sindhi, Sanskrit,
Punjabi, Tamil, Telugu, Kannada, Malayalam, Kashmiri, Manipuri, Konkani, Maithali,
Santhali, Dogari, Urdu, and English. The 12 major modern scripts currently being used
are: Bangla, Devanagari, Kannada, Gurumukhi, Gujarati, Telugu, Tamil, Malayalam,
Manipuri, Oriya, Urdu, and Roman. Of these, the Urdu and Roman scripts are derived
from the Persian and Latin scripts, respectively. The early Brahmi script (300 BC) is
the mother of the Ô¨Årst 10 scripts, which are also referred to as Indic scripts. Table 3.1
illustrates some vital information about eight scripts used for the present work,
whereas Figure 3.1 shows a portion of handwritten document pages written in eight
scripts.

52
Hybrid Intelligence for Image Analysis and Understanding
Table 3.1 Important information related to scripts [26] used in the present work
Scripts
Origin
Basic character set
Writing
style
Number
of native
speakers
(millions)
in india
Used to write
languages
Vowels
Consonants
Bangla
11
39
Left to right
207
Bengali, Assamese,
Manipuri, etc.
Devanagari
15
33
366
Hindi, Nepali, Marathi,
Konkani, Sindhi,
Sanskrit, etc.
Gurumukhi
Brahmi
12
30
57.1
Punjabi, Sanskrit,
Sindhi, Braj Bhasha,
Khariboli, etc.
Oriya
14
38
32.3
Oriya, Sanskrit, etc.
Malayalam
13
36
35.71
Malayalam.
Telugu
16
37
69.7
Telugu.
Roman
Latin
5
21
341
German, English,
Spanish, Italian, etc.
Urdu
Persian
10
28
Right to left
60.3
Urdu, Bati, Burushaski,
etc.
3.4
Proposed Work
The literature review of the script identiÔ¨Åcation work has already revealed promising
outcomes in recognizing the restricted number of script types in idyllic conditions. In
reality, there are a number of disadvantages still prevalent in the preceding techniques,
leading to diÔ¨Éculties in the classiÔ¨Åcation of some scripts. For example, the computation
of convex hull features from the word images by P.K. Singh et al. as described in [13]
is exceedingly prone to noise and dependent on the quality of the input word image, as
high variances can be observed if the word images possess these attributes. One of the
main drawbacks of the technique proposed by A.L. Spitz [1] is that eÔ¨Äective discrim-
ination between scripts having similar character shapes, such as between Latin-based
scripts and Han-based scripts, is not achieved even though such scripts can be easily
visually classiÔ¨Åed by any common observers. Since texture analysis provides a global
measure of the properties of a region, they are considered as reasonable alternatives for
Ô¨Ånding the solution to script identiÔ¨Åcation problems without requiring analysis of each
component of the script classes. Texture is a key component of human visual perception,
and texture descriptors are used to measure the content of texture present in an image.
Since an image is made up of pixels, texture can also be deÔ¨Åned as an entity consisting
of mutually related pixels and group of pixels. This group of pixels is called the tex-
ture primitive or texture element (texel) [27]. The main objective of script identiÔ¨Åcation
problems is to discover a connected set of pixels that satisfy a given gray-level property
and that occur repetitively in a particular script constituting a textured region. Texture
characteristics present in each of these regions are unique in nature. As a result, a usual
way of recognizing the script document image may be based on its visual appearance

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
53
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 3.1 Segments of sample document pages written in: (a) Bangla, (b) Devanagari, (c) Gurumukhi,
(d) Oriya, (e) Malayalam, (f) Telugu, (g) Urdu, and (h) Roman scripts, respectively.
as observed at a glimpse by a casual witness. This does not require the investigation of
the individual character patterns present in the script document. The algorithms based
on visual appearance extract discriminatory texture features from each script region in
order to facilitate classiÔ¨Åcations of the input script patterns. This is the motive behind
using texture-based features in our present work. A two-stage technique for the said
purpose has been used, which is discussed in detail in the next subsection.
3.4.1
Discrete Wavelet Transform
A number of texture features has been reported in the literature. Features extracted from
the Fourier transform perform poorly in practice, due to its lack of spatial localization.
Gabor Ô¨Ålters provide a better way for spatial localization; nevertheless, their eÔ¨Äective-
ness is restricted in reality for the reason that one cannot localize a spatial formation in
natural textures for a single Ô¨Ålter resolution. In comparison to the Gabor transform, the
features based on wavelet transform possess a few advantages:
‚Ä¢ Representation of textures at the most appropriate scale can be performed by varying
the spatial resolution.

54
Hybrid Intelligence for Image Analysis and Understanding
‚Ä¢ As there is a variety of wavelet functions available in the literature, so the selection of
the best suited wavelets for the script identiÔ¨Åcation problem can be done accordingly
based on texture analysis of the given scripts to be identiÔ¨Åed.
Discrete wavelet transform (DWT) is basically a linear transformation that works on
a data vector having length of an integer power of two. This data vector is changed into a
numerically diÔ¨Äerent vector of the same length. It is a tool that divides data into diÔ¨Äerent
frequency components, and then each component is studied with resolution matched
to its scale [27]. For an image f (x), the DWT can be deÔ¨Åned as follows:
f (x) =
‚àë
ùìÅ‚ààZ
c(ùìÅ)ùúë
ùìÅ(x) +
‚àû
‚àë
j=0
n
‚àë
k‚ààZ
d(j, k)Wj,k(x)
(3.1)
where c(ùìÅ) is called the wavelet coeÔ¨Écient; and the term d(j, k) is called the expansion
coeÔ¨Écient, which represents detailed information of the original image. The wavelet
series converts a continuous function as a set of expansion coeÔ¨Écients. ùúë
ùìÅ(x) is called the
scaling function. In the wavelet domain, this function is deÔ¨Åned in terms of a set of basis
functions that are Ô¨Ånite over a range of frequencies and locations (i.e., frequency and
spatially localized functions). This means that each coeÔ¨Écient provides some limited
information about the position and frequency of the function. There are many basis
functions that can satisfy the above requirement. One such basis function is given as
follows:
ùúôsùìÅ(x) = 2‚àís‚àï2ùúô(2sx ‚àíùìÅ)
(3.2)
where ùúôis called the mother wavelet, whose magnitude is given by 2‚àís‚àï2; the term s deter-
mines the scale of the wavelet and is also known as the width of the wavelet; whereas the
term ùìÅdetermines the position of the wavelet. Once a basis function is obtained, one
can derive the set of basis functions by applying translation and dilation. Dilation is used
to rescale the mother wavelet to any power of an integer, where the power of stretching
or compressing is called scaling. By translation, it changes the oÔ¨Äset of the wavelet for
delaying it or hastening it.
The resolutions of the mother wavelet can be changed to yield a scaling function that
is given as:
W(x) =
N‚àí2
‚àë
k=‚àí1
(‚àí1)kCk+1ùúô(2x + k)
(3.3)
where Ck is called the wavelet coeÔ¨Écient. The necessary conditions for a wavelet coeÔ¨É-
cient are the following:
1. The sum of odd- and even-numbered coeÔ¨Écients should be one.
2. The product of adjacent odd or even coeÔ¨Écients should be zero.
3. The sum of the products of the coeÔ¨Écients and their complex conjugates should be
two.
The values of the wavelet coeÔ¨Écients can be obtained using the formula:
c(ùìÅ) = ‚à´f (x)ùúë
ùìÅ(x) dx
(3.4)

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
55
Similarly, the values of the expansion coeÔ¨Écients can be calculated as:
d(j, k) = ‚à´f (x)Wj,k(x) dx
(3.5)
3.4.1.1
Haar Wavelet Transform
Haar used a square pulse as a wavelet to approximate a function [27]. The basis vec-
tors should be orthogonal. These orthogonal basis functions are scale-varying wavelets.
Mathematically, Haar transform is deÔ¨Åned as:
ùúôj
i = ùúô(2jx ‚àíi) where, i = 0, 1, 2, 3, ‚Ä¶ , 2j ‚àí1
(3.6)
The mother wavelet function ùúô(x) of the Haar wavelet is deÔ¨Åned as:
ùúô(x) =
‚éß
‚é™
‚é®
‚é™‚é©
1
for 0 ‚â§x ‚â§1‚àï2
‚àí1
for 1‚àï2 ‚â§x ‚â§1
0
otherwise
(3.7)
The implementation of a wavelet is also possible by using a set of Ô¨Ånite impulse
response Ô¨Ålters such as low-pass and high-pass Ô¨Ålters, which are generally used in
pairs. Wavelet transforms are implemented by quadrature mirror Ô¨Ålters. There are two
kinds of Ô¨Ålters: g (high-pass) and h (low-pass). A pair of such Ô¨Ålters is used to divide
the frequency into subbands. This procedure is repeated recursively till the lowest
frequency band of the given input image is reached. Since the application of two Ô¨Ålters
to an image increases its size, down-sampling is done to make sure that the size of the
image does not increase, thus ensuring that the result is of the same size as the original
image. Here, LL represents the approximation coeÔ¨Écients and can be subjected to the
next level of decomposition. HH, HL, and LH represent the detail coeÔ¨Écients. This is
called one-level decomposition. Similarly, the decomposition can be continued till it
is practically possible. The representation of the levels of the image is called a wavelet
decomposition tree. A two-level decomposition of a wavelet transformation is shown
in Figure 3.2. The DWT is performed Ô¨Årst for all rows and then for all columns of an
image. Down-sampling is represented by the symbol ‚Üì2. In the proposed work, the g
and h Ô¨Ålters for a Haar wavelet are expressed as follows:
g =
1
‚àö
2
[1, 1]
(3.8)
g (Low-
pass FILTER)
Rows
Columns
2D Image
h (High-
pass FILTER)
2
2
2
h
g
HH
HL
2
2
2
h
g
LH
LL
Figure 3.2 Wavelet decomposition tree for a 2D image using both low-pass and high-pass Ô¨Ålters.

56
Hybrid Intelligence for Image Analysis and Understanding
h =
1
‚àö
2
[1, ‚àí1]
(3.9)
The proposed algorithm for performing 2D wavelets is as follows:
1. Read the given image f (x, y).
2. Apply g to f (x, y), and down-sample the result by 2 along x. This gives the image T1.
3. Apply h to f (x, y), and down-sample the result by 2 along x. This gives the image T2.
4. Apply g to T1, and down-sample the result by 2 along x. This gives the image LL.
5. Apply h to f (x, y), and down-sample the result by 2 along x. This gives the image LH.
6. Apply g to T2, and down-sample the result by 2 along x. This gives the image HL.
7. Apply h to T2, and down-sample the result by 2 along x. This gives the image HH.
8. Display the image.
9. Exit.
The application of the Haar wavelet to a sample word image written in Bangla script is
shown in Figure 3.3. Depending on the requirement, we can discard any of the high-
or low-frequency components. It is generally seen that the low-frequency component
is the smoother representation of the original image, whereas the high-frequency
component represents a sharp transition like edge, line, and so on. So, wavelet analysis
can also be useful for edge detection purposes. Unlike the DCT, the wavelet transform
is not Fourier-based, and, therefore, wavelets perform a better job of handling discon-
tinuities in data. Here, we have used discrete Haar wavelet transform. The Haar wavelet
is computed by calculating the sums and diÔ¨Äerences of adjacent elements in a given
image. The 2D Haar wavelet transforms data into four bands: LL, HL, LH, and HH. It
can decompose the original image into diÔ¨Äerent components in the frequency domain,
that is the average component (LL) and three detail components (LH, HL, and HH), as
shown in Figure 3.3. In order to preserve the most representative information and to
avoid unnecessary computations by performing diÔ¨Äerent image-smoothing operations,
only the resultant LL component of the word image has been taken into account for
Grayscale word
image of size
(114 √ó 320)
HL Component
HH Component
LH Component
Approximate image (LL
Component) of size
(57 √ó 160)
Figure 3.3 Pictorial description of the LL, HL, LH, and HH components after applying Haar wavelet
transform to the original grayscale word image written in Bangla script.

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
57
the proposed work. It may also be noted that the LL component of the word image is
smaller than the original word image (due to down-sampling). For handwritten word
images, smoothing is an important task as data may be noisy. So, the preprocessing
task, which is generally an important phase in any image-processing domain, is avoided
by performing Haar wavelet transform.
3.4.2
Radon Transform (RT)
Radon transform (RT) for 2D function is named after Johann Radon. RT was Ô¨Årst
invented in 1917 by Radon, who also implemented this transform for 2D space [28].
Later, it was also generalized for higher dimensional space. RT is generally applied in
tomography, where an image is created with the help of projection data related with
cross-sectional scans of an object. Let an unknown density correspond to a function
f , and the output of a tomographic scan is achieved in the form of projection data,
which is represented by the RT. Hence, the original density can be reconstructed from
the projection data through the inverse of the RT. Thus, it leads to the mathematical
foundation for tomographic reconstruction, which is also known as image reconstruc-
tion. Since the RT of a Dirac delta function is a distribution supported on the graph
of a sine wave, so the RT data is sometimes also referred to as a sinogram. As a result,
the RT of a number of small objects can be expressed graphically as a number of
blurred sine waves having diÔ¨Äerent amplitudes and phases. The RT is useful in barcode
scanners, computed axial tomography (CAT) scans, reÔ¨Çection seismology, and electron
microscopy of macromolecular assemblies like viruses and protein complexes, and in
searching for solutions of hyperbolic partial diÔ¨Äerential equations [29]. It is mainly used
for line detection applications in computer vision, image processing, and seismology.
It can extract line parameters from a 2D image that has embedded lines. If an image
contains lines, then for each line, the RT creates a peak positioned at the corresponding
line parameters. This property of the RT can detect a line present at any angle. Although
this transform is mainly used to detect lines, generalized RT was proposed for the
detection of shapes, which include arbitrary curves such as circles, hyperbola, and
so on. The authors in [30] used RT for skew angle detection in Bangla script. In our
present work, features are extracted from the resultant word images, attained after the
application of Haar wavelet transform on the original word images. Here, in the second
stage, only the LL component of the word images undergoes RT.
The RT is computed as the projections of an image matrix taken along speciÔ¨Åed direc-
tions. A projection of a 2D function f (x, y) is deÔ¨Åned as a set of line integrals. The Radon
function calculates the line integrals from multiple sources along parallel paths or rays
originating from a source in a certain direction. The beams are spaced 1 pixel unit apart.
The source and sensor have a rotation angle ùúÉabout the center of the image. Now, this
process is repeated for a given set of angles, usually ùúÉ‚àà[0‚àò, 180‚àò). It is obvious that
angle 180‚àòand angle 0‚àòhave the same result. Each ray has a distance x‚Ä≤ from the center
of the image for a particular angle. The x‚Ä≤-axis and each ray are perpendicular to each
other [28].
To represent an image, the Radon function takes multiple, parallel-beam projections
of the image from diÔ¨Äerent angles by rotating the source of the parallel beam around the
center of the image. Figure 3.4 shows a single projection at a speciÔ¨Åed rotation angle on
a handwritten Bangla word image.

58
Hybrid Intelligence for Image Analysis and Understanding
Sensor
Source
Image f(x, y)
Rotation ANGLE Œ∏
œÅ‚Äìaxis
Figure 3.4 Illustration of a single projection at a speciÔ¨Åed rotation angle ùúÉon a handwritten Bangla
word image.
RT can also be deÔ¨Åned as the projection of the image intensity along a radial line at a
speciÔ¨Åc angle. The radial coordinates are the values along the x‚Ä≤-axis, which is oriented
at ùúÉ‚àòcounterclockwise from the x-axis. Both the axes originate from the center of the
image. For example, the line integral of f (x, y) in the vertical direction is the projection of
f (x, y) onto the x-axis, and the line integral in the horizontal direction is the projection
of f (x, y) onto the y-axis. Figure 3.5 shows the horizontal and vertical projections for a
simple 2D function.
y
x
f(x,y)
Projection onto the x-AXIS
Projection onto the y-AXIS
Figure 3.5 Horizontal and vertical projections of a word image written in Bangla script.

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
59
Projection can be computed along any angle ùúÉby using the general equation [29] of
the RT, as given here:
‚Ñú(x‚Ä≤, ùúÉ) = ‚à´
‚àû
‚àí‚àû‚à´
‚àû
‚àí‚àû
f (x, y)
ùõø(x cos ùúÉ+ y sin ùúÉ‚àíx‚Ä≤)
dx dy
(3.10)
where x‚Ä≤ = x cos ùúÉ+ y sin ùúÉ, ùúÉis the angle of incidence of the beams; and ùõøis a dirac
delta function (i.e., ùõø(b) = 1 if b = 0 and otherwise ùõø(b) = 0). Figure 3.6 illustrates the
geometry of the RT.
RT has very interesting properties [31, 32] relating to the application of aÔ¨Éne trans-
formations. RT of any translated, rotated, or scaled image can be computed by knowing
the transformation of the original image and the parameters of the aÔ¨Éne transformation
applied to it. RT of handwritten word images written in eight handwritten scripts ‚Äì
Bangla, Devanagari, Gurumukhi, Oriya, Malayalam, Telugu, Urdu, and Roman ‚Äì are
shown in Figure 3.7.
The output of the RT is the radon-transformed matrix where each column represents
the RT at a particular angle ùúÉ. It is obvious that the sum of values of each column of
the radon-transformed matrix will be the same. For simplicity, the word images are
rotated at 12 diÔ¨Äerent orientations (ùúÉ= 0‚àò, 15‚àò, 30‚àò, 45‚àò, 60‚àò, 75‚àò, ‚Ä¶ , 165‚àò), and some
informative statistical measures have been estimated at each of these orientations. It
has been observed that the values present in the‚àòcolumn (i.e., for each rotation angle)
in the radon-transformed matrix are not symmetrically distributed. So, for each column,
column-wise skewness is computed that is taken as feature value. In general, skew-
ness is the measure of symmetry. It is used because column-wise data distribution is
oÔ¨Ä-centered. An example of symmetrical distribution is normal distribution that has no
skewness. Skewness mainly occurs due to the existence of extremely large and small val-
ues in a data set. If the tail of the distribution points toward the positive x-axis, then it is
x¬¥
y¬¥
x¬¥
x
y
x¬¥-AXIS
Rotation ANGLE Œ∏
R(x¬¥,135¬∞)
R(x¬¥ 45¬∞)
Figure 3.6 Pictorial description of the geometry of the radon transformation.

60
Hybrid Intelligence for Image Analysis and Understanding
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 3.7 Illustration of RT of the word images written in (a) Bangla, (b) Devanagari, (c) Gurumukhi,
(d) Oriya, (e) Malayalam, (f) Telugu, (g) Urdu, and (h) Roman scripts, respectively.
called positively skewed distribution. Here, unlike with the normal distribution, mean,
median, and mode are scattered. For positively skewed distribution, median is on the
right side to the mode and mean is on the right side to the median. Similarly, for nega-
tively skewed distribution, the tail of the distribution points toward the negative x-axis.
For more illustration, refer to Figure 3.8.
Skewness of a distribution is measured using the coeÔ¨Écient of skewness. Four
well-known coeÔ¨Écients of the skewness are:
1. Pearson coeÔ¨Écient of skewness: It is also known as the Pearson mode skewness, which
is deÔ¨Åned as:
Skp = (mean ‚àímode)
ùúé
= 3(mean ‚àímedian)
ùúé
(3.11)
where ùúéis known as the standard deviation.
As from the above discussion, it is understood that for positively skewed distribu-
tion, mean is greater than median and standard deviation is always a positive term;
mean = median = mode
(a)
(b)
(c)
mode median mean
mode
median
mean
Figure 3.8 Graph for: (a) normal distribution, (b) positively skewed distribution, and (c) negatively
skewed distribution.

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
61
thus, this coeÔ¨Écient (i.e., Skp) is greater than zero for positively skewed distribution.
Similarly, the coeÔ¨Écient is less than zero for negatively skewed distribution and zero
for symmetric distribution.
2. Quartile coeÔ¨Écient of skewness: In this method, the distribution is divided into four
parts, where Q1, Q2, and Q3 are the 1st, 2nd, and 3rd quartiles, respectively. Q2 is also
the median of the distribution, as it is known that median divides the distribution
into two equal halves. Quartile coeÔ¨Écient of skewness is deÔ¨Åned as:
Skq = (Q3 ‚àímedian) ‚àí(median ‚àíQ1)
(Q3 ‚àíQ1)
(3.12)
For positively skewed distribution, (Q3 ‚àímedian) is greater than (median ‚àíQ1);
thus, this coeÔ¨Écient (i.e., Skq) is greater than zero for positively skewed distribution.
Similarly, this coeÔ¨Écient is less than zero for negatively skewed distribution, and
coeÔ¨Écient is zero for symmetric distribution.
3. Percentile coeÔ¨Écient of skewness: In this method, the distribution is divided into 100
parts, where P10 is the 10thpercentile and P90 is the 90th percentile. Percentile coeÔ¨É-
cient of skewness is deÔ¨Åned as:
Skpp = (P90 ‚àímedian) ‚àí(median ‚àíP10)
(P90 ‚àíP10)
(3.13)
For positively skewed distribution, (P90 ‚àímedian) is greater than (median ‚àíP10);
thus, this coeÔ¨Écient (i.e., Skpp) is greater than zero for positively skewed distribu-
tion. Similarly, this coeÔ¨Écient is less than zero for negatively skewed distribution,
and coeÔ¨Écient is zero for symmetric distribution.
4. Moment coeÔ¨Écient of skewness: The moment coeÔ¨Écient of skewness of a data set is
deÔ¨Åned as:
g1 =
m3
m23‚àï2
(3.14)
where
m3 =
n
‚àë
i=1
(x ‚àíx)3‚àïn
(3.15)
and
m2 =
n
‚àë
i=1
(x ‚àíx)2‚àïn
(3.16)
and where x is the mean, n is the sample size, m3 is the third moment, and m2 is the
variance of the data set.
For positively skewed distribution, this coeÔ¨Écient is greater than zero. Similarly, this
coeÔ¨Écient is less than zero for negatively skewed distribution and zero for symmetric
distribution.
We have used the moment coeÔ¨Écient of skewness in the selected columns of the
radon-transformed matrix. Next, the peakedness of the distribution has been consid-
ered by calculating kurtosis. So, for each column (i.e., for each rotation angle) in the
radon-transformed matrix, row-wise kurtosis has been computed and taken as a feature
value. Basically, kurtosis measures the way that values are bundled across the center

62
Hybrid Intelligence for Image Analysis and Understanding
of the distribution. According to the value of kurtosis coeÔ¨Écient, the distribution can
have three types of shape as shown in Figure 3.9. A distribution that is less peaked than
normal is called platykurtic. If a distribution is identical to the normal distribution,
then it is called mesokurtic. If a distribution is more peaked than normal, then it is
called leptokurtic. Two well-known coeÔ¨Écients of kurtosis are as follows:
1. Percentile coeÔ¨Écient of kurtosis: This measure of kurtosis is based on both quartiles
and percentiles and is deÔ¨Åned as:
k =
Q
P90 ‚àíP10
(3.17)
where Q = 1
2(Q3 ‚àíQ1) is the semi-interquartile range. If k = 0.263, then distribution
is said to be normal or mesokurtic. If k < 0.263 and k > 0.263, then distribution is
said to be platykurtic and leptourtic, respectively.
2. Moment coeÔ¨Écient of kurtosis: This measure of kurtosis uses the fourth moment
about the mean and is expressed in dimensionless form as:
a4 = m4
ùúé4 = m4
m22
(3.18)
where m4, the fourth moment about the mean, is deÔ¨Åned as:
and
m4 =
n
‚àë
i=1
(x ‚àíx)4‚àïn
(3.19)
If the value of a4 is less than or greater than 3, then the distribution is known as
platykurtic or leptourtic. If the value of a is equal to 3, then the distribution is known
as mesokurtic.
For the present work, the moment coeÔ¨Écient of kurtosis is chosen to measure the kur-
tosis in selected columns of the radon-transformed matrix. A binarized word image
(foreground pixel taken as 0, and background pixel taken as 1) is inverted, and radon
transformation is performed in the same way with a 15‚àòinterval gap. This is done inten-
tionally, as radon transform for an image f (x, y) at a speciÔ¨Åed rotation angle ùúÉis a pro-
jection that computes the line integral. So, this inversion should be reÔ¨Çected in a radon
transform matrix. As in this case, foreground pixels are set to 0 so background pixels
are contributing, while calculating the line integral in radon transform and column-wise
standard deviation is computed at 12 orientations. Standard deviation is the most com-
monly used measure of dispersion, which is a measure of spread of data about the mean.
Here, skewness and kurtosis calculations are not included because, considering only
background pixels (which are large in number compared to foreground pixels), all the
line integrals along the parallel beam sum to a large number that makes the calculation
of skewness less important. Similarly, most results show that column-wise data distri-
bution is platykurtic in nature, so the calculation of column-wise kurtosis is excluded in
the present work. The pictorial representation of the feature extraction procedure from
the RT is also shown in Figure 3.10. The proposed algorithm for the present work is
discussed here:
‚Ä¢ Step 1: Convert the RGB word image into a grayscale image.
‚Ä¢ Step 2: Perform a discrete Haar wavelet transform, and select the upper left zone (LL)
component.

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
63
Figure 3.9 Illustration of (a) mesokurtic, (b)
leptourtic, and (c) platykurtic curves.
(a)
(b)
(c)
Binarization
Invert the binarized image
Perform
Radon
transform
Perform
Radon
transform
Calculate statistical features
along 12 orientations
(F37‚ÄìF48)
Calculate statistical
features along 12
orientations
(F1‚ÄìF36)
LL component
Figure 3.10 Pictorial representation of the two-stage approach of the proposed script identiÔ¨Åcation
technique.
‚Ä¢ Step 3: Binarize the LL component using Otsu‚Äôs global thresholding approach.
‚Ä¢ Step 4: Apply radon transform to the binarized LL component for angle 0‚àòto 180‚àò.
Output of this step is called a radon-transformed matrix.
‚Ä¢ Step 5: Extract features from this matrix by calculating column-wise standard devia-
tion, kurtosis, and skewness along 12 diÔ¨Äerent orientations (ùúÉ= 0‚àò, 15‚àò, 30‚àò, 45‚àò, 60‚àò,
75‚àò,‚Ä¶, 165‚àò).
‚Ä¢ Step 6: Perform Step 3 by considering the foreground pixel as 0 and background as 1.
‚Ä¢ Step 7: For this radon-transformed matrix, compute column-wise standard deviation
along 12 orientations, as in Step 5.
‚Ä¢ Step 8: Exit.
3.5
Experimental Results and Discussion
For the evaluation of the current script identiÔ¨Åcation technique, a data set of a total of
16,000 handwritten words has been collected. Here, each of the eight scripts (Bangla,
Devanagari, Gurumukhi, Oriya, Malayalam, Telugu, Urdu, and Roman) contains
about 2000 words. 100 diÔ¨Äerent native writers belonging to diÔ¨Äerent age groups and
educational levels assisted us in creating the database. The writers were asked to use
black or blue ink pens for writing 50 to 100 words inside an A4-sized sheet. The

64
Hybrid Intelligence for Image Analysis and Understanding
materials of the contents were collected from diÔ¨Äerent types of sources (viz., class notes
of students of diÔ¨Äerent age groups, articles of a popular daily newspaper, etc.). This is
done to ensure that the probability of getting dissimilar words becomes higher. A few
word samples written in the above-mentioned eight scripts from our data set are shown
in Figure 3.11. The handwritten sheets are then scanned using a Ô¨Çatbed scanner with
300 dpi grayscale image resolution. The original gray-tone word images are converted
into two-tone images (0 and 1) by Otsu‚Äôs global thresholding approach [33], where
the label 1 represents the object and 0 represents the background. A total of 9600
words (1200 words per script) have been used for the training purpose, whereas the
remaining 6400 words (800 words per script) have been used for the testing purpose.
To realize the eÔ¨Äectiveness of the proposed approach, our comprehensive experimental
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 3.11 Sample word images of eight handwritten scripts from our database written in:
(a) Bangla, (b) Devanagari, (c) Gurumukhi, (d) Oriya, (e) Malayalam, (f) Telugu, (g) Urdu, and (h) Roman,
respectively.

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
65
tests, which are discussed in the subsequent subsections, can be summarized as
follows:
1. The proposed approach is tested using multiple classiÔ¨Åers on multiple data sets.
These data sets are randomly selected from the test data sets, and, based on the
consequences of the statistical signiÔ¨Åcance tests, the best classiÔ¨Åer for the approach
is chosen.
2. The statistical performance analysis of the best classiÔ¨Åer with respect to diÔ¨Äerent
parameters is evaluated.
3. The individual performance of DWT and Radon transform is observed and compared
with the two-stage approach. Also, a comparison of the current approach is done with
some other conventional methods previously proposed.
4. Finally, the reasons for misclassiÔ¨Åcation of some of the word images are brieÔ¨Çy ana-
lyzed.
3.5.1
Evaluation of the Present Technique
The designed feature set has been individually applied to seven well-known classiÔ¨Åers
namely, naive Bayes, Bayes net, MLP, support vector machine (SVM), random forest,
bagging, and multiclass classiÔ¨Åer. The script identiÔ¨Åcation performances of the present
technique on each of these classiÔ¨Åers, and their corresponding success rates achieved at
a 95% conÔ¨Ådence level, are shown in Table 3.2. The model building times required for
the above-mentioned classiÔ¨Åers are shown in Figure 3.12. All the experiments are imple-
mented in MATLAB 2010 under a Windows XP environment on an Intel Core 2 Duo
2.4 GHz processor with 2 GB of RAM. The accuracy rate, used as an assessment crite-
rion for measuring the recognition performance of the proposed system, is expressed as
follows:
Accuracy rate(%) = No. of correctly classiÔ¨Åed text words
Total no. of text words
X 100(%)
(3.20)
It can be observed from Table 3.2 that the highest recognition accuracy of the proposed
script identiÔ¨Åcation system is found to be 97.69% using SVM classiÔ¨Åer. Since that SVM
classiÔ¨Åer is found to be suitable for the current problem, a brief discussion about this
classiÔ¨Åer will be noteworthy.
SVM classiÔ¨Åer eÔ¨Äectively maps pattern vectors to a high-dimensional feature space
where a ‚Äúbest‚Äù separating hyperplane (the maximal margin hyperplane) is constructed.
To construct an optimal hyperplane, SVM employs an iterative training algorithm that
minimizes an error function, which can be deÔ¨Åned as follows:
Table 3.2 Success rates of the proposed script identiÔ¨Åcation technique using seven
well-known classiÔ¨Åers
ClassiÔ¨Åers
Naive
bayes
Bayes
net
MLP
SVM
Random
forest
Bagging
Multiclass
classiÔ¨Åer
Success rate (%)
87.62
85.07
95.1
97.69
90.82
89.79
91.55
95% conÔ¨Ådence
score (%)
91.88
89.49
96.39
99.65
93.94
90.3
95.23

66
Hybrid Intelligence for Image Analysis and Understanding
2.05
5.14
50.91
10.19
40.55
38.75
273.33
Naive
Bayes
0
50
100
150
200
250
300
BayesNet
SVM
Classifiers
Comparison of Model Building Time by Different Classifiers
Model Building Time
Random
Forest
Bagging
Multiclass
Classifier
MLP
Figure 3.12 Graphical comparison of model building time (seconds) required by seven diÔ¨Äerent
classiÔ¨Åers.
Find WùúñIRm and bùúñIR to minimize 1‚àï2 W TW + C ‚àëN
i=1 ùúâi subject to constraints
yi(W Tùúô(xi) + b) ‚â•1 ‚àíùúâi and ùúâi ‚â•0 (where i = 1, 2, ‚Ä¶ , N), where C is the capacity
constant; W is the vector of coeÔ¨Écients; b is a constant; and ùúâi represents parameters
for handling nonseparable data (inputs). The index i labels the N training cases. Note
that yùúñ¬± 1 represents the class labels, and xi represents the independent variables. The
kernel ùúôis used to transform data from the input (independent) to the feature space. It
should be noted that the larger the C, the more the error is penalized. Thus, C should be
chosen with care to avoid overÔ¨Åtting. A number of kernels can be used in SVM models.
These include linear, polynomial, radial basis function (RBF), and sigmoid kernels. For
the present work, we have applied a polynomial kernel, which can be written as:
K(Xi, Xj) = (ùõæXi.Xj + C)d
(3.21)
where K(Xi, Xj) = ùúô(Xi) ‚ãÖùúô(Xj) is the kernel function that represents a dot product of
input data points mapped into the higher dimensional feature space by transforma-
tion ùúô.
3.5.1.1
Statistical SigniÔ¨Åcance Tests
The statistical signiÔ¨Åcance of the experimental setup has also been measured as an essen-
tial part for validating the performance of the multiple classiÔ¨Åers using multiple data
sets. To do so, we have performed a safe and robust nonparametric Friedman test [13]
with the corresponding post-hoc tests. For the experimentation, the number of data sets
(N) and the number of classiÔ¨Åers (k) are set as 12 and 7, respectively. These data sets are
chosen randomly from the test set. The performances of the classiÔ¨Åers on diÔ¨Äerent data
sets are shown in Table 3.3. On the basis of these performances, the classiÔ¨Åers are then
ranked for each data set separately; the best performing algorithm gets the rank 1, the
second best gets rank 2, and so on (see Table 3.3). In case of ties, average ranks are
assigned.

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
67
Table 3.3 Recognition accuracies of seven classiÔ¨Åers and their corresponding ranks on 12 diÔ¨Äerent
data sets (ranks in parentheses are used for performing the Friedman test)
ClassiÔ¨Åers
Data sets
Naive
bayes (%)
Bayes
net (%)
MLP (%)
SVM (%)
Random
forest (%)
Bagging (%)
Multiclass
classiÔ¨Åer %
1
90(5)
87(7)
95(2)
98(1)
88(6)
91(4)
93(3)
2
88(6.5)
88(6.5)
89(5)
96(1)
91(3)
90(4)
93(2)
3
93(3)
92(4)
97(1)
95(2)
90(6.5)
90(6.5)
91(5)
4
91(5.5)
92(4)
91(5.5)
97(1)
93(3)
90(7)
95(2)
5
90(6.5)
90(6.5)
95(2.5)
96(1)
93(4)
91(5)
95(2.5)
6
96(2.5)
96(5.5)
93(4)
98(1)
90(6.5)
91(5)
90(6.5)
7
93(2.5)
93(2.5)
91(6)
95(1)
91(6)
92(4)
91(6)
8
88(7)
89(6)
96(1.5)
96(1.5)
92(4)
90(5)
95(3)
9
95(2)
93(4)
91(5.5)
97(1)
90(7)
91(5.5)
94(3)
10
92(2)
89(4)
88(5)
94(1)
90(3)
86(6.5)
86(6.5)
11
90(4.5)
89(6)
88(7)
95(1)
93(2)
91(3)
90(4.5)
12
89(6.5)
89(6.5)
92(4)
96(1)
94(2.5)
90(5)
94(2.5)
Mean Rank R1 = 4.458 R2 = 4.958 R3 = 4.083 R4 =1.125 R5 =4.458 R6 = 5.042 R7 =3.875
Let rj
i be the rank of the jth classiÔ¨Åer on ith data set. Then, the mean of the ranks of the
jth classiÔ¨Åer over all the N data sets will be computed as:
Rj = 1
N
N
‚àë
i=1
rj
i
(3.22)
The null hypothesis states that all the classiÔ¨Åers are equivalent, and so their ranks Rj
should be equal. To justify it, the Friedman statistic [13] is computed as follows:
ùúíF
2 =
12N
k(k + 1)
[
‚àë
j
Rj
2 ‚àík(k + 1)2
4
]
(3.23)
Under the current experimentation, this statistic is distributed according to ùúíF
2 with
k ‚àí1(= 6) degrees of freedom. Using Equation (3.23), the value of ùúíF
2 is calculated as
27.52. From the table of critical values (see any standard statistical book), the value of
ùúíF
2 with 6 degrees of freedom is 18.548 for ùõº=0.05 (where ùõºis known as the level of
signiÔ¨Åcance). It can be seen that the computed ùúíF
2 diÔ¨Äers signiÔ¨Åcantly from the standard
ùúíF
2. So the null hypothesis is rejected.
Iman et al. [13] have derived a better statistic using the following formula:
FF =
(N ‚àí1)ùúíF
2
N(k ‚àí1) ‚àíùúíF2
(3.24)
where FF is distributed according to the F-distribution with k ‚àí1(=6) and (k ‚àí1)(N ‚àí1)
(=66) degrees of freedom. Using Equation (3.22), the value of FF is calculated as 6.806.
The critical value of F(6, 66) for ùõº=0.05 is 2.1829 (see any standard statistical book),
which shows a signiÔ¨Åcant diÔ¨Äerence between the standard and calculated values of FF.
Thus, both Friedman and Iman et al. statistics reject the null hypothesis.

68
Hybrid Intelligence for Image Analysis and Understanding
As the null hypothesis is rejected, a post-hoc test known as the Nemenyi test [13] is
carried out for pairwise comparisons of the best and worst performing classiÔ¨Åers. The
performances of two classiÔ¨Åers are signiÔ¨Åcantly diÔ¨Äerent if the corresponding average
ranks diÔ¨Äer by at least the critical diÔ¨Äerence (CD), which is expressed as:
CD = qùõº
‚àö
k(k + 1)
6N
(3.25)
For Nemenyi‚Äôs test, the value of q0.05 for seven classiÔ¨Åers is 2.949 (see Table 5a of [34]). So,
the CD is calculated as 2.949
‚àö
7.8
6.12, that is, 2.6 using Equation (3.23). Since the diÔ¨Äerence
between mean ranks of the best and worst classiÔ¨Åer is much greater than the CD, we
can conclude that there is a signiÔ¨Åcant diÔ¨Äerence between the performing ability of the
classiÔ¨Åers. For comparing all the classiÔ¨Åers with a control classiÔ¨Åer (here, SVM), we have
applied the Bonferroni‚ÄìDunn test [13]. For this test, CD is calculated using the same
Equation (3.23). But here, the value of q0.05 for seven classiÔ¨Åers is 2.638 (see Table 5b of
[34]). So, the CD for the Bonferroni‚ÄìDunn test is calculated as 2.638
‚àö
7.8
6.12 (i.e., 2.32). As
the diÔ¨Äerence between the mean ranks of any classiÔ¨Åer and SVM is always greater than
CD, so the chosen control classiÔ¨Åer performs signiÔ¨Åcantly better than other classiÔ¨Åers.
A graphical representation of the above-mentioned post-hoc tests for comparison of
seven diÔ¨Äerent classiÔ¨Åers is shown in Figure (3.13).
After performing the above-mentioned statistical signiÔ¨Åcance tests on the 12 data sets
using seven classiÔ¨Åers, it can be concluded that SVM outperforms all other classiÔ¨Åers.
3.5.2
Statistical Performance Analysis of SVM ClassiÔ¨Åer
Detailed error analysis of SVM classiÔ¨Åer with respect to diÔ¨Äerent statistical parameters
is also performed in the present work. These parameters are as follows: kappa statistics,
mean absolute error (MAE), root mean square error (RMSE), true positive rate (TPR),
false positive rate (FPR), precision, recall, F-measure, Matthews correlation coeÔ¨Écient
(MCC), and area under the ROC curve (AUC). Table 3.4 provides a statistical perfor-
mance analysis with respect to the said seven parameters for each of the scripts used in
the present work. The above-mentioned parameters are deÔ¨Åned here in detail:
1. Kappa statistics: Cohen‚Äôs kappa indicates the agreement between two raters who each
classify N items into C mutually exclusive categories. It speciÔ¨Åes the proportion of
agreement beyond that expected by chance (i.e., the achieved beyond-chance agree-
ment as a proportion of the possible beyond-chance agreement). It takes the form:
k = observed agreement ‚àíchance agreement
1 ‚àíchance agreement
(3.26)
In terms of symbols, this can be expressed as:
k = Po ‚àíPc
1 ‚àíPc
(3.27)
where, Po is the proportion of observed agreements; and Pc is the proportion of
agreements expected by chance.

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
69
3.333
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
2.75
2.6
3.333
3.833
Difference of
Mean Ranks
CD
Differences of Mean Ranks
(a)
(b)
Pariwise Comparison of Classifiers for Nemenyi‚Äôs Test
|R4 ‚Äì R1| |R4 ‚Äì R2| |R4 ‚Äì R3| |R4 ‚Äì R5| |R4 ‚Äì R6| |R4 ‚Äì R7|
Mean Ranks
3.917
2.958
3.333
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
2.75
2.32
3.333
3.833
Difference of
Mean Ranks
CD
Differences of Mean Ranks with Control Classifier
Comparison of Classifiers with SVM for Bonferroni‚ÄìDunn‚Äôs Test
|R4 ‚Äì R1| |R4 ‚Äì R2| |R4 ‚Äì R3| |R4 ‚Äì R5| |R4 ‚Äì R6| |R4 ‚Äì R7|
Mean Ranks
3.917
2.958
Figure 3.13 Comparison of multiple classiÔ¨Åers for: (a) Nemenyi‚Äôs test and (b) Bonferroni‚ÄìDunn‚Äôs test.
2. Mean absolute error (MAE): This is a model evaluation metric used with a regression
model. The MAE of a model with respect to a test set is the mean of the absolute
values of the individual prediction errors over all instances in the test set. Each pre-
diction error is the diÔ¨Äerence between the true value and the predicted value for the
instance. It is deÔ¨Åned as:
MAE =
‚àën
i=1 abs(yi ‚àíùúÜ(xi))
n
(3.28)
where yi is the true target value for test instance xi; ùúÜ(xi) is the predicted target value
for test instance xi; and n is the number of test instances.
3. Root mean square error (RMSE): This is also a model evaluation metric used with a
regression model. The RMSE of a model with respect to a test set is the square of
the values of the individual prediction errors over all instances in the test set. Each
prediction error is square of the diÔ¨Äerence between the true value and the predicted

70
Hybrid Intelligence for Image Analysis and Understanding
Actual
Positive
Positive
Negative
Negative
Predicted
TP
FN
FP
TN
Figure 3.14 Confusion matrix for a classiÔ¨Åcation
rule.
value for the instance. It is deÔ¨Åned as:
RMSE =
‚àën
i=1 (yi ‚àíùúÜ(xi))2
n
(3.29)
The performance of a classiÔ¨Åcation rule with respect to predictive accuracy can be
summarized by a confusion matrix. Let us take a classiÔ¨Åcation problem where there
are only two classes to be predicted, referred to as the ‚Äúpositive‚Äù class and the ‚Äúneg-
ative‚Äù class. In this case, the confusion matrix will be a 2√ó2 matrix as illustrated in
Figure 3.14.
Let the number of true positives, false positives, false negatives, and true negatives
be symbolized by TP, FP, FN, and TN, respectively. In general, positive stands for
identiÔ¨Åed and negative stands for misclassiÔ¨Åed samples. Therefore, true positive can
be the number of correctly identiÔ¨Åed word images, false positive can be the number
of incorrectly identiÔ¨Åed word images, true negative can be correctly misclassiÔ¨Åed,
and false negative can be incorrectly misclassiÔ¨Åed word images. It is obvious that the
classiÔ¨Åcation rule is better for higher values of TP and TN, and lower values of FP
and FN.
4. TP rate (TPR): This is deÔ¨Åned as the proportion of positive cases that are correctly
identiÔ¨Åed as positive. It is also known as recall or sensitivity and can be written as:
TPR =
TP
TP + FN
(3.30)
5. FP rate (FPR): This is deÔ¨Åned as the proportion of negative cases that are incorrectly
classiÔ¨Åed as positive. It is also known as speciÔ¨Åcity and can be written as:
FPR =
FP
FP + TN
(3.31)
6. Precision: This is deÔ¨Åned as the proportion of the predicted positive cases that are
correct. It is also known as consistency or conÔ¨Ådence, and can be written as:
Precision =
TP
TP + FP
(3.32)
7. F-measure: This measure has been widely employed in information retrieval and is
deÔ¨Åned as the harmonic mean of recall and precision.
F ‚àíMeasure = 2 ‚ãÖrecall ‚ãÖprecision
recall + precision
(3.33)
8. Matthews correlation coeÔ¨Écient (MCC): The MCC is used to measure the quality of
binary (two-class) classiÔ¨Åcation in machine learning. It considers all the values of
TP, FP, FN, and TN and is generally viewed as a balanced measure that can be used
even if the classes are of very diÔ¨Äerent sizes. The MCC is mathematically deÔ¨Åned as
a correlation coeÔ¨Écient between the observed and predicted binary classiÔ¨Åcations;

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
71
Table 3.4 Statistical performance measures along with their respective means (shaded in gray)
achieved by the proposed technique for eight handwritten scripts
Kappa
statistic MAE
RMSE TP rate
FP
rate
Precision Recall
F-measure MCC
AUC
Bangla
0.973
0.001
0.994
0.973
0.983
0.981
0.986
Devanagari
0.983
0.004
0.970
0.983
0.977
0.973
0.989
Gurumukhi
0.952
0.006
0.957
0.952
0.954
0.948
0.973
Oriya
0.994
0.000
0.999
0.994
0.996
0.996
0.997
Malayalam
0.9736
0.0058 0.076
0.974
0.008
0.945
0.974
0.959
0.953
0.983
Telugu
0.977
0.004
0.971
0.977
0.974
0.970
0.986
Roman
0.962
0.002
0.983
0.962
0.972
0.968
0.980
Urdu
1.000
0.000
1.000
1.000
1.000
1.000
1.000
Mean
0.977 0.003
0.977
0.977
0.977
0.974
0.987
it returns a value between ‚àí1 and +1. A coeÔ¨Écient of +1 signiÔ¨Åes a perfect predic-
tion, 0 denotes no better than random prediction, and ‚àí1 speciÔ¨Åes total disagreement
between prediction and observation. The MCC can be directly calculated from the
confusion matrix using the formula:
MCC =
TP √ó TN ‚àíFP √ó FN
‚àö
(TP + FP)(TP + FN)(TN + FP)(TN + FN)
(3.34)
9. Area under the ROC curve (AUC): A receiver operating characteristic (ROC), or ROC
curve, is a graphical plot that illustrates the performance of a binary classiÔ¨Åcation
system as its discrimination threshold is varied. The curve is created by plotting the
TPR against the FPR at various threshold settings. A classiÔ¨Åcation rule no better than
chance is reÔ¨Çected by an ROC curve that follows the diagonal of the square, from
the lower left corner to the top right corner. A classiÔ¨Åcation rule that is worse than
chance would produce an ROC curve that lay below the diagonal‚Äìbut in this case,
performance superior to chance could be obtained by inverting the labels of the class
predictions. The AUC is then simply the area under the ROC curve. The AUC of
a ROC curve is a way to reduce ROC performance to a single value representing
expected performance. The performance of SVM classiÔ¨Åer, represented as a point on
the ROC curve for the present work, is shown in Figure 3.15.
3.5.3
Comparison with Other Related Works
For the justiÔ¨Åcation of the two-stage approach used in the present work, both the DWT
and RT have been individually applied on the present database. For DWT, the statistical
features have been computed in two ways: (1) considering only the LL component and
(2) considering the entire four components of the original word image. Similarly, for RT,
feature extraction is done both with and without considering the Ô¨Årst stage of our pro-
posed approach (i.e., DWT). This means that in the Ô¨Årst case, the RT is applied directly
on the original word images, and in the second case the transformation is applied only
on the LL component of the word images. In short, the statistical features are extracted
by considering the following four cases:

72
Hybrid Intelligence for Image Analysis and Understanding
1
0.99
Bangla
Devanagari
Gurumukhi
Oriya
Malayalam
Telugu
Roman
Urdu
0.1
0
0.2
0.3
0.4
0.5
FPR
TPR
0.6
0.7
0.8
0.9
1
0.98
0.97
0.96
0.95
0.94
0.93
Figure 3.15 Graph showing the performance of SVM classiÔ¨Åer on the ROC curve for eight handwritten
scripts.
‚Ä¢ Case I: Computation taking all four components from the original word image
obtained after DWT.
‚Ä¢ Case II: Computation taking only the LL component of the word image obtained after
DWT.
‚Ä¢ Case III: Computation after performing RT from the original word image.
‚Ä¢ Case IV: Computation after performing RT from the LL component of the word
image.
A comparison of the results considering all of these cases is shown in Table 3.5. Based
on the values of statistical performance parameters deÔ¨Åned above, it can be concluded
that the fourth case (i.e., our proposed approach) has surpassed all the remaining cases
except for the execution time. Here, the execution time is calculated as the time taken
from the extraction of statistical features to the computation of the respective param-
eters. It can be noted that the execution time for the fourth case is the maximum, but
still it takes only about 4 minutes of additional time than the best case (Case II).
Table 3.5 Comparison of statistical performance parameters for the four cases (best case is styled
in bold)
Kappa
statistic MAE
RMSE
TP
rate
FP
rate
Precision Recall F-measure MCC
AUC
Execution
time (secs)
Case I
0.8703 0.0533 0.1537 0.889 0.019
0.888
0.889
0888
0.870 0.979
450.720
Case II
0.8835 0.0426 0.1449 0.900 0.017
0.900
0.900
0.900
0.883 0.981
398.060
Case III 0.9185 0.0316 0.1257 0.930 0.012
0.931
0.930
0.930
0.919 0.985
505.132
Case IV 0.9736 0.0058 0.076
0.977 0.003
0.977
0.977
0.977
0.974 0.987
612.697

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
73
Table 3.6 Comparison of the present script identiÔ¨Åcation result with state-of-the art methods
Researchers
Feature set
Feature
dimension
Type of indic
scripts used
Execution
time (sec)
Success
rate (%)
Sarkar et al. [11]
Holistic features
8
955.803
85.02
Singh et al. [13]
Topological and
Convex hull based
Features
39
1076.575
91.47
Bashir et al. [14]
Density based
statistical features
4
649.518
76.23
Obaidullah et al.
[15]
Abstract/
mathematical,
structure based and
script dependent
features
41
725.962
90.14
Hiremath et al.
[20]
Wavelet based
co-occurrence
histogram
32
Bangla,
Devanagari,
Gurumukhi, Oriya,
801.244
92.55
Ma et al. [21]
Gabor Ô¨Ålter based
Features
32
Malayalam, Telugu,
Urdu and Roman.
651.046
93.81
Chaudhuri et al.
[22]
Gabor Ô¨Ålters
30
665.95
94.36
Padma et al. [23]
Wavelet packet
based Features
44
682.507
91.54
Singh et al. [24]
Shape based and
HOG features
87
861.207
93.43
Singh et al. [25]
DCT and Moment
Invariants
92
759.860
94.25
Propoased
Method
DWT and Radon
Transform
48
612.697
97.69
For comparison of the present work with some recent works, the feature set as
described in [11, 13‚Äì15] and [20‚Äì25] has been implemented and evaluated on the
present script data set. From the experiment, it was noted that the result with the
current feature set not only gives higher identiÔ¨Åcation accuracies but also is very
fast compared to other methods. So, it may be concluded from the analysis that the
proposed technique outperforms the previous ones. Comparisons in terms of the
accuracy and the computational complexity obtained by diÔ¨Äerent state-of-the-art
feature sets with the proposed feature set are detailed in Table 3.6.
3.5.4
Error Analysis
Sample handwritten word images that are successfully classiÔ¨Åed by the present tech-
nique are shown in Figure 3.16. It is observed from the confusion matrix that all the word
images written in Urdu script have been classiÔ¨Åed quite successfully. In certain cases,
some small words containing fewer than three characters (see Figure 3.17a) are misclas-
siÔ¨Åed by the present script recognition technique. The possible reason for this might be
that a signiÔ¨Åcant number of discriminatory feature values is not found to distinguish
them from the others. Also, the presence of skewness in some of the word images (see

74
Hybrid Intelligence for Image Analysis and Understanding
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 3.16 Samples of successfully classiÔ¨Åed handwritten word images written in: (a) Bangla, (b)
Devanagari, (c) Gurumukhi, (d) Oriya, (e) Malayalam, (f) Telugu, (g) Urdu, and (h) Roman scripts,
respectively.
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 3.17 Sample handwritten word images misclassiÔ¨Åed by the present technique due to the
presence of: (a) a signiÔ¨Åcantly smaller number of characters constituting the word; (b) skewness; (c‚Äìe)
structural similarity in Devanagari (misclassiÔ¨Åed as Gurumukhi), Gurumukhi (misclassiÔ¨Åed as
Devanagari), and Oriya (misclassiÔ¨Åed as Bangla); (f) Matra-like structure in Roman script; and (g‚Äìh)
abrupt spaces in Malayalam and Telugu scripts, respectively.
Figure 3.17b) and structural similarity among the characters of diÔ¨Äerent scripts, such
as Devanagari and Gurumukhi (Figure 3.17c‚Äìd) and Bangla and Oriya (Figure 3.17e),
lead to wrong identiÔ¨Åcation. Even discontinuities in Matra in certain scripts like Bangla
and Devanagari and the existence of Matra-like structure in Roman script (Figure 3.17f)
misclassify them among each other. The reasons for misclassiÔ¨Åcation of Malayalam and
Telugu scripts are mainly due to the existence of nonuniform spaces in between charac-
ters of a single word (Figure 3.17g‚Äìh).
3.6
Conclusion
Multiscript identiÔ¨Åcation from handwritten documents is a challenging research area
in the document image analysis domain. There is an increasing concern for designing
methods that can handle documents written in more than a single script or language.
Motivated by the demands of growing world trade, it can be expected that multiscript
OCR systems will become more widespread in both academic and commercial worlds.
IdentiÔ¨Åcation of the script or language of the documents, therefore, becomes a crucial
need in the automation of multilingual text recognition.

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
75
Handwritten script identiÔ¨Åcation is a very signiÔ¨Åcant task in any multiscript and mul-
tilingual OCR development. In this chapter, a two-stage approach for word-level hand-
written script identiÔ¨Åcation from eight popular scripts in India has been presented. The
proposed scheme is based on texture-based features. At present, 48 distinct features
have been extracted and tested using multiple classiÔ¨Åers. Finally, an accuracy of 97.69%
has been achieved using the SVM classiÔ¨Åer. Experiments on handwritten word images
show excellent script identiÔ¨Åcation results that also exceed other state-of-the-art tech-
niques. This guarantees the suitability of the scheme to script recognition problem. In
addition to this, the scheme also works well for single-character words. This scheme
is applicable for diÔ¨Äerent writing styles of handwriting, which ensures Ô¨Çexibility of the
present technique.
In future, this technique can also be extended to identify other scripts in any multi-
script environment. More data samples considering all oÔ¨Écial Indic scripts need to be
collected in near future for the comprehensive evaluation of the proposed methodol-
ogy. In short, the proposed methodology could be used as a common word-level script
identiÔ¨Åcation module for the development of any multiscript OCR system.
Acknowledgments
The authors are thankful to the Center for Microprocessor Application for Training Edu-
cation and Research (CMATER) and Project on Storage Retrieval and Understanding of
Video for Multimedia (SRUVM) of the Computer Science and Engineering Department,
Jadavpur University, for providing infrastructure facilities during progress of the work.
References
1 Spitz, A.L. (1997) Determination of script and language content of document
images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19 (3),
235‚Äì245.
2 Wood, S.L., Yao, X., Krishnamurthy, K., and Dang, L. (1995) Language identiÔ¨Åcation
for printed text independent of segmentation. IEEE of International Conference on
Image Processing, pp. 428‚Äì431.
3 Hochberg, J., Kerns, L., and Thomas, P.K.T. (1997) Automatic script identiÔ¨Åcation
from images using cluster based templates. IEEE Transactions on Pattern Analysis
and Machine Intelligence, 19 (2), 176‚Äì181.
4 Pal, U. and Chaudhuri, B.B. (2002) IdentiÔ¨Åcation of diÔ¨Äerent script lines from
multi-script documents. Image and Vision Computing, 20 (13‚Äì14), 945‚Äì954.
5 Chaudhuri, B.B. and Bera, S. (2009) Handwritten text line identiÔ¨Åcation in Indian
scripts. 10th International Conference on Document Analysis and Recognition
(ICDAR).
6 Padma, M.C. and Vijaya, P.A. (2009) IdentiÔ¨Åcation of Telugu, Devnagari and English
scripts using discriminating features. International Journal of Computer Science and
Information Technology (IJCSIT), 1 (2).
7 Hangarge, M. and Dhandra, B.V. (2010) OÔ¨Ñine handwritten script identiÔ¨Åcation in
document images. International Journal of Computer Applications (IJCA), 4 (6).

76
Hybrid Intelligence for Image Analysis and Understanding
8 Sangame, S.K., Ramteke, R.J., Andure, S., and Gundge, Y.V. (2012) Script identiÔ¨Åca-
tion of text words from a bilingual document using voting techniques. World Journal
of Science and Technology, 2 (5), 114‚Äì119.
9 Roy, K. and Pal, U. (2006) Word-wise handwritten script separation for Indian postal
automation. International Workshop on Frontiers in Handwriting Recognition, La
Baule, pp. 521‚Äì526.
10 Roy, K., Pal, U., and Chaudhuri, B.B. (2005) Neural network based word-wise hand-
written script identiÔ¨Åcation system for Indian postal automation. International
Conference on Intelligent Sensing and Information Processing, Chennai, pp. 581‚Äì586.
11 Sarkar, R., Das, N., Basu, S., Kundu, M., Nasipuri, M., and Basu, D.K. (2010) Word
level script identiÔ¨Åcation from Bangla and Devnagari handwritten texts mixed with
Roman scripts. Journal of Computing, 2, 103‚Äì108.
12 Singh, P.K., Sarkar, R., Das, N., Basu, S., and Nasipuri, M. (2013) IdentiÔ¨Åcation of
Devnagari and Roman script from multiscript handwritten documents. 5th Interna-
tional Conference on PReMI, LNCS 8251, pp. 509‚Äì514.
13 Singh, P.K., Sarkar, R., Das, N., Basu, S., and Nasipuri, M. (2014) Statistical compar-
ison of classiÔ¨Åers for script identiÔ¨Åcation from multi-script handwritten documents.
International Journal of Applied Pattern Recognition (IJAPR), 1 (2), 152‚Äì172.
14 Bashir, R. and Quadri, S.M.K. (2015) Density based script identiÔ¨Åcation of a mul-
tilingual document image. International Journal of Image, Graphics and Signal
Processing, 2, 8‚Äì14.
15 Obaidullah, S.M., Roy, K., and Das, N. (2013) Comparison of diÔ¨Äerent classiÔ¨Åers for
script identiÔ¨Åcation from handwritten document. IEEE International Conference on
Signal Processing, Computing and Control (ISPCC), pp. 1‚Äì6.
16 Saidani, A., Echi, A.K., and Belaid, A. (2013) IdentiÔ¨Åcation of machine-printed and
handwritten words in Arabic and Latin scripts. 12th IEEE International Conference
on Document Analysis and Recognition (ICDAR), pp. 798‚Äì802.
17 Tan, T.N. (1998) Rotation invariant texture features and their use in automatic script
identiÔ¨Åcation. IEEE Transaction on Pattern Analysis and Machine Intelligence, 20 (7),
751‚Äì756.
18 Busch, A., Boles, W.W., and Sridharan, S. (2005) Texture for script identiÔ¨Åcation.
IEEE Transaction on Pattern Analysis and Machine Intelligence, 27 (11), 1720‚Äì1732.
19 Peake, G.S. and Tan, T.N. (1998) Script and language identiÔ¨Åcation from document
images. Lecture Notes in Computer Science: Asian Conference Computer Vision,
LNCS-1352, pp. 97‚Äì104.
20 Hiremath, P.S. and Shivashankar, S. (2008) Wavelet based co-occurrence histogram
features for texture classiÔ¨Åcation with an application to script identiÔ¨Åcation in a
document image. Pattern Recognition Letters 29, pp. 1182‚Äì1189.
21 Ma, H. and Doermann, D. (2004) Word level script identiÔ¨Åcation on scanned docu-
ment images. SPIE Conference on Document Recognition and Retrieval, San Jose, CA,
pp. 124‚Äì135.
22 Chaudhuri, S. and Gulati, R.M. (2016) Script identiÔ¨Åcation using Gabor feature and
SVM classiÔ¨Åer. 7th International Conference on Communication, Computing and
Virtualization, Procedia Computer Science, pp. 85‚Äì82.
23 Padma, M.C. and Vijaya, P.A. (2010) Global approach for script identiÔ¨Åcation using
wavelet packet based features. International Journal of Signal Processing, Image
Processing and Pattern Recogntion, 20 (3).

A Two-Stage Approach to Handwritten Indic Script IdentiÔ¨Åcation
77
24 Singh, P.K., Mondal, A., Bhowmik, S., Sarkar, R., and Nasipuri, M. (2014) Word-level
script identiÔ¨Åcation from multi-script handwritten documents. 3rd International
Conference on Frontiers in Intelligent Computing Theory and Applications (FICTA),
pp. 551‚Äì558.
25 Singh, P.K., Khan, A., Sarkar, R., and Nasipuri, M. (2014) A texture based approach
to word-level script identiÔ¨Åcation from multi-script handwritten documents. 6th
IEEE International Conference on Computational Intelligence and Communication
Networks (ICCICN), pp. 228‚Äì232.
26 Languages of India. Available from http:// www.newworldencyclopedia.org/entry/
Languages_of_India
27 Gonzalez, R.C. and Woods, R.E. (1992) Digital Image Processing, vol. 1,
Prentice-Hall, Noida, India.
28 Deans, S.R. (1983) The Radon transform and some of its applications, John Wiley
and Sons, New York.
29 Lei, Y., Zheng, L., and Huang, J. (2014) Geometric invariant features in the Radon
transform domain for near-duplicate image detection, Pattern Recognition, 47,
pp. 3630‚Äì3640.
30 Habib, S.M.M., Noor, N.A., and Khan, M. (2006) Skew angle detection of Bangla
script using radon transform. 9th International Conference on Computer and Infor-
mation Technology, pp. 105‚Äì109.
31 Hoang, T.V. and Tabbone, S. (2012) Invariant pattern recognition using the RFM
descriptor. Pattern Recognition, 45 (1), 271‚Äì284.
32 Lei, Y., Wang, Y., and Huang, J. (2011) Robust image hash in radon transform
domain for authentication. Signal Process.: Image Commun., 26 (6), 280‚Äì288.
33 Ostu, N. (1978) A thresholding selection method from gray-level histogram. IEEE
Trans. Systems Man Cybernet. SMC-8, pp. 62‚Äì66.
34 Demsar, J. (2006) Statistical comparisons of classiÔ¨Åers over multiple data sets.
Journal of Machine Learning Research, 7, 1‚Äì30.

79
4
Feature Extraction and Segmentation Techniques in a Static
Hand Gesture Recognition System
Subhamoy Chatterjee, Piyush Bhandari, and Mahesh Kumar Kolekar
Indian Institute of Technology Patna, Bihta, Patna, Bihar, India
4.1
Introduction
Today, human‚Äìcomputer interaction (HCI) and human alternative and augmentative
communication (HAAC) are the two most rapidly growing Ô¨Åelds of research. It‚Äôs even
a challenging task to control computers and intelligent machines without any physical
interaction. The huge development of intelligent and fast computing has encouraged us
to think about HCI by means of some intelligent computing and algorithms.
Gestures are the most popular means for HCI by some intelligent computing and algo-
rithms. A gesture is a form of nonverbal communication in which visible body parts
express certain meaningful information. Gestures include the movement of hands, legs,
face, head, or any other body parts, or meaningful poses of hands, legs, or face. Static
and dynamic hand gestures are the most popularly used gestures and have a widespread
application in HCI, robot control, sign language communication, and surveillance [1].
Hand gestures have the capability to express some meaningful information to interact
with the surroundings. For an example, waving the hand in the air to express ‚ÄúGoodbye‚Äù
or showing a ‚ÄúV‚Äù sign to express victory are some popular hand gestures that have been
used by human beings for a long period.
There are mainly two types of gestures: static gestures [2] that include some poses of
body, Ô¨Ångers, and palms; and dynamic gestures [3] like movement of hands or any other
body parts. However, static hand gestures are more popularly used in HCI compared to
dynamic hand gestures because they are easy for human beings to imitate. Sometimes,
static hand gestures act as a speciÔ¨Åc transition stage in a dynamic hand gesture recog-
nition system, too. For that reason, HCI by means of some well-known hand gestures is
currently becoming popular. The most popular techniques used in static hand gesture
recognition systems include glove-based techniques [4] and vision-based techniques [2].
Glove-based techniques are not an automatic choice for a gesture recognition system as
the glove and its associated cables make the system very cumbersome and hamper the
free movement of hands. In addition to it, gloves and sensors are very expensive. On
the other hand, vision-based techniques require only one or more cameras to capture
the gesture images. Hence, vision-based techniques are very easy to provide HCI.
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

80
Hybrid Intelligence for Image Analysis and Understanding
Input
Gesture
Recognized
Gesture
Feature
Extraction
Preprocessing
Figure 4.1 Operational Ô¨Çowchart of proposed static hand gesture recognition system.
As shown in Figure 4.1, a vision-based static hand gesture recognition approach
consists of three steps: preprocessing, feature extraction, and classiÔ¨Åcation. In the pre
processing step, captured hand gesture images are segmented from the background,
and the region of interest (ROI) is extracted by a proper skin color region segmen-
tation algorithm [2]. Some morphological operations [5] are also performed in the
preprocessing step to obtain a smooth, closed, and complete contour or shape of the
gestures. Many researchers have employed various rotation [6] and illumination nor-
malization [7] techniques. Skin color‚Äìbased ROI segmentation is a vital issue in hand
gesture‚Äìbased HCI systems. In Section 4.2, various gesture segmentation algorithms
and our proposed illumination and skin color complexion invariant segmentation
algorithm are discussed in detail.
To Ô¨Ånd a proper feature descriptor that is capable of representing the shapes or
contours of hand gesture images is another important aspect in a static hand gesture
recognition system. Feature extraction is one of the most important steps in static
hand gesture recognition algorithms. Feature extraction techniques employed in
static hand gesture recognition are basically classiÔ¨Åed into two classes: contour-based
features [6] and silhouette-based features [2]. The feature descriptors used in the
literature are localized contour sequences [6], Krawtchouk moments [2], Tchebichef
moments [8], geometric moments [2], Zernike moments [2], and Gabor wavelets
[9]. In user-independent gesture recognition, none of these features have shown
satisfactory gesture recognition accuracy. To improve gesture recognition accuracy,
researchers have proposed methods like parallel feature fusion [8], serial feature fusion
[8], and multifeature fusion [10]. None of these proposed feature fusion methods are
satisfactory choices in user-independent gesture recognition, however, as they are not
eÔ¨Écient in real time. All the feature extraction and feature-level fusion techniques used
in static hand gesture recognition algorithms will be discussed in Sections 4.3 and 4.4.
In user-independent gesture recognition, some misclassiÔ¨Åcation occurs because of
diÔ¨Äerent hand shapes, aspect ratios, and styles of gesticulation by the users. In addition,
a static hand gesture sign language database contains some gestures that are of similar
shapes [11]. For an example in American Sign Language, gestures 7, 8, and 9 have similar
shapes, and these gestures cause a deterioration in the overall gesture recognition rate.
MisclassiÔ¨Åcation of similar-shape gestures is a frequently occurring problem in static
hand gesture recognition systems. Some recent trends in classifying similar-shape ges-
tures [12‚Äì14] have been reported in Section 4.5. In [14], researchers have employed the
F-ratio-based weighted feature extraction technique for similar-shape character recog-
nition. The F-ratio-based weighted feature technique has shown a satisfactory improve-
ment in recognition of digits 7, 8, and 9 of American Sign Language digits. Researchers
have also proposed techniques based on zoning methods [15‚Äì17] for similar-shape ges-
ture classiÔ¨Åcation. Various zoning methods and their eÔ¨Äectiveness in user-independent
gesture recognition are discussed in Section 4.4.

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
81
4.2
Segmentation Techniques
The main objective of gesture segmentation is to extract the ROI from the background
of the image. A Ô¨Çowchart of the hand region extraction method is given in Figure 4.2.
In this section, we will discuss various gesture segmentation algorithms, rotation and
illumination normalization techniques, and morphological operations.
4.2.1
Otsu Method for Gesture Segmentation
The Otsu method, [6] named after Nobuyunki Otsu, is vividly used in image pro-
cessing for segmentation of gray images. Otsu‚Äôs algorithm automatically performs
clustering-based image thresholding and converts the gray-level image into a binary
image. The algorithm assumes that the image contains two classes of pixels following
bimodal histogram (foreground pixels and background pixels); it then calculates
the optimum threshold separating the two classes so that their combined spread or
intraclass variance is minimum and their interclass variance is maximum. The Otsu
algorithm is described by the following steps:
Step 1: Select an initial threshold value T for segmentation of gray-level image f (x, y).
Step 2: Using this threshold T, the binary image is segmented into two classes.
Step 3: Determine the optimum threshold T‚àó. T‚àóis the value of T for which the ratio of
between-class variance and total variance is maximized. The optimum thresh-
old value T‚àóconverts the gray-level histogram to a bimodal histogram, as shown
in Figure 4.3.
In other words, the algorithm treats the segmentation of a grayscale image as a binary
classiÔ¨Åcation problem. Using a threshold T, the L gray levels image is segmented in two
classes, Œ©0 = 1, 2, ‚Ä¶ T and Œ©1 = T + 1, T + 2, ‚Ä¶ L. The optimal threshold T‚àóis deter-
mined as that value of T for which the ratio of between-class variance ùúé2
b and total class
variance ùúé2
T is maximum. If the number of pixels at the ith gray level is ni and the total
number of pixels is N, then for a given T, the between-class variance and the total class
Input
Gesture
Morphological
operations
Extracted
hand region
Rotation
normalization
Illumination
normalization
Skin color
region
extraction
Figure 4.2 Flowchart of hand region extraction method.

82
Hybrid Intelligence for Image Analysis and Understanding
Figure 4.3 Otsu method for thresholding:(a) gray-level image, (b) bimodal histogram, and (c)
segmented binary image.
variance are deÔ¨Åned as follows:
ùúé2
b = ùúî0(ùúá0 ‚àíùúáT)2 + ùúî1(ùúá1 ‚àíùúáT)2
(4.1)
ùúé2
T =
L
‚àë
i=1
(i ‚àíùúáT)2Pi
(4.2)
where ùúî0 = ‚àëT
i=1 Piùúîi, ùúá0 = ‚àëL
i=T+1 iPi‚àïùúî1, and ùúáT = ‚àëL
i=1 iPi.
Typically, the entire histogram is scanned to Ô¨Ånd the optimum threshold T.
Otsu‚Äôs segmentation has been widely used in various literature [6, 17, 18]. The main
drawback of the Otsu method is that it assumes that the image histogram is bimodal. So,
it fails if the number of classes is variable or the two classes have diÔ¨Äerent sizes. In com-
plex background color images, the number of clusters usually varies and the cluster sizes
are diÔ¨Äerent. For that reason, it can‚Äôt perform skin color segmentation in color images
and complex background images. Various skin color-based segmentation methods [2,
8, 19] have been proposed by the researchers for segmentation of color and complex
background database images.
4.2.2
Color Space‚ÄìBased Models for Hand Gesture Segmentation
Most of the researchers have proposed color space‚Äìbased [19] skin color region seg-
mentation methods for static hand gesture recognition because of their simplicity and
eÔ¨Äectiveness. Most of the color space models use some primary colors like red, green,
or blue to represent the images. Various color space models like RGB [20], YCbCr [8],
YIQ [21], and HSV [22] color spaces have been employed for skin color region detec-
tion in static hand gesture recognition algorithms. A brief study on the skin color region
detection using these color space models is given in this section.
4.2.2.1
RGB Color Space‚ÄìBased Segmentation
RGB color space [20] is the most popular color space model in image processing and
computer vision because of its similarity with the human visual system. Red, green, and
blue: these three primary colors are used in RGB color space to represent an image.
Although it is very similar to the human visual system, RGB color space models are very
sensitive to illumination changes.

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
83
The normalized RGB color space models [8] have been proposed to make the color
space model illumination invariant. The normalization techniques are given here:
Rnormalized = Red‚àï(Red + Green + Blue)
(4.3)
Gnormalized = Green‚àï(Red + Green + Blue)
(4.4)
Bnormalized = Blue‚àï(Red + Green + Blue)
(4.5)
Here, the sum of the normalized values of the red, green, and blue components are
unity.
Rnormalized + Gnormalized + Bnormalized = 1
(4.6)
The normalized RGB color space model is very useful in illumination-invariant skin
color detection and is widely used in static hand gesture recognition algorithms. In [23],
researchers discovered threshold values of R, G, and B components for Asian and Euro-
pean skin color, which are given here:
R > 95, G > 40, B > 20
(4.7)
max(R, G, B) ‚àímin(R, G, B) > 15
(4.8)
|R ‚àíG| > 15, R > G, R > B
(4.9)
4.2.2.2
HSI Color Space‚ÄìBased Segmentation
The HSI color space [23] consists of hue, saturation, and intensity components of the
RGB color space points. It is basically a combination of HSV and HSL color space. RGB
color space models represent color components in Cartesian coordinates, whereas HSI
color space models represent colors in cylindrical coordinates. For that reason, it is per-
ceptually more reliable than the RGB color space models. The threshold values for skin
color region segmentation in the HSI model are as follows:
0.23 < S < 0.68, 0 < H < 50, 0 < I < 64
(4.10)
4.2.2.3
YCbCr Color Space‚ÄìBased Segmentation
YCbCr color space [24] is widely used in image processing and computer vision as
it represents statistically independent color components: Y, Cb, and Cr, where Y is
the luminance component of the color and Cb and Cr are the chrominance between
blue-yellow and red-yellow components. YCbCr color space has mainly two advan-
tages: it shows uniform clustering of color components, and it has a uniform separation
between chrominance and luminance.
The threshold values for Asian and European skin color detection [8] are:
Y > 80, 85 < Cb < 135, 135 < Cr < 180
(4.11)
4.2.2.4
YIQ Color Space‚ÄìBased Segmentation
YIQ color space is derived from YCbCr color space. Here, Y corresponds to luminance
value, I corresponds to intensity value, and Q corresponds to chrominance value.
Researchers have proposed hand gesture segmentation using the YIQ color space
model [2] and speciÔ¨Åed threshold values for skin color regions for intensity (I) and
amplitude (ùúÉ).
30 ‚â§I ‚â§100, 105 ‚â§ùúÉ‚â§150
(4.12)

84
Hybrid Intelligence for Image Analysis and Understanding
where ùúÉis the amplitude of the Chroma cue vector.
ùúÉ= tan‚àí1(V‚àïU)
(4.13)
where U and V are the orthogonal vectors of YUV space.
4.2.3
Robust Skin Color Region Detection Using K-Means Clustering
and Mahalanobish Distance
Color space‚Äìbased methods struggle to exhibit proper ROI segmentation with illumi-
nation changes and diÔ¨Äerent skin color complexion of users. Most of the times, diÔ¨Äerent
illumination levels and skin color complexions change the predeÔ¨Åned threshold values of
diÔ¨Äerent color space models. A skin color region detection method using K-means clus-
tering and Mahalanobish distance has been proposed, which is robust to illumination
and skin color complexion changes.
In this proposed algorithm, we have used hand gesture images of uniform background
for semisupervised learning about skin color regions and used this a priori knowledge
to Ô¨Ånd out the skin color regions of the complex background images. Uniform back-
ground hand gesture images have only two clusters: foreground and background. For
that reason, these images are very eÔ¨Äective to Ô¨Ånd out the skin color region through
K-means clustering [25]. We found Mahalanobish distance [26] between the skin color
region centroids and the complex background images and then again perform K-means
clustering for extracting the foreground region using proposed Algorithm 4.1.
Algorithm 4.1 Hand gesture segmentation using K-means clustering and
Mahalanobish distance
Input: Hand gesture images of complex and uniform background databases
Output: Segmented region of interest (ROI)
1. Convert the RGB images of the uniform background database to YCbCr and
reshape it.
2. Perform modiÔ¨Åed K-means clustering with k=2.
[a1, b1] = mkmeans(image1, 2)
3. As the background is uniform, the hand region has the minimum area. Let Hand
be the minimum area that corresponds to all uniform background images.
4. Repeat step 2 for Hand with cluster size (k) = 2.
5. Convert the RGB color images of the complex database to normalized RGB color
images.
6. Repeat step 1.
7. Find the Mahalanobish distance (d) between cluster centers of Hand and
YCbCr images of complex databases.
8. Repeat step 2 with cluster size (k)=2 for d. [a3, b3] = mkmeans(d, 2).
9. if b = 2, then
10.
return Foreground;
11. else
12.
return Background;
13. end

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
85
4.2.3.1
Rotation Normalization
A real-time eÔ¨Écient static hand gesture database must contain images captured in diÔ¨Äer-
ent angles to make it rotation variant, as shown in Figure 4.4. All the hand gesture images
in our complex background database are captured in diÔ¨Äerent angles so that the hand
gesture images are rotation variant. To make the gesture recognition algorithm rota-
tion invariant, we have employed a rotation-invariant algorithm based on the following
steps.
Algorithm 4.2 Rotation normalization technique
Input:
Segmented hand gesture images
Output:Rotation normalized hand gestures
1. Find the direction of Ô¨Årst principal axes of a segmented hand gesture.
2. Find the rotation angle between the Ô¨Årst principal axes and the vertical axes.
3. Rotate it by an angle equal to the rotation angle.
4.2.3.2
Illumination Normalization
Illumination changes result in improper ROI segmentation as well as gesture misclas-
siÔ¨Åcation. To improve the overall gesture recognition accuracy, we have implemented
some illumination-invariant algorithms. Our illumination-invariant method consists of
three steps: (1) Power law transform [6], (2) RGB to normalized RGB [8], and (3) homo-
morphic Ô¨Åltering [6], as shown in Figure 4.5.
4.2.3.3
Morphological Filtering
To remove the undesirable background noises and to get the proper shape of hand
regions, some morphological operations are performed on the segmented binary hand
gesture images. In this work, a sequence of four morphological operations ‚Äì (1) erosion,
Figure 4.4 Hand gesture images captured in diÔ¨Äerent angles.
Input
image
Logarithm
DFT
IDFT
Exponential
H(u, v)
g(x, y)
Figure 4.5 Block diagram of homomorphic Ô¨Åltering.

86
Hybrid Intelligence for Image Analysis and Understanding
Figure 4.6 (a) RGB color image, (b) segmented image, and (c) ROI after morphological operation.
(2) dilation, (3) opening, and (4) closing ‚Äì is performed in order to get the proper binary
hand silhouette [10], as shown in Figure 4.6.
4.3
Feature Extraction Techniques
Both contour-based [5, 24] and shape-based features [2, 8, 9] are used to repre-
sent the shape information of binary silhouettes in static hand gesture recognition.
Contour-based features represent images based on some values calculated on the
boundary pixels of the binary hand silhouettes. In contrast, shape-based features
represent the shape of binary hand silhouettes. The main problem associated with the
feature extraction part is to discriminate similar-shape gestures. Gestures of similar
shapes have almost the same boundary proÔ¨Åles, and for that reason contour-based
feature extraction techniques misclassify gestures with similar boundary proÔ¨Åles. On
the other hand, shape-based features are calculated on the whole shape of any image.
So, these kind of features are the most appropriate features for similar-shape gesture
classiÔ¨Åcation.
In this section, we will discuss both shape-based and contour-based features to extract
desired information from the segmented hand regions. Nonorthogonal moments like
geometric moments [2] and orthogonal moments like Tchebichef [8] and Krawtchouk
moments [2] are used as shape-based features. All these moments are discrete. Contour
signature [24] and localized contour sequence (LCS) [6] are used as contour features.
We have designed an artiÔ¨Åcial neural network (ANN)-based [27] classiÔ¨Åer for gesture
classiÔ¨Åcation. The basic steps of feature extraction in a Krawtchouk static hand ges-
ture recognition system are shown in Figure 4.7. First, a Krawtchouk segmented hand
gesture is cropped so that the ROI contains the maximum area in the bounding box.
Then, the corresponding feature matrices are calculated.
4.3.1
Theory of Moment Features
Moment features [2] are wildly used in image processing and pattern recognition as
these features can represent both the globalized and localized properties of an image
Feature
extraction
Feature
matrix
Figure 4.7 Basic steps of feature extraction.

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
87
shape. Both continuous and discrete moment features are used by the researchers in
static hand gesture recognition. Discrete moment features are more popular than the
continuous moment features because continuous moments result in some discretiza-
tion error, which aÔ¨Äects the gesture recognition accuracy. Moment features can also be
classiÔ¨Åed as orthogonal and nonorthogonal features. Here, in this work, we have used
both orthogonal and nonorthogonal moment features.
For an image I(x, y) of size A √ó B, a geometric moment [2], Krawtchouk moment [2],
and Tchebichef moment [8] are explained here:
1. Geometric moment: The (a + b) order geometric moment is given as:
GMab =
A‚àí1
‚àë
x=0
B‚àí1
‚àë
y=o
xaybI(x, y)
(4.14)
where a = 0, 1, 2, 3 ‚Ä¶..A, b = 0, 1, 2 ‚Ä¶‚Ä¶ .B.
The corresponding central moment of order (a + b) is deÔ¨Åned as:
ùúáab =
A‚àí1
‚àë
x=0
B‚àí1
‚àë
y=0
(x ‚àíx)a(y ‚àíy)bI(x, y)
(4.15)
where x = GM10‚àïGM00 and y = GM01‚àïGM00.
Now, the normalized central moment is deÔ¨Åned as:
ùúÇab = ùúáab‚àïùúáùõæ
00
(4.16)
where ùõæis the scaling normalization factor and is given by ùõæ= (a + b)‚àï2 + 1.
2. Krawtchouk moment: Krawtchouk moments are discrete orthogonal moments
derived from the Krawtchouk polynomials. The nth-order Krawtchouk polynomial
at a discrete point x with (0 < p1 < 1, 0 < p2 < 1) is deÔ¨Åned in terms of hyper
geometric function [2] as:
k1(x, p, n) = F(‚àín, ‚àíx, ‚àíN, 1‚àïn)
(4.17)
By deÔ¨Ånition,
F(a, b, c, z) =
n
‚àë
ùë£=0
aùë£bùë£zùë£‚àï(cùë£ùë£!)
(4.18)
where aùë£is the Pochhammer function and is given by the following function:
aùë£= a(a ‚àí1) ‚Ä¶.(a + ùë£‚àí1)
(4.19)
From the Krawtchouk polynomials, we can get a weight function that is used to
normalize feature values. The weight function of the Krawtchouk polynomial is given
as:
W(x, p, n) =
N!
(x!(N ‚àíx)!)px
1(1 ‚àíp1)N‚àíx
(4.20)
Krawtchouk polynomials are normalized by dividing with weight function as given
here:
k1
1(x, p1, N) = k1(x, p1, N)
‚àö
W(x, p1, N)‚àïùúå(x, p1, N)
(4.21)

88
Hybrid Intelligence for Image Analysis and Understanding
and
ùúå(n, p1, N) = (‚àí1)n(1 ‚àíp1)nn!
pn
1(‚àíN)n
(4.22)
The constants p1 and p2 are called shift parameters. Normally, these are set to 0.5 to
make the ROI centralized. The Krawtchouk moment of order (n+m) is given by the
following equation:
Qnm =
N‚àí1
‚àë
x=0
M‚àí1
‚àë
y=0
I(x, y)k1
1(x, p1, n)k1
2(y, p2, m)
(4.23)
3. Tchebichef moment: Tchebichef moments [8] are same as Krawtchouk moments, as
they are also derived from orthogonal basis functions. The 1D Tchebichef polynomial
at a discrete point x is deÔ¨Åned as:
tn(x) = (1 ‚àíN)n3F2(‚àín, ‚àíx, 1 + n, 1, 1 ‚àíN, 1)
(4.24)
where 3F2(.) is a hypergeometric function and is deÔ¨Åned by the following equation:
3F2(a1, a2, a3; b1, b2; z) = (a1)ùë£(a2)ùë£(a3)ùë£zùë£
(b1)ùë£(b2)ùë£ùë£!
(4.25)
where aùë£is the Pochhammer function and is given as:
aùë£= a(a ‚àí1) ‚Ä¶..(a + ùë£‚àí1)
(4.26)
The Tchebichef moment of order (n + m) is given as:
Tnm =
1
‚àö
ùúå(n, N)ùúå(m, M)
N‚àí1
‚àë
x=0
M‚àí1
‚àë
y=0
tn(x)tm(y)I(x, y)
(4.27)
where ùúå(n, N) is the normalization constant and is given by the following equation:
ùúå(n, N) = (2n)!
(N + n)!
(2n + 1)!(N ‚àín ‚àí1)!
(4.28)
4.3.2
Contour-Based Features
The shape of contours of static hand gestures is a very important property, which
could be used to distinguish hand gestures. In this section, we will discuss mainly two
contour-based features, contour signature and localized contour sequence. Both of
these features are popularly used by the researchers in static hand gesture recognition.
1. Localized contour sequence: The LCS has been conÔ¨Årmed by various researchers
as a very eÔ¨Écient representation of contours [6]. As LCS is not bounded by the
shape complexity of the gestures, many researchers have used it as an eÔ¨Écient feature
descriptor of static hand gesture recognition. The Canny edge detection algorithm is
used for edge detection of hand gesture images in LCS feature descriptors.
LCS uses a contour-tracking algorithm in either a clockwise or anticlockwise
direction. The contour pixels are numbered sequentially from the topmost left pixel.
A contour sequence is calculated on each pixel (xi, yi) by the following algorithm:

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
89
A. Create a window W.
B. Find out the intersection points (ai, bi) of the window (W) and the contour. Find
the chord (C) connecting the bottom-left and bottom-right points of the contour.
C. Calculate the ith sample h(i) by computing the perpendicular Eucledian distance
from the top-left point of the contour to the chord.
D. For N number of contour points, we will get N number of contour sequence h(i).
F. Find the average number of boundary pixels in contours. As the number of con-
tour points are diÔ¨Äerent, h(i) should be sampled to the average value.
The LCS feature set is position and size invariant, and for that reason it is a robust
feature descriptor in static hand gesture recognition.
2. Contour signature: Contour signature is another powerful feature descriptor used
by the researchers [24]. Contour signature Ô¨Ånds out the amplitude and directions of
the boundary points from the contour centroids. A contour signature is calculated
by the following algorithm.
A. Find out the edges of the gesture images by the Canny edge detection method.
B. Find out the centroids (Cx, Cy) of the contours.
C. Find the amplitude (Ai) and phase (ùúÉi) of the boundary points (xi, yi) from the
centroids. Let Cx and Cx be the centroids of the contour of the hand gesture image.
Let the ith boundary point be (xi, yi). Then the amplitude (Ai) of the ith boundary
point is given by:
Ai =
‚àö
(x2
i + y2
i )
(4.29)
The phase (ùúÉi) is given by the following equation:
ùúÉi = tan‚àí1
(yi
xi
)
(4.30)
D. Find out the feature vector Fi by concatenating Ai and ùúÉi.
Fi = [AiùúÉi]
(4.31)
E. Contours of diÔ¨Äerent gestures have diÔ¨Äerent numbers of boundary pixels. So, the
length of feature vector Fi also varies with images. To overcome this problem,
all the feature vectors are sampled to a value N, where N is the average value of
feature lengths.
Contour signature is aÔ¨Äected by shape complexity and contour noises. For that reason,
in user-independent gesture recognition, Contour signature doesn‚Äôt show satisfactory
gesture recognition accuracy.
4.4
State of the Art of Static Hand Gesture Recognition
Techniques
In user-independent gesture recognition, as the gesture images of diÔ¨Äerent users are
used for training and testing purposes, some misclassiÔ¨Åcation occurs because of diÔ¨Äer-
ent hand shapes, aspect ratios, and styles of gesticulation by the users. In this section,
we will introduce some feature enhancement techniques to improve user-independent
gesture recognition.

90
Hybrid Intelligence for Image Analysis and Understanding
4.4.1
Zoning Methods
A zoning method (ZM) [15] can be considered as a partition of feature set (F) into M
subset, named zones (ZM) = [z1z2 ‚Ä¶ zM], where z1z2 ‚Ä¶ zM are the zones or smaller sub-
spaces of feature vectors, each one providing zonal information of patterns. The main
goal of zoning is to extract useful information from the subregions of the images to
recognize similar-shape images. Zoning topologies can be classiÔ¨Åed into two main cat-
egories: static [17] and adaptive [28]. Static topologies are designed without using a
priori information on feature distribution in pattern classes. Conversely, adaptive zon-
ing topologies can be considered as the results of the optimization procedure for zoning
design. As the adaptive zoning topologies use a priori information on feature distribu-
tion and classiÔ¨Åcation, these methods are not real-time eÔ¨Écient. Hence, various static
zoning topologies are discussed in this chapter. The taxonomy of zoning topologies is
shown in Figure 4.8.
Static zoning topologies can be classiÔ¨Åed into two broad classes: uniform zoning [29]
and nonuniform zoning [30]. In uniform zoning, gesture images are divided into grids
of the same sizes. We have used 2 √ó 2, 3 √ó 2, 2 √ó 3, 1 √ó 4, and 4 √ó 1 regular grids for
uniform zoning. In nonuniform zoning, gesture images are divided into unequal grids
based on some partition criteria. We have partitioned gesture images into zones by
centroid-based partition. We have used 4 √ó 1 and 5 √ó 1 grids in nonuniform zoning.
4.4.2
F-Ratio-Based Weighted Feature Extraction
F-ratio-based weighted feature extraction [14] is a very useful technique for
similar-shape gesture recognition. To improve the user-independent gesture recogni-
tion, F-ratio-based weighted feature extraction will be discussed in this section. F-ratio
is a statistical measure that is deÔ¨Åned by the ratio of the between-class variance and
within-class variance. The F-ratio is calculated statistically from the features of the
similar-shape gestures, and it is multiplied with the feature vectors for feature enhance-
ment. The weighted feature vector includes more useful information to discriminate
the similar classes. The F-ratio (Fi) is deÔ¨Åned by:
Fi =
S2
bi
S2
wi
(4.32)
Non-uniform
Uniform
Adaptive
Static
Zoning Topologies
Figure 4.8 Zoning topology.

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
91
where S2
bi and S2
wi are between-class and within-class variance. They are deÔ¨Åned by the
following equation with class number l, class size L, and a priori probability P(ùúîl) of
class ùúîl.
S2
wi =
L
‚àë
l=1
P(ùúîl)E(xi ‚àímli)2‚àïùúîl
(4.33)
S2
bi =
L
‚àë
l=1
P(ùúîl)(mli ‚àím0i)2
(4.34)
where mli = E(xi‚àïùúîl) and m0i = E(xi) = ‚àëL
l=1 P(ùúîl)mli. Here, the function E calculates
the expectation value of its argument.
Though the F-ratio-based weighted feature extraction technique results in a signiÔ¨Å-
cant improvement of user-independent gesture recognition accuracy, it is not real-time
eÔ¨Écient as the similar-shape gestures are determined manually.
4.4.3
Feature Fusion Techniques
In feature fusion, two diÔ¨Äerent feature sets are combined by a proper rule or algorithm.
Various feature fusion techniques are reported in literature [8, 10, 31]. In this section,
we will discuss two feature fusion methods, MLE-based hidden feature fusion, and serial
feature fusion and parallel feature fusion.
1. MLE-based hidden feature fusion: In this method, it is assumed that there must
be a hidden feature ( fh) of two diÔ¨Äerent feature sets f1 and f2. It is assumed that the
original two features are obtained by two diÔ¨Äerent transformations [31], as given here:
f1 = Afh + ùúñ1, f2 = Bfh + ùúñ2
(4.35)
where ùúñ1 and ùúñ2 are independent noises from Gaussian distribution N(0, ùúéI). A and
B are constants.
Let f1 and f2 be represented by:
f1 = [ f 1
1 , f 2
1 , f 3
1 ‚Ä¶‚Ä¶ . f n
1 ]
(4.36)
and
f2 = [ f 1
2 , f 2
2 , f 3
2 ‚Ä¶‚Ä¶ . f n
2 ]
(4.37)
Then the log likelihood for a hidden feature is given by:
Fh = [ f 1
h , f 2
h , f 3
h ‚Ä¶‚Ä¶ . f n
h ]
(4.38)
The linear transformation of Fh, A, and B is given by:
L(Fh, a, b) = ‚àí1
2ùúé2
n
‚àë
i=1
‚Äñ‚Äñ‚Äñ f1 ‚àíAf 1
h ‚Äñ‚Äñ‚Äñ2
2 ‚àí
1
2ùúé2
n
‚àë
i=1
‚Äñ‚Äñ‚Äñ f2 ‚àíBf 1
h ‚Äñ‚Äñ‚Äñ2
2 + c
(4.39)
where c is constant. This equation is closely related to the problem of dictionary learn-
ing. So, we can say that it is a joint dictionary-learning problem in which A and B are
the dictionaries of two features f1 and f1, respectively.

92
Hybrid Intelligence for Image Analysis and Understanding
2. Serial feature fusion and parallel feature fusion: In [8], researchers have intro-
duced two new feature fusion strategies, which enhance the classiÔ¨Åcation accuracy
signiÔ¨Åcantly. Let X and Y be two features that consist of n and m numbers of feature
vectors xi and yi. Here, xiùúñX and yiùúñY. Then the serially combined feature is deÔ¨Åned
as Fs = [xi yi]. The dimension of the resultant feature is (n + m). In case of paral-
lel feature fusion, the resultant feature is expressed by a supervector Fp = [xi + yi].
If the dimensions of X and Y are not the same, then the lower dimension vector is
up-sampled to the dimension of an upper dimensional feature. Thus, the size of the
resultant parallel feature becomes m if m > n or n if n > m.
For an example, let X = [x1 x2] and Y = [y1 y2 y3]. Then, to Ô¨Ånd the parallel fea-
ture fusion of X and Y, we have to make X and Y the same length by up-sampling X
to a length of 3. Now, X = [x1 x2 x3]. Parallel combination is deÔ¨Åned in a superplane
as Fp = [xi + yi].
Numerical unbalance is the main problem in feature-level fusion. Let a feature
vector X = [0.5 0.9], and another is Y = [15 14]. Then, after parallel combination,
attributes of Y will be more than X. It means almost all the signiÔ¨Åcance of X will
be lost. To overcome this numerical unbalance condition, features have to be nor-
malized Ô¨Årst. Normalization of features is commonly done by dividing its maximum
value: X = X‚àïmax(X) and Y = Y‚àïmax(Y).
Even after normalization, numerical imbalance may occur because of diÔ¨Äerence in
feature sizes. To avoid this, the lower dimensional one is multiplied by a normaliza-
tion constant. We have empirically selected this normalization constant as ùë£= n2‚àïm2
assuming n > m. Then the serial feature fusion and parallel fused feature vectors are
given by:
Fs = [x vy]
(4.40)
and
Fp = [xi + ùë£yi]
(4.41)
4.5
Results and Discussion
Extracting ROI from the captured hand gesture color images is a challenging task in
static hand gesture images. We have used images captured in complex backgrounds,
varring the illumination level, skin color complexion, and rotation angle. Most of the
color space‚Äìbased skin color segmentation algorithm fails to extract ROI from the com-
plex background color images. Hence, we have emphasized rotation and illumination
normalization in the preprocessing part. Our proposed skin color segmentation algo-
rithm has shown robustness in varring the illumination level and rotation angle.
DiÔ¨Äerent styles of gesticulation by the users and diÔ¨Äerent hand shapes and aspect
ratios are the most signiÔ¨Åcant problems in user-independent static hand gesture recog-
nition. So, to choose a proper feature descriptor that could overcome all the aforemen-
tioned problems is of paramount importance in user-independent static hand gesture
recognition. In American Sign Language digits, gestures 7, 8, and 9 have almost sim-
ilar shape and aÔ¨Äect the gesture recognition accuracy seriously. Most of the moment
and contour-based features misclassify these similar-shape gestures. To overcome this

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
93
similar-shape gesture misclassiÔ¨Åcation, F-ratio-based feature enhancement technique,
serial feature fusion and parallel feature fusion, MLE-based hidden feature fusion, and
zoning methods have been employed. Improvement in gesture recognition accuracy by
these techniques will be discussed in this section.
4.5.1
Segmentation Result
1. Database: We have used two hand gesture databases. In case of the Ô¨Årst database,
a uniform black background is used behind the user to avoid background noises as
shown in Figure 4.9. The second one is taken in a complex background as shown in
Figure 4.10. In both the databases, the forearm region is separated by wrapping a
black cloth. In the second database, the hand region is restricted to have maximum
area compared to other regions.
A Logitech c120 webcam has been used to capture the hand gesture images. The
resolution of the grabbed image is 320√ó240 for both the databases. All images are
taken in various angles and in diÔ¨Äerent light conditions to make our gesture recogni-
tion algorithm rotation and illumination invariant. We have used the uniform back-
ground database only for semisupervised learning purposes. The second database
is mainly used for testing and training purposes. The data set consists of 1500 ges-
tures of 10 classes, with 15 samples from each class of 10 users. We have conducted
two experiments to evaluate the gesture recognition performance by our proposed
feature descriptors. In the Ô¨Årst experiment, gesture recognition performance is eval-
uated in the user-dependent condition. 1000 gestures of 10 classes for all the users
are used for training purposes, and 500 gestures of 10 classes for all the users are
used for testing purposes. In the second experiment, the data set is equally divided
into training and testing data sets of 750 gestures of 10 classes for Ô¨Åve diÔ¨Äerent users
to make the system user independent.
2. Experimental results: We have used two diÔ¨Äerent skin color segmentation meth-
ods: YCbCr color model‚Äìbased skin color segmentation, and K-means clustering and
Mahalanobish distance‚Äìbased skin color segmentation. Skin color region detection
Figure 4.9 Uniform Background Database: (a) digit 1, (b) digit 2, and (c) digit 3.
Figure 4.10 Complex Background Database: (a) digit 0, (b) digit 1, and (c) digit 2.

94
Hybrid Intelligence for Image Analysis and Understanding
in complex backgrounds and varying illumination conditions is a diÔ¨Écult task for
researchers. Most of the researchers have employed color space‚Äìbased skin color
segmentation for static hand gesture recognition. Color space‚Äìbased skin color seg-
mentation methods are not robust for skin color detection, because in varying illumi-
nation conditions and in complex backgrounds, threshold values for the color space
models also vary. The result of YCbCr color model‚Äìbased segmentation is shown in
Figure 4.11. It can be concluded from the segmentation result that the extracted fore-
ground is quite diÔ¨Äerent from the desired response and may result in misclassiÔ¨Åcation
in gesture recognition by any classiÔ¨Åer.
We have proposed a skin color detection process using semisupervised learn-
ing based on K-means clustering and Mahalanobish distance, which has shown
robustness in varying illumination conditions and complex backgrounds as shown
in Figure 4.12. Our proposed illumination normalization technique has also shown
eÔ¨Äectiveness in varying illumination conditions.
4.5.2
Feature Extraction Result
We have used orthogonal and nonorthogonal moment features as the shape-based
feature descriptors and contour signature and localized contour sequence (LCS) as
contour-based feature descriptors. Moment features are calculated on the pixel values
of images. For that reason, we have resized and cropped our original images into a
40√ó40 size so that the hand region becomes the maximum region. We have empirically
selected the order of geometric moment as 49 (n=m=7), and the Krawtchouk, and
Tchebichef moments as 64 (m=8=n=8). We have empirically selected a translation
parameter p as 0.5 of the Krawtchouk moment to make the feature scale invariant. On
the other hand, the geometric moment has an order of 49, so the size of the feature is
49. contour-based features like contour signature and LCS are calculated on boundary
pixels of images.
Figure 4.11 (a) Input gesture, (b) YCbCr segmented image, and (c) extracted hand region after
morphological operation.
Figure 4.12 (a) input gesture, (b) segmented image by our proposed method, and (c) extracted hand
region after morphological operation.

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
95
1. ClassiÔ¨Åcation performance of the proposed feature descriptors: Both probabilis-
tic classiÔ¨Åers like the hidden Markov model (HMM) [32] and Bayesian belief network
[33] and statistical classiÔ¨Åers like support vector machines (SVMs) [34] have been
used for static hand gesture classiÔ¨Åcation. In this work, we have implemented a feed-
forward multilayer ANN classiÔ¨Åer. Performance of the proposed feature descriptors
in terms of classiÔ¨Åcation accuracy in both user-dependent and user-independent
conditions are shown in Table 4.1. It shows that the Krawtchouk moment is the
best in terms of classiÔ¨Åcation accuracy. In user-independent conditions, neither
these moment features nor the contour-based features have shown satisfactory
classiÔ¨Åcation accuracy. The geometric moment shows the worst performance in both
user-dependent and user-independent conditions. Though both the contour-based
features have shown signiÔ¨Åcant gesture recognition accuracy in the user-dependent
condition, they are unable to perform well in user-independent gesture recognition.
The Krawtchouk moment, Tchebichef moment, contour signature, and LCS features
have shown more than 95% accuracy in the user-dependent condition. In the
user-independent condition, only the Krawtchouk moment has shown signiÔ¨Åcant
gesture recognition accuracy, as shown in Table 4.1.
2. Why the Krawtchouk moment is performing better: To classify similar-shape
gestures as shown in Figure 4.13, the localized properties of images should be
included with the global feature descriptor. The appropriate selection of shifting
parameters p1 and p2 of the Krawtchouk moment feature descriptor enables to
extract signiÔ¨Åcant local properties along with global features. The parameters p1
and p2 shift the support of the moment horizontally and vertically, respectively.
In the globalized Krawtchouk moment, features p1 and p2 are set to 0.5, so that
the support of the moment feature is centralized. The confusion matrix of the
Krawtchouk moment is shown in Table 4.2. From Table 4.2, it is clear that most of
the misclassiÔ¨Åcation occurs for the similar-shape gestures 7, 8, and 9. To classify
similar-shape gestures 7, 8, and 9 as shown in Figure 4.12, some localized property
of Krawtchouk moments should be introduced, which has been done by introducing
zoning methods.
3. Why other features are not performing well: In the cases of the geometric and
Tchebichef moments, mismatches occur more than with Krawtchouk moments, as
shown in Table 4.3 and Table 4.4. This is because the geometric moment is a global
feature, and it only represents the statistical attributes of the shape. On the other
Table 4.1 User-dependent and user-independent classiÔ¨Åcation
results
Features
ClassiÔ¨Åcation
accuracy
(user-dependent
condition) (%)
ClassiÔ¨Åcation
accuracy
(user-independent
condition) (%)
Krawtchouk moment
99.27
91.53
Tchebichef moment
95.56
82.67
Geometric moment
88.7
76.2
Contour signature
98.5
78.8
LCS
98.8
81.2

96
Hybrid Intelligence for Image Analysis and Understanding
Table 4.2 Confusion matrix of the Krawtchouk moment for user-independent
condition
Class
0
1
2
3
4
5
6
7
8
9
0
138
0
10
0
1
1
0
0
0
0
1
0
150
0
0
0
0
0
0
0
0
2
0
0
150
0
0
0
0
0
0
0
3
0
0
0
132
0
1
4
0
0
13
4
0
0
0
0
150
0
0
0
0
0
5
14
1
0
6
12
112
0
0
0
5
6
0
0
0
1
0
0
148
0
1
0
7
0
0
0
0
0
0
0
132
1
17
8
0
0
1
0
1
0
1
10
133
4
9
0
0
0
0
4
1
4
0
13
128
Table 4.3 Confusion matrix of the Tchebichef moment for the user-independent
condition
Class
0
1
2
3
4
5
6
7
8
9
0
131
0
6
2
0
0
2
0
1
8
1
0
150
0
0
0
0
0
0
0
0
2
0
1
147
0
0
0
0
0
1
1
3
2
0
0
145
0
3
0
0
0
0
4
0
0
1
0
120
0
15
14
0
0
5
0
0
0
20
0
124
0
0
1
5
6
0
0
1
0
10
0
136
0
3
0
7
0
9
6
0
3
0
12
106
10
4
8
0
2
15
0
23
0
7
30
58
15
9
7
0
1
0
0
1
1
2
15
123
hand, although the Tchebichef moment is orthogonal, it doesn‚Äôt show satisfactory
classiÔ¨Åcation performance in the user-independent condition. In the case of the geo-
metric moment, gesture 7 is misclassiÔ¨Åed as 8 and 9; gesture 8 is misclassiÔ¨Åed as 6, 7,
and 9; and gesture 9 is misclassiÔ¨Åed as 6 and 7, as shown in Table 4.4. In the case of
the Tchebichef moment, gesture 8 has been misclassiÔ¨Åed as 7 and 9, and gesture 9 has
been misclassiÔ¨Åed as 7 and 8, as shown in Table 4.3. In the case of the Krawtchouk
moment, mismatch occurs less than with the geometric and Tchebichef moments, as
shown in Table 4.2.
4. ClassiÔ¨Åcation results of feature enhancement, zoning, and feature fusion tech-
niques: To overcome this similar-shape gesture mismatch problem, we have intro-
duced some feature enhancement techniques, feature fusion and zoning methods as
described in Section 4.4. Here we will discuss the improvements in gesture recogni-
tion by these feature enhancement methods. Table 4.5 has shown the classiÔ¨Åcation
results of Krawtchouk moment features by zoning methods. By using 4 √ó 1 nonuni-
form zoning methods, gesture recognition accuracy has been improved signiÔ¨Åcantly,

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
97
Table 4.4 Confusion matrix of the geometric moment for the user-independent
condition
Class
0
1
2
3
4
5
6
7
8
9
0
150
0
0
0
0
0
0
0
0
0
1
0
148
0
2
0
0
0
0
0
0
2
0
0
150
0
0
0
0
0
0
0
3
3
2
0
142
0
3
0
0
0
0
4
10
10
0
0
112
1
10
12
5
0
5
20
0
20
0
0
93
7
2
8
0
6
6
0
10
0
3
0
124
0
6
1
7
1
0
7
0
10
14
0
79
7
32
8
0
0
6
0
6
8
21
42
45
22
9
1
0
5
0
0
2
15
27
0
100
Table 4.5 ClassiÔ¨Åcation result of Krawtchouk
moment zonal features
Zoning methods
Accuracy (%) for
Krawtchouk moment
2 √ó 2 uniform grid
92.5
3 √ó 2 uniform grid
93.1
2 √ó 3 uniform grid
92.8
4 √ó 1 uniform grid
94.7
5 √ó 1 uniform grid
94.33
4 √ó 1 nonuniform grid
95.8
5 √ó 1 nonuniform grid
95.4
and it shows 95.8% classiÔ¨Åcation accuracy for Krawtchouk moments. Table 4.6 has
shown classiÔ¨Åcation results of various feature-level fusion and MLE-based hidden
feature fusion of moment features. Parallel fusion of Krawtchouk and Tchebichef
moments has shown the best gesture recognition accuracy of 95.33%, and the hidden
feature of Krawtchouk and Tchebichef moments has shown the best gesture recog-
nition accuracy of 94.5% among the hidden features. Gesture classiÔ¨Åcation results by
F-ratio-based enhanced features have been discussed in Table 4.7. From Table 4.5 to
Table 4.7, it is clear that our proposed methods have shown a great improvement in
user-independent gesture classiÔ¨Åcation accuracy. Among the proposed methods, 41
centroid-based nonuniform Krawtchouk zonal features have shown the best gesture
recognition accuracy of 95.8%.
4.6
Conclusion
In this chapter, we have compared various static hand gesture image segmenta-
tion and feature extraction techniques. The problems associated with real-time

98
Hybrid Intelligence for Image Analysis and Understanding
Figure 4.13 ASL similar-shape gestures: (a) 7, (b) 8, and (c) 9.
Table 4.6 ClassiÔ¨Åcation results of serial, parallel,
and MLE-based hidden feature fusion
Fused features
Accuracy (%)
KM+TM serial fused feature
93.53
KM+TM parallel fused feature
95.33
KM+GM serial fused feature
94.93
KM+GM parallel fused feature
94.2
MLE-based hidden features
Krawtchouk+Tchebichef
94.5
Krawtchouk+Geometric
92.7
Geometric+Tchebichef
84.7
Table 4.7 ClassiÔ¨Åcation result of F-ratio-based
enhanced features
F-ratio-based enhanced features
Accuracy (%)
Krawtchouk moment
91.88
Tchebichef moment
84.56
Geometric moment
79.45
Contour signature
80.5
LCS
83.5
user-independent static hand gesture recognition, like illumination variance, rotation
variance, and similar-shape gesture misclassiÔ¨Åcation, have been addressed. In varying
illumination conditions and skin color complexions, threshold values for skin color
regions also change, which results in deterioration of gesture segmentation. To over-
come this problem, we have proposed a skin color segmentation method based on
K-means clustering and Mahalanobish distance, and it shown a great improvement in
static hand gesture image segmentation. We have also proposed the following shape-
and contour-based feature extraction techniques: geometric moments, Krawtchouk
moments, Tchebichef moments, contour signature, and LCS. Their performance in
both user-independent and user-dependent gesture recognition has been reported.
Due to diÔ¨Äerent gesticulation styles of users and similar-shape gesture images, these
well-known feature descriptors exhibit some misclassiÔ¨Åcation in user-independent
gesture recognition. To overcome this similar-shape gesture mismatch problem,
we have proposed the following feature enhancement techniques: F-ratio-based

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
99
feature enhancement, serial feature fusion and parallel feature fusion, MLE-based
hidden feature fusion, and zoning methods. Experimental results show that centroid
partition-based 4 √ó 1 nonuniform zoning of Krawtchouk moments has the highest
gesture classiÔ¨Åcation accuracy of 95.8%.
4.6.1
Future Work
Our database contains images with small rotation and illumination variations. In future,
we want to perform gesture recognition with more illumination and rotation angle varia-
tions so that we can test the performance of proposed algorithms in real-time situations.
In future, we will test with more variation in skin color complexion data. This work has
been tested only for American Sign Language digits. We would like to test proposed
gesture recognition algorithms with other data sets, such as ASL alphabets, to verify the
capability of similar-shape gesture classiÔ¨Åcation. In real-time scenarios, hand gestures
might be occluded with face or any other body parts. In future, we will work on such
occluded hand gesture images. We are also interested in using gesture recognition for
video surveillance applications such as abnormal activity recognition.
Acknowledgment
The authors would like to acknowledge the support of NIT Rourkela for capturing the
American Sign Language static hand gesture digit database.
References
1 Himanshu Rai, Maheshkumar H Kolekar, Neelabh Keshav, and JK Mukherjee. Tra-
jectory based unusual human movement identiÔ¨Åcation for video surveillance system.
Progress in Systems Engineering, pages 789‚Äì794, 2015.
2 S Padam Priyal and Prabin Kumar Bora. A robust static hand gesture recognition
system using geometry based normalizations and Krawtchouk moments. Pattern
Recognition, 46 (8): 2202‚Äì2219, 2013.
3 Ruiduo Yang, Sudeep Sarkar, and Barbara Loeding. Handling movement epenthesis
and hand segmentation ambiguities in continuous sign language recognition using
nested dynamic programming. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 32 (3): 462‚Äì477, 2010.
4 David J Sturman and David Zeltzer. A survey of glove-based input. Computer
Graphics and Applications, 14 (1): 30‚Äì39, 1994.
5 Rafael C Gonzalez, Richard E Woods, and Steven L Eddins. Digital image processing
using MATLAB. 2002.
6 Dipak Kumar Ghosh and Samit Ari. A static hand gesture recognition algorithm
using k-mean based radial basis function neural network. In International Con-
ference on Information, Communications and Signal Processing, pages 1‚Äì5‚Äì. IEEE,
2011.
7 Sauvik Das Gupta, Souvik Kundu, Rick Pandey, Rahul Ghosh, Rajesh Bag, and
Abhishek Mallik. Hand gesture recognition and classiÔ¨Åcation by discriminant and
principal component analysis using machine learning techniques. Hand, 1 (9), 2012.

100
Hybrid Intelligence for Image Analysis and Understanding
8 Subhamoy Chatterjee, Dipak Kumar Ghosh, and Samit Ari. Static hand gesture
recognition based on fusion of moments. In Intelligent Computing, Communication
and Devices, pages 429‚Äì434. Springer, 2015.
9 Deng-Yuan Huang, Wu-Chih Hu, and Sung-Hsiang Chang. Gabor Ô¨Ålter-based
hand-pose angle estimation for hand gesture recognition under varying illumination.
Expert Systems with Applications, 38 (5): 6031‚Äì6042, 2011.
10 Liu Yun, Zhang Lifeng, and Zhang Shujun. A hand gesture recognition method
based on multi-feature fusion and template matching. Procedia Engineering, 29:
1678‚Äì1684, 2012.
11 Dipak Kumar Ghosh and Samit Ari. On an algorithm for vision-based hand gesture
recognition. Signal, Image and Video Processing, pages 1‚Äì8, 2015.
12 Kinjal Basu, Radhika Nangia, and Umapada Pal. Recognition of similar shaped hand-
written characters using logistic regression. In IAPR International Workshop on Doc-
ument Analysis Systems, pages 200‚Äì204. IEEE, 2012.
13 Sukalpa Chanda, Umapada Pal, and Katrin Franke. Similar shaped part-based char-
acter recognition using g-surf. In International Conference on Hybrid Intelligent
Systems, pages 179‚Äì184. IEEE, 2012.
14 Tetsushi Wakabayashi, Umapada Pal, Fumitaka Kimura, and Yasuji Miyake. F-ratio
based weighted feature extraction for similar shape character recognition. In Inter-
national Conference on Document Analysis and Recognition, pages 196‚Äì200. IEEE,
2009.
15 Donato Impedovo and Giuseppe Pirlo. Zoning methods for handwritten character
recognition: a survey. Pattern Recognition, 47 (3): 969‚Äì981, 2014.
16 ZC Li, HJ Li, CY Suen, HQ Wang, and SY Liao. Recognition of handwritten charac-
ters by parts with multiple orientations. Mathematical and computer modelling, 35
(3): 441‚Äì479, 2002.
17 Glenn Baptista and KM Kulkarni. A high accuracy algorithm for recognition of
handwritten numerals. Pattern Recognition, 21 (4): 287‚Äì291, 1988.
18 MP Devi, T Latha, and C Helen Sulochana. Iterative thresholding based image
segmentation using 2D improved Otsu algorithm. In Global Conference on Commu-
nication Technologies, pages 145‚Äì149. IEEE, 2015.
19 Khamar Basha Shaik, P Ganesan, V Kalist, BS Sathish, and J Merlin Mary Jenitha.
Comparative study of skin color detection and segmentation in HSV and YCbCr
color space. Procedia Computer Science, 57: 41‚Äì48, 2015.
20 Harpreet Kaur Saini and Onkar Chand. Skin segmentation using RGB color model
and implementation of switching conditions. Skin, 3 (1): 1781‚Äì1787, 2013.
21 Xiaolong Teng, Bian Wu, Weiwei Yu, and Chongqing Liu. A hand gesture recog-
nition system based on local linear embedding. Journal of Visual Languages &
Computing, 16 (5): 442‚Äì454, 2005.
22 P Ganesan, V Rajini, BS Sathish, and Khamar Basha Shaik. HSV color space based
segmentation of region of interest in satellite images. In International Conference on
Control, Instrumentation, Communication and Computational Technologies, pages
101‚Äì105. IEEE, 2014.
23 Zaher Hamid Al-Tairi, Rahmita Wirza OK Rahmat, M Iqbal Saripan, and Puteri
Suhaiza Sulaiman. Skin segmentation using YUV and RGB color spaces. JIPS, 10 (2):
283‚Äì299, 2014.

Feature Extraction and Segmentation Techniques in a Static Hand Gesture Recognition System
101
24 P Peixoto, J Goncalves, and H Araujo. Real-time gesture recognition system based
on contour signatures. In International Conference on Pattern Recognition, volume 1,
pages 447‚Äì450. IEEE, 2002.
25 Maheshkumar H. Kolekar, Somnath Sengupta, and Gunasekaran Seetharaman.
Semantic concept mining based on hierarchical event detection for soccer video
indexing. Journal of Multimedia, pages 298‚Äì312, 2009.
26 Suli Zhang and Xin Pan. A novel text classiÔ¨Åcation based on Mahalanobis distance.
In International Conference on Computer Research and Development, volume 3,
pages 156‚Äì158, 2011.
27 Simon Haykin. Neural network. MacMillan, New York, 1994.
28 Simone BK Aires, Cinthia Oa Freitas, Fl√°vio Bortolozzi, and Robert Sabourin.
Perceptual zoning for handwritten character recognition. In Conference of the Inter-
national Graphonomics Society, W≈Çochy, 2005.
29 Brijesh Verma, Jenny Lu, Moumita Ghosh, and Ranadhir Ghosh. A feature extraction
technique for online handwriting recognition. In International Joint Conference on
Neural Networks, volume 2, pages 1337‚Äì1341. IEEE, 2004.
30 Jaehwa Park, Venu Govindaraju, and Sargur N Srihari. OCR in a hierarchical fea-
ture space. IEEE Transactions on Pattern Analysis and Machine Intelligence, 22 (4):
400‚Äì407, 2000.
31 Jun Cheng, Can Xie, Wei Bian, and Dacheng Tao. Feature fusion for 3D hand ges-
ture recognition by learning a shared hidden space. Pattern Recognition Letters, 33
(4): 476‚Äì484, 2012.
32 Maheshkumar H Kolekar and S Sengupta. Hidden Markov model based video index-
ing with discrete cosine transform as a likelihood function. pages 157‚Äì159, 2004.
33 Maheshkumar H Kolekar and S Sengupta. Bayesian network-based customized high-
light generation for broadcast soccer videos. IEEE Transactions on Broadcasting, 61
(2): 195‚Äì209, 2015.
34 Maheshkumar H Kolekar and D. P. Dash. A nonlinear feature based epileptic seizure
detection using least square support vector machine classiÔ¨Åer. pages 1‚Äì6, 2015.

103
5
SVM Combination for an Enhanced Prediction of Writers‚Äô
Soft Biometrics
Nesrine Bouadjenek, Hassiba Nemmour, and Youcef Chibani
Laboratoire d‚ÄôIng√©nierie des Syst√®mes Intelligents et Communicants (LISIC), Faculty of Electronics and Computer Sciences,
University of Sciences and Technology Houari Boumediene (USTHB), Algiers, Algeria
5.1
Introduction
Nowadays, handwriting recognition has been addressed by many researchers for diÔ¨Äer-
ent purposes such as digit recognition, indexation in historical documents, writer‚Äôs iden-
tiÔ¨Åcation, or soft-biometrics prediction. Any behavioral or physical characteristic that
gives some information concerning the identity of someone, but does not oÔ¨Äer decent
proof to exactly verify the identity, may be referred to as a soft-biometric attribute, such
as ethnicity, handedness, gender, age range, height, weight, eye color, scars, marks, tat-
toos, and so on [1]. In forensic analysis, incorporating soft-biometric attributes could
enhance the identiÔ¨Åcation process provided by traditional biometric identiÔ¨Åers like Ô¨Ån-
gerprint, face, iris, and voice. Over the past years, most of the published studies in the
literature have dealt with predictions of soft biometrics from face images [2‚Äì4] or from
speech signals [5‚Äì8]. From a medical perspective, numerous studies tried to clarify how
gender can manage human behavior. Specially, the gender impact has been proved in
Alzheimer‚Äôs disease [9], asthma in childhood and adolescence [10], mental health [11],
as well as crimes and violence [12, 13]. Thus, researchers in handwriting recognition
were confronted with a simple question: are gender and other soft biometrics aÔ¨Äect-
ing the handwriting style? Beech and Mackintosh showed that prenatal sex hormones
can aÔ¨Äect a woman‚Äôs handwriting when investigating the relationship between sex hor-
mones and the handwriting style in [14]. In [15], authors investigated gender and age
inÔ¨Çuence in handwriting performance in children and adolescents, where the results
showed that in the Ô¨Årst grades of primary school, females have better performances
than males. In some earlier psychological investigations, diÔ¨Äerences between male‚Äôs and
female‚Äôs handwriting were inspected [16, 17]. On the other hand, handedness prevalence
was investigated in various studies, among them its impact on deaf individuals [18], the
relationship between handedness and implicit or explicit self-esteem [19], or highlight-
ing the relationship between handedness and language dominance [20, 21]. As to age
inÔ¨Çuence over the handwriting performance, it was inspected in [22‚Äì25]; signiÔ¨Åcant
diÔ¨Äerences were found between age ranges.
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

104
Hybrid Intelligence for Image Analysis and Understanding
Automatic soft biometrics prediction formulates a modern topic of interest in the
handwritten document analysis sphere. However, only a few studies are reported in
the literature that deal with gender, handedness, age range, and nationality prediction.
The Ô¨Årst work was introduced by Cha et al. [26] in 2001. Thereafter, some works have
followed.
A prediction system consists of two essential points, which are feature generation
and classiÔ¨Åcation where it is important to wisely choose valuable methods in each step
to attain good performance. In earlier works on soft biometrics prediction, diverse
classiÔ¨Åers like SVM, neural networks, and random forests were utilized, whereas the
feature generation step was based on standard curvature, direction, and edge features.
In most handwriting recognition tasks, SVMs seem to be the rational choice as they
usually defeat other classiÔ¨Åers, like neural networks and hidden Markov models (HMM)
[27‚Äì30]. Indeed, SVMs use the spirit of the structural risk minimization, which answers
two major issues of statistical learning theory: overÔ¨Åtting and controlling the model
complexity [31]. Also, their training process can easily handle large amounts of data
without claiming for dimensionality reduction even when training examples contain
errors. Besides, in our earlier works, gender, handedness, and age range prediction
Ô¨Åndings revealed in [32‚Äì34] aÔ¨Érm that by using one SVM classiÔ¨Åer associated to
a proper feature, the combination of several systems is beaten if they operate with
powerless descriptors. Hence, a genuine approach to produce an eÔ¨Écient prediction is
to ally robust features and SVMs.
In this work, three prediction systems are developed using SVM classiÔ¨Åers associated
to various gradient and textural features: rotation-invariant uniform local binary
patterns, a histogram of oriented gradients (HOG), and gradient local binary patterns
(GLBPs). Since distinctive features proÔ¨Åt from opposed aspects of characterization,
we investigate classiÔ¨Åer combination in order to improve the prediction accuracy. In
this respect, the fuzzy integral, which has enjoyed strong success in several applica-
tions of land cover classiÔ¨Åcation [35], digit recognition [36], face recognition [37], as
well as combining document object locators [38], is used to produce a robust soft
biometrics prediction. Presently, Sugeno‚Äôs fuzzy integral and its modiÔ¨Åed form, fuzzy
min-max, are used. Two standard Arabic and English data sets are used to prove
the eÔ¨Äectiveness of using a fuzzy integral operator for improving soft biometrics
prediction.
The rest of this chapter is organized as follows: Section 5.2 presents the related back-
ground. Section 5.3 introduces the proposed system and methodologies. Experiments
are presented and discussed in Section 5.4 and Section 5.5, respectively. The main con-
clusions are given in Section 5.6.
5.2
Soft Biometrics and Handwriting Over Time
After a certain meditation on the impact of soft biometrics on handwriting, researchers
began to investigate the automatization process of predicting gender, handedness, age
range, or ethnicity from handwritten text. The Ô¨Årst work on writers‚Äô soft biometrics
predictions was published by Cha et al. [26]. The aim was to classify the US population
into various sub categories, such as white/male/age group 15‚Äì24 and white/female/age

SVM Combination for an Enhanced Prediction of Writers‚Äô Soft Biometrics
105
group 45‚Äì64, based on handwritten images of uppercase letters. Various features
such as pen pressure, writing movement, and stroke formation were used with
artiÔ¨Åcial neural networks. Experiments reveal a performance of 70.2% and 59.5%
for gender and handedness prediction, respectively. Next, boosting techniques were
employed to achieve higher performance where accuracies reached 77.5%, 86.6%,
and 74.4% for gender, age, and handedness classiÔ¨Åcation [39]. Thereafter, a research
group on computer vision and artiÔ¨Åcial intelligence at Bern University developed
the IAM handwriting data set, which is devoted to writer identiÔ¨Åcation as well as
gender and handedness prediction [40, 41]. Authors utilized a set of 29 on-line and
oÔ¨Ä-line features through a combination of support vector machines and Gaussian
mixture models (GMMs). OÔ¨Ä-line features are based on conventional structural
traits such as ascenders, descenders, and the number of points above or below the
corpus line. Reported gender prediction accuracy is about 67.57% after classiÔ¨Åer
combination. The handedness prediction using GMM classiÔ¨Åer is about 84.66%. In
[32‚Äì34], similar gender and handedness prediction experiments were conducted on
the IAM dataset by using more eÔ¨Äective oÔ¨Ä-line descriptors, which were pixel density,
pixel distribution, local binary patterns (LBPs), HOG, and GLBPs associated to SVM
classiÔ¨Åers; the prediction accuracies were about 76% and 100%, respectively. Later,
authors proposed a fuzzy min-max algorithm to combine SVM outputs trained with
pixel density, pixel distribution, and GLBP to predict writers‚Äô gender, handedness,
and age range [42]. The proposed algorithm considerably improved the prediction
accuracy.
Furthermore, Al Maadeed et al. [43] employed a k-nearest neighbors (KNN) algo-
rithm for handedness detection from oÔ¨Ä-line handwriting. A set of direction, curvature,
tortuosity, and edge-based features was used. The experiments were conducted on the
QUWI data set collected at Qatar University by asking 1017 writers to reproduce two
texts in both English and Arabic languages [44]. The prediction accuracy was about 70%.
Then, the same features were used for gender, age range, and nationality prediction
with random forest and kernel discriminant analysis classiÔ¨Åers [45]. The best predic-
tion scores were about 70% for gender prediction, 60.62% for age range prediction, and
less than 50% for nationality classiÔ¨Åcation. After that, in [46], authors proposed a fuzzy
conceptual reduction approach for handedness prediction in order to select only the
most charaterizing features among those mentioned earlier. The reported precision is
over 83.43% with a reduction of 31.1% of the feature vector dimensionality. Not long
after that, using the QUWI data set and another set of Arabic and French handwrit-
ten text, Siddiqi et al. [47] investigated gender classiÔ¨Åcation using curvature, fractal,
and textural features. The classiÔ¨Åcation was based on neural networks and SVM clas-
siÔ¨Åers. This same data set was used by Ibrahim et al. [48] for gender identiÔ¨Åcation,
comparing the performance of local and global features that are wavelet domain LBP,
gradient features, and some features provided in the ICDAR 2013 competition on gen-
der prediction. The classiÔ¨Åcation task was achieved using SVM classiÔ¨Åers. The results
showed that local gradient features outperform all other global features with a preci-
sion of 94.7%. Unfortunately, the QUWI data set is not publicly available to perform
comparison.
The inspection of all previous works reveals that predicting writers‚Äô soft biometrics
is a very complicated task, since the classiÔ¨Åcation scores are commonly around 75%. In

106
Hybrid Intelligence for Image Analysis and Understanding
this work, individual predictors based on robust data features are developed and subse-
quently combined to provide eÔ¨Äective prediction.
5.3
Soft Biometrics Prediction System
The aim of soft biometrics prediction systems is to classify writers into distinctive classes
such as ‚Äúmale‚Äù or ‚Äúfemale‚Äù for gender prediction, ‚Äúleft hand‚Äù or ‚Äúright hand‚Äù for hand-
edness prediction, and various age ranges in the case of age prediction. As to any hand-
writing recognition system, feature generation and classiÔ¨Åcation steps are needed. As
depicted in Figure 5.1, features are locally computed by applying equi-spaced grids over
text images. For each descriptor, the diÔ¨Äerent histogram cells are concatenated to form
the feature vector of the whole image. Then, SVM outputs are combined through fuzzy
integrals to aggregate a robust decision for soft biometrics prediction.
Feature
Generation
Feature
Generation
SVM
Male/Female
Gender
Handedness
or
Age range
Left/Right-handed
Age 1/Age 2/...
Figure 5.1 Soft-biometrics prediction system.

SVM Combination for an Enhanced Prediction of Writers‚Äô Soft Biometrics
107
5.3.1
Feature Extraction
5.3.1.1
Local Binary Patterns
LBP was introduced by Ojala et al., in [49] as a simple and eÔ¨Écient texture operator
that has been used in image processing, and it has been applied successfully to face
recognition [50], signature veriÔ¨Åcation [51], and texture analysis [52]. An LBP operator is
utilized to perform statistical and structural analysis of textural patterns [53]. They label
the pixels of an image by thresholding the neighborhood of each pixel (x, y) according
to the value of the central pixel taken as a threshold value, then consider the result as a
binary number. Explicitly, the LBP code is obtained by comparing the gray-level value
of the central pixel with neighboring gray levels. It takes 1 when the value of the central
pixel is bigger than the neighboring pixel. Otherwise, it takes 0. The decimal value of the
LBP code for P neighbors situated on a circle of radius R is computed as follows:
LBPP,R(x, y) =
P
‚àë
p=0
S(gp ‚àígc)2p
(5.1)
with:
S(l) =
{
1
l ‚â•0
0
l < 0
gc: gray value of the central pixel.
gp: gray value of the pth neighbor.
Commonly, the neighbors are taken by considering a circular neighborhood; conse-
quently, the pth neighbor does not belong to a pixel. Therefore, the adequate gray-level
value is computed by interpolation as [49]:
gp = I
(
x + Rsin2ùúãp
P , y ‚àíRcos2ùúãp
P
)
(5.2)
Then, image features are obtained by considering the LBP histogram, whose length is
equal to 2P.
Further extension to the original operator is the so-called rotation-invariant uniform
LBP (LBPriu), as reported in [53, 54], which allows invariance with respect to rotation.
This extension was inspired by the fact that some binary patterns occur more commonly
in texture images than others. A local binary pattern is called uniform if the binary pat-
tern contains at most two bitwise transitions from 0 to 1 or vice versa when the bit
pattern is considered circularly. It is deÔ¨Åned as follows:
LBPriu2
P,R (x, y) =
‚éß
‚é™
‚é®
‚é™‚é©
P‚àí1
‚àë
p=0
S( gp ‚àígc) U(x, y) ‚â§2
P + 1
otherwise
(5.3)
with: U(x, y) =
P
‚àë
p=1
|s(gp ‚àígc) ‚àís( gp‚àí1 ‚àígc)|, and gp = g0. Moreover, the LBPriu2
(P,R) reduces
the size of the LBP histogram to (P + 2) [53].

108
Hybrid Intelligence for Image Analysis and Understanding
5.3.1.2
Histogram of Oriented Gradients
The HOG feature was introduced for human detection by Dalal and Triggs [55]. HOG is
meant to characterize the local object apperance and the shape of objects based on the
distribution of local intensity gradients or edge directions. This descriptor has exhibited
high performance in various applications, such as face recognition [56] and handwritten
signature veriÔ¨Åcation [57]. Concretely, HOG is computed by dividing the image into
small connected regions called cells, and for the pixels within each cell, a histogram of
gradient directions is accumulated, as reported in Algorithm 5.1. Note that orientations
are selected according to the Freeman code. Besides, for each cell, the HOG histogram
is normalized to scale in the range [0, 1].
Algorithm 5.1
HOG computation
Within each cell, the HOG feature is calculated by conforming to these steps:
1. For each pixel (x,y), horizontal and vertical gradient information are computed as:
gx(x, y) = (x + 1, y) ‚àí(x ‚àí1, y)
gy(x, y) = (x, y + 1) ‚àí(x, y ‚àí1)
(5.4)
2. The gradient magnitude and phase are obtained by using the following equations:
M(x, y) =
gx(x, y)2 + gy(x, y)2
‚àö
ùúë(x, y) = arctan
(
gx(x,y)
gy(x,y)
)
(5.5)
3. Establish the histogram of gradients by accumulating magnitudes according to their
orientations.
Figure 5.2 presents an example of HOG descriptor calculation over a handwritten text
image. This Ô¨Ågure shows the HOG phase and magnitude as well as the concatenated
histograms. In this example, the considered image was divided into 3 √ó 3 cells. From the
histogram analysis, it is easy to see that HOG does not take the same magnitudes within
the diÔ¨Äerent image regions. This outcome indicates that a global histogram calculated
over the entire image cannot highlight subtle handwriting characteristics.
5.3.1.3
Gradient Local Binary Patterns
GLBPs were recently introduced for human detection by Jiang et al. [58]. In a GLBP,
gradient information and texture information are combined. Its principle idea consists
of exploiting uniform LBPs to compute the histogram of oriented gradients. Presently,
we investigate its eÔ¨Éciency for handwritten text characterization.
Thus, for a given cell, a GLBP table is settled as explained in Algorithm 5.2. First, the
LBP code is calculated, then neighbor pixels with the same binary value stick together to
get several ‚Äú1‚Äù areas and ‚Äú0‚Äù areas so that only uniform patterns are considered. Recall
that, when the ‚Äú1‚Äù area and ‚Äú0‚Äù area appear only once, the pattern is called uniform.
The size of the GLBP table is deÔ¨Åned by all possible angle and width values. Precisely,
there are eight possible Freeman directions or angle values, while the number ‚Äú1‚Äù in the
uniform patterns can vary from 1 to 7. This yields a 7 √ó 8 GLBP table in which gradient
values are accumulated. At last, the GLBP table constituting the histogram is exploded
into a vector of 56 elements. The Ô¨Çow of calculation is shown in Figure 5.3.

SVM Combination for an Enhanced Prediction of Writers‚Äô Soft Biometrics
109
0
0
10
20
30
40
(a) Original handwriting text
(b) Gradient magnitude
(c) Gradient phase
(d) Histogram of gradients
50
60
70
80
90
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figure 5.2 HOG feature calculated on a handwritten text image.
5.3.2
ClassiÔ¨Åcation
The soft biometrics prediction step is based on SVMs, which are machine learning tools
used to solve real word classiÔ¨Åcation problems such as image classiÔ¨Åcation and text
analysis. SVMs are binary classiÔ¨Åers, given a set of training examples, each labeled for
belonging to one of two classes; the SVM training algorithm builds a model that assigns
new examples into one class or the other, by creating an optimal linear separating hyper-
plane between two classes [59]. Explicitly, let (kn, cn)ùúñRM √ó {¬±1} a set of training samples
so that M corresponds to data dimension {n = 1, ‚Ä¶ , Nc}, and Nc is the number of sam-
ples per a class c. SVM training selects the function f , which maximizes the margin

110
Hybrid Intelligence for Image Analysis and Understanding
Algorithm 5.2
How to compute GLBP for a given cell
For each pixel:
1. Calculate the LBP code and consider only uniform patterns.
2. Generate the width and angle values such that:
‚Äì The width corresponds to the number ‚Äú1‚Äù in the LBP code.
‚Äì The angle is the Freeman direction of the middle pixel within the‚Äú1‚Äù area in the LBP
code.
3. Calculate the gradient value on the 1 to 0 (or 0 to 1) transitions in the LBP code using
the value of the original pixel and the value of its neighbors.
4. Width and angle values are used for mapping the position within the GLBP table in
which gradient values are accumulated.
Figure 5.3 GLBP feature extraction for each pixel.
between the two classes by minimizing an upper bound on the generalization error [31].
Then, data are classiÔ¨Åed according to:
f (k) = sign
( SV
‚àë
i=1
ùõΩiqiK(ki, k) + b
)
(5.6)

SVM Combination for an Enhanced Prediction of Writers‚Äô Soft Biometrics
111
where b is the bias, qi is the lass label, ùõΩi is the Lagrange multipliers, and SV is the number
of support vectors
As most data sets are not linearly separable in their given form, explicit mapping is
needed to get linear learning algorithms to learn a nonlinear function or decision bound-
ary. Kernels provide a way around this problem by providing a way to calculate the dot
product of two vectors without explicitly mapping them to a speciÔ¨Åc higher dimension.
There are four popular SVM kernels in the literature [31], namely, linear, polynomial,
quadratic, and radial basis function (RBF), but the latter is the most promoted for pat-
tern recognition. This kernel is described as follows:
K(ki, k) = exp
(
‚àí1
2ùúé2 ‚Äñki ‚àík‚Äñ2)
(5.7)
where ùúé: user-deÔ¨Åned parameter.
5.3.3
Fuzzy Integrals‚ÄìBased Combination ClassiÔ¨Åer
The prediction performance relies essentially on the feature extraction that assists SVM
to discriminate between writers. Indeed, distinctive features proÔ¨Åt from discriminative
power, providing more complementary information to the other feature, which leads to
eminent diversity between classiÔ¨Åers [51]. Hence, the combination of such predictors
can improve the prediction accuracy. Recall that classiÔ¨Åer combination was introduced
for soft biometrics prediction in [41], by using classical max, min, and average rules. The
investigation was conducted for gender prediction by combining GMMs. The prediction
accuracy was improved from 64.25% to 67.57%. Presently, the intention is to combine
diÔ¨Äerent SVM decisions in order to aggregate more accurate predictions using diÔ¨Äerent
fuzzy integral operators. The main reason behind the use of fuzzy logic operators is that
they allow modeling a priori knowledge about individual predictors‚Äô pertinence through
fuzzy measure operators. Recall that fuzzy measure is a set of function A that certiÔ¨Åes
the following properties [35]:
‚Äì g(Œ¶) = 0
‚Äì g(Z) = 1
‚Äì g(Zi) ‚â§g(Zj) if Zi ‚äÇZj
5.3.3.1
gùõåFuzzy Measure
The so-called gŒª fuzzy measure satisÔ¨Åes the following properties [35]: Let Z = {Zi}i=1‚à∂N
formulate the set of classiÔ¨Åers (SVM), while g(Zi) refers to their performances. Because
of the nature of fuzzy measures, Sugeno stated that the fuzzy measure for the union of
two classiÔ¨Åers does not correspond to the sum of individual fuzzy measures. To over-
come this limitation, he proposed the Œª fuzzy measure that expresses the degree of
interaction between two classiÔ¨Åers Zi and Zj as:
g(Zi ‚à™Zj) = g(Zi) + g(Zj) + Œªg(Zi)g(Zj)
(5.8)
For each class, Œª is the unique nonzero root of equation (5.9) that belongs in the interval
[‚àí1, ‚Ä¶ , +‚àû[.
Œª + 1 =
N
‚àè
i=1
(1 + Œªg(Z¬±
i ))
(5.9)

112
Hybrid Intelligence for Image Analysis and Understanding
SVM outputs are converted into membership degrees in the two classes of interest by
adjusting the membership model proposed in [35]. SpeciÔ¨Åcally, the decision of each
SVM Zi is converted into membership degrees h+(Zi) and h‚àí(Zi) in both positive and
negative classes. Recall that, in theory, SVM outputs are deÔ¨Åned by values that are at least
greater than 1 for the positive class and at max equal ‚àí1 for the negative class. Thus, if
the absolute value of the SVM decision is greater than 1, the sample is said to entirely
belong to one of the classes, and its membership degree for the respective class equals 1
and that of the other class is 0. On the contrary, if the SVM decision belongs in the sepa-
rating margin ]‚Äì1, +1[, the decision is confused and the sample can belong to each class
according to complementary membership degrees as shown in Algorithm 5.3. Then, the
set of SVM is rearranged such that the following relation holds: h(Z1) ‚â•... ‚â•h(ZN) ‚â•0.
We obtain an ascending sequence of SVM Ai = Z1, ‚Ä¶ , Zi, whose fuzzy measures are
constructed as:
g(A1) = g(Z1)
(5.10)
g(Ai) = g(Ai‚àí1 ‚à™Zi)
g(Ai) = g(Ai‚àí1) + g(Zi) + ùúÜg(Ai‚àí1)g(Zi)
(5.11)
It is important to stress that equation (5.11) allows to construct the fuzzy measures in
order to provide the weight of a single SVM classiÔ¨Åer. Though there is no proper rule to
follow for the attribution of g(Zi) values, in fact, they can be subjectively assigned by an
expert or computed from the training data [35]. Presently, g(Zi) of the SVM predictor
in both negative and positive classes is derived from the training accuracy. Let (t+
i , t‚àí
i )
be the training accuracy of the SVM Zi in the two classes. These accuracies are handled
through a weighted soft-max function [42], such that:
g(Z¬±
i ) = ùõº
1 + exp(t¬±
i )
‚àëN
i=1[1 + exp(t¬±
i )]
(5.12)
where N is the number of trained SVMs, while the weight parameter ùõºscales in the range
]0.1, 1] to control the importance assigned to the fuzzy measure. It is experimentally set
to allow the best training accuracy. Thereafter, Sugeno‚Äôs fuzzy integral and the fuzzy
min-max algorithm are computed to aggregate the Ô¨Ånal prediction.
Algorithm 5.3
Fuzzy membership degrees calculation
If Zi ‚â•1 then:
{
h+(Zi) = Zi
h‚àí(Zi) = 0
Else:
{
If Zi ‚â§‚àí1 then:
{
h+(Zi) = 0
h‚àí(Zi) = Zi
Else:
{
h+(Zi) = (1 + Zi)‚àï2
h‚àí(Zi) = (1 ‚àíZi)‚àï2
}

SVM Combination for an Enhanced Prediction of Writers‚Äô Soft Biometrics
113
5.3.3.2
Sugeno‚Äôs Fuzzy Integral
Sugeno‚Äôs integral IS, of a function h: Z ‚Üí[0, 1] with respect to a fuzzy measure g over
Z, is computed for each class ¬± by:
I¬±
S = max[min(h¬±(Zi), g¬±(Ai)]i=1‚à∂N
(5.13)
5.3.3.3
Fuzzy Min-Max
The fuzzy min-max algorithm of a function h : Z ‚Üí[0, 1] with respect to a fuzzy measure
g over Z takes advantage from the Ô¨Çexibility and eÔ¨Äectiveness of fuzzy logic operators
and is computed for each class ¬± as [42]:
Fuzzy Min ‚àíMax¬± = min[max(h¬±(Zi), g¬±(Ai)]i=1‚à∂N
(5.14)
5.4
Experimental Evaluation
In order the evaluate the proposed methods, two corpuses extracted from two public
data sets were used. The samples were collected in a multiscript unconstrained writing
environment. These data sets, namely, IAM and KHATT, contain handwritten sentences
in English and Arabic languages, respectively.
5.4.1
Data Sets
5.4.1.1
IAM Data Set
The IAM On-Line Handwriting Database was developed by a research group on com-
puter vision and artiÔ¨Åcial intelligence at Bern University in Switzerland.1 The database
was collected by the contribution of more than 200 writers, each of them participating
with eight English texts constituting an average of seven lines. The data acquisition was
carried out using a whiteboard. The database is presented in a form of handwritten sen-
tences, where each sentence is indexed according to the writer‚Äôs gender, handedness,
age, and educational level. The number of collected samples for the prediction of soft
biometrics is performed according to the availability of data in each subcategory within
the database. Figure 5.4 presents some samples from this data set.
Presently, according to the Ô¨Årst work published on automatic gender and handedness
prediction using IAM data set [40, 41], a Ô¨Årst corpus (IAM-1) was collected. This corpus
is selected by taking only one sample from each writer. For gender prediction, 75 samples
Figure 5.4 IAM data set samples.
1 http://www.iam.unibe.ch/fki

114
Hybrid Intelligence for Image Analysis and Understanding
per class were randomly chosen and divided into 40 samples for training, 10 samples for
validation, and 25 samples for testing. As to handedness prediction, unfortunately the
database contains only 20 samples for the left-handed class. So for both classes, 15 sam-
ples were assigned to the training stage, while the remaining 5 samples were utilized to
test the prediction performance. In a second step, the investigation was extended to a
larger data set (IAM-2) to get a deeper analysis on gender and age classiÔ¨Åcation. Pre-
cisely, 165 samples per class are collected for gender prediction. On the other hand,
two main age ranges are available, 25‚Äì34 years and 35‚Äì56 years, to perform age predic-
tion, where 84 samples were selected for each class. As for most of the classiÔ¨Åcation and
data-mining tasks [60] as well as the soft biometrics state of the art [45], two-thirds of
samples were used for the training step, while the remaining one-third were used for
testing the system.
5.4.1.2
KHATT Data Set
Not that long ago, Mahmoud et al. [61, 62] published a new Arabic data set, namely,
KHATT (KFUPM Handwritten Arabic TexT), designed to serve research in Arabic
handwritten text recognition, Arabic writer identiÔ¨Åcation and veriÔ¨Åcation, forms
analysis, and segmentation of paragraphs, lines, words, subwords, and characters.2 The
database is composed of 1000 forms that cover all the shapes of Arabic characters,
Ô¨Ålled by 1000 writers from 18 diÔ¨Äerent countries. Each writer has participated with one
handwritten form segmented into paragraphs, then into handwritten text lines. This
data set was collected by considering gender, qualiÔ¨Åcation, handedness, age category,
and nationality, but up to now it was not used for soft biometrics prediction. Figure 5.5
presents some examples from this data set.
To perform gender, handedness, and age range prediction, three corpuses were ran-
domly selected. SpeciÔ¨Åcally, 90 training samples and 45 test samples were collected per
class for both gender and age range applications. Note that two age ranges are consid-
ered, which are ‚Äú16 to 25 years.‚Äù and ‚Äú26 to 50 years‚Äù. The handedness corpus is composed
of 84 samples for both right-hand and left-hand classes, divided as 56 training samples
and 28 test samples.
5.4.2
Experimental Setting
An ideal implemation of SVM classiÔ¨Åers requires an optimal parameters selection. First,
four popular kernels available in the literature, namely, linear, polynomial, quadratic,
and RBF, were tested. We investigate the best choice of SVM kernel before proceeding to
Figure 5.5 KHATT data set samples.
2 http://khatt.ideas2serve.net

SVM Combination for an Enhanced Prediction of Writers‚Äô Soft Biometrics
115
Table 5.1 InÔ¨Çuence of SVM kernels for gender prediction on the
IAM-1 corpus (%)
Linear
Polynomial
Quandratic
RBF
66.00
62.00
62.00
72.00
the experiments. Tests were performed using the pixel density feature, which is the ratio
between the number of pixels that belong to the text and the cell‚Äôs size. The regularization
parameter was varied in the interval [0.01:10:200]. The degree value of the polynomial
kernel was varied from 1 to 3, while the RBF kernel sigma was varied in the range [2:2:80].
Then, the couple giving the best training performance was selected in each test. Results
obtained in terms of accuracy for gender prediction on the IAM-1 corpus are given in
Table 5.1. As can be seen, the RBF kernel provides the best accuracy (i.e., 72%), exceeding
other kernels with at least 6%.
After establishing that the RBF kernel is the best choice for us, experiments on the
IAM-1 corpus were conducted on the global image to optimize the performance of the
LBP by choosing the type of the operator, the number of considered neighbors P, as well
as the radius of the neighborhood R. The obtained results are shown in Figure 5.6. As can
be seen, the LBPriu allows an improvement of at least 2% over the classical operator. Also,
LBPriu
8,1 provides the best accuracy compared to the other conÔ¨Ågurations. Hence, this
conÔ¨Åguration is utilized for the rest of the experiments. Moreover, as claimed in [51], the
appropriate grid size depends on the feature and the database that are used. Therefore,
several grid size conÔ¨Ågurations were tested to Ô¨Ånd the optimal one by allowing the best
training accuracy. For LBPriu
8,1 and HOG features, the number of cells was experimentally
varied from 1 √ó 1 cell (which corresponds to the whole image) to 5 √ó 10 cells and from
2 √ó 2 to 6 √ó 6, respectively. For GLBP, which returns a 56-element feature vector in each
cell, the grid size was varied from 1 √ó 1 to 4 √ó 8 cells. Findings revealed that the number
of grid cells has a meaningful impact over the prediction performance. Figure 5.7 plots
the most relevant outcomes obtained using the diÔ¨Äerent features. It is easy to see that
58
4 √ó 1
Classification accuracy (%)
8 √ó 1
(PxR)
16 √ó 2
LBPriu
LBP
58
64
62
60
58
Figure 5.6 LBP operators; performances for gender prediction on the IAM-1 corpus.

116
Hybrid Intelligence for Image Analysis and Understanding
1√ó1
1√ó2
1√ó3
1√ó4
1√ó5
1√ó6
1√ó7
2√ó1
2√ó2
2√ó3
2√ó4
2√ó6
2√ó7
2√ó8
3√ó1
3√ó2
3√ó3
3√ó4
3√ó5
3√ó6
3√ó7
4√ó1
4√ó2
4√ó3
4√ó4
4√ó5
4√ó6
4√ó7
4√ó8
75
70
65
Classification accuracy (%)
Classification accuracy (%)
60
50
55
80
75
70
Classification accuracy (%)
65
55
60
80
70
50
60
40
10
0
20
30
Number of cells
(a) LBPriu 8√ó1
Number of cells
(b) HOG
Number of cells
(c) GLBP
1√ó1
2√ó2
2√ó3
2√ó4
2√ó5
2√ó6
3√ó2
3√ó3
3√ó4
3√ó5
3√ó6
4√ó2
4√ó3
4√ó4
4√ó5
4√ó6
5√ó2
5√ó3
5√ó4
5√ó5
5√ó6
6√ó2
6√ó3
6√ó4
6√ó5
6√ó6
1√ó3
1√ó5
1√ó7
1√ó9
2√ó1
2√ó3
2√ó5
2√ó7
2√ó9
3√ó1
3√ó3
3√ó5
3√ó7
3√ó9
4√ó1
4√ó3
4√ó5
4√ó7
4√ó9
5√ó1
5√ó3
5√ó5
5√ó7
5√ó9
Figure 5.7 InÔ¨Çuence of the grid size for gender prediction on the IAM-1 corpus.

SVM Combination for an Enhanced Prediction of Writers‚Äô Soft Biometrics
117
the best performance of LBPriu
8,1 is about 74% using a grid of 5 √ó 8 cells and allowing a gain
of 10% over the LBPriu
8,1 computed on the global image. As to the HOG feature, the best
accuracy (i.e., 72%) is obtained using a grid of 2 √ó 3 cells, while GLBP precision (76%) is
obtained with a grid of 1 √ó 7 cells. From this analysis, one can say that the grid size has
indeed an inÔ¨Çuential eÔ¨Äect on the prediction performance. Analogous experimentation
was carried out for each application, and since similar behavior has been noticed, only
gender prediction results using the IAM-1 data set are given.
5.4.3
Gender Prediction Results
Table 5.2 summarizes gender prediction results. As can be seen, GLBP exceeds LBP8,1
riu
and HOG for the IAM-1 and IAM-2 corpuses. On the contrary, for the KHATT cor-
pus, LBP8,1
riu gives the best performance (i.e., 75.56%), beating GLBP precision with more
than 1% . Furthermore, the performance of fuzzy combination rules is evaluated com-
paratively to classical mean and max rules. As reported in Table 5.3, the proposed fuzzy
min-max combination outperforms Sugeno‚Äôs integral. Compared to individual systems,
the advanced combination rule allowed a gain of 6%, 1.19%, and 7.77% for the IAM-1,
IAM-2, and KHATT corpuses, respectively.
5.4.4
Handedness Prediction Results
Similarly to gender prediction experiments, the Ô¨Årst step was initially focused on setting
the parameters mentioned in this chapter. Table 5.4 reports the best results in terms of
prediction accuracy. For this task, recall that the IAM-1 corpus contains only 20 samples
Table 5.2 Results of individual systems for gender prediction (%)
Corpus
LBP8,1
riu
HOG
GLBP
IAM-1
74.00
72.00
76.00
IAM-2
72.72
74.45
75.45
KHATT
75.56
68.89
74.44
Table 5.3 Results of combination systems for gender prediction (%)
Corpus
Max
Mean
Is
Fuzzy min-max
IAM-1
76.00
76.00
78.00
82.00
IAM-2
71.11
73.33
75.55
76.64
KHATT
77.78
70.00
83.33
83.33
Table 5.4 Results of handedness prediction for individual systems (%)
Corpus
LBP8,1
riu
HOG
GLBP
IAM-1
100.00
100.00
100.00
KHATT
75.00
83.93
78.57

118
Hybrid Intelligence for Image Analysis and Understanding
Table 5.5 Results of combination systems for handedness
prediction (%)
Corpus
Max
Mean
Is
Fuzzy min-max
KHATT
76.79
76.79
82.14
83.93
Table 5.6 Results of handedness prediction for individual systems (%)
Corpus
LBP8,1
riu
HOG
GLBP
IAM-2
69.64
73.21
69.64
KHATT
70.00
67.78
70.00
Table 5.7 Results of combination systems for age prediction (%)
Corpus
Max
Mean
Is
Fuzzy min-max
IAM-2
71.42
64.29
73.21
75.79
KHATT
62.22
77.78
78.89
75.56
for each class; for this reason, the prediction scores are high and reach 100% with all fea-
tures. As to the KHATT corpus, the best precision (i.e., 83.93%) is obtained using HOG
features. Furthermore, the combination step was limited to the KHATT data set since for
IAM-1, the optimal performance was provided by all individual systems. Results of the
diÔ¨Äerent combination rules are reported in Table 5.5. As can be seen, a fuzzy min-max
combination provides the best precision (i.e., 83.93%), but they did not provide any gain
comparatively to the individual system trained on HOG features.
5.4.5
Age Prediction Results
Table 5.6 gives the results of age range prediction, while those obtained for the combi-
nation step are reported in Table 5.7. Recall that, for this experiment, two diÔ¨Äerent age
ranges were considered in the IAM-2 and KHATT corpuses. For the IAM-2 corpus, the
HOG feature outperforms the others with more than 3.5%, while for the KHATT cor-
pus, the best precision (i.e., 70.00%) is obtained using both LBP8,1
riu and GLBP features.
Also, by combining these systems, the prediction accuracies are improved to more than
75.79% with the fuzzy min-max for the IAM-2 corpus. On the contrary, Sugeno‚Äôs inte-
gral gives the best improvement for the KHATT corpus with a gain that exceeds 8.5%
over individual systems.
5.5
Discussion and Performance Comparison
This chapter focused on developing a soft biometrics prediction system using a fuzzy
integral‚Äìbased combination method to ensure robust gender, handedness, and age
range prediction from handwriting analysis. In fact, as reported in Table 5.8, only a few
research works are available in the literature and are unfortunately using private data

Table 5.8 State-of-the-art results
Soft-
biometrics
Reference
Data set
# Training
data
# Test
data
Features
ClassiÔ¨Åer
Precision
rate (%)
Gender
[63]
English + Urdu
30
Human performance
67.84
[45]
QUWI
‚Äî
‚Äî
Direction + curvature + tortuosity + chain code
Kernel discriminant
analysis
73.70
[47]
QUWI Arabic
300
100
Slant + curvature+ LBP
Neural network
71.00
[47]
QUWI English
300
100
Slant + curvature
SVM
70.00
[47]
MSHD French
42
42
Slant + curvature
SVM
68.25
[47]
MSHD Arabic
42
42
LBP
SVM
74.20
[48]
QUWI
282
Local gradient-based features
SVM
94.7
[41]
IAM-1
80
50
OÔ¨Ä-line+on-line
GMM
67.57
[41]
IAM-1
24
Human performance
63.88
Proposed
IAM-1
80
50
Fuzzy min-max( LBP8,1
riu + HOG + GLBP)
82.00
IAM-2
220
110
Fuzzy min-max( LBP8,1
riu + HOG + GLBP)
77.78
KHATT
180
90
Fuzzy min-max( LBP8,1
riu + HOG + GLBP)
84.44
Handedness
[43]
QUWI
‚Äî
‚Äî
Direction + curvature +
tortuosity + chain code
KNN
70.00
[46]
QUWI
121
Fuzzy conceptual reduction
KNN
83.43%
[40]
IAM-1
30
10
OÔ¨Ä-line+on-line
GMM
86.64
[40]
IAM-1
20
Human performance
62.00
[34]b
IAM-1
30
10
GLBP
SVM
100.00
Proposed
KHATT
112
56
Fuzzy min-max( LBP8,1
riu + HOG + GLBP)
83.93
Age
[45]
QUWI
‚Äî
‚Äî
Direction + curvature + tortuosity
Random forest
62.40
Proposed
IAM-2
112
56
Fuzzy min-max( LBP8,1
riu + HOG + GLBP)
76.78
KHATT
180
90
Fuzzy min-max( LBP8,1
riu + HOG + GLBP)
78.89

120
Hybrid Intelligence for Image Analysis and Understanding
sets, which does not promote a decent comparison. However, from methods analysis,
the superiority of fuzzy integral‚Äìbased prediction systems can easily be deduced.
This fact can be explained by the incorporation of LBP features that capture the local
structure information, along with HOG features that extract gradient information
according to a given orientation, and GLBP features that combine texture and gradient
information.
‚Äì From all experiments, we can say that for soft biometrics prediction, uniform LBP
operators perform better than the classical LBP, specially with the (P = 8, R = 1) con-
Ô¨Åguration, which allowed at least a gain of 2%. Furthermore, computing the proposed
features locally by applying equispaced grids over text images allows to get local infor-
mation of the image content. The obtained Ô¨Åndings highlighted the impact of the grid
size on the reliability of the feature characterization. Precisely, for LBP8,1
riu, the local
calculation allowed a considerable gain of 10%. Prediction accuracies given by indi-
vidual systems vary between 52% and 76%. From the inspection of all data set results,
one can say that the three SVM predictors give satisfying performances, but there
is no descriptor that provides the best discriminative power for SVM. According to
the theory, such diÔ¨Äerences can be explored through a combination framework to
improve performances.
‚Äì Arabic is written cursively and from the right to the left, where letters are normally
connected to the baseline with horizontal strokes, compared to English handwrit-
ing where in general each character is connected to the next character with diagonal
strokes and is written from the left to the right. Also, Arabic language has its speciÔ¨Åc
diacritical marking to represent vowels such as dumma (‚Äô), hamza ( ), or chadda ( ).
Regardless of all these diÔ¨Äerences between the two languages, results on both data
sets are typically in the same range.
‚Äì Fuzzy integral‚Äìbased combination results reveal promising results on both the IAM
and KHATT databases in term of accuracy. Indeed, the proposed combination rules
outperform individual prediction systems where the improvement varies from 1 to 8%
for the diÔ¨Äerent data sets. Also, they give comparable results in most cases, because
they share the same computation concept.
5.6
Conclusion
This chapter proposed the use of fuzzy integrals as a combination classiÔ¨Åer method, to
improve the prediction of soft biometrics traits from handwriting. Firstly, three SVM
predictors paired to three distinct data features were developed to achieve writers‚Äô
gender, handedness, and age range predictions. Following that, SVM decisions were
combined to enhance the prediction performance. Exhaustive experiments carried
out on two diÔ¨Äerent English and Arabic handwritten corpuses indicated that the
aforementioned combination methods enhence the prediction accuracy. Besides, the
proposed approach has proven itself compared to the diÔ¨Äerent combination rules as
well as the state of the art. As the results have shown, the combination process handles
SVM outputs supplied with relevant information, which indicates that any kind of
data features can be used. To enhance the results more, we plan in a future work to
investigate new classiÔ¨Åers such as artiÔ¨Åcial immune recognition systems (AIRSs) and

SVM Combination for an Enhanced Prediction of Writers‚Äô Soft Biometrics
121
convolutional neural networks (CNNs). Also, it would be interesting to develop a
multiclass prediction system of various soft biometrics traits.
References
1 Karthik Nandakumar and Anil K. Jain. Soft Biometrics, pages 1235‚Äì1239. Springer
US, Boston, MA, 2009.
2 Yasmina Andreu, Pedro Garca-a-Sevilla, and R. A. Mollineda. Face gender classiÔ¨Åca-
tion: a statistical study when neutral and distorted faces are combined for training
and testing purposes. Image and Vision Computing, 32 (1):27‚Äì36, 2014.
3 Di Huang, Huaxiong Ding, Chen Wang, Yunhong Wang, Guangpeng Zhang, and
Liming Chen. Local circular patterns for multi-modal facial gender and ethnicity
classiÔ¨Åcation. Image and Vision Computing, 32 (12):1181‚Äì1193, 2014.
4 Grigory Antipov, Sid-Ahmed Berrani, and Jean-Luc Dugelay. Minimalistic
CNN-based ensemble model for gender prediction from face images. Pattern Recog-
nition Letters, 70:59‚Äì65, 2016.
5 R. Djemili, H. Bourouba, and M.C.A. Korba. A speech signal based gender identiÔ¨Å-
cation system using four classiÔ¨Åers. In Multimedia Computing and Systems (ICMCS),
2012 International Conference, pages 184‚Äì187, May 2012.
6 Vinay S. Gupta, and A. Mehra. Gender speciÔ¨Åc emotion recognition through speech
signals. In Signal Processing and Integrated Networks (SPIN), 2014 International
Conference, pages 727‚Äì733, February 2014.
7 I. Bisio, A. DelÔ¨Åno, F. Lavagetto, M. Marchese, and A. Sciarrone. Gender-driven
emotion recognition through speech signals for ambient intelligence applications.
IEEE Transactions on Emerging Topics in Computing, 1 (2):244‚Äì257, December 2013.
8 M. Gomathy, K. Meena, and K. R. Subramaniam. ClassiÔ¨Åcation of speech signal
based on gender: a hybrid approach using neuro-fuzzy systems. Int. Journal of
Speech Technology, 14 (4):377‚Äì391, December 2011.
9 Jose Vina and Ana Lloret. Why women have more Alzheimer‚Äôs disease than men:
gender and mitochondrial toxicity of amyloid-beta peptide. Journal of Alzheimers
Disease, 20:527‚Äì533, 2010.
10 C. Almqvist, M. Worm, B. Leynaert, and for the working group of GA2LEN WP
2.5 Gender. Impact of gender on asthma in childhood and adolescence: a GA2LEN
review. Allergy, 63 (1):47‚Äì57, 2008.
11 Andrea R. Ennis, Peter McLeod, Margo C. Watt, Mary Ann Campbell, and Nicole
Adams-Quackenbush. The role of gender in mental health court admission and
completion. Canadian Journal of Criminology and Criminal Justice, 58 (1):1‚Äì30,
2016.
12 Sarah Bennett, David P. Farrington, and L. Rowell Huesmann. Explaining gender dif-
ferences in crime and violence: The importance of social cognitive skills. Aggression
and Violent Behavior, 10 (3):263‚Äì288, 2005.
13 Rachael E. Collins. The eÔ¨Äect of gender on violent and nonviolent recidivism: A
meta-analysis. Journal of Criminal Justice, 38 (4):675‚Äì684, 2010.
14 John R. Beech and Isla C. Mackintosh. Do diÔ¨Äerences in sex hormones aÔ¨Äect hand-
writing style? Evidence from digit ratio and sex role identity as determinants of the
sex of handwriting. Personality and Individual DiÔ¨Äerences, 39:459‚Äì468, July 2005.

122
Hybrid Intelligence for Image Analysis and Understanding
15 M. Genna and A. Accardo. Gender and age inÔ¨Çuence in handwriting performance in
children and adolescents. In 5th European Conference of the International Federation
for Medical and Biological Engineering: 14‚Äì18 September 2011, Budapest, Hungary,
pages 141‚Äì144. Springer, Berlin, 2012.
16 James Hartley. Sex diÔ¨Äerences in handwriting: a comment on Spear. British Educa-
tional Research Journal, 17 (2):141‚Äì145, 1991.
17 W.N. Hayes. Identifying sex from handwriting. Perceptual and Motor Skills, 83 (3 Pt
1):791‚Äì800, Dec 1996.
18 Marietta Papadatou-Pastou and Anna Safar. Handedness prevalence in the deaf:
Meta-analyses. Neuroscience & Biobehavioral Reviews, 60:98‚Äì114, 2016.
19 Quanlei Yu, Qiuying Zhang, Shenghua Jin, Jianwen Chen, Yingjie Han, and Huimi
Cao. The relationship between implicit and explicit self-esteem: the moderating
eÔ¨Äect of handedness. Personality and Individual DiÔ¨Äerences, 89:1‚Äì5, 2016.
20 S. Knecht, B. Dr√§ger, M. Deppe, L. Bobe, H. Lohmann, A. Fl√∂el, E.-B. Ringelstein,
and H. Henningsen. Handedness and hemispheric language dominance in healthy
humans. Brain, 123 (12):2512‚Äì2518, 2000.
21 Abdulaziz Al-Musa Alkahtani. The inÔ¨Çuence of right or left handedness on the abil-
ity to simulate handwritten signatures and some elements of signatures: a study of
Arabic writers. Science & Justice, 53 (2):159‚Äì165, 2013.
22 I.C. Friesen, R.A. Dixon, and D. Kurzman. Handwriting performance in younger
and older adults: age, familiarity, and practice eÔ¨Äects. Psychology and Aging, 8
(5):360‚Äì370, September 1993.
23 N.A. Lannin, N. van Drempt, and A. McCluskey. A review of factors that inÔ¨Çu-
ence adult handwriting performance. Australian Occupational Therapy Journal, 58
(5):321‚Äì328, October 2011.
24 A. McCluskey and D.K. Burger. Australian norms for handwriting speed in healthy
adults aged 60‚Äì99 years. Australian Occupational Therapy Journal, 58 (5):355‚Äì363,
October 2011.
25 Sara Rosenblum, Batya Engel-Yeger, and Yael Fogel. Age-related changes in executive
control and their relationships with activity performance in handwriting. Human
Movement Science, 32 (2):363‚Äì376, 2013.
26 S.H. Cha and S.N. Srihari. A priori algorithm for sub-category classiÔ¨Åcation analysis
of handwriting. In International Conference on Document Analysis and Recognition,
pages 1022‚Äì1025, Seattle, WA, September 2001.
27 Edson J.R. Justino, Fl√°vio Bortolozzi, and Robert Sabourin. A comparison of SVM
and HMM classiÔ¨Åers in the oÔ¨Ä-line signature veriÔ¨Åcation. Pattern Recognition Letters,
26 (9):1377‚Äì1385, July 2005.
28 E. Frias-Martinez, A. Sanchez, and J. Velez. Support vector machines versus
multi-layer perceptrons for eÔ¨Écient oÔ¨Ä-line signature recognition. Engineering
Applications of ArtiÔ¨Åcial Intelligence, 19 (6):693‚Äì704, 2006.
29 Parveen Kumar, Nitin Sharma, and Arun Rana. Handwritten character recognition
using diÔ¨Äerent kernel based SVM classiÔ¨Åer and MLP neural network (a comparison).
International Journal of Computer Applications, 53 (11):25‚Äì31, September 2012.
30 Chayaporn Kaensar. A comparative study on handwriting digit recognition classiÔ¨Åer
using neural network, support vector machine and k-nearest neighbor. In The 9th
International Conference on Computing and Information Technology (IC2IT2013):

SVM Combination for an Enhanced Prediction of Writers‚Äô Soft Biometrics
123
9th‚Äì10th May 2013 King Mongkut‚Äôs University of Technology North Bangkok, pages
155‚Äì163. Springer, Berlin, 2013.
31 Christopher J.C. Burges. A tutorial on support vector machines for pattern recogni-
tion. Data Mining and Knowledge Discovery, 2 (2):121‚Äì167, 1998.
32 Nesrine Bouadjenek, Hassiba Nemmour, and Youcef Chibani. Local descriptors to
improve oÔ¨Ä-line handwriting-based gender prediction. In 6th International Con-
ference of Soft Computing and Pattern Recognition (SoCPaR), 2014, pages 43‚Äì47,
Tunisia, August 2014.
33 Nesrine Bouadjenek, Hassiba Nemmour, and Youcef Chibani. Age, gender and hand-
edness prediction from handwriting using gradient features. In Document Analysis
and Recognition (ICDAR), 2015 13th International Conference, pages 1116‚Äì1120,
August 2015.
34 Nesrine Bouadjenek, Hassiba Nemmour, and Youcef Chibani. Histogram of oriented
gradients for writer‚Äôs gender, handedness and age prediction. In Innovations in Intel-
ligent SysTems and Applications (INISTA), 2015 International Symposium, pages
220‚Äì224, September 2015.
35 Hassiba Nemmour and Youcef Chibani. Multiple support vector machines for land
cover change detection: an application for mapping urban extensions. {ISPRS} Jour-
nal of Photogrammetry and Remote Sensing, 61 (2):125‚Äì133, 2006.
36 A. Gattal, Y. Chibani, B. Hadjadji, H. Nemmour, I. Siddiqi, and C. Djeddi.
Segmentation-veriÔ¨Åcation based on fuzzy integral for connected handwritten digit
recognition. In Image Processing Theory, Tools and Applications (IPTA), 2015 Inter-
national Conference, pages 588‚Äì591, November 2015.
37 Gabriela E. Martinez, Patricia Melin, Olivia D. Mendoza, and Oscar Castillo. Face
recognition with a Sobel edge detector and the Choquet integral as integration
method in a modular neural networks. In Design of Intelligent Systems Based on
Fuzzy Logic, Neural Networks and Nature-Inspired Optimization, pages 59‚Äì70.
Springer, Cham, 2015.
38 Jung Soh. Computational method for document object locator combination. Image
and Vision Computing, Proceedings from the 15th International Conference on Vision
Interface, 22 (12):1015‚Äì1029, 2004.
39 K. R. Bandi and S. N. Srihari. Writer demographic classiÔ¨Åcation using bagging and
boosting. In Proceedings of International Graphonomics Society Conference, pages
133‚Äì137, Salerno, Italy, 2005.
40 M. Liwicki, A. Schlapbach, P. Loretan, and H. Bunke. Automatic detection of gen-
der and handedness from on-line handwriting. In Conference of the International
Graphonomics Society, pages 179‚Äì183, Melbourne, Australia, 2007.
41 Marcus Liwicki, Andreas Schlapbach, and Horst Bunke. Automatic gender detec-
tion using on-line and oÔ¨Ä-line information. Pattern Analysis Application, 14:87‚Äì92,
February 2011.
42 Nesrine Bouadjenek, Hassiba Nemmour, and Youcef Chibani. Robust soft biometrics
prediction from oÔ¨Ä-line handwriting analysis. Applied Soft Computing, 46:980‚Äì990,
2016.
43 S. Al-Maadeed, F. Ferjani, S. Elloumi, and A. Hassaine. Automatic handedness detec-
tion from oÔ¨Ä-line handwriting. In GCC Conference and Exhibition (GCC), 2013 7th
IEEE, pages 119‚Äì124, Doha, Qatar, November 2013.

124
Hybrid Intelligence for Image Analysis and Understanding
44 S. Al Maadeed, W. Ayouby, A Hassaine, and J.M. Aljaam. Quwi: an Arabic and
English handwriting data set for oÔ¨Ñine writer identiÔ¨Åcation. In International Con-
ference on Frontiers in Handwriting Recognition (ICFHR), pages 746‚Äì751, Bari, Italy,
September 2012.
45 S. Al-Maadeed and A Hassaine. Automatic prediction of age, gender, and nationality
in oÔ¨Ñine handwriting. EURASIP Journal on Image and Video Processing, 2014.
46 Somaya Al-Maadeed, Fethi Ferjani, Samir Elloumi, and Ali Jaoua. A novel approach
for handedness detection from oÔ¨Ä-line handwriting using fuzzy conceptual reduc-
tion. EURASIP Journal on Image and Video Processing, 2016 (1):1‚Äì14, 2016.
47 Imran Siddiqi, Chawki Djeddi, Ahsen Raza, and Labiba Souici-Meslati. Automatic
analysis of handwriting for gender classiÔ¨Åcation. Pattern Analysis and Applications,
pages 1‚Äì13, 2014.
48 A.S. Ibrahim, A.E. Youssef, and A.L. Abbott. Global vs. local features for gen-
der identiÔ¨Åcation using Arabic and English handwriting. In Signal Processing
and Information Technology (ISSPIT), 2014 IEEE International Symposium, pages
000155‚Äì000160, Dec 2014.
49 Timo Ojala, Matti Pietik√§inen, and David Harwood. A comparative study of texture
measures with classiÔ¨Åcation based on featured distributions. Pattern Recognition, 29
(1):51‚Äì59, 1996.
50 Bo Yang and Songcan Chen. A comparative study on local binary pattern (LBP)
based face recognition: {LBP} histogram versus {LBP} image. Neurocomputing,
120:365‚Äì379, 2013.
51 D. Bertolini, L.S. Oliveira, E. Justino, and R. Sabourin. Reducing forgeries in
writer-independent oÔ¨Ä-line signature veriÔ¨Åcation through ensemble of classiÔ¨Åers.
Pattern Recognition, 43 (1):387‚Äì396, 2010.
52 Yilmaz Kaya, Omer Faruk Ertugrul, and Ramazan Tekin. Two novel local binary pat-
tern descriptors for texture analysis. Applied Soft Computing, 34:728‚Äì735, 2015.
53 J.F. Vargas, M.A. Ferrer, C.M. Travieso, and J.B. Alonso. OÔ¨Ä-line signature veriÔ¨Åca-
tion based on grey level information using texture features. Pattern Recognition, 44
(2):375‚Äì385, 2011.
54 Matti Pietik√§inen, Abdenour Hadid, Guoying Zhao, and Timo Ahonen. Computer
Vision Using Local Binary Patterns. Springer-Verlag, London, 2011.
55 N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In
Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society
Conference, vol. 1, pages 886‚Äì893, June 2005.
56 O. Deniz, G. Bueno, J. Salido, and F. De la Torre. Face recognition using histograms
of oriented gradients. Pattern Recognition Letters, 32 (12):1598‚Äì1603, 2011.
57 M.B. Yilmaz, B. Yanikoglu, C. Tirkaz, and A. Kholmatov. OÔ¨Ñine signature veriÔ¨Åca-
tion using classiÔ¨Åer combination of HOG and LBP features. In Biometrics (IJCB),
2011 International Joint Conference, pages 1‚Äì7, October 2011.
58 Ning Jiang, Jiu Xu, Wenxin Yu, and S. Goto. Gradient local binary patterns for
human detection. In IEEE International Symposium on Circuits and Systems (ISCAS),
2013, pages 978‚Äì981, Beijing, China, May 2013.
59 Vladimir N. Vapnik. The Nature of Statistical Learning Theory. Springer-Verlag, New
York, 1995.
60 Tsau Young Lin, Ying Xie, Anita Wasilewska, and Churn-Jung Liau. Data Mining:
Foundations and Practice, vol. 118. Springer, Berlin, 2008.

SVM Combination for an Enhanced Prediction of Writers‚Äô Soft Biometrics
125
61 Sabri A. Mahmoud, Irfan Ahmad, Mohammad Alshayeb, WasÔ¨ÅG. Al-Khatib,
Mohammad Tanvir Parvez, Gernot A. Fink, Volker Margner, and Haikal El Abed.
Khatt: Arabic oÔ¨Ñine handwritten text database. In Proceedings of the International
Conference on Frontiers in Handwriting Recognition, pages 449‚Äì454, Seattle, WA,
2012. IEEE Computer Society.
62 Sabri A. Mahmoud, Irfan Ahmad, WasÔ¨ÅG. Al-Khatib, Mohammad Alshayeb,
Mohammad Tanvir Parvez, Volker M√§rgner, and Gernot A. Fink. KHATT: an open
Arabic oÔ¨Ñine handwritten text database. Pattern Recognition, 47 (3):1096‚Äì1112,
2014.
63 Sarah Hamid and Kate Miriam Loewenthal. Inferring gender from handwriting in
Urdu and English. The Journal of Social Psychology, 136 (6):778‚Äì782, 1996.

127
6
Brain-Inspired Machine Intelligence for Image Analysis:
Convolutional Neural Networks
Siddharth Srivastava and Brejesh Lall
Department of Electrical Engineering, Indian Institute of Technology Delhi, India
6.1
Introduction
The human brain is a sophisticated and wonderful biological marvel, and more than a
third of the brain‚Äôs processing power is dedicated to visual processing. The evolution of
humans and their triumph on earth can be attributed to the ability of the visual system
to coordinate with the brain and perform complicated pattern analysis, within a frac-
tion of a second. Provided the humongous amount of objects and visual stimuli that
humans are able to identify and discern, there is no surprise that the functioning of our
visual system and its subsequent processing by the brain have always intrigued everyone.
Scientists and philosophers have been pondering over understanding the basic nature
and functioning of our visual system for centuries. But it wasn‚Äôt till the 1940s [1] that
any practical system attempted to replicate the working of the human brain or, to be
more precise, the biological neurons. Since then, artiÔ¨Åcial intelligence (AI) practitioners
have performed numerous experiments [2‚Äì6] to understand this mysterious part of the
human anatomy.
Deep learning [7, 8] is the modern buzz word when it comes to algorithms mimicking
the functioning of the brain. Deep learning techniques have been swiftly outperforming
traditional machine learning and AI algorithms, and by a large margin [9‚Äì11]. Therefore,
the study of deep learning techniques utilizing images as inputs has become interesting.
From being labeled as useless by the research community to being the most success-
ful techniques of all time, deep learning and neural networks have progressed through
decades of success and failures. To this end, in the next few paragraphs, the reader is
introduced to the evolution of neural networks and deep learning. We believe that in
order to appreciate the complexity and strength of the contemporary research in deep
learning involving convolutional neural networks (CNNs), the reader should be familiar
with fundamentals of CNNs. Therefore, detailed discussion on state-of-the-art work is
deferred till the end of this chapter.
In 1943, McCulloch proposed a simple neuron model [1], showing that it can per-
form basic logic operations (AND/OR/NOT). The work caught the immediate attention
of AI researchers since performing logical operations on computers was a signiÔ¨Åcant
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

128
Hybrid Intelligence for Image Analysis and Understanding
achievement back then and it was the ultimate objective of AI. But this model lacked
a model for learning. In 1949, Hebb [12] put forth an idea that learning in the brain
happens due to formation and changes of synapses among neurons. Using this idea,
Rosenblatt in 1958 devised the perceptron [13]. It was a simple mathematical model
allowing learning similar to that of biological neurons. Its eÔ¨Écacy was demonstrated
on a simple shape classiÔ¨Åcation problem. But more importantly, it was the Ô¨Årst model
that showed that computers can learn functions based on provided input and expected
output data. Although perceptrons yield single output, adding layers to such a model
for higher level classiÔ¨Åcation was a straightforward extension as per Rosenblatt and
other AI researchers who saw potential in neural networks. But in a book published in
1969 [14] by Prof. Minsky of MIT, through rigorous analysis it was shown that percep-
trons have signiÔ¨Åcant limitations such as inability to learn a XOR function. This work
is believed to have inÔ¨Çuenced the downfall of neural networks, which led to a freeze
in any further research in this Ô¨Åeld during that time. In 1986, it was shown that by
using back-propagation [15], the multilayer perceptrons can be eÔ¨Éciently trained. This
was the time when it was realized that Minsky‚Äôs work actually meant that for learning
complicated functions, multilayer perceptrons were needed instead of a single-layer per-
ceptron. In 1989, Hornik et al. [16] showed that multilayer perceptrons can be used to
model any mathematical functions. This provided a major thrust to the neural network,
and in the same year LeCun applied back-propagation to recognize handwritten digits
[17]. It was an important work because, till then, it was believed that only humans have
the capability to eÔ¨Äectively distinguish among handwritten digits.
Following the ideas of multilayer neural networks, reinforcement learning-based
approaches were proposed for playing games such as backgammon [18]. But a few
years later, it was shown that such networks fail miserably in learning to play chess
[19], and the primary reason behind it was that the network spent most of its time
computing parameters. Therefore, nonavailability of computational resources or for
that matter eÔ¨Écient techniques that were suitable for hardware available during that
era made the adoption of neural networks even more diÔ¨Écult. In addition to this, lack
of any eÔ¨Écient method to perform back-propagation on large networks added to the
agony of neural networks. During this period, a supervised learning technique, the
support vector machines (SVMs) [20], was getting much attention. In fact Lecun, who
is now considered among the pioneers in deep learning research, showed that SVMs
worked better than neural networks for hand-digit recognition [21]. It was now widely
accepted that neural networks are dead and any research eÔ¨Äort toward them is a waste
of resources. This can be corroborated by the fact that during this period, the funding
for research projects related to neural networks dropped signiÔ¨Åcantly and consequently
the research in this area was toward a dead-end.
Despite such a negative environment around neural networks, Prof. Hinton was
able to secure funding from the Canadian Institute for Advanced Research (CIFAR)
for research in this area. And after several years of eÔ¨Äort, he along with his team
proposed A fast learning algorithm for deep belief nets [22], which formally paved the
path for deep learning. The main idea was that if the weights in the neural networks
are initialized in a speciÔ¨Åc way instead of randomly, they can be trained very eÔ¨Éciently.
The technique primarily involved training each layer sequentially in an unsupervised
way. It achieved a state-of-the-art result on the MNIST data set [23]. Following this
work, another deep learning maestro of the modern world, Prof. Yoshua Bengio, and

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
129
his team showed that deep learning algorithms perform better on complex tasks than
other learning paradigms [24]. They in fact justiÔ¨Åed why unsupervised pre-training
works and that it learns useful representations from data. Even though deep learning
was becoming popular, it was still plagued by limitations of computational resources
in research laboratories across the globe. During this time, Andrew Ng of Stanford
University collaborated with Google and utilized their enormous computing resources
to learn, in an unsupervised way, the object category labels. By this time, both industry
and research laboratories had realized the power of deep learning. In 2010, problems
behind subpar working of back-propagation was analyzed [25]. It was identiÔ¨Åed that
choice of nonlinear activation and initialization of weights signiÔ¨Åcantly impact the
performance of a deep network.
After this discovery, sincere research eÔ¨Äorts were put into improvising the deep
networks. It resulted in unprecedented progress in AI research and especially computer
vision, where CNNs were reborn. Since then, CNNs have demonstrated remarkable
capability in tasks related to image analysis [26‚Äì29]. Convolutional networks, in
general, are motivated from biological processes [30]. This is corroborated by the fact
that CNNs are hierarchical networks that abstractly resemble the working of the human
mind. The authors in [31] evaluate CNNs for their resemblance with the human brain
using functional magnetic resonance imaging (fMRI). They analyze the contribution of
each layer of a CNN against the observed brain activity for visual recognition tasks. The
study found that the visual processing and layers of CNNs simulate similar hierarchical
computational responses in various parts of the brain. An interesting observation from
the study is that CNNs are able to explain the intrinsic functioning of the brain. CNNs
in their existing form only correspond to the functional Ô¨Çow of information in the visual
cortex and the brain. Despite such great advancements, it should be noted here that in
practice, CNNs are far from attaining human-level intelligence. It has been shown that
traditional CNN architectures can be fooled [32] into recognizing objects in images
appearing nonsensical to the human brain. But it is important to note that it is the
potential of CNNs that has led the research community to continuously improvise upon
such failures and come up with stronger architectures. The struggle to attain human
brain-level performance continues, with CNNs being the strongest in achieving it.
6.2
Convolutional Neural Networks
The popularity of CNNs for image-related tasks, as we know today, can be attributed
to the work on large-scale image classiÔ¨Åcation by Krizhevsky et al. [27] in 2012. They
achieved more than 10% reduction in error rates on a data set with 1.3 million images
belonging to a thousand classes. Although CNNs have existed for a long time, their
practical use has become possible only now when appropriate computational power is
available. Table 6.1 compares CNNs and the human brain at a coarse level to bring out
the diÔ¨Äerences in their functioning. We expect that it will also allow the reader to appre-
ciate the fact that simulating brain (especially for image analysis) is an extremely diÔ¨Écult
task, and while CNNs may lack in many aspects, they are still the best available today.
To explain the working of CNNs, we start with a discussion on the building blocks
of CNNs.

130
Hybrid Intelligence for Image Analysis and Understanding
Table 6.1 Conceptual diÔ¨Äerences between CNN and the brain/visual system
CNN
Human brain/visual system
DiÔ¨Äerence
Architecture of CNN is
inspired from biological
processes.
They are the biological organs.
Man vs. Nature
CNNs process images in
hierarchical order.
Study shows that brain
processes visual stimulus
in hierarchical order.
The Ô¨Çow of information could be
similar, but the channels through
which this information is passed
are diÔ¨Äerent.
CNNs work very well if
trained with a huge amount of
input data.
Can learn with as little as one
image.
Human brain has evolved over
thousands of years.
Has millions of parameters or
neuronal connections
Studies suggest that human
brains have billions or trillions
of neuronal connections.
The computational complexity
and eÔ¨Éciency of the human brain
are huge when compared to
CNNs.
Generally uses gradient
descent
DiÔ¨Écult to determine if it is a
single algorithm
Implicit learning vs. explicit
learning
Usually trained with a
single modality (images)
Can multitask with multiple
sensory inputs (audio, visual,
touch, etc.)
Unlike a CNN, the brain has
various sensory inputs working
in coherence.
6.2.1
Building Blocks
The basic architecture of a neural network is an artiÔ¨Åcial neuron. Figure 6.1 depicts
a set of inputs (x1, x2, ‚Ä¶ , xn) to a neuron N, which essentially weighs the input with
weights (say, ùë§i), and sums them. This value may then pass through an activation func-
tion (explained later in this chapter) to produce the desired output. The neuron hence
yields a single numeric output. This output may also be produced by a set of cascaded
layers of neurons, as shown in Figure 6.2. The intermediate layers between the input and
output layers of the network are also called hidden layers.
Neuron (N)
Activation
Function
x1
w1
w2
o
wn
x2
xn
Figure 6.1 A neuron.

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
131
Input Neurons
Hidden Layer
Hidden layers have neurons
that receive inputs from other
neurons
Output
N1
N‚Ä≤1
o
N‚Ä≤2
N2
Nn
Figure 6.2 Hidden layers.
Mathematically, a neuron can be represented as follows:
f (xi, ùë§i) = ùúô
( n
‚àë
i=1
(ùë§i.xi)
)
(6.1)
where xi is the input, ùë§i is the weight corresponding to n inputs, and ùúôis the activation
function. An activation function is meant to bound the output of a neuron. The appli-
cations of activation functions are not limited to neural networks. For example, with
SVMs, they are used to transform the input feature space to reÔ¨Çect a decision bound-
ary. But what makes them interesting in the context of biologically motivated neural
networks is their capability to abstractly represent the rate of change in the electrical
membrane potential or, more speciÔ¨Åcally, the action potential of a cell. Numerous acti-
vation functions have been proposed in literature; Table 6.2 shows a few such activation
functions and their corresponding plots. An activation function is usually applied at the
hidden layers of a neural network and/or at the output layer.
Activation at hidden layers: The rectiÔ¨Åed linear units (ReLUs) [33] have become very
popular in the last few years and are now the recommended activation function to be
used at the hidden layers of a neural network. Mathematically, it is expressed as in
equation (6.2):
ùúô(x) = max(0, x)
(6.2)
The reason for preferring ReLUs over other activation functions are: Ô¨Årst, it is a
nonsaturating function. It can be observed from Table 6.2 that while other functions
saturate to a value of ‚àí1, 0, or 1 (vanishing gradient), ReLU doesn‚Äôt. This results in faster
convergence of gradient descent as compared to other activation functions, especially
sigmoid/tanh. Second, the computation is simpler and faster since a ReLU only
requires thresholding at zero. It also induces sparsity in the hidden units. This becomes
signiÔ¨Åcant when we consider that CNNs have a huge number of matrix computations.
Other similar activation functions are parametric ReLU [34] and Maxout [35]. The
ReLU function [equation (6.2)] has the problem that the gradients become 0 (due to
thresholding) if the learning rate is set too high. The parametric ReLU instead has a

132
Hybrid Intelligence for Image Analysis and Understanding
Table 6.2 Activation functions
Plot
Activation function
Equation
8
7
6
5
4
3
2
1
8 9
7
6
5
4
3
2
1
‚Äì1
‚Äì1
‚Äì2
‚Äì3
‚Äì4
‚Äì5
‚Äì6
‚Äì7
‚Äì8
‚Äì2
‚Äì3
‚Äì4
‚Äì5
‚Äì6
‚Äì7
‚Äì8
Linear
ùúô(x) = x
4
3
2
1
‚Äì1
‚Äì2
‚Äì3
‚Äì4
4
3
2
1
‚Äì1
‚Äì2
‚Äì3
‚Äì4
Step
ùúô(x) =
{
0
x < 0
1
x ‚â•0
1
‚Äì1
1
‚Äì1
Sigmoid
ùúô(x) =
1
1 + e‚àíx
small slope (Table 6.2) when the value is less than 0. Maxout function, on the other
hand, is a generalized form of the ReLU and parametric ReLU. Though it combines the
advantages of both ReLU and parametric ReLU, the number of parameters per neuron
are doubled, leading to increased complexity of the network.
Activation at output layers: The de facto choice for the activation function at the
output layer in a CNN is a softmax function. It computes the probability of the input
belonging to each of the possible output classes. The mathematical representation of a

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
133
Table 6.2 (Continued)
Plot
Activation function
Equation
1
‚Äì1
1
‚Äì1
TanH
ùúô(x) = tanh(x)
4
3
2
1
‚Äì1
‚Äì2
‚Äì3
‚Äì4
4
3
2
1
‚Äì1
‚Äì2
‚Äì3
‚Äì4
RectiÔ¨Åed linear unit
(ReLU) [33]
ùúô(x) =
{
0
x < 0
x
x ‚â•0
1
‚Äì1
1
‚Äì1
Parametric rectiÔ¨Åed
linear unit (leaky
ReLU) [34]
ùúô(x) =
{
ùõº.x
x < 0
x
x ‚â•0
softmax function is given as:
ùúôi =
ezi
‚àëezj
(6.3)
where zi is the value of the ith output neuron, while zj represents all the output neurons
in the output layers.

134
Hybrid Intelligence for Image Analysis and Understanding
x1
w1
w2
Output
Binary Inputs
Classification
Binary Output
wn
x2
xn
Figure 6.3 A perceptron.
6.2.1.1
Perceptron
While the neuron is the basic unit in a neural network, we still need a model that can
learn how to classify the given inputs to a desired set of outputs. One such model is
a perceptron [13]. More precisely, a perceptron (Figure 6.3) is a supervised learning
model for building a linear binary classiÔ¨Åer. It takes binary inputs and produces a binary
output.
Mathematically, a perceptron is represented as:
output =
{
1
‚àën
i=1 ùë§i.xi > threshold
0
‚àën
i=1 ùë§i.xi ‚â§threshold
(6.4)
A careful comparison of Equations (6.1) and (6.4) shows that the output of a percep-
tron is essentially a threshold applied over the output of a neuron. An alternative way to
look at it is that a perceptron weighs up an input only if it is important.
Let us understand the perceptron with the help of an example. Suppose you want to
buy a book for studying CNNs. You choose to decide whether you would buy a particular
book based on the following factors (other factors are not considered relevant for this
discussion):
‚Ä¢ Are the authors trustworthy?
‚Ä¢ Are the reviews on e-commerce sites positive?
‚Ä¢ Does the book mention it is meant for beginners?
‚Ä¢ Can you aÔ¨Äord it?
Each of the above factors can be represented as a binary variable. For instance, if
x1 symbolizes Are the authors trustworthy?, then we would have x1 = 1 if you Ô¨Ånd the
authors trustworthy (Yes), and x1 = 0 if otherwise (No). Similarly we choose x2, x3, and
x4 for the next three factors, respectively. The assignment of variables to various factors
is shown in Table 6.3.
Now, if you are a novice in this Ô¨Åeld and want to understand the basics of CNNs
(i.e., Does the book mention it is meant for beginners ?), you may be willing to overlook
other factors. You would model it by assigning a higher weight to x3 (i.e., a higher value
of ùë§3). Suppose that you assign ùë§1 = 2, ùë§2 = 3, ùë§3 = 10, and ùë§4 = 2, and you choose

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
135
Table 6.3 Factors and assigned variables for the perceptron model
Sample no.
Factor
Input variable
Weight variable
1.
Are the authors trustworthy?
x1
ùë§1
2.
Are the reviews on e-commerce sites positive?
x2
ùë§2
3.
Does the book mention it is meant for beginners?
x3
ùë§3
4.
Can you aÔ¨Äord it?
x4
ùë§4
the threshold as 8. This infers that output of the perceptron [equation (6.4)] can never
be greater than 8 if the book is not meant for beginners (Factor 3). Alternatively, you
can say that you will buy the book if and only if it satisÔ¨Åes the third factor. One may get
alternative models by suitably changing the weights and threshold.
As perceptrons are the fundamental model behind large neural networks, let us sim-
plify the notation and provide some biological perspective. The simpliÔ¨Åed notation is
given by equation (6.5):
output =
{
1
w.x + b > 0
0
w.x + b ‚â§0
(6.5)
where w and x are the vectors representing the weights (ùë§i) and inputs (xi) from
equation (6.4); and b is the bias, which can be understood as the negative of the
threshold being added to the left-hand side of equation (6.4). It can also be interpreted
as the ease with which a perceptron can be Ô¨Åred (similar to biological neurons).
The above discussion shows you how a perceptron can weigh various factors to reach
a decision. Though it is still far from the decision-making capability of the human brain,
still it is an important element in computationally realizing this behavior at a fundamen-
tal level. We must also understand that when the perceptron was proposed in the 1950s,
the scientiÔ¨Åc community had limited understanding on the actual functioning of our
brain. Therefore, artiÔ¨Åcial neural networks attempt to replicate the Ô¨Çow of processes and
information in the human brain as a black box rather than actually performing similar
computations.
6.2.2
Learning
We have discussed the perceptron model. A question that arises is, how does a percep-
tron learn? Prior to explaining how learning is performed, you should understand that
while a perceptron is a good model for conceptual understanding, practically we use
functions such as sigmoid, ReLU, maxout, and so on for computing weights at the hid-
den layers. The reason is that a perceptron outputs only 0 or 1, which makes the tuning of
the network a little rigid when a large number of parameters are involved. Moreover, by
simple algebraic assumptions, any activation function can be converted to a perceptron
having binary outputs.
In Figure 6.2, a multilayer network is shown. Such a network is called a feedforward
network, as it doesn‚Äôt contain any feedback into the network. Although feedback to
the same network may sound nonintuitive at Ô¨Årst, such networks exist and are called
recurrent neural networks. Theoretically, recurrent neural networks have a closer
resemblance to the functioning of the human brain, but they have limited practical

136
Hybrid Intelligence for Image Analysis and Understanding
application, mostly due to lack of eÔ¨Écient learning algorithms when compared to
feedforward networks. Therefore, in this text, we focus on the feedforward networks.
Any type of learning requires a deÔ¨Åned cost function. In general, a cost function can
be represented as:
C(w, b) = min
N
f (||prediction ‚àíactual||)
(6.6)
where C deÔ¨Ånes the cost function over weights ùë§, and biases b; N is the total number
of training inputs; f is the loss function expression, such as root mean square (RMS);
prediction is the predicted output value using the desired activation [in the case of a
perceptron, it is given by equation (6.5)]; and actual is the output obtained from the
network. The objective of the learning algorithm is to learn weights and biases, such
that the cost C is minimized.
6.2.2.1
Gradient Descent
Gradient descent is a popular algorithm to compute cost with the constraints discussed
previously. It works by slowly moving to the minima of the cost function. For this, it com-
putes the gradient at each weight and minimizes it. Pictorially, it can be represented as in
Figure 6.4. Technically, a gradient is the rate of change of a property, that is, the gradient
of a function f (x, y) would be a vector given by its partial derivatives [equation (6.7)]:
‚àáf =
[ùúïf
ùúïx, ùúïf
ùúïy
]
(6.7)
In general, the algorithm begins with a random guess for a weight ùë§, and then grad-
ually moves toward the minima by moving in the opposite direction of the gradient.
Having provided the intuition behind the algorithm, we will not delve into further details
of gradient descent, as in practice we use a faster algorithm called back-propagation
for learning the optimum parameter values for the network, which is described
next.
6.2.2.2
Back-Propagation
Back-propagation is the process of propagating errors back to a network. It was the
discovery of fast methods for computing the gradient of loss function of a network [15]
that led to applications of neural networks to practical tasks.
Error
Minima
Weights
Figure 6.4 Gradient of a single weight.

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
137
Back-propagation works in two stages:
‚Ä¢ Propagation
‚Äì Forward propagation: It performs a forward pass to generate output activations
based on the provided input.
‚Äì Backward propagation: The diÔ¨Äerence in actual and predicted value (loss) is com-
puted for every neuron in the neural network (hidden, output) based on the output
activations.
‚Ä¢ Parameter update
‚Äì Gradient computation: Obtain gradient of the weight from the diÔ¨Äerence values
obtained in the previous step.
‚Äì Weight update: Update the weight by subtracting from it a fraction of the gradient
value.
The rest of the section describes various steps in detail regarding the sequence in
which they appear during a practical implementation. Therefore, Algorithm 6.1 shows
pseudo-code for a back-propagation algorithm with one hidden layer to clarify upon the
implementation perspective; it sets the context for the upcoming discussion.
Algorithm 6.1
Back-propagation for a network with one hidden layer
1. procedure Back-propagation
2. loop:
3.
for each training example
4.
prediction = predicted value // forward pass
5.
actual = actual value
6.
compute error = f(||prediction - actual||)
7.
compute partial derivatives // backward pass
8.
update weights //parameter update
9. until stopping criteria //For ex. error rate or number of iterations
Propagation: Back-propagation is based on recursively applying the chain rule to
the gradients to update the parameter values. Let us understand it with the help of an
example. Suppose there is a function given as:
f (a, b, c) = (a + b) ‚àóc
The above function may be rewritten as:
f (x, c) = x ‚àóc where x = (a + b)
The gradients (or the partial derivatives) are computed as:
ùúïf
ùúïx = c and ùúïf
ùúïc = x.
(forward pass)
Since we want the derivatives with reference to a, b, and c, the chain rule comes into
play. It says that the required derivatives may be computed as:
ùúïf
ùúïc = x, ùúïf
ùúïa = ùúïf
ùúïx
ùúïx
ùúïa, ùúïf
ùúïb = ùúïf
ùúïx
ùúïx
ùúïb.
(Backward pass)

138
Hybrid Intelligence for Image Analysis and Understanding
It can be observed that these equations simply require multiplication of gradient values.
(In this particular example, ùúïx
ùúïa = 1 and ùúïx
ùúïb = 1.) To further enhance the understanding,
let us work out a numerical example:
Back-Propagation Example
In the discussed example, let a = ‚àí4, b = 5, and c = ‚àí2.
Forward pass
x = a + b (gives x = 1)
f = x ‚àóc (gives f = ‚àí2)
Backward pass
ùúïf
ùúïc = x (gradient on c = 1 as x = 1 from above)
ùúïf
ùúïx = c (gradient of x = ‚àí2 as c = ‚àí2 from above)
Back-propagation through x
ùúïf
ùúïa = 1.0 ‚àóùúïf
ùúïx = ‚àí2
ùúïf
ùúïb = 1.0 ‚àóùúïf
ùúïx = ‚àí2
Parameter update: When the weights are adjusted in a network, the error of the network
should decrease. There are diÔ¨Äerent ways to update the weights in the network. The
weights can be updated using online mode, batch mode, or stochastic gradient descent.
‚Ä¢ Online mode: In online mode, the weights are updated for each input element. The
number of updates is equal to the number of input elements.
‚Ä¢ Batch mode: The gradients are summed for each input element up to the limit of the
speciÔ¨Åed batch size. The weights are updated once a batch has been processed. The
number of updates therefore is InputSize
batchSize.
‚Ä¢ Stochastic gradient descent (SGD): The third and the most popular (also recom-
mended) way of updating weights is SGD. It can work in both online and oÔ¨Ñine
mode. While training in online mode, the SGD selects an input element at random,
computes the gradient, and updates the weight. This continues until the desired error
is attained. Similarly, in batch mode, the SGD randomly selects B input elements
where B is the prespeciÔ¨Åed batch size. The process is similar to the batch mode
discussed earlier, with the only diÔ¨Äerence being that each batch has randomly chosen
elements.
Mathematically, weights are updated as:
‚àáùë§i = ùõº‚àáùë§i‚àí1 + (‚àíùúÇ) ùúïC
ùúïùë§i
(6.8)
where, ‚àáùë§i is the weight in ith iteration, C is the error or the cost as given by equation
(6.6), and, ùõºis an optional scaling factor. ùõºgives the percentage of the gradient on
weight from a previous iteration that should be applied in the current iteration. It helps
in algorithms to avoid local minima. ùúÇis the learning rate. It impacts the speed of the
learning process. If it is set too high, then the network may never converge because it

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
139
may simply be hopping across the minima, while if it is too low, it may take too much
time to converge.
Now that we have discussed the basics of neural networks and learning paradigms, we
now move toward discussing the convolution operation and its application on images.
6.2.3
Convolution
Convolution is a mathematical operation that is very popular in the domain of signal
processing for mixing signals with a speciÔ¨Åc rule. For discrete signals (we would consider
discrete convolutions as images are discrete) f and g, convolution over a set of integers
Z is deÔ¨Åned as:
( f ‚àóg)[n] =
‚àû
‚àë
k=‚àí‚àû
f [k]g[n ‚àík] =
‚àû
‚àë
k=‚àí‚àû
f [n ‚àík]g[k]
(6.9)
Equation (6.9) shows that convolution of two functions can be computed by sliding
one function and performing computation over the overlapping values. A similar
concept, when applied in two dimensions, can be used for performing convolution with
images.
Applying convolution to images: Convolution to images is applied in two dimensions
(i.e., along the width and height). Here the convolution is performed between a region
in the image and an equally sized convolution kernel. A convolution kernel is a Ô¨Çoating
point matrix that is applied over the image. For example, if the size of the image is 256 √ó
256 and the chosen kernel has a size of 3 √ó 3, then the convolution is performed on
equivalent-sized patches from the input images (i.e., 3 √ó 3). The overlapping values are
then multiplied and summed to obtain the resultant value for one pixel (the center pixel
of the local patch). The kernel is moved across the image, hence producing a 2D response
of the convolution kernel to the image. In terms of deep learning, this 2D output is called
the feature map. The above process can be mathematically expressed as:
response = I ‚àóK =
n+1
2‚àë
c=‚àín‚àí1
2
‚éõ
‚éú‚éú‚éú
‚éù
m+1
2‚àë
r=‚àím‚àí1
2
I(a + r, b + c)K(r, c)
‚éû
‚éü‚éü‚éü
‚é†
(6.10)
where I is the input image, K is the mxn convolution kernel, (a, b) is the image pixel for
which convolution is being computed, and m and n are chosen as odd so that the kernel
is symmetric about the center. Figure 6.5 pictorially shows how a convolution kernel
is applied over a patch in the image. In the shown example, the convolution kernel is
placed at the center of the image (the pixel with a dark black border). The response at
this particular pixel is computed by multiplication of corresponding elements and their
summation. When this process is repeated for all the pixels, a 2D feature map is formed.
It should be noted that padding may need to be applied for computing the response at
the border pixels.
The deÔ¨Ånition of the convolution kernel allows us to perform many useful operations
on images. Figure 6.6 shows a few such kernels.
Use of convolution in machine learning: DiÔ¨Äerent convolution kernels can be used to
extract speciÔ¨Åc features from images such as edges, blobs, and so on (Figure 6.6). Such
features are generally used in machine learning algorithms to perform a variety of higher
level tasks, such as object detection, recognition, classiÔ¨Åcation, and so on. Now, the

140
Hybrid Intelligence for Image Analysis and Understanding
6
8
7
3
6
1
1
2
1
*
=
‚Äì1
‚Äì1
‚Äì1
25
1
2
4
1
4
2
4
8
2
1
5
8
9
1
6
0
9
9
1
6
0
9
Convolution Kernel
Input Image
response = 1*1 + 4*2 + 2*1 + 2*(‚Äì1) + 1*(‚Äì1) + 5*(‚Äì1) + 1*1 + 6*2 + 0*1 = 25
Figure 6.5 Example of applying convolution to images.
0
0
0
0
0
0
0
1
0
1
1
1
1
1
1
1
(1/9)
(a) Identity
(b) Edge Detection
(c) Sharpen
(d) Box Blur
(e) Gaussian Blur
(1/16)
1
1
1
2
1
1
2
1
2
4
2
0
‚àí1
0
0
‚àí1
0
‚àí1
5
‚àí1
1
0
‚àí1
‚àí1
0
1
0
0
0
‚àí1
‚àí1
‚àí1
‚àí1
‚àí1
‚àí1
‚àí1
8
‚àí1
0
1
0
0
1
0
1
‚àí4
1
Figure 6.6 Convolution kernels.
performance of such tasks is directly dependent upon the quality of input features. Let
us take the edge detection convolution kernel as an example to clarify the importance
of convolution in machine learning. As can be seen from Figure 6.6b, that for Ô¨Ånding
edges at diÔ¨Äerent orientations, one needs to adjust the weights in the kernel. Figure 6.7
shows an example of two edge detection techniques. The Sobel operator (Figure 6.8) is a
simple matrix operation where the weights in the matrix govern the detection of edges
at a particular orientation, while the Canny edge detector is a multistep edge detection
algorithm. Discussion of the Canny edge detector is beyond the scope of this chapter. It
has been shown here to emphasize that to obtain relevant features, one needs to identify
speciÔ¨Åc techniques that are suitable to the problem domain. This is also known as feature
engineering. Since convolution operations can be used to extract low-level features
from images, and since features are crucial for any machine learning algorithm, they
become decisive in obtaining state-of-the-art results in almost any problem scenario. In
the next section, we will see that CNNs try to solve exactly this problem. They attempt
to learn weights of convolution kernels to detect various types of low-level features.

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
141
(a) Original Image
(b) Sobel Edge Detector
(c) Canny Edge Detector
Figure 6.7 Edge detection in images.
0
0
0
(a) Horizontal Lines
(b) Vertical Lines
1
2
1
‚àí1
‚àí1
‚àí1
‚àí2
‚àí2
1
1
2
0
0
0
‚àí1
Figure 6.8 Sobel operator.
6.2.4
Convolutional Neural Networks: The Architecture
CNNs have multilayer architectures similar to those of the neural networks discussed in
the previous section. The CNN architecture, in general, comprises the following layers:
‚Ä¢ Convolution (Conv) layer
‚Ä¢ Pooling layer
‚Ä¢ Fully connected (FC) layer
A general architecture diagram for a CNN is shown in Figure 6.9.
The core working of CNNs is similar to that of neural networks, where the input is
given to neurons, certain operations are performed on them, and weights and biases are
learned based on a loss function. The only additional assumption is that CNNs explic-
itly take only raw image pixels as inputs. This assumption allows several modiÔ¨Åcations
in the architecture, resulting in a more eÔ¨Écient network with a far smaller number of
parameters as compared to a traditional neural network.
Arrangement of neurons: The input to a CNN are raw images. Correspondingly, the
neurons are arranged as a 3D volume I i.e., width, height, and depth). More precisely, for
a N √ó M-sized image with R, G, and B channels (#channels = depth = 3), the neurons
at the input layer are arranged in a volume with dimensions N √ó M √ó 3. Additionally,
unlike regular neural networks, the layers of CNNs are not necessarily fully connected.
This is sometimes also called sparse connectivity and is shown in Figure 6.10.

142
Hybrid Intelligence for Image Analysis and Understanding
Conv Layer
Conv Layer
Activation
Pooling
Activation
Pooling
FC Layer
Repetitions
Repetitions
Output
Feature
Figure 6.9 Convolutional neural network: architecture.
Output
Input
Hidden Layer
Regular Neural Network
Convolutional Neural Network
Figure 6.10 Connections in a regular neural network versus a convolutional neural network.
Each layer has a speciÔ¨Åc purpose and has associated hyperparameters that must be
speciÔ¨Åed for a CNN to work. We now describe the working of each layer.
6.2.4.1
Convolution Layer
The convolution layer is the most important part of a CNN. We begin by providing an
intuitive explanation of the working of convolution layers, followed by more intricate
details.
Intuitive explanation: The main purpose of the convolution layer is to detect features
from images (lines, edges, etc.). The convolution layer consists of a set of Ô¨Ålters that can
be learned in order to detect these features. The Ô¨Ålters are small in terms of width and
height, but extend to the complete depth of the input image or, more precisely, the input
volume as described in this chapter. Now convolution between the input volume and
Ô¨Ålter is performed by sliding the Ô¨Ålter across the width and height of the input volume
while computing the dot product on the overlapping values at a location. The dot prod-
ucts over a spatial region result in a 2D activation map containing the Ô¨Ålter response at

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
143
every location. Intuitively, it can be understood that these activation maps are in fact the
responses to the features in an image such as edges on the Ô¨Årst layer. Now considering
that there are multiple convolution layers in a network, at higher layers the activation
may correspond to more complex features such as object shapes or patterns. An interest-
ing question to consider here is what must be the output of the convolution layer, since
the subsequent layers are generally convolution layers again. As mentioned, the input
to the Ô¨Årst convolution layer is a 3D volume (RGB image). Therefore, to stack multiple
convolution layers, the output has to be a volume as well. Hence, the output volume is
formed by stacking along the depth dimension, the 2D activation maps of each of the
Ô¨Ålters in a convolution layer.
Hyperparameters: There are many parameters that are considered while designing the
convolution layers; these parameters are termed as hyperparameters since they decide
the size of the output volume.
‚Ä¢ Depth: A Ô¨Ålter is a 2D matrix whose elements are learned by a CNN. The Ô¨Ålters are
responsible for detection of features, such as edges, lines, blobs, and so on. The depth
here is related to the number of Ô¨Ålters that would be used. The higher the number of
Ô¨Ålters, the more complex features a CNN can learn.
‚Ä¢ Size of Ô¨Ålter: The size of the Ô¨Ålter in a CNN essentially allows us to convey how local
the low-level features are in an image. For example, a Ô¨Ålter of size 1 √ó 1 essentially
means that features are local on the pixel level, where they do not have any relation
with the neighboring pixels. On the other side, a Ô¨Ålter of the size of the images would
mean that there would be connections among every input pixel to every neuron in
the convolution layer (i.e., it becomes a fully connected layer). In practice, the size of
the Ô¨Ålter is kept small (3 √ó 3 or 5 √ó 5).
‚Ä¢ Stride: Stride is the number of pixels a Ô¨Ålter is shifted while performing the convolu-
tion. The stride is chosen such that if the Ô¨Ålter begins operating at the top-left pixel,
it should be able to reach the bottom right of the image. The stride is at least 1 (else
the Ô¨Ålter would not move). The common values for stride are 1 or 2. Larger strides are
uncommon in the case of images, but the reader should not deduce that they are not
useful. Recursive neural networks have been used with larger strides to form tree-like
structures and have obtained impressive results for natural language processing appli-
cations [36].
‚Ä¢ Zero-padding: It controls the number of rows/columns of zeros that must be added
to the image border. With zero-padding, the convolution can be applied to the border
pixels of an image and hence can be used to control the size of the output volume.
Input and output volumes: As discussed in this chapter, the input to the convolution
layer is a volume of size N √ó M √ó 3 for an N √ó M RGB image. Each Ô¨Ålter in the convo-
lution layer sweeps across the input with the given stride and padding. Here, stride is
the steps in which the Ô¨Ålter moves across the input, while padding is the number of zero
border pixels in the area where the Ô¨Ålter is operating. Technically, this is viewed as a
convolution of the Ô¨Ålter with the input. The output for each Ô¨Ålter is stacked together in
the dimension of the depth and is said to be the output volume of the convolution layer.
It is important to mention again that both the input and output to the convolution
layer are volumes. If the input to the layer are image pixels, then the input volume is
N √ó M √ó 3 for an RGB image. The number of weights for an input volume with depth D
would be:
#Weights = FS √ó F ‚àíS √ó D
(6.11)

144
Hybrid Intelligence for Image Analysis and Understanding
Parameter Sharing
A
B
C
1
2
3
4
5
Strong lines (from neurons 1, 2, and 3), dotted lines
(from neurons 2, 3, and 4) and broken lines 
(from neurons 3, 4, 5) show weight sharing
among respective connections. 
Figure 6.11 Example of parameter sharing among neurons.
For example, an input image and Ô¨Ålter of sizes 10 √ó 10 √ó 37 (7 √ó 7), would result in
7 √ó 7 √ó 3 = 147 number of weights. This implies that the Ô¨Ålters are connected locally to
regions in an image but are fully connected in the input depth dimension.
The size of the output volume (O √ó O) is governed by the size of the input volume (I √ó
I), the Ô¨Ålter size or the receptive Ô¨Åeld size (FS), padding (P), and stride (S), and is given as:
O = I ‚àíFS + 2P
S
+ 1
(6.12)
Now, if the convolution layer has a depth D, the output volume would have a dimen-
sion of O √ó O √ó D. This output volume either is given as is to another convolution layer
as input or is down-sampled by passing it through a pooling layer.
Parameter sharing: Parameter sharing helps in controlling the number of parameters.
It allows for achieving computational and memory-eÔ¨Écient implementation of CNNs. It
works with the assumption that if a patch has been used for computation at one spatial
location, it would also be useful for computation at another location. An example of
parameter (weight) sharing is shown in Figure 6.11.
Example: To clarify upon the concepts discussed here, we now discuss a numerical
example demonstrating the computations being performed in a convolution layer. Let
us consider an architecture with the following parameters:
Size of Input Image (I) = 227 √ó 227 Size of Filter or Receptive Field Size (FS) = 13
Stride (S) = 2
Padding (P) = 0
Depth of Convolution Layer (D) = 96
Using equation (6.12), the size of the output volume can be computed as:
O = 227 ‚àí13 + 0
2
+ 1 = 108
which results in a output volume of size:
Size of Output Volume = 108 √ó 108 √ó 96
This means that each of 108 √ó 108 √ó 96 neurons in this volume is connected to the
same 13 √ó 13 √ó 3 region in the input volume. We can also observe that the number of

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
145
neurons in the output volume is 108 √ó 108 √ó 96 = 1,119, 744. Each neuron has 13 √ó 13 √ó
3 = 507 weights and a bias. This results in 1,119, 744 √ó 507 = 567,710, 208 parameters
in the Ô¨Årst convolution layer itself. A billion parameters in the Ô¨Årst layer (or any layer
for, that matter) is clearly an exorbitantly high number. This is where parameter sharing,
as discussed earlier, becomes useful. If we consider that all the neurons in a depth level
(D = 96 in this example) have the same weight and bias, it would result in 96 √ó 13 √ó 13 √ó
3 = 48,672 weights or 48,768 parameters with 96 biases, which dramatically reduces the
computational complexity and memory footprint of the operations being performed on
this layer.
6.2.4.2
Pooling Layer
This layer is responsible for progressively down-sampling the input volume from the
convolution layer. In other words, it condenses the feature map produced by the convo-
lution layer. It is an optional layer and is put between successive convolution layers. It
serves the following purposes:
‚Ä¢ Reduces the number of parameters
‚Ä¢ Avoids overÔ¨Åtting.
Hyperparameters: It has two hyperparameters:
‚Ä¢ Spatial extent (F): It means that patches of F √ó F would be scaled down to one pixel.
‚Ä¢ Stride (S): This is the number of pixels by which the pooling operation moves across
the input.
The commonly used values for the hyperparameters are F = 2 and S = 2. It is impor-
tant to understand here that if pooling is performed with very large spatial extents,
information from a larger region gets diluted. Therefore, pooling with large receptive
Ô¨Åelds (spatial extents) is not recommended and in practice has a negative impact on the
results.
Input and output volumes: The output of the convolution layer is provided as input to
the pooling layer. Since this layer doesn‚Äôt consist of any weight, training has no eÔ¨Äect on
it. If an input volume with dimensions N √ó M √ó D is given as input to the pooling layer,
it reduces it to a volume of size N‚Ä≤ √ó M‚Ä≤ √ó D with the following mapping functions:
N‚Ä≤ = N ‚àíF
S
+ 1
M‚Ä≤ = M ‚àíF
S
+ 1
Types of pooling: The most common form of pooling layer is max pooling. With
max pooling and the commonly used setting of F = 2, S = 2, each 2 √ó 2 grid would be
replaced with the maximum value in that grid. This also results in 75% loss of pixel
information (i.e., a 4 √ó 4 grid would become 2 √ó 2). Average pooling works similarly
to max pooling, but instead of replacing the resulting grid with the maximum value,
average pooling replaces it with the average of the region. The operation is shown with
the help of an example in Figure 6.12. This process is repeated for each of the D feature
maps of size N √ó M in the input volume.
Though max pooling is the most commonly used, there are other forms as well, such as
L2-norm pooling, fractional max pooling, 3 √ó 3 pooling region [27], stochastic pooling
[37], or just removing the pooling layer altogether [38].

146
Hybrid Intelligence for Image Analysis and Understanding
2√ó2 Filter
Output: 2√ó 2
Input: 4 √ó4 Depth slice
(a) Max Pooling
(b) Average Pooling
Output: 2√ó 2
Input: 4√ó 4 Depth slice
6
8
7
7
3
4
10
10
6
9
4
7
3
5
4
2
8
2
1
5
9
1
6
0
6
8
7
3
4
10
4
2
8
2
1
5
9
1
6
0
Stride = 2
2√ó2 Filter
Stride = 2
Figure 6.12 Example of pooling.
6.2.4.3
Dense or Fully Connected Layer
The Ô¨Ånal layer in a CNN is generally a dense layer. It is a fully connected layer from
the output volume of the convolutional/pooling layer to neurons in this layer. The CNN
architecture can contain multiple dense layers, with the last layer performing the task of
classiÔ¨Åcation. As discussed in this chapter, the Ô¨Ånal layer uses softmax activation while
other layers may use any activation function, with ReLU being the recommended acti-
vation function.
The reason that fully connected layers are used toward the end is twofold. First, the
convolution layer is better at exploiting the spatial structure in the input image. Second,
fully connected layers require a huge number of parameters, as was explained while
discussing the convolution layer. Therefore, if fully connected layers are used at the
beginning, not only would it be computationally ineÔ¨Écient but also millions of redun-
dant parameters would be needed.
6.2.5
Considerations in Implementation of CNNs
‚Ä¢ Hardware considerations:
‚Äì GPU: Almost all the modern CNN architecture is based on GPUs. In fact,
the progress of CNNs was halted for many years due to unavailability of high-
performance computing machines. Most of the recent research reports result
on Nvidia‚Äôs GTX TitanX or GTX 980. These GPUs support eÔ¨Écient convolution
operations and hence are the recommended choice for executing experiments
involving CNNs.
‚Äì CPU: Most of the code of CNNs is executed on GPUs, but the CPU still plays an
important role as it is the hardware component that initiates calls to GPU routines,
reads and writes variables, executes instructions, and creates mini-batches, among
many other low-level tasks. Most of the deep learning libraries utilize a single CPU
core (if the GPU is also used). Therefore, it is recommended to select CPUs such
that there is one CPU core per GPU. Also, note that it is better to select a CPU that
can allocate two threads per GPU since many asynchronous function calls rely on
a second CPU thread to relay information.
‚Äì RAM: As a rule of thumb, a system supposed to be used for executing CNNs should
have at least as much RAM as that of the GPU. Additional RAM is always use-
ful, since the larger the RAM, the larger the chunk of data set (or mini-batches)

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
147
that can Ô¨Åt into it. Given that a GPU operates on mini-batches and switches them
very frequently, having a number of mini-batches readily available in RAM would
help in saving many CPU clock cycles, which may get wasted in disk read/write
operations, although it must be noted here that such eÔ¨Éciency is implementation
speciÔ¨Åc.
‚Äì Hard disk drive (HDD): The rate at which the GPU processes mini-batches
decides eÔ¨Écient read/write operations from HDD are needed. Usually, multiple
mini-batches are needed to be read asynchronously. As the GPU speeds are
increasing, it is recommended to use solid-state drives (SSDs) to keep pace with it.
‚Ä¢ Computational considerations: CNNs require a very huge amount of memory, which
presents many bottlenecks in their implementation. Therefore, the following factors
need to be taken care of while implementing CNNs:
‚Äì Size of intermediate volumes: Most of the activations in a CNN are concentrated
toward the Ô¨Årst few layers. An eÔ¨Écient CNN implementation should only store
current activations at any layer. This kind of optimization is most suited at test
time.
‚Äì Parameter size: Since the data structure storing parameters involve storing
network parameters, the gradients during back-propagation, weights, and a few
architecture-speciÔ¨Åc values, it is recommended that while allocating memory
for the parameter vector, it should be at least thrice the size of the network
parameters.
‚Äì Split the image data: Since CNNs operate on very large data sets, it is advisable to
split the image data (e.g., into mini-batches).
‚Ä¢ Avoiding overÔ¨Åtting: OverÔ¨Åtting in a neural network can be reduced by regulariza-
tion. Popular regularization techniques are L1 regularization, L2 regularization, and
dropout [39]. These are discussed here:
‚Äì L1 regularization: In this form of regularization, for each weight ùë§, a term ùúÜ|ùë§|
is added to the objective function where ùúÜrepresents the strength of regulariza-
tion. L1 normalization causes the weight vectors to become sparse, and hence they
become invariant to noisy inputs.
‚Äì L2 regularization: In this from, for every weight ùë§, a term 1
2ùúÜ‚àóùë§2 is added to
the objective function. It has the property that it diÔ¨Äuses the weight vectors.
This allows the network to utilize a majority of inputs instead of only a few of
them.
‚Äì Dropout: Dropout means switching oÔ¨Äa few neurons while training the network.
The idea is that it will force the remaining network to optimize without the dropped
neurons. Dropout can be implemented as a layer. Such a layer acts as a regular fully
connected layer with periodic dropout of neurons while training. It is important to
note that dropout does not permanently remove any neuron, hence the size of the
network remains the same after the training. In fact, once the training is over, all
the neurons are used by the network to compute the result.
6.2.6
CNN in Action
In this section, we introduce the readers to practical examples using CNNs. To get you
started with CNNs, we discuss various tools that can be used for developing CNNs as
well as work through some coding examples.

148
Hybrid Intelligence for Image Analysis and Understanding
6.2.7
Tools for Convolutional Neural Networks
‚Ä¢ CaÔ¨Äe: CaÔ¨Äe [40] is a deep learning framework developed by Berkley Vision and
Learning Center. The primary programming language is C++, but it has wrappers
for many languages. It is one of the most popular tools for implementing CNN-based
algorithms, primarily owing to its highly eÔ¨Écient implementation.
‚Ä¢ Torch: This is an open-source machine learning library [41]. It is implemented in C
and uses LuaJIT scripting language for programming.
‚Ä¢ MatConvNet: MatConvNet [42] is a recent MATLAB Toolbox speciÔ¨Åcally targeted
toward computer vision problems. It is based on MATLAB and has many pretrained
models.
‚Ä¢ MATLAB 2016a: MATLAB 2016a introduces a deep learning toolbox where many of
the CNN-related operations are supported.
6.2.8
CNN Coding Examples
We now introduce the reader to MatConvNet and show an example of how to visualize
the output of various layers in CNN architecture.
6.2.8.1
MatConvNet
MatConvNet has a simple installation process and easy-to-follow documentation. Here,
we introduce the reader to how to set up MatConvNet and a few other basic details, but
we recommend an enthusiastic reader to look into the oÔ¨Écial documentation [43] for
more insights. The following code snippet allows you to set up MatConvNet on your
system. The commands have to be executed from within your MATLAB Workspace. We
have tested these commands on MATLAB 2014b. The example installs MatConvNet and
shows you a simple example of how to classify an input image with a pretrained model.
We have added comments to the relevant sections of the code to explain their working.
% install MatConvNet
untar('<path to matconvnet tar>') ;
cd <path to extracted directory>
% compile MatConvNet
run matlab/vl_compilenn
% download a pre-trained CNN from
% http://www.vlfeat.org/matconvnet/models/
urlwrite(...
'<path to mat file from
http://www.vlfeat.org/matconvnet/models/', ...
'model.mat') ;
% setup MatConvNet
run
matlab/vl_setupnn
% load the pre‚Äìtrained CNN
net = load('model.mat') ;

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
149
% Here we use a default image
% load and preprocess an image (Example from MatConvNet
website)
im = imread('peppers.png') ;
im_ = single(im) ; % note: 0‚Äì255 range
% resize the image before providing it to CNN
im_ = imresize(im_, net.meta.normalization.imageSize(1:2)) ;
%Normalize an image to reduce noise
im_ = im_ - net.meta.normalization.averageImage ;
% execute CNN
res = vl_simplenn(net, im_) ;
% show the classification result
%extract scores
scores = squeeze(gather(res(end).x)) ;
%find the highest score from the softmax layer
[bestScore, best] = max(scores) ;
%display the image
figure(1) ; clf ; imagesc(im) ;
title(sprintf('%s (%d), score %.3f',...
net.meta.classes.description{best}, best, bestScore));
6.2.8.2
Visualizing a CNN
The best way to understand a CNN is to visualize the outputs of its various layers. The
following code example loads images for you and extracts the image outputs from vari-
ous CNN layers.
clear all;close all;clc;
run ./matconvnet-1.0-beta17/matlab/vl_setupnn
net = load('imagenet-vgg-m.mat') ;
% read image
I=imread('lena.jpg');
imshow(I);
feat = [];
im_ = single(I) ;
im_ = imresize(im_, net.meta.normalization.imageSize(1:2)) ;
im_ = im_ ‚Äì net.meta.normalization.averageImage ;
figure(5) ; clf ; imagesc(im_)

150
Hybrid Intelligence for Image Analysis and Understanding
res = vl_simplenn(net, im_) ;
% find feature vector
% first layer res(5), for 2nd layer
res(9)
% for 3rd layer res(11), for 4th layer
res(13) and so on
% because each convolution layer has four sublayers
featureVector = res(5).x;
featureVector = featureVector(:);
feature = [feature; featureVector'];
fprintf('extract %d image\n\n', i);
feat_norm = feature;
%% for 2d image matrix
for all output image
ft=featureVector;
%% for first layer image dimension is 54*54 and the depth
is 96
ty=54*54;
d = zeros(54,54,96);
for i=1:96
n=i-1
a= ft((ty*n+1):(n+1)*ty);
b=vec2mat(a,54);
d(:,:,i) = b';
end
% for displaying image
figure;
n=0 % 1,2,3,4,5,6,7,8,9
%display first nine images
% the reader may change this to visualize other images
for i= 1:9
subplot(3,3,i);
J=(i+n*9)
imshow(d(:,:,J));
end
The above code uses VGGNet [44] Architecture implementation of CNN, which
earned second place in the 2014 ImageNet Large Scale Visual Recognition Challenge
(ILSVRC) [45]. It showed that the depth of a network is an important factor for the per-
formance of a CNN. Their network consisted of 16 convolutional and fully connected
layers. Surprisingly, the architecture is very simple, performing 3 √ó 3 convolution
operations (Stride = 1, Padding = 1) and 2 √ó 2 pooling operations (max pooling; Stride
= 2) throughout the network. Due to its simplicity and ability to give strong features,
we have chosen it for demonstration in this example. MatConvNet and CaÔ¨Äe both
provide pretrained models for this architecture.
Here, we show and discuss the output of VGGNet at various layers for an example
image with the help of the above code. Due to space considerations, we will show only

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
151
Figure 6.13 Input image.
Figure 6.14 Output from the Ô¨Årst sublayer of the Ô¨Årst layer.
nine output images from each layer. The reader is encouraged to execute the code to
visualize all the output images. Figure 6.13 is our input image to the code. The Ô¨Årst
layer (i.e., the convolutional layer of the network) has four sublayers: convolution
(conv1), ReLU (relu1), normalization (norm1), and pooling (pool1). Figure 6.14 shows
the output of the Ô¨Årst sublayer (i.e. the conv1 layer), while Figure 6.15 shows the output
after the completion of the Ô¨Årst layer. As can be observed, the Ô¨Årst layer extracts the
edges from the image. This is a crucial part in any image analysis or, more speciÔ¨Åcally,
visual recognition task. The research for identiÔ¨Åcation and localization of objects spans
decades [46‚Äì49]. Figures 6.16‚Äì6.18, and 6.19 show the output up to the Ô¨Åfth layer of
the network. As you can observe, the identiÔ¨Åed elements in the images go on becoming

152
Hybrid Intelligence for Image Analysis and Understanding
Figure 6.15 Output from the Ô¨Årst layer.
Figure 6.16 Output from the second layer.
coarser, with the Ô¨Åfth layer showing rectangular boxes over the regions of interest. This
behavior can be compared with basic image segmentation and object localization tasks
where a rectangular window is moved over an image as a mask to identify regions of
interest. In fact, this is how our visual system works as well [3, 50]. This demonstrates
that CNNs also learn Ô¨Ålters, which agrees with the fundamental works in the Ô¨Åeld
of machine learning and artiÔ¨Åcial intelligence, with a twist that they learn it without
specifying what Ô¨Ålters to use (as if humans have bestowed CNNs with powers to learn
by themselves, but as with everything, it is still far from being ideal!).

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
153
Figure 6.17 Output from the third layer.
Figure 6.18 Output from the fourth layer.
6.2.8.3
Image Category ClassiÔ¨Åcation Using Deep Learning
For this coding example, we use the framework provided by MATLAB 2016a and
MatConvNet.
% Download the compressed data set from the following
location
url = 'http://www.vision.caltech.edu/Image_Datasets/
Caltech101/
101_ObjectCategories.tar.gz';
% Store the output in a temporary folder
% define output folder
outputFolder = fullfile(tempdir, 'caltech101');
% download only once

154
Hybrid Intelligence for Image Analysis and Understanding
Figure 6.19 Output from the Ô¨Åfth layer.
if ‚àºexist(outputFolder, 'dir')
disp('Downloading 126MB Caltech101 data set...');
untar(url, outputFolder);
end
rootFolder = fullfile(outputFolder, '101_ObjectCategories');
categories = {'airplanes', 'ferry', 'laptop'};
imds = imageDatastore(fullfile(rootFolder, categories),
'LabelSource', 'foldernames');
tbl = countEachLabel(imds);
% determine the smallest amount of images in a category
minSetCount = min(tbl{:,2});
% Use splitEachLabel method to trim the set.
imds = splitEachLabel(imds, minSetCount, 'randomize');
% Notice that each set now has exactly the same number of
images.
countEachLabel(imds);
% Find the first instance of an image for each category
airplanes = find(imds.Labels == 'airplanes', 1);
ferry = find(imds.Labels == 'ferry', 1);
laptop = find(imds.Labels == 'laptop', 1);

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
155
figure
subplot(1,3,1);
imshow(imds.Files{airplanes})
subplot(1,3,2);
imshow(imds.Files{ferry})
subplot(1,3,3);
imshow(imds.Files{laptop});
%Download Pre-trained Convolutional Neural Network (CNN)
% Location of pre-trained "AlexNet"
cnnURL = 'http://www.vlfeat.org/matconvnet/models/beta16/
imagenet-caffe-alex.mat';
% Store CNN model in a temporary folder
cnnMatFile = fullfile(tempdir, 'imagenet-caffe-alex.mat');
if ‚àºexist(cnnMatFile, 'file') % download only once
disp('Downloading pre-trained CNN model...');
websave(cnnMatFile, cnnURL);
end
% Load MatConvNet network into a SeriesNetwork
convnet = helperImportMatConvNet(cnnMatFile);
% Set the ImageDatastore ReadFcn
imds.ReadFcn = @(filename)readAndPreprocessImage(filename);
[trainingSet, testSet] = splitEachLabel(imds, 0.3,
'randomize');
% Get the network weights for the second convolutional layer
w1 = convnet.Layers(2).Weights;
% Scale and resize the weights for visualization
w1 = mat2gray(w1);
w1 = imresize(w1,5);
% Display a montage of network weights.
% There are 96 individual sets of weights in the first
layer.
figure
montage(w1)
title('First convolutional layer weights')
featureLayer = 'fc7';
trainingFeatures = activations(convnet, trainingSet,
featureLayer, ...

156
Hybrid Intelligence for Image Analysis and Understanding
'MiniBatchSize', 32, 'OutputAs', 'columns');
% Train a multi-class SVM
% Get training labels from the trainingSet
trainingLabels = trainingSet.Labels;
% Train multiclass SVM classifier using a fast linear
solver,
%and set 'ObservationsIn' to 'columns' to match the
arrangement used for
% training features.
classifier = fitcecoc(trainingFeatures, trainingLabels, ...
'Learners', 'Linear', 'Coding', 'onevsall',
'ObservationsIn',
'columns');
% Classifier Evaluation
% Extract test features using the CNN
testFeatures = activations(convnet, testSet, featureLayer,
'MiniBatchSize',32);
% Pass CNN image features to trained classifier
predictedLabels = predict(classifier, testFeatures);
% Get the known labels
testLabels = testSet.Labels;
% Tabulate the results using a confusion matrix.
confMat = confusionmat(testLabels, predictedLabels);
% Convert confusion matrix into percentage form
confMat = bsxfun(@rdivide,confMat,sum(confMat,2))
% Display the mean accuracy
mean(diag(confMat))
%% Test the classifier
newImage = fullfile(rootFolder, 'airplanes', 'image_0690
.jpg');
% Pre-process the images as required for the CNN
img = readAndPreprocessImage(newImage);
% Extract image features using the CNN
imageFeatures = activations(convnet, img, featureLayer);
% Make a prediction using the classifier

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
157
label = predict(classifier, imageFeatures)
function Iout = readAndPreprocessImage(filename)
I = imread(filename);
if ismatrix(I)
I = cat(3,I,I,I);
end
% Resize the image as required for the CNN.
Iout = imresize(I, [227 227]);
end
6.3
Toward Understanding the Brain, CNNs, and Images
We have already discussed neural networks and CNNs, and have related their working to
abstract biological functioning of the human brain and visual system. Therefore, we can
now move toward mentioning a few works that give more insight into CNNs, their sim-
ilarity with the functioning of the human brain, and their applications. We believe that
a reader wanting to go deeper into working with CNNs and brain-inspired algorithms
should deÔ¨Ånitely go into the details of these works, which would provide a pathway for
the reader to explore further.
6.3.1
Applications
Nearly all the problem areas in computer vision have witnessed the application of
CNNs. In fact, much of the success of CNNs can be attributed to the industry, which
contributed signiÔ¨Åcantly by providing high-performance computing infrastructure and
open-source implementations of various deep learning architectures. It would be not
only unfair but also diÔ¨Écult to diÔ¨Äerentiate industrial research from academic research,
as most of the recent research work has happened with a swift collaboration among
them. Therefore, we discuss the state-of-the-art work categorized by application areas
where CNNs have had a dramatic impact.
‚Ä¢ Object classiÔ¨Åcation: The hysterical popularity of CNNs in recent times can be
attributed to the work on ImageNet classiÔ¨Åcation in 2012 [27]. Despite being a recent
paper and provided that, in the last few years, CNNs have already seen better and
bigger architectures [51‚Äì53] (discussed in Section 6.3.2), this work stands out as
classic.
‚Ä¢ Object memorability: Object memorability is a concept that aims at Ô¨Ånding entities
in an image that are worth remembering. What makes it interesting is that while our
brain is exceptionally good at selective extraction of information from visual stimulus,
a recent research paper entitled ‚ÄúWhat makes an object memorable?‚Äù [54] by a group
at the Massachusetts Institute of Technology (MIT) has shown that CNNs can be used
to some extent to perform such kind of selection. It extends upon previous works on
image memorability [55] and photograph memorability [56].

158
Hybrid Intelligence for Image Analysis and Understanding
‚Ä¢ Object segmentation: The authors in [57] obtained CNN-based region proposals from
images and subsequently used them for object segmentation. This is among the Ô¨Årst
work to use CNNs for region proposals. The authors in [58] proposed a fully convolu-
tional network for semantic segmentation, which is obtained by using a skip architec-
ture augmented by a shallow layer trained with appearance models. Following this, the
authors in [59] propose an end-to-end architecture for obtaining object boundaries.
The architecture is based upon the VGGNet [44].
Additionally, we would like to remind the reader that CNNs are still far from achiev-
ing human-level intelligence. Hence, we also discuss a few attempts by the research
community in trying to Ô¨Ånd limiting cases of CNNs or exploring a diÔ¨Äerent learning
paradigm altogether.
‚Ä¢ Fooling CNNs: With the help of evolutionary algorithms, the authors in [32] demon-
strated that CNNs can be made to incorrectly recognize images having no meaning
to a human. In fact, the images had no pattern at all. However, despite this failure of
CNNs, there are two key takeaways. First, a CNN extracts more low- and middle-level
features as compared to features such as boundary, shape, and so on, which are con-
sidered high-level features. Second, the primary reason why it was fooled was because
it learned patterns irrespective of what an image actually contains.
‚Ä¢ Hierarchical and temporal machine: There is another school of thought that says that
if humans can learn from single-input instances, why can‚Äôt a machine do the same?
Such techniques are based on the observation that the brain works with both temporal
and spatial patterns rather than only spatial ones (in the case of visual stimulus). For
example, everything that the brain sees is a temporal pattern or simply a short video
sequence, while traditional learning architectures including CNNs are based on static
images. Theis class of techniques claiming to actually function like the brain is based
on hierarchical and temporal models. These techniques haven‚Äôt yet evolved to a stage
where they may work with images, but they should be a useful read as we still do not
understand the brain completely. The PhD thesis of Dileep George [60] from Stan-
ford University provides in-depth analysis of this paradigm along with comparison to
contemporary algorithms.
6.3.2
Case Studies
There is a CNN architecture behind the success of algorithms in each of the application
areas discussed in this chapter. Therefore, in this section, we give a brief overview of
the evolution of CNN architectures and diÔ¨Äerences among them. The architectures are
based on the fundamental organization discussed in Section 6.2.4. For detailed expla-
nation, we recommend readers to refer to the corresponding publications.
‚Ä¢ LeNet [61]: This architecture was developed in the 1990s for digit recognition in zip
codes and the like. This was the Ô¨Årst framework that formalized the use of CNNs
in practical applications with the use of convolutional, max pooling, and dense lay-
ers. This architecture was similar to the fundamental architecture discussed in this
chapter.
‚Ä¢ AlexNet [27]: The architecture was proposed in 2012 and can be actually considered as
the baseline for the modern CNN‚Äôs popularity in computer vision tasks. AlexNet won
the ILSVRC in 2012 by signiÔ¨Åcantly outperforming the contemporary techniques. It
acheived a top-Ô¨Åve error rate of 16%, as compared to 26% achieved by the runner-up.

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
159
The primary diÔ¨Äerence was that this network had more layers, had a larger number
of parameters, and utilized the power of GPU computation. In fact, the authors had
written an extremely eÔ¨Écient implementation of convolution on the GPU. The archi-
tecture ran on two 3 GB GPUs with clever oÔ¨Ñoading of computations among them
to gain signiÔ¨Åcant performance optimizations. Interestingly, they showed that ignor-
ing certain computations (e.g., those among layers not residing on the same GPU)
allowed them to achieve better error rates.
‚Ä¢ ZFNet [62]: This architecture won the ILSVRC in 2013. It tweaked various
hyper-parameters, speciÔ¨Åcally reducing the stride and size of the Ô¨Ålters in the Ô¨Årst
few layers. It also increased the size of the intermediate convolution layers, which
resulted in signiÔ¨Åcant performance gain.
‚Ä¢ GoogleNet [53]: GoogleNet won the ILSVRC in 2014. It proposed an inception module
that signiÔ¨Åcantly reduced the number of parameters. As discussed in Section 6.2.5, the
network parameters present a huge computational bottleneck. In fact, if we consider
that the progress of CNNs was at halt for decades due to these bottlenecks, one can
better appreciate the contribution of GoogleNet. It reduced the number of parameters
to 4 million as compared to AlexNet‚Äôs 60 million while consisting of 22 layers.
‚Ä¢ VGGNet [44]: This architecture secured second place in the ILSVRC in 2014. It
showed that the depth of a network critically impacts the performance of a network.
Throughout the network, convolution is performed by 3 √ó 3 Ô¨Ålters and 2 √ó 2 pooling
operations, but it is computationally very expensive and requires almost 2.5 times
the number of parameters of AlexNet.
‚Ä¢ ResNet [63]: ResNet won the ILSVRC in 2015. In order to avoid the vanishing gradient
problem, it skips connection while also utilizing batch normalization very heavily. At
the time of writing this text, ResNets and its variations are the state-of-the-art CNN
architectures, and the reader is recommended to use ResNets for solving practical
problems in computer vision.
6.4
Conclusion
In this chapter, we introduced convolutional neural networks as an algorithmic way of
understanding how the brain might work. We also explained concepts to allow read-
ers to appreciate that CNNs may not reÔ¨Çect the exact way that brains work but do
provide an intuition for development of advanced theories in this already complicated
area of understanding the human brain, the visual system, and their behavior. Lastly, we
presented coding examples and pointers to future reading materials so that interested
readers can expand upon the knowledge acquired from this chapter.
References
1 McCulloch, W.S. and Pitts, W. (1943) A logical calculus of the ideas immanent in
nervous activity. The Bulletin of Mathematical Biophysics, 5 (4), 115‚Äì133.
2 Thorpe, S., Fize, D., Marlot, C. et al. (1996) Speed of processing in the human visual
system. Nature, 381 (6582), 520‚Äì522.

160
Hybrid Intelligence for Image Analysis and Understanding
3 Blakemore, C.T. and Campbell, F. (1969) On the existence of neurones in the human
visual system selectively sensitive to the orientation and size of retinal images. The
Journal of Physiology, 203 (1), 237.
4 Field, D.J., Hayes, A., and Hess, R.F. (1993) Contour integration by the human visual
system: evidence for a local ‚Äúassociation Ô¨Åeld.‚Äù Vision Research, 33 (2), 173‚Äì193.
5 Ungerleider, L.G., Courtney, S.M., and Haxby, J.V. (1998) A neural system for human
visual working memory. Proceedings of the National Academy of Sciences, 95 (3),
883‚Äì890.
6 Isik, L., Meyers, E.M., Leibo, J.Z., and Poggio, T. (2014) The dynamics of invariant
object recognition in the human visual system. Journal of Neurophysiology, 111 (1),
91‚Äì102.
7 Bengio, Y. (2009) Learning deep architectures for ai. Foundations and Trends¬Æ; in
Machine Learning, 2 (1), 1‚Äì127.
8 Schmidhuber, J. (2015) Deep learning in neural networks: an overview. Neural
Networks, 61, 85‚Äì117.
9 Lee, H., Pham, P., Largman, Y., and Ng, A.Y. (2009) Unsupervised feature learning
for audio classiÔ¨Åcation using convolutional deep belief networks, in Advances in
Neural Information Processing Systems, pp. 1096‚Äì1104.
10 Glorot, X., Bordes, A., and Bengio, Y. (2011) Domain adaptation for large-scale
sentiment classiÔ¨Åcation: a deep learning approach, in Proceedings of the 28th Inter-
national Conference on Machine Learning (ICML-11), pp. 513‚Äì520.
11 Arel, I., Rose, D.C., and Karnowski, T.P. (2010) Deep machine learning: a new fron-
tier in artiÔ¨Åcial intelligence research [research frontier]. Computational Intelligence
Magazine, IEEE, 5 (4), 13‚Äì18.
12 Hebb, D.O. (1949) The organization of behavior: a neuropsychological theory. New
York, 4.
13 Rosenblatt, F. (1958) The perceptron: a probabilistic model for information storage
and organization in the brain. Psychological Review, 65 (6), 386.
14 Minsky, M. and Papert, S. (1969) Perceptrons. MIT Press, Cambridge, MA.
15 Rumelhart, D.E., Hinton, G.E., and Williams, R.J. (1988) Learning representations by
back-propagating errors. Cognitive Modeling, 5 (3), 1.
16 Hornik, K., Stinchcombe, M., and White, H. (1989) Multilayer feedforward networks
are universal approximators. Neural Networks, 2 (5), 359‚Äì366.
17 LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., and
Jackel, L.D. (1989) Backpropagation applied to handwritten zip code recognition.
Neural Computation, 1 (4), 541‚Äì551.
18 Tesauro, G. (1995) TD-Gammon: a self-teaching backgammon program, in Applica-
tions of Neural Networks, Springer, pp. 267‚Äì285.
19 Thrun, S. (1995) Learning to play the game of chess. Advances in Neural Information
Processing Systems, 7.
20 Cortes, C. and Vapnik, V. (1995) Support-vector networks. Machine Learning, 20 (3),
273‚Äì297.
21 LeCun, Y., Jackel, L., Bottou, L., Brunot, A., Cortes, C., Denker, J., Drucker, H.,
Guyon, I., Muller, U., Sackinger, E. et al. (1995) Comparison of learning algorithms
for handwritten digit recognition, in International Conference on ArtiÔ¨Åcial Neural
Networks, Vol. 60, pp. 53‚Äì60.

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
161
22 Hinton, G.E., Osindero, S., and Teh, Y.W. (2006) A fast learning algorithm for deep
belief nets. Neural Computation, 18 (7), 1527‚Äì1554.
23 LeCun, Y., Cortes, C., and Burges, C.J. (1998) The MNIST database of handwritten
digits. Available from http://yann.lecun.com/exdb/mnist/
24 Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H. et al. (2007) Greedy layer-wise
training of deep networks. Advances in Neural Information Processing Systems, 19,
153.
25 Glorot, X. and Bengio, Y. (2010) Understanding the diÔ¨Éculty of training deep feed-
forward neural networks, in Aistats, vol. 9, pp. 249‚Äì256.
26 Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. (2014) CNN features
oÔ¨Ä-the-shelf: an astounding baseline for recognition, in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition Workshops, pp. 806‚Äì813.
27 Krizhevsky, A., Sutskever, I., and Hinton, G.E. (2012) Imagenet classiÔ¨Åcation with
deep convolutional neural networks, in Advances in Neural Information Processing
Systems, pp. 1097‚Äì1105.
28 LeCun, Y. and Bengio, Y. (1995) Convolutional networks for images, speech, and
time series. The Handbook of Brain Theory and Neural Networks, 3361 (10), 1995.
29 Dosovitskiy, A., Tobias Springenberg, J., and Brox, T. (2015) Learning to generate
chairs with convolutional neural networks, in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pp. 1538‚Äì1546.
30 Matsugu, M., Mori, K., Mitari, Y., and Kaneda, Y. (2003) Subject independent facial
expression recognition with robust face detection using a convolutional neural
network. Neural Networks, 16 (5), 555‚Äì559.
31 Ramakrishnan, K., Scholte, S., Lamme, V., Smeulders, A., and Ghebreab, S. (2015)
Convolutional neural networks in the brain: an fMRI study. Journal of Vision, 15
(12), 371‚Äì371.
32 Nguyen, A., Yosinski, J., and Clune, J. (2015) Deep neural networks are easily fooled:
high conÔ¨Ådence predictions for unrecognizable images, in Computer Vision and Pat-
tern Recognition (CVPR), 2015 IEEE Conference, IEEE, pp. 427‚Äì436.
33 Nair, V. and Hinton, G.E. (2010) RectiÔ¨Åed linear units improve restricted Boltzmann
machines, in Proceedings of the 27th International Conference on Machine Learning
(ICML-10), pp. 807‚Äì814.
34 He, K., Zhang, X., Ren, S., and Sun, J. (2015) Delving deep into rectiÔ¨Åers: surpassing
human-level performance on ImageNet classiÔ¨Åcation, in Proceedings of the IEEE
International Conference on Computer Vision, pp. 1026‚Äì1034.
35 Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. (2013)
Maxout networks, in Proceedings of the 30th International Conference on Machine
Learning, pp. 1319‚Äì1327.
36 Socher, R., Perelygin, A., Wu, J.Y., Chuang, J., Manning, C.D., Ng, A.Y., and Potts,
C. (2013) Recursive deep models for semantic compositionality over a sentiment
treebank, in Proceedings of the Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), vol. 1631, Citeseer, p. 1642.
37 Zeiler, M.D. and Fergus, R. (2013) Stochastic pooling for regularization of deep con-
volutional neural networks. ICLR 2013.
38 Springenberg, J., Dosovitskiy, A., Brox, T., and Riedmiller, M. Striving for simplicity:
the all convolutional net, in ICLR (workshop track).

162
Hybrid Intelligence for Image Analysis and Understanding
39 Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R.
(2014) Dropout: a simple way to prevent neural networks from overÔ¨Åtting. Journal of
Machine Learning Research, 15 (1), 1929‚Äì1958.
40 Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S.,
and Darrell, T. (2014) CaÔ¨Äe: convolutional architecture for fast feature embedding.
arXiv preprint arXiv:1408.5093.
41 Torch. (2016) Available from http://torch.ch/#
42 Vedaldi, A. and Lenc, K. (2015) Matconvnet convolutional neural networks for
Matlab, in Proceeding of the ACM International Conference on Multimedia.
43 MatConvNet. (2016). Available from http://www.vlfeat.org/matconvnet/.
44 Simonyan, K. and Zisserman, A. (2014) Very deep convolutional networks for
large-scale image recognition. CoRR, abs/1409.1556.
45 Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., and Fei-Fei, L. (2015) ImageNet
Large Scale Visual Recognition Challenge. International Journal of Computer Vision
(IJCV), 115 (3), 211‚Äì252, 10.1007/s11263-015-0816-y.
46 Lowe, D.G. (1999) Object recognition from local scale-invariant features, in
Computer Vision, 1999. The Proceedings of the Seventh IEEE International
Conference, vol. 2, IEEE, pp. 1150‚Äì1157.
47 Warrington, E.K. and Taylor, A.M. (1973) The contribution of the right parietal lobe
to object recognition. Cortex, 9 (2), 152‚Äì164.
48 Lowe, D.G. (1987) Three-dimensional object recognition from single
two-dimensional images. ArtiÔ¨Åcial Intelligence, 31 (3), 355‚Äì395.
49 Srivastava, S., Mukherjee, P., and Lall, B. (2015) Characterizing objects with sika fea-
tures for multiclass classiÔ¨Åcation. Applied Soft Computing.
50 Field, G.D., Gauthier, J.L., Sher, A., Greschner, M., Machado, T.A., Jepson, L.H.,
Shlens, J., Gunning, D.E., Mathieson, K., Dabrowski, W. et al. (2010) Functional
connectivity in the retina at the resolution of photoreceptors. Nature, 467 (7316),
673‚Äì677.
51 Simonyan, K. and Zisserman, A. (2014) Very deep convolutional networks for
large-scale image recognition. arXiv preprint arXiv:1409.1556.
52 Ovtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K., and Chung, E.S. (2015)
Accelerating deep convolutional neural networks using specialized hardware. Avail-
able from http://research.microsoft.com/apps/pubs/default.aspx?id=240715.
53 Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
Vanhoucke, V., and Rabinovich, A. (2015) Going deeper with convolutions, in
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1‚Äì9.
54 Dubey, R., Peterson, J., Khosla, A., Yang, M.H., and Ghanem, B. (2015) What makes
an object memorable?, in Proceedings of the IEEE International Conference on Com-
puter Vision, pp. 1089‚Äì1097.
55 Isola, P., Xiao, J., Torralba, A., and Oliva, A. (2011) What makes an image memo-
rable?, in Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference,
IEEE, pp. 145‚Äì152.
56 Isola, P., Xiao, J., Parikh, D., Torralba, A., and Oliva, A. (2014) What makes a photo-
graph memorable? Pattern Analysis and Machine Intelligence, IEEE Transactions, 36
(7), 1469‚Äì1482.

Brain-Inspired Machine Intelligence for Image Analysis: Convolutional Neural Networks
163
57 Girshick, R., Donahue, J., Darrell, T., and Malik, J. (2014) Rich feature hierarchies
for accurate object detection and semantic segmentation, in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 580‚Äì587.
58 Long, J., Shelhamer, E., and Darrell, T. (2015) Fully convolutional networks for
semantic segmentation, in Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pp. 3431‚Äì3440.
59 Ren, S., He, K., Girshick, R., and Sun, J. (2015) Faster r-CNN: towards real-time
object detection with region proposal networks, in Advances in Neural Information
Processing Systems, pp. 91‚Äì99.
60 George, D. (2008) How the brain might work: a hierarchical and temporal model for
learning and recognition, PhD thesis, Stanford University.
61 LeCun, Y., Bottou, L., Bengio, Y., and HaÔ¨Äner, P. (1998) Gradient-based learning
applied to document recognition. Proceedings of the IEEE, 86 (11), 2278‚Äì2324.
62 Zeiler, M.D. and Fergus, R. (2014) Visualizing and understanding convolutional
networks, in European Conference on Computer Vision, Springer, pp. 818‚Äì833.
63 He, K., Zhang, X., Ren, S., and Sun, J. (2015) Deep residual learning for image recog-
nition, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-
nition.

165
7
Human Behavioral Analysis Using Evolutionary Algorithms and
Deep Learning
Earnest Paul Ijjina and Chalavadi Krishna Mohan
Visual Learning and Intelligence Group (VIGIL), Department of Computer Science and Engineering, Indian Institute of
Technology Hyderabad (IITH), Hyderabad, Telangana, India
7.1
Introduction
Human behavior analysis refers to the use of machine learning techniques and computer
vision to recognize and classify human behavior. One may classify human behavior
into gesture, event, action, and activity based on the duration for which the subjects
motion is analyzed. Depending on the region of interest, this can be further classiÔ¨Åed
into facial expression, hand-gesture, or upper/lower-body action. In the last decade,
recognizing actions in videos has gained a lot of interest in the computer vision research
community due to its applications in ambient assisted living, health monitoring, video
analytics, sports analysis, robotics, and automatic video surveillance. Various surveys
are proposed in the literature, analyzing and categorizing existing approaches to human
action recognition based on the characteristic considered for recognition. For instance,
[1] covers various approaches for recognizing actions of a single subject, [2] discusses
approaches for multiview human action recognition, and [3] presents approaches that
categorize full-body motion into spatial and temporal structures. A survey of various
video data sets proposed over the years for human action recognition is available in
[4]. These data sets diÔ¨Äer in objective of creation like detection of realistic interactions,
actions, and multiview actions. The traditional approaches to action recognition
consider features extracted from input observations to provide the necessary discrimi-
native information. The most commonly used features in the literature are bag of visual
words [5], histograms-oriented gradient [6], histograms of optical Ô¨Çow [6], motion
boundary histograms [7], action bank representation [8], and dense trajectories [9].
Due to the task-speciÔ¨Åc design of these features, these features may not be eÔ¨Écient for
other visual recognition tasks that lead to data-driven approaches like deep learning,
and that can automatically learn a hierarchy of discriminative features from input
representation [10].
Deep learning is an emerging machine learning technique that has seen rapid
evolution in the last decade. The availability of general-purpose graphic processing
units (GPGPUs) has aided this growth by enabling the parallel implementation of
these deep learning models. The implementation of deep learning models by neural
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

166
Hybrid Intelligence for Image Analysis and Understanding
networks makes them referred to as deep neural network (DNN) models. Even though
neural network models were proposed in the early 1940s, due to the lack of eÔ¨Äective
training algorithms, only shallow neural networks were used till the early 2000s. Some
of the seminal work by pioneers in deep learning like Yann LeCun, Yoshua Bengio, and
GeoÔ¨Ärey Hinton has set the groundwork on which the Ô¨Åeld of deep learning is evolving.
A convolutional neural network (CNN) proposed by Yann LeCun et al. in [11, 12] uses
weight sharing to reduce the number of free parameters to be learned during training.
The greedy layer-wise training of deep belief networks [13, 14] is some of the most exten-
sively used work by the deep learning community. The deep learning models are trained
on fairly raw data, to learn a hierarchy of features for achieving a recognition task. This
ability to extract discriminative features from raw data led to their usage on various
modalities of data like text, audio, and video, to name a few. Some of the well-known
applications of convolutional neural networks (CNN) is the MNIST digit recognition
[15], ILSVRC challenge for object recognition [16‚Äì18], face detection [19], ACM Facial
Expression recognition in the Wild (AFEW) challenge [20], and action recognition
[21, 22]. Among the deep learning models, CNN is the most widely used approach for
visual recognition that consists of alternating layers of convolution and subsampling.
In contrast to the conventional techniques that rely on (hand-crated) features for
discriminative information, deep learning models extract a hierarchy of discriminative
features from input data. Baccouche et al. [23] utilized the spatiotemporal evolution
of features generated by a 3D CNN for a sequence of frames with a recurrent neural
network to recognize human actions in the KTH data set. Shuiwang Ji et al. [21]
proposed a 3D CNN model using gray, optical-Ô¨Çow, and gradient information along
x and y directions, features for recognizing human actions in surveillance videos. In
[24], Keze Wang et al. extended CNNs by incorporating a structure to manipulate
the activation of neurons. During recognition, the variation in temporal composition
of activities is handled by the partial activation of neural network conÔ¨Åguration. The
videos are decomposed into temporal segments of subactivities by a spatiotemporal
CNN to recognize human actions in [25]. The radius-margin regularization is used in
the learning algorithm employed to iteratively optimize the human action recognition
model for RGBD videos. A recurrent neural network is used for modeling the temporal
evolution of state dynamics in [26] for action recognition. A two-stream CNN to
recognize human actions from still frames and motion between frames is proposed in
[27]. The softmax scores across the streams are combined to determine the action label.
Even though deep learning models are eÔ¨Äective for various classiÔ¨Åcation tasks [28],
eÔ¨Äective training deep architecture [29] is still a challenge. In a fully connected DNN
trained using a gradient descent algorithm, the gradient decreases as the error is
back-propagated from the output to the input layer, thereby causing the vanishing
gradient problem [30]. As a result, the weights in the beginning layers won‚Äôt be
optimized. This is addressed in a CNN by reducing the number of distinct weights at
each layer and the number of neurons from which a neuron computes its output (i.e.,
local connectivity). A CNN trained using back-propagation algorithms (BPAs) may
not be optimal as it may get stuck in a local optima, and its performance after training
depends on its initial weights. Therefore, the optimization of a CNN classiÔ¨Åer is now
translated to Ô¨Ånding optimum initial weights of the neural network. There are several
other optimization and regularization techniques like: (1) early stopping [31] may
avoid overÔ¨Åtting the data, (2) co-adaptation can be avoided using dropout, (3) rectiÔ¨Åed

Human Behavioral Analysis Using Evolutionary Algorithms and Deep Learning
167
linear units (ReLUs), and (4) eÔ¨Écient weight initialization through pre-training [32]
proposed in the literature. Evolutionary techniques were also used in the last few
years to optimize DNN. David et al. [33] used evolutionary algorithm (EA)-assisted
back-propagation to optimize the weights of a sparse autoencoder. Fedorovici et al.
[34] used evolutionary optimization techniques like the gravitational search algorithm
[35] and particle swarm optimization [36] for optimizing CNN weights.
This chapter proposes a hybrid deep learning approach to determine the weights of
CNN classiÔ¨Åers by exploiting the local and global search capabilities of gradient descent
and EAs for human action recognition in videos. The novelty of this work lies in: (1)
using EAs for exploring diÔ¨Äerent basins, (2) utilizing the eÔ¨Äectiveness of the gradient
descent algorithm in Ô¨Ånding the local optimum for a given basin, and (3) combining
complementary information across classiÔ¨Åers generated by the evolutionary framework.
In this chapter, Section 7.2 describes the evolutionary deep learning approach to action
recognition. The experimental evaluation of this work is given in Section 7.3, followed
by conclusions and future work in Section 7.4.
7.2
Human Action Recognition Using Evolutionary
Algorithms and Deep Learning
In this section, we present a DNN model optimized using EA for human action recog-
nition in videos. The block diagram of the proposed action recognition framework is
shown in Figure 7.1. As shown in the Ô¨Ågure, the EA (with evolutionary strategy) is used
for initializing the weights of the CNN classiÔ¨Åer that recognizes actions from action
bank representation of videos. As the convolution kernels impact the eÔ¨Äectiveness of
a CNN classiÔ¨Åer, we use the value of kernels and the seed value given to the random
INITIALIZE
1
1
2
3
4
5
6
4
5
6
3
2
SELECTION
MUTATION
FITNESS
EVALUATION
RANDOM #
GENERATOR
CHROMOSOME
CROSSOVER
POPULATION
{c1, c2, ..., cn}
FINAL
POPULATION
CONVOLUTION
LAYER with
1@1x21 mask 
ACTION BANK
FEATURES
(mx72) 
ACTION BANK
FEATURES
COMBINATION
ACTION
LABEL
classifier
CNN1
classifier
CNN2
classifier
STEP
initialize the population.
evaluate the fitness of chromosomes,
by initializing a CNN classifier.
execute evolutionary framework
for multiple iterations.
initialize the CNN classifiers using
the final population
combine the classification evidences
of CNN classifiers generated using
evolutionary approach.
harvest the final population.
STEP
STEP
STEP
STEP
STEP
CNNn
FEATURE EXTRACTION
CLASSIFIER
FEATURE MAP 1
(1@mx52) 
FEATURE MAP 2
(1@mx26) 
FEATURE MAP 3
(2@mx6) 
FEATURE MAP 4
(2@mx3) 
CONVOLUTION
LAYER with
2@1x21 mask 
SUBSAMPLING
LAYER with
1@1x2 mask 
SUBSAMPLING
LAYER with
2@1x2 mask 
FULLY-
CONNECTED
NEURAL
NETWORK
ACTION
LABEL
Figure 7.1 Pictorial representation of various steps in the proposed classiÔ¨Åcation system. Best viewed
in color.

168
Hybrid Intelligence for Image Analysis and Understanding
number generator of fully connected neural networks classiÔ¨Åer as the chromosome. As
multiple (n) solutions are generated by evolutionary algorithms, the evidence across
these classiÔ¨Åers is combined using fusion rules to determine the class label.
An introduction to EA for search optimization is presented in Section 7.2.1. Next,
we discuss action bank features in Section 7.2.2, followed by the DNN architecture
used to recognize human actions in Section 7.2.3. As the eÔ¨Äectiveness of DNN models
trained using BPAs is dependent on weight initialization, we aim to Ô¨Ånd an optimum
weight initialization using EAs. We conclude with results, analysis, and future work of
the proposed action recognition framework. The following subsection introduces EAs
and elaborates on how the Ô¨Åtness of the population improves over EA iterations.
7.2.1
Evolutionary Algorithms for Search Optimization
EAs are based on Charles Darwin‚Äôs principle of survival of the Ô¨Åttest [37]. EA model
an optimization problem by capturing the most signiÔ¨Åcant attributes of the system as a
chromosome. For instance, the kernels of a CNN that determine the eÔ¨Äectiveness of a
CNN classiÔ¨Åer can by used as the chromosome to optimize the CNN classiÔ¨Åer. These
chromosomes representing various candidate solutions need to be evaluated using a
cost function that represents their Ô¨Åtness as it is used in crossover and mutation oper-
ations. A crossover operation constructs new chromosomes from existing ones, and
mutation disrupts the genes in the existing chromosomes. When using EA to optimize
the weights of a CNN classiÔ¨Åer, mutation will be more eÔ¨Äective in keeping the solution
from getting stuck in a local optima. Hence, the (with evolutionary strategy, i.e., muta-
tion probability is greater than crossover probability) is used in this work to optimize the
weights of a CNN classiÔ¨Åer. As the weights of a practical neural network are real num-
bers, using only EA to learn the weights would take a greater number of iterations. To
overcome this challenge, we use BPA to optimize the weights initialized by EAs, before
computing its Ô¨Åtness. As a result, an evolutionary DNN model leveraging the fast con-
vergence of BPAs and search capabilities of EAs is produced. The next section introduces
action recognition using action bank features.
7.2.2
Action Bank Representation for Action Recognition
Action bank representation for human action recognition was proposed by Sadanand
et al. in [8]. Similar to a bag of visual words (BoVW) approach, action bank represen-
tation consists of a set of video templates for the actions to be recognized. To compute
action bank features of a video, it is compared with the template videos to generate a
1 √ó 73 vector corresponding to each template, which captures the similarity of video
with the template. If an action bank with m templates is used in this computation, then
an action bank representation of size m √ó 73 gets generated. As diÔ¨Äerent observations
of the same action may have similar motion, action bank representation of observations
may contain similar local patterns. From the action bank features of observations the
KTH data set in Figure 7.2, it can be observed that observations of the same action may
have similar local patterns.
As the location and extent of similarity of action bank representation depend on the
similarity of motion between the video and the templates, a visual recognition algorithm
is needed to learn the patterns associated with each class. As a CNN classiÔ¨Åer can learn
the necessary discriminative local features from the input data, we use a CNN classiÔ¨Åer

Human Behavioral Analysis Using Evolutionary Algorithms and Deep Learning
169
(i)
(ii)
(iii)
(iv)
(v)
(vi)
Figure 7.2 Action bank features of observations in KTH: (i‚Äìiii) are videos of boxing action and (iv‚Äìvi)
are videos of running.
to recognize actions from action bank representation. The next section elaborates the
architecture of CNN classiÔ¨Åer considered to recognize human actions.
7.2.3
Deep Convolutional Neural Network for Human Action Recognition
The architecture of the CNN classiÔ¨Åer considered for recognizing human actions from
action bank representation of videos is shown in Figure 7.3. The Ô¨Årst 72 features of action
bank representation are used to represent videos as a m √ó 72 image. Here, m represents
the number of templates in the action bank used to generate action bank representation.
As each row in this image represents the similarity of the video with one template, there
is no correlation between pixels along the vertical axis. Hence, we use 1 √ó 21 kernels in
convolution layers and 1 √ó 2 kernels in sub sampling layer. Refer to [38] for an overview
to the working of the CNN classiÔ¨Åer. The (Convnet) features generated by the deep CNN
are used by a classiÔ¨Åer for action recognition.
CONVOLUTION
LAYER with
1@1x21 mask 
ACTION BANK
FEATURES
(mx72) 
FEATURE EXTRACTION
CLASSIFIER
FEATURE MAP 1
(1@mx52) 
FEATURE MAP 2
(1@mx26) 
FEATURE MAP 3
(2@mx6) 
FEATURE MAP 4
(2@mx3) 
CONVOLUTION
LAYER with
2@1x21 mask 
SUBSAMPLING
LAYER with
1@1x2 mask 
SUBSAMPLING
LAYER with
2@1x2 mask 
FULLY-
CONNECTED
NEURAL
NETWORK
ACTION
LABEL
Figure 7.3 CNN classiÔ¨Åer for recognizing actions in videos.

170
Hybrid Intelligence for Image Analysis and Understanding
As the convolution kernels in a CNN are the feature detectors, optimizing these matri-
ces is essential for the eÔ¨Äectiveness of the CNN classiÔ¨Åer. The next subsection describes
the proposed hybrid training approach for optimizing CNN classiÔ¨Åers.
7.2.4
CNN ClassiÔ¨Åer Optimized Using Evolutionary Algorithms
As the eÔ¨Äectiveness of a CNN classiÔ¨Åer trained using a gradient descent algorithm is
dependent on its initial weights, optimal initialization of DNN is essential for designing
an eÔ¨Äective CNN classiÔ¨Åer. Even though the DNN model trained using a BPA converges
quickly, the solution might get trapped in a local minima. Hence, an evolutionary search
algorithm is employed to identify the optimum weight initialization that maximizes per-
formance. To avoid overÔ¨Åtting using BPAs and to reduce the number of iterations to Ô¨Ånd
the optimum weights of the DNN using EAs, we use a hybrid approach. We train the
CNN classiÔ¨Åer initialized using EAs for a small number of epochs (p) using BPAs before
evaluating its Ô¨Åtness. As a result, we exploit the fast convergence capability of BPAs and
the search capability of EAs. The proposed system aims to Ô¨Ånd the global optimum solu-
tion by using EA to identify various basins in the error surface and BPAs to Ô¨Ånd the local
optimum quickly in a given basin. The next section describes the experimental study on
the KTH and UCF50 data sets.
7.3
Experimental Study
The details of the experimental setup used to evaluate the proposed approach on the
KTH [39] and UCF50 [40] data sets are discussed in this section. Precomputed action
bank features for these data sets, available at [41], are used in this work for comparability
of results with existing approaches. The proposed system is implemented in MATLAB
using [42] to implement the CNN classiÔ¨Åer shown in Figure 7.3 and the native optimiza-
tion functionality for designing EAs with evolutionary strategy. The range of weights of
CNN kernels is set to vary between ‚àí100 and 100. Similarly, the seed value of the ran-
dom number generator is set to vary between 0 to 5000. The population size (n) of 20
is used in EA, and the algorithm is run for Ô¨Åve iterations with a mutation probability of
0.8. The optimum value for these parameters is determined empirically. The outputs of
the CNN classiÔ¨Åers is binary coded, and an input is assigned the class label correspond-
ing to the output with the highest value. The fusion of evidence across classiÔ¨Åers applies
the fusion rule on the corresponding indexes of outputs of CNN classiÔ¨Åer, to obtain
the binary coded outputs of the fusion model. The outputs are interpreted similar to a
classiÔ¨Åer with binary coded outputs to determine the class label. The next subsections
describe the experimental setup used for the UCF50 and KTH data sets.
7.3.1
Evaluation on the UCF50 Data Set
The UCF50 data set is a collection of realistic videos corresponding to 50 human actions
gathered from Youtube. The large number of classes and unconstrained realistic nature
of videos make this a challenging data set. We consider the pre-computed action bank
features available at [41] computed with an action bank of 205 (m) templates, which
also ensures comparability of results with existing approaches. The data set is evaluated
using Ô¨Åvefold cross-validation (CV), where the entire data is split into Ô¨Åve sets, and four

Human Behavioral Analysis Using Evolutionary Algorithms and Deep Learning
171
sets are used for training and one set as test data. This process is repeated Ô¨Åve times
changing the set considered for testing in each execution. An n-fold CV splits the data
into n parts and uses one part for testing and the remaining parts for training, performed
n times and changing the part considered for testing each time. We use the notation
Set-i for representing an execution using the ith set for testing and the remaining sets
as training data. The overall performance across the n executions is considered as the
accquracy for n-fold cross-validation. This is the most widely used evaluation strategy as
it ensured that each observation is used for testing. The CNN classiÔ¨Åer initialized by the
evolutionary algorithm is trained by the gradient descent algorithm (BPA) in batch mode
for 50 (p) epochs before evaluating its Ô¨Åtness. The parameters are optimized empirically.
In this work, we aim to reduce the number of iterations EA needs for convergence, by
expediting the local search in a given basin using the steepest-descent algorithm.
For Ô¨Åve-fold CV on UCF50, the Ô¨Årst four sets are trained using a batch size of 10 and
the Ô¨Åfth set is trained with a batch size of 9. The variation in Ô¨Åtness value of the popula-
tion over EA iterations, for UCF50, is given in Table 7.1. The steady decrease in Ô¨Åtness
of the population over iterations indicate the convergence of EA and thereby the proper
selection of parameters. This completes step 4 of Figure 7.1, which results in the gener-
ation of 20 (n) candidate weights for CNN classiÔ¨Åers for each fold. The performance of
these candidate solutions {c01, c02,..., c20} is given in Table 7.2. It can be observed that
the average performance of candidate solutions across the splits is less than 97%.
As a single number is considered for optimizing the weights of a neural network (NN)
classiÔ¨Åer in Figure 7.1, its performance may not be optimum. Experiments are con-
ducted by replacing the NN classiÔ¨Åer with an ELM classiÔ¨Åer [43] for all the candidate
solutions. The performance of all the candidate solutions generated for all the splits
using the ELM classiÔ¨Åer is given in Table 7.3. It can be observed that average perfor-
mance of candidate solutions across all the splits is more than 98.8%, which is better
than the one using the NN classiÔ¨Åer given in Table 7.2, which could be due to better
generalization ability of ELM.
As discussed in step 6 of Figure 7.1, the classiÔ¨Åcation evidence generated by the n
candidate solutions is combined using various fusion rules. The performance of the pro-
posed hybrid approach on the UCF50 dataset using diÔ¨Äerent fusion functions and ELM
classiÔ¨Åer is shown in Table 7.4. It can be observed that the accuracy across all fusion
rules is above 99% and the best performance of 99.9% is achieved with Max fusion-rule,
which could be due to the low deviation in accuracy across classiÔ¨Åers used in fusion.
Table 7.1 The variation in Ô¨Åtness value of EA populations over iterations for UCF50
Iteration
Set-1
Set-2
Set-3
Set-4
Set-5
best
mean
best
mean
best
mean
best
mean
best
mean
1
17.5
48.6
26.5
45.3
2.8
24.2
21.3
50.3
14.4
41.4
2
13.6
35.9
5.3
28.6
2.4
5.7
5.4
15.9
6.1
19.4
3
5.2
17.4
3.9
10.0
2.4
3.5
4.9
10.9
5.9
7.9
4
5.1
9.1
3.8
8.7
2.4
3.2
4.6
6.8
5.6
8.1
5
3.0
5.6
3.8
8.4
2.4
3.2
4.4
6.3
5.6
7.4

172
Hybrid Intelligence for Image Analysis and Understanding
Table 7.2 Accuracy (in %) of candidate solutions generated for UCF50 using neural network classiÔ¨Åer
Candidate
ClassiÔ¨Åcation accuracy
solution
Set-1
Set-2
Set-3
Set-4
Set-5
c01
95.91
96.14
97.13
95.06
94.36
c02
94.80
90.98
96.98
91.79
90.93
c03
94.50
94.62
97.13
93.31
91.62
c04
91.30
90.61
97.21
93.84
92.91
c05
95.91
95.23
97.13
92.24
92.61
c06
96.95
94.32
97.36
94.45
93.06
c07
96.80
94.55
97.06
90.04
92.00
c08
93.09
93.03
95.09
92.17
92.30
c09
91.38
90.53
97.43
94.30
92.84
c10
93.31
91.82
96.83
94.37
92.38
c11
91.75
90.38
97.36
93.69
92.30
c12
96.58
92.50
96.68
92.47
92.00
c13
91.82
83.71
97.21
91.86
93.45
c14
95.84
94.85
89.36
94.30
92.23
c15
90.71
91.52
96.98
93.00
92.53
c16
96.13
94.92
96.91
92.47
94.28
c17
95.84
91.74
96.53
94.22
93.22
c18
94.50
86.44
96.45
94.30
92.23
c19
92.57
92.80
96.75
92.93
92.23
c20
94.20
78.94
94.64
91.79
92.23
Mean
94.19 ¬± 2.06
91.48 ¬± 4.22
96.41 ¬± 1.81
93.13 ¬± 1.27
92.58 ¬± 0.82
The confusion matrix for UCF50 corresponding to 99.90% accuracy is shown in
Table 7.5. The true class labels are given on the vertical axis, and the predicted class
labels on the horizontal axis. Thus, all nondiagonal elements in this matrix represent
the misclassiÔ¨Åed observations. It can be observed that only 6 out of 6617 observations
got misclassiÔ¨Åed by the proposed approach. The performance of the existing and
proposed approaches for UCF50 is given in Table 7.6. The high performance for the
proposed approach indicates the eÔ¨Äectiveness in identifying the optimum weights
of the CNN. It can be observed that the proposed approach using an evolutionary
algorithm for weight initialization has better performance when compared to random
weight initialization used by Earnest Ijjina et al. in [44]. The next section describes the
experimental study conducted on the KTH data set.
7.3.2
Evaluation on the KTH Video Data Set
The KTH video data set, proposed in 2004 by Christian Schuldt et al. in [39], consists
of six actions (running, walking, jogging, hand clapping, hand waving, and boxing) per-
formed in an outdoor environment by 25 subjects. The action bank representation is
generated with an action bank of 202 templates. The CNN classiÔ¨Åers initialized by EA are

Human Behavioral Analysis Using Evolutionary Algorithms and Deep Learning
173
Table 7.3 Performance (in %) of candidate solutions generated for UCF50 using ELM classiÔ¨Åer
Candidate
ClassiÔ¨Åcation accuracy
solution
Set-1
Set-2
Set-3
Set-4
Set-5
c01
100.00
99.92
100.00
99.85
99.39
c02
100.00
98.94
100.00
98.56
98.93
c03
100.00
99.77
100.00
99.47
98.17
c04
100.00
98.71
100.00
99.16
98.48
c05
100.00
99.70
100.00
99.24
99.54
c06
99.70
99.85
100.00
99.70
98.48
c07
99.93
99.85
100.00
97.49
99.31
c08
100.00
99.62
99.85
99.47
98.70
c09
100.00
98.48
100.00
99.70
98.86
c10
100.00
99.92
100.00
99.77
98.78
c11
100.00
98.64
100.00
99.39
98.78
c12
99.78
99.55
100.00
99.70
99.01
c13
100.00
96.74
100.00
98.71
99.16
c14
100.00
99.55
98.87
99.70
98.48
c15
100.00
98.79
99.92
99.85
99.01
c16
99.93
99.70
100.00
99.70
99.54
c17
100.00
99.62
100.00
99.70
99.09
c18
100.00
97.20
99.92
99.70
98.86
c19
100.00
99.70
100.00
99.85
99.01
c20
100.00
93.03
99.70
98.33
99.54
Mean
99.97 ¬± 0.08
98.86 ¬± 1.63
99.91 ¬± 0.26
99.35 ¬± 0.62
98.96 ¬± 0.39
Table 7.4 Accuracy (in # misclassiÔ¨Åed observations) using fusion on UCF50
Data
No.
Fusion-rule
Majority
fold
observations
Min
Max
Avg
Prod
Median
voting
Set-1
1345
0
0
0
0
0
0
Set-2
1320
3
0
0
0
1
2
Set-3
1325
0
0
0
0
0
0
Set-4
1315
3
3
3
4
3
3
Set-5
1312
9
3
8
8
11
11
Total
6617
15
6
11
12
15
16
Accuracy (in %) =
99.77
99.90
99.83
99.81
99.77
99.75

174
Hybrid Intelligence for Image Analysis and Understanding
Table 7.5 Confusion matrix for UCF50
Baseball pitch 150
Basket ball
137
Bench press
160
Biking
144  1
Billiards
150
Breast stroke
101
Clean and jerk
112
Diving
153
Drumming
161
Fencing
111
Golf swing
142
High Jump
123
Horse Race
127
Horse Riding
196
Hula Hoop
125
Javelin Throw
117
Juggling Balls
122
Jump Rope
144
Jumping Jack
123
Kaya king
157
Lunges
141
Military Parade
127
Mixing
141
Nun chucks
132
Pizza Tossing
114
Playing Guitar
160
Playing Piano
105
Playing Tabla
111
Playing Violin
100
Pole vault
160
Pommel horse
123
Pull ups
100
Punch
160
Push Ups
102
Rock Climbing Indoor
143
Rope Climbing
130
Rowing
136
Salsa Spin
133
Skate Boarding
120
Skiing
143
1
Skijet
100
Soccer Juggling
156
Swing
136
1
1
1
Tai Chi
100
Tennis Swing
167
Throw Discus
131
Trampoline Jumping
119
Volley ball spiking
116
Walking with dog
122
1
YoYo
Baseball pitch
Basket ball
Benchp ress
Biking
Billiards
Breast stroke
Clean and jerk
Diving
Drumming
Fencing
Golf swing
High Jump
Horse Race
Horse Riding
Hula Hoop
Javelin Throw
Juggling Balls
Jump Rope
Jumping Jack
Kaya king
Lunges
Military Parade
Mixing
Nun chucks
Pizza Tossing
Playing Guitar
Playing Piano
Playing Tabla
Playing Violin
Pole vault
Pommel horse
Pull ups
Punch
Push Ups
Rock Climbing Indoor
Rope Climbing
Rowing
Salsa Spin
Skate Boarding
Skiing
Skijet
Soccer Juggling
Swing
Tai Chi
Tennis Swing
Throw Discus
Trampoline Jumping
Volleyball spiking
Walking with dog
YoYo
128
Table 7.6 Performance on the UCF50 data set
Approach
Accuracy %
Sreemanananth et al. [8]
57.9
Kliper-Gross et al. [45]
68.5
Feng et al. [46]
71.7
Wang et al. [47]
71.7
Wang et al. [9]
75.7
Zhou et al. [48]
80.2
Ijjina et al. [49]
94.0
Ballas et al. [50]
94.1
Ijjina et al. [44]
99.68
Proposed approach
99.90

Human Behavioral Analysis Using Evolutionary Algorithms and Deep Learning
175
trained using a gradient-descent algorithm in batch mode for 15 (p) epochs before eval-
uating their Ô¨Åtness. The parameters are optimized empirically. Similar to the previous
data set, the steps in Figure 7.1 are also followed for this data set. The average accuracy
of the population using NN and ELM classiÔ¨Åers is shown in Table 7.7. The evidences of
all candidate solutions generated by the proposed approach with ELM classiÔ¨Åer com-
bined using an Avg-fusion rule is 96.76%, as given in this table. The accuracy of CNN
classiÔ¨Åer trained using BPA is very low (16.77%), which could be due to the possibility
of the NN solution getting stuck in a local optima when trained using BPA. The perfor-
mance of various approaches for this data set is shown in Table 7.8. It can be observed
that the accuracy of the proposed approach is comparable with existing state-of-the-art
approaches. The relatively low performance of the proposed approach for the KTH data
set (when compared to UCF50) could be due to the use of a Ô¨Åxed training set with a
smaller number of training observations. As the performance of neural network mod-
els depends on the number of observations used to train them, the proposed approach
with neural network implementation has low accuracy for the KTH data set, when com-
pared to (99.9% for) the UCF50 data set. The next section presents the analysis of results
generated by this approach.
Table 7.7 Performance (in %) of CNN features with NN and ELM classiÔ¨Åers generated by the proposed
approach [with back-propagation algorithm (BPA) and evolutionary algorithms (EA)] on the KTH data
set
Training approach
Accuracy
CNN classiÔ¨Åer with only EA (i.e., initialized using EA)
84.07 ¬± 7.31
CNN classiÔ¨Åer without EA (i.e., trained using BPA)
16.77
CNN features with NN classiÔ¨Åer (generated using both EA and BPA)
94.12 ¬± 1.77
CNN features with ELM classiÔ¨Åer (generated using both EA and BPA)
96.22 ¬± 0.50
Fusion of CNN features with ELM classiÔ¨Åer (generated using both EA and BPA)
96.76
Table 7.8 Performance (in %) on the KTH data set
Approach
Accuracy
Kliper-Gross et al. [51]
83.3
Ji et al. [21]
90.2
Ryoo et al. [52]
91.1
Laptev et al. [53]
91.8
Le Quoc et al. [54]
93.9
IosiÔ¨Ådis et al. [55]
93.52
Amer et al. [56]
96.8
Proposed approach
96.76

176
Hybrid Intelligence for Image Analysis and Understanding
7.3.3
Analysis and Discussion
In this section, we analyze the various components used in the proposed system to iden-
tify their signiÔ¨Åcance. The performance of the CNN classiÔ¨Åer is given in Figure 7.3 when
trained using gradient-descent algorithms (BPAs and evolutionary algorithms (EA), and
the hybrid approach is shown in Table 7.9. It can be observed that the proposed hybrid
training approach achieves better performance than the rest of the alternatives for the
UCF50 data set. Similar observations can be made from Table 7.7 for the KTH data set.
These tables report the average performance across classiÔ¨Åers generated using EAs. The
experiments corresponding to using only EAs were conducted for Ô¨Åve iterations with a
population size of 200. Thus, by training a CNN initialized using EAs with BPAs, optimal
weights were found in less iterations using a small population. The average accuracy of
the candidate solutions generated by the proposed approach using NN and ELM clas-
siÔ¨Åers is given in Table 7.10. The ELM classiÔ¨Åer is found to perform better than the NN
classiÔ¨Åer. Similar observations can be made from Table 7.7 for the KTH data set. From
Table 7.4, the proposed approach has an accuracy of 99.9% due to misclassiÔ¨Åcation of 6
out of 6617 observations.
The major issues with training a deep neural network model using a gradient-descent
algorithm are: (1) overÔ¨Åtting, and (2) the solution getting stuck in a local optima. Neural
networks trained for a large number of epochs will overÔ¨Åt to the training data, thereby
producing small errors for training data and large errors for test data, as shown by the
red dotted line in Figure 7.4. To visualize the eÔ¨Äectiveness of EAs in initializing the CNN
and the impact of training using BPAs before Ô¨Åtness evaluation, we plot the accuracy of
CNN classiÔ¨Åer when it is initialized and after training with a BPA for p epochs. In this 2D
plot, each CNN classiÔ¨Åer is represented by a circle with its (x, y) coordinates representing
the classiÔ¨Åcation error for training and test data, respectively. We also color these circles
to indicate the EA iteration in which it is explored.
Table 7.9 Accuracy (in %) of CNN classiÔ¨Åers using BPAs, EAs and the hybrid approach for UCF50
Training
Set-1
Set-2
Set-3
Set-4
Set-5
Average
CNN classiÔ¨Åer with only EA
(i.e., initialization using EA)
18.06
20.15
25.43
13.15
27.28
20.81
CNN classiÔ¨Åer without EA
(i.e., training using BPA)
86.02
87.50
18.26
80.30
23.39
59.19
CNN classiÔ¨Åer with EA
(with EA and BPA)
94.19
91.48
96.41
93.12
92.58
93.55
Table 7.10 Accuracy (in %) of the proposed approach using NN and ELM classiÔ¨Åers for UCF50
ClassiÔ¨Åcation
Set-1
Set-2
Set-3
Set-4
Set-5
Average
Proposed approach using NN classiÔ¨Åer
94.19
91.48
96.41
93.12
92.58
93.55
Proposed approach using ELM classiÔ¨Åer
99.96
98.86
99.91
99.34
98.95
99.40

Human Behavioral Analysis Using Evolutionary Algorithms and Deep Learning
177
Errors before
training
Classification error
Train data
Train error
Solutions
initialized by GA
Solutions
initialized by GA
and trained using
BPA
Train error
Test
error
Test
error
Variation in classification error against epochs
for a CNN trained using BPA 
Test data
0
p
# of epochs 
Ebe1
Ebr1
(Ebr1, Ebe1)
(Ear1, Eae1)
Eae1
Ear1
Errors after
training
Figure 7.4 Analysis of classiÔ¨Åcation errors of solutions generated by the proposed hybrid training
approach.
In Figure 7.5 part i, we plot the classiÔ¨Åcation error of solutions generated by EAs for
Set-1 of the UCF50 data set before they are trained using BPAs. Figure 7.5 part ii depicts
the solutions in Figure 7.5 part i after they are trained with gradient-descent algorithms
for 50 (p) epochs. Each circle in the plots represents a CNN classiÔ¨Åer generated by an EA.
As shown in the color scale of these plots, blue is used for solutions generated in the Ô¨Årst
iteration and yellow for solutions generated in the Ô¨Ånal iteration. The same methodology
is used to visualize the solutions of Set-2 to Set-5 given in Fig 7.6. Some of the observa-
tions from the subÔ¨Ågures of Fig 7.5 and Fig 7.6, are: (1) the proposed hybrid training
approach improves the accuracy of the action recognition framework signiÔ¨Åcantly, (2)
the location of solutions on the 45‚àòline suggests the similarity of patterns in both train-
ing and test data (which could be the result of using k-fold CV), and (3) the concentration
of yellow circles at the bottom-left corner in the subÔ¨Ågures with BPA-trained solutions
indicates the eÔ¨Äectiveness of training. Similarly, the candidate solutions explored by the
proposed approach for the KTH data set are shown in Figure 7.7. From the plots, it can
be observed that the solutions have fewer training errors when compared to test errors,
which could be due to the Ô¨Åxed training set and smaller number of training observations,
when compared to n-fold CV of the UCF50 data set. This could be the reason behind
the low accuracy of the proposed approach for the KTH data set when compared to the
UCF50 data set with 99.9%. The pictorial representation of chromosomes generated by
the proposed approach for the KTH and UCF50 data sets is shown in Figure 7.8. From
this visualization, it can be observed that the candidate solutions generated by the pro-
posed hybrid training approach are near identical, which could be the reason behind the
low deviation in performance across solutions in the population generated for these data
sets, as given in Table 7.2 and Table 7.7. The next section discusses the computational
aspects of this work.
7.3.4
Experimental Setup and Parameter Optimization
The proposed approach is implemented in MATLAB 2016a by using the in-built global
optimization toolbox for deÔ¨Åning the evolutionary strategy and extending the CNN
implementation of [42] for rectangular masks. The double vector representation of chro-
mosomes is used with range constraints, tournament selection strategy, intermediate

178
Hybrid Intelligence for Image Analysis and Understanding
(i) Solutions after weight initialization by EA chromosome
(ii) Candidate in (i) after training with BPA
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
Classification error (in %) on training data
50
60
70
80
90
100
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
Classification error (in %) on training data
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90
100
Figure 7.5 Visualization of EA population generated by the proposed approach for Set-1 of UCF50:
(i) after initialization by EA and (ii) after training the solutions with BPA for p epochs.
crossover operation, and a mutation probability of 0.8 for the experimental study in this
work. We empirically determine the seed value used for initializing the EA population
and the optimum number of epochs (p) the CNN classiÔ¨Åer should be trained for con-
vergence, without overÔ¨Åtting the training data (as shown in Figure 7.4). The number of
epochs (p) should be optimized to avoid undertraining or overÔ¨Åtting of CNN classiÔ¨Åer,

0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90 100
Classification error (in %) on training data
(i) Solutions after initialization by EA
(ii) Solutions in (i) after training with BPA
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90 100
Classification error (in %) on training data
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90 100
Classification error (in %) on training data
(iii) Solutions after initialization by EA
(iv) Solutions in (iii) after training with BPA
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90 100
Classification error (in %) on training data
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
Figure 7.6 Visualization of EA population generated by the proposed approach for Set-2 to Set-5 of UCF50. The sub-Ô¨Ågures (i), (ii) correspond to Set-2; (iii), (iv)
are for Set-3; (v), (vi) correspond to Set-4 and (vii), (viii) are for Set-5.

0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90 100
(v) Solutions after initialization by EA
Classification error (in %) on training data
(vi) Solutions in (v) after training with BPA
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90 100
Classification error (in %) on training data
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90 100
Classification error (in %) on training data
(vii) Solutions after initialization by EA
(viii) Solutions in (vii) after training with BPA
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
50
60
70
80
90 100
Classification error (in %) on training data
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
Figure 7.6 (Continued)

Human Behavioral Analysis Using Evolutionary Algorithms and Deep Learning
181
(i) Plot of candidate solutions after initialization by EA
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
Classification error (in %) on training data
50
60
70
80
90
100
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
(ii) Candidate in (i) after training with BPA
0
10
20
30
40
50
60
70
80
90
100
0
10
20
30
40
Classification error (in %) on training data
50
60
70
80
90
100
Final population
Initial population
FIRST GENERATION
LAST GENERATION
EA ITERATIONS
Classification error (in %) on test data
Figure 7.7 Visualization of EA population generated for KTH: (i) visualization after initialization by EA
and (ii) visualization after training with BPA.

182
Hybrid Intelligence for Image Analysis and Understanding
(a) Representation of EA chromosome X
(b) KTH
(c) Set-1
(d) Set-2
(e) Set-3
(f) Set-4
(g) Set-5
Figure 7.8 Visualization of chromosomes (candidate solutions) generated by the proposed hybrid
training approach for UCF50: (a) The pictorial representation of convolution masks and seed values
corresponding to a chromosome X of size 64; (b) chromosomes generated for the KTH data set; and
(c‚Äìg) correspond to chromosomes generated for UCF50 Set-1 to Set-5, respectively.
which impacts the Ô¨Åtness value of the chromosome and in turn the convergence of EAs.
The next section discusses the computational aspects of this work.
7.3.5
Computational Complexity
In this chapter, we use EAs and CNN classiÔ¨Åer to design a human action recognition
system. The Ô¨Åtness evaluation of candidate solutions in EA can be parallelized, and
eÔ¨Écient GPU-based implementation of CNN classiÔ¨Åers like cuDNN can be used to
reduce the training time. As the CNN is trained using a gradient-descent algorithm
for p epochs (i.e., 50 epochs for UCF50), the CNN classiÔ¨Åer gets trained in a short

Human Behavioral Analysis Using Evolutionary Algorithms and Deep Learning
183
time. Several other multi-GPU-based implementations of CNN like NVIDIA‚Äôs DIGITS,
Theano, Torch, and Berkeley‚Äôs CaÔ¨Äe are also available. The CNN classiÔ¨Åers are today
used for many real-time applications like image recognition and speech processing in
mobile devices. These systems are trained on high-end GPU platforms and deployed on
low-end mobile hardware, as training is more computationally expensive than testing.
Hence, this approach can be used to Ô¨Ånd the optimum weights of CNNs, which can be
deployed for real-time use.
7.4
Conclusions and Future Work
In this work, we proposed a deep neural network model optimized using evolution-
ary algorithms for recognizing human actions in videos. As the performance of a DNN
model (trained using a back-propagation algorithm) depends on its initial weights, we
aim to optimize the dnn architecture by using an evolutionary algorithm to search for
the optimum initial weights. As a result, the DNN is initialized to avoid getting stuck in
a local minimum and overÔ¨Åtting to training data. The fusion across models generated
using evolutionary algorithms overcomes the constraints of individual models, by com-
bining complementary information across classiÔ¨Åers. The experimental study on KTH
and UCF50 data sets suggests the eÔ¨Äectiveness of the proposed approach (accuracy of
99.9% for UCF50). The future work will extend this approach to other spatiotemporal
features like 3DHOG features [57].
References
1 Aggarwal, J. and Ryoo, M. (2011) Human activity analysis: a review. ACM
Computing Surveys, 43 (3), 1‚Äì43.
2 Holte, M.B., Tran, C., Trivedi, M.M., and Moeslund, T.B. (2011) Human action
recognition using multiple views: a comparative perspective on recent develop-
ments, in Joint ACM Workshop on Human Gesture and Behavior Understanding,
ACM, New York, J-HGBU‚Äô11, pp. 47‚Äì52.
3 Weinland, D., Ronfard, R., and Boyer, E. (2011) A survey of vision-based methods
for action representation, segmentation and recognition. CVIU, 115 (2), 224‚Äì241.
4 Chaquet, J.M., Carmona, E.J., and Fern√°ndez-Caballero, A. (2013) A survey of video
datasets for human action and activity recognition. CVIU, 117 (6), 633‚Äì659.
5 Foggia, P., Percannella, G., Saggese, A., and Vento, M. (2013) Recognizing human
actions by a bag of visual words, in SMC, pp. 2910‚Äì2915.
6 Laptev, I., Marszalek, M., Schmid, C., and Rozenfeld, B. (2008) Learning realistic
human actions from movies, in CVPR, pp. 1‚Äì8.
7 Dalal, N., Triggs, B., and Schmid, C. (2006) Human detection using oriented
histograms of Ô¨Çow and appearance, in ECCV - vol. pt. 2, Springer-Verlag, Berlin,
Heidelberg, ECCV 06, pp. 428‚Äì441.
8 Sadanand, S. and Corso, J.J. (2012) Action bank: A high-level representation of
activity in video, in CVPR, pp. 1234‚Äì1241.
9 Wang, H., Klaser, A., Schmid, C., and Liu, C.L. (2011) Action recognition by dense
trajectories, in CVPR, pp. 3169‚Äì3176.

184
Hybrid Intelligence for Image Analysis and Understanding
10 Bengio, Y., Courville, A., and Vincent, P. (2013) Representation learning: a review
and new perspectives. PAMI, 35 (8), 1798‚Äì1828.
11 LeCun, Y., Kavukcuoglu, K., and Farabet, C. (2010) Convolutional networks and
applications in vision, in ISCAS, pp. 253‚Äì256.
12 Lecun, Y., Bottou, L., Bengio, Y., and HaÔ¨Äner, P. (1998) Gradient-based learning
applied to document recognition. Proc. of the IEEE, 86 (11), 2278‚Äì2324.
13 Bengio, Y., Lamblin, P., Popovici, D., Larochelle, H., Montr√©al, U.D., and Qu√©bec, M.
(2007) Greedy layer-wise training of deep networks, in NIPS, MIT Press.
14 Hinton, G.E., Osindero, S., and Teh, Y.W. (2006) A fast learning algorithm for deep
belief nets. Neural Computation, 18 (7), 1527‚Äì1554.
15 Lecun, Y. and Cortes, C. (2009) The MNIST database of handwritten digits.
Available from http://yann.lecun.com/exdb/mnist/.
16 Krizhevsky, A., Sutskever, I., and Hinton, G.E. (2012) Imagenet classiÔ¨Åcation with
deep convolutional neural networks, in NIPS, pp. 1097‚Äì1105.
17 Girshick, R.B., Donahue, J., Darrell, T., and Malik, J. (2013) Rich feature hierarchies
for accurate object detection and semantic segmentation. CoRR, abs/1311.2524.
18 Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M.S., Berg, A.C., and Fei-Fei, L. (2014) Ima-
genet large scale visual recognition challenge. CoRR, abs/1409.0575.
19 Lawrence, S., Giles, C., Tsoi, A.C., and Back, A. (1997) Face recognition:
a convolutional neural-network approach. IEEE Transactions on Neural Networks, 8
(1), 98‚Äì113.
20 Matsugu, M., Mori, K., Mitari, Y., and Kaneda, Y. (2003) Subject independent facial
expression recognition with robust face detection using a convolutional neural
network. Neural Networks, 16 (5-6), 555‚Äì559.
21 Ji, S., Xu, W., Yang, M., and Yu, K. (2013) 3d convolutional neural networks for
human action recognition. PAMI, 35 (1), 221‚Äì231.
22 Tran, D., Bourdev, L.D., Fergus, R., Torresani, L., and Paluri, M. (2014) C3D: generic
features for video analysis. CoRR, abs/1412.0767.
23 Baccouche, M., Mamalet, F., Wolf, C., Garcia, C., and Baskurt, A. (2011) Sequential
deep learning for human action recognition, in ICHBU, Springer-Verlag, pp. 29‚Äì39.
24 Wang, K., Wang, X., Lin, L., Wang, M., and Zuo, W. (2014) 3D human activity
recognition with reconÔ¨Ågurable convolutional neural networks, in ACMMM, ACM,
New York, NY, USA, MM‚Äô14, pp. 97‚Äì106.
25 Lin, L., Wang, K., Zuo, W., Wang, M., Luo, J., and Zhang, L. (2015) A deep struc-
tured model with radius-margin bound for 3D human activity recognition. IJCV, pp.
1‚Äì18.
26 Veeriah, V., Zhuang, N., and Qi, G. (2015) DiÔ¨Äerential recurrent neural networks for
action recognition. CoRR, abs/1504.06678.
27 Simonyan, K. and Zisserman, A. (2014) Two-stream convolutional networks for
action recognition in videos. CoRR, abs/1406.2199.
28 Bengio, Y. and Delalleau, O. (2011) On the expressive power of deep architectures,
in ALT, Springer-Verlag, Berlin, pp. 18‚Äì36.
29 Bengio, Y. (2009) Learning deep architectures for AI. Foundation and Trends in
Machine Learning, 2 (1), 1‚Äì127.
30 Bengio, Y., Simard, P., and Frasconi, P. (1994) Learning long-term dependencies with
gradient descent is diÔ¨Écult. IEEE Transactions on Neural Networks, 5 (2), 157‚Äì166.

Human Behavioral Analysis Using Evolutionary Algorithms and Deep Learning
185
31 Prechelt, L. (1997) Early stopping - but when? In Neural networks: tricks of the
Trade, vol. 1524 of LNCS, ch. 2, Springer-Verlag, pp. 55‚Äì69.
32 Erhan, D., Bengio, Y., Courville, A., Manzagol, P.A., Vincent, P., and Bengio, S.
(2010) Why does unsupervised pre-training help deep learning? JMLR, 11, 625‚Äì660.
33 David, O.E. and Greental, I. (2014) Genetic algorithms for evolving deep neural net-
works, in GECCO, ACM, New York, NY, USA, pp. 1451‚Äì1452.
34 Fedorovici, L.O., Precup, R.E., Dragan, F., and Purcaru, C. (2013) Evolutionary
optimization-based training of convolutional neural networks for ocr applications, in
ICSTCC, pp. 207‚Äì212.
35 Rashedi, E., Nezamabadi-pour, H., and Saryazdi, S. (2009) GSA: a gravitational
search algorithm. Information Sciences, 179 (13), 2232‚Äì2248.
36 Kennedy, J. and Eberhart, R.C. (1995) Particle swarm optimization, in Int. Conf. on
Neural Networks, vol 4, pp. 1942‚Äì1948.
37 Bascom, J. (1871) Darwin‚Äôs theory of the origin of species. American Theological
Review, 3, 349‚Äì379.
38 Wikipedia. (2016), Convolutional neural network. Available from https://en
.wikipedia.org/wiki/Convolutional_neural_network.
39 Schuldt, C., Laptev, I., and Caputo, B. (2004) Recognizing human actions: a local
SVM approach, in ICPR, vol 3, pp. 32‚Äì36.
40 Reddy, K.K. and Shah, M. (2012) Recognizing 50 human action categories of web
videos. Machine Vision and Applications, 24 (5), 971‚Äì981.
41 Corso, J.J. (2004), Action bankTM: a high-level representation of activity in video.
Available from http://web.eecs.umich.edu/jjcorso/r/actionbank/.
42 Palm, R.B. (2012) Prediction as a candidate for learning deep hierarchical models of
data, Master‚Äôs thesis, Technical University of Denmark, Asmussens Alle, Denmark.
43 Huang, G.B., Zhou, H., Ding, X., and Zhang, R. (2012) Extreme learning machine
for regression and multiclass classiÔ¨Åcation. SMC, Part B: Cybernetics, 42 (2),
513‚Äì529.
44 Ijjina, E.P. and Mohan, C.K. (2016) Hybrid deep neural network model for human
action recognition. Applied Soft Computing, 46, 936‚Äì952.
45 Kliper-Gross, O., Gurovich, Y., Hassner, T., and Wolf, L. (2012) Motion inter-
change patterns for action recognition in unconstrained videos, in ECCV - vol. pt. 6,
Springer-Verlag, Berlin, Heidelberg, ECCV‚Äô12, pp. 256‚Äì269.
46 Shi, F., Petriu, E., and Laganiere, R. (2013) Sampling strategies for real-time action
recognition, in CVPR, pp. 2595‚Äì2602.
47 Wang, L., Qiao, Y., and Tang, X. (2013) Motionlets: Mid-level 3D parts for human
motion recognition, in CVPR, pp. 2674‚Äì2681.
48 Zhou, Q., Wang, G., Jia, K., and Zhao, Q. (2013) Learning to share latent tasks for
action recognition, in ICCV, pp. 2264‚Äì2271.
49 Ijjina, E.P. and Mohan, C. (2014) Human action recognition based on recognition
of linear patterns in action bank features using convolutional neural networks, in
ICMLA, pp. 178‚Äì182.
50 Ballas, N., Yang, Y., Lan, Z.Z., Delezoide, B., Preteux, F., and Hauptmann, A. (2013)
Space-time robust representation for action recognition, in ICCV.
51 Niebles, J.C., Wang, H., and Fei-Fei, L. (2008) Unsupervised learning of human
action categories using spatial-temporal words. IJCV, 79 (3), 299‚Äì318.

186
Hybrid Intelligence for Image Analysis and Understanding
52 Ryoo, M.S. and Aggarwal, J.K. (2009) Spatio-temporal relationship match: video
structure comparison for recognition of complex human activities, in ICCV,
pp. 1593‚Äì1600.
53 Laptev, I., Marszalek, M., Schmid, C., and Rozenfeld, B. (2008) Learning realistic
human actions from movies, in CVPR, pp. 1‚Äì8.
54 Le, Q., Zou, W., Yeung, S., and Ng, A. (2011) Learning hierarchical invariant
spatio-temporal features for action recognition with independent subspace analysis,
in CVPR, pp. 3361‚Äì3368.
55 IosiÔ¨Ådis, A., Tefas, A., and Pitas, I. (2013) Minimum class variance extreme learning
machine for human action recognition. CSVT, 23 (11), 1968‚Äì1979.
56 Amer, M.R. and Todorovic, S. (2016) Sum product networks for activity recognition.
PAMI, 38 (4), 800‚Äì813.
57 Klaser, A., Marsza≈Çek, M., and Schmid, C. (2008) A spatio-temporal descriptor
based on 3D-gradients, in BMVC, British Machine Vision Association, pp. 275‚Äì281.

187
8
Feature-Based Robust Description and Monocular Detection:
An Application to Vehicle tracking
Ramazan Y√≠ld√≠z and Tankut Acarman
Computer Engineering Department, Galatasaray University, Istanbul, Turkey
8.1
Introduction
This study is focused in particular on the state-of-art feature extraction methods
used in computer vision, their comparison in the context of object description and
implementation into real-time recognition and tracking of land vehicles. Throughout
this chapter, scale-invariant feature transform (SIFT) [1, 2], speeded-up robust features
(SURF) extraction [3], biological visual cortex inspired features (the HMAX model)
[4], and Haar-like features [5] are elaborated. On one hand, SIFT and SURF extraction
are used for local image analysis; both methods extract distinctive local image features.
On the other hand, the HMAX model and Haar-like features are global image features;
each feature is constituted by thickness and orientation of contours, junctions, intensity
changes, geometry of shapes, and objects. The HMAX model uses multiscale Gabor
Ô¨Ålters applied in diÔ¨Äerent orientations. In [5], Haar-like features are presented, and they
are used to describe object models. In this study, motivated by [5], Haar-like features
are implemented with Adaboost classiÔ¨Åer to detect land vehicles in a road traÔ¨Éc video.
In this chapter, we present a new method by fusing the information provided by global
and local features. The Ô¨Årst approach is based on local features for image object descrip-
tion and tracking. A SIFT-based model is presented to assure robust description with
an improved object tracking performance versus the state-of-the-art SIFT method. The
model is tested with the Multi-View Car Dataset of the Computer Vision Laboratory
(CVLAB), √âcole Polytechnique F√©d√©rale de Lausanne, France [6], and it is implemented
in the context of object tracking in a video.
The second approach uses global features for image object description, detection,
and real-time tracking. A video object detection and tracking algorithm is developed
and elaborated. Haar-like features are used for detection purposes, and regions of
interest (ROIs) are deÔ¨Åned. Validation algorithms are developed based on low-level
image analysis, enabling the presented detection scheme to be robust and safe. Finally,
a novel algorithm leveraging temporal information of detection history is presented
to perform tracking. To illustrate the eÔ¨Äectiveness of this algorithm, a set of videos
recorded under diÔ¨Äerent traÔ¨Éc conditions from a monocular camera mounted on a
vehicle traveling on the Transit European Motorway (TEM) and Europe 5 (E5) freeway,
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

188
Hybrid Intelligence for Image Analysis and Understanding
and the well-known vehicle detection data set LISA-Q Front FOV [7], are used for
evaluation purposes. Results are discussed, and performance metrics are calculated
and compared with respect to the state-of-the-art methods.
8.2
Extraction of Local Features by SIFT and SURF
SIFT extracts local particularities [2]. Local particularities are robust to changes in
illumination, noise, and viewpoint up to 30‚àò, and hence they are distinctive and can be
identiÔ¨Åed iteratively in diÔ¨Äerent scenes. But SIFT is sensitive and fragile when object
texture and geometry deformations suddenly occur, and in the domain of vehicle
tracking in abruptly changing traÔ¨Éc scenes, these deformations frequently occur. Haar
responses in SURF provide image information that is also invariant to illumination
changes. SURF is invariant against noise and image rotation changes, but it is sensitive
and inaccurate with viewpoint changes.
Although SIFT is suitable to image scene description tasks, feature extraction
on a CPU cannot fulÔ¨Åll the requirements of real-time applications; for instance,
feature extraction of an image with a size of 1000 √ó 700 pixels requires processing
of approximately 4000 key points. This task costs 5 seconds of processing time on a
Intel Quad-Cores i5 processor with a 2.70 GHz single core frequency, and 2 MB smart
cache, and 2 GB memory. In terms of responsiveness, SURF provides three times faster
description of images in comparison to SIFT [8]. Furthermore, implementation of SIFT
on GPUs allows performing computer vision tasks on a real-time basis [9, 10].
Feature extraction in the SIFT model generates vectors or so-called descriptors.
Resemblance between two vectors is calculated by using Euclidean distance [4]. Calcu-
lation of the dot products of two vectors is computationally inexpensive; for instance,
computation of arccosines of the two unit vectors‚Äô dot products is simply calculated by
the angles‚Äô ratio between them, which determines their resemblance level. In [2], Lowe‚Äôs
SIFT model computes the ratio of two vectors‚Äô angle by computing the angle ratio; and,
when it is a small value, the resemblance is concluded to be very close to the resemblance
calculated by Euclidean distance. In Figure 8.1, Lowe‚Äôs matching method is illustrated;
descriptors are normalized to a unit vector. Query image SIFT descriptors are denoted
by d1, ‚Ä¶ , dn, and image SIFT descriptors to be compared are denoted by d1, ‚Ä¶ , dm.
Dot products of di for i = 1, ‚Ä¶ , m and the set of d1, ‚Ä¶ , dm are calculated. After sorting
and ranking the most elevated two resemblances, for instance, r1 and r2 are chosen
for illustration purposes in Figure 8.1; if their ratio (i.e., r1
r2 ) is less than 0.6, then di for
i = 1, ‚Ä¶ , n cannot assure a conÔ¨Ådent correspondence. In other words, it is not distinc-
tively matched because it has another candidate matching descriptor. Otherwise, di is
distinctively matched. The set of matching vectors yields the resemblance of compared
images, and matching with existing objects over sequential frames is repeated.
Generic points extraction is presented in [11]. These generic key points represent the
corresponding originals that are called robust SIFT descriptors. Generic points and
their corresponding points are matched to obtain the preprocessed frame descriptors.
Matching the set of generic points aided robust description (GPRD) descriptors
is similar to Lowe‚Äôs matching, but the resemblance value of 0.75 is chosen for the
distance ratio, instead of the resemblance value of 0.6 that is the default ratio chosen
to match irrelevant random images over classic SIFT descriptors. These factors are

Feature-Based Robust Description and Monocular Detection: An Application to Vehicle tracking
189
d1
1
128
...
...
di
1
128
...
...
dn
1
128
Query image
SIFT Descriptors
Compared image
SIFT Descriptors
r1:ressemblance
of di to dp
r2:ressemblance
of di to dh
...
d1
1
128
...
...
dp
1
128
...
...
...
dh
1
128
...
dm
1
128
...
Figure 8.1 Illustration of Lowe‚Äôs matching method.
Figure 8.2 Generic points aided robust description (GPRD) matching scheme.
experimentally validated and have improved the false match elimination algorithm
results (see, e.g., [11]). In the case of GPRD matching, the query image is pretreated
subject to a perspective planar transformation, and transformed images are compared
with the query image. This matching scheme is illustrated in Figure 8.2.
Our method is inspired by the extraction stage describing objects for object category
detection in [8]. This is an intermediate step for the HMAX model C1 global image;

190
Hybrid Intelligence for Image Analysis and Understanding
Figure 8.3 Matching with Lowe‚Äôs SIFT description.
oriented Ô¨Ålters are used to extract the responses of contours in four directions, and
two diÔ¨Äerent Ô¨Ålter sizes of 11√ó11 and 13√ó13 are used. At the end of this step, the
signiÔ¨Åcant contour responses are extracted. Then the dilatation operation is applied
to these extracted contour images. Dilated contour images become roughly described
contour images. Hence, a general representation of the global image particularities
such as contours and junctions is generated. By using this representative model, we are
able to compare other objects‚Äô global features in order to detect the class of the object.
By following the same approach, representation of a SIFT descriptor subject to some
logical deformations is achieved.
This method is applied on a multiview data set [6]. Some preliminary comparisons are
made by using the matching method presented in [11]:
‚Ä¢ The object of the car rear is compared with its identical but 40‚àò-rotated version.
Since SIFT points are invariant and insensitive subject to a limited object rotation
invariance of 30‚Äì45‚àò, a matching result after a given rotation of 45‚àòis unreliable for
SIFT descriptors. In this comparison, the matching result of the original SIFT imple-
mentation when the distance ratio is equal to 0.6 yields 11 correct matches, as plotted
in Figure 8.3.
‚Ä¢ When the same object is matched by using the GPRD approach, the number of correct
matches is signiÔ¨Åcantly higher when the distance ratio is chosen to be equal to 0.6 and
it is increased when the ratio is chosen to be 0.7, 0.75, or 0.8. The GPRD matching
results are plotted in Figure 8.4 for diÔ¨Äerent ratio values. For the ratio of 0.8, 30 correct
matchings and three incorrect matchings are generated. When the ratio is chosen to
be lower than or equal to 0.75, matchings are always correct.
‚Ä¢ When the identical object to be matched is subject to strong shadows, rotation
more than 45‚àò, or cluttered background scenes, SIFT matching is not accurate
and these results are not reliable. In these perturbed scenes, SIFT GPRD matching
outperforms the classical SIFT. In Figure 8.5, the same object is matched with its
perturbed identical object, and visual matching points are plotted for both GPRD
and Lowe‚Äôs SIFT description method.
GPRD assures more robust tracking in comparison with other feature extraction
methods.
8.3
Global Features: Real-Time Detection and Vehicle
Tracking
A boosted cascade of simple Haar-like rectangular features is used to detect vehicles.
This method was originally introduced by Viola and Jones for face detection in [5]. In

Feature-Based Robust Description and Monocular Detection: An Application to Vehicle tracking
191
Figure 8.4 Matching with GPRD descriptors.
general terms, Haar-like rectangular features are well suited for object shape detection.
These features are sensitive to image global features like edges, bars, vertical and
horizontal details, and symmetric structures. The original algorithm used by Viola
and Jones allows for rapid object detection that can be suitable for implementation
on a real-time basis. The usage of integral images provides fast and eÔ¨Écient feature
extraction. Extracted resulting values are eÔ¨Äective weak learners, which are classiÔ¨Åed
by Adaboost. Adaboost performs classiÔ¨Åcation based on a weighted majority vote of
weak learners. It is a discriminative learning algorithm. A cascade of classiÔ¨Åer stages is
constructed with Adaboost learning. Scores computed from feature extraction make
the decision about rejection at each stage. Candidate objects are eliminated at each
stage within the cascaded approach, and remaining candidates at the Ô¨Ånal stage are
taken as positive detections.
Integral images, introduced by [5], are deÔ¨Åned like look-up tables in the form of a
matrix having the same size as the original image. Each element of the matrix contains

192
Hybrid Intelligence for Image Analysis and Understanding
[A]
[B]
[C]
[D]
[E]
[F]
[A]
[B]
[C]
[D]
[E]
[F]
Figure 8.5 Visual comparison of GPRD and Lowe‚Äôs SIFT description: The frame on the left side (blue
colored) is the result of GPRD, and the frame on the right side (red colored) is the result of the SIFT
description.
the sum of all pixels located on the upper-left region of the original image. This provides
eÔ¨Äective processing by using only four look-ups. Haar-like features are extracted by
using box Ô¨Ålters, which tend to behave like Haar wavelets of degree one. A Haar-like
feature is extracted by summing up the pixel intensities over two adjacent rectangles
and then subtracting the two sums. Basically, it is the diÔ¨Äerence between pixel intensity
sums over two rectangles; total pixel intensity change in the adjacent region presents a
global feature that is used by a weak learner like Adaboost. This diÔ¨Äerence is then used

Feature-Based Robust Description and Monocular Detection: An Application to Vehicle tracking
193
to categorize subsections of an image. For example, let‚Äôs consider an image database of
human faces; a common observation is that among all faces, the region of the eyes is
darker in comparison to the region of the cheeks. Therefore, a common Haar feature
for face detection is a set of two adjacent rectangles that lie above the eye and the cheek
region. The position of these rectangles is deÔ¨Åned in relation to a detection window that
acts like a bounding box to the target object (that is simply the face in this example).
In the detection phase of [5], a window of the target size is moved over the input image,
and for each subsection of the image the Haar-like feature is calculated. This diÔ¨Äerence
is then compared to a learned threshold that separates nonobjects from objects.
Because such a Haar-like feature is only a weak learner, its detection quality is slightly
better than random guessing, and a large number of Haar-like features is required to
describe an object with a suÔ¨Écient level of accuracy. In [5] again, the Haar-like features
are organized, such as in a cascaded classiÔ¨Åer, to form a strong learner or classiÔ¨Åer.
But still, detection using Haar-like features with Adaboost is not robust with respect
to challenging scene conditions; this detection scheme may produce a signiÔ¨Åcant
amount of false positives besides true positives. For laboratory testing purposes, we
used Haar-like features in the context of face recognition. Subject to object pose
changes, false positives occur frequently. Further processing is required in order to
achieve a reliable recognition prototype. For instance, through sequences plotted in
Figure 8.6, the big circle (colored blue) is the ROI, which is detected by using Haar-like
features. Toward vehicle recognition and tracking, we develop further validation
processes to achieve a reliable recognition system.
Figure 8.6 Assessment of Haar-like features in the context of face recognition.

194
Hybrid Intelligence for Image Analysis and Understanding
8.4
Vehicle Detection and Validation
Low-level image analysis is applied to enhance detection. Namely, texture-based
X-symmetry analysis and edge-based prominent horizontal line search are integrated
in the overall detection and tracking solution given in Figure 8.7. In addition, detection
history is used for validation and tracking purposes while creating a short-term
memory dedicated to enhancement in tracking the sudden disappearance of vehicles
due to occlusions by other surrounding vehicles and their possible reappearance. In
the block diagram plotted in Figure 8.7, a video frame is subjected to a smoothing
operation and then subsampled. A subsampled color image is then subjected to Canny
edge detection and Haar detection with Adaboost. Haar detection provides candidate
object locations that are labeled as object bounding boxes (OBBs).
8.4.1
X-Analysis
Colored image data inside the bounding box of a detected candidate object (in our case,
it is a land vehicle) is analyzed, and based on the image data, symmetry is searched
around the vertical axis. To accomplish this task, two adjacent sliding windows are
used. A sliding window is scaled laterally by a minimum of one-Ô¨Åfth to the maximum of
half-width of an OBB window with the same height. While the window is sliding from
left to right at each iteration, the diÔ¨Äerence between the sum of the image data corre-
sponding to the left and right window is calculated. The minimum image data diÔ¨Äerence
2k
2w
Video Frame,
Color Image
Object
Bounding
Box (OBB)
Haar
Detection
with
Adaboost
Canny
Edge
Detection
(1,100)
X-Symmetry
Analyze
Temporal
Information
Analyze
Horizontal Line
Frequency Analyze
Recognition
AND
OR
Edge Image,
Black and White
Smooth
Color Image
Smoothing
Subsampled
Smooth
Color Image
Subsample
2k
2w
k
w
k
w
Figure 8.7 Recognition and tracking scheme.

Feature-Based Robust Description and Monocular Detection: An Application to Vehicle tracking
195
over all iterations is considered as the candidate symmetry axis. If the diÔ¨Äerence value
corresponding to the candidate symmetry axis is not smaller than a threshold value, then
symmetry detection is rejected. The threshold value is chosen to be 15. Color image data
is used to detect the symmetry axis. We used three colored channels and subsampled
smooth images. Since the three channels and a single channel yield to the same decision
result, a single channel is considered at each iteration to reduce computational costs.
Alternatively, to detect the symmetry axis, we analyzed edge images, which are
single-channel images containing only white- or black-colored contours of the original
image. Edge images are extracted with the Canny edge detection algorithm. Before
applying the Canny algorithm, to reduce noise and increase performance, colored
images are blurred and subsampled as presented in Figure 8.7. Again, the sliding window
approach is applied, and at each iteration, the ratio between the count of white pixels
and the whole pixel‚Äôs count corresponding to the sliding window area is compared.
Similarly, the left-side window is scanned and the same ratio is extracted. The two
ratios are compared at each iteration, and the ratio pair providing maximal similitude
is considered to detect symmetry axis. If the two ratios are not suÔ¨Éciently close to each
other (i.e., the ratio is below the threshold), then symmetry detection is rejected.
8.4.2
Horizontal Prominent Line Frequency Analysis
While analyzing low-level, global vehicle image features, one can exploit that the shape
of a vehicle produces more horizontal edges than on-road background textures, and
these horizontal edges are signiÔ¨Åcantly longer and condensed in parallel inside the
vehicle bounding box.
QuantiÔ¨Åcation of prominent horizontal edges is a good indicator when validating a
candidate vehicle given within a bounding box. Toward detection of vehicle horizontal
lines, edge images are used at the Ô¨Årst stage. Images are blurred and then subsampled
by two; irrelevant noise data is eliminated. Afterward, the Canny edge detection
algorithm is used to extract edge images consisting of only white- and black-colored
data, containing only relevant edges marked with white color over a black background;
see, for instance, the edge images plotted in Figure 8.8.
The edge image (candidate vehicle bounding box) is scanned by beginning from the
bottom to the top of the frame, and horizontal edges are sought. As illustrated in the left
side of Figure 8.8, the scanning procedure is started from point A and iteratively goes
along the bounding box symmetry axis denoted by S until the top point, B. Scanning
resolution is 1 pixel, and at each iteration [i.e., for a given central point (x, y) located on
w
B
A
(x, y)
Edge Image
w
S
Edge Image
B
A
C
Figure 8.8 Scan for lines‚Äô vertical response (right), and discontinuity on lines (left).

196
Hybrid Intelligence for Image Analysis and Understanding
the symmetry axis denoted by S], a continuous line search is executed by scanning in the
horizontal direction from left to right. Left-side horizontal scanning is started from
the point (x, y), terminated at the point (x ‚àíùë§, y); and right-side horizontal scanning is
started from (x, y), terminated at (x + ùë§, y). At each iteration of horizontal scanning, the
pixel value at (x, y) is checked. If the pixel value of (x, y) is white and both (x, y ‚àí1) and
(x, y + 1) are black, then the next iteration is continued with (x + 1, y). Otherwise, the
value of the adjacent pixel (x, y + 1) and (x, y ‚àí1) is checked. If (x, y + 1) is white, then
the next step is continued with (x, y + 1); otherwise, if (x, y ‚àí1) is white, then the next
step is continued with (x, y ‚àí1); and, if x cannot be incremented more than one iteration,
then scanning is terminated. This seeking approach provides tolerance for detection of
horizontal edges.
Another approach to seek horizontal edges is implemented by setting a gap of two
pixels. According to the walking algorithm illustrated on the left side of Figure 8.8, if
(x, y) is black, then the adjacent pixel located at (x, y ‚àí1) and (x, y + 1) is checked. If
those upper and lower neighbor pixels are also black, iteration continues at (x + 1, y). If a
white pixel is not detected, the gap at the edges is increased to two pixels. Gap over edges
may be discontinued, as denoted by B on the right side of Figure 8.8. The points denoted
by A and C are scan end points. At the end points, the obliquity of the line is checked
by comparing the angle between the line and horizontal axis, and if this diÔ¨Äerence is
more than 10‚àò, the detected horizontal edge is rejected. The strength of the line can
be considered as a constraint for decision making, and if the detected horizontal line‚Äôs
length is shorter than 10 pixels, then it is rejected.
We consider prominent lines to enhance reliability of validation information. This
constraint increases robustness, but it also decreases the detection rate of distant vehi-
cles subject to lower scaling. If a candidate vehicle has an active state, then we decrease
10 pixels in length with a weighted factor determined with the input from previous
frames in the detection history of this vehicle. This provides adaptation against scale
change when the relative distance between the tracked vehicle and the monocular cam-
era sensor is increased or decreased.
8.4.3
Detection History
Temporal information is useful for enhancing tracking performance. In our implemen-
tation, temporal information is used for both tracking and detection. At each frame
processing, history is updated, only vehicle objects detected within the last 30 frames
are conserved, and others are removed from the detection history log Ô¨Åle.
When a vehicle is detected, it is also searched in the history, and if it has been already
detected within the last 30 frames, then its position is updated and its status is marked
as active. The state of vehicles logged is updated at each frame, and if a vehicle is not
redetected within the last 15 frames, then it is marked as inactive. But it is not discarded
from the history until it is not redetected within the last 30 frames. And, all active
vehicles in the history are weighted for voting. This voting weight is calculated by using
the detection rate over the last 15 frames, which is simply calculated as the detection
counts over 15.
Overall, the detection and tracking algorithm in Figure 8.7 can be summarized as
follows:
1. Step 1: If the candidate vehicle bounding box does not satisfy an X-symmetry axis,
then detection is rejected.

Feature-Based Robust Description and Monocular Detection: An Application to Vehicle tracking
197
Figure 8.9 Screenshots of the recognition and tracking results on the Istanbul TEM highway.
2. Step 2: Vehicle edges may produce horizontal lines. At least four horizontal lines with
a width of 10 pixels should be inside the bounding box.
3. Step 3: Detection history is used as an indicator for tracking and detection purposes.
The candidate vehicle‚Äôs bounding box center must have 28 close matchings over the
last 30 frames in which detection has occurred. The distance between this center and
the center of the detected vehicle captured from history must be less than two times
the radius of the bounding box. This redundancy attenuates false positives.
4. If either Step 2 or Step 3 is not veriÔ¨Åed, then detection is rejected.
Two captured samples illustrate the detection performance as the application summary
of the presented method in Figure 8.9.
8.5
Experimental Study
This section is divided in three parts. In the Ô¨Årst part, local features are evaluated, and
GPRD-based description and tracking results are compared to the SIFT and SURF
results. The second part is oriented toward global features, and the results of local and
global features‚Äìbased hybrid feature tracking solutions are presented in the Ô¨Ånal part.
8.5.1
Local Features Assessment
To evaluate local feature‚Äìbased description methods, a multiview vehicle rear data set
is used [6]. This data set includes images subject to shadows, rotation, diÔ¨Äerent objects
in the background, and cluttered scenes. In Figure 8.10, 30 snapshots are created by
rotating the original vehicle view with a resolution of 3‚àòbetween ‚àí45‚àòand 45‚àò. Local
features about a vehicle‚Äôs rear view are compared during 20 sequences, and at least three
correct matchings are decided to be the true detection. SIFT GPRD outperforms the
SIFT and SURF methods. In Figure 8.11, the responses of false matching are compared,
and SIFT GPRD generates fewer false matchings.
A set of features in the ROI of the vehicle is detected and tracked by implementing
SIFT GPRD to a road traÔ¨Éc video recorded on the TEM. Tracking results are plotted
in Figure 8.12, and inside the ROIs, matching and unmatched points are plotted with
green and red dots, respectively.
8.5.2
Global Features Assessment
To evaluate the performance of recognition and tracking by using global features, the
following metrics are used: the true positive rate (TPR), false detection rate (FDR), true

198
Hybrid Intelligence for Image Analysis and Understanding
0
1
2
3
4
5
6
7
8
9
10
Car Rear
Correct Matchings
11 12 13 14 15 16 17 18 19 20
2
4
6
8
10
Success per Car Rear
12
14
16
18
20
SIFT
SIFT GPRD
SURE
Figure 8.10 The responses of true matchings for SIFT, SURF, and SIFT GPRD‚Äìbased detection.
0
1
2
3
4
5
6
7
8
9
10
Car Rear
False Matchings
11 12 13 14 15 16 17 18 19 20
5
10
15
20
Error per Car Rear
25
30
SIFT
SIFT GPRD
SURE
Figure 8.11 The responses of false matchings.
positive per frame (TPF), and false positive per frame (FPF).
TPR =
tp
N
(8.1)
FDR =
fp
tp + fp
(8.2)
TPF =
tp
N
|F
|F
(8.3)
FDR =
fp
N
|F
|F
(8.4)
TPR is a measure of recall and localization. In contrast, FDR is a measure of precision
and localization. TPF is a measure of robustness; FPF is a measure of robustness,
localization, and scalability. For instance, consider a road scene where N vehicles are
traveling. True positives (tp) refer to the number of vehicles in this scene that are
correctly recognized and tracked by using global features, while false positives (fp) refer

Feature-Based Robust Description and Monocular Detection: An Application to Vehicle tracking
199
Figure 8.12 Evaluation of SIFT GPRD in a road traÔ¨Éc video.
the number of vehicles not in this scene that are incorrectly recognized and tracked.
Similarly, true positives per frame (tp|F) are the average number of vehicles correctly
recognized and tracked versus the number of successive frames denoted by N|F, and
(fp|F) is the average number of vehicles incorrectly recognized and tracked versus the
number of successive frames.
To evaluate the local and global features‚Äìbased hybrid feature tracking solution, video
records captured under diÔ¨Äerent traÔ¨Éc conditions from a monocular camera mounted
on a vehicle traveling on the TEM and the E5 freeway, and the well-known vehicle detec-
tion data set LISA-Q Front FOV, are used. Three videos were recorded during diÔ¨Äerent
hours of the day and on diÔ¨Äerent road segments of the TEM and E5 roads. These three
video recordings are used for evaluation purposes. The metrics are calculated and given
in Table 8.1.
The presented hybrid system tracks other vehicles on the highway with a high rate
of true positives per frame, and results are plotted in Figure 8.13. Figure 8.13 and
Figure 8.14 present the captured results of the presented recognition and tracking
method.
Table 8.1 Performance results
Video dataset
TPR
FDR
TP/frame
FP/frame
TEM/E5 1
98.8%
4.6%
1.7
0.011
TEM/E5 2
93.5%
8.2 %
5.8
0.151
TEM/E5 2
96.4 %
5.5 %
2.2
0.039

200
Hybrid Intelligence for Image Analysis and Understanding
Figure 8.13 Screenshots of recognition and tracking system result for TEM highway during rush hour.
Figure 8.14 Screenshots of recognition and tracking system results for LISA-Q Front FOV 1 during rush
hour.
Publicly available video recordings are used for comparison purposes. For diÔ¨Äerent
conditions of traÔ¨Éc and road structure, the presented system is tested with the videos
entitled in [12] as Lisa-Q Front FOV 1 (delivering rush hour traÔ¨Éc conditions), Lisa-Q
Front FOV 2 (for highway scenes), and Lisa-Q Front FOV 3 (for urban driving). The
results are given in Table 8.2.
Validation stages presented in Section 8.4 improve the matching accuracy of the
presented recognition and tracking method, and fewer error-prone results are gener-
ated in comparison with the active learning scheme presented by ALVeRT in [12]. By
comparing the metric results given in Table 8.2, the true positive rate (TPR) results are
higher than ALVeRT for the two videos named Lisa-Q Front FOV 2 (highway traÔ¨Éc)
and Lisa-Q Front FOV 3 (urban traÔ¨Éc). When comparing with a passively trained
model for all three driving scenarios such as rush hour, highway, and urban traÔ¨Éc, the

Feature-Based Robust Description and Monocular Detection: An Application to Vehicle tracking
201
Table 8.2 Performance results (video data set belonging to [12])
Video dataset
TPR
FDR
TP/frame
FP/frame
LISA-Q FOV 1: rush hour
83.3%
15.3%
1.6
2.2
LISA-Q FOV 1: highway
96.3%
5.6 %
2.6
0.03
LISA-Q FOV 1: urban
99.6 %
1.6 %
0.98
0.01
false detection rate (FDR) of our recognition and tracking system is signiÔ¨Åcantly lower
than the passively trained results presented in [12]. Lower FDR validates the higher
level in accuracy for detection and tracking of land vehicles for various traÔ¨Éc scenes.
Results of recognition and tracking system during rush hour are plotted in Figure 8.14.
8.5.3
Local versus Global Features Assessment
Performance regarding true positives and false positives per frame is evaluated for
tracking by using local and global features applied to the videos recorded on the TEM
highway. GPRD is used for tracking of local features as descriptors, and it outperforms
the SIFT descriptors at the matching stage. The true positive per frame metrics for the
case of global and local features are evaluated on the three diÔ¨Äerent videos, and the
results are plotted in Figure 8.15. In Figure 8.16, false positive per frame results are
given, and object detection is not evaluated by using local features.
Considering the usage of computational resources, real-time processing of local
features with four frames per second requires a custom CUDA-enabled NVIDIA GPU,
whereas global features are more suitable for real-time processing, and 15 frames per
second processing performance is achievable on a Intel Core i5 CPU.
8.6
Conclusions
In this section, state-of-the-art image feature extraction methodologies and their imple-
mentations are presented. Instead of local feature extraction methodologies sensitive
to local image data changes that happen frequently for vehicle surfaces moving on a
Figure 8.15 Comparison of true positive per
frame metrics for global and local features.
0
TEM/E5 1 TEM/E5 2
TEM/E5 3
2
4
6
Global features
Local features

202
Hybrid Intelligence for Image Analysis and Understanding
Global features
Local features
TEM/E5 1 TEM/E5 2 TEM/E5 3
0
0.1
0.2
0.3
Figure 8.16 Comparison of false positive
per frame metrics for global and local
features.
road scene, global feature extraction methodologies are used in the context of object
description. Haar-like features are adopted for global feature extraction and used for
describing object models. A set of vehicle rear-view image data is provided for training
purposes, and obtained indexation is used as a preliminary object detection step in
our implementation with the Adaboost classiÔ¨Åer. This preliminary detection provides
ROIs involving global features resembling the features of the trained object model. But
this preliminary detection is not accurate and generates a signiÔ¨Åcant amount of false
positives. A validation algorithm is developed to enhance accuracy without increasing
the computational cost of the method. We examined the low-level image characteristics
of the vehicle rear-view images as the objects in our study. Vehicle images present
symmetric features, particularly on the rear side. Color channels are used to seek a
texture-based symmetry axis. In addition to symmetry search, contour images are
extracted from detected ROIs. Vehicle textures generate horizontal lines remarkably
signiÔ¨Åcantly in the road scene. Preliminary detection is validated depending on sym-
metric features and horizontal line frequency. A tracking algorithm is implemented
using temporal detection history in which previous vehicle detections are classiÔ¨Åed
as either active or inactive. As a voting mechanism, the temporal detection history is
used for preliminary validation as well. An eÔ¨Écient hybrid method is presented toward
active safety of intelligent vehicles.
References
1 Lowe, D.G. (1999) Object recognition from local scale-invariant features, in
Computer Vision, 1999. The Proceedings of the Seventh IEEE International
Conference, vol. 2, pp. 1150‚Äì1157, doi: 10.1109/ICCV.1999.790410.
2 Lowe, D.G. (2004) Distinctive image features from scale-invariant keypoints. Int. J.
Comput. Vision, 60 (2), 91‚Äì110, doi: 10.1023/B:VISI.0000029664.99615.94. Available
from http://dx.doi.org/10.1023/B:VISI.0000029664.99615.94.
3 Bay, H., Ess, A., Tuytelaars, T., and Gool, L.V. (2008) Speeded-up robust
features (SURF). Computer Vision and Image Understanding, 110 (3),

Feature-Based Robust Description and Monocular Detection: An Application to Vehicle tracking
203
346‚Äì359, doi:http://dx.doi.org/10.1016/j.cviu.2007.09.014. Available from
http://www.sciencedirect.com/science/article/pii/S1077314207001555.
4 Serre, T., Wolf, L., and Poggio, T. (2005) Object recognition with features inspired by
visual cortex, in Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE
Computer Society Conference, vol. 2, pp. 994‚Äì1000, doi: 10.1109/CVPR.2005.254.
5 Viola, P. and Jones, M. (2001) Rapid object detection using a boosted cascade of
simple features, in Computer Vision and Pattern Recognition, 2001. CVPR 2001.
Proceedings of the 2001 IEEE Computer Society Conference, vol. 1, pp. I‚Äì511‚ÄìI‚Äì518,
doi: 10.1109/CVPR.2001.990517.
6 Ozuysal, M., Lepetit, V., and Fua, P. (2009) Pose estimation for category speciÔ¨Åc
multiview object localization, in Conference on Computer Vision and Pattern
Recognition, Miami, FL.
7 Trivedi, M.M. (2014) Computer Vision and Robotics Research Laboratory, University
of California, San Diego.
8 Moreno, P., Marin-Jimenez, M.J., Bernardino, R., and Blanca, N.P.D.L. (2007)
A comparative study of local descriptors for object category recognition: SIFT vs
HMAX.
9 Sinha, S.N., Frahm, J.M., Pollefeys, M., and Genc, Y. (2006) GPU-based video feature
tracking and matching, Tech, Rep., in Workshop on Edge Computing Using New
Commodity Architectures.
10 Ziegler, G., Tevs, A., Theobalt, C., and Seidel, H.P. (2006) GPU point list
generation through histogram pyramids, in Proceedings of VMV, Aachen, Germany,
pp. 137‚Äì141.
11 Y√≠ld√≠z, R. and Acarman, T. (2012) Image feature based video object description
and tracking, in Vehicular Electronics and Safety (ICVES), 2012 IEEE International
Conference, pp. 405‚Äì410, doi: 10.1109/ICVES.2012.6294255.
12 Sivaraman, S. and Trivedi, M.M. (2010) A general active-learning framework
for on-road vehicle recognition and tracking. IEEE Transactions on Intelligent
Transportation Systems, 11 (2), 267‚Äì276.

205
9
A GIS Anchored Technique for Social Utility Hotspot Detection
Anirban Chakraborty1, J.K. Mandal2, Arnab Patra1, and Jayatra Majumdar1
1Department of Computer Science, Barrackpore Rastraguru Surendranath College, Barrackpore, Kolkata, West Bengal,
India
2Department of Computer Science & Engineering, University of Kalyani, Kalyani, Nadia, West Bengal, India
9.1
Introduction
Criminals and crime are not new, since such a sense of cruelty is originated right from
the epic age of Mahabharata and Ramayana, or even before. The merciless and ruthless
professions of crime prove to be very much eÔ¨Äective in triggering a puÔ¨Äof fear and
revulsion in people‚Äôs mind. But why does someone choose such heartless professions?
One of the main reasons can be poverty. In a country like India, the sentence ‚ÄúThe rich
get richer, and the poor get poorer‚Äù proves to be very true as poverty is successful in
climbing at a very snappy pace over time. Because of this, mortals suÔ¨Äering from not
being able to fulÔ¨Åll even the basic amenities resort to illegal means that may include
snatching property and even taking the lives of innocent people.
The proposed technique makes an attempt for eradicating crime. It aims at predicting
the spread of criminal activities and fruitful measures for combating crime. The
proposed technique enlightens one of the most common and important measures for
controlling criminal activities: uniformly setting up police stations, especially in the
crime-sensitive zones. Initially, 21 Anchals under seven police stations of the state of
West Bengal in India (i.e., Egra, Barrackpore, Uttarpara, Mogra, Ranaghat, Matigara,
and Coochbehar) have been chosen as the case study area, to draw a relationship
between population and number of crimes (with associated weighted factors). All of
the 32 Anchals under the Egra police station, which is situated at Purba Medinipur in
West Bengal, India, have been picked for detailed analysis. All information (including
the map of Egra) related to the proposed work is acquired from the Egra police station.
The map of Egra is shown in Figure 9.1.
The proposed task targets the crime-keen areas for the years 2011, 2012, and 2013. The
areas detected as crime prone (i.e., hotspots) are highlighted by red color on the map,
making it understandable for all users. Using the method of the fuzzy c-means (FCM)
clustering technique, the present methodology also clusters the pointed hotspot zone
and suggests the suitable locations for setting up new police stations for eradication of
crime.
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

206
Hybrid Intelligence for Image Analysis and Understanding
Figure 9.1 Map of Egra police station, West Bengal, India.
Along with its numerous applications in a variety of areas, GIS could be considered
as a very powerful tool for creation or analysis of spatial data and, moreover, as smart
visualization of the results on the map that has been fed input. A diverse number of
industries in various sectors are thus beneÔ¨Åciaries by GIS. Nowadays, there is positive
awareness regarding the impact on the society that GIS can have. IdentiÔ¨Åcation of
hotspots is just one of them. A hotspots have some special statistical signiÔ¨Åcance. On
the basis of some statistical data analysis reports, a hotspot is heavily distinguished
from the rest of a given study area. Till now, only criminal hotspots, which are areas
sensitive with respect to crime, have received concentrated attention. But identiÔ¨Åcation
of earthquake hotspots (areas that have a high probability of an earthquake occurring)
and identiÔ¨Åcation of accident hotspots (regions having a high probability of road
accidents) are also very helpful for the modern society and prevent loss of human lives
(a loss that cannot be compensated), properties, and money.
The above-mentioned hotspot-revealing methodology can also be extended for
unmasking many other similar hotspot-disclosing jobs, such as:
‚Ä¢ Detection of earthquake-prone areas
‚Ä¢ Detection of landslide-sensitive areas
‚Ä¢ Detection of accident-prone (road accidents) areas.

A GIS Anchored Technique for Social Utility Hotspot Detection
207
Table 9.1 Hotspot-detecting tasks, depending on respective
factors
Hotspot-detecting tasks
with respect to
InÔ¨Çuencing factors
Earthquakes
‚Ä¢ Level of seismic hazards [3]
‚Ä¢ Changes in Vp/Vs [4]
‚Ä¢ Radon emission [4]
‚Ä¢ Electromagnetic variations [4]
Landslides [5]
‚Ä¢ Proximity of earthquake
‚Ä¢ Encumbering on rock material
‚Ä¢ Banishment of lateral support
‚Ä¢ Changes in the water sample of
rock or soil frame
‚Ä¢ Clogged drainage
Note: Vp: velocity of a seismic wave passing through rock; and Vc:
velocity of a secondary or shear wave.
However, the factors causing all such additional hotspot detection tasks to be
conducted would be unalike. For example, the inÔ¨Çuencing factors that would prompt
the disclosure of earthquake-prone spots and landslide-sensitive areas are simulated in
Table 9.1.
A number of existing hotspot detection methods exist, having varying application
areas. ISODATA [10, 11] and STAC [9] are used for identiÔ¨Åcation of criminal hotspots.
ISODATA uses the software TNT-mips 6.4 for handling maps, and to get eÔ¨Écient
results, values of a number of parameters such as lumping parameters, initial cluster
means, splitting parameters, and so on need to be specify beforehand. Although STAC
doesn‚Äôt need these types of initial values to specify, it mainly generates circular clusters
always; also, this mainframe program has not yet been tested in real life. For detection
of earthquake hotspot zones, RADIUS [12] [13] is an existing technique that works
with the help of MS Excel, but unfortunately it gives very general approximations and
the output is not properly visualized. Multicriteria evaluation (MCE) [15] is a tool that
helps in detection of landslide hotspot zones. As the name reveals, it is based on many
analytical hierarchy processes (AHPs), making it much too complex.
Section 9.2 of this chapter throws light on the methodology used, Section 9.3 discusses
the case study areas adapted for the purpose, Section 9.4 shows the result obtained after
application of the technique on the case study area, Section 9.5 deals with analysis of
the proposed method, as well as comparison with other existing techniques. Finally,
conclusions are drawn in Section 9.6.
9.2
The Technique
The present technique (which is basically a rank-based clustering method) uses the
crime data report obtained from Ô¨Åeld study as its backbone. There is no strict rule for
determining the number of case study areas to be chosen, but increasing the number
of study areas increases the accuracy of results obtained from regression analysis. It is
better to choose the study areas far apart from each other for better results. All these
areas should be grouped into three major categories in terms of criminal activities:
very sensitive, moderately sensitive, and least sensitive. A numerical rank-based value
(which may be a 10-point scale) is tagged thereafter. Unprejudiced answers obtained

208
Hybrid Intelligence for Image Analysis and Understanding
from police personnel through a method of questionnaire are very eÔ¨Äective for the
above ranking. To overcome human bias, quite a large number of persons should
be interviewed in each study area, the average of which forms the Ô¨Ånal output. For
ranking various criminal activities, the penal code of any country could be a pioneer.
A numerical value is also assigned with each type of crime, depicting how heinous is it,
driven by the country‚Äôs penal code. This forms the basis of the regression analysis.
The nth-degree polynomial of the form y = a0xn + a1xn‚àí1 + ‚Ä¶ an is considered for
the purpose of regression analysis. To evaluate the unknown variables, data obtained
(population and criminal activities reported to the police station for the considered
year) from Ô¨Åeld study is taken into account. Evaluating the unknowns enables to rank
any unknown region on a particular scale (say, 10), if only its population and crime
reports are known.
For the entire large region intended for clustering on the basis of criminal hotspots,
each of its constituent smaller areas should be ranked on a particular scale, using the
above procedure. Let the ranking be done on a 10-point scale. Then, not only the regions
with rank 10 would be considered as hotspots, but a region more than half of whose
neighbors are hotspots also would be considered alarming for the future and is pointed
out as a hotspot.
The objective of the present method is not only to Ô¨Ågure out the crime hotspots but
also to control the criminal activities by suggesting proper locations for police posting
(i.e., the construction of new beat houses). The following steps are carried out in order
to determine the locations for construction of police beat houses. To achieve this task,
the following trails need to be worked through:
The entire crime hotspot zone already demarcated is considered, with two diÔ¨Äerent
cases:
‚Ä¢ Case 1: If the zone contains more than three constituent regions, then for every Ô¨Åve
regions (obtained by rounding up the result total_number_of_regions / 5) one beat
house is proposed, that is, for 4 to 7, one beat house is proposed; for 8 to 12, two
beat houses; for 13 to 17, three beat houses; and so on. Let this number of proposed
beat houses be denoted by n. Thus, the target is to divide the zone into n clusters, for
which FCM clustering could be helpful because it produces compact clusters.
For the present purpose, hard FCM has been considered where each data sample is
assigned to only a particular class. U is a 2D matrix denoting the membership value
of any data point to any class. Let this membership assignment of kth data sample in
the ith class be denoted by ùúÜij. dik is a Euclidean distance measure between the kth data
sample and ith cluster, given by dik = d(xk ‚àíùë£i), where vi is the ith cluster center.
Thus, after determining the number of clusters c(2 ‚â§c < n) for partitioning n data
points, the membership matrix is initialized, and cluster centers are calculated
accordingly. At each iteration, characteristic functions (for all i, k) are updated, using
ùúÜik = 1 for d(r)
ik = min{d(r)
jk } for all j ‚ààc, or else ùúÜik = 0. The iteration stops when
||U(r+1)‚å£U(r) ‚â§ùúâ|| (tolerance level).
Centroid of each cluster is the target location of construction of police beat houses.
‚Ä¢ Case 2: If the cluster contains less than four constituent regions, the centroid of the
entire hotspot zone is the proposed location for a beat house.
As discussed in Section 9.1, the intended work can also be very successful in
unmasking many other hotspot detection tasks (e.g., earthquake-prone areas,

A GIS Anchored Technique for Social Utility Hotspot Detection
209
landslide-sensitive zones, a-prone areas, etc.). The only variation lies in the
inÔ¨Çuencing factors to take care of. However, it is not always necessary to select the
scale of 10 units for accomplishing the proposed work, since any unit of scale (e.g., 5,
8, 15, etc.) can be picked up for accomplishing the intended work.
9.3
Case Study
The already-mentioned technique has successfully been applied for determining
criminal hotspots of the Egra police station area, situated at Purba Medinipur in
West Bengal, India. The stepwise procedure applied helps in better understanding the
technique just discussed. Egra comprises 32 Anchals. The existing police stations are
situated at Kosba-1, Vivekananda-1, and Chatri-2, as depicted in Figure 9.2.
Detection of crime hotspots (i.e., the crime-sensitive areas) of Egra is done for the
years 2011, 2012, and 2013. In order to accomplish this task, a procedure of tying up
rank (an integer value) to each region is performed. The execution of associating rank
to each region is rendered, based on the criminal activities of that speciÔ¨Åc region. The
rank assessment is performed on the scale of 10. When the rank of a region is 10, it is
regarded as the ‚Äúhighest crime-prone,‚Äù area and similarly a region that is the least crime
prone is tagged with rank 1. To evaluate the rank associated with a particular region,
information is gathered from seven distinct police stations throughout the state of West
Bengal. The highest, moderate, and least crime-prone areas (as shown in Table 9.2),
coming under the wing of each of the respective police stations, are grasped, based on
data supplied by a number of police personnel after surveying them through a separate
questionnaire, from each of the seven police stations, which serves as the base of the
study.
Figure 9.2 Existing police stations of Egra.

210
Hybrid Intelligence for Image Analysis and Understanding
Table 9.2 Table depicting the least, moderate, and highest
crime-prone regions situated in seven police stations
throughout West Bengal State, India
Crime-sensitive areas
Police stations
Highest
sensitive
Moderate
sensitive
Least
sensitive
1. Egra
Jumki-1
Chetri-1
Kosba-1
2. Barrackpore
Ardali
Aamtala
Anandapuri
Bazar
3. Uttarpara
Kanaipur
Kotrong
Bhadrakali
Colony
4. Mogra
Kalitala
Damra
Joypur
5. Ranaghat
Kuparse
Nasra
Payradanga
6. Matigara
Phansideoa
Sivmandir
Babupara
7. Coochbehar
Bamanhat
Pundibari
Sadar
Table 9.3 ClassiÔ¨Åcation of crime types with respect to their rankings
Ranking
Types of crime
10
Murder
9
Dacoity
8
Arms Act
Rape (376 IPC)
7
Crime against women attempt to murder (498/302 IPC)
6
Kidnapping (363A/366A IPC)
Dowry death (498A/304B IPC)
Rioting
OÔ¨Äence Against Women (OAW)
5
Robbery
4
Crime against women (498A/323IPC)
Assemble for preparation of dacoity (399/402 IPC)
Crime related to drug (NDPS ACT)
3
Burglary
2
Theft
1
Molestation (354 IPC)
Others (such as pickpocketing)
For the present study, 10 diÔ¨Äerent crime types have been considered. Each type is
adorned with a rank, the evaluation of which is done on the basis of the Indian Penal
Code as shown in Table 9.3, where the higher the rank, the more heinous is the crime.
A weight in a 5-point scale is associated with each item of Table 9.3. Thus, a crime
with rank 10 has a weight of 5, and that of rank 1 is 0.5.
The intended work is then buttoned up using the regression analysis strategy [2].
For the regression analysis procedure, ninth-degree polynomial curve Ô¨Åtting has been

A GIS Anchored Technique for Social Utility Hotspot Detection
211
considered, the form of which is shown in Equation (9.1):
y = a + bx + cx2 + dxùüë+ exùüí+ fxùüì+ gxùüî+ hxùüï+ ixùüñ+ jxùüó
(9.1)
In order to cook out Equation (9.1), the following simultaneous equations need to be
executed:
na + b
n
‚àë
k=1
xk + c
n
‚àë
k=1
x2
k + d
n
‚àë
k=1
x3
k + e
n
‚àë
k=1
x4
k + f
n
‚àë
k=1
x5
k
+ g
n
‚àë
k=1
x6
k + h
n
‚àë
k=1
x7
k + i
n
‚àë
k=1
x8
k + j
n
‚àë
k=1
x9
k ‚àí
n
‚àë
k=1
yk = 0
(9.1a)
a
n
‚àë
k=1
xk + b
n
‚àë
k=1
x2
k + c
n
‚àë
k=1
x3
k + d
n
‚àë
k=1
x4
k + e
n
‚àë
k=1
x5
k
+ f
n
‚àë
k=1
x6
k + g
n
‚àë
k=1
x7
k + h
n
‚àë
k=1
x8
k + i
n
‚àë
k=1
x9
k + j
n
‚àë
k=1
x10
k ‚àí
n
‚àë
k=1
xkyk = 0
(9.1b)
a
n
‚àë
k=1
x2
k + b
n
‚àë
k=1
x3
k + c
n
‚àë
k=1
x4
k + d
n
‚àë
k=1
x5
k + e
n
‚àë
k=1
x6
k
+ f
n
‚àë
k=1
x7
k + g
n
‚àë
k=1
x8
k + h
n
‚àë
k=1
x9
k + i
n
‚àë
k=1
x10
k + j
n
‚àë
k=1
x11
k ‚àí
n
‚àë
k=1
x2
kyk = 0
(9.1c)
a
n
‚àë
k=1
x3
k + b
n
‚àë
k=1
x4
k + c
n
‚àë
k=1
x5
k + d
n
‚àë
k=1
x6
k + e
n
‚àë
k=1
x7
k
+ f
n
‚àë
k=1
x8
k + g
n
‚àë
k=1
x9
k + h
n
‚àë
k=1
x10
k + i
n
‚àë
k=1
x11
k + j
n
‚àë
k=1
x12
k ‚àí
n
‚àë
k=1
x3
kyk = 0
(9.1d)
a
n
‚àë
k=1
x4
k + b
n
‚àë
k=1
x5
k + c
n
‚àë
k=1
x6
k + d
n
‚àë
k=1
x7
k + e
n
‚àë
k=1
x8
k
+ f
n
‚àë
k=1
x9
k + g
n
‚àë
k=1
x10
k + h
n
‚àë
k=1
x11
k + i
n
‚àë
k=1
x12
k + j
n
‚àë
k=1
x13
k ‚àí
n
‚àë
k=1
x4
kyk = 0
(9.1e)
a
n
‚àë
k=1
x5
k + b
n
‚àë
k=1
x6
k + c
n
‚àë
k=1
x7
k + d
n
‚àë
k=1
x8
k + e
n
‚àë
k=1
x9
k
+ f
n
‚àë
k=1
x10
k + g
n
‚àë
k=1
x11
k + h
n
‚àë
k=1
x12
k + i
n
‚àë
k=1
x13
k + j
n
‚àë
k=1
x14
k ‚àí
n
‚àë
k=1
x5
kyk = 0
(9.1f)
a
n
‚àë
k=1
x6
k + b
n
‚àë
k=1
x7
k + c
n
‚àë
k=1
x8
k + d
n
‚àë
k=1
x9
k + e
n
‚àë
k=1
x10
k
+ f
n
‚àë
k=1
x11
k + g
n
‚àë
k=1
x12
k + h
n
‚àë
k=1
x13
k + i
n
‚àë
k=1
x14
k + j
n
‚àë
k=1
x15
k ‚àí
n
‚àë
k=1
x6
kyk = 0
(9.1g)
a
n
‚àë
k=1
x7
k + b
n
‚àë
k=1
x8
k + c
n
‚àë
k=1
x9
k + d
n
‚àë
k=1
x10
k + e
n
‚àë
k=1
x11
k

212
Hybrid Intelligence for Image Analysis and Understanding
+ f
n
‚àë
k=1
x12
k + g
n
‚àë
k=1
x13
k + h
n
‚àë
k=1
x14
k + i
n
‚àë
k=1
x15
k + j
n
‚àë
k=1
x16
k ‚àí
n
‚àë
k=1
x7
kyk = 0
(9.1h)
a
n
‚àë
k=1
x8
k + b
n
‚àë
k=1
x9
k + c
n
‚àë
k=1
x10
k + d
n
‚àë
k=1
x11
k + e
n
‚àë
k=1
x12
k
+ f
n
‚àë
k=1
x13
k + g
n
‚àë
k=1
x14
k + h
n
‚àë
k=1
x15
k + i
n
‚àë
k=1
x16
k + j
n
‚àë
k=1
x17
k ‚àí
n
‚àë
k=1
x8
kyk = 0
(9.1i)
a
n
‚àë
k=1
x9
k + b
n
‚àë
k=1
x10
k + c
n
‚àë
k=1
x11
k + d
n
‚àë
k=1
x12
k + e
n
‚àë
k=1
x13
k
+ f
n
‚àë
k=1
x14
k + g
n
‚àë
k=1
x15
k + h
n
‚àë
k=1
x16
k + i
n
‚àë
k=1
x17
k + j
n
‚àë
k=1
x18
k ‚àí
n
‚àë
k=1
x9
kyk = 0
(9.1j)
where n is the number of data points.
The tactics behind extracting the value of x and y in order to carry out the above
equations [and thereby executing Equation (9.1)] is on the basis of the data collected
from seven police stations, with three regions existing beneath the wrap of each partic-
ular police station, classiÔ¨Åed as the highest, moderate, and least crime-prone regions.
‚Ä¢ The region with the ‚Äúmaximum‚Äù crime-prone tag has a y value of 10.
‚Ä¢ The region with the ‚Äúmoderate‚Äù crime-prone tag has a y value of 5.
‚Ä¢ The region with the ‚Äúleast‚Äù crime-prone tag has a y value of 1.
The x‚Äôs associated with each of these y‚Äôs is determined using the following two factors:
‚Ä¢ Total population of the particular region
‚Ä¢ Total cases received with each classiÔ¨Åed crime types against the same region.
Finally, a particular x value (with respect to a particular y) is plucked out using
Equation (9.2).
x = Total population of a particular region
(‚àë10
i=1 Ci √ó J) of the same region
(9.2)
where J = weighted value, as obtained from Table 9.3; and Ci = total number of cases
received with respect to the ith ranked crime type. Table 9.4 portrays the required data
for computing Equation (9.2).
Now, by dividing numerators by the respective denominators, the x values are
obtained from the above table and the corresponding y values are fetched from
Table 9.2 (by assigning values high=10, moderate=5, and least=1, as discussed).
Tables 9.5, 9.6, and 9.7 give a Ô¨Çeeting look on the Ô¨Ånal computed x‚Äôs and y‚Äôs forming the
basis of regression analysis.
Thus, here the process of regression analysis is dealing with 21 data points (i.e., the
value of n).

Table 9.4 Table depicting the numerator and denominator values of respective regions calculated with respect to the crime types
(as obtained from Table 9.3)
Region
name
Value of the
numerator of eq(b)
Crime types
Value of the
denominator of eq(b)
CH1
CH2
CH3
CH4
CH5
CH6
CH7
CH8
CH9
CH10
Weighted factors
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
Kosba-I
14,490
0
0
0
0
1
0
0
1
5
1
10
Anandapuri
17,064
0
0
0
0
0
0
1
3
1
9
12
Bhadrakali
15,235
0
0
0
0
0
0
0
5
2
2
10.5
Joypur
12,027
0
0
0
0
0
0
0
0
2
13
8.5
Payradanga
15,829
0
0
0
0
1
0
1
2
3
0
11
Babupara
13,547
0
0
0
1
0
0
1
0
4
0
9.5
Sadar
15,130
0
0
1
0
2
0
0
0
0
1
10.5
Chatri-I
11,792
0
0
0
0
2
1
0
1
3
6
16
Aamtala
18,399
0
2
0
0
3
1
2
0
0
0
24.5
Kotrong
12,176
0
0
1
1
1
0
2
1
0
0
16
Damra
13,072
0
0
0
0
0
5
0
3
0
1
17.5
Nasra
15,603
0
1
1
1
0
1
0
2
7
2
21
Sivmandir
14,781
1
0
2
2
1
1
0
1
0
1
19.5
Pundibari
17,155
0
3
1
1
0
0
1
3
0
0
23.5
Jumki-I
15,334
4
4
14
14
22
10
0
10
0
1
225.5
Ardali Bazar
10,325
3
10
1
1
30
32
2
14
2
21
295
Kanaipur Colony
11,067
0
0
28
28
11
13
14
9
0
0
217
Kalitala
11,560
3
2
0
0
10
3
16
15
32
83
189.5
Kuparse
11,895
2
12
5
5
16
0
26
32
36
39
305
Phansideoa
10,626
12
4
0
0
0
28
14
22
13
18
231
Bamanhat
19,345
6
10
3
3
15
14
40
0
0
7
265

214
Hybrid Intelligence for Image Analysis and Understanding
Table 9.5 Table delineating the Ô¨Ånal values of x‚Äôs
and y‚Äôs (data for y=1)
x 1449 1422 1451 1415 1439 1426 1441
y
1
1
1
1
1
1
1
Table 9.6 Table delineating the Ô¨Ånal values of
x‚Äôs and y‚Äôs (data for y=5)
x 737 751 761 747 743 758 730 737
y
5
5
5
5
5
5
5
5
Table 9.7 Table delineating the Ô¨Ånal
values of x‚Äôs and y‚Äôs (data for y=10)
x 68 35 51 61 39 46 73 68
y
10 10 10 10 10 10 10 10
Conclusively, ‚àën=21
k=1 xk = 15,647 and ‚àën=21
k=1 yk = 112 (obtained from Tables 9.5, 9.6,
and 9.7) and the following values could also be computed:
n
‚àë
k=1
xk = 15647
n
‚àë
k=1
x2
k = 18339438,
n
‚àë
k=1
x3
k = 23602892464,
n
‚àë
k=1
x4
k = 3.18663E + 13,
n
‚àë
k=1
x5
k = 4.42386E + 16,
n
‚àë
k=1
x6
k = 6.23811E + 19,
n
‚àë
k=1
x7
k = 8.87138E + 22,
n
‚àë
k=1
x8
k = 1.26738E + 26,
n
‚àë
k=1
x9
k = 1.815E + 29,
n
‚àë
k=1
x10
k = 2.60266E + 32,
n
‚àë
k=1
x11
k = 3.73484E + 35,
n
‚àë
k=1
x12
k = 5.36175E + 38,
n
‚àë
k=1
x13
k = 7.69931E + 41,
n
‚àë
k=1
x14
k = 1.10579E + 45,
n
‚àë
k=1
x15
k = 1.58834E + 48,
n
‚àë
k=1
x16
k = 2.28172E + 51,
n
‚àë
k=1
x17
k = 3.27809E + 54,
n
‚àë
k=1
x18
k = 4.70994E + 57,
n
‚àë
k=1
yk = 112,
n
‚àë
k=1
xkyk = 39933,
n
‚àë
k=1
x2
kyk = 34151972,
n
‚àë
k=1
x3
kyk = 35284771798,
n
‚àë
k=1
x4
kyk = 4.05878E + 13,
n
‚àë
k=1
x5
kyk = 5.07567E + 16,
n
‚àë
k=1
x6
kyk = 6.72536E + 19,
n
‚àë
k=1
x7
kyk = 9.2357E + 22,
n
‚àë
k=1
x8
kyk = 1.29462E + 26,
n
‚àë
k=1
x9
kyk = 1.83538E + 29

A GIS Anchored Technique for Social Utility Hotspot Detection
215
Table 9.8 Values of the unknown variables
used in Equation (9.1)
Unknowns
Values
a
10.230838005070808
b
‚àí0.0041658198089687196
c
‚àí3.3045749026962936E-6
d
‚àí1.265402658730446E-9
e
1.639773297320292E-14
f
6.076272376250422E-16
g
4.14913478269917E-19
h
4.201979398479235E-22
i
‚àí9.735005727709414E-26
j
‚àí1.4005417625307426E-28
The next step is to calculate the unknowns [i.e., a, b, ‚Ä¶ , j of Equation (9.1)]. To
accomplish this task, the Gaussian elimination method is used. The Ô¨Ånal values of the
unknowns are listed in Table 9.8.
Conclusively, Equation (9.1) now becomes:
y = ùüèùüé.ùüêùüëùüéùüñùüëùüñùüéùüéùüìùüéùüïùüéùüñùüéùüñ+ (‚àíùüé.ùüéùüéùüíùüèùüîùüìùüñùüèùüóùüñùüéùüñùüóùüîùüñùüïùüèùüóùüî)x
+ (‚àíùüë.ùüëùüéùüíùüìùüïùüíùüóùüéùüêùüîùüóùüîùüêùüóùüëùüîE ‚àíùüî)xùüê+ (‚àíùüè.ùüêùüîùüìùüíùüéùüêùüîùüìùüñùüïùüëùüéùüíùüíùüîE ‚àíùüó)xùüë
+ (ùüè.ùüîùüëùüóùüïùüïùüëùüêùüóùüïùüëùüêùüéùüêùüóùüêE ‚àíùüèùüí)xùüí+ (ùüî.ùüéùüïùüîùüêùüïùüêùüëùüïùüîùüêùüìùüéùüíùüêùüêE ‚àíùüèùüî)xùüì
+ (ùüí.ùüèùüíùüóùüèùüëùüíùüïùüñùüêùüîùüóùüóùüèùüïE ‚àíùüèùüó)xùüî+ (ùüí.ùüêùüéùüèùüóùüïùüóùüëùüóùüñùüíùüïùüóùüêùüëùüìE ‚àíùüêùüê)xùüï
+ (‚àíùüó.ùüïùüëùüìùüéùüéùüìùüïùüêùüïùüïùüéùüóùüíùüèùüíE ‚àíùüêùüî)xùüñ+ (‚àíùüè.ùüíùüéùüéùüìùüíùüèùüïùüîùüêùüìùüëùüéùüïùüíùüêùüîE ‚àíùüêùüñ)xùüó(9.3)
As already discussed, the Egra police station area (total population: 397,612), under
the Purba Medinipur district of West Bengal State, India, comprising 32 Anchals, has
been considered as the study area. For year 2013, data regarding the number of crimes
held in each of the 32 Anchals was collected. After multiplying with their respective
weights and summing up, the denominator of Equation (9.2) is found. Considering the
population of the region [numerator of Equation (9.2)], the value of x, corresponding
to the region, is cooked out. Finally, putting the value of x into Equation (9.1), the cor-
responding y (with rounding up to the nearest integer) is obtained; this is nothing but
the rank of that region, reÔ¨Çecting how crime prone the area is, as depicted in Table 9.9.
Based on the basis of their ranks, the Anchals are marked, as shown in Figures 9.3
and 9.4.
The present technique also tries to point out those regions that are not crime prone
(i.e., rank 10) presently, but have a tendency to become more crime-prone areas in near
future. The procedure for achieving this goal is as follows.
For any region with rank > 7, if more than 50% of its neighbor areas are crime prone
(i.e., rank 10), then it is also regarded as a crime hotspot for near future (equivalent to
rank 10). This is done by checking the adjacency of each region satisfying the following
condition:
ùüï< rank of a region ‚â§9 ‚Ä¶ Condition(1)

216
Hybrid Intelligence for Image Analysis and Understanding
Table 9.9 The ranks of respective Anchals (arranged in descending order of rank)
Anchal
Population
Crime types (from Table 9.3)
Rank
CH1
CH2
CH3
CH4
CH5
CH6
CH7
CH8
CH9
CH10
acquired
Weighted factors
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
Jumki-1
15,334
4
4
8
14
22
10
0
10
0
1
10
Panchrol-2
8618
3
3
4
4
5
5
6
8
10
14
10
Sahara-2
14,065
6
6
7
10
8
11
13
18
26
27
10
Basudebpur-1
7598
4
5
13
12
11
10
9
0
1
4
10
Paniparul-2
8023
10
3
2
5
16
12
0
0
7
0
10
Bathuyari-1
17,559
0
15
10
36
0
0
0
0
0
19
10
Dubda-2
18,500
7
8
13
18
11
19
0
0
0
45
10
Sahara-1
16,477
2
0
0
16
0
0
0
0
17
0
9
Paniparul-1
11,399
0
2
0
0
2
0
0
5
11
29
9
Chatri-2
11,011
0
3
0
2
0
0
0
0
4
19
8
Manjushree-2
9920
0
0
1
0
5
2
0
0
3
0
8
Jumki-2
16,071
3
2
2
2
0
1
0
0
0
0
8
Deshbandhu-2
13,047
0
2
1
3
2
0
0
0
0
7
8
Basudebpur-2
11,919
0
1
2
0
0
3
0
0
0
7
7
Rishi
8969
0
0
4
0
0
0
0
0
1
0
7
Bankimchandra-2
Dubda-1
13,065
0
1
0
4
2
0
0
0
1
0
7
Sarboday-2
8852
0
2
0
0
0
0
0
0
5
0
6
Deshbandhu-1
15,298
0
0
0
5
2
0
0
0
1
0
6
Vivekananda-1
11,759
3
0
0
0
1
0
0
0
0
1
6
Panchrol-1
13,035
0
0
0
0
0
8
0
0
1
0
6
Sarboday-1
8351
0
0
0
0
3
0
0
0
2
0
5
Chatri-1
11,792
0
0
0
0
2
1
0
1
3
6
5
Rishi
10,462
0
0
0
0
0
0
5
1
0
5
5
Bankimchandra-1
Bathuyari-2
14,232
0
2
2
0
0
0
0
1
0
0
5
Jerthan-2
9760
0
1
1
0
0
0
0
0
3
0
4
Vivekananda-2
12,574
2
0
0
0
0
0
0
1
0
7
4
Jerthan-1
13,056
0
0
2
1
0
0
0
0
2
0
3
Manjushree-1
11,415
0
0
3
0
0
0
0
0
0
0
3
Let A be a region satisfying Condition(1). The following two counting gimmicks need
to be performed:
‚Ä¢ Counting the number of adjacent regions of A.
‚Ä¢ Counting the number of adjacent regions of A having rank=10.
If the latter counting yields a value that is 50% or more than the former one, that is,
Total number of adjacent regions with rank 10 ‚â•50% of Total number of adjacent

A GIS Anchored Technique for Social Utility Hotspot Detection
217
Figure 9.3 Cluster formation after acquiring ranks.
1
3
5
7
9
Numbers indicating Ranks
2
4
6
8
10
Figure 9.4 Rank indicator.
regions ... Condition (2), then region A will also be treated as a crime hotspot. After
considering all such regions, the formation of a hotspot zone is depicted in Figure 9.5.
This task is performed to expose the spread of criminal activity in future.
After applying FCM clustering, for deciding suitable locations of police beat houses,
the outcome depicted in Figure 9.6 is obtained.
Here the original red hotspot zone, comprising 10 regions, is clustered into two, as
shown in Figure 9.7. Finally, the centroid position of each of these clusters, C1 and C2, is
considered to be the most preferential location for setting up beat houses (as shown in
Figure 9.8).

218
Hybrid Intelligence for Image Analysis and Understanding
Figure 9.5 Hotspot zone formation after considering Condition(1) and Condition(2).
Figure 9.8 shows the most favorable locations for building up beat houses.
Now, if the location of a predicted beat house is close enough to an existing one, then
the predicted location will be of no use, because the existing police station is suÔ¨Écient
enough to control that particular red zone. The Manhattan distance formula is used to
conduct this step. If the distance between an existing and a predicted police station is less
than 6 km (which has been properly converted to be Ô¨Åtted with the present resolution of
the map), that predicted location will no longer exist, as per our technique. Figures 9.9
and 9.10 demonstrate the outcome of this step.
If the technique proposes the construction of a new beat house at the almost same
location constantly for three years, then the matter should be seriously considered by the
respective authority. For example, in Figure 9.20, a beat house was suggested to the left
hotspot in 2011 only, but not in the following years. So this suggestion could be ignored.
However, two beat houses for the lower hotspot areas are suggested in each of 2011,
2012, and 2013 (Figures 9.20, 9.21, and 9.22), so this suggestion should be examined.

A GIS Anchored Technique for Social Utility Hotspot Detection
219
Figure 9.6 Depicting the encircled ‚Äúred‚Äù hotspot zone.
Figure 9.7 Splitting the ‚Äúred‚Äù zone into two clusters, C1 and C2.

220
Hybrid Intelligence for Image Analysis and Understanding
Predicted police station or beat house.
Figure 9.8 Depicting suitable locations for construction of beat houses.
Predicted beat house exists before
applying the above step.
Figure 9.9 Before.
Predicted beat house does not exist
after applying the above step, as it
is closer to an existing one.
Figure 9.10 After.

A GIS Anchored Technique for Social Utility Hotspot Detection
221
9.4
Implementation and Results
In this section, results of implementation, operations, and outputs are exhibited. To
fulÔ¨Åll the intended tasks, NetBeans (JAVA) [6, 7] was used. To carry out this imple-
mentation job, a Ô¨Çat-Ô¨Åle system is selected for data storage; no database is taken up for
such purposes, to increase portability.
The acts of culling up ‚ÄúCreate New ProÔ¨Åle‚Äù or ‚ÄúOpen an Existing ProÔ¨Åle,‚Äù creating a
new proÔ¨Åle, and recessing an existing proÔ¨Åle are portrayed in the Figures 9.11, 9.12, and
9.13, respectively.
The technique has an inbuilt user-friendly digitization tool. For digitization of a
newly fed map, at Ô¨Årst a suitable name should be given. It is suggested to use the
Geographic-Name of the area being digitized (Figure 9.12). By simply mouse dragging
through the border line of the raster map, the digitization [1] task of the present raster
map [1] is fulÔ¨Ålled. By the virtue of the blue trailing line, how much portion has already
Figure 9.11 Choosing options.
Figure 9.12 Creating a new proÔ¨Åle.
Figure 9.13 Opening an existing proÔ¨Åle.

222
Hybrid Intelligence for Image Analysis and Understanding
Figure 9.14 Digitization of a raster map.
digitized can be manifested. Such a digitized map is shown in Figure 9.14. Eventually,
the digitization procedure is completed by clicking the ‚ÄúSAVE‚Äù button. This saves the
digitization action carried out, with a proper display of an indication message to the
user. Later, when one wants to work on the previously digitized map, the ‚ÄúOpen Existing
ProÔ¨Åle‚Äù option is chosen.
After digitization of the raster map, imparting associated data to each region is
performed. To do so, the region requiring data association is clicked, and then the
‚ÄúEnter Data‚Äù button is clicked. A window, requiring name and population of the clicked
region, pops out. Finally, the ‚ÄúOK‚Äù button of the data association window is clicked,
after insinuating suitable information related to the considered region. The associated
data is then saved in a Ô¨Åle. Now, if the user requires fetching data with respect to any
speciÔ¨Åc region, he or she has to click on the ‚ÄúEnter Data‚Äù button, after clicking on the
considered region. After doing so, a message displaying the name and population of
the clicked region pops out, as depicted in Figure 9.15.
The next task is to enter the number of cases received with respect to each crime
type (shown in Table 9.3), related to a particular region in a speciÔ¨Åc year. To do so, the

A GIS Anchored Technique for Social Utility Hotspot Detection
223
Figure 9.15 Data association.
Figure 9.16 ‚ÄúClick Here‚Äù button.
Figure 9.17 Crime-Info window.
region for which crime information is needed is clicked. The user is then required to
click on the ‚ÄúClick Here‚Äù button, under the ‚ÄúCrime Info‚Äù label (shown in Figure 9.16).
A window comprising two buttons, namely, ‚ÄúTO UPDATE CRIME-INFO‚Äù and ‚ÄúTO
VIEW CRIME-INFO,‚Äù pops out (shown in Figure 9.17) and the user can click on the
desired button. For updating the desired values with respect to each crime type need
to be entered (Figure 9.18). After entering suitable data, the ‚ÄúOK‚Äù button of the crime
update window is clicked for storing the imparted information in a Ô¨Åle (Figure 9.18).
Now, if the user needs to retrieve crime information with respect to a particular region,
he or she just has to click on the ‚ÄúTO VIEW CRIME-INFO‚Äù button, after clicking on that
particular region. In order to take a glance at the existing or predictive police stations,
the user needs to click on the ‚ÄúExisting PS‚Äù or ‚ÄúPredictive PS‚Äù, under the label ‚ÄúPolice
Station,‚Äù as shown in Figure 9.19.
The proposed technique Ô¨Ånally conveys a quick look at the crime-sensitive areas of
the map considered (here, Egra, situated at Purba Medinipur in West Bengal, India) for
the desired years (here, for 2011, 2012, and 2013).
It is very clear from the above three outcomes that construction of two new police
stations is being suggested for the 3 consecutive years, whereas, in addition to these two,
one more police station was suggested only on the basis of the crime report for 2011. So
the concerned higher authority should consider constructing the two new police stations
in areas as nearby as possible to those suggested, and the suggestion based on only one
year‚Äôs crime report could be ignored.

224
Hybrid Intelligence for Image Analysis and Understanding
Figure 9.18 Number of cases received against respective crime types.
Figure 9.19 Buttons under ‚ÄúPolice Station‚Äù label.
9.5
Analysis and Comparisons
This section constitutes a number of tables, outlining a comparative study between the
proposed technique and a number of other clustering techniques and existing hotspot
detection methods.
‚Ä¢ Comparison with the K-means clustering technique [8]: In K-means clustering
technique, one needs to specify the value of K (i.e., the number of clusters), which
is not always easy to determine beforehand. DeÔ¨Åning the number of clusters can
be advantageous or disadvantageous with respect to the purpose of use. When the
area of application is hotspot detection, it is never possible to mention the value
of K beforehand. Thus, in gist, a comparison can be drawn between the proposed
technique and the K-means clustering technique, illustrated in Table 9.10.
‚Ä¢ Comparison with the fuzzy clustering method [14]: The fuzzy clustering method is
used to generate clusters based on the distance only. All regions have probabilities to
be included in a cluster. A region may even be a member of more than one cluster,
which can prove to be a disadvantage while performing hotspot detection tasks using

A GIS Anchored Technique for Social Utility Hotspot Detection
225
Figure 9.20 In the year 2011 (3 suggested police stations).
Figure 9.21 In the year 2012 (2 suggested police stations).

226
Hybrid Intelligence for Image Analysis and Understanding
Figure 9.22 In the year 2013 (2 suggested police stations).
Table 9.10 Depicting the comparative study between K-means and the proposed
method
Characteristics
Techniques
K-means
Proposed method
Automatic determination of
clusters
No
Yes
Basis of cluster formation
Based on minimum
distance (Ô¨Ånd the
closest centroid)
Mainly based on
associated data (crime
reports) as well as
distance
Prediction of new beat house
or police station
No
Yes
FCM. Table 9.11 depicts the summarized version of the comparative study drawn
between the proposed work and FCM clustering technique.
‚Ä¢ Comparison with ISODATA clustering method for hotspot detection [10, 11]: The
ISODATA clustering method can also be used to group the regions that are aÔ¨Äected
by crime. To detect hotspots using this method, the TNTmips 6.4 software is used,
which is problematic for native users. Besides, to run ISODATA, it is needed to

A GIS Anchored Technique for Social Utility Hotspot Detection
227
Table 9.11 Portraying the comparative study between the fuzzy clustering
method and proposed method
Characteristics
Techniques
Fuzzy c-means
Proposed method
Automatic determination of
clusters
No, since the
number of clusters
to be formed needs
to be given as input
Yes
Basis of cluster formation
Based on minimum
distance (Ô¨Ånd the
closest centroid)
Mainly based on
associated data (crime
reports) as well as
distance
Prediction of new beat house
or police station
No
Yes
Table 9.12 Illustrating the comparative study between ISODATA and the proposed
method
Characteristics
Techniques
ISODATA
Proposed method
Automatic determination of
clusters
Yes
Yes
Software dependency of
cluster
Yes (to detect hotspots using
this method, we need to use
TNTmips 6.4 software)
Yes
Prediction of new beat house
or police station
No
Yes
Complexity
More complex
Less complex
Initial mandatory parameters
Yes and must be speciÔ¨Åed to
run the method
No
specify various mandatory parameters like initial cluster means, splitting parameters,
lumping parameters, the minimum number of pixels in a cluster, and the number of
iterations, which are actually problematic for novices. Moreover, another disadvan-
tage of the software is that the result does not always reÔ¨Çect the same properties and
selected options. All the settings should be tried and balanced to reach the desired
number of groups. Thus, a summary can be drawn out, as depicted in Table 9.12.
‚Ä¢ Comparison with the STAC clustering method of hotspot detection [9]: Another
method is the STAC clustering method, which is also a crime hotspot detection
technique. But the problem of using this method is that valuable information about
the densely populated crime areas in the map is overlooked. Also, a Ô¨Åxed distance
gives too big clusters, which are unable to attach useful data. Since it was a mainframe
program, this had never been tested in real life (i.e., a police department). Besides all
this, in this technique, clusters that are formed as hotspots may overlap; as such, the
same incidents may be shared by diÔ¨Äerent clusters. Another issue is that, as STAC
illustrates the clusters as circles, some regions can be properly described by this;
however, as the actual shapes of regions are irregular, they cannot be well described
by a circle. Table 9.13 shows a comparative study in gist.

228
Hybrid Intelligence for Image Analysis and Understanding
Table 9.13 The comparative study between STAC and proposed method is drawn out
Characteristics
Techniques
STAC
Proposed method
Automatic determination of
clusters
No (the program
reports how many
hotspot areas it has
found, and then the
user has to choose how
many of these to map)
Yes
Basis of cluster formation
Based on current,
local-level law
enforcement and
community
information
Mainly based on
associated data (crime
reports) as well as
distance
Software dependency
Yes
No
Prediction of new beat house
or police station
No
Yes
Complexity
More complex
Less complex
Table 9.14 Illustrating the advantages of the proposed method over
RADIUS methodology
Characteristics
Techniques
RADIUS
Proposed method
Software dependency
Yes (implemented
using MS Excel)
No
Output generation
Not properly
displayed and gives
very general
approximation
Output comes with
a detail graphical
representation
Software dependency
Yes
No
Prediction of new beat
house or police station
No
Yes
Complexity
More complex
Less complex
As illustrated in section 9.1, the present method is well applicable for diÔ¨Äerent hotspot
detections other than crime, such as earthquake-prone areas, landslide-prone areas, and
so on. The present method thus could be easily compared to some existing clustering
techniques in those Ô¨Åelds, illustrated here:
1. Comparison with RADIUS ‚Äì a technique for hotspot detection related to earthquakes
[12, 13]: Table 9.14 depicts a comparative study between the RADIUS methodology
and the proposed technique.
2. Comparison with MCE ‚Äì a technique for hotspot detection related to landslides
[15]: MCE is a tool utilized for landslide detection works. Table 9.15 portrays the
advantages of the proposed method and landslide detection methods utilizing
MCE.

A GIS Anchored Technique for Social Utility Hotspot Detection
229
Table 9.15 Comparison with MCE ‚Äì a technique for hotspot detection
related to landslides
Characteristics
Techniques
MCE
Proposed method
Basis
Based on many
analytical hierarchy
processes (AHPs)
Based on factors
like proximity of
earthquake,
exposure of rock
material, etc.
Software dependency
AHP method is
accessible as a
built-in tool inside
IDRISI Andes
Software
No
Prediction of new beat
house or police station
No
Yes
Complexity
More complex
Less complex
9.6
Conclusions
In a country like India, depiction of crime takes place in a number of abhorrent ways,
like rape, murder, robbery, kidnapping, and so on. These crimes do not require any
introduction since these sights appear frequently in everyday newspapers. The reason
behind such ruthless professions can be many. However, the main reason lies in poverty.
Aristotle once said, ‚ÄúPoverty is the parent of revolution and crime.‚Äù However, who is
facing the maleÔ¨Åc consequences of crime? It is the common and innocent people. As
such, the common people are perpetually in search of slugs that can readily extend a
supportive hand for expelling crime and its vicious employees, the criminals. Initially,
police prevent common and innocent individuals from coming in contact with the
harmful consequences of crime. Thus, police stations are required to be set up after
going through a proper survey of crime-sensitive areas.
The intended work plays a decisive role in stretching out various measures in order to
get rid of crime. It not only points out the crime-sensitive zones, to make the common
people aware, but also predicts the most suitable locations for setting up beat houses
for eliminating crime from such sensitive zones. By just having a quick view on the
outcome of the intended work, anyone can understand how crime Ô¨Çourishes as well as
the eÔ¨Äorts to stop such Ô¨Çourishing. This would deÔ¨Ånitely torment criminals‚Äô mind, since
they would be forced to think twice before giving birth to such villainous actions. As
such, the proposed work would surely play a key role in discouraging crime in society.
Acknowledgments
The authors expresses a $1695.01 deep sense of hearty gratitude to the police personnel
of each of the seven police stations taken into consideration, for supplying all the
needful information that is the backbone of the proposed works. The authors are thank-
ful to the Department of Computer Science, Barrackpore Rastraguru Surendranath

230
Hybrid Intelligence for Image Analysis and Understanding
College, Kolkata, India, for providing all the infrastructural support to carry out the
intended work.
References
1 Anirban Chakraborty, J.K. Mandal, Arun Kumar Chakraborti (2011) A File base
GIS Anchored Information Retrieval Scheme (FBGISIRS) through vectorization of
raster map. International Journal of Advanced Research in Computer Science, 2 (4),
132‚Äì138.
2 S.A. Mollah. Numerical analysis and computational procedure: including computer
fundamentals and programming in Fortran77, 2nd ed. Books and Allied, New Delhi.
3 FEMA. (2005). Earthquake hazard mitigation for nonstructural elements: Ô¨Åeld
manual. FEMA 74-FM. FEMA, Washington, DC. Available from http://mitigation
.eeri.org/Ô¨Åles/FEMA74_FieldManual.pdf
4 Wikipedia. (N.d.) Earthquake prediction. Available from www.en.wikipedia.org/wiki/
Earthquake-prediction
5 Shilpi Chakraborty https://en.wikipedia.org/wiki/Earthquake_prediction and Ratika
Pradhan. (2012) Devlopment of GIS based landslide information system for the
region of East Sikkim. International Journal of Computer Application, 49 (7).
6 Zetcode. (N.d.) Java Swing tutorial. Available from www.zetcode.com/tutorials/
javaswingtutorial
7 Tutorials Point. (N.d.) Java tutorials. Available from www.tutorialspoint.com/java/
index.htm
8 Anirban Chakraborty, J.K. Mandal, S.B. Chandrabanshi, and S. Sarkar. (2013) A GIS
anchored system for selection of utility service stations through k-means method of
clustering, Conference Proceedings Second International Conference on Computing
and Systems (ICCS-2013)‚Äù McGraw-Hill Education (India), Noida, pp 244‚Äì251.
9 Illinois Criminal Justice Information Authority (ICJIA). STAC user manual. ICJIA,
Chicago. Available from: http://www.icjia.org/public/pdf/stac/hotspot.pdf
10 Liu, W., Hung, C.-C., Kuo, B.C., and Coleman, T. (2008) An adaptive clustering
algorithm based on the possibility clustering and ISODATA for multispectral image
classiÔ¨Åcation. Available from http://www.academia.edu/2873708/AN-ADAPTIVE-
CLUSTERING-ALGORITHM-BASED-ON-THE-POSSIBILITY-CLUSTERING-
AND-ISODATA-FOR-MULTISPECTRAL-IMAGE-CLASSIFICATION
11 Ball, G.H., and Hall, D.J. (1965) Isodata: a novel method of data analysis and pattern
classiÔ¨Åcation. Available from www.dtic.mil/cgi-bin/GetTRDoc?Location=U2&
doc=GetTRDoc.pdf&AD=AD0699616
12 van Westen, C., Slob, S., Montoya de Horn, L., and Boerboom, L. (N.d.) Application
of GIS for earthquake hazard and risk assessment: Kathmandu, Nepal. Interna-
tional Institute for Geo-Information Science and Earth Observation, Enschede,
the Netherlands. Available from http://adpc.net/casita/Case_studies/Earthquake
%20hazard%20assessment/Application%20of%20GIS%20for%20earthquake%20hazard
%20assessment%20Kathmandu%20%20Nepal/Data_preparation_image_Ô¨Åles.pdf.
13 Ravi Sinha, K.S.P. Aditya, and Achin Gupta. (2000), GIS-based urban seismic risk
assessment using Risk. IITB, ISET. Journal of Earthquake Technology, 45 (3‚Äì4),
41‚Äì63.

A GIS Anchored Technique for Social Utility Hotspot Detection
231
14 Nikhil R. Pal, Kuhu Pal, James M. Keller, and James C. Bezdek. (2005) A possibilistic
fuzzy c-means clustering algorithm. IEEE Transaction on Fuzzy Systems, 13 (4).
15 Ulrich Kamp, Benjamin J. Growley, Ghazanfar A. Khattak, and Lewis A. Owen.
(2008) GIS-based landslide susceptibility mapping for the 2005 Kashmir earthquake
region. Geomorphology, 101, 631‚Äì642.

233
10
Hyperspectral Data Processing: Spectral Unmixing,
ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
Vaibhav Lodhi, Debashish Chakravarty, and Pabitra Mitra
Indian Institute of Technology, Kharagpur, West Bengal, India
10.1
Introduction
Hyperspectral imaging is also termed as imaging spectroscopy because of the conver-
gence of spectroscopy and imaging. It is an emerging and widely used remote-sensing
technique that has been used in laboratory and earth observation applications in
diÔ¨Äerent domains like food inspection [1], vegetation [2], water resources [3], mining
and mineralogical operations [4], agricultural and aquacultural [5], biomedical [6], and
industrial engineering [7, 8]. In earth observation, data is normally collected using
spaceborne, airborne, and unmanned aerial system (UAS) platforms. A hyperspectral
sensor collects a stack of images with a spectral resolution of a few nanometers,
and generates a continuous spectral signature in terms of reÔ¨Çectance/radiance value
as a function of wavelength. On the other hand, multispectral imaging, a subset of
hyperspectral, generates discrete spectral proÔ¨Åles due to its low spectral resolution.
Hyperspectral data consists of two spatial dimensions along X and Y along with a third
spectral dimension (n) to construct a 3D data (X ‚àóY ‚àón). Due to this, hyperspectral
data is also known as hyperspectral data cubes. A hyperspectral imaging concept,
where a spectral signature at location (Xi, Yj) is a function of the Xspectral dimension,
is shown in Figure 10.1.
Hyperspectral data refers to a hypercube, due to its 3D nature (two spatial and
one spectral dimension). In hyperspectral imaging, a reÔ¨Çectance spectrum for each
pixel of the object under study is collected. During data collection from spaceborne
and airborne platforms, hyperspectral data suÔ¨Äers from atmospheric interactions,
internal system eÔ¨Äects, dark current, system noise, and so on. In order to minimize
these eÔ¨Äects and for postprocessing operations of the hyperspectral data, one requires
preprocessing operations. On the contrary, for characterization and identiÔ¨Åcation of
materials, classiÔ¨Åcation and analysis, parameters retrieval, and so on, postprocessing
operations are required. Eventually, hyperspectral image processing is divided into
two broad categories ‚Äì preprocessing and postprocessing. Atmospheric correction,
calibration, denoising, dimension reduction, and so on come under the category of
preprocessing, while spectral unmixing, classiÔ¨Åcation, target identiÔ¨Åcation, and the
like are in postprocessing operations. Numerous algorithms are available to perform
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

234
Hybrid Intelligence for Image Analysis and Understanding
Spatial Dimension = X*Y
Spectral Signature
at Xi, Yj Location
Spectral
Dimension
(n)
Y
X
Figure 10.1 Hyperspectral imaging concept.
pre- and postprocessing operations over hyperspectral data. Each algorithm has its
own advantages, disadvantages, and limitations to perform the designated processing
operations. Hence, in order to exploit the advantages for overcoming the limitations
and disadvantages, a combination of algorithms (i.e., hybridization) is generally fol-
lowed. Hybridization of algorithms in hyperspectral imaging improves image analysis,
accuracy, and understanding, leading to better interpretation. The objective of this
chapter is to present understanding of hyperspectral imaging, processing algorithms,
and discussion of hybrid approaches under spectral unmixing, classiÔ¨Åcation, and target
identiÔ¨Åcation sections.
This chapter is organized in the following way. In Sections 10.2 and 10.3, we discuss
background, a brief hardware description, and steps of image processing for hyperspec-
tral imaging. In Section 10.4, ‚ÄúSpectral Unmixing,‚Äù we review the unmixing process
chain, mixing model, and methods used in unmixing. In Section 10.5, supervised
classiÔ¨Åcation methods are discussed in hyperspectral image classiÔ¨Åcation. In Section
10.6, some methods under target identiÔ¨Åcation are discussed. Hybrid techniques are
discussed under spectral unmixing, classiÔ¨Åcation, and target identiÔ¨Åcation sections.
Finally, a conclusion is presented in Section 10.7.
10.2
Background and Hyperspectral Imaging System
The term spectroscopy, Ô¨Årst used in the late 19th century, provided the empirical foun-
dation for atomic and molecular physics. Subsequently, astronomers began to use it for
Ô¨Ånding radial velocities of cluster, stars, and galaxies and stellar compositions. Advance-
ment in technology of spectroscopy since the 1960s and the potentiality of the same led
to the development of initial research-level imaging spectrometer [9]. Subsequently,

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
235
signiÔ¨Åcant improvement has been achieved in the development of airborne imaging
spectrometry. The limitations of airborne spectrometry led to the development of
spaceborne imaging spectrometry [10]. Usability of hyperspectral imaging is not
limited to earth observations only but is growing in Ximportance in industrial and
research-oriented usages also. It has been used in numerous applications such as non-
destructive food inspection, mineral mapping and identiÔ¨Åcation, coastal ocean studies,
solid and gaseous target detection, aviation fuel inspection, and biomedical applications.
Generally, a hyperspectral imaging system (HIS) contains illumination source
(light source), dispersive unit, charge-coupled device (CCD)/ complementary metal
oxide semiconductor (CMOS) cameras, and image acquisition system, as shown in
Figure 10.2. A brief description of the HIS components is provided here.
Light sources illuminate the material or sample under study. The tungsten halogen
lamp is most commonly used in HIS; it consists of a halogen element (F / Cl / Br / I)
in the quartz tubes. It has higher luminous eÔ¨Éciency and a longer lifetime compared to
normal light bulbs.
The wavelength dispersive unit plays a vital role in the HIS. It disperses the broad-band
wavelengths into narrow-band wavelengths. Normally, any one of the components [i.e.,
Ô¨Ålter wheels, tunable Ô¨Ålter, linear variable Ô¨Ålter (LVF), grating, or prism] is used to
achieve spectral dispersion. In general, Ô¨Ålter wheels carries a set of discrete band pass
Ô¨Ålters. A band pass Ô¨Ålter allows light of a particular wavelength while rejecting other
To PC
Detector +
Wavelength
Filter
Fore-optics
Sample
Hyperspectral
Camera
Light Source
Figure 10.2 Diagrammatic representation of the schematics for a commonly used hyperspectral
imaging system.

236
Hybrid Intelligence for Image Analysis and Understanding
wavelengths of light. Available Ô¨Ålter wheels operate over a wide range, starting from the
ultraviolet (UV) to infrared (IR) range.
A tunable Ô¨Ålter scans the area under investigation electronically, wavelength by wave-
length, as per the full-width half maximum (FWHM) criterion. Liquid crystal tunable
Ô¨Ålter (LCTF) and acousto-optic tunable Ô¨Ålter (AOTF) are the most widely used elec-
tronically controlled tunable Ô¨Ålters. The LCTF-based HIS are relatively slower but have
better imaging performance than the systems using AOTFs. LCTFs have relatively wider
Ô¨Åeld of view (WFOV), larger apertures, and low wavefront distortions compared to the
AOTF-based ones.
LVF is also known as a wedge Ô¨Ålter due to its wedge-shaped geometry at the top-view
end. Coating of the Ô¨Ålter is done in such a manner that it passes narrow bands from one
end to another that vary linearly along the length of LVF.
Prism and grating are the most commonly used components for the dispersion in HIS.
An image sensor placed within a camera is a 2D focal plane array (FPA) that collects
the spatial and spectral information simultaneously. CCD and CMOS are the two most
frequently used image sensors in cameras. Basic components of CCD and CMOS are
photodiodes, made up of light-sensitive materials to convert light energy into electri-
cal energy. The main diÔ¨Äerence between the CCD and CMOS image sensors is that the
photo detector and readout ampliÔ¨Åer are included for each pixel in a CMOS sensor,
while this is absent in the case of a CCD sensor. Silicon (Si), mercury cadmium tel-
lurium (HgCdTe or MCT), and indium gallium arsenide (InGaAs) are the normally used
materials in detector arrays for hyperspectral imaging to work in diÔ¨Äerent wavelength
ranges. The spectral response of the silicon is in visible-near-infrared (VNIR) range,
while InGaAs and MCT operate in the short-wave infrared (SWIR) range and long-wave
infrared (LWIR) range. CCD and CMOS have their own associated advantages and dis-
advantages. Advantages of CCD are low cost, high pixel count and sensitivity, wide
spectrum response, and high integration capability. There have been four diÔ¨Äerent archi-
tectures of CCD that are available, and they are interline CCD, frame interline CCD,
frame transfer CCD, and full frame CCD to improve the performance and usability of
the image sensor. On the other hand, advantages of CMOS are low power consumption,
on-chip functionality, and low operating voltage. An image acquisition system (frame
grabber) is an electronic device to trigger the camera and acquire the digital frame suc-
cessively.
10.3
Overview of Hyperspectral Image Processing
Hyperspectral image processing refers to the combined execution, of complex algo-
rithms for storage, extraction, and manipulation of the hypercube data to perform tasks
such as classiÔ¨Åcation, analysis, target identiÔ¨Åcation, and parameter retrieval. [11‚Äì13].
Hyperspectral image-processing work Ô¨Çow is quite diÔ¨Äerent from that of color image
processing; it includes data calibration, atmospheric correction, pixel purity with end-
member selection, dimension reduction, and data processing. To compare, color image
processing involves steps like preprocessing (linearization, dark current compensation,
etc.), white balance, demosaicking, color transformation, postprocessing, and display.
Color image postprocessing steps involve image enhancement, which leads to loss of

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
237
Image Acquisition
Calibration
Spectral/Spatial Preprocessing
Dimension Reduction
Postprocessing 
(Spectral 
Unmixing,
Classification, Target Detection, etc.)
Figure 10.3 General processing steps involved for hyperspectral data.
spectral information in hyperspectral imaging. General processing steps involved for
hyperspectral data have been presented in Figure 10.3.
10.3.1
Image Acquisition
Hyperspectral data is 3D data where two dimensions are spatial and the third one is
spectral. The detector used in a hyperspectral imager is suitable to capture data in a
2D framework. So, in order to acquire the third dimension, either a scanning mecha-
nism or tunable Ô¨Ålter is used. In case of the use of a scanning mechanism, the dispersive
components like prism, grating, and wedge Ô¨Ålter are normally used. Generally, AOTF
[14] and LCTF [15] are the most commonly available tunable Ô¨Ålters that have been used
in real-life imagers. Three types of data formats are obtained during acquisition: BSQ
(band sequential), BIL (band interleaved by line), and BIP (band interleaved by pixel).
In BSQ, each image belongs to one narrow-wavelength band. The BSQ format remains
suitable for spatial processing in order to create regions of interest (ROIs) or to extract
spatial features. In the BIL format, the Ô¨Årst row of all spectral bands is gathered in the
order of their wavelengths, and followed by the second row, the third row, and so on.
This is suitable for both spectral and spatial processing operations. In BIP, the Ô¨Årst pix-
els of all spectral bands are stored as per the respective wavelength bands, and followed
by the second pixels, the third pixels, and so on. This is suitable for spectral process-
ing. The choice of the optimal data format is one of the most desirable features for the
processing of hyperspectral images. However, the BIL format is a good compromise for
carrying out better processing of images.
10.3.2
Calibration
Raw data that is obtained as the output of the acquisition setup is related to the radiance
as a function of spatial position and spectral band at the sensor‚Äìpupil plane. Radiance
depends on the properties of the object of interest and is aÔ¨Äected by factors like dark cur-
rent, responsivity at the FPA, optical aberrations and transmission, and so on. Extracting

238
Hybrid Intelligence for Image Analysis and Understanding
quantitative information from the raw data requires a method to determine the system
response along three important dimensions, namely, spectral, spatial, and radiometric
[16]. This method is referred to as hyperspectral sensor calibration. Calibration of the
data is required to ascertain the accurateness and repeatability of the results generated
by the hyperspectral imaging system. In this section, spectral, spatial, and radiometric
calibrations are discussed.
In
spectral
calibration,
band
numbers
are
linked
with
the
corresponding
wavelength-related data [16]. Generally, monochromatic laser sources and pencil-type
calibration lamps are used to calibrate the wavelength. Linear and nonlinear regression
analysis is used to guess the wavelength at unknown bands. Spectral calibration of the
instrument has to be done at the manufacturing end before commercializing in the
market for the customers to use.
Spatial calibration is also known as geometric correction, and its function is to assign
every image pixel to a known unit such as meters or known features in the object space
[16]. Spatial calibration gives information about the spatial dimensions of every sen-
sor pixel on the surface of the object. It reduces the optical aberrations, like smile and
keystone eÔ¨Äects, which distort the imaging geometry and thereby introduce the spatial
error components. Due to advancements, in recent years, such sensors are available that
minimize the smile and keystone well below the tolerance limits.
A digital imaging sensor produces images with inherent artifacts called camera
shading, which arises due to nonuniform sensitivity at the sensor plane with respect
to the properties of received signals obtained from the whole of the object space.
Camera-shading corrections apply to the image in case of variations in sensitivity of the
imaging sensor. In radiometric calibration, one has to recalculate the digital numbers in
an image based on several factors such as dark current, exposure time, camera shading
correction, and so on to obtain obtaining the radiometrically corrected image. Standard
radiometric calibration methods are the lamp-integrating sphere, detector-integrating
sphere, detector-diÔ¨Äuser [17], and so on.
10.3.3
Spatial and Spectral Preprocessing
Spatial preprocessing is to manipulate or enhance the image information in the spatial
dimension. Normal digital image-processing techniques like enhancement and Ô¨Åltering
can be applied here. In the case of hyperspectral images, spatial preprocessing tech-
niques for denoising and sharpening are not applied to raw and calibrated hyperspectral
images, generally because they aÔ¨Äect the spectra of the data [16].
Roughly, spectral preprocessing can be grouped into two parts: (1) endmember extrac-
tion and (2) chemometric analysis. Endmember refers to the pure spectra and has impor-
tance in remote-sensing applications. It can be acquired in Ô¨Åeld or on the ground and
in laboratories by spectro-radiometry in order to build a spectral library of diÔ¨Äerent
materials. Endmembers can be extracted from the hyperspectral images by spectral pre-
processing algorithms like the Pixel Purity Index (PPI), N-Ô¨Ånder (N-FINDR) [11], and so
on. Obtained endmembers can be used in target identiÔ¨Åcation, spectral unmixing, and
classiÔ¨Åcation. A chemometrics approach refers to the extraction of information from
a system by data-driven means. Spectral data obtained from chemometrics is used for
multivariate analysis techniques like partial least squares (PLS), principal component
analysis (PCA), and so on.

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
239
10.3.4
Dimension Reduction
Hyperspectral data contains a signiÔ¨Åcant amount of redundancy. In order to process
the data eÔ¨Éciently, one needs to reduce or compress the dimension of the hyper-
cube data. The objective of dimension reduction is to map high-dimensional data
into low-dimensional space without distorting the major features of the original
data. Dimensionality reduction techniques can be either transformation-based or
selection-based approaches; the diÔ¨Äerence between these two techniques is whether
they transform or preserve the original data sets during the dimensional reduction
process.
10.3.4.1
Transformation-Based Approaches
Transformation-based approaches are commonly used where originality of the data
sets is not needed for further processing, but these preserve the important features
under consideration. Under these approaches, many methods like PCA [18], discrete
wavelet transform (DWT) [19], Fisher‚Äôs linear discriminant analysis (FLDA) [20],
minimum noise fraction, and others have been used for dimensional reduction. Some
of the methods are discussed in brief here:
Principal component analysis: In hyperspectral data, adjacent bands are correlated and
approximately contain the same information as the sample under study. PCA reduces
the correlation among the adjacent bands and converts the original data into lower
dimensional space. PCA analysis is based on the statistics to determine the correlation
among adjacent bands of hyperspectral data.
Discrete wavelet transform: Wavelet transform is one of the powerful tools in signal pro-
cessing and has been used in remote-sensing applications like feature extraction, data
compression and fusion, and so on. It is applied to single pixel locations in the spectral
domain while transforming the data into lower dimensional space of the object under
study. During transformation, it preserves the characteristics of the spectral signature.
Fisher‚Äôs linear discriminant analysis: In pattern recognition, FLDA is used for dimension
reduction, which projects original data into low-dimensional space. It is a parametric
feature extraction method and well suitable for normally distributed data.
10.3.4.2
Selection-Based Approaches
Selection-based approaches attempt to Ô¨Ånd a minimal subset of the original data set
without missing their physical meaning. This approach is based on the band selection
method to reduce the dimension of the data. In band selection, selecting the subset
bands contains class information from the original bands. Categorization of band selec-
tion approach based on class information is divided into two types: 1) Supervised band
selection methods and 2) Unsupervised band selection methods.
When class information is available, supervised band selection is the preferred one
for dimension reduction of data. It involves two steps: subset generation and subset
evaluation. In case of non-availability of class information, unsupervised band selec-
tion is used. There are a number of methods available for unsupervised band selection,
like methods based on information theory, Constrained Band Selection (CBS) [21], and
so on.

240
Hybrid Intelligence for Image Analysis and Understanding
10.3.5
Postprocessing
The hyperspectral imagery consists of a lot of information. Interpreting the information
requires pre- and postprocessing, analysis, and accordingly making conclusions.
Spectral unmixing, classiÔ¨Åcation, target identiÔ¨Åcation, and so on. come under the
category of hyperspectral postprocessing. In spectral unmixing, this involves unmixing
of mixed pixels into sets of endmembers and their abundance fractions. Mixing may
be linear or nonlinear. ClassiÔ¨Åcation assigns each pixel of hypercube to a particular
class depending on its spatial and spectral information; while, in target identiÔ¨Åcation,
a hyperspectral sensor measures the reÔ¨Çected/emitted light from the object under
observation in the working wavelength range. Processing of these obtained data using
algorithms/techniques assists in identiÔ¨Ång the target of interest. The theme of this
chapter is to discuss spectral unmixing, classiÔ¨Åcation, and target identiÔ¨Åcation in
postprocessing, which will be discussed further in this chapter.
10.4
Spectral Unmixing
Hyperspectral imaging systems record the scenes consisting of numerous diÔ¨Äerent
materials that contribute to the formation of spectra at a given pixel (single pixel). This
is generally referred to as a mixed pixel in a hypercube. In such a mixed pixel, one
needs to identify the individual pixel spectra and their proportions present. Spectral
unmixing is the process by which identiÔ¨Åcation of individual pure pixel spectras or pure
spectral signatures, called endmembers, and their abundance values (i.e., proportion
present in the mixed pixel) are characterized and analyzed. Generally, endmembers
represent the pure pixel spectrum present in the image, and percentages of endmember
at each pixel represent the abundance of materials. Mixed pixels contain a mixture of
two or more diÔ¨Äerent materials and in general occur due to two main reasons. The
Ô¨Årst reason is that spatial resolution of the hyperspectral sensing devices is quite low,
leading to a condition where adjacent endmembers of the scene jointly occupy the
projected location of a single pixel, target, or area of interest that is smaller compared
to the pixel size. Generally, it occurs in the spaceborne and airborne platforms. The
second reason is because two diÔ¨Äerent materials are mixed in such a way as to generate
a homogeneous‚Äìheterogenous material composition (e.g., moist soil, mineral, etc.).
In a broad way, spectral unmixing is a special case of an inverse problem in which
estimating constituents and abundances at each pixel location is carried out by using
one or more observations of the received mixed signals.
10.4.1
Unmixing Processing Chain
Steps involved in the processing chain of hyperspectral unmixing have been shown in
Figure 10.4. These steps are atmospheric correction, dimension reduction, and unmix-
ing (endmembers and inversion). Radiant light at the sensor coming from the scene
attenuates and gets scattered by the atmospheric particulate materials. Atmospheric
correction compensates these eÔ¨Äects by transforming radiance data into reÔ¨Çectance
data. To improve the performance of the algorithms for analyzing, one needs to exe-
cute the dimension reduction step. This step is optional and is used by some algorithms

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
241
Atmospheric
Correction
Radiance
Data
Reduced
Data
End-Member
Determination (+) Inversion
Abundance
Map
End-
Members
Unmixing
Dimension
Reduction
Reflectance
Data
Figure 10.4 Hyperspectral unmixing processing chain.
in order to reduce complexity problems as well as the execution time of the same. In the
endmember determination step, estimation of the endmembers present in the mixed
pixel of the scene is carried out. At last, in the inversion step, abundance maps are gen-
erated in order to Ô¨Ånd the partial fractions of the components present in each mixed
pixel by using the observed and determined endmembers spectra.
10.4.2
Mixing Model
Implementation of any approach for hyperspectral unmixing depends upon the model
chosen, which describes how individual materials combine in a pixel-producing mixed
spectrum at the sensor end. Mixing models try to describe the fundamental physics that
are the basis of hyperspectral imaging. On the other hand, unmixing uses these models
for endmembers determination and inverse operation in order to Ô¨Ånd the endmembers
and their fractional abundance present in each of the mixed pixels. Mixing models have
been categorized into two parts, linear mixing models and nonlinear mixing models,
which are described in brief in this section.

242
Hybrid Intelligence for Image Analysis and Understanding
10.4.2.1
Linear Mixing Model (LMM)
In LMMs, the mixing scale is macroscopic and incident light supposedly interacts with
just one substance. The reÔ¨Çected light beams from all the diÔ¨Äerent materials, which are
completely and exclusively separated linearly, get mixed when measured at the sensor
end. In this way, a linear relationship is assumed to exist between the fractional abun-
dance of the materials comprising the area and spectra of the reÔ¨Çected radiation. In
unmixing, LMMs have been used broadly in the past few decades due to their simplicity
and ability to provide approximate solutions. A very simple conceptual model of appli-
cation of LMMs for three diÔ¨Äerent materials is illustrated in Figure 10.5.
If L indicates the number of spectral bands, si represents the endmember spectra of the
ith endmember, and ùõºi represents the ith endmember abundance value, then the observed
spectra y of any pixel in the scene can be represented as Equation (10.1):
y = ùõº1s1 + ùõº2s2 ‚Ä¶ ùõºMsM + ùë§
(10.1)
=
M
‚àë
i=1
ùõºisi + ùë§= Sùõº+ ùë§
(10.2)
where M refers to the number of endmembers; S refers to the matrix of endmembers;
and ùë§accounts for error terms for additive noise and other model inadequacies. The
component ùõºrepresents the fractional abundance, which then satisfy the constraints
ùõºi ‚â•0, for i = 1, ‚Ä¶ , M and are named the abundance non-negativity constraint (ANC)
and abundance sum constraint (ASC), respectively.
10.4.2.2
Nonlinear mixing model
Normally, formation of an intimate mixture of materials is obtained when all the
components of the material are uniformly mixed in a random manner that leads to the
generation of mixed ground representative cells producing mixed hyperspectral image
pixels. Due to this, incoming radiations falling on the sensor experience reÔ¨Çections
from multiple materials (i.e., multiple locations), and the resultant spectra of reÔ¨Çected
radiation may no longer hold the linear proportion of constituent material spectra. The
nonlinearity in the aggregate spectra makes LMMs not suitable for use to analyze this
condition, and hence a diÔ¨Äerent approach called the nonlinear mixing model is used for
y
s3
s2
s1
Œ±3
Œ±2
Œ±1
Figure 10.5 Conceptual diagram of a simple linear mixture model geometry.

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
243
Figure 10.6 Nonlinear mixture
model.
analyzing the mixed spectra. One simple representation of a nonlinear mixing model is
illustrated in Figure 10.6.
10.4.3
Geometrical-Based Approaches to Linear Spectral Unmixing
The underlying assumption of geometrical-based approaches under LMMs is that the
spectral vectors come under a simplex, whose vertices correspond to pure reference
materials (endmembers). Geometrical-based approaches discuss algorithms for the
presence or absence of pure pixels in the data set. In the case of the pure pixels,
algorithms are much easier than with the nonpure pixels case [22]. In general, the
geometrical-based techniques are divided into two broad parts: pure pixel-based and
minimum volume (MV) based on [23]. Brief discussions on pixel-based and MV-based
techniques are given in this section.
10.4.3.1
Pure Pixel-Based Techniques
These algorithms presume the presence of at least one pure pixel per endmember com-
ponent. This assumption enables the design of eÔ¨Äective algorithms from the computa-
tional view and is a prerequisite that may not hold in many cases. In any case, these
techniques Ô¨Ånd the set of the most pure pixels in the hypercube. In hyperspectral linear
unmixing applications, they have presumably been the most frequently used because of
their clear conceptual meaning and easygoing computational complexity. Some of the
techniques are discussed here:
Pixel purity index (PPI): In the PPI [24], data are subjected to dimensional reduction and
improvement of the signal-to-noise ratio (SNR, or noise whitening) using the minimum
noise fraction (MNF) transform process. The PPI projects data repeatedly onto a num-
ber of random vectors. The points corresponding to the extreme values in each of the
projections are recorded. A cumulative account records the number of times every pixel
(i.e., a given spectral vector) is found to be an element of the extremes. The points with
the highest extreme score are the purest ones.
N-FINDR: N-FINDR [25] is an autonomous algorithm to Ô¨Ånd the pure pixels in the image.
It works by ‚ÄúinÔ¨Çating‚Äù a simplex inside the data and starts with a random set of image

244
Hybrid Intelligence for Image Analysis and Understanding
pixels. For every pixel and every endmember, the endmember is replaced with the spec-
tra of the pixel, followed by recalculation of the volume. If it increases, the spectra of the
new pixel replace that endmember. This process is repeated until no more replacements
are possible.
Vertex component analysis (VCA): VCA [26] is an unsupervised endmember extraction
method to unmix any given linear mixture. This method exploits two basic facts: (1)
the endmembers belong to the set of vertices of simplex, and (2) the aÔ¨Éne transfor-
mation of simplex is also simplex. VCA works with projected and unprojected data.
It iteratively projects data onto the orthogonal direction of the subspace spanned by
the endmembers already identiÔ¨Åed and determined. The new endmember signature(s)
corresponds to the extreme(s) of the projection. The algorithm keeps iterating until all
the endmembers are exhausted. VCA performs much better than PPI and better than
or comparable to N-FINDR.
Iterative error analysis (IEA): IEA is a linear-constrained endmember extraction algo-
rithm based on LMM. This algorithm selects endmembers one by one based on the
previously extracted endmembers. In an unmixed image, pixels that minimize the
remaining errors are selected as endmembers in every iteration. This technique look
for endmembers that reduce the remaining error in an abundance map by involving a
series of linear-constrained spectral-unmixing steps.
Simplex growing algorithm (SGA): SGA [27] Ô¨Ånds the vertices corresponding to the max-
imum volume by growing simplex, iteratively. Initially, it starts with two vertices and
begins to grow simplex by increasing its vertices one at a time. This algorithm termi-
nates when the number of vertices reaches the number of endmembers, estimated by
virtual dimensionality (VD; i.e., the number of spectrally distinguishable signatures in
hyperspectral imagery).
10.4.3.2
Minimum Volume-Based Techniques
Minimum volume-based techniques are used in the case of non-availability of pure
pixels in the object under study. In other words, it violates the assumption of pure pix-
els. Some of the minimum volume-based techniques and their extensions are mini-
mum volume simplex analysis (MVSA) [28], simplex identiÔ¨Åcation via split augmented
lagrangian (SISAL) [30], minimum volume enclosing simplex (MVES) [29], and robust
minimum volume enclosing simplex (RMVES) [31], among others.
10.4.4
Statistics-Based Approaches
Geometrical-based approaches yield poor results in the case of huge mixing of
spectral mixtures. In these cases, statistical methods are eÔ¨Écient as compared to
geometrical-based approaches. In statistical methods, unmixing algorithms process
the mixed pixel using statistical representations. The illustrations can be analytical
expressions that denote the probability density function (parametric). It comes with the
price of higher complexity and computation.
Normally, the number of materials and their reÔ¨Çectance values (in terms of both the
magnitude and the proportional composition) remain unknown. In such cases, hyper-
spectral unmixing comes into the section of the blind source separation problem. The

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
245
Independent Component Analysis (ICA) [32] is a well-known technique for solving
blindly unmixing problems. It assumes that the spectral vectors of diÔ¨Äerent substances
are non-Gaussian in nature and are independent of each other. Due to this assumption,
ICA uses all the higher order statistics-based parameters for the mixed signal rather
than Ô¨Årst-order (mean, etc.) and second-order statistics (covariance, etc.). Algorithms
like PCA and the like use only second-order statistics. ICA is able to provide correct
unmixing in the case of independent sources, since the minimum of the mutual infor-
mation corresponds only to the independent sources. In case of dependence fractional
abundances, ICA is not able to provide the correct unmixing. To overcome the limita-
tions imposed by the ICA, a new unmixing method has been proposed named depen-
dent component analysis (DECA) [33]. This technique models the abundance fractions
as mixtures of Dirichlet densities, thus enforcing the constraints on abundance frac-
tions imposed by the acquisition process, namely, non-negativity and constant sum.
The mixing matrix is inferred by a generalized expectation-maximization (GEM)-type
algorithm.
Finally, some methods have been used for spatial and spectral information that
combined the order to improve the endmembers extraction. Automated morphological
endmember extraction (AMEE) [34] is a mathematical morphology-based unsuper-
vised pixel purity determination and endmember extraction algorithm. It uses both the
spectral and spatial information in a combined manner. Spatial-spectral endmember
extraction (SSEE) [35] is a projection-based unmixing algorithm to analyze the scene
into subsets. As a result, the spectral contrasts of low-contrast endmembers increase
while improving the potential for these extracted endmembers to be selected for
discrimination.
10.4.5
Sparse Regression-Based Approach
A semi-supervised approach, the sparse regression-based approach, is also used to Ô¨Ånd
the endmembers in a spectral library (S) containing many spectra (p) in which only a
few of them remain present in the pixel and p << L (L refers to the number of spectral
bands). This means that the vector of fractional abundances is sparse in nature. From
Section 10.4.2.1, the equation for LMM can be rewritten as given in Equation (10.3) as:
y = Sf + ùë§
(10.3)
where f represents fractional abundances (L ‚àíby ‚àí1 vector) of the members contained
in S. In unmixing, the mixing matrix (M) is to Ô¨Ånd the vector of fractional abundances f ,
given y and S. As the number of actual endmembers (q) is much lesser than the number
of spectra (p) contained in S, the vector of fractional abundances f is sparse.
10.4.5.1
Moore‚ÄìPenrose Pseudoinverse (MPP)
S is not a square matrix (i.e., not normally invertible); it is not possible to determine the
ÃÇf of f by multiplying the inverse of S with y:ÃÇf = S‚àí1R. As per S, the product of STS is
square and invertible. An estimate of f can be found using Equation (10.4):
ÃÇf = (STS)‚àí1STR = S‚ôØR
(10.4)
where (S‚ôØ= (STS)‚àí1ST) is the moore‚Äìpenrose pseudoinverse code of the matrix S; and
ÃÇf = f only in case of no error term (w). In case of a bad conditioned case of S, the error
term becomes more important.

246
Hybrid Intelligence for Image Analysis and Understanding
10.4.5.2
Orthogonal Matching Pursuit (OMP)
OMP is an alternative to matching pursuit. It is an iterative technique that Ô¨Ånds the
spectral signatures from the spectral library that best explain a predetermined resid-
ual at each iteration. At the Ô¨Årst iteration, the initial residual is equal to the observed
spectrum of the pixel, the vector of fractional abundances is null, and the matrix of the
indices of selected endmembers is empty. At each iteration, OMP searches the mem-
ber from the spectral library that best explains the actual residual, adds this member to
the endmembers matrix, updates the residual, and calculates the estimate of fractional
abundance using the selected endmembers.
10.4.5.3
Iterative Spectral Mixture Analysis (ISMA)
ISMA is an iterative technique extracted from the spectral mixture analysis. It searches
the optimal endmember set by investigating the variation in the root mean squared
(RMS) error along the iterations. This algorithm branches into two parts. Initially, it
calculates an unconstrained solution of the unmixing problem using all the available
spectra in the spectral library; then, it removes the spectra with the lowest estimated
fractional abundance and repeats the process with the remaining endmembers, until
one endmember remains. The second part of ISMA consists in Ô¨Ånding the critical iter-
ation, which corresponds to the iteration of the Ô¨Årst abrupt variation in the RMS error,
calculated as given in Equation (10.5):
‚ñµRMS = 1 ‚àí
RMSj‚àí1
RMSj
(10.5)
where RMSj corresponds to the jth iteration RMS error. The critical iteration corre-
sponds to the optimal set of endmembers. The notion of retrieving the true endmember
set by analyzing the change in the RMSE is based on the fact that before extracting the
optimal set of endmembers, the RMSE changes within a certain (small) range and limits,
and it has bigger changes when one endmember from the optimal set is removed, as the
remaining endmembers are not suÔ¨Écient to generate a model with good accuracy with
respect to the actual observations. ISMA calculates, at every iteration, an unconstrained
solution in place of a constrained one, as it is expectable that when the endmember set
approaches the optimal one, the abundance fractions will approach the true ones.
10.4.6
Hybrid Techniques
Spectral unmixing is one of the mandate tasks of spectra-based imaging. Unmixing
determines the pure pixels and their abundance value. Mixing may be linear or nonlin-
ear based. Numerous algorithms or approaches are available to solve spectral unmixing.
However, hybrid approaches have been researched in order to solve spectral unmixing
with better accuracy in the presence or absence of endmembers, and to solve cases like
unsupervised or semi-supervised unmixing or the like. Some of the hybrid techniques
as proposed in spectral unmixing are explained in brief here.
In [36], a hierarchical Bayesian model is proposed for a semi-supervised hyperspec-
tral linear spectral-unmixing process. An assumption of this model is that the pixel
reÔ¨Çectances contain linear mixing of pure spectra along with an additive Gaussian noise.
Similarly, for endmember extraction and linear unmixing for hyperspectral data, a joint
Bayesian [37] model is proposed for betterment of unmixing results.

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
247
Generally, endmember extraction algorithms are applied for obtaining the presence
of materials that are either known or to be determined. However, [38] a proposed tech-
nique called genetic orthogonal projection (GOP) for endmember extraction is a fully
unsupervised approach. This proposed technique is a hybridization of orthogonal pro-
jection and genetic algorithms for endmember extraction.
Conventional hyperspectral image processing performs denoising and spectral
unmixing separately. However, in [39], a sparse representation framework is proposed
to improve both the denoising and spectral unmixing components iteratively in a
feedback manner. In a proposed hybrid-combined representation framework, sparse
coding for denoising and sparsity for spectral unmixing are used in an iterative manner
for improving the performance of both denoising and unmixing results.
In [40], a neural network-based hybrid mixture model is proposed to obtain the infor-
mation from nonlinearly mixed pixels. The proposed model is composed of three stages
or processes, namely, (1) extracting endmembers using the N-FINDR algorithm, (2)
using endmembers by LMM for abundance estimation, and (3) feeding training samples
with abundance value to neural network-based multilayer perceptron (MLP) architec-
ture as an input, to get the reÔ¨Åned output in case of the nonlinear mixture class as well.
Here, the hybridization using the MLP helps in better segregation of nonlinear unmixing
approaches.
10.5
ClassiÔ¨Åcation
Hyperspectral image classiÔ¨Åcation is one of the dominant research areas in recent times,
in classiÔ¨Åcation for a given data cube to assign a unique class for given pixel vectors. High
spatial resolution is quite important to assume the data content of pure pixels (single
spectral signature). In contrast, for the case of mixed pixels, spectral-unmixing methods
have been used for analysis. In this part, we discuss some of the key methods of a super-
vised classiÔ¨Åcation approach. Supervised classiÔ¨Åcation has been mostly used in [41], but
it has certain limitations like high dimensionality of data and limited training samples
[42], among others. To solve these problems, feature mining [23], semi-supervised learn-
ing, and sub space-based approaches [43] have also been developed and used. Feature
mining and supervised learning are discussed brieÔ¨Çy in this section.
10.5.1
Feature Mining
Hyperspectral data contains rich amounts of information for a variety of applications
in diÔ¨Äerent Ô¨Åelds of study. It provides opportunity for a wide range of applications, not
focused to solve a given single problem. Each band may or may not reveal the unique
absorption feature(s) for the object under study in a given problem. Thus, a given band
may be able to solve one problem but not the other suÔ¨Éciently. Therefore, the original
hyperspectral bands are essentially the candidate features for a particular application.
Identifying eÔ¨Écient features is one of the critical preprocessing steps for image classiÔ¨Å-
cation. Generally, it is not a good procedure to use the entire spectral band‚Äôs information
to solve a particular problem. As a result, it increases the computational load and the
algorithms face signiÔ¨Åcant challenges. Feature mining is one of the important tasks
included in the feature selection (FS) and feature extraction (FE) stages. Finding eÔ¨Äective

248
Hybrid Intelligence for Image Analysis and Understanding
subspace that contains a minimum number of attributes is needed to describe selected
properties of given data for hyperspectral image analysis. Feature mining is discussed in
two sections here, feature selection and feature extraction.
10.5.1.1
Feature Selection (FS)
Suppose X indicates the original hyperspectral data, that is, X = [xi], where xi ‚àà‚ÑúL; i =
1, 2, ‚Ä¶ , d; and d and L represent the number of pixels and spectral bands. The objective
of the FS is to Ô¨Ånd the good subset p from the total features L, p < L. There are two main
types of approaches for FS: the Ô¨Ålter approach and wrapper approach. For FS, the Ô¨Ålter
approach is classiÔ¨Åer independent, while the wrapper approach is classiÔ¨Åer dependent.
Here, Ô¨Ålter-based approaches (supervised and unsupervised) will be discussed.
The parametric supervised FS involves class modeling of the data using a training
set. JeÔ¨Äries-Matusita (JM) distance has been broadly used to measure the separability
of two class density functions and performs fairly when the Gaussian distribution
presumption is applicable. Other distance measures such as Euclidean distance,
spectral angle, Mahalanobis distance, and so on. have also been used; in the case of
nonparametric supervised FS, the use of training data directly without modeling class
data is reported. Information theory can also be employed to execute nonparametric
supervised FS. Mutual information (MI) provides a degree of linear and nonlinear
dependency between two variables and can be used in both FS and FE. In case of
unsupervised FS, without considering any speciÔ¨Åc applications, the basis for the
selection criteria of FS is SNR. MI is used to Ô¨Ånd the subset bands that have minimum
dependency [44].
10.5.1.2
Feature Extraction
Numerous approaches have been proposed in hyperspectral imaging to execute feature
extraction prior to data classiÔ¨Åcation. Feature extraction exploits the entire available
spectral band in order to extract relevant features. The most available approach for the
generation of features is based on extractions obtained from PCA and MNF algorithms.
In these approaches, data is projected onto new space in which the Ô¨Årst few components
contain most of the complete information of the data, and therefore only a few fea-
tures are retained. Segmented PCA is better than conventional PCA because it reduces
the computational load. DWT has been used for generation of feature space that sep-
arates high- and low-frequency components. This allows a form of derivative analysis
that has been also used to generate features prior to hyperspectral image classiÔ¨Åcation
[45]. Other broadly used techniques are decision boundary feature extraction (DBFE)
and nonparametric weighted feature extraction (NWFE) [42]. Another strategy for fea-
ture extraction has been based on the grouping of neighboring bands, using methods
such as the weighted sum or average of each group [46]; this approach also performs
equally well for most data sets.
10.5.2
Supervised ClassiÔ¨Åcation
In supervised classiÔ¨Åcation, the target of interest in the data is identiÔ¨Åed using avail-
able training samples. In hyperspectral data, each voxel has its own spectra compared
with the available training samples, that is, a spectral signature, which is used for the
classiÔ¨Åcation of images accordingly.

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
249
10.5.2.1
Minimum Distance ClassiÔ¨Åer
This classiÔ¨Åer is used to classify unknown image data into classes that minimize the
distance between the image data and the class in case of multifeature space. In this,
a minimum distance indicates the index of similarity. First-order and second-order
statistics-based classiÔ¨Åers are described brieÔ¨Çy here.
First-order statistics classiÔ¨Åers: First-order statistics classiÔ¨Åers provide local statistical
information (e.g., mean), which is given here:
Euclidean distance (ED): ED is one of the most broadly used features to measure the
distance between spectral signatures. Suppose that two spectral signature vectors
are represented as Sj = (Sj1, Sj2, ‚Ä¶ , SjL)T and Sk = (Sk1, Sk2, ‚Ä¶ , SkL)T, and let L be
the number of spectral bands; then, the ED is given by Equation (10.6):
ED(Sj, Sk) = ||Sj ‚àíSk|| =
‚àö
‚àö
‚àö
‚àö
L
‚àë
l=1
(Sjl ‚àíSkl)2
(10.6)
City block distance (CBD): CBD is given in Equation (10.7):
CBD(Sj, Sk) =
L
‚àë
l=1
|Sjl ‚àíSkl|
(10.7)
where Sj = (Sj1, Sj2, ‚Ä¶ , SjL)T and Sk = (Sk1, Sk2, ‚Ä¶ , SkL)T are the two spectral sig-
nature vectors; and L is the number of spectral bands.
Tchebyshev (maximum) distance (TD): TD is represented and computed as provided
in Equation (10.8):
TD(Sj, Sk) = max
1‚â§l‚â§L |Sjl ‚àíSkl|
(10.8)
where Sj = (Sj1, Sj2, ‚Ä¶ , SjL)T and Sk = (Sk1, Sk2, ‚Ä¶ , SkL)T are two spectral signature
vectors; and L is the number of spectral bands.
Second-orderstatisticsclassiÔ¨Åers: Covariance-based classiÔ¨Åers are known as second-order
statistics classiÔ¨Åers, which are given here:
Mahalanobis distance (MD) classiÔ¨Åer: MD has been used for diÔ¨Äerent purposes, such
as observing the diÔ¨Äerence between two data sets and detecting outliers. It cal-
culates the distance using variance‚Äìcovariance values of two variables, which are
given in Equation (10.9):
MD =
‚àö
(xi ‚àíx)C‚àí1
x (xi ‚àíx)T
(10.9)
where Cx is the variance‚Äìcovariance matrix of two variables, x1 and x2. The ellipse,
formed based on the distance values, and it represents the equal MDs toward the
center point of data. Prior knowledge of threshold is needed in order to distinguish
the two classes.
Bhattacharyya distance (BD) classiÔ¨Åer: The BD is given in Equation (10.10)
BDi,j = 1
8(Si ‚àíSj)T
(‚àë
i + ‚àë
j
2
)‚àí1
(Si ‚àíSj) + 1
2 ln
‚éõ
‚éú
‚éú
‚éú‚éù
||
(‚àë
i + ‚àë
j
)‚àï2||
‚àö
||
‚àë
i||‚àë
j||
‚éû
‚éü
‚éü
‚éü‚é†
(10.10)

250
Hybrid Intelligence for Image Analysis and Understanding
where ‚àë
i and ‚àë
j are class sample covariance matrices that correspond to the two
spectral signatures, namely, Si and Sj.
10.5.2.2
Maximum Likelihood ClassiÔ¨Åer (MLC)
MLC is a pixel-based method, that assumes that the members of each class are normally
distributed in the feature space. A pixel with an associated observed feature vector X is
assigned to class ci if:
X ‚ààci
if
gi(X) > gj(X)
for all
i ‚â†j, i, j = 1, 2..N
For multivariate Gaussian distributions, gj(X) is given in Equation (10.11):
gj(X) = ln(p(ci)) ‚àí1
2 ln
|||
|||
|||
‚àë
j
|||
|||
|||
‚àí1
2(X ‚àíMj)T
‚àí1
‚àë
j
(X ‚àíMj)
(10.11)
where Mj and ‚àë
j are the sample mean vector and covariance matrix of class j, respec-
tively; and gj is the discriminating function. It involves the estimation of class mean
vectors and covariance matrices.
10.5.2.3
Support Vector Machines (SVMs)
SVM are used to solve supervised classiÔ¨Åcation and regression problems; they were
introduced by Boser, Guyon, and Vapnik [47, 48]. Numerous classiÔ¨Åcation algorithms
have been proposed to classify remote-sensing images. Other classiÔ¨Åcation algorithms
are MLCs, neural network classiÔ¨Åers [49], decision tree classiÔ¨Åers [49], and so on. MLC is
a parametric classiÔ¨Åer based on statistical theory and has some limitations. Neural net-
work classiÔ¨Åers use a nonparametric approach to avoid the problems of MLC. MLP is the
most broadly used neural network classiÔ¨Åcation algorithm in remotely sensed images.
A decision tree classiÔ¨Åer uses diÔ¨Äerent approaches for classiÔ¨Åcation. It breaks a com-
plex problem into a number of simple decision-making processes. SVM is one of the
competitive methods in classifying high-dimensional data sets and employs the opti-
mization techniques in order to obtain the boundaries between the classes. SVMs have
been used in multispectral and hyperspectral data classiÔ¨Åcation tasks [50, 51]. SVMs
may be classiÔ¨Åed into linear SVMs and nonlinear SVMs. A linear SVM is based upon the
statistical learning theory that divides linearly separable feature space into two classes
with the maximum margin. Nonlinear SVMs can be used in case the feature space is not
linearly separable. In nonlinear SVMs, the underlying concept is to represent the data
in higher dimensional feature space such that nonlinearly mapped data is separable in
higher dimensional space and linear SVMs can be applied to the newly mapped data.
Nonlinear mapping via dot products into high-dimensional space is computationally
complex. Nonlinear SVM using a kernel becomes computationally feasible, providing
the same output as nonlinear mapping.
10.5.3
Hybrid Techniques
In classiÔ¨Åcation, one labels the group(s) of pixels to a particular class. A number of
approaches or algorithms are available to classify the data. However, hybrid approaches
for hyperspectral image classiÔ¨Åcation have been proposed in order to solve problems
in a diÔ¨Äerent way and/or to improve certain factors like output accuracy, eÔ¨Écient

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
251
framework, and so on. Some of the hybrid approaches are explained in brief in this
section.
In [52], the authors proposed a hybrid approach for semi-supervised classiÔ¨Åcation
for analysis of hyperspectral data. In the proposed approach, the multinomial logistic
regression (MLR) integrates with diÔ¨Äerent spectral-unmixing chains in order to exploit
the spectral-unmixing and classiÔ¨Åcation approaches together. In another work [53], the
researchers proposed a hybrid approach to improve the classiÔ¨Åcation performance of
hyperspectral data. The proposed hybrid approach consists of rotation forests that inte-
grate with Markov random Ô¨Åelds (MRFs).
For hyperspectral image classiÔ¨Åcation, several authors are using genetic algorithms
and SVMs, as eÔ¨Äective feature selection strategies [54]. To enhance the classiÔ¨Åcation
accuracy, several authors fused supervised and unsupervised learning techniques [55].
In particular, authors fused the strengths of the SVM classiÔ¨Åer and the fuzzy c-means
clustering algorithms.
In [56], deep convolutional neural networks (DCNNs) are introduced for
spectral-spatial hyperspectral image classiÔ¨Åcation. A proposed framework is hybridiza-
tion of PCA, logistic regression (LR), and DCNNs. Similarly, to solve nonlinear
classiÔ¨Åcation problems, a new framework [57] is proposed based on SVMs using
multicenter models and MRFs in probabilistic decision frameworks.
10.6
Target Detection
In reconnaissance and surveillance applications, hyperspectral imaging has been used
to identify targets of interest. Target detection is considered as a binary classiÔ¨Åcation
whose aim is to label each pixel in the data as a target or background. The general aim
of target detection is to identify rare target classes embedded in broadly populated and
heterogeneous background classes. The process of identifying and detecting the target
of interest in hyperspectral imagery is normally a two-stage process. The Ô¨Årst stage is an
anomaly detector [58, 59] that detects the spectral vectors that are signiÔ¨Åcantly diÔ¨Äerent
from the background pixels spectra. The second stage is to detect and conÔ¨Årm that the
detected anomaly is a target or natural clutter. This can be accomplished by using spec-
tra from the spectral library or training sets. Spectral signature‚Äìbased target detectors
(SSTDs) have been used broadly due to the availability of spectral signatures of mate-
rials in spectral libraries. This assumes that the target signature is known and tries to
detect all the pixels that have a high degree of correlation with the target signature. This
method depends upon the reference target spectra, which have been collected by using a
handheld spectro-radiometer. However, hyperspectral data collected using spaceborne
or airborne platforms need atmospheric correction; its accuracy is vital for identiÔ¨Åca-
tion of the target. The performance of SSTDs is limited due to factors like atmospheric
compensation, spatial-spectral and radiometric calibrations, and so on. This section dis-
cusses some of the basic algorithms of target detection.
10.6.1
Anomaly Detection
Anomaly detectors are also called novelty detectors or outlier detectors, and they are
used to identify the target isolated from the background pixels. This method is per-
ceived as a special case of target detection in which no prior information is provided

252
Hybrid Intelligence for Image Analysis and Understanding
about the spectra of targets of interest. The aim is to Ô¨Ånd anomalous objects in the
image with reference to the background. Anomalies vary according to the particular
applications. They can be as varied as mineral identiÔ¨Åcation, crop stress determina-
tion, and manmade object detection. Unlike SSTDs, anomaly detection can be applied
to raw radiance image data, and it does not require atmospheric compensation to be
considered. This is due to the intrinsic nature of anomaly detection, which does not
need prior information about the target spectra and can focus on the data for exploring
those pixels whose spectra are notably diÔ¨Äerent from the background. Anomaly detec-
tion is the Ô¨Årst step for image analysis to provide ROIs that may consist of potential
targets to be explored using spectral matching techniques to Ô¨Ånally determine the tar-
get of interest [60]. Lots of techniques have been proposed for anomaly detection, but
an Reed‚ÄìXiaoli (RX) algorithm is considered as the benchmark of anomaly detection
for hyperspectral data. In this subsection, an overall review of anomaly detection is
discussed.
10.6.1.1
RX Anomaly Detection
The RX anomaly detection algorithm has been developed for spectral detection of
anomaly, by identifying the targets of unknown spectral characteristics against a back-
ground clutter with an unknown spectral covariance relationship. This technique has
been applied often in hyperspectral image analysis-based applications and is viewed as a
standard anomaly detection technique in hyperspectral and multispectral applications.
The RX algorithm is a constant false alarm rate (CFAR) adaptive anomaly detector
that is extracted from the generalized likelihood ratio test (GLRT) [61]. CFAR allows
the detector to use a single threshold value to maintain the desired false alarm rate
regardless of the background variation at diÔ¨Äerent positions in the scene. Suppose that
a pixel x = [x1, x2, ‚Ä¶ , xL]T ‚àà‚ÑúL is the observation test vector consisting of L number
of bands; the output of the RX algorithm will be given as shown in Equation (10.12):
RX(x) = (x ‚àíùúáb)TC‚àí1
b (x ‚àíùúáb)
(10.12)
where ùúáb is the the global sample mean and Cb is the sample covariance matrix of the
image. The RX algorithm applies the square of the MD between the text pixel and local
background mean. The background mean and covariance matrix can be estimated glob-
ally from the whole hyperspectral data and locally using a double concentric sliding
window approach. To estimate the covariance globally, one needs more advanced meth-
ods or techniques like a linear or stochastic mixture model or some clustering tech-
niques. On the other hand, a double-sliding window approach can be used to estimate
the covariance matrix locally to consist of test pixels, an inner window region (IWR), a
guard band, and an outer window region (OWR), as shown in Figure 10.7. Local mean
vector and covariance matrix are calculated from the pixel(s) falling within the OWR.
Inner window size depends upon the size of target of interest in the image. Guard band
size is marginally larger than the IWR and smaller than the OWR. It is used to denote
the probability that some target spectra will inhabit the OWR and hence aÔ¨Äect the back-
ground model [62]. A local RX algorithm calculation is computationally quite intensive
compared to the global RX algorithm due to the need of estimation of a covariance
matrix at each location of test pixel following the double concentric sliding window
approach. Several variations of the RX detector approach have been proposed to surpass
some of the limitations.

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
253
Outer
Window
Region (OWR)
Guard
Window
Test Pixel
Inner Window
Region (IWR)
Figure 10.7 Double concentric sliding window.
10.6.1.2
Subspace-Based Anomaly Detection
In a subspace anomaly detection algorithm, projection vectors are generated using
the OWR and IWR covariance matrices [63]. Denoting a spectral vector for the
IWR centered at test pixel xk = (xk(1), xk(2), ‚Ä¶ , xk(L)), where L refers to the num-
ber of spectral bands; and k = 1, 2, ‚Ä¶ , Niw, that is, there are a total of Niw number
of pixels in the IWR, matrix X = [x1, x2, ‚Ä¶ , xNiw]L‚àóNiw. In the same way, spectral
vector for OWR yl = (yl(1), yl(2), ‚Ä¶ , yl(L)), where L indicates the number of spec-
tral bands and l = 1, 2 ‚Ä¶ , Now (i.e., the total number of pixels in the OWR matrix
Y = [y1, y2, ‚Ä¶ , yNow]L‚àóNow). The covariance matrices for the IWR and OWR are given by
Equations (10.13) and (10.14):
Cx =
1
Niw ‚àí1(X ‚àíÃÇùúáX)(X ‚àíÃÇùúáX)T
(10.13)
Cy =
1
Now ‚àí1(Y ‚àíÃÇùúáY)(Y ‚àíÃÇùúáY)T
(10.14)
where ÃÇùúáX and ÃÇùúáY are the statistical means of the IWR and OWR (back-ground),
respectively. The projection separation statistics for test pixel r is represented as
Equation (10.15):
s‚Ä≤ = (r ‚àíÃÇùúáY)TWW T(r ‚àíÃÇùúáY)
(10.15)
where W = [ùë§1, ùë§2 ‚Ä¶ ùë§m] is the matrix with m projection vectors; WW T is known as
projection vectors and indicates a subspace characterizing the spectra used to generate
the projection vectors ùë§i; and, if the projection separation statistics s
‚Ä≤ is greater than
some threshold value, anomaly is detected.
10.6.2
Signature-Based Target Detection
The performance of the anomaly detection is limited by no prior knowledge of target
spectra in hyperspectral imagery. With signature-based detection, the reference spec-
tral signature of the target becomes available. A spectral Ô¨Ångerprint is collected in the
form of reÔ¨Çectance from the laboratory or Ô¨Åeld. For some target detection applications,
a spectral characteristic of the target is known a priori. Spectral characteristics of tar-
gets can be deÔ¨Åned by a single target spectrum or target subspace. In this manner, the

254
Hybrid Intelligence for Image Analysis and Understanding
background gets modeled as a statistically Gaussian distribution or with subspace that
represents the local or whole background statistics. In this subsection, focus is on the
classical detection algorithm of a spectral angle mapper, spectral matched Ô¨Ålter, and
matched subspace detector, which are brieÔ¨Çy explained here.
10.6.2.1
Euclidean distance
The ED for signature-based target detection is given in Equation (10.16).
ED(Sj, Sk) = ||Sj ‚àíSk|| =
‚àö
‚àö
‚àö
‚àö
‚àö
L
‚àë
l=1
(Sjl ‚àíSkl)2
(10.16)
where Sj = (Sj1, Sj2, ‚Ä¶ , SjL)T and Sk = (Sk1, Sk2, ‚Ä¶ , SkL)T are two spectral signature vec-
tors; and L is the number of spectral bands.
10.6.2.2
Spectral Angle Mapper (SAM)
SAM is one of the most frequently used methods under the supervised hyperspectral
imaging classiÔ¨Åcation approach. Normally, it compares the image spectral signature
with the known spectral signature (collected in the laboratory or in the Ô¨Åeld with a
spectro-radiometer) or an endmember. In SAM, a pixel with zero or minimum spec-
tral angle compared to the reference spectra is assigned to the class that is deÔ¨Åned
by the reference vector. On modifying the threshold for classiÔ¨Åcation using SAM, the
probability for target identiÔ¨Åcation may increase. This algorithm is based on the assump-
tion that every pixel contains only one ground material and can be uniquely assigned
to one class. Spectral similarity is obtained by considering the spectral signature as
a vector in L-dimensional space, where L refers to the number of spectral bands. It
measures the spectral similarity by determining angles between two spectra, treating
them as vectors in space with dimensionality equal to the number of bands. SAM deter-
mines the spectral similarity between the test spectra and reference spectra by using
Equation (10.17) [64]:
ùõº= cos‚àí1
(
‚àëL
i=1 tiri
( ‚àëL
i=1 t2
i
)1‚àï2( ‚àëL
i=1 r2
i
)1‚àï2
)
(10.17)
where ùõº= spectral angle between the test and reference spectra; L = number of bands;
ti = test spectra; and ri = reference spectra.
10.6.2.3
Spectral Matched Vilter (SMF)
In SMFs, hyperspectral target detection can be expressed using two complementary
hypotheses. The assumption for the spectral matched Ô¨Ålter is:
H0 = X ‚à∂m, Target absent
H1 = X ‚à∂as + m, Target present
where a represents an unknown target abundance measure (i.e., a=0 in case of no target
and a>0 in case of target); s = [s1, s2, ‚Ä¶ , sn]T is target spectra; and m is the zero-mean
Gaussian random additive background clutter noise. This technique is based on the pre-
sumption that Gaussian distributions for background clutter noise N(0, ÃÇCb) and target
N(as, ÃÇCb) have the same covariance matrices (where a represents the scalar abundance

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
255
value indicating target strength). The output of this method for a test pixel x using GLRT
is given [65] by:
DSMF(x) =
sT ÃÇC‚àí1
b x
‚àö
sT ÃÇC‚àí1
b s
‚â∑H1
H0 ùúÇ
(10.18)
where ÃÇCb represents the estimated covariance matrix for the centered observation data;
and ùúÇrepresents any threshold value.
10.6.2.4
Matched Subspace Detector (MSD)
In MSD [66], target pixel vectors are represented as a linear combination of target
and background spectra that are presented by subspace target and background spec-
tra, respectively. The target identiÔ¨Åcation subject is represented as two competing
hypotheses, which are given as:
H0 = X ‚à∂Bùúâ+ m, Target absent
H1 = X ‚à∂SŒò + Bùúâ+ m, Target present
where S and B indicate target and background subspaces, respectively; Œò and ùúâare the
coeÔ¨Écients that indicate the abundance in S and B; and m represents Gaussian random
noise. The MSD [66] model using GLRT has been given in Equation (10.19):
DùúÇMSD(x) = XT(1 ‚àíPb)X
XT(1 ‚àíPtb)X ‚â∑H1
H0 ùúÇMSD
(10.19)
where Pb = BB‚ôØis a projection matrix associated with the background subspace; Ptb =
[SB][SB]‚ôØis a projection matrix associated with the target-and-background subspace;
and DùúÇMSD(x) is compared with threshold ùúÇMSD in order to Ô¨Ånd which hypothesis is
related to x.
10.6.3
Hybrid Techniques
In the workÔ¨Çow of hyperspectral image processing and analysis, target identiÔ¨Åcation and
veriÔ¨Åcation tasks are one of the vital tasks of postprocessing and analysis. There are a
number of algorithms available in order to identify the target of interest in applications
like search and rescue, defense and intelligence, and so on. However, algorithms face
some critical challenges (e.g., no prior information of target, etc.) in order to identify the
objects of interest using approaches like subpixel target detection, anomaly detection,
and so on. In order to overcome these critical challenges, combinations and/or frame-
works of collectively applied algorithms are proposed (i.e., hybridization of algorithms)
for target identiÔ¨Åcation.
Normally, spatial resolution is limited in hyperspectral image and subpixel targets that
occupy some part of the pixel. Therefore, subpixel target detection is a challenging task
in which the target on ground itself is smaller than the pixel size in the image. Hybrid
algorithms and/or frameworks are proposed in order to identify these subpixel targets.
Adaptive matched subspace detector (AMSD) is used to identify targets that are diÔ¨Äer-
ent from background image pixels based on the statistical characteristics. In [67], the
authors proposed a hybrid detector system that uses the advantages of both detectors
[i.e., fully constrained least squares (FCLS) and AMSD] for increasing the accuracy of

256
Hybrid Intelligence for Image Analysis and Understanding
a subpixel target detection operation. In another work [68], the authors proposed two
hybrid detectors to exploit the advantages of these two approaches by modeling the
background using the characteristics of both physics- and statistics-based approaches
in order to accurately identify the target(s). Other diÔ¨Äerent hybrid detectors systems
and approaches [69‚Äì71] are proposed for subpixel target identiÔ¨Åcation utilizing existing
and/or new techniques in a varied manner.
In situations and applications like surveillance, no prior knowledge related to the
object of interest is available. In such situations, one needs to perform object detec-
tion and identiÔ¨Åcation in conditions with no a priori knowledge. Hybridization and/or
frameworks of algorithms have also been proposed for eÔ¨Écient identiÔ¨Åcation of such
anomalies. The RX detector is one of the most frequently used anomaly detection algo-
rithms. Several authors have proposed hybrid and/or modiÔ¨Åed versions of RX [72, 73]
detector systems in order to increase the eÔ¨Éciency of the anomaly detection step for
further processing the outputs.
10.7
Conclusions
Hyperspectral imaging involves multiple disciplines of knowledge adapting vast
concepts from signal and image processing, statistics, and machine learning to hybrid
computing techniques. The importance of the hyperspectral imaging has been increas-
ing due to the large number of spaceborne and airborne sensors systems available,
commercially available sensors, and the number of applications supported by this tech-
nique. This imposes some challenges like high dimension, spectral unmixing (linear
and nonlinear), atmospheric eÔ¨Äect elimination, and so on, which require sophisticated
and hybrid algorithms for data processing, analysis, and interpretation. In this chapter,
we have presented some major data analysis methods and algorithms for spectral
unmixing, classiÔ¨Åcation, and target detection along with a critical discussion of the
hybrid approaches for systems and algorithms. As a conclusion, hybrid hyperspectral
image and analyses comprise a prominent and emerging Ô¨Åeld, contributing actively with
frontier cross-disciplinary research activities. It is expected to mature in the coming
years from all aspects of the hybrid approaches in sensor development, algorithms, and
applications.
References
1 Feng, Y.Z. and Sun, D.W. (2012) Application of hyperspectral imaging in food safety
inspection and control: a review. Critical Reviews in Food Science and Nutrition, 52
(11), 1039‚Äì1058.
2 Adam, E., Mutanga, O., and Rugege, D. (2010) Multispectral and hyperspectral
remote sensing for identiÔ¨Åcation and mapping of wetland vegetation: a review.
Wetlands Ecology and Management, 18 (3), 281‚Äì296.
3 Olmanson, L.G., Brezonik, P.L., and Bauer, M.E. (2013) Airborne hyperspectral
remote sensing to assess spatial distribution of water quality characteristics in large
rivers: the Mississippi River and its tributaries in Minnesota. Remote Sensing of
Environment, 130, 254‚Äì265.

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
257
4 Kruse, F.A., Boardman, J.W., and Huntington, J.F. (2003) Comparison of airborne
hyperspectral data and eo-1 hyperion for mineral mapping. IEEE Transactions on
Geoscience and Remote Sensing, 41 (6), 1388‚Äì1400.
5 Dale, L.M., Thewis, A., Boudry, C., Rotar, I., Dardenne, P., Baeten, V., and Pierna,
J.A.F. (2013) Hyperspectral imaging applications in agriculture and agro-food
product quality and safety control: a review. Applied Spectroscopy Reviews, 48
(2), 142‚Äì159.
6 Lu, G. and Fei, B. (2014) Medical hyperspectral imaging: a review. Journal of
Biomedical Optics, 19 (1), 010 901‚Äì010 901.
7 ElMasry, G., Wang, N., ElSayed, A., and Ngadi, M. (2007) Hyperspectral imaging for
nondestructive determination of some quality attributes for strawberry. Journal of
Food Engineering, 81 (1), 98‚Äì107.
8 Qiao, J., Ngadi, M.O., Wang, N., Gari√©py, C., and Prasher, S.O. (2007) Pork quality
and marbling level assessment using a hyperspectral imaging system. Journal of
Food Engineering, 83 (1), 10‚Äì16.
9 Solomon, J. and Rock, B. (1985) Imaging spectrometry for earth remote sensing.
Science, 228 (4704), 1147‚Äì1152.
10 Pearlman, J.S., Barry, P.S., Segal, C.C., Shepanski, J., Beiso, D., and Carman, S.L.
(2003) Hyperion, a space-based imaging spectrometer. Geoscience and Remote
Sensing, IEEE Transactions, 41 (6), 1160‚Äì1173.
11 Chang, C.I. (2013) Hyperspectral data processing: algorithm design and analysis,
John Wiley & Sons, Hoboken, NJ.
12 Eismann, M.T. (2012) Hyperspectral remote sensing. SPIE, Bellingham, WA.
13 Sun, D.W. (2010) Hyperspectral imaging for food quality analysis and control. Else-
vier, Amsterdam.
14 Inoue, Y. and Penuelas, J. (2001) An AOTF-based hyperspectral imaging system for
Ô¨Åeld use in ecophysiological and agricultural applications. International Journal of
Remote Sensing, 22 (18), 3883‚Äì3888.
15 Wang, W., Li, C., Tollner, E.W., Rains, G.C., and Gitaitis, R.D. (2012) A liquid crys-
tal tunable Ô¨Ålter based shortwave infrared spectral imaging system: design and
integration. Computers and Electronics in Agriculture, 80, 126‚Äì134.
16 Park, B. and Lu, R. (2015) Hyperspectral imaging technology in food and agriculture,
Springer, New York.
17 Yu, X., Sun, Y., Fang, A., Qi, W., and Liu, C. (2014) Laboratory spectral calibration
and radiometric calibration of hyper-spectral imaging spectrometer, in Systems and
Informatics (ICSAI), 2014 2nd International Conference, IEEE, pp. 871‚Äì875.
18 Rodarmel, C. and Shan, J. (2002) Principal component analysis for hyperspectral
image classiÔ¨Åcation. Surveying and Land Information Science, 62 (2), 115.
19 Pu, R. and Gong, P. (2004) Wavelet transform applied to eo-1 hyperspectral data
for forest lai and crown closure mapping. Remote Sensing of Environment, 91 (2),
212‚Äì224.
20 Du, Q. (2007) ModiÔ¨Åed Fisher‚Äôs linear discriminant analysis for hyperspectral
imagery. IEEE Geoscience and Remote Sensing Letters, 4 (4), 503.
21 Chang, C.I. and Wang, S. (2006) Constrained band selection for hyperspectral
imagery. IEEE Transactions on Geoscience and Remote Sensing, 44 (6), 1575‚Äì1585.

258
Hybrid Intelligence for Image Analysis and Understanding
22 Bioucas-Dias, J.M. and Plaza, A. (2010) Hyperspectral unmixing: geometrical, statis-
tical, and sparse regression-based approaches, in Remote sensing. SPIE, Bellingham,
WA, pp. 78 300A‚Äì78 300A.
23 Bioucas-Dias, J.M., Plaza, A., Dobigeon, N., Parente, M., Du, Q., Gader, P., and
Chanussot, J. (2012) Hyperspectral unmixing overview: geometrical, statistical, and
sparse regression-based approaches. Selected Topics in Applied Earth Observations
and Remote Sensing, IEEE Journal, 5 (2), 354‚Äì379.
24 Boardman, J.W., Kruse, F.A., and Green, R.O. (1995) Mapping target signatures via
partial unmixing of AVIRIS data, in Proceedings of Annual JPL Airborne Geoscience
Workshop, Pasadena, CA, pp. 23‚Äì26.
25 Winter, M.E. (1999) N-FINDR: an algorithm for fast autonomous spectral
end-member determination in hyperspectral data, in SPIE‚Äôs International Sympo-
sium on Optical Science, Engineering, and Instrumentation. SPIE, Bellingham, WA,
pp. 266‚Äì275.
26 Nascimento, J.M. and Dias, J.M.B. (2005) Vertex component analysis: a fast algo-
rithm to unmix hyperspectral data. Geoscience and Remote Sensing, IEEE Transac-
tions, 43 (4), 898‚Äì910.
27 Chang, C.I., Wu, C.C., Liu, W., and Ouyang, Y.C. (2006) A new growing method for
simplex-based endmember extraction algorithm. IEEE Transactions on Geoscience
and Remote Sensing, 44 (10), 2804‚Äì2819.
28 Li, J. and Bioucas-Dias, J.M. (2008) Minimum volume simplex analysis: a fast
algorithm to unmix hyperspectral data, in IGARSS 2008-2008 IEEE International
Geoscience and Remote Sensing Symposium, vol. 3, IEEE, pp. III‚Äì250.
29 Chan, T.H., Chi, C.Y., Huang, Y.M., and Ma, W.K. (2009) A convex analysis-based
minimum-volume enclosing simplex algorithm for hyperspectral unmixing. IEEE
Transactions on Signal Processing, 57 (11), 4418‚Äì4432.
30 Bioucas-Dias, J.M. (2009) A variable splitting augmented Lagrangian approach to
linear spectral unmixing, in 2009 First Workshop on Hyperspectral Image and Signal
Processing: Evolution in Remote Sensing, IEEE, pp. 1‚Äì4.
31 Ambikapathi, A., Chan, T.H., Ma, W.K., and Chi, C.Y. (2011) Chance-constrained
robust minimum-volume enclosing simplex algorithm for hyperspectral unmixing.
IEEE Transactions on Geoscience and Remote Sensing, 49 (11), 4194‚Äì4209.
32 Bayliss, J.D., Gualtieri, J.A., and Cromp, R.F. (1998) Analyzing hyperspectral data
with independent component analysis, in 26th AIPR Workshop: Exploiting New
Image Sources and Sensors. SPIE, Bellingham, WA, pp. 133‚Äì143.
33 Nascimento, J.M. and Bioucas-Dias, J.M. (2007) Dependent component analysis:
a hyperspectral unmixing algorithm, in Pattern Recognition and Image Analysis,
Springer, Berlin, pp. 612‚Äì619.
34 Plaza, A., Martinez, P., P√©rez, R., and Plaza, J. (2002) Spatial/spectral endmember
extraction by multidimensional morphological operations. Geoscience and Remote
Sensing, IEEE Transactions, 40 (9), 2025‚Äì2041.
35 Rogge, D., Rivard, B., Zhang, J., Sanchez, A., Harris, J., and Feng, J. (2007) Integra-
tion of spatial‚Äìspectral information for the improved extraction of endmembers.
Remote Sensing of Environment, 110 (3), 287‚Äì303.
36 Dobigeon, N., Tourneret, J.Y., and Chang, C.I. (2008) Semi-supervised linear spec-
tral unmixing using a hierarchical bayesian model for hyperspectral imagery. IEEE
Transactions on Signal Processing, 56 (7), 2684‚Äì2695.

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
259
37 Dobigeon, N., Moussaoui, S., Coulon, M., Tourneret, J.Y., and Hero, A.O. (2009)
Joint Bayesian endmember extraction and linear unmixing for hyperspectral
imagery. IEEE Transactions on Signal Processing, 57 (11), 4355‚Äì4368.
38 Rezaei, Y., Mobasheri, M.R., Zoej, M.V., and Schaepman, M.E. (2012) Endmember
extraction using a combination of orthogonal projection and genetic algorithm.
IEEE Geoscience and Remote Sensing Letters, 9 (2), 161‚Äì165.
39 Yang, J., Zhao, Y.Q., Chan, J.C.W., and Kong, S.G. (2016) Coupled sparse denoising
and unmixing with low-rank constraint for hyperspectral image. IEEE Transactions
on Geoscience and Remote Sensing, 54 (3), 1818‚Äì1833.
40 Kumar, U., Raja, K.S., Mukhopadhyay, C., and Ramachandra, T. (2012) A neural
network based hybrid mixture model to extract information from non-linear mixed
pixels. Information, 3 (3), 420‚Äì441.
41 Plaza, A., Benediktsson, J.A., Boardman, J.W., Brazile, J., Bruzzone, L., Camps-Valls,
G., Chanussot, J., Fauvel, M., Gamba, P., Gualtieri, A., et al. (2009) Recent advances
in techniques for hyperspectral image processing. Remote Sensing of Environment,
113, S110‚ÄìS122.
42 Landgrebe, D.A. (2005) Signal theory methods in multispectral remote sensing,
vol. 29. John Wiley & Sons, Hoboken, NJ.
43 Bioucas-Dias, J.M. and Nascimento, J.M. (2008) Hyperspectral subspace identiÔ¨Åca-
tion. Geoscience and Remote Sensing, IEEE Transactions, 46 (8), 2435‚Äì2445.
44 Hossain, M.A., Pickering, M., and Jia, X. (2011) Unsupervised feature extraction
based on a mutual information measure for hyperspectral image classiÔ¨Åcation, in
Geoscience and Remote Sensing Symposium (IGARSS), 2011 IEEE International,
IEEE, pp. 1720‚Äì1723.
45 Bruce, L.M. and Li, J. (2001) Wavelets for computationally eÔ¨Écient hyperspec-
tral derivative analysis. Geoscience and Remote Sensing, IEEE Transactions, 39 (7),
1540‚Äì1546.
46 Kumar, S., Ghosh, J., and Crawford, M.M. (2001) Best-bases feature extraction algo-
rithms for classiÔ¨Åcation of hyperspectral data. Geoscience and Remote Sensing, IEEE
Transactions, 39 (7), 1368‚Äì1379.
47 Boser, B.E., Guyon, I.M., and Vapnik, V.N. (1992) A training algorithm for optimal
margin classiÔ¨Åers, in Proceedings of the Fifth Annual Workshop on Computational
Learning Theory. ACM, New York, pp. 144‚Äì152.
48 Vapnik, V. (2013) The nature of statistical learning theory. Springer Science & Busi-
ness Media, New York.
49 Goel, P., Prasher, S., Patel, R., Landry, J., Bonnell, R., and Viau, A. (2003) ClassiÔ¨Åca-
tion of hyperspectral data by decision trees and artiÔ¨Åcial neural networks to identify
weed stress and nitrogen status of corn. Computers and Electronics in Agriculture,
39 (2), 67‚Äì93.
50 Huang, C., Davis, L., and Townshend, J. (2002) An assessment of support vector
machines for land cover classiÔ¨Åcation. International Journal of Remote Sensing, 23
(4), 725‚Äì749.
51 Gualtieri, J.A. and Cromp, R.F. (1999) Support vector machines for hyperspec-
tral remote sensing classiÔ¨Åcation, in The 27th AIPR Workshop: Advances in
Computer-Assisted Recognition. SPIE, Bellingham, WA, pp. 221‚Äì232.
52 D√≥pido, I., Li, J., Gamba, P., and Plaza, A. (2014) A new hybrid strategy com-
bining semisupervised classiÔ¨Åcation and unmixing of hyperspectral data. IEEE

260
Hybrid Intelligence for Image Analysis and Understanding
Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 7 (8),
3619‚Äì3629.
53 Xia, J., Chanussot, J., Du, P., and He, X. (2015) Spectral‚Äìspatial classiÔ¨Åcation
for hyperspectral data using rotation forests with local feature extraction and
markov random Ô¨Åelds. IEEE Transactions on Geoscience and Remote Sensing, 53
(5), 2532‚Äì2546.
54 Li, S., Wu, H., Wan, D., and Zhu, J. (2011) An eÔ¨Äective feature selection method for
hyperspectral image classiÔ¨Åcation based on genetic algorithm and support vector
machine. Knowledge-Based Systems, 24 (1), 40‚Äì48.
55 Alajlan, N., Bazi, Y., Melgani, F., and Yager, R.R. (2012) Fusion of supervised and
unsupervised learning for improved classiÔ¨Åcation of hyperspectral images. Informa-
tion Sciences, 217, 39‚Äì55.
56 Yue, J., Zhao, W., Mao, S., and Liu, H. (2015) Spectral‚Äìspatial classiÔ¨Åcation of
hyperspectral images using deep convolutional neural networks. Remote Sensing
Letters, 6 (6), 468‚Äì477.
57 Tang, B., Liu, Z., Xiao, X., Nie, M., Chang, J., Jiang, W., Li, X., and Zheng, C. (2015)
Spectral‚Äìspatial hyperspectral classiÔ¨Åcation based on multi-center SAM and MRF.
Optical Review, 22 (6), 911‚Äì918.
58 Stein, D.W., Beaven, S.G., HoÔ¨Ä, L.E., Winter, E.M., Schaum, A.P., and Stocker, A.D.
(2002) Anomaly detection from hyperspectral imagery. Signal Processing Magazine,
IEEE, 19 (1), 58‚Äì69.
59 Matteoli, S., Diani, M., and Corsini, G. (2010) A tutorial overview of anomaly detec-
tion in hyperspectral images. Aerospace and Electronic Systems Magazine, IEEE, 25
(7), 5‚Äì28.
60 Chang, C.I. and Chiang, S.S. (2002) Anomaly detection and classiÔ¨Åcation for
hyperspectral imagery. Geoscience and Remote Sensing, IEEE Transactions, 40 (6),
1314‚Äì1325.
61 Reed, I.S. and Yu, X. (1990) Adaptive multiple-band CFAR detection of an optical
pattern with unknown spectral distribution. Acoustics, Speech and Signal Processing,
IEEE Transactions, 38 (10), 1760‚Äì1770.
62 Ranney, K.I. and Soumekh, M. (2006) Hyperspectral anomaly detection within the
signal subspace. Geoscience and Remote Sensing Letters, IEEE, 3 (3), 312‚Äì316.
63 Goldberg, H. and Nasrabadi, N.M. (2007) A comparative study of linear and
nonlinear anomaly detectors for hyperspectral imagery, in Defense and Security
Symposium. SPIE, Bellingham, WA, pp. 656 504‚Äì656 504.
64 Kruse, F., LefkoÔ¨Ä, A., Boardman, J., Heidebrecht, K., Shapiro, A., Barloon, P., and
Goetz, A. (1993) The spectral image processing system (SIPS)‚Äìinteractive visualiza-
tion and analysis of imaging spectrometer data. Remote Sensing of Environment, 44
(2), 145‚Äì163.
65 Fuhrmann, D.R., Kelly, E.J., and Nitzberg, R. (1992) A CFAR adaptive matched Ô¨Ålter
detector. Transactions on Aerospace and Electronic Systems, 28 (1), 208‚Äì216.
66 Scharf, L.L. and Friedlander, B. (1994) Matched subspace detectors. Signal Process-
ing, IEEE Transactions, 42 (8), 2146‚Äì2157.
67 Broadwater, J., Meth, R., and Chellappa, R. (2004) A hybrid algorithm for subpixel
detection in hyperspectral imagery, in Geoscience and Remote Sensing Symposium,
2004. IGARSS‚Äô04. Proceedings. 2004 IEEE International, vol. 3, IEEE, pp. 1601‚Äì1604.

Hyperspectral Data Processing: Spectral Unmixing, ClassiÔ¨Åcation, and Target IdentiÔ¨Åcation
261
68 Broadwater, J. and Chellappa, R. (2007) Hybrid detectors for subpixel targets. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 29 (11), 1891‚Äì1903.
69 Zhang, L., Du, B., and Zhong, Y. (2010) Hybrid detectors based on selective end-
members. IEEE Transactions on Geoscience and Remote Sensing, 48 (6), 2633‚Äì2646.
70 Zhou, L., Zhang, X., Guan, B., and Zhao, Z. (2014) Dual force hybrid detector for
hyperspectral images. Remote Sensing Letters, 5 (4), 377‚Äì385.
71 Xu, Y., Wu, Z., Li, J., Plaza, A., and Wei, Z. (2016) Anomaly detection in hyperspec-
tral images based on low-rank and sparse representation. IEEE Transactions on Geo-
science and Remote Sensing, 54 (4), 1990‚Äì2000.
72 Zare-Baghbidi, M., Homayouni, S., and Jamshidi, K. (2015) Improving the RX
anomaly detection algorithm for hyperspectral images using FFT. Modeling and
Simulation in Electrical and Electronics Engineering, 1 (2), 33‚Äì39.
73 Khazai, S. and Mojaradi, B. (2015) A modiÔ¨Åed kernel-RX algorithm for anomaly
detection in hyperspectral images. Arabian Journal of Geosciences, 8 (3), 1487‚Äì1495.

263
11
A Hybrid Approach for Band Selection of Hyperspectral Images
Aditi Roy Chowdhury1, Joydev Hazra2, and Paramartha Dutta3
1Department of Computer Science and Technology, Women‚Äôs Polytechnic, Jodhpur Park, Kolkata, West Bengal, India
2Department of Information Technology, Heritage Institute of Technology, Kolkata, West Bengal, India
3Department of Computer and System Sciences, Visva-Bharati University, Santiniketan, West Bengal, India
11.1
Introduction
Image processing is an emerging Ô¨Åeld of research. Image classiÔ¨Åcation, analysis, and
processing are very useful in diÔ¨Äerent areas of research like pattern recognition [1],
image registration [2‚Äì4], image forensics, and so on. Another form of image process-
ing is hyperspectral images. The human eye can observe wavelengths between 380 nm
to 760 nm, and they are classiÔ¨Åed as the visible spectrum. Other forms of radiation are
perceived as infrared and ultraviolet (UV) light, as given in Figure 11.1. Most multispec-
tral images (Landsat, AVHRR, etc.) measure radiation reÔ¨Çected from wide and separate
wavelength bands. But most hyperspectral images measure radiation as narrow and con-
tiguous wavelength bands. A hyperspectral image consists of diÔ¨Äerent spectral bands
with a very Ô¨Åne spectral resolution. The Airborne Visible/Infrared Imaging Spectrome-
ter (AVIRIS) from NASA Jet Propulsion Laboratory (NASA/JPL) and the Hyperspectral
Digital Imagery Collection Experiment (HYDICE) from Naval Research Laboratory are
examples of two sensors that can collect image information with hundreds of spectral
bands. For accurate object identiÔ¨Åcation, spectral information provides an important
role. Hyperspectral images have suÔ¨Écient spectral information to identify and distin-
guish diÔ¨Äerent materials uniquely. The image spectra of the hyperspectral image can be
compared with some laboratory reÔ¨Çectance spectra in order to recognize and to prepare
a ground truth map of surface materials, such as particular types of land, vegetation, or
minerals. The hyperspectral image itself is an image cube as represented in Figure 11.2,
where the z-axis represents the band. Here, each pixel is represented by a vector con-
sisting of the reÔ¨Çectance of the object of a band. The length of each vector is equal to the
number of bands of any particular data set.
Thus, the data volume to be processed for a hyperspectral image is huge. This vast
amount of data creates problem in data transmission and storage. For image analysis,
the computational complexity is also very high. Particularly, traditional image analysis
techniques face challenges to handle this huge volume of data. From this purview, band
selection is necessary. It is obvious to perform this selection process without hampering
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

264
Hybrid Intelligence for Image Analysis and Understanding
106
105
104
Gamma rays
X-rays
Ultraviolet Visible Infrared
Microwaves
Radio waves
103
102
101
10‚Äì1
Energy of one photon (electron volts)
10‚Äì1
10‚Äì2
10‚Äì3
10‚Äì4
10‚Äì5
10‚Äì6
10‚Äì7
10‚Äì8
10‚Äì9
Figure 11.1 Electromagnetic spectrum [5].
Pixel Spectrum
radiance
3500
3000
2500
2000
1500
1000
500
0
50
100
Number of Band
150
200
0
Pixel Spectrum
radiance
3500
3000
2500
2000
1500
1000
500
0
50
100
Number of Band
Spatial (x)
Spatial (y)
150
200
0
Pixel Spectrum
radiance
2500
2000
1500
1000
500
0
50
100
Number of Band
150
200
0
Spectral (z)
Figure 11.2 AVIRIS hyperspectral image cube [6].
classiÔ¨Åcation accuracy. For the hyperspectral image, some bands are aÔ¨Äected by vari-
ous atmospheric eÔ¨Äects like noise. These bands are irrelevant. Some redundant bands
are also there to increase the complexity of the analysis system and produce an incor-
rect prediction. Again, some brands contain vast information about the scene, but their
predictions are incorrect. So, minute investigation is needed to detect the relationship
between the bands and the scene.
Data dimensionality can be reduced by mapping the high-dimensional data onto
a low-dimensional space by using certain techniques. Principal component analysis
(PCA), maximum noise fraction transform (MNF) [7, 8], and linear discriminant
analysis (LDA) have been used as dimension reduction techniques. PCA is used to
maximize the variance of the transformed data (or to minimize the reconstruction
error), whereas Fisher‚Äôs LDA is used to maximize the class separability. PCA and
noise-adjusted PCA were proposed for unsupervised band [9] selection. However, the

A Hybrid Approach for Band Selection of Hyperspectral Images
265
main problem with these methods is that they do not recover the original bands that
correspond to wavelengths in the main data sets. Thus, some crucial information may
be lost or distorted.
The orthogonal subspace approach was also used to transform the data [10]. Merg-
ing diÔ¨Äerent adjacent bands is also used in dimensionality reduction [11, 12]. Another
dimensionality reduction approach is band selection. In band selection, we can reduce
the dimensionality of hyperspectral images by selecting only the relevant bands without
losing their physical meaning. Hence, it is advantageous over dimensionality reduction
since it preserves the original data of each band. Band selection can be done based
on feature selection methodology or by extracting new bands containing the maximal
information. Hyperspectral image analysis deals with the detection or classiÔ¨Åcation of
objects. The main aim of band selection is to select a combination of bands from the
original one so that the accuracy is unchanged or tolerably degraded, and the compu-
tational complexity is signiÔ¨Åcantly reduced. Hence, band selection is popular among
researchers.
Recently, diÔ¨Äerent band selection methods have been proposed. Basically, band
selection techniques can be broadly classiÔ¨Åed into two groups, supervised and
unsupervised, depending upon the availability of image data. Supervised meth-
ods are basically a predictive technique. They use training data to establish a
model for prediction. These methods preserve a priori knowledge of image data.
Among diÔ¨Äerent supervised techniques, canonical analysis was employed in [9].
In MVPCA (maximum variance-based PCA), the bands are sorted depending on
the importance of individual bands as well as the correlation with other bands.
Key problems of supervised band selection are the number of selected bands and
the criteria to choose the band. DiÔ¨Äerent measurement criteria have been pro-
posed like consistency measure [13], mutual information [14], and information
measure [15]. An exhaustive search strategy can give an optimal solution. Popular
search strategies are like maximum inÔ¨Çuence feature selection (MIFS), minimum
redundancy‚Äìmaximum relevance (mRMR), and so on. Most recently, a clonal selec-
tion algorithm (CSA) was used as an eÔ¨Écient search strategy. In [16], a CSA based
on JeÔ¨Äris‚ÄìMatusita distance measure was used for dimensionality reduction. The
trivariate mutual information (TMI) and semisupervised TMI-based (STMI) methods
[17] are used as band selection methods incorporating aa priori CSA as a search
strategy.
Due to unavailability of the required prior knowledge, the supervised technique
is sometimes not useful. Therefore, unsupervised techniques that can oÔ¨Äer better
performance over supervised techniques regardless of the type of objects to be detected
are more useful. Band index (BI) and mutual information (MI) have been used to
measure the statistical dependence between diÔ¨Äerent bands [18]. The linear prediction
(LP)-based band similarity metric performs well for a small number of pixels [19‚Äì21].
Another unsupervised technique is band clustering, where the initial spectral bands
are split into disjoint clusters or subbands [22].
Another approach in band selection is feature selection. DiÔ¨Äerent feature extraction
techniques like locality preserving projection (LPP) [23], PCA [24], and locally linear
embedding (LLE) [25] transform or map data from high dimensions to low dimensions.
However, feature selection reduces the dimensionality by selecting some important fea-
tures among a set of original features. Feature ranking [26, 9] and feature clustering

266
Hybrid Intelligence for Image Analysis and Understanding
[27, 28] are feature selection techniques. Mitra et al. [29] propose an unsupervised fea-
ture selection algorithm based on similarity between features.
In this chapter, we used a hybrid technique to select important bands. Here, we used
CSA to identify the best band combinations based on the fuzzy k-nearest neighbors
(KNN) technique, incorporating the features extracted by 2D PCA on each class sep-
arately. ClassiÔ¨Åcation accuracy of diÔ¨Äerent band combinations selects the best set of
bands.
The organization of this chapter is as follows: Section 11.2 revisits a brief description
of diÔ¨Äerent concepts of relevant techniques used in the proposed work. The proposed
methodology and underlying algorithms are described in Section 11.3. Section 11.4
presents a brief description of data sets used in the experiment as well as the experi-
mental values and result analysis. Section 11.5 contains conclusive remarks.
11.2
Relevant Concept Revisit
In this section, a brief description of feature selection using 2D PCA, CSA, and fuzzy
KNN is provided.
11.2.1
Feature Extraction
The term feature is very important in image processing and analysis. It can be deÔ¨Åned
as a function of some quantiÔ¨Åable properties of an image. Feature extraction and selec-
tion are mechanisms to highlight some important and signiÔ¨Åcant characteristics of an
object. There are many feature extraction techniques. Features can be broadly classiÔ¨Åed
into two groups: application speciÔ¨Åc and application independent. Application-speciÔ¨Åc
features are like human faces, Ô¨Ångerprints, veins and some other conceptual features.
Application-independent features can be divided into: local features (features calculated
on image segmentation, clustering, and edge identiÔ¨Åcation), global features (features
calculated on the entire image or some prespeciÔ¨Åed regular sub-image), and pixel-level
features (features calculated on texture, color, and pixel coordinates). Another form of
classiÔ¨Åcation is low level versus high level. Low-level features are directly related with
the original image. Some feature extraction techniques are directly used on the origi-
nal image to extract low-level features. Low-level feature extraction techniques include
the Canny, Sobel, and Prewitt edge detection techniques. High-level features are basi-
cally based on these low-level features, and they concern Ô¨Ånding shapes in an image
object.
11.2.2
Feature Selection Using 2D PCA
PCA is one of the popular feature extraction techniques. It is used as both a classical
feature extraction and data representation [30] technique. It determines the covariance
structure of a set of variables. Basically, it allows us to identify the principal directions
along which the data vary. In dimensionality reduction, one of the well-known methods
is PCA. The Ô¨Årst N projections consist of the principal component (i.e., they contain the
most variance in an N-dimensional subspace). Hence, PCA maps N-dimensional data
to 1D subspace. However, the main diÔ¨Éculty related to PCA is that it ignores spatial
information (i.e., it considers only the spectral information of the image for reduction).

A Hybrid Approach for Band Selection of Hyperspectral Images
267
According to Wiskott et al. [31], if the training data set does not provide explicit infor-
mation regarding invariance, then PCA could not capture even the simplest invariance.
Here, the 2D image is converted to the 1D vector, and then the covariance matrix is
calculated. But, according to Wiskott et al. [31], PCA cannot capture any small invari-
ance in the training data unless it is explicitly provided. To overcome this problem, Yang
et al. [30] propose an updated version named 2D PCA. It is operated on 2D matrices
rather than 1D vectors. An image covariance matrix can be directly constructed from
the original image data.
Let I be the digital image of size m √ó n and Y be the n-dimensional unitary column
vector. Using the linear transformation given here:
X = IV
(11.1)
we can project image I onto V. A covariance matrix can be calculated on the projected
feature vector of the training sample and can be represented as:
Ci = E(X ‚àíEX)(X ‚àíEX)T
or:
= E[(I ‚àíEI)V][(I ‚àíEI)V]T
(11.2)
Now, the total scatter of the projected vector can be characterized by trace.
So the trace of Ci is:
tr(Ci) = V T(E(I ‚àíEI)T(I ‚àíEI)]V
(11.3)
Let the covariance matrix of the image be:
Ic = E[(I ‚àíEI)T(I ‚àíEI)]
(11.4)
The discriminatory power of the projection vector V can be measured using the follow-
ing formula:
tr(Ci) = V TIcV
(11.5)
A set of optimal projection axes V1, V2, ‚Ä¶ , Vk can be obtained by maximizing equation
(11.4) (i.e., the eigenvector of Ic). Actually, V1, V2, ‚Ä¶ , Vk are the orthonormal eigen-
vectors of Ic corresponding to the Ô¨Årst k largest eigenvalues. For these Ô¨Årst k values, we
have a set of projected feature vectors X1, X2, ‚Ä¶ , Xk. This feature vector set of size m √ó n
represents the feature matrix of the original image I.
11.2.3
Immune Clonal System
An evolutionary search algorithm is a random search technique used in diÔ¨Äerent
optimization problems. Immune clonal systems are inspired by biological immune
systems and belong to the evolutionary search strategy [32]. Nature is an important
source of inspiration for researchers to develop various computational algorithms. The
biological immune system is a robust technique that defends the body from foreign
pathogens. The ultimate target of all immune response is to identify and destroy an
antigen (Ag), which is usually a foreign molecule. Illness-causing microorganism and
viruses are called pathogens. Living organisms like animals, plants, and so on are
exposed to diÔ¨Äerent pathogens. Pathogens are generally antigens. The immune system

268
Hybrid Intelligence for Image Analysis and Understanding
is capable of remembering each infection or antigen so that the next exposure to
the same antigen can be more eÔ¨Éciently and eÔ¨Äectively dealt with. The adaptive or
acquired immune system consists of lymphocytes, mainly B- and T-cells. These cells
are responsible for identifying or destroying any speciÔ¨Åc substances. like antigens. After
stimulation, the immune system generates antibodies to protect the body from the
foreign antigens. The basic mechanism of the biological immune system is a complex
process. At Ô¨Årst after detecting some antigen, antigen presenting cells (APCs) fragment
into antigenic peptide and are joined with major histocompatibility complex (MHC)
molecules. Then T-cells can recognize them as a diÔ¨Äerent peptide‚ÄìMHC combination
and secrete some chemical signals. B-cells respond to this signal and separate the
antigens from the MHC molecule. B-cells then secrete antibodies, and these antibodies
can neutralize or destroy the antigens they found in the MHC molecule. Some B-cells
and T-cells are memory cells to protect the body from any second attempt of the
same antigens. Antibodies on the B-cell perform aÔ¨Énity maturation (i.e., mutation and
editing) to improve their response against antigens. The biological immune system is
brieÔ¨Çy depicted in Figure 11.3, and a corresponding Ô¨Çowchart is given in Figure 11.4. An
immune clonal system or AIS can be used as an unsupervised learning technique. It is
useful since it can maintain diversity in population and uses a global search mechanism.
The
step-by-step
procedure
of
the
CLONALG
algorithm
is
described
in
Algorithm 11.1.
11.2.4
Fuzzy KNN
ClassiÔ¨Åcation of objects is an important part of much research work. KNN is a popular
classiÔ¨Åcation technique. One of the problems with this technique is that each sample of
the sample space gives equal importance to identify the class memberships of any pat-
tern or object, irrespective of their importance [34]. Thus, the fuzzy version of the KNN
is introduced to overcome this shortcoming. The basic idea of the fuzzy KNN (FKNN)
is to assign membership as a function of the object distance from its k-nearest neigh-
bors. It is a sophisticated pattern recognition procedure. The procedure is as follows:
APC
Antigen
Peptide
B-Cells
T-Cells
MHC Protein
Lymphokines
Activated T-Cell
Activated B-Cells
(Plasma Cell)
Figure 11.3 Basic biological immune system [33].

A Hybrid Approach for Band Selection of Hyperspectral Images
269
START
Initialize Population
Split them into memory antibody (m) and rest (r)
Calculate affinity for each antibody of the
population.
Select the highest affinity from the r population and 
clone these antibodies according to the clone rate
Mutate the clonal set by affinity maturation
Calculate affinity value of the clonal set
Replace the lowest affinity antibody in the memory 
by the memory candidate containing highest affinity
Replace the lowest affinity antibody with random 
antibody in the remaining antibody (r).
> max iteration
No
Take memory pool as algorithm result
Figure 11.4 Flowchart of the clonal selection technique.
Let V = ùë£1, ùë£2, ‚Ä¶ , ùë£l be a set of data of length l. Each object ùë£i is deÔ¨Åned by t charac-
teristics ùë£i=(ùë£i1, ùë£i2, ‚Ä¶ , ùë£it). Let y be the unclassiÔ¨Åed elements, N be the set of n-nearest
neighbors, and mi(y) be the membership of y in class i. Let the data set contain c classes.
Hence, the algorithm of FKNN is given in Algorithm 11.2.
Here, class membership can be calculated as:
mi(y) =
n
‚àë
j=1
mij ‚àó(1‚àï||y ‚àíùë£i||2‚àï(m‚àí1))‚àï
n
‚àë
j=1
(1‚àï||y ‚àíùë£i||2‚àï(m‚àí1))
(11.6)

270
Hybrid Intelligence for Image Analysis and Understanding
Algorithm 11.1
Clonal selection algorithm
1. Initialization: In the CLONALG technique, the Ô¨Årst step is initialization [i.e., prepara-
tion of an antibody population (P) of size N]. This population consists of two compo-
nents, a memory part (m) that becomes the solution of the problem and a remaining
part (r) used for diÔ¨Äerent operations of the CLONALG technique.
2. Loop: Each iteration is termed a generation. The algorithm then proceeds by execut-
ing diÔ¨Äerent generations to the antibodies of the predeÔ¨Åned population. The number
of generations or stopping criteria is problem speciÔ¨Åc.
a. Select antibody: A single antibody is selected from the pool of antibody for the
current generation.
b. AÔ¨Énity calculation: AÔ¨Énity values are calculated for all antibodies. AÔ¨Énity cal-
culation is problem dependent.
c. Selection: A set of n antibodies that have the highest aÔ¨Énity value are selected
from the entire antibody pool.
d. Cloning: The set of selected antibodies is then cloned in proportion to their aÔ¨Én-
ity value (highest aÔ¨Énity = more cloning).
e. Mutation: In aÔ¨Énity maturation or mutation, there is modiÔ¨Åcation of a bit string
where the Ô¨Çipping of a bit is achieved by an aÔ¨Énity proportionate probability dis-
tribution. Here, the degree of maturation is inversely proportional to their parent‚Äôs
aÔ¨Énity (i.e., for the highest aÔ¨Énity, the mutation rate is lower).
f. Clone exposure: AÔ¨Énity value is calculated for clones, and they are ranked
accordingly.
g. Candidature: Among the clone antibody, the highest aÔ¨Énity value is selected as
a candidate memory antibody. If the aÔ¨Énity value of the candidate memory anti-
body is greater than the aÔ¨Énity value of the antibody in the memory (m) pool,
then it replaces the said antibody.
h. Replacement: Finally, the d individuals in the remaining r antibody pool with the
lowest aÔ¨Énity are replaced.
3. Finish: After reaching the stopping criteria, the m component of the antibody pool
is taken as a solution. Solutions may be single or multiple based on the problem
domain.
Algorithm 11.2
Fuzzy KNN algorithm
1. Initialize: The number of closest element n.
2. For i = 1:
3. Calculate the distance among y and ùë£i.
4. Check if i <= k, then add ùë£i to N; otherwise, check whether ùë£i is closer to y than any
previously recorded nearest neighbor, and, if true, then delete the farthest neighbor
and include ùë£i in the set N.
5. For each class c, calculate the membership value given in equation (11.6).
6. Repeat steps 2 to 5 until i = t.

A Hybrid Approach for Band Selection of Hyperspectral Images
271
11.3
Proposed Algorithm
In hyperspectral images, selecting the band combination that gives the best result is the
most challenging task. Among a set of B bands, one has to select b number of bands
(B). The proposed semisupervised band selection method can be divided into the fol-
lowing steps: Extract features of each class of the image using 2D PCA, perform the
artiÔ¨Åcial clonal selection algorithm on a randomly chosen population, identify the most
promising bands using classiÔ¨Åcation accuracy, and then rank the bands and obtain the
results. In the proposed band selection technique, at Ô¨Årst, the 2D PCA technique was
used to extract features from the hyperspectral image cube. A ground truth table is used
to obtain the detailed information about each class. Using 2D PCA, we can extract fea-
tures of each class separately. In the second step (i.e., the clonal selection step), a pool
of antibody population was initialized. Algorithm 11.3 clearly depicts the basic mech-
anism of the proposed technique. Figure 11.5 represents the Ô¨Çowchart of the proposed
algorithm. From this pool of antibody, n number of antibody is selected for m-cells and
the rest in r-cells. ClassiÔ¨Åcation accuracy for each class is performed by the FKNN tech-
nique. This accuracy value is considered as an aÔ¨Énity value of the CSA, and antibodies
with the highest accuracy are selected for cloning and mutation. In two successive iter-
ations, no change in m-cells indicates the result of the algorithm.
Algorithm 11.3
Algorithm of the Proposed technique
1. Read the hyperspectral image of P number of diÔ¨Äerent bands and C number of
classes.
2. Extract features of each class using 2D PCA.
3. Initialize the antibody population of size N as given in the CSA.
4. Calculate the classiÔ¨Åcation accuracy of each antibody by fuzzy KNN, and best sets
are separately stored in m-cells.
5. Antibodies in m-cells are selected for clones and perform mutations on the newly
generated clone set.
6. Calculate the aÔ¨Énity of the newly generated antibodies.
7. Antibodies with the highest aÔ¨Énity value are selected for m-cells.
8. Steps 4 to 7 are repeated until stopping criteria are met.
The advantages and disadvantages of diÔ¨Äerent feature selection and classiÔ¨Åcation tech-
niques of hyperspectral images are unevenly distributed. Hence, a combined or hybrid
approach may often provide the best performance.
11.4
Experiment and Result
Performance of the proposed method can be evaluated by performing several experi-
ments on the Indian Pines data set.

272
Hybrid Intelligence for Image Analysis and Understanding
Read hyperspectral image consists of R bands
Initialize population (P) 
Calculate feature value of each antibody of
the population using 2DPCA 
Calculate classification accuracy using Fuzzy-
KNN which is considered as a affinity value
for clonal selection algorithm
Clonal Selection algorithm
Antibody with highest affinity value is
selected
Figure 11.5 Flowchart of the proposed
technique.
11.4.1
Description of the Data Set
Indian Pines data set: The most commonly used data set is the Indian Pines data set
obtained by the AVIRIS. In 1992, it was instrumented over the agricultural area of north-
western Indiana‚Äôs Indian pine. The spatial dimension of Indian pine is 145 √ó 145, and it
has 224 spectral bands. The data has been captured within the spectral range of 400 to
2500 nm, and the spatial and spectral resolution are 20 m per pixel and 10 nm respec-
tively. Irrelevant bands like four zero bands and the 35 lower SNR bands aÔ¨Äected by
atmospheric absorption are discarded. Among the lower SNR bands, water absorption
bands (104‚Äì108, 150‚Äì163, 220) and noisy bands (1‚Äì3, 103, 109‚Äì112, 148‚Äì149, 159,
164‚Äì165, 217‚Äì219) are there. The remaining 185 bands are preserved. The data set con-
tains 16 classes and also 10,366 labeled pixels, as depicted in Table 11.1. The ground truth
map as represented in Figure 11.6 contains 16 mutually exclusive land-cover classes.

A Hybrid Approach for Band Selection of Hyperspectral Images
273
Table 11.1 Indian Pines data set: classes with number of samples
Class
Land type
No. of samples
C1
Alfalfa
52
C2
Corn
224
C3
Buildings-grass-trees-drives
380
C4
Grass-pasture-mowed
20
C5
Corn-min till
734
C6
Corn-no till
1234
C7
Hay-windrowed
486
C8
Grass/pasture
495
C9
Grass/trees
746
C10
Soybean-min till
2408
C11
Oats
19
C12
Soybean-no till
898
C13
Soybean-clean till
610
C14
Woods
1290
C15
Stone-steel-towers
90
C16
Wheat
210
(a)
Alfalfa
Corn-notill
Corn-mintill
Corn
Grass-pasture
Grass-trees
Grass-pasture-mowed
Hay-windrowed
Oats
Soybean-notill
Soybean-mintill
Soybean-clean
Wheat
Woods
Building-grass-trees-drives
Stone-steel-towers
(b)
Figure 11.6 Indiana Pines image. (a) Indiana Pines image. (b) Ground truth.

274
Hybrid Intelligence for Image Analysis and Understanding
Pavia Center data set: The Pavia data set was acquired by the ReÔ¨Çective Optics Spec-
trographic Imaging System 03 (ROSIS-03) sensor. It was instrumented over the center of
Pavia, Italy. The spatial dimension of Pavia is 1096 √ó 1096 pixels, and it has 115 spectral
bands. Removing a 381-pixel-wide black stripe in the left part of the image resulted in
an image with 1,096,715 pixels. Irrelevant bands like the four lower SNR bands aÔ¨Äected
by atmospheric absorption are discarded. Among the lower SNR bands, 13 noisy bands
are there. The remaining 102 bands are preserved. The data set contains nine classes.
The ground truth map as represented in Figure 11.7 contains nine land-cover classes.
Table 11.2 lists the land-cover classes with the corresponding number of samples.
11.4.2
Experimental Details
DiÔ¨Äerent experiments are conducted on the Indian Pines data set, as described in
the next section. Details of the data set are given in the previous section. A 2D PCA
Asphalt
Meadows
Gravel
Trees
Painted metal sheets
Bare Soil
Bitumen
Self-Blocking Bricks
Shadows
(a)
(b)
Figure 11.7 Pavia image. (a) Three-band color composite image. (b) Ground truth.

A Hybrid Approach for Band Selection of Hyperspectral Images
275
Table 11.2 Pavia data set: classes with number of samples
Class
Land type
No. of samples
C1
Water
65,971
C2
Trees
7598
C3
Asphalt
3090
C4
Self-blocking bricks
2685
C5
Bitumen
6584
C6
Tiles
9248
C7
Shadows
7287
C8
Meadows
42,826
C9
Bare soil
2863
technique was used to extract features from each band of the original image. The total
number of the antibody population is N, and the total number of selected antibodies
in each population is n(n = 5 to 20). We apply 2D PCA on the selected antibody and
calculate the classiÔ¨Åcation accuracy using FKNN. This value becomes the aÔ¨Énity value.
Now, based on these aÔ¨Énity values, select the antibody with the highest value. Perform
the clonal selection technique to Ô¨Ånd the best antibody. The result of the proposed
antibody is reported in Table 11.3 and 11.4. In our experiment, we varied the number of
selected bands from 5 to 30, and overall accuracy is reported in Figure 11.8. However,
the optimum number of selected bands will be addressed by the future scope of our
work. Comparisons of diÔ¨Äerent methods with the proposed method for diÔ¨Äerent
data sets are represented in Figure 11.9 and 11.10, which depict the overall accuracy
(in percentage) with the number of selected bands. Experimental results show that
the proposed method provides convincing results in terms of accuracy for diÔ¨Äerent
numbers of selected bands. Table 11.4 shows the list of selected bands with diÔ¨Äerent k
values using our proposed method.
11.4.3
Analysis of Results
The performance of the proposed method for band selection is compared with other
well-known methods. Among them, the paper presented in [35] is a competitive
Table 11.3 Selected bands for Indian Pines data set obtained by the proposed method
Band number
Selected band
k = 5
16, 42, 71, 138, 181
k = 10
4, 9, 19, 54, 55, 102, 138, 143, 172, 184
k = 15
8, 17, 19, 30, 31, 44, 58, 69, 85, 100, 124, 127, 128, 135, 171
k = 20
4, 8, 16, 18, 24, 31, 32, 35, 44, 46, 67, 80, 82, 87, 117, 124, 127, 135, 171, 184
Table 11.4 Accuracy of classiÔ¨Åcation using the algorithm
Band number
5
7
9
11
13
15
17
19
20
Overall accuracy
83.44
83.81
84.23
84.66
85.21
85.65
86.23
86.12
86.03

276
Hybrid Intelligence for Image Analysis and Understanding
86.5
86
85.5
85
84.5
Overall accuracy
84
83.5
83
5
10
Band number
15
20
Figure 11.8 Overall accuracy of the proposed algorithm on the Indiana data set.
90
85
80
75
70
65
605
10
15
Number of Selected Bands
20
25
30
Overall Accuracy (%)
All Bands
WaLuDl
TMI
MI
Proposed Method
Figure 11.9 Comparison of the proposed method with MI, WaLuDi, and TMI in terms of overall
accuracy for the Indiana data set.
algorithm based on the mutual information (MI) between a band and the reference
image of the data set. More MI means more information content in the band with
respect to the reference map. Thus, redundancy can be minimized. The next method is
a semisupervised method [17]. Trivariate MI (TMI) is used to measure the trivariate
correlation among two bands and the reference image. Semisupervised TMI (STMI)
is also used, as desired prior information about classes in a hyperspectral image is
not available sometimes. A CSA-based search strategy is used to optimize the result.
Another technique is feature clustering [27]. The wards linkage strategy using diver-
gence (WaLuDi) is a feature-clustering technique where the Ô¨Ånalized cluster is further
used for band selection. The optimal number of selected bands is diÔ¨Écult to identify
as it varies from image to image. In the present chapter, the comparisons carried

A Hybrid Approach for Band Selection of Hyperspectral Images
277
90
80
70
60
50
5
10
15
Number of Selected Bands
20
25
30
Overall Accuracy (%)
All Bands
WaLuDl
TMI
MI
Proposed Method
Figure 11.10 Comparison of the proposed method with MI, WaLuDi, and TMI in terms of overall
accuracy for the Pavia Center data set.
out for diÔ¨Äerent numbers of selected bands for each method are between n(n = 5
to 30). Figure 11.9 and 11.10 depict the comparison results of the above-mentioned
method with our proposed method. The graph shows the average overall accuracy (in
percentage) versus the number of bands. Experimental results justify the eÔ¨Äectiveness
of the proposed technique. The proposed method (i.e., the fuzzy KNN classiÔ¨Åer) is
compared with another well-known classiÔ¨Åer named the support vector machine
(SVM) [36]. SVMs are based on linear discriminant functions. Thus, the classiÔ¨Åcation is
based on a function of the form ùë§tX + b, where ùë§and b are learned from training data.
For a linearly separable two-class problem, the positive class can be characterized by a
hyperplane ùë§tX + b = 1, and the negative class can be characterized by ùë§tX + b = ‚àí1.
The decision boundary for the separating plane is characterized by ùë§tX + b = 0.
Figure 11.11 depicts this idea clearly. The graph presented in Figure 11.12 shows
the result obtained using the proposed method with a SVM. The x-axis represents
the overall accuracy of the diÔ¨Äerent classiÔ¨Åer obtained by the Indiana image, and the
y-axis represents a diÔ¨Äerent number of bands. The proposed method is also compared
margin
margin
Figure 11.11 Basics of a SVM.

278
Hybrid Intelligence for Image Analysis and Understanding
20
19
17
15
13
11
9
7
5
80
82
84
86
88
Overall Accuracy
proposed
Overall Accuracy SVM
Figure 11.12 Comparison of the proposed method (2D PCA and fuzzy KNN) with 2D PCA and SVM in
terms of overall accuracy for the Indiana data set.
20
19
17
15
13
11
9
7
5
76
78
80
82
84
86
88
Overall Accuracy
Proposed
Overall Accuracy LDA
Figure 11.13 Comparison of the proposed method (2D PCA) and LDA in terms of overall accuracy for
the Indiana data set.
with another feature extraction technique, LDA. Fisher‚Äôs linear discriminant is used
in supervised classiÔ¨Åcation. It is used to map n-dimensional data to 1D data using
a projection vector. ClassiÔ¨Åcation is then carried out in this 1D data. The graph
represented in Figure 11.13 shows the result obtained from a 2D PCA (proposed) and
LDA classiÔ¨Åer.
11.5
Conclusion
In this chapter, a novel semisupervised hybrid band selection technique is presented.
This hybrid technique is a combination of 2D PCA and clonal selection algorithm (CSA).

A Hybrid Approach for Band Selection of Hyperspectral Images
279
Basically, to remove redundant bands, this technique calculates the feature information
on each band and selects the most informative bands using a CSA. The CSA uses the
fuzzy KNN technique to identify the most informative bands. The overall accuracy mea-
sure has been incorporated to measure the performance of the proposed method. This
method shows a convincing result in comparison with other well-known methods like
MI, WaLuDi, and TMI for diÔ¨Äerent data sets of hyperspectral images, namely, Indiana
and Pavia Center. A hybrid combination of 2D PCA with fuzzy KNN gives slightly bet-
ter performance than a 2D PCA and SVM combination. Comparison of the proposed
method with the Bayesian belief network and hidden Markov model (HMM) in place
of fuzzy KNN is left for future investigation. In the proposed algorithm, the number
of selected bands varies from 5 to 30, but the number of selected bands for optimum
results is left for future research to determine.
References
1 Ayadi, M.F. and Kamel, M.S. (2011) Survey on speech emotion recognition fea-
tures,classiÔ¨Åcation schemes and databases. Pattern Recognition, 44 (3), 572‚Äì587.
2 Hazra, J. and Roy Chowdhury, A (2013) An approach for determining angle of rota-
tion of a gray image using weighted statistical regression. International Journal of
ScientiÔ¨Åc and Engineering Research, 4 (8), 1006‚Äì1013.
3 Hazra, J. and Roy Chowdhury, A. (2014) Statistical regression based rotation estima-
tion technique of color image. International Journal of Computer Applications, 102
(15), 1‚Äì4.
4 Hazra, J. and Roy Chowdhury, A. (2015) Transformation parameter estimation using
parallel output based neural network. Applied Soft Computing, 46, 868‚Äì874.
5 Gonzalez, R.E. and Woods, R.F. (2002) Digital image processing, 2nd ed. Prentice
Hall, Upper Saddle River, NJ.
6 Aviris‚ÄìAirborne Visible/Infrared Imaging Spectrometer. Available from http://aviris
.jpl.nasa.gov.
7 Green, A. A., Berman, M. and Craig, M.D. (Jan. 1988) A transformation for ordering
multispectral data in terms of image quality with implications for noise removal.
IEEE Trans. Geosci. and Remote Sens., 26 (1), 65‚Äì73.
8 Lee, J. B. and Berman, M. (May 1990) Enhancement of high spectral resolution
remote-sensing data by a noise-adjusted principal components transform. IEEE
Trans. Geosci. and Remote Sens, 28 (3), 295‚Äì304.
9 Chang, C.I, Du, T.L.S. and Althouse, M.L.G. (Jun. 1999) A joint band prioritiza-
tion and band decorrelation approach to band selection for hyperspectral image
classiÔ¨Åcation. IEEE Trans. Geosci. and Remote Sens., 37 (6), 2631‚Äì2641.
10 Harsanyi, J.C. and Chang, C.I. (Jul. 1994) Hyperspectral image classiÔ¨Åcation and
dimensionality reduction: an orthogonal subspace projection approach. IEEE Trans.
Geosci. and Remote Sens, 32 (4), 779‚Äì785.
11 De Backer, S., Kempeneers, P. and Scheunders, P. (Jul. 2005) A band selection tech-
nique for spectral classiÔ¨Åcation. IEEE Geosci. and Remote Sens. Lett., 2 (3), 319‚Äì323.
12 Kumar, S. and Crawford, M.M. (Jul. 2001) Best-bases feature extraction algorithms
for classiÔ¨Åcation of hyperspectral data. IEEE Trans. Geosci. and Remote Sens., 39 (7),
1368‚Äì1379.

280
Hybrid Intelligence for Image Analysis and Understanding
13 Lashkia, G. and Anthony, L. (Apr. 2004) Relevant, irredundant feature selection
and noisy example elimination. IEEE Trans. Syst., Man, Cybern. B, Cybern., 34 (2),
888‚Äì897.
14 Shannon, E. (Jul.-Oct. 1948) A mathematical theory of communication. Bell Syst.
Tech. J., 27 (3), 379‚Äì423.
15 Huang, R. and He, M. (Apr. 2005) Band selection based on feature weighting for
classiÔ¨Åcation of hyperspectral data. IEEE Geosci. Remote Sens. Lett., 2 (2), 156‚Äì159.
16 Zhang, L. and Zhong, Y. (Dec. 2007) Dimensionality reduction based on clonal selec-
tion for hyperspectral imagery. IEEE Geosci. Remote Sens. Lett., 45 (12), 4172‚Äì4186.
17 Jie Feng, L.C., Jiao, X.Z. and Sun, T. (July 2014) Hyperspectral band selection based
on trivariate mutual information and clonal selection. IEEE Trans. Geosci. and
Remote Sens., 52 (7), 4092‚Äì4105.
18 Bajcsy, P. and Groves, P. (2004) Methodology for hyperspectral band selection.
Photogrammetric Engineering and Remote Sensing, 70, 793‚Äì802.
19 Tan, K. and Du, P. (2011) Combined multi-kernel support vector machine and
wavelet analysis for hyperspectral remote sensing image classiÔ¨Åcation. Chinese Optics
Letters, 9 (1), 011 003‚Äì110 006.
20 Du, Q. and Yang, H. (2008) Similarity-based unsupervised band selection for hyper-
spectral image analysis. IEEE Geosci. Remote Sens. Lett., 5 (4), 564‚Äì568.
21 Yang, H. and Chen, G. (2011) Unsupervised hyperspectral band selection using
graphics processing units. IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens., 4 (3),
660‚Äì668.
22 Cariou, C. and Chehdi, K. (2011) Bandclust: An unsupervised band reduction
method for hyperspectral remote sensing. IEEE Geosci. Remote Sens. Lett., 8 (3),
565‚Äì569.
23 Lei, L., Prasad, S. and Bruce, L.M. (Apr. 2012) Localitypreserving dimensionality
reduction and classiÔ¨Åcation for hyperspectral image analysis. IEEE Trans. Geosci.
Remote Sens., 50 (4), 1185‚Äì1198.
24 Agarwal, A., El-Ghazawi, T. and Le-Moigne, J. (2007) EÔ¨Écient hierarchical-PCA
dimension reduction for hyperspectral imagery. Proc. IEEE Int. Symp. Signal Process.
Inf. Technol., 353‚Äì356.
25 Roweis, S.T. and Saul, L.K. (Dec. 2000) Nonlinear dimensionality reduction by locally
linear embedding. Science, 290 (22), 2323‚Äì2326.
26 Chang, C.I. and Wang, S. (Jun. 2006) Constrained band selection for hyperspectral
imagery. IEEE Trans. Geosci. Remote Sens., 44 (6), 1575‚Äì1585.
27 Mart√≠nez-Us√≥, A., Pla, F. and Garc√≠a-Sevilla, P. (Dec. 2007) Clustering based hyper-
spectral band selection using information measures. IEEE Trans. Geosci. Remote
Sens., 45 (12), 4158‚Äì4171.
28 Mart√≠nez-Us√≥, A., Pla, F. and Garc√≠a-Sevilla, P. (Apr. 2012) Clustering based hyper-
spectral band selection using information measures. IEEE Trans. Geosci. Remote
Sens., 5 (2), 531‚Äì543.
29 Mitra, P. and Pal, S. (2002) Unsupervised feature selection using feature similarity.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 24 (3), 301‚Äì312.
30 Yang, J., Zhang, D. and Yu Yang, J. (2004) Two-dimensional PCA: a new approach to
appearance-based face representation and recognition. IEEE Transaction on Pattern
Analysis and Machine Intelligence, 26 (1), 131‚Äì137.

A Hybrid Approach for Band Selection of Hyperspectral Images
281
31 Wiskott, L., Fellous, J.M. and von der Malsburg, C. (1997) Face recognition by
elastic bunch graph matching. IEEE Transaction on Pattern Analysis and Machine
Intelligence, 19 (7), 775‚Äì779.
32 D. Castro, L.N. and von Zuben, F.J. (2000) The clonal selection algorithm with
engineering application. GECCO‚Äô00-Workshop Proceedings, pp. 36‚Äì37.
33 Hazra, J. and Roy Chowdhury, A. (Nov. 2015) Immune based feature selection in
rigid medical image registration using supervised neural network. Handbook of
Research on Advanced Hybrid Intelligent Techniques and Applications, pp. 551‚Äì581.
34 Keller, J. and Gray, M. (1985) A fuzzy k-nearest neighbor algorithm. IEEE Transac-
tion on System, Man, Cybernetics, 15 (4), 580‚Äì585.
35 Guo, B. and Gunn, R.D.J.N. (2006) Band selection for hyperspectral image classiÔ¨Å-
cation using mutual information. IEEE Geoscience and Remote Sensing Letters, 3 (4),
522‚Äì526.
36 Kolekar, M.H. and Dash, D.P. (2015) A nonlinear feature based epileptic seizure
detection using least square support vector machine classiÔ¨Åer. IEEE Region 10
Conference TENCON.

283
12
Uncertainty-Based Clustering Algorithms for Medical Image
Analysis
Deepthi P. Hudedagaddi and B.K. Tripathy
School of Computer Science and Engineering (SCOPE), VIT University, Vellore, Tamil Nadu, India
12.1
Introduction
Image segmentation frames the basic and crucial step in pattern recognition and analysis
of the image. It includes segregating the image in accordance with a few characteristics
like intensity and texture [1]. In recent years, the fuzzy c-means (FCM) clustering algo-
rithm and its variations have been extensively used [2, 3]. In comparison with the hard
c-means algorithm, FCM has been proved to give better and optimum results. Procur-
ing several segmentation results of the image helps in analyzing an image in several
perspectives. In this regard, techniques of parallel processing have to be exploited.
Images can be deÔ¨Åned as 2D coordinate representations of pixels. Image processing is
a technique used in the early part of the 20th century. The study of image segmentation
plays a vital role in image analysis and has attracted several researchers. Imprecision
in data occurs due to various reasons. One has to pay a heavy cost if he or she tries to
remove imprecision before any kind of processing. This is true for any kind of data, and
image data is no exception. It has been established of late that hybrid techniques are
more eÔ¨Écient than the individual ones. So, while considering segmentation, we focus
on hybrid techniques obtained from uncertainty-based models. Several techniques, like
Otsu threshold and performance metrics such as statistical methods, PSNR and RMSE,
and the David Bouldin (DB) index and Dunn (D) index, are used for measuring the eÔ¨É-
ciency of diÔ¨Äerent image segmentation algorithms.
12.2
Uncertainty-Based Clustering Algorithms
Taking Zadeh‚Äôs fuzzy sets and Pawlak‚Äôs rough sets [4], Attanassov [5] came up with
intuitionistic fuzzy sets; Dubois and Prade came up with rough fuzzy sets. Accord-
ingly, clustering algorithms have found an evolution in this direction. FCM [6], rough
c-means (RCM) [7, 8], hybrid variations like rough FCM (RFCM) [9‚Äì11], and intuition-
istic FCM (IFCM) [5] have been developed. From literature, it can be observed that
hybrid algorithms that deal with vagueness and uncertainty have better performance
when compared with several indices [12].
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

284
Hybrid Intelligence for Image Analysis and Understanding
Segmenting images into regions is a clustering or classifying process. It simpliÔ¨Åes
image representation and makes it more understandable for analysis. The process can be
conventional techniques (crisp) and uncertainty-based models (soft). Some of the con-
ventional techniques used are Canny, Sobel, Prewitt, Robert, and hard c-means, where
vagueness and uncertainty are not distinguished, and which have been well considered
by uncertainty models in the artiÔ¨Åcial intelligence domain.
As vagueness and uncertainty concepts are the most suitable that have not been dis-
tinguished properly, they need to be controlled for image analysis.
Cluster analysis is a major aspect in data mining. It has found applications in Ô¨Åelds
such as web mining, biology, image processing, and market segmentation. Clustering
involves segregating samples in a way that the samples are extremely similar to one
another in the same cluster and unidentical to ones in the other.
C-means is a traditional crisp clustering algorithm where every object belongs to a
single cluster, thereby providing a quick rate of convergence. However, it fails in handling
overlapping. Since most real-world data is uncertain and has boundaries overlapping,
it paved the way for developing uncertainty-based clustering algorithms. These have
been successful in meeting the needs of a particular scenario by being open to minor
modiÔ¨Åcations.
12.2.1
Fuzzy C-Means
FCM was introduced by James C. Bezdek. Fuzzy, or soft, clustering allows an object
to belong to more than one cluster. These come with a set of membership values that
provide an insight to the level of membership to a particular cluster.
1. Allocate initial cluster centers.
2. Compute Euclidean distance dik from data elements xk and centroids ùë£i:
d(x, y) =
‚àö
(x1 ‚àíy1)2 + (x2 ‚àíy2)2 + ¬∑ ¬∑ ¬∑ + (xn ‚àíyn)2
(12.1)
3. Generate membership matrix U:
If dij > 0, then
ùúáik =
1
‚àëC
j=1
(
dik
djk
)
2
m‚àí1
(12.2)
Else
ùúáik = 1
4. Centroids are computed with
Vi =
‚àëN
j=1 (ùúáij)mxj
‚àëN
j=1 (ùúáij)m
(12.3)
5. Compute new U using steps 2 and 3.
6. If ||U(r) ‚àíU(r+1)|| < ùúñ, then stop. If not, redo from step 4.

Uncertainty-Based Clustering Algorithms for Medical Image Analysis
285
12.2.2
Rough Fuzzy C-Means
RFCM [13] was developed by S. Mitra and P. Maji; with the combination of fuzzy and
rough set concepts. The property of rough sets to deal with uncertainty, incompleteness,
and vagueness along with a fuzzy set concept of membership that evaluates overlapping
clusters have been used in RFCM.
1. Assign ùë£i for c clusters.
2. Calculate ùúáik using equation (12.2).
3. Let ùúáik be maximum and ùúájk be next-to-maximum membership values of object xk.
If ùúáik ‚àíùúájk < ùúñ, then
xk ‚ààBUi and xk ‚ààBUj, and xk shall not belong to lower approximation.
Else
xk ‚ààBUi
4. Compute new cluster means with:
Vi =
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™‚é©
ùë§loùë§
‚àë
xk‚ààBUixk
|BUi|
+ ùë§up
‚àë
xk‚ààBUi‚àíBUiùúáik
mxk
‚àë
xk‚ààBUi‚àíBUiùúáikm
if |BUi| ‚â†‚àÖand |BUi ‚àíBUi| ‚â†‚àÖ
‚àë
xk‚ààBUi‚àíBUiùúáik
mxk
‚àë
xk‚ààBUi‚àíBUiùúáikm
if |BUi| = ‚àÖand |BUi ‚àíBUi| ‚â†‚àÖ
‚àë
xk‚ààBUixk
|BUi|
ELSE
(12.4)
5. Redo from step 2 till termination criteria are obtained.
Note: Range for values of m is [1.5, 2.5]. However, for all practical purposes, it is taken
to be 2.
12.2.3
Intuitionistic Fuzzy C-Means
The IFCM proposed by T. Chaira includes a component called hesitation value (ùúã),
which increases clustering accuracy.
1. Initial cluster centers are assigned.
2. Compute Euclidean distance dik between data objects xk and centroids ùë£i with
equation (12.1).
3. Generate membership matrix U:
If dij > 0, then compute ùúáik using equation (12.2)
Else
ùúáik = 1
4. Compute the hesitation matrix ùúã.

286
Hybrid Intelligence for Image Analysis and Understanding
5. Compute the modiÔ¨Åed matrix U‚Ä≤ with
ùúá‚Ä≤
ik = ùúáik + ùúãik
(12.5)
6. Centroids are computed using
Vi =
‚àëN
j=1 (ùúá‚Ä≤
ij)mxj
‚àëN
j=1 (ùúá‚Ä≤
ij)m
(12.6)
7. Calculate new U using steps 2 to 5.
8. If ||U‚Ä≤(r) ‚àíU‚Ä≤(r+1)|| < ùúñ, then stop. If not, redo from step 4.
12.2.4
Rough Intuitionistic Fuzzy C-Means
1. Initial c clusters are assigned [14].
2. Calculate Euclidean distance dik using equation (12.1).
3. Compute U matrix.
If dik = 0 or xj ‚ààBUi, then
ùúáik = 1
Else compute ùúáik using equation (12.2)
4. Compute ùúãik.
5. Compute ùúá‚Ä≤
ik.
ùúá‚Ä≤
ik = ùúáik + ùúãik
6. Let ùúá‚Ä≤
ik be maximum and ùúá‚Ä≤
jk be next-to-maximum membership values of object xk.
If ùúá‚Ä≤
ik ‚àíùúá‚Ä≤
jk < ùúñ, then
xk ‚ààBUi and xk ‚ààBUj and xk shall not be member of any lower approximation.
Else
xk ‚ààBUi
7. Compute new cluster means with Equation (12.4).
12.3
Image Processing
Image processing involves performing operations on an image by converting it into dig-
ital format. This is a rapidly growing Ô¨Åeld Ô¨Ånding its applications in various aspects of
diagnosis of disease, weather forecasting, classiÔ¨Åcation of areas, and so on. It has been
a core research area and has witnessed large-scale developments in varied Ô¨Åelds. It is
characterized by three important steps: obtaining the image, image manipulation, and
recognizing patterns.
The medical Ô¨Åeld is facing challenges in diagnosis of diseases. It has become a diÔ¨Écult
and time-consuming process. Image-processing techniques have been applied to
preprocess any digital image, and soft computing methods are applied to cluster images
to distinguish between vagueness and uncertainty that exist in real-time environments.
Thereby, investigations are carried out to process digital images for enhancement
of quality, which is segmented based on uncertainty algorithms for reconstruction

Uncertainty-Based Clustering Algorithms for Medical Image Analysis
287
of images. In turn, the reconstructed image is visualized in 3D map generation for
interpretation of images for various purposes of medical and satellite applications with
minimum time complexity and improvised performance.
12.4
Medical Image Analysis with Uncertainty-Based
Clustering Algorithms
Several clustering algorithms presume distinction among clusters such that one pat-
tern belongs to one cluster. With the numerous medical images available, handling and
analyzing them in computational ways have become the need of the hour. Image seg-
mentation algorithms have a major role to play in biomedical applications like tissue
volume quantifying, diagnosis, anatomy, and computer-integrated surgeries.
With the advancement and advantages of magnetic resonance imaging (MRI) over
other diagnostic imaging, it has been a practice of several researchers to use them for
their study [15‚Äì18]. Of these, fuzzy techniques have better performance as they retain
much information from the original image. FCM assigns pixels to clusters without
labeling. But, due to the spatial intensity inhomogeneity induced by the radiofrequency
coil in MR images, conventional FCM has not been eÔ¨Écient. The inhomogeneity
issue of the images is easily dealt with by modeling the image and with the use of
multiplier Ô¨Åelds. Of late, researchers have begun using spatial information to obtain
better segmentation results. Tolias and Panas [19] developed a rule-based fuzzy
technique to impose spatial continuity, and in [20], a small constant was used to change
membership of the cluster center. Pham et al. [21] made changes to the objective
function of FCM by introducing a multiplier Ô¨Åeld. In the same manner, Ahmed et al.
developed an algorithm to label a pixel by considering its immediate neighbor. Of late,
Pham made FCM‚Äôs objective function to constrain the behavior of the membership
functions.
Several clustering algorithms like FCM, RCM, IFCM, RFCM, RIFCM, spatial FCM,
and spatial IFCM [12‚Äì14, 22] have been developed to date, which have been further
extended to both kernel-based and possibilistic versions such that their eÔ¨Éciency is
increased and they can be applied to a greater number of applications. FCM with respect
to adaptive clustering has already been developed. Though FCM is powerful in several
ways, it fails to determine the right number of clusters for pattern classiÔ¨Åcation. It needs
the user to specify the number of clusters.
12.4.1
FCM with Spatial Information for Image Segmentation
Chuang et al. [23] introduced a FCM segmentation technique that reduced noise eÔ¨Äect
and made the clustering more homogeneous. Usually, in conventional FCM, a noisy
pixel is misclassiÔ¨Åed. They incorporated spatial information that recomputed member-
ship value, and hence the degree of a pixel belonging to a cluster was altered.
They used the concept of correlation among neighboring pixels. This means the prob-
ability that neighboring pixels belong to the same cluster is great. This concept has not
been used in standard FCM algorithms. A spatial function is deÔ¨Åned in equation (12.1).
The clustering process is a two-step process. The initial step is similar to FCM in cal-
culating membership function. The second step includes the membership value of each

288
Hybrid Intelligence for Image Analysis and Understanding
pixel computed from spatial domain and spatial function. FCM continues with new val-
ues that are used with spatial function. When the diÔ¨Äerence between two cluster centers
with two iterations is less than threshold, iteration can be stopped.
They evaluated their results based on cluster validity functions. Partition coeÔ¨Écient
Vpc and partition entropy Vpe signify the fuzzy partitions. The partitions that have less
fuzziness indicate better performance. Higher Vpc value and lesser Vpe value indicate
better clustering.
Vpc =
‚àëN
j
‚àëc
i u2
ij
N
(12.8)
and
Vpe = ‚àí
‚àëN
j
‚àëc
i[uij log uij]
N
(12.9)
A good clustering result provides pixels that are compact within a single cluster
and pixels that are diÔ¨Äerent between diÔ¨Äerent clusters. Minimizing Vxb leads to good
clustering. Application of sFCM on MRI image is shown in Figure 12.1.
Vxb =
‚àí‚àëN
j
‚àëc
i uij?xj ‚àíùë£i?2
N(mini‚â†k{||ùë£k ‚àíùë£i||2})
(12.10)
(a)
(c)
(b)
Figure 12.1 MRI image segmentation using (a) FCM, (b) sFCM1,1, and (c) sFCM1,2.

Uncertainty-Based Clustering Algorithms for Medical Image Analysis
289
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 12.2 Segmentation results on (a) original image, (b) same image with mixed noise, results of
(c) FCM_S1, (d) FCM_S2, (e) EnFCM, (f) FGFCM_S1, (g) FGFCM_S2, and (h) FGFCM.
Figure 12.3 MRI image ‚Äì speckle noise.

290
Hybrid Intelligence for Image Analysis and Understanding
(a)
(c)
(d)
(e)
(f)
(g)
(b)
Figure 12.4 Noisy image segmentation. (a) FCM, (b) sFCM1,1, (c) sFCM1,2, (d) sFCM2,1, (e) sIFCM1,1,
(f) sIFCM1,2, and (g) sIFCM2,1.
12.4.2
Fast and Robust FCM Incorporating Local Information for Image
Segmentation
Cai et al. [24] found FCM with spatial constraints (FCM_S) provide better results for
image segmentation. They also found the disadvantages that come along, like: insensitiv-
ity to noise, robustness to noise, and time for segmentation. They have worked on using
local spatial and gray information together to frame a fast generalized FCM (FGFCM).
This overcomes the drawbacks of FCM_S and provides better performance with respect
to time and segmentation. They have also introduced special cases such as FGFCM_S1
and FGFCM_S2. The characteristics of these are: (1) factor Sij as both a spatial and gray
similarity measure to guarantee noise immunity for images, and (2) fast clustering, that

Uncertainty-Based Clustering Algorithms for Medical Image Analysis
291
is, segmenting time is dependent on the number of gray levels q rather than the size
N(>q) of the image. Also, computational complexity is reduced from O(NcI1) to O(qcI2),
where c indicates the number of clusters, and I1 and I2 indicate the number of iterations.
The experiments of FGFCM on synthetic and real-world images in Figure 12.2 prove it
to be eÔ¨Äective and eÔ¨Écient [25, 26].
12.4.3
Image Segmentation Using Spatial IFCM
Tripathy et al. [27] developed the IFCM with spatial information (sIFCM).This was an
extension to Chuang‚Äôs work. The algorithm is provided in detail in [27].
They used DB and D indices to measure the cluster quality in addition to the evalu-
ation metrics used by Chuang et al. [8, 9]. Speckle noise of mean 0 and variance 0.04
was induced in the image. FCM and sFCM were applied to the image. Vpc and Vpe are
calculated.
The DB index is deÔ¨Åned as the ratio of the sum of within-cluster distance to
between-cluster distance.
DB = 1
c
c‚àë
i=1
maxk‚â†i
{S(ùë£i) + S(ùë£k)
d(ùë£i, ùë£k)
}
for 1 < i, k < c
(12.11)
It aims to minimize within-cluster distance and maximize intercluster separation.
Therefore, a good clustering procedure should have low DB value.
The D index is used for identiÔ¨Åcation of clusters that are compact and separated.
Dunn = mini
{
mink‚â†i
{ d(ùë£i, ùë£k)
maxl S(ùë£l)
}}
for 1 < k, i, l < c
(12.12)
It aims at maximizing the intercluster distance and minimizing the intracluster dis-
tance. Hence, a higher D value proves to be more eÔ¨Écient.
A brain MRI image of 225 √ó 225 dimensions was used for proving their results.
The number of clusters c = 3. Results of MRI imaging with speckle noise is shown in
Figures 12.3 and 12.4.
The results are distinct in images with speckle noise. Conventional FCM misclassiÔ¨Åes
spurious blobs and spots. Increasing the parameter q, which is the degree of the spatial
function, modiÔ¨Åes the membership function to accommodate spatial information to a
greater degree, and produces better results. The table 12.1 shows the performance of the
diÔ¨Äerent techniques applied on the noisy image.
Table 12.1 Cluster evaluation results on speckle noise image
Method
Results on the noisy image
Vpc
Vpe
DB
D
FCM
0.6975
2.8195 √ó 10‚àí4
0.4517
3.4183
sFCM1,1
0.7101
5.9541 √ó 10‚àí9
0.4239
3.6734
sFCM2,1
0.6922
7.7585 √ó 10‚àí12
0.4326
3.4607
sFCM1,2
0.6874
4.2711 √ó 10‚àí12
0.4412
3.6144
sIFCM1,1
0.7077
1.1515 √ó 10‚àí08
0.4254
3.6446
sIFCM2,1
0.7135
8.1312 √ó 10‚àí13
0.4276
3.7472
sIFCM1,2
0.713
4.6770 √ó 10‚àí13
0.4393
3.4968

292
Hybrid Intelligence for Image Analysis and Understanding
Table 12.2 Performance indices of sFCM on leukemia image
Index
FCM
sFCM2,1
sFCM1,1
sFCM1,2
Vpc
0.2118
0.2300
0.2219
0.4963
Vpe
0.0178
2.05 √ó 10‚àí6
0.0003
7.69 √ó 10‚àí8
Vxb
0.0168
0.0060
0.0074
0.0522
DB
0.4670
0.4185
0.4197
0.4173
D
2.2254
2.9951
2.9318
3.4340
Table 12.3 Performance indices of sIFCM on leukemia image
Index
FCM
IFCM
sIFCM1,1
sIFCM1,2
sIFCM2,1
Vpc
0.2118
0.2074
0.4762
0.2241
0.2281
Vpe
0.0178
0.02585
3.56E-005
1.68E-005
5.90E-006
Vxb
0.0168
0.02096
0.05929
0.0069
0.0063
DB
0.467
0.4906
0.4126
0.4208
0.4188
D
2.2254
2.0529
3.5866
3.0203
2.9655
(a)
(b)
(d)
(c)
(e)
Figure 12.5 (a) Original image. Segmented images of leukemia using (b) FCM, (c) sFCM2,1, (d) sFCM1,1,
and (e) sFCM1,2.
12.4.3.1
Applications of Spatial FCM and Spatial IFCM on Leukemia Images
Deepthi et al. [28, 29] have applied the spatial clustering algorithms to leukemia images
and have found the following results.
The results in Table 12.2 show that the sFCM succeeds in providing better results than
conventional FCM.
Figure 12.5 shows the segmented images with the application sFCM provide better
clarity and understanding of the presence of leukemia cells than images with conven-
tional FCM.

Uncertainty-Based Clustering Algorithms for Medical Image Analysis
293
(a)
(b)
(d)
(e)
(f)
(c)
Figure 12.6 (a) Original image. Segmented images of leukemia using (b) FCM, (c) IFCM, (d) sFCM2,1,
(e) sFCM1,1, and (f) sFCM1,2.
The results in Table 12.3 show that the sIFCM succeeds in providing better results
than conventional FCM.
The segmented images in Figure 12.6 with the application sIFCM provide better clarity
and understanding of the presence of leukemia cells than images with conventional FCM
and IFCM.
12.5
Conclusions
The Ô¨Åeld of uncertainty-based clustering has varied applications. One major applica-
tion is image segmentation, which is a vital process in image analysis. These algorithms
provide more realistic information and hence help in diagnosis of diseases. DiÔ¨Äerent
modiÔ¨Åcations of FCM and IFCM are applied on medical images. Similarly extensive
work is happening in the application of rough and fuzzy-based hybrid algorithms for
analyzing medical images.
References
1 Cheng, H.D., Jiang, X., Sun, Y., and Wang, J. (2001) Color image segmentation:
advances and prospects. Pattern Recognition, 34 (12), 2259‚Äì2281.
2 Pham, D.L. and Prince, J.L. (1999) An adaptive fuzzy c-means algorithm for image
segmentation in the presence of intensity inhomogeneities. Pattern Recognition
Letters, 20 (1), 57‚Äì68.

294
Hybrid Intelligence for Image Analysis and Understanding
3 Chen, W., Giger, M.L., and Bick, U. (2006) A fuzzy c-means (FCM)-based approach
for computerized segmentation of breast lesions in dynamic contrast-enhanced
MRimages 1. Academic Radiology, 13 (1), 63‚Äì72.
4 Pawlak, Z. (1982) Rough sets. International Journal of Computer & Information
Sciences, 11 (5), 341‚Äì356.
5 Atanassov, K.T. (1986) Intuitionistic fuzzy sets. Fuzzy Sets and Systems, 20 (1),
87‚Äì96.
6 Maji, P. and Pal, S.K. (2007) RFCM: a hybrid clustering algorithm using rough and
fuzzy sets. Fundamenta Informaticae, 80 (4), 475‚Äì496.
7 Lingras, P. and West, C. (2004) Interval set clustering of web users with rough
k-means. Journal of Intelligent Information Systems, 23 (1), 5‚Äì16.
8 Maji, P. and Pal, S.K. (2007) Rough set based generalized fuzzy-means algorithm and
quantitative indices. IEEE Transactions on Systems, Man, and Cybernetics, Part B
(Cybernetics), 37 (6), 1529‚Äì1540.
9 Michalopoulos, M., Dounias, G., Thomaidis, N., and Tselentis, G. (2002), Decision
making using fuzzy c-means and inductive machine learning for managing bank
branches performance. Paper presented at the European Symposium on Intelligent
Techniques (ESIT‚Äô99), Crete, Greece, March.
10 Mitra, S., Banka, H., and Pedrycz, W. (2006) Rough&# 8211; fuzzy collaborative clus-
tering. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 36
(4), 795‚Äì805.
11 Tripathy, B., Tripathy, A., Govindarajulu, K., and Bhargav, R. (2014) On kernel
based rough intuitionistic fuzzy c-means algorithm and a comparative analysis,
in Advanced Computing, Networking and Informatics-Volume 1, Springer, pp.
349‚Äì359.
12 Tripathy, B., Tripathy, A., and Rajulu, K.G. (2014) Possibilistic rough fuzzy c-means
algorithm in data clustering and image segmentation, in Computational Intelligence
and Computing Research (ICCIC), 2014 IEEE International Conference, IEEE, pp.
1‚Äì6.
13 Bhargava, R. and Tripathy, B. (2013) Kernel based rough-fuzzy c-means, in Interna-
tional Conference on Pattern Recognition and Machine Intelligence, Springer, pp.
148‚Äì155.
14 Bhargava, R., Tripathy, B., Tripathy, A., Dhull, R., Verma, E., and Swarnalatha, P.
(2013) Rough intuitionistic fuzzy c-means algorithm and a comparative analysis, in
Proceedings of the 6th ACM India Computing Convention, ACM, p. 23.
15 Wells, W.M., Grimson, W.E.L., Kikinis, R., and Jolesz, F.A. (1996) Adaptive segmen-
tation of MRI data. IEEE Transactions on Medical Imaging, 15 (4), 429‚Äì442.
16 Bezdek, J.C., Hall, L., and Clarke, L. (1992) Review of MRimage segmentation tech-
niques using pattern recognition. Medical Physics, 20 (4), 1033‚Äì1048.
17 Dawant, B.M., Zijdenbos, A.P., and Margolin, R.A. (1993) Correction of intensity
variations in MR images for computer-aided tissue classiÔ¨Åcation. IEEE Transactions
on Medical Imaging, 12 (4), 770‚Äì781.
18 Bezdek, J.C. (2013) Pattern recognition with fuzzy objective function algorithms,
Springer Science & Business Media, New York.
19 Tolias, Y.A. and Panas, S.M. (1998) On applying spatial constraints in fuzzy image
clustering using a fuzzy rule-based system. IEEE Signal Processing Letters, 5 (10),
245‚Äì247.

Uncertainty-Based Clustering Algorithms for Medical Image Analysis
295
20 Tolias, Y.A. and Panas, S.M. (1998) Image segmentation by a fuzzy clustering algo-
rithm using adaptive spatially constrained membership functions. IEEE Transactions
on Systems, Man, and Cybernetics ‚Äì Part A: Systems and Humans, 28 (3), 359‚Äì369.
21 Pham, D.L., Xu, C., and Prince, J.L. (2000) Current methods in medical image seg-
mentation 1. Annual Review of Biomedical Engineering, 2 (1), 315‚Äì337.
22 Tripathy, B. and Ghosh, A. (2012) Data clustering algorithms using rough sets.
Handbook of Research on Computational Intelligence for Engineering, Science, and
Business, p. 297.
23 Chuang, K.S., Tzeng, H.L., Chen, S., Wu, J., and Chen, T.J. (2006) Fuzzy c-means
clustering with spatial information for image segmentation. Computerized Medical
Imaging and Graphics, 30 (1), 9‚Äì15.
24 Cai, W., Chen, S., and Zhang, D. (2007) Fast and robust fuzzy c-means cluster-
ing algorithms incorporating local information for image segmentation. Pattern
Recognition, 40 (3), 825‚Äì838.
25 Wang, X.Y. and Bu, J. (2010) A fast and robust image segmentation using FCM with
spatial information. Digital Signal Processing, 20 (4), 1173‚Äì1182.
26 Wang, J., Kong, J., Lu, Y., Qi, M., and Zhang, B. (2008) A modiÔ¨Åed FCM algorithm
for MRI brain image segmentation using both local and non-local spatial constraints.
Computerized Medical Imaging and Graphics, 32 (8), 685‚Äì698.
27 Tripathy, B., Basu, A., and Govel, S. (2014) Image segmentation using spatial intu-
itionistic fuzzy c means clustering, in Computational Intelligence and Computing
Research (ICCIC), 2014 IEEE International Conference, IEEE, pp. 1‚Äì5.
28 Hudedagaddi, D. and Tripathy, B. (2016) Application of spatial IFCM on leukemia
images. Soft Computing for Medical Data and Satellite Image Analysis, 7 (5), 33‚Äì40.
29 Hudedagaddi, D. and Tripathy, B. (2015) Application of spatial FCM on cancer cell
images, in Proceedings of NCICT, Upendranath College, India, pp. 32‚Äì36.

297
13
An Optimized Breast Cancer Diagnosis System Using a Cuckoo
Search Algorithm and Support Vector Machine ClassiÔ¨Åer
Manoharan Prabukumar1, Loganathan Agilandeeswari1, and Arun Kumar Sangaiah2
1School of Information Technology & Engineering, VIT University, Vellore, Tamil Nadu, India
2School of Computing Science & Engineering, VIT University, Vellore, Tamil Nadu, India
13.1
Introduction
A disease that is responsible for many deaths across the world is cancer. In 2008, the
percentage of deaths due to cancer was about 13%, and it is expected to increase signif-
icantly up to 12 billion in the year 2030 as per the World Health Organization (WHO)
[1]. Around the world, the second most dangerous problem in health among women
is breast cancer. The American Cancer Society report stated in 2009 that around 15%
of people (especially female) die because of breast cancer, out of 269,800 cancer deaths
approximately. In addition, as per Meselhy, out of 713,220 diagnosed cancer cases, 27%
of were related to it [2]. To reduce the mortality rate of the patients, early cancer detec-
tion plays an important role. The method that is the gold standard used for such early
detection is the mammography. Each mammographic examination includes four forms
of images: namely, cranio caudal (CC) corresponding to the right breast and medio-
lateral oblique (MLO) of the left breast. The chances of identifying the non-palpable
breast cancer tissues will be greater with the use of CC and MLO, since it improves the
visualization. Then the radiologists use these images to determine the abnormalities
present.
Due to the repetitive task of mammography, the radiologists may become confused,
and it leads to failure to detect malignancy for about 10 to 30% [3]. The masses and
microcalciÔ¨Åcations are the mammographic signs of malignancy. Also, based on the
quality of an image and the radiologist‚Äôs expertise level, the sensitivity of screening
mammography gets aÔ¨Äected. Thus, the automatic system is required for classifying the
doubtful areas in a mammogram to assist radiologists in the screening process and
to avoid unnecessary biopsy. Computer-aided design (CAD) is very helpful to aid the
radiologists in diagnosis. It can easily classify microcalciÔ¨Åcation, masses, and architec-
tural distortions [4]. Several approaches are proposed by various researchers to study
the mammograms and classify their images as the malignant or benign breast cancer
type. For microcalciÔ¨Åcation, many research works are presented recently for automatic
detection with a sensitivity of about 98%. The challenge is with mass detection; because
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

298
Hybrid Intelligence for Image Analysis and Understanding
masses are normally indistinguishable from their adjacent tissues, the following issues
may arise: (1) image contrast will be poor, (2) speculated lesions are mostly connected
to the surrounding parenchymal tissue, (3) masses are surrounded by no uniform
tissue background, and (4) they have no deÔ¨Ånite size, shape, and density [5]. Hence,
the accuracy of diagnosis in terms of sensitivity can be improved by double reading,
which means that the reading is made by two radiologists. This may lead to increases
in the operating cost. Then, the mammographic images from the same patient are
compared, that is, the images from both views (i.e., CC and MLO) of the same breast are
compared. Some researchers proved that the detection and diagnostic performances
can be improved by this approach [6, 7]. But this approach reduces the recalls for a
second inspection of the patient. The CAD system is one that operates independently
on each view of mammography. Several authors have been involved in building CAD
systems that can analyze two views on breast cancer detection to avoid confusion [8].
References [9] and [10] proposed the classiÔ¨Åcation of mammograms by density
through computation techniques. The classiÔ¨Åcation presented considers local statistics
and texture measures to divide the mammograms in to fat or dense tissue. They found
one of the measures (local skewness in tiles) gives a good separation between fatty and
dense patches.
Hadjiiski et al. in 1999 [11] proposed a mammographic-based breast cancer detec-
tion system using a hybrid classiÔ¨Åer such as linear discriminant analysis (LDA) and
adaptive resonance theory (ART). They utilized LDA and a neural network scheme for
classiÔ¨Åcation to overlook previously learned expert knowledge during classiÔ¨Åcation. The
uniqueness of the hybrid classiÔ¨Åer is that the classiÔ¨Åcation process is done at two levels.
In the Ô¨Årst-level classiÔ¨Åcation process, the ART classiÔ¨Åer is used for preliminary check of
the classes. If the class is malignant according to the ART classiÔ¨Åer, no further process-
ing is done. But, if the class is labeled as mixed, then for further classiÔ¨Åcation, the data is
processed through a LDA classiÔ¨Åer to classify the cancer as malignant or benign. They
compared the performance of the proposed hybrid classiÔ¨Åer with the LDA classiÔ¨Åer and
a back-propagation neural network (BPN) classiÔ¨Åer-based system. They achieved the
average area under the ROC (receiver operating characteristic) curve 0.81 as compared
to 0.78 for the LDA classiÔ¨Åer-based system and 0.80 for the BPN classiÔ¨Åer-based system.
They proved that the hybrid classiÔ¨Åer improves the classiÔ¨Åcation accuracy of the CAD
system through their results.
Mousa et al. in 2005 [12] proposed a breast cancer diagnosis system based on wavelet
analysis and fuzzy-neural. The wavelet features have been known to be used in extracting
useful information from mammographic images, and fuzzy-neural classiÔ¨Åers are used
to classify the breast cancer type. Statistical features in a multiple-view mammogram
with support vector machines (SVMs) and kernel Fisher discriminant (KFD) for a digital
mammographic breast image classiÔ¨Åcation system were et al. in proposed by Liyang Wei
et al. [13]. The proposed system achieved 85% classiÔ¨Åcation accuracy. Szekely et al. [14]
proposed a CAD system for mammographic breast cancer image classiÔ¨Åcation using the
texture features. It used a et al. in hybrid classiÔ¨Åer such as decision trees and multireso-
lution Markov random models for classiÔ¨Åcation of cancer as type benign or malignant.
The proposed system achieved 94% classiÔ¨Åcation accuracy.
To detect and classify the breast cancer in digital mammograms, Alolfe et al. in 2009
[15] presented a CAD system based on SVMs and LDA classiÔ¨Åers. The presented sys-
tem has four stages: region of interest (ROI) identiÔ¨Åcation, feature extraction, feature

Àò
An Optimized Breast Cancer Diagnosis System Using a Cuckoo Search Algorithm
299
selection, and feature classiÔ¨Åcation. They used a et al. in forward stepwise linear regres-
sion method for feature selection and a et al. in hybrid classiÔ¨Åer using SVM and LDA
for benign and malignant classiÔ¨Åcation. They compared the classiÔ¨Åcation performance
of the proposed system with the SVM, LDA, and fuzzy c-means classiÔ¨Åer-based system.
They achieved a speciÔ¨Åcity of 90% and sensitivity of 87.5%.
Balakumaran et al. [16] proposed a system that proves to be a better one with the
implementations of multiresolution transformations and the neural networking phe-
nomenon. The classiÔ¨Åcation approach helps in the adaptive learning pattern, which has
helped in the identiÔ¨Åcation of various shapes and sizes of the microcalciÔ¨Åcations, and
it diÔ¨Äerentiates between the various textures (i.e., whether it is a duct, tissue, or micro-
calciÔ¨Åcation). When a mass is detected, it is diÔ¨Écult to distinguish if it is benign or
malignant, but there are diÔ¨Äerences in the features of shape and texture between them.
Benign masses are typically smooth and distinct, and their shapes are similar to round.
On the other hand, malignant masses are irregular, and their boundaries are usually
blurry [17].
Moayedi et al. [18] proposed a system for automatic mass detection of mammograms
based on fuzziÔ¨Åcation and SVMs. The geometrical features are extracted from the
curvelet coeÔ¨Écients, and genetic algorithms are applied for feature weighting. Then the
extracted features of the digital mammograms are applied to the fuzzy-SVM (FSVM)
classiÔ¨Åers to classify mammogram images. The performance of the proposed was
compared with the fuzzy role-based SVM, the support vector‚Äìbased fuzzy neural
network (SVFNN) classiÔ¨Åer. They achieved 95.6% classiÔ¨Åcation accuracy for the FSVM
classiÔ¨Åer. They proved the FSVM has a strong capability to classify the mass and
nonmass images.
Balanica et al. [19] has proposed a system that classiÔ¨Åes on the basis of the fuzzy sys-
tem that helps in the identiÔ¨Åcation of the geometrical pattern of the textures. The tumor
features are extracted using the fuzzy logic technique for predicting the risk of breast
cancer based on a set of chosen fuzzy rules utilizing patient age automatically.
Meselhy et al. [2] have proposed a method for breast cancer diagnosis in digital
mammogram images using mresolution transformations, namely, wavelet or curvelet
transformations. Here, mammogram images are converted into frequency coeÔ¨Écients
and construct the feature matrix. Then, based on the statistical t-test method, a better
feature extraction method was developed and the optimized features are extracted using
the dynamic threshold method. This also helps in studying the various frequencies of
the textures in the image. SVMs are used to classify benign and malignant tumors. The
classiÔ¨Åcation accuracy rate achieved by the proposed method using wavelet coeÔ¨Écients
is 95.84% and using curvelet coeÔ¨Écients is 95.98%. The obtained results show the
importance of the feature extraction step in developing a CAD system.
A innovative approach for detection of microcalciÔ¨Åcation in digital mammograms is
the swarm optimization neural network (SONN) presented by Dheeba et al. in 2012 [20].
To capture descriptive texture information, Laws‚Äô texture features are extracted from the
mammograms. These features are used to extract texture energy measures from the ROI
containing microcalciÔ¨Åcation. A feedforward neural network that is used for detection
of abnormal regions in breast tissue is optimally designed using the particle swarm opti-
mization algorithm. The proposed intelligent classiÔ¨Åer is evaluated based on the MIAS
database where 51 malignant, 63 benign, and 208 normal images are utilized. It also
been tested on 216 real-time clinical images with abnormalities, which showed that the

300
Hybrid Intelligence for Image Analysis and Understanding
results are statistically signiÔ¨Åcant. Their proposed methodology achieved an area under
the ROC curve (Az) of 0.9761 for the MIAS database and 0.9138 for real clinical images.
A computerized scheme for automatic detection of cancerous lesions in mammo-
grams was examined by Dheeba et al. [21]. They proposed a supervised machine
learning algorithm, the DiÔ¨Äerential Evolution Optimized Wavelet Neural Network
(DEOWNN), to detect tumor masses in mammograms. The texture features of the
abnormal breast tissues and normal breast tissues are extracted prior to classiÔ¨Åcation.
The DEOWNN classiÔ¨Åer is used to determine normal or abnormal tissues in mammo-
grams. The performance of the CAD system is evaluated using a mini-database from
MIAS. They achieved sensitivity of 96.9% and speciÔ¨Åcity of 92.9%.
Prabukumar et al. [22] proposed a novel approach to diagnosis breast cancer using dis-
crete wavelet transform, SVM, and neuro-fuzzy logic classiÔ¨Åer. To optimize the number
of features and achieve the maximum classiÔ¨Åcation accuracy rate, dynamic thresholds
are applied. The FSVM method is used to classify between normal and abnormal tissues.
The proposed approach obtained classiÔ¨Åcation accuracy rates of 93.9%.
Guzm√°n-Cabrera et al. [23] proposed a CAD scheme to eÔ¨Äectively analyze digital
mammograms based on texture segmentation for classifying the early-stage tumors.
They used morphological operations and, clustering techniques to segment the ROI
from the background of the mammogram images. The proposed algorithm was tested
with the well-known digital database. It was applied for screening mammography for
cancer research and diagnosis, and it was found to be absolutely suitable to distinguish
masses and microcalciÔ¨Åcations from the background tissue. The breast abnormalities
in digital mammograms investigated by Dheeba et al. [24] involved a novel classiÔ¨Åca-
tion approach based on particle swarm optimized wavelet neural networks. The laws‚Äô
texture energy measures are extracted from the mammograms to classify the suspicious
regions. They used a real clinical database to test the proposed algorithm. They achieved
an area under the ROC curve of 0.96853 with a sensitivity of 94.167% and speciÔ¨Åcity of
92.105%. They compared the performance of the proposed system with a SONN-and
DEOWNN-based system.
To detect and segment regions in mammographic images, Pereira et al. [25] presented
a method using a wavelet and genetic algorithm. They adopted wavelet transform and
Wiener Ô¨Åltering for image denoising and enhancement in the preprocessing stage. They
employed a genetic algorithm for suspicious regions segmentation. They achieved a sen-
sitivity of 95% with a FP rate of 1.35 per image. Researchers proposed a system for breast
cancer diagnosis using fuzzy neural networks. They used statistical features to train the
classiÔ¨Åer and achieved 83% classiÔ¨Åcation accuracy.
A novel mammographic image preprocessing method was proposed by He et al.
in 2015 [26] to improve image quality of the digital mammogram images. Instead of
using an assumed correlation between smoothed pixels and breast thickness to model
the breast thickness, they used a shape outline derived from MLO and CC views
for the same. Then they used a selective approach to target speciÔ¨Åc mammograms
more accurately. The proposed approach improved the mammographic appearances
not only in the breast periphery but also across the mammograms. To facilitate a
quantitative and qualitative evaluation, mammographic segmentation and risk/density
classiÔ¨Åcation were performed. When using the processed images, the results indicated
more anatomically correct segmentation in tissue-speciÔ¨Åc areas, and subsequently
better classiÔ¨Åcation accuracies were achieved. Visual assessments were conducted in a

An Optimized Breast Cancer Diagnosis System Using a Cuckoo Search Algorithm
301
clinical environment to determine the quality of the processed images and the resultant
segmentation. The developed method has shown promising results. It is expected to be
useful in early breast cancer detection, risk-stratiÔ¨Åed screening, and aiding radiologists
in the process of decision making prior to surgery and/or treatment.
To detect abnormalities or suspicious areas in digital mammograms and classify them
as malignant or nonmalignant, Sharma et al. in 2015 [27] proposed a CAD system.
The suspicious areas in digital mammogram patches are extracted from the original
large-sized digital mammograms manually. Zernike moments of diÔ¨Äerent orders are
computed from the extracted patches and stored as a feature vector for training and
testing purposes. They tested the performance of the proposed system on benchmarked
data sets such as the Image Retrieval In Medical Application (IRMA) reference data set
and Digital Database for Screening Mammography (DDSM) mammogram database.
Their experimental study shows that the Zernike moments of order 20 and SVM
classiÔ¨Åer give the best results compared to the other studies. The performance of the
proposed CAD system is compared with the other well-known texture descriptors
such as gray-level co-occurrence matrix (GLCM) and discrete cosine transform (DCT)
to verify the applicability of Zernike moments as a texture descriptor. The proposed
system achieved 99% sensitivity and 99% speciÔ¨Åcity on the IRMA reference data set
and 97% sensitivity and 96% speciÔ¨Åcity on the DDSM mammogram database.
In this chapter, we propose an optimized breast cancer diagnosis system using a
cuckoo optimization algorithm and SVM classiÔ¨Åer. To segment the ROI, a morpho-
logical segmentation algorithm is used in our approach. It will segment the nodule
of interest accurately, which leads to better classiÔ¨Åcation accuracy of the proposed
system. In order to increase the accuracy of classiÔ¨Åcation, the optimized features are
determined using the cuckoo optimization algorithm. Then these optimized features
are extracted from the segmented breast cancer region, and the same is trained by SVM
classiÔ¨Åer to classify the nodules as benign or malignant.
The organization of the chapter is given as follows: Section 13.2 deals with the
technical background, and architecture of the proposed system is discussed in Section
13.3. In Section 13.4, results and discussion are given. Finally, a conclusion is given in
Section 13.5.
13.2
Technical Background
In this section, we describe all the methods we need to form our proposed system.
13.2.1
Morphological Segmentation
In general, morphological segmentation is performed on a thresholded image to remove
unwanted regions or segment the ROI. First, the image is converted into a binary cluster
by obtaining a desired threshold of an image using threshold-based segmentation. Then,
perform some morphological operations on the binary cluster to obtain the desired
region. The basic morphological operations that can be performed are erosion, opening,
closing, and dilation. To perform a morphological operation, an appropriate structuring
element is to be recognized and applied to the binary image. There are two situations
for morphological operations, hit and Ô¨Åt. The structuring element is said to Ô¨Åt the image

302
Hybrid Intelligence for Image Analysis and Understanding
if, for each of its pixels set to 1, the corresponding image pixel is also 1. Similarly, a
structuring element is said to hit, or intersect, an image if, for at least one of its pixels
set to 1, the corresponding image pixel is also 1. Morphological opening opens up a gap
between objects connected by bridge and hence is found to be best among all opera-
tions for segmentation. The structuring element for each magnetic resonance imaging
(MRI) scan image may be diÔ¨Äerent as the structure of breast tissue is diÔ¨Äerent for dif-
ferent computed tomography (CT) slices. Hence, choosing the appropriate structuring
element for each slice may be a problem. Morphological opening may even destroy brain
tumor structure as the size of the structuring element increases [28, 29].
13.2.2
Cuckoo Search Optimization Algorithm
A new nature-inspired metaheuristic cuckoo search algorithm proposed by Yang and
Deb (2010) [30] has been used to determine the most predominant features among the
features extracted from the segmented breast cancer region. Each egg in a nest rep-
resents a solution, and a cuckoo egg represents a new solution. The aim is to employ
the new and potentially better solutions (cuckoos) to replace not-so-good solutions in
the nests. In the simplest form, each nest has one egg. The algorithm can be extended
to more complicated cases in which each nest has multiple eggs representing a set of
solutions.
The cuckoo search is based on three idealized rules: (1) each cuckoo lays one egg at a
time, and dumps it in a randomly chosen nest; (2) the best nests with high-quality eggs
(solutions) will carry over to the next generations; and (3) the number of available host
nests is Ô¨Åxed, and a host can discover an alien egg with probability pa √é [0,1]. In this case,
the host bird can either throw the egg away or abandon the nest to build a completely
new nest in a new location.
For simplicity, the last assumption can be approximated by a fraction pa of the n
nests being replaced by new nests, having new random solutions. For a maximization
problem, the quality or Ô¨Åtness of a solution can simply be proportional to the objective
function. Other forms of Ô¨Åtness can be deÔ¨Åned in a similar way to the Ô¨Åtness function in
genetic algorithms. Based on the above-mentioned rules, the basic steps of the cuckoo
search can be summarized as the pseudo-code, as follows:
1: Objective function f (x), x = (x1, x2, x3, ‚Ä¶ , xd);
2: Generate initial population of n host xi where i = 1, 2, 3, ‚Ä¶ , n;
3: while t ‚â§MaxGeneration do
4:
Get a cuckoo randomly by L√©vy Ô¨Çights;
5:
Evaluate its Ô¨Åtness Fi;
6:
Choose a nest among n(sayj) randomly
7:
if Fi > Fj then
8:
Replace j by the new solution;
9:
end if
10:
A fraction (pa) of worse nests is abandoned and new ones are built;
11:
Keep the best solutions (or nest with quality solutions);
12:
Rank the solutions and Ô¨Ånd the current best;
13: end while
14: Post process results and visualization.

An Optimized Breast Cancer Diagnosis System Using a Cuckoo Search Algorithm
303
When the new solution Xi(t + 1) is generating for the ith cuckoo, the following Levy
Ô¨Çight is used:
Xi(t + 1) = Xi(t) + ùõº‚äïLevy(ùúÜ)
(13.1)
where ùõºis the step size, which should be related to the scale of the problem of interest;
and the product ‚äïmeans entry-wise multiplications.
13.2.3
Support Vector Machines
SVMs are eÔ¨Äective for classiÔ¨Åcation of both linear and nonlinear data. They were intro-
duced by Vapnik et al. [31]. They are trained with the data set in which it is known to
which class each point of that data set belongs. SVM training builds a model accord-
ing to which it classiÔ¨Åes the new data point. Each data point is viewed as a vector with
p-dimensions. The data points are separated with a hyperplane.
1. Linear separable data: For linear separable data, data points can be separated with
a p ‚àí1 dimensional hyperplane, and the hyperplane that separates the two classes
with the largest margin is considered.
2. Nonlinear separable data: ClassiÔ¨Åcation of nonlinear data cannot be done on that fea-
ture space. The present feature space has to be mapped with some higher dimension
feature space where training sets are separable. This mapping of present feature space
into higher dimension feature space is done using the kernel trick. In simple terms,
SVM uses kernels for separation of nonlinear data. The linear classiÔ¨Åer depends on
the inner product between vectors. Data points are mapped into higher feature space
by some transformation K(xi, xj) = xT
i xj and ùúô‚à∂‚Üíùúô(x) the inner product becomes.
Features that are to be classiÔ¨Åed are nonlinear in nature. They have to be mapped to
some higher dimension. In our approach, we have used the radial basis function, because
it provides inÔ¨Ånite dimensional spaces.
13.3
Proposed Breast Cancer Diagnosis System
We proposed an optimized breast cancer diagnosis system using morphological seg-
mentation, cuckoo search optimization algorithm, and multiclass SVM classiÔ¨Åer. The
proposed system involves the following phases: (1) preprocessing of input mammo-
graphic breast images using Otsu‚Äôs thresholding and morphological segmentation to
segment the tissues of interest; (2) from the segmented image, various features, namely
statistical, texture, geometrical, and invariant feature moments, are extracted; (3) opti-
mized feature extraction for better diagnosis is done using the cuckoo search algorithm;
(4) a multiclass SVM classiÔ¨Åer is used for better classiÔ¨Åcation of mammography breast
images of type benign, malignant, and noncancerous. The block diagram of the proposed
optimized system is given in Figure 13.1.
13.3.1
Preprocessing of Breast Cancer Image
In this stage, digital mammographic breast cancer image quality is improved using a
median Ô¨Åltering process. It will remove the noise present in the mammographic breast
cancer image, then enhance the mammographic breast cancer image converting to a

304
Hybrid Intelligence for Image Analysis and Understanding
Preprocessing
Grey to Binary Image
Conversion Using Otsu‚Äôs
Method
Morphological
Segmentation
Feature Extraction
Texture Features
Geometrical Features
Statistical Features
Invariant Moments Features
Features Selection Using Cuckoo Search Optimization
Algorithms
Feature Classification
Multiclass SVM Training
Multiclass SVMT Esting
Diagnosis Result
Normal
Benign
Malignant
Median-filtered
Mammographic Breast Image
Figure 13.1 Proposed breast cancer diagnosis system.
binary image using Otsu‚Äôs thresholding approach. Segmentation is the most diÔ¨Écult and
vital process to be carried out in the diagnosis process. In order to segment the tissue of
interest accurately, a morphological segmentation algorithm is used in our approach.
13.3.2
Feature Extraction
The feature extraction is the next step after the segmentation of the cancer region.
In general, the features of tissues contain very crucial information about them. With
the help of these features, it is possible to classify the tissues of various kinds. In our
approach, we have calculated various features, namely statistical, texture, geometrical,
and invariant feature moments, from each segmented tissue; and then these features
are used to determine the optimum features using the cuckoo search optimization
algorithm. Detailed descriptions of the various features extracted from the segmented
region are given in this section.
13.3.2.1
Geometric Features
For calculating the geometric features, the segmented binary image of the breast cancer
is used. The geometric features that are calculated for distinction are:
1. Area: This feature is calculated on the binary image of the extracted tissue.
Calculation of area in binary form will not make any diÔ¨Äerence as it counts the
number of pixels involved in tissue. It will make the calculation more accurate.

An Optimized Breast Cancer Diagnosis System Using a Cuckoo Search Algorithm
305
2. Convex area: This feature counts the number of pixels engaged in the tissue of interest
in its convex form. Calculations are made in the ROI‚Äôs binary form.
3. Perimeter: The number of pixels present on the boundary of tissue of interest
comprises the perimeter for that tissue of interest. A binary form of an image will
provide accurate results.
4. Equivalent diameter: It is the diameter of the circle with the same area as the tissue of
interest. As it works on area, it can also be calculated on the binary form of an image.
5. Solidity: Solidity is the proportion of pixels present in the convex area and area. It is
expressed as:
Solidity = Area‚àïConvexArea
(13.2)
6. Irregularity index: Irregularity index is a scalar that tells about irregularity in shape,
mainly because of their borders. It is expressed as:
IrregularityIndex = 4ùúã. Area‚àïPerimeter2
(13.3)
7. Eccentricity: This is the ratio of distance between the foci of the tissue and its major
axis length. Length contributes to a complete pixel; hence, it has to be calculated on
a binary image.
8. Size: Calculation of the major axis length of the tissue is considered as size in our
approach. Size in centimeters is determined for the extracted tissue of interest.
Calculation of size will let us know that even for small-sized tissues, malignancy is
possible.
13.3.2.2
Texture Features
The following texture features of the tissue of interest are calculated in the segmented
gray-level image of each tissue.
9. Energy: Energy of tissue refers to uniformity or angular second moment. It is
presented in the following way:
Energy =
M
‚àë
i=1
N
‚àë
j=1
p2(i, j)
(13.4)
where p(i, j) is the intensity value of the pixel at the point (i, j); and MN is the size
of the image.
10. Contrast: It calculates contrast between a pixel‚Äôs intensity and its surrounding pixels.
It is expressed as:
Contrast =
M
‚àë
i=1
N
‚àë
j=1
p2(i, j)
(13.5)
where p(i, j) is the intensity value of the pixel at the point (i, j); and MN is the size
of the image.
11. Correlation: It is the measure of relationship among the pixels. It calculates the
extent a pixel is correlated to its neighbors.
Correlation =
M
‚àë
i=1
N
‚àë
j=1
p(i, j) ‚àíùúár. ùúác
ùúér. ùúéc
(13.6)

306
Hybrid Intelligence for Image Analysis and Understanding
12. Homogeneity: It calculates how closely the elements of GLCM are distributed to the
GLCM of the tissue of interest. It is expressed as:
Homogeneity =
M
‚àë
i=1
N
‚àë
j=1
p(i, j)
1 + |i ‚àíj|
(13.7)
13. Entropy: It calculates random distribution of gray levels. As it deals with gray levels,
it has to be calculated on gray-level tissue. It is expressed as:
Entropy = ‚àí
M
‚àë
i=1
N
‚àë
j=1
p(i, j). logp(i, j)
(13.8)
13.3.2.3
Statistical Features
14. Mean: This is basically used for obtaining the mean of gray levels present in the
image. It is expressed as:
Mean =
1
MN
M
‚àë
i=1
N
‚àë
j=1
p(i, j)
(13.9)
15. Variance: Variance actually observes and checks the distribution of gray levels. It is
expressed as:
Variance =
1
MN
M
‚àë
i=1
N
‚àë
j=1
(p(i, j) ‚àíùúá)
(13.10)
16. Standard deviation: Standard deviation is the measure of diÔ¨Äerence or deviation
existing from midpoint. It is expressed as:
Standarddeviation =
‚àö
Variance
(13.11)
17. Invariant moments:
1stordermomentsùúô1 = ùúÇ20 + ùúÇ02
(13.12)
2ndordermomentsùúô2 = (ùúÇ20 ‚àíùúÇ02)2 + 4ùúÇ2
11
(13.13)
3rdordermomentsùúô3 = (ùúÇ30 ‚àí3ùúÇ12)2 + (3ùúÇ21 ‚àíùúá03)2
(13.14)
4thordermomentsùúô4 = (ùúÇ30 + 3ùúÇ12)2 + (3ùúÇ21 + ùúá03)2
(13.15)
13.3.3
Features Selection
Some of the features extracted from the ROIs in the mammographic image are not
signiÔ¨Åcant when observed alone, but in combination with other features, they can
be signiÔ¨Åcant for classiÔ¨Åcation. The best set of features for eliminating false positives
and for classifying types as benign or malignant are selected using the cuckoo search
optimization algorithm in the feature selection process. The best features selection
procedure by the cuckoo search algorithm is given in the form of Algorithm 13.1.

An Optimized Breast Cancer Diagnosis System Using a Cuckoo Search Algorithm
307
Algorithm 13.1 Cuckoo search algorithm for optimal feature selection
1. Step 1: Initialization:
Initialize the cuckoo search parameters as number of nest n = 50, nest size s = 25,
minimum number of generations t=0, step size ùõº= 0.30, and maximum generations
Gmax = 1000.
State Ô¨Åtness function (Fn) to select the optimum features using MCC as: Fn =
(TP√óTN)‚àí(FP√óFN)
‚àö
(TP+FP)(TP+FN)(TN+FP)(TN+FN) ;
where TP = true positive; TN = true negative; FP = false positive; and FN = false
negative.
2. Step 2: Output: Choose the suitable features for determining the benign or malignant
tissue using the Ô¨Åtness function (Fn) given in the above step.
3. Step 3: Process:
A. While t < Gmax
i. Get a cuckoo randomly by levy Flights i.
ii. Evaluate the Ô¨Åtness function Fi using MCC
iii. Randomly select a nest among n nests as j, and evaluate its Ô¨Åtness function Fj.
iv. If Fi < Fj
a. Replace j by new solution.
v. Else
a. Let i as a solution and
b. Abandon the worst nest and build the new nest by Levy Ô¨Çights.
c. Keep the current one as best
B. Rank the Ô¨Åtness function of various n nests, R(Fn)
C. Find the best Fn used as optimum feature Of.
13.3.4
Features ClassiÔ¨Åcation
SVMs have been successfully applied in various classiÔ¨Åcation problems [32]. Training
data for SVMs should be represented as labeled vectors in a high-dimensional space
where each vector is a set of features that describes one class. For correct classiÔ¨Åcation
of tissues, this representation is constructed to preserve as much information as
possible about the features. SVMs are trained with the optimized features identiÔ¨Åed by
the cuckoo search algorithm. Trained SVMs are used for classiÔ¨Åcation of the tissues of
various types, such as normal, benign, and malignant.
13.4
Results and Discussions
The performance of the proposed optimized system for breast cancer diagnosis is
validated and evaluated using the sample MIAS database. The data set consists of 322
images of 161 patients, where 51 were diagnosed to be malignant, 4 were benign, and
the rest (207) were said to be normal. The empirical analysis of the proposed system is
tested by implementing the same using MATLAB 2013a in an Intel Core i5 processor
with 8 GB RAM. The output of the entire process of the proposed optimization system
for breast cancer diagnosis is given in Figures 13.2, 13.3, and 13.4.

308
Hybrid Intelligence for Image Analysis and Understanding
Figure 13.2 Sample mammographic images.
Figure 13.3 Enhanced mammographic images.
Figure 13.4 Otsu‚Äôs thresholded and morphological segmented breast tissues.
From the segmented images, the 23 features such as shape, texture, invariant
moments, and statistical features required for classiÔ¨Åcation, as stated in Section 13.2,
are calculated for the further extraction of nodules. In order to obtain the optimized
features for classiÔ¨Åcation, the mentioned 23 features are given as an initial solution
to the cuckoo search optimization algorithm. The optimization algorithm is used to
select the features that are appropriate for classiÔ¨Åcation using the Ô¨Åtness function. First,
random selection of features is done and evaluates the Ô¨Åtness function. The Ô¨Åtness
function is based on the Mathew correlation coeÔ¨Écient (MCC), which performs the
correlation between the true-positive rate and false-positive rate. The MCC value varies
between ‚àí1 and +1 [33].
This process of computing the Ô¨Åtness function is repeated until the decided accuracy
has been obtained with optimized features. As a result, this further improves the
selection of features for classiÔ¨Åcation. These features and the data set are given as input
to the SVM for training. Once the SVM is trained, it is Ô¨Åt to use with the actual data
set that is to be classiÔ¨Åed. The SVM classiÔ¨Åer will read all the recorded data features
and classify them according to the knowledge that it has after training. The SVM is
trained with these limited optimized features and produces better classiÔ¨Åcation with
limited computational cost and time. In this experiment, 85% of the images of each
class were used to form the training set, and the other 15% of the images were used
to form the test set. The performance of the proposed breast cancer diagnosis system,
compared with the existing methods such as FSVM [18], DWT-Neural Networks [16],
and DWT-NeuroFuzzy Logic [22], in terms of the classiÔ¨Åcation accuracy, sensitivity,

An Optimized Breast Cancer Diagnosis System Using a Cuckoo Search Algorithm
309
and speciÔ¨Åcity are given by:
ClassiÔ¨ÅcationAccuracy =
TP + TN
TP + FP + TN + FN
(13.16)
Sensitivity =
TP
TP + FN
(13.17)
SpeciÔ¨Åcity =
TN
FP + TN
(13.18)
where TP (true positive) is the probability that the person with a cancer is found to
be having cancer; FP (false positive) is the probability that the detection value for the
healthy person is found to be cancer; TN (true negative) is the probability that the person
with cancer is found to be healthy; and FN (false negative) is explained as a person
without cancer, or healthy, is found to be having cancer. The same is shown in Table 13.1.
From Table 13.1 values, we can say that classiÔ¨Åcation accuracy, sensitivity, and
speciÔ¨Åcity of the proposed Cuckoo search and SVM-based breast cancer diagnosis
system are high when compared to the existing approaches such as FSVM, DWT-neural
networks, and DWT-neuro fuzzy logic-based breast cancer diagnosis systems. The
sensitivity, speciÔ¨Åcity, and accuracy values for the two classes show that the classiÔ¨Åer is
capable of eÔ¨Éciently discriminating breast cancer tissues as either benign or malignant
from the other segmented region.
In Table 13.2, we have recorded the average execution time of both the training and
the test operations. By comparing the execution time of each approach, we found that
the proposed system recorded the lowest time when compared to the existing systems.
Table 13.1 Comparison of classiÔ¨Åcation accuracy of proposed approach with the existing methods
Sample
no.
Method
Class
ClassiÔ¨Åcation
accuracy
Average
accuracy
Average
sensitivity
Average
speciÔ¨Åcity
1
Fuzzy SVM
Normal/benign/
malignant
100/85/89
91.4
86.43
91.69
2
DWT neural
networks
Normal/benign/
malignant
100/88/84
90.5
85.33
90.99
3
DWT neuro
fuzzy logic
Normal/benign/
malignant
100/99.9/90
93.9
91.82
93.33
4
Proposed
Normal/benign/
malignant
100/95.79/94.36 96.72
98.88
93.68
Table 13.2 Comparison of computational costs
Sample no.
Method
Class
Avg. computation time (sec)
1
Fuzzy SVM
Normal/benign/
malignant
143.9
2
DWT neural
networks
Normal/benign/
malignant
191.5
3
DWT neuro
fuzzy logic
Normal/benign/
malignant
123.6
4
Proposed
Normal/benign/
malignant
110.08

310
Hybrid Intelligence for Image Analysis and Understanding
13.5
Conclusion
In this chapter, an optimized breast cancer diagnosis system using a cuckoo search
algorithm and SVM classiÔ¨Åer is proposed. The morphological segmentation algorithm
is used to segment the tissue of interest from the mammographic breast images. From
the experimental results, we found that the proposed segmentation approach detects
tissues of diÔ¨Äerent shapes and sizes eÔ¨Éciently and accurately. The more appropriate
features, such as texture, geometrical, statistical, and invariant moments, required for
classiÔ¨Åcation from the segmented nodules are computed. The number of features used
to classify the breast cancer nodules is further reduced in our proposed system by
using a Cuckoo search optimization algorithm without compromising the classiÔ¨Åcation
accuracy. The optimized features are trained in the SVM classiÔ¨Åer, and the same can
be used to classify the test sample for benign and malignant nodules. The classiÔ¨Åcation
accuracy of the proposed system is validated against the MIAS database. From the
experimental results, we infer that the classiÔ¨Åcation accuracy of the proposed system is
96.72%, which is higher than that of the existing breast cancer diagnosis system.
13.6
Future Work
The performance of the proposed system was tested with mammogram images only
from the MIAS database. In a future work, the proposed system will be tested with other
well-known breast cancer database images. The classiÔ¨Åcation accuracy of the proposed
system needs to improve by identifying suitable techniques in the segmentation and
feature extraction stages.
References
1 World Health Organization. Cancer. Fact sheet no. 297. Available from http://www
.who.int/mediacentre/factsheets/fs297/en.
2 Eltoukhy, M.M., Faye, I., and Samir, B.B. (2012) A statistical based feature extraction
method for breast cancer diagnosis in digital mammogram using multiresolution
representation. Computers in Biology and Medicine, 42 (1), 123‚Äì128. Available from
http://dx.doi.org/10.1016/j.compbiomed.2011.10.016.
3 Elter, M. and Horsch, A. (2009) Cadx of mammographic masses and clustered
microcalciÔ¨Åcations: a review. Medical Physics, 36 (6), 2052‚Äì2068.
4 Giger, M.L., Chan, H.P., and Boone, J. (2008) Anniversary paper: History and status
of CAD and quantitative image analysis: the role of medical physics and aapm.
Medical Physics, 35 (12), 5799‚Äì5820, doi: http://dx.doi.org/10.1118/1.3013555.
Available from http://scitation.aip.org/content/aapm/journal/medphys/35/12/10
.1118/1.3013555.
5 Hong, B.W. and Sohn, B.S. (2010) Segmentation of regions of interest in mammo-
grams in a topographic approach. IEEE Transactions on Information Technology in
Biomedicine, 14 (1), 129‚Äì139, doi: 10.1109/TITB.2009.2033269.
6 Gupta, S., Chyn, P.F., and Markey, M.K. (2006) Breast cancer cadx based on
bi-radsTM descriptors from two mammographic views. Medical Physics, 33 (6),

An Optimized Breast Cancer Diagnosis System Using a Cuckoo Search Algorithm
311
1810‚Äì1817, doi: http://dx.doi.org/10.1118/1.2188080. Available from http://scitation
.aip.org/content/aapm/journal/medphys/33/6/10.1118/1.2188080.
7 van Engeland, S. and Karssemeijer, N. (2007) Combining two mammographic
projections in a computer aided mass detection method. Medical Physics, 34 (3),
898‚Äì905, doi: http://dx.doi.org/10.1118/1.2436974. Available from http://scitation.aip
.org/content/aapm/journal/medphys/34/3/10.1118/1.2436974.
8 Paquerault, S., Petrick, N., Chan, H.P., Sahiner, B., and Helvie, M.A. (2002)
Improvement of computerized mass detection on mammograms: fusion of two-view
information. Medical Physics, 29 (2), 238‚Äì247.
9 Hajnal, S., Taylor, P., Dilhuydy, M.H., Barreau, B., and Fox, J. (1993) Classifying
mammograms by density: rationale and preliminary results. Presented at SPIE
Medical Imaging conference. doi: 10.1117/12.148662. Available from http://dx.doi
.org/10.1117/12.148662.
10 Taylor, P., Hajnal, S., Dilhuydy, M.H., and Barreau, B. (1994) Measuring image tex-
ture to separate ‚ÄúdiÔ¨Écult‚Äùfrom ‚Äúeasy‚Äùmammograms. The British Journal of Radiology,
67 (797), 456‚Äì463, doi: 10.1259/0007-1285-67-797-456. Available from http://dx.doi
.org/10.1259/0007-1285-67-797-456, pMID: 8193892.
11 Hadjiiski, L., Sahiner, B., Chan, H.P., Petrick, N., and Helvie, M. (1999) ClassiÔ¨Åcation
of malignant and benign masses based on hybrid art2lda approach. IEEE Transac-
tions on Medical Imaging, 18 (12), 1178‚Äì1187, doi: 10.1109/42.819327.
12 Mousa, R., Munib, Q., and Moussa, A. (2005) Breast cancer diagnosis system based
on wavelet analysis and fuzzy-neural. Expert Systems with Applications, 28 (4),
713‚Äì723, doi: http://dx.doi.org/10.1016/j.eswa.2004.12.028. Available from http://
www.sciencedirect.com/science/article/pii/S0957417404001757.
13 Wei, L., Yang, Y., Nishikawa, R.M., and Jiang, Y. (2005) A study on several
machine-learning methods for classiÔ¨Åcation of malignant and benign clustered
microcalciÔ¨Åcations. IEEE Transactions on Medical Imaging, 24 (3), 371‚Äì380.
Available from http://dx.doi.org/10.1109/TMI.2004.842457.
14 Sz√©kely, N., T√≥th, N., and Pataki, B. (2006) A hybrid system for detecting masses in
mammographic images. IEEE Transactions on Instrumentation and Measurement, 55
(3), 944‚Äì952. Available from http://dx.doi.org/10.1109/TIM.2006.870104.
15 Alolfe, M.A., Mohamed, W.A., Youssef, A.B.M., Mohamed, A.S., and Kadah, Y.M.
(2009) Computer aided diagnosis in digital mammography using combined sup-
port vector machine and linear discriminant analyasis classiÔ¨Åcation, in ICIP, IEEE,
pp. 2609‚Äì2612. Available from conf/icip/2009.
16 Balakumaran, T., Vennila, D.I., and Shankar, C.G. (2013) MicrocalciÔ¨Åcation detection
using multiresolution analysis and neural network. Available from http://citeseerx.ist
.psu.edu/viewdoc/summary?doi=10.1.1.381.8882; http://ijrte.academypublisher.com/
vol02/no02/ijrte0202208211.pdf.
17 Zhao, H., Xu, W., Li, L., and Zhang, J. (2011) ClassiÔ¨Åcation of breast masses based
on multi-view information fusion using multi-agent method, in Bioinformatics
and Biomedical Engineering, (iCBBE) 2011 5th International Conference, pp. 1‚Äì4,
10.1109/icbbe.2011.5780304.
18 Moayedi, F. and Dashti, E. (2010) Subclass fuzzy-SVM classiÔ¨Åer as an eÔ¨Écient
method to enhance the mass detection in mammograms. Iranian Journal of Fuzzy
Systems, 7 (1), 15‚Äì31. Available from http://ijfs.usb.ac.ir/article_158.html.

312
Hybrid Intelligence for Image Analysis and Understanding
19 Caramihai, M., Severin, I., Blidaru, A., Balan, H., and Saptefrati, C. (2010) Evaluation
of breast cancer risk by using fuzzy logic, in Proceedings of the 10th WSEAS Inter-
national Conference on Applied Informatics and Communications, and 3rd WSEAS
International Conference on Biomedical Electronics and Biomedical Informatics,
World ScientiÔ¨Åc and Engineering Academy and Society (WSEAS), Stevens Point, WI,
AIC‚Äò10/BEBI‚Äô10, pp. 37‚Äì42. Available from http://dl.acm.org/citation.cfm?id=2170353
.2170366.
20 Dheeba, J. and Selvi, S.T. (2012) A swarm optimized neural network system for
classiÔ¨Åcation of microcalciÔ¨Åcation in mammograms. Journal of Medical Systems, 36
(5), 3051‚Äì3061, doi: 10.1007/s10916-011-9781-3. Available from http://dx.doi.org/10
.1007/s10916-011-9781-3.
21 Dheeba, J. and Tamil Selvi, S. (2012) An improved decision support system
for detection of lesions in mammograms using diÔ¨Äerential evolution opti-
mized wavelet neural network. Journal of Medical Systems, 36 (5), 3223‚Äì3232,
doi: 10.1007/s10916-011-9813-z. Available from http://dx.doi.org/10.1007/s10916-
011-9813-z.
22 Manoharan, P., Prasenjit, N., and Sangeetha, V. (2013) Feature extraction method for
breast cancer diagnosis in digital mammograms using multi-resolution transforma-
tions and SVM-fuzzy logic classiÔ¨Åer. IJCVR, 3 (4), 279‚Äì292. Available from http://dx
.doi.org/10.1504/IJCVR.2013.059102.
23 Guzm√°n-Cabrera, R., Guzm√°n-Sep√∫lveda, J.R., Torres-Cisneros, M., May-Arrioja,
D.A., Ruiz-Pinales, J., Ibarra-Manzano, O.G., Avi√±a-Cervantes, G., and Parada, A.G.
(2013) Digital image processing technique for breast cancer detection. Interna-
tional Journal of Thermophysics, 34 (8), 1519‚Äì1531, doi: 10.1007/s10765-012-1328-4.
Available from http://dx.doi.org/10.1007/s10765-012-1328-4.
24 Dheeba, J., Singh, N.A., and Selvi, S.T. (2014) Computer-aided detection of breast
cancer on mammograms: a swarm intelligence optimized wavelet neural network
approach. Journal of Biomedical Informatics, 49, 45‚Äì52. Available from http://dx.doi
.org/10.1016/j.jbi.2014.01.010.
25 Pereira, D.C., Ramos, R.P., and do Nascimento, M.Z. (2014) Segmentation and detec-
tion of breast cancer in mammograms combining wavelet analysis and genetic
algorithm. Computer Methods and Programs in Biomedicine, 114 (1), 88‚Äì101.
Available from http://dx.doi.org/10.1016/j.cmpb.2014.01.014.
26 He, W., Hogg, P., Juette, A., Denton, E.R.E., and Zwiggelaar, R. (2015) Breast image
pre-processing for mammographic tissue segmentation. Computers in Biology and
Medicine, 67, 61‚Äì73. Available from http://dx.doi.org/10.1016/j.compbiomed.2015.10
.002.
27 Sharma, S. and Khanna, P. (2015) Computer-aided diagnosis of malignant
mammograms using Zernike moments and SVM. Journal of Digital Imaging, 28
(1), 77‚Äì90, doi: 10.1007/s10278-014-9719-7. Available from http://dx.doi.org/10.1007/
s10278-014-9719-7.
28 Kuhnigk, J.M., Dicken, V., Bornemann, L., Bakai, A., Wormanns, D., Krass, S., and
Peitgen, H.O. (2006) Morphological segmentation and partial volume analysis for
volumetry of solid pulmonary lesions in thoracic CT scans. IEEE Transactions on
Medical Imaging, 25 (4), 417‚Äì434, 10.1109/TMI.2006.871547.
29 Kubota, T., Jerebko, A.K., Dewan, M., SalganicoÔ¨Ä, M., and Krishnan, A. (2011)
Segmentation of pulmonary nodules of various densities with morphological

An Optimized Breast Cancer Diagnosis System Using a Cuckoo Search Algorithm
313
approaches and convexity models. Medical Image Analysis, 15 (1), 133‚Äì154,
doi: http://dx.doi.org/10.1016/j.media.2010.08.005. Available from http://www
.sciencedirect.com/science/article/pii/S136184151000109X.
30 Yang, X.S. and Deb, S. (2010) Engineering optimisation by cuckoo search. ArXiv
e-prints.
31 Vapnik, V. (1982) Estimation of dependences based on empirical data. Springer Series
in Statistics (Springer Series in Statistics). Springer-Verlag, New York.
32 Akay, M.F. (2009) Support vector machines combined with feature selection for
breast cancer diagnosis. Expert Systems with Applications, 36 (2, Pt. 2), 3240‚Äì3247,
doi: http://dx.doi.org/10.1016/j.eswa.2008.01.009. Available from http://www
.sciencedirect.com/science/article/pii/S0957417408000912.
33 Gorodkin, J. (2004) Comparing two k-category assignments by a k-category
correlation coeÔ¨Écient. Computational Biology and Chemistry, 28 (5-6), 367‚Äì374,
doi: http://dx.doi.org/10.1016/j.compbiolchem.2004.09.006. Available from http://
www.sciencedirect.com/science/article/pii/S1476927104000799.

315
14
Analysis of Hand Vein Images Using Hybrid Techniques
R. Sudhakar, S. Bharathi, and V. Gurunathan
Department of Electronics and Communication Engineering, Dr. Mahalingam College of Engineering and Technology,
Coimbatore, Tamil Nadu, India
14.1
Introduction
Nowadays in this informational society, there is a need of authentication and identi-
Ô¨Åcation of individuals. Biometric authentication [1] is a common and reliable way to
recognize the identity of a person based on physiological or behavioral characteris-
tics. Biometric technology oÔ¨Äers promise of a trouble-free and safe method to make
an extremely precise authentication of a person. Alternative unique symbols like pass-
words and identity cards have the susceptibility of being easily misplaced, pooled, or
stolen. Passwords may also be easily cracked by means of social engineering and dic-
tionary attacks [2, 3] and oÔ¨Äer insigniÔ¨Åcant protection. As biometric techniques make
it a requirement for the client to be actually present during authentication, it prevents
the possibility of clients making false repudiation claims at a later stage [4]. Several of
the biometric systems installed in real-world applications are unimodal [5‚Äì7], which
depends on a single source of information for authentication. Some of the restrictions
enacted by unimodal biometric systems can be removed by using several sources of
information for creating identity. When various sources of information are used for
biometric recognition, the system is expected to be more consistent due to the exis-
tence of multiple forms of evidence. One such method is recognition of individuals by
means of multimodal biometrics. Multimodal biometric recognition [8, 9] is a method
of recognizing a person by using two or more biometric modalities.
The signiÔ¨Åcant objective of multimodal biometrics is to improve the accuracy of
recognition over a deÔ¨Ånite method by combining the results of several traits, sensors,
or algorithms. The choice of the relevant modality is a demanding task in multimodal
biometric systems for the identiÔ¨Åcation of a person. In recent days, human authen-
tication by means of hand vein modalities has attracted the ever-increasing interest
of investigators because of its superior performance over other modalities [10‚Äì12].
A personal authentication system using Ô¨Ånger vein patterns was proposed by Naoto
Miura et al. [13]. They have used a repeated line-tracking method to extract features
from the Ô¨Ånger vein images and template matching method to Ô¨Ånd the similarity
between the vein images. The performance of this method was improved by using a
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

316
Hybrid Intelligence for Image Analysis and Understanding
method of calculating local maximum curvatures in cross-sectional proÔ¨Åles [14] of a
vein image by the same author. Also, morphological algorithm-based feature extraction
for Ô¨Ånger vein images was proposed by Mei et al. [15], and multifeatures such as
local and global features from the same vein patterns were proposed by Yang and
Zhang [16].
Lee [17] has proposed a biometric recognition system based on the palm vein.
He has extracted the necessary features from the palm vein images based on its texture.
A 2D Gabor Ô¨Ålter was used by him to extract local features of the palm, and the vein
features are encoded by using a directional coding technique. Finally, the similarity
between the vein codes was measured by Hamming distance metric. Wu et al. [18]
have used a directional Ô¨Ålter bank to extract line-based features of palm vein, and a
minimum directional code was proposed by them to encode the palm vein features.
Determination of the nonvein pixels was done by estimating the magnitude of the
directional Ô¨Ålter, and those pixels were assigned with a non-orientation code. Xu et al.
[19] proposed a vein recognition system using an adaptive hidden Morkov model in
which the parameters are optimized by using a stepper increasing method. They have
used radon transform to Ô¨Ånd the direction and location information of the dorsal
vein image. They concluded that the recognition time of their system was suÔ¨Écient
to meet the real-time requirement. Biometric authentication using fast correlation
of near-infrared (NIR) hand vein images was proposed by Shahin et al. [20]. The
performance of the system was veriÔ¨Åed on their own database of dorsal hand vein
images. They concluded that the hand vein pattern is unique for each and every person
and is also distinctive for each hand.
Though the veinbased recognition systems discussed here produced satisfactory per-
formance as unimodal systems, their performance can be improved further by combin-
ing two or more modalities. Kumar and Prathyusha [21] proposed an authentication
system using a combination of hand vein and knuckle shape features. This approach is
based on the structural similarity of hand vein triangulation and knuckle shape features.
A weighted combination of four diÔ¨Äerent types of triangulation is developed by them
to extract key points from the hand vein structure, and knuckle shape features are also
simultaneously extracted from the same image. Finally, these two features are combined
at the score level using a weighted score-level fusion strategy to authenticate individuals.
A multimodal biometric recognition system using simultaneously captured Ô¨Ångerprint
and Ô¨Ånger vein information from the same device was suggested by Park et al. [22]. The
Ô¨Ångerprint analysis was done based on the minutiae points of the ridge area, and Ô¨Ånger
vein analysis was performed based on the local binary pattern (LBP) with the appear-
ance information of the Ô¨Ånger area. Their results show that the combined biometrics at
the decision level provides improved performance, with a lower false acceptance rate
(FAR) and false rejection rate (FRR) than the individual traits.
A contactless palm print and palm vein recognition system was proposed by Goh et al.
[23]. They have used a linear ridge enhancement (LRE) technique for preprocessing in
order to get high-contrast images. Features from palm print and palm vein images are
extracted using directional coding techniques, which aim to encode the line patterns of
the images. The fusion of these features was carried out by them using a support vector
machine. Wang et al. [24] proposed a person recognition method based on Laplacian
palm by fusing palm print and palm vein images. Image-level fusion was carried out for
palm print and palm vein images by an edge-preserving and contrast-enhancing wavelet

Analysis of Hand Vein Images Using Hybrid Techniques
317
fusion method. The modiÔ¨Åed multiscale edges of the palm print and palm vein images
are combined by them. Using the locality preserving projections (LPPs), the Laplacian
palm feature is extracted from the fused images. They have observed that the results for
the fused palm print and palm vein images are better than the results for the palm print
and palm vein separately.
This chapter deals with biometric recognition using multiple vein images present in
the hand, such as dorsal, palm, Ô¨Ånger, and wrist vein images. In multimodal biometric
recognition systems, the fusion [25] of various modalities can be done at diÔ¨Äerent levels,
such as the sensor level, feature level [26, 27], score level [28], and decision level [29].
The basic block diagram of a multimodal biometric recognition system using hand vein
images is shown in Figure 14.1. Hand vein pattern is one of the biometric traits used
for secured authentication systems. The shape of the vascular patterns in the hand is
distinct for each hand. The shape of the veins changes with the change in the length of
the physique from childhood, but it almost always remains stable in adult life and so
is a reliable source of authentication. Veins are the network of blood vessels beneath a
person‚Äôs skin. Since the veins are hidden under the skin, it is almost impossible for a
pretender to duplicate the vein pattern. Vein biometrics has the following advantages:
1. Vein patterns are imperceptible by human eyes due to their concealed nature.
2. Not so easy to forge or steal
3. Captured by noninvasive and contact-less methods
4. Guarantees handiness and cleanliness
Biometric Trait
from Sensor
Preprocessing
and Feature
Extraction
Preprocessing
and Feature
Extraction
Biometric Trait
from Sensor
Matching
with
Database
Matching
with
Database
2. Decision
Decision
Decision
Accept / Reject
Accept / Reject
Accept / Reject
1. Fusion
1. Fusion
3 . Decision
1. Fusion
2. Matching
with Database
Figure 14.1 The basic block diagram of a multimodal biometric recognition system.

318
Hybrid Intelligence for Image Analysis and Understanding
5. Vein pattern can be acquired from a live body only.
6. Natural and substantial evidence that the subject is alive.
This chapter deals with the vein-based biometric recognition system using hybrid intel-
ligence techniques at both the spatial domain and frequency domain.
14.2
Analysis of Vein Images in the Spatial Domain
In this chapter, we Ô¨Årst study the analysis of hand vein images in the spatial domain.
Here, the processing of vein images is done by direct manipulation of pixels [30] in the
image plan itself. The features from the vein images are extracted using a modiÔ¨Åed 2D
Gabor Ô¨Ålter [31] in the spatial domain, and they are stored in the database. Then, these
extracted features are combined at the feature level using a simple sum rule [32] and
at the matching-score level [33] using fuzzy logic. Finally, the authentication rate of the
biometric system is evaluated. The Ô¨Çow diagram of the analysis of vein images in the
spatial domain is shown in Figure 14.2.
14.2.1
Preprocessing
In order to modify the original input images into the required formats, the preprocess-
ing operation will be carried out on the input images. The hand vein images captured
Database
Dorsal
Vein
Palm
Vein
Finger
Vein
Wrist
Vein
Preprocessing
Feature
Extraction
Score
Level
Fusion
Matching
Accept / Reject
Figure 14.2 Flow diagram of the analysis of vein images in the spatial domain.

Analysis of Hand Vein Images Using Hybrid Techniques
319
by using NIR cameras are of a low-contrast nature, because their blood and bio-cells for
NIR lighting have diÔ¨Äerent absorption and scattering coeÔ¨Écients. Hence, these images
are to be preprocessed in order to enrich their features for impending analyses such
as feature extraction and classiÔ¨Åcation of the individuals. Here, the vein images are
enhanced using a hierarchical method called diÔ¨Äerence of Gaussian and contrast-limited
adaptive histogram equalization.
DiÔ¨Äerence of Gaussian [34] comprises subtraction of the distorted version of an orig-
inal image from another, less distorted version of the original. Subtracting one image
from the other conserves spatial information that lies between the range of frequencies
that are conserved in the two blurred images. Hence the diÔ¨Äerence of Gaussian acts as
a band pass Ô¨Ålter that provides the required spatial information. Contrast-limited adap-
tive histogram equalization improves the contrast of the image, and it operates on the
small regions of the image, called blocks, rather than the entire image. Each block‚Äôs con-
trast is enhanced by using bilinear interpolation, and the neighboring blocks are then
combined to remove artiÔ¨Åcially created boundaries. The contrast in each block can be
limited to avoid the ampliÔ¨Åcation of any kind of noise that exists in the image.
14.2.2
Feature Extraction
Feature extraction is a distinct form of dimensionality reduction in the Ô¨Åeld of image
processing and pattern recognition. When the input data to the system is very large
to be processed and it is suspected to be too redundant, then it will be represented as
a set of features (also named feature vectors). If the features are chosen carefully and
extracted, then the desired task will be performed easily on these features rather than
on the entire image. From the preprocessed images, the necessary features are extracted
here using a modiÔ¨Åed 2D Gabor Ô¨Ålter.
The Gabor Ô¨Ålter, named after Dennis Gabor, is a linear Ô¨Ålter used for detecting edges in
image processing. A 2D Gabor Ô¨Ålter is a Gaussian kernel function in the spatial domain
that is modulated by a sinusoidal plane wave. Depictions of Gabor Ô¨Ålters in terms of
frequency and orientation are similar to those of the human visual system, and hence
image analysis using these Ô¨Ålters is assumed to be similar to the perception of the human
visual system. Due to its 2D spectral eÔ¨Éciency for texture as well as its variation with
2D spatial positions, Gabor Ô¨Ålters are enormously useful for texture analysis. Daugman
developed the 2D Gabor functions, which have good spatial localization, orientation
selectivity, and frequency selectivity.
In recent years, Gabor Ô¨Ålters have been successfully used in many applications, such
as target detection, face recognition, edge detection, texture segmentation, image
analysis, and compression. The general form of the Gabor Ô¨Ålter is represented as in
Equation (14.1).
h(x, y, f , ùúé, ùúÉ) =
1
‚àöùúãùúéxùúéy
exp
{
‚àí1
2
(
x2
ùúÉ
ùúé2
x
+
y2
ùúÉ
ùúé2
y
)}
exp{i( fxx + fyy)}
(14.1)
where:
xùúÉ= xcosùúÉ+ ysinùúÉ; yùúÉ= ‚àíxsinùúÉ+ ycosùúÉ
ùúéx = c1
f ; ùúéy = c2
f ; fx = f cos ùúÉ; fy = f sinùúÉ
and where the variables x and y are the pixel coordinates of the image, ùúéx and ùúéy are
standard deviations of the 2D Gaussian envelope, c1 and c2 are constants, f is the central

320
Hybrid Intelligence for Image Analysis and Understanding
frequency of the pass band, and ùúÉrefers to the spatial orientation. The frequency values
fk are selected in the interval of 0 to ùúã, and orientations (ùúÉm) are uniformly distributed
in the interval of 0 to ùúã.
Numerous Gabor Ô¨Ålter designs are obtained by making some modiÔ¨Åcations to the
analytical formula given in Equation (14.1). There are no general methods for selection of
Gabor Ô¨Ålter parameters. The real (even-symmetry) and imaginary (odd-symmetry) parts
of the Gabor Ô¨Ålter are used for ridge and edge detections, respectively. The analytical
form of an even-symmetric Gabor Ô¨Ålter is given in Equation (14.2):
h(x, y, f , ùúé, ùúÉ) =
1
‚àöùúãùúéxùúéy
exp
{
‚àí1
2
(
x2
ùúÉ
ùúé2
x
+
y2
ùúÉ
ùúé2
y
)}
cos( fxx + fyy)
(14.2)
The 2D Gabor Ô¨Ålter is modiÔ¨Åed by introducing the smoothness parameter ùúÇ, which is
deÔ¨Åned in Equation (14.3). The modiÔ¨Åed 2D Gabor Ô¨Ålter is employed to extract the fea-
tures from hand vein images because the Gabor Ô¨Ålter is invariant to intensity of light,
rotation, scaling, and translation. Furthermore, they are immune to photometric distur-
bances, for instance changes in the illumination of the image and noise in the image.
G(x, y) = f 2
ùúãùõæùúÇexp
{‚àíx‚Ä≤2 + ùõæ2y‚Ä≤2
2ùúé2
}
exp{(j2ùúãfx‚Ä≤ + ùúô)}
(14.3)
where x‚Ä≤ = xcosùúÉ+ ysinùúÉ, y‚Ä≤ = ‚àíxsinùúÉ+ ycosùúÉ, f is the frequency of the sinusoidal fac-
tor, ùúÉis the orientation of the Gabor function, ùúôis the phase oÔ¨Äset, ùúéis the standard,
deviation, and ùúÇis the smoothness parameter of the Gaussian envelope in the direction
of the wave and orthogonal to it. The parameters ùúéand ùúÇadjust the smoothness of the
Gaussian function, and ùõæis the spatial aspect ratio that speciÔ¨Åes the ellipticity of the
Gabor function. The frequency response of the Gabor Ô¨Ålter is made to cover the entire
frequency plane by changing the values fmax and ùúÉsimultaneously. The maximum fre-
quency fmax can be varied from 0.1 to 2.0, ùúÉcan be varied from ùúã
8 to ùúã, and the aspect
ratio from 0 to 1.5. We employed the modiÔ¨Åed Gabor Ô¨Ålter by keeping ùúîand ùúÇas con-
stants and varying the parameters ùúÉand ùõæ, so as to extract the features from the hand
vein images; and then these extracted features are stored in the database.
14.2.3
Feature-Level Fusion
In feature-level fusion, the features of multiple biometric traits are amalgamated to form
a single feature vector in order to have an appreciable recognition rate. Here, the fea-
ture vectors of hand vein images such as dorsal, palm, wrist, and Ô¨Ånger vein images
are combined using a simple sum rule to generate a single feature vector. The fused
images are given in the ‚ÄúResults and Discussion‚Äù sections of this chapter (Sections 14.2.5
and 14.3.5).
14.2.4
Score-Level Fusion
In the score-level fusion, there are two approaches for consolidating the scores obtained
from diÔ¨Äerent matchers such as classiÔ¨Åcation and combination techniques. In the classi-
Ô¨Åcation method, a feature vector is created by using matching scores of the individuals,
and then it is classiÔ¨Åed as ‚ÄúAccept‚Äù or ‚ÄúReject.‚Äù In the combination method, the distinct
matching scores are united to generate a single scalar score for making the Ô¨Ånal decision.

Analysis of Hand Vein Images Using Hybrid Techniques
321
After extracting the necessary features from the hand vein images, the fusion
of them is done at the score level using fuzzy logic. In the recognition stage,
the matching score of hand vein images is computed by using the Euclidean dis-
tance metrics. Let pt = (pt1, pt2, ‚Ä¶ , ptn) be the feature vector of the test palm
vein images, ptr = (ptr1, ptr2, ‚Ä¶ , ptrn) be the feature vector of the trained palm
vein images, dt= (dt1, dt2, ‚Ä¶ , dtn) be the feature vector of the test dorsal vein
images, dtr = (dtr1, dtr2, ‚Ä¶ , dtrn) be the feature vector of the trained dorsal vein
images, ùë§t = (ùë§t1, ùë§t2, ‚Ä¶ , ùë§tn) be the feature vector of the test wrist vein images,
ùë§tr = (ùë§tr1, ùë§tr2, ‚Ä¶ , ùë§trn) be the feature vector of the trained wrist vein images,
ft = ( ft1, ft2, ‚Ä¶ , ftn) be the feature vector of the test Ô¨Ånger vein images, and
ftr = ( ftr1, ftr2, ‚Ä¶ , ftrn) be the feature vector of the trained Ô¨Ånger vein images. The
corresponding score values of the vein images can be calculated by the Euclidean
distance using formulations (14.4) through (14.7):
S1 = D(pt, ptr) =
‚àö
(pt1 ‚àíptr2)2 + (pt2 ‚àíptr2)2 + .... + (ptn ‚àíptrn)2
(14.4)
S1 = D(pt, ptr) =
‚àö
‚àö
‚àö
‚àö
n
‚àë
i=1
(pti ‚àíptri)2
S2 = D(dt, dtr) =
‚àö
(dt1 ‚àídtr2)2 + (dt2 ‚àídtr2)2 + .... + (dtn ‚àídtrn)2
(14.5)
S2 = D(dt, dtr) =
‚àö
‚àö
‚àö
‚àö
n
‚àë
i=1
(dti ‚àídtri)2
S3 = D(ùë§t, ùë§tr) =
‚àö
(ùë§t1 ‚àíùë§tr2)2 + (ùë§t2 ‚àíùë§tr2)2 + .... + (ùë§tn ‚àíùë§trn)2
(14.6)
S3 = D(ùë§t, ùë§tr) =
‚àö
‚àö
‚àö
‚àö
n
‚àë
i=1
(ùë§ti ‚àíùë§tri)2
S4 = D( ft, ftr) =
‚àö
( ft1 ‚àíftr2)2 + ( ft2 ‚àíftr2)2 + .... + ( ftn ‚àíftrn)2
(14.7)
S4 = D( ft, ftr) =
‚àö
‚àö
‚àö
‚àö
n
‚àë
i=1
( fti ‚àíftri)2
Decisions based on fuzzy fusion scores: Finally, the person is authorized to compute the
Ô¨Ånal score from the scores of input vein images. Score values of the individual vein
images are combined using fuzzy logic to obtain the Ô¨Ånal score value. Fuzzy logic‚Äìbased
score-level fusion is obtained through the following steps. The Ô¨Årst step is fuzziÔ¨Åca-
tion: the fuzziÔ¨Åer receives the Ô¨Ånal matching score, and it converts the numerical data
into the linguistic values (high, medium, and low) using the membership function. The
membership function describes how each score is converted into a membership value
(or degree of membership). The Gaussian membership function is used to obtain the
degree of membership and is deÔ¨Åned as in the following Equation (14.8).
Gaussian(x; c, ùúé) = exp‚àí1
2
(x ‚àíc
ùúé
)2
(14.8)

322
Hybrid Intelligence for Image Analysis and Understanding
where x represents the membership function, c represents the center, and ùúérepresents
its width.
The next step is rule designing: The main signiÔ¨Åcant module in any fuzzy system is
designing fuzzy rules. Four input variables and one output variable are used here for
deÔ¨Åning fuzzy if-then rules. The rules deÔ¨Åned in the rule base are as follows:
If S1, S2, and S3 are HIGH and S4 is LOW, then output is recognized.
If S1, S2, and S4 are HIGH and S3 is LOW, then output is recognized.
If S1, S3, and S4 are HIGH and S2 is LOW, then output is recognized.
If S2, S3, and S4 are HIGH and S1 is LOW, then output is recognized.
If S1, S2, S3, and S4 are HIGH, then output is recognized.
The inference engine: receives the information from the fuzziÔ¨Åer, and it compares the
speciÔ¨Åc fuzziÔ¨Åed input with the rule base. Hence, the score is computed, and the infer-
ence engine provides the output in the form of linguistic values (high, medium, and low).
Finally, with the defuzziÔ¨Åer, the linguistic values from the inference engine are converted
to crisp values by the defuzziÔ¨Åer. The crisp value indicates the Ô¨Ånal fuzzy score and is
used for authentication.
14.2.5
Results and Discussion
The experimental results of the proposed method, for analyzing the hand vein images in
the spatial domain, are discussed here. The database used for investigation is obtained
from the standard databases of the dorsal hand vein (Hand vein database: Prof. Dr.
Ahmed M. Badawi, Systems and Biomedical Engineering, Cairo University), Ô¨Ånger
vein (Finger vein database: http: //mla.sdu.edu.cn/sdumla-hmt.html), and palm vein
and wrist images (Palm and Wrist vein database: http://biometrics.put.poznan.pl/vein-
dataset). The experimental outcomes of hand vein images at various stages in spatial
domain are as follows. The input hand vein images and their respective preprocessed
images are illustrated in the Figure 14.3.
Dorsal vein
Palm vein
Finger vein
Wrist vein
Figure 14.3 Input hand vein images (Ô¨Årst row). Preprocessed vein images (second row).

Analysis of Hand Vein Images Using Hybrid Techniques
323
14.2.5.1
Evaluation Metrics
For experimental evaluation, FAR, FRR, and recognition accuracy will be considered as
evaluation metrics.
False acceptance rate: FAR is the measure of the likelihood that a biometric security
system will incorrectly accept an access attempt by an unauthorized user. FAR typically
is stated as the ratio of the number of false acceptances divided by the number of iden-
tiÔ¨Åcation attempts.
False rejection rate: FRR is the measure of the likelihood that the biometric security
system will incorrectly reject an access attempt by an authorized user. FRR typically is
stated as the ratio of the number of false rejections divided by the number of identiÔ¨Åca-
tion attempts.
As we have discussed in Section 14.2, the Gabor Ô¨Ålter is designed by Ô¨Åxing the value
of center frequency ùúî= 0.25 and varying the value of aspect ratio and orientation
simultaneously. The output of the Gabor Ô¨Ålter for the above four kinds of hand vein
images are illustrated in Figure 14.4‚Äì14.6, and 14.7. Fusion of hand vein images at the
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Figure 14.4 Output of Gabor Ô¨Ålter for Palm vein images with ùúî= 0.25: (a) ùúÉ= ùúã
8 , ùõæ= 0.15; (b) ùúÉ= 2ùúã
8 ,
ùõæ= 0.15; (c) ùúÉ= 3ùúã
8 , ùõæ= 0.15; (d) ùúÉ= ùúã
8 , ùõæ= 0.55; (e) ùúÉ= 2ùúã
8 , ùõæ= 0.55; (f) ùúÉ= 7ùúã
8 , ùõæ= 0.55;
(g) ùúÉ= 3ùúã
8 , ùõæ= 0.55; (h) ùúÉ= 5ùúã
8 , ùõæ= 0.15; and (i) ùúÉ= 7ùúã
8 , ùõæ= 0.15.

324
Hybrid Intelligence for Image Analysis and Understanding
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Figure 14.5 Output of Gabor Ô¨Ålter for dorsal vein images with ùúî= 0.25: (a) ùúÉ= ùúã
8 , ùõæ= 0.15; (b) ùúÉ= 2ùúã
8 ,
ùõæ= 0.15; (c) ùúÉ= 3ùúã
8 , ùõæ= 0.15; (d) ùúÉ= ùúã
8 , ùõæ= 0.55; (e) ùúÉ= 2ùúã
8 , ùõæ= 0.55; (f) ùúÉ= 7ùúã
8 , ùõæ= 0.55;
(g) ùúÉ= 3ùúã
8 , ùõæ= 0.55; (h) ùúÉ= 5ùúã
8 , ùõæ= 0.15; and (i) ùúÉ= 7ùúã
8 , ùõæ= 0.15.
(a)
(b)
(c)
Figure 14.6 Output of Gabor Ô¨Ålter for Wrist vein images with ùúî= 0.25: (a) ùúÉ= ùúã
8 , ùõæ= 0.15; (b) ùúÉ= 2ùúã
8 ,
ùõæ= 0.15; (c) ùúÉ= 3ùúã
8 , ùõæ= 0.15; (d) ùúÉ= ùúã
8 , ùõæ= 0.55; (e) ùúÉ= 2ùúã
8 , ùõæ= 0.55; (f) ùúÉ= 7ùúã
8 , ùõæ= 0.55;
(g) ùúÉ= 3ùúã
8 , ùõæ= 0.55; (h) ùúÉ= 5ùúã
8 , ùõæ= 0.15; and (i) ùúÉ= 7ùúã
8 , ùõæ= 0.15.

Analysis of Hand Vein Images Using Hybrid Techniques
325
(d)
(e)
(f)
(g)
(h)
(i)
Figure 14.6 (Continued)
(a)
(b)
(c)
(d)
(e)
(f)
Figure 14.7 Output of Gabor Ô¨Ålter for Finger vein images with ùúî= 0.25: (a) ùúÉ= ùúã
8 , ùõæ= 0.15; (b) ùúÉ= 2ùúã
8 ,
ùõæ= 0.15; (c) ùúÉ= 3ùúã
8 , ùõæ= 0.15; (d) ùúÉ= ùúã
8 , ùõæ= 0.55; (e) ùúÉ= 2ùúã
8 , ùõæ= 0.55; (f) ùúÉ= 7ùúã
8 , ùõæ= 0.55;
(g) ùúÉ= 3ùúã
8 , ùõæ= 0.55; (h) ùúÉ= 5ùúã
8 , ùõæ= 0.15; and (i) ùúÉ= 7ùúã
8 , ùõæ= 0.15.

326
Hybrid Intelligence for Image Analysis and Understanding
(g)
(h)
(i)
Figure 14.7 (Continued)
feature level is carried out using a sum rule, and the resultant fused images are shown
in Figures 14.8 to 14.10. Figure 14.8 illustrates the fusion of two modalities of hand
vein features, and the fusion of three modalities of vein features is given in Figure 14.9.
Finally, Figure 14.10 shows the fusion of all four vein modalities. Table 14.1 indicates
the analysis of hand vein images in unimodal mode (single biometric trait) in terms
of FAR and FRR. Table 14.2 indicates the analysis of hand vein images in multimodal
mode at both the feature level and score level in terms of FAR and FRR. From the above
results, we observed that the performance of the vein-based biometric system in the
multimodal mode is better compared to the unimodal mode.
14.3
Analysis of Vein Images in the Frequency Domain
In this section, we study how the analysis of hand vein images can be carried out in
the frequency domain [35]. Here, after preprocessing the input hand vein images, the
necessary features are extracted by applying contourlet transform [36]. These extracted
hand vein features in the form of coeÔ¨Écients are stored in the database. Then, these
features are fused at the feature level using the multiresolution singular value decom-
position (MSVD) [37] method, and their recognition will be done using SVM [38, 39]
classiÔ¨Åer. Finally, the recognition rate of the system will be analyzed. The Ô¨Çow diagram
of the method for analyzing the hand vein images in the frequency domain is shown in
Figure 14.11.
14.3.1
Preprocessing
Here, the input hand vein images are enhanced initially using the diÔ¨Äerence of Gaussian
(the same as in the above time domain technique), and then adaptive thresholding is
performed in order to make the images more suitable for frequency domain analysis.
14.3.2
Feature Extraction
As seen in the discussion on time domain analysis, the feature extraction is a special
form of dimensionality reduction. Here, the necessary features are extracted using the
contourlet transform. The contourlet transform is a new 2D transform method for
image representations. It has properties of multiresolution, localization, directionality,

Analysis of Hand Vein Images Using Hybrid Techniques
327
Palm
Palm
Finger
Finger
Dorsal
Dorsal
Dorsal
Wrist
Wrist
Fused Image
Fused Image
Palm
Fused Image
Fused Image
Finger
Fused Image
Wrist
Fused Image
Figure 14.8 Fusion of two modalities of hand vein features.

328
Hybrid Intelligence for Image Analysis and Understanding
Palm
Palm
Finger
Finger
Fused Image
Fused Image
Fused Image
Dorsal
Dorsal
Dorsal
Wrist
Wrist
Figure 14.9 Fusion of three modalities of hand vein features.
Palm
Finger
Fused Image
Dorsal
Wrist
Figure 14.10 Fusion of all four modalities of hand vein features.

Analysis of Hand Vein Images Using Hybrid Techniques
329
Table 14.1 Analysis of hand vein images in unimodal
mode
Sample no.
Image
FAR
FRR
Accuracy
1
Dorsal
0.225
0.2
83.21
2
Wrist
0.2
0.15
81.75
3
Palm
0.3
0.175
82.42
4
Finger
0.2
0.125
84.37
Table 14.2 Analysis of hand vein images in multimodal mode
Sample
no.
Image
Feature-level
fusion
Accuracy
Score-level
fusion
Accuracy
FAR
FRR
FAR
FRR
1
Palm + Ô¨Ånger
0.14
0.12
87.18
0.12
0.11
88.45
2
Palm + dorsal
0.12
0.2
86.75
0.12
0.12
89.36
3
Palm + wrist
0.14
0.12
85.5
0.11
0.1
88.12
4
Finger + dorsal
0.16
0.2
85.72
0.1
0.1
90.37
5
Finger + wrist
0.1
0.12
86.5
0.08
0.06
88.75
6
Dorsal + wrist
0.2
0.2
85.75
0.1
0.1
89.67
7
Palm + Ô¨Ånger + dorsal
0.05
0.04
90.12
0
0.02
94.4
8
Finger + dorsal + wrist
0.04
0.02
88.78
0.012
0.01
93.67
9
Palm + dorsal + wrist
0.02
0.04
91.37
0.012
0
94.12
10
Palm + dorsal + Ô¨Ånger + wrist
0.011
0.01
93.75
0.01
0
95.38
Database
Dorsal
Vein
Palm
Vein
Finger
Vein
Wrist
Vein
Preprocessing
Feature
Extraction and
Fusion
(MSVD)
Matching
and
Classification
(SVM)
Accept or Reject
Figure 14.11 Flow diagram of the analysis of vein images in the frequency domain.

330
Hybrid Intelligence for Image Analysis and Understanding
critical sampling, and anisotropy. Its basic functions are multiscaling and multidimen-
sioning. A double Ô¨Ålter bank structure that comprises a Laplacian pyramid (LP) and
directional Ô¨Ålter bank (DFB) is used by the contourlet transform in order to get the
smooth contours of images. With the combination of the DFP and Laplacian pyramid,
the multiscale decomposition is achieved and also the low-frequency components
are removed. Hence, the image signals are passed through LP subbands in order to
get band pass signals and through DFB to acquire the directional information of the
image.
14.3.3
Feature-Level Fusion
In order to combine the features of multiple biometric traits, here the MSVD technique
is used. MSVD is very similar to wavelet transform, and here the idea is to replace the
Ô¨Ånite impulse response Ô¨Ålters with singular value decomposition (SVD). The schematic
of the fusion for the hand vein images is shown in Figure 14.12.
The vein images to be fused are decomposed into L (l=1, 2,‚Ä¶ , L) levels using MSVD.
At each decomposition level (l=1, 2,‚Ä¶ , L), the fusion rule will select the sum of the
four MSVD detailed coeÔ¨Écients, since the detailed coeÔ¨Écients correspond to sharper
intensity changes in the image, such as lines and edges of the vein images and so on.
The approximation coeÔ¨Écients are not considered for fusion since the approximation
coeÔ¨Écients at the coarser level are the smoothed and subsampled version of the original
image and similarly MSVD eigen-matrices. The fused vein image can be represented as
Equation (14.9)
If = {f ùúìùë£
l ,f ùúìH
l ,f ùúìL
l
}
(14.9)
Here, the contourlet transformed vein images are fused using MSVD so that the
improved vein features were achieved. These fused images are classiÔ¨Åed using SVM
classiÔ¨Åer in order to know whether the person is authenticated or not.
MSVD
fœàl
V = sum (1œàl
V, 2œàl
V, 3œàl
V, 4œàl
V)
fœàl
H = sum (1œàl
H, 2œàl
H, 3œàl
H, 4œàl
H)
fœàl
D = sum (1œàl
D, 2œàl
D, 3œàl
D, 4œàl
D)
MSVD
Images to
be fused
MSVD
MSVD
MSVD
Fused
Image
MSVD of
fused image
Fusion rules
MSVDs of
vein images
Figure 14.12 Fusion for the hand vein images in the frequency domain.

Analysis of Hand Vein Images Using Hybrid Techniques
331
Figure 14.13 A linear support vector machine.
Hyperplane
Support vectors
+
+
+
‚Äì
‚Äì
‚Äì
++++++
+++++++
+++++++
+++++++
14.3.4
Support Vector Machine ClassiÔ¨Åer
SVMs are one of the supervised learning methods used for classiÔ¨Åcation, regression, and
outlier detection. The concept of a hyperplane is used by the classiÔ¨Åer to calculate a bor-
der between groups of points. The objective of the SVM is to Ô¨Ånd not only a separation
between the classes but also the optimum hyperplane. Figure 14.13 shows the principle
of a linear SVM.
Let the training samples be represented as pairs (‚Éóxi ‚àíyi), where ‚Éóxi is the weighted
feature vector of the training sample and yi ‚àà(‚àí1, +1) is the label. For linearly separa-
ble data, we can determine a hyperplane f (x) = 0 that separates the data, as shown in
Equation (14.10):
f (x) =
n
‚àë
i=1
ùë§xi + b = 0
(14.10)
where ùë§is an n-dimensional vector and b is a scalar value. The vector ùë§and the scalar
b determine the position of the separating hyperplane. For each i, either is as shown in
Equation (14.11):
{
ùë§xi ‚àíb ‚â•1 for one class
ùë§xi ‚àíb ‚â§‚àí1 for another class
(14.11)
14.3.5
Results and Discussion
The experimental results of the proposed method, for analyzing the hand vein images in
the frequency domain, are discussed here. As mentioned in Section 14.2.5, the database
utilized for our experimentation is taken from the standard databases of dorsal hand
vein, palm vein, Ô¨Ånger vein, and wrist images. The experimental results of hand vein
images at various stages are as follows. Figure 14.14 shows the input hand vein images
and their respective preprocessed images.
The feature-extracted hand vein images using contourlet transform are shown in
Figure 14.15. These extracted vein features are fused by using MSVD, and this fused
vector is given to a SVM for classiÔ¨Åcation. Here we have used linear SVM in order

332
Hybrid Intelligence for Image Analysis and Understanding
Input vein
images
DOG images
Thresholding
image
Filtered and ROI
image
Figure 14.14 Preprocessed hand vein images at various stages.
to classify the features, which in turn enables us to know whether the person is
authenticated or not.
Table 14.3 illustrates the frequency domain analysis of hand vein images in terms
of sensitivity, speciÔ¨Åcity, and accuracy. From the table, we observed that the accuracy
rate of recognition of the fused image is as high as 96.66% compared to the unimodal
(single biometric trait) system. Also, SVM classiÔ¨Åer provides a good accuracy rate of
above 90% irrespective of the modalities of the biometric system.
14.4
Comparative Analysis of Spatial and Frequency Domain
Systems
This section deals with the comparison of spatial and frequency domain systems and the
comparative analysis of multimodal biometric systems.
Biometrics is a technology of analyzing human characteristics to authenticate a per-
son in a secured manner. In order to identify a person, the biometric features can be

Analysis of Hand Vein Images Using Hybrid Techniques
333
(a)
(b)
(c)
(d)
Figure 14.15 Contourlet-transformed vein images: (a) dorsal hand vein, (b) palm vein, (c) wrist vein,
and (d) Ô¨Ånger vein.
Table 14.3 Frequency domain analysis of hand vein images
Sample no.
Image
FAR
FRR
Sensitivity
SpeciÔ¨Åcity
Accuracy
1
Dorsal
0.12
0.03
0.909
0.95
92.95
2
Wrist
0.2
0.11
0.88
0.95
91.5
3
Palm
0.15
0.02
0.887
0.9375
91.22
4
Finger
0.11
0.04
0.889
0.9411
91.803
5
Fused vein image
0.001
0
0.9687
0.9642
96.66
extracted and analyzed using spatial and frequency domain methods. There are numer-
ous techniques available in these domains for analyzing the features.
In the spatial domain, the required features are extracted from the biometric traits
directly by applying spatial domain techniques on image pixels without converting them
into the transform domain. Here, a modiÔ¨Åed 2D Gabor Ô¨Ålter is used to extract the fea-
tures from the vein modalities in the spatial domain, and the extracted features are
combined as a single feature vector at both the feature level and score level for mul-
timodal recognition.
In the frequency domain, initially the biometric traits are converted from the spatial
domain to the transform domain, and then the required features are extracted in the
form of frequency coeÔ¨Écients. Here, a contourlet transform is used to extract the
features from the vein modalities in the frequency domain, and the extracted frequency
coeÔ¨Écients are combined at the feature level in order to have multimodal recognition.
Features extracted using both the spatial domain and frequency domain techniques
can be combined to obtain the hybrid features, and these features can be used for

Table 14.4 Comparative analysis of related work on vein-based biometric recognition
Author
Biometric features
Methodology
No. of users
Performance
Lin et al. [40]
Palm-dorsal vein
Multiresolution analysis and
combination
32
FAR = 1.5
FRR = 3.5
EER = 3.75
Kumar [21]
Hand vein and knuckle shape
Matching vein triangulation
shape features
100
FAR = 1.14
FRR = 1.14
Raghavendra et al. [41]
Hand vein and palm print
Log Gabor transform
50
FAR = 7.4
FRR = 4.8
Raghavendra et al. [41]
Hand vein and palm print
Nonstandard mask
50
FAR = 2.8
FRR = 1.4
Ferrer et al. [42]
Hand geometry, palm and
Ô¨Ånger textures, dorsal hand
vein
Simple sum rule
50
FAR = 0.01
FRR = 0.01
EER = 0.01
Yang et al. [16]
Finger vein and Ô¨Ångerprint
Supervised local-preserving
canonical correlation analysis
method
640
FAR = 1.35
FRR = 0
Li et al. [43]
Finger vein and Ô¨Ånger shape
Finger vein network
Extraction algorithm
250
FAR = 2.25
Rattani et al. [44]
Face and Ô¨Ångerprint
Scale-invariant feature
Transform features Minutiae
matching
400
FAR = 4.95
FRR = 1.12
Yuksel [45]
Hand vein
ICA1
ICA2
LEM
NMF
100
EER = 5.4
EER = 7.24
EER = 7.64
EER = 9.17
Proposed method
Dorsal hand vein, palm vein
Ô¨Ånger vein, and wrist vein
Gabor Ô¨Ålter, sum rule, and
fuzzy logic
100
FAR = 0.01
Accuracy = 95.38
Proposed method
Dorsal hand vein, palm vein
Ô¨Ånger vein, and wrist vein
Contourlet transform and
MSVD
100
FAR = 0.001
FRR = 0
Accuracy = 96.67

Analysis of Hand Vein Images Using Hybrid Techniques
335
recognition. The comparative analysis of the related work on vein-based authentication
is given in Table 14.4. The results oÔ¨Äered in this table are in terms of equal error rate
(EER), FAR, and FRR. EER is deÔ¨Åned as a point at which FAR is equal to FRR. The lower
the values of EER, the better the performance of the system. The performance of the
system varies according to the imaging technique, type of biometric trait, number of
traits used for fusion, methodologies used for feature extraction and fusion, level of
fusion of these features, and number of users in the database.
14.5
Conclusion
Analysis of hand vein images such as dorsal, palm, wrist, and Ô¨Ånger vein images is carried
out in the spatial domain as well as the frequency domain. In the spatial domain, a mod-
iÔ¨Åed Gabor Ô¨Ålter is employed to extract the features from vein images, and the fusion
of these vein features is done at both the feature level and score level. The results show
that the score-level fusion of vein features using fuzzy logic provides improved results
compared to the feature-level fusion. Similarly, in the frequency domain, a contourlet
transform is utilized for feature extraction, and the fusion of these features is done at the
feature level using the multiresolution singular value decomposition technique. Finally,
these are classiÔ¨Åed with the help of SVM classiÔ¨Åer, and the results show higher accuracy
of 96.66%. Hence, by analyzing hand vein images with the help of hybrid intelligence
techniques, the improved results in terms of higher recognition accuracy with lower
false acceptance and false rejection rates are obtained. Furthermore, the results can be
improved with hybrid features, which are obtained by combining both spatial and fre-
quency domain features.
References
1 Jain, A.K., Ross, A. and Prabhakar, S. (2004) An introduction to biometric recog-
nition. IEEE Transactions on Circuits and Systems for Video Technology, 14 (1),
4‚Äì20.
2 Akhtar, Z., Fumera, G., Marcialis, G. and Roli, F. (2012) Evaluation of multimodal
biometric score fusion rules under spoof attacks, in Biometrics, 2012, Proceedings of
the Third IEEE International Conference, IEEE, pp. 402‚Äì407.
3 Jain, A.K., Ross, A. and Prabhakar, S. (2006) Biometrics: a tool for information secu-
rity. IEEE Transactions on Information Forensics and Security, 1 (2), 125‚Äì143.
4 Jain, A.K., Nandakumar, K., Lu, X. and Park, U. (2004) Integrating faces,
Ô¨Ånger-prints and soft biometric traits for user recognition, in Authentication, 2004.
BioAW 2004, Proceedings of ECCV International Workshop, pp. 259‚Äì269.
5 Khan, M.H.-M., Khan, N.M. and Subramanian, R.K. (2010) Feature extraction of
dorsal hand vein pattern using a fast modiÔ¨Åed PCA algorithm based on Cholesky
decomposition and Lanczos technique. World Academy of Science, Engineering and
Technology, 61 (2), 279‚Äì283.
6 Wang, J., Yu, M., Qu, H. and Li, B. (2013) Analysis of palm vein image quality and
recognition with diÔ¨Äerent distance, in Digital Manufacturing and Automation, 2013,
Proceedings of Fourth International Conference, pp. 215‚Äì218.

336
Hybrid Intelligence for Image Analysis and Understanding
7 Liu, Z. and Song, S.L. (2012) An embedded real-time Ô¨Ånger-vein recognition system
for mobile devices. IEEE Transactions on Consumer Electronics, 58 (2), 522‚Äì527.
8 Sanjekar, P.S. and Patil, J.B. (2013) An overview of multimodal biometrics. Signal &
Image Processing: An International Journal, 4 (1), 57‚Äì64.
9 Zhu, L. and Zhang, S. (2010) Multimodal biometric identiÔ¨Åcation system based
on Ô¨Ånger geometry, knuckle print and palm print. Pattern Recognition Letters, 31,
1641‚Äì1649.
10 Al-Juboori, A.M., Bu, W., Wu, X. and Zhao, Q. (2013) Palm vein veriÔ¨Åcation using
Gabor Ô¨Ålter. International Journal of Computer Science Issues, 10 (1), 678‚Äì684.
11 Zhang, Y.-B., Li, Q., You, J. and Bhattacharya, P. (2007) Palm vein extraction and
matching for personal authentication. Advances in Visual Information Systems, 4781,
154‚Äì164.
12 Yang, J. and Shi, Y. (2012) Finger-vein ROI localization and vein ridge enhancement.
Pattern Recognition Letters, 33 (12), 1569‚Äì1579.
13 Miura, N., Nagasaka, A. and Miyatake, T. (2004) Feature extraction of Ô¨Ånger-vein
patterns based on repeated line tracking and its application to personal identiÔ¨Åca-
tion. Machine Vision and Applications, 15 (4), 194‚Äì203.
14 Miura, N., Nagasaka, A. and Miyatake, T. (2005) Extraction of Ô¨Ånger-vein patterns
using maximum curvature points in image proÔ¨Åles, in Machine Vision Applications,
2005, Proceedings of IAPR Conference, pp. 347‚Äì350.
15 Mei, C.-L., Xiao, X., Liu, G.-H., Chen, Y. and Li, Q-A. (2009) Feature extraction of
Ô¨Ånger-vein image based on morphologic algorithm, in Fuzzy Systems and Knowledge
Discovery, 2009, Proceedings of IEEE International Conference, IEEE, pp. 407‚Äì411.
16 Yang, J. and Zhang, X. (2010) Feature-level fusion of global and local features for
Ô¨Ånger-vein recognition, in Signal Processing, 2010, Proceedings of IEEE International
Conference, IEEE, pp. 1702‚Äì1705.
17 Lee, J.C. (2012) A novel biometric system based on palm vein image. Pattern Recog-
nition Letters, 33 (12), 1520‚Äì1528.
18 Wu, K.-S., Lee, J.-C., Lo, T.-M., Chang, K.-C. and Chang, C.-P. (2013) A secure palm
vein recognition system. The Journal of Systems and Software, 86 (11), 2870‚Äì2876.
19 Jia, X., Cui, J., Xue, D. and Pan, F. (2012) A adaptive dorsal hand vein recognition
algorithm based on optimized HMM. Journal of Computational Information Systems,
8 (1), 313‚Äì322.
20 Shahin, M.K., Badawi, A. and Kamel, M. (2007) Biometric authentication using fast
correlation of near infrared hand vein patterns. International Journal of Biological
and Medical Sciences, 2 (3), 141‚Äì148.
21 Kumar, A. and Prathyusha, K.V. (2009) Personal authentication using hand vein
triangulation and knuckle shape. IEEE Transactions on Image Processing, 18 (9),
2127‚Äì2136.
22 Park, Y.H., Tien, D.T., Lee, H.C., Park, K.R., Lee, E.C., Kim, S.M. and Kim, H.C.
(2011) A multimodal biometric recognition of touched Ô¨Ångerprint and Ô¨Ånger-vein, in
Multimedia and Signal Processing, 2011, Proceedings of International Conference, pp.
247‚Äì250.
23 Goh, K.O.M., Tee, C. and Teoh, A.B.J. (2010) Design and implementation of a con-
tactless palm print and palm vein sensor, in Control, Automation, Robotics and
Vision, 2010, Proceedings of International Conference, pp. 1268‚Äì1273.

Analysis of Hand Vein Images Using Hybrid Techniques
337
24 Wang, J.G., Yau, W.Y., Suwandy, A. and Sung, E. (2008) Person recognition by fusing
palm print and palm vein images based on laplacian palm representation. Pattern
Recognition, 41, 1514‚Äì1527.
25 Ross, A. and Jain, A. (2003) Information fusion in biometrics. Pattern Recognition
Letters, 24 (24), 2115‚Äì2125.
26 Chin, Y.J., Ong, T.S., Teoh, A.B.J. and Goh, K.O.M. (2014) Integrated biometrics
template protection technique based on Ô¨Ånger print and palm print feature-level
fusion. Journal of Information Fusion, 18, 161‚Äì174.
27 Yang, J. and Zhang, X. (2012) Feature level fusion of Ô¨Ångerprint and Ô¨Ånger-vein for
personal identiÔ¨Åcation. Pattern Recognition Letters, 33 (5), 623‚Äì628.
28 Hanmandlu, M. Grover, J., Gureja, A. and Gupta, H.M. (2011) Score level fusion of
multimodal biometrics using triangular norms. Pattern Recognition Letters, 32 (14),
1843‚Äì1850.
29 Saleh, I.A. and Alzoubiady, L.M. (2014) Decision level fusion of iris and signature
biometrics for personal identiÔ¨Åcation using ant colony optimization. International
Journal of Engineering and Innovative Technology, 3 (11), 35‚Äì42.
30 Gonzalez, R.C. and Woods, R.E. (2009) Digital Image Processing, 3rd ed. Prentice
Hall, Upper Saddle River, NJ.
31 Padilla, P., Gorriz, J.M., Ramirez, J., Chaves, R., Segovia, F., Alvarez, I.,
Salas-Gonzalez, D., Lopez, M. and Puntonet, C.G. (2010) Alzheimer‚Äôs disease detec-
tion in functional images using 2D Gabor wavelet analysis. IET Electronics Letters,
46 (8), 556‚Äì558.
32 De Marsico, M., Nappi, M., Riccio, D. and Tortora, G. (2011) Novel approaches
for biometric systems. IEEE Transactions Systems, Man, and Cybernetics, 41 (4),
481‚Äì493.
33 Sim, H.M., Asmuni, H., Hassan, R. and Othman, R.M. (2014) Multimodal biomet-
rics: weighted score level fusion based on non-ideal iris and face images. Expert
Systems with Applications, 41 (11), 5390‚Äì5404.
34 Yuan, Y., Liu, Y., Dai, G., Zhang, J. and Chen, Z. (2014) Automatic foreground
extraction based on diÔ¨Äerence of Gaussian. The ScientiÔ¨Åc World Journal, 2014,
1‚Äì9.
35 Conti, V., Militello, C., Sorbello, F. and Vitabile, S. (2010) A frequency-based
approach for features fusion in Ô¨Ångerprint and iris multimodal biometric identiÔ¨Å-
cation systems. IEEE Transactions Systems, Man, and Cybernetics, 40 (4), 384‚Äì395.
36 Do, M.N. and Vetterli, M. (2005) The contourlet transform: an eÔ¨Écient directional
multiresolution image representation. IEEE Transactions on Image Processing, 14
(12), 2091‚Äì2106.
37 Naidu, V.P.S. (2011) Image fusion technique using multi-resolution singular value
decomposition. Defence Science Journal, 61 (5), 479‚Äì484.
38 Kumar, G.S. and Devi, C.J. (2014) A multimodal SVM approach for fused biometric
recognition. International Journal of Computer Science and Information Technologies,
5 (3), 3327‚Äì3330.
39 Wu, J.D. and Liu, C.T. (2011) Finger vein pattern identiÔ¨Åcation using SVM and
neural network technique. Expert Systems with Applications, 38, 14284‚Äì14289.
40 Lin, C.L. and Fan, K.C. (2004) Biometric veriÔ¨Åcation using thermal images of
palm-dorsa vein patterns. IEEE Transactions on Circuits and Systems for Video
Technology, 14 (2), 199‚Äì213.

338
Hybrid Intelligence for Image Analysis and Understanding
41 Raghavendra, R., Imran, M., Rao, A. and Kumar, G.H. (2010) Multi modal biomet-
rics: analysis of hand vein and palm print combination used for personal veriÔ¨Åcation,
in Emerging Trends in Engineering and Technology, 2010, Proceedings of IEEE Inter-
national Conference, pp. 526‚Äì530.
42 Ferrer, M.A., Morales, A., Travieso, C.M. and Alonso, J.B. (2009) Combining hand
biometric traits for personal identiÔ¨Åcation, in Security Technology, 2009, Proceedings
of IEEE International Carnahan Conference, IEEE, pp. 155‚Äì159.
43 Li, Z., Sun, D., Liu, D. and Liu, H. (2010) Two modality-based bi-Ô¨Ånger vein veri-
Ô¨Åcation system, in Signal Processing, 2010, Proceedings of IEEE 10th International
Conference, IEEE, pp. 1690‚Äì1693.
44 Rattani, A., Kisku, D.R., Gupta, P. and Sing, J.K. (2007) Feature level fusion of face
and Ô¨Ångerprint biometrics, in Biometric: Theory, Applications and Systems, 2007,
Proceedings of International Conference, pp. 1‚Äì6.
45 Yuksel, A., Akarun, L. and Sankur, B. (2010) Biometric identiÔ¨Åcation through hand
vein patterns, in Emerging Techniques and Challenges for Hand Based Biometrics,
2010, Proceedings of International Workshop, pp. 1‚Äì6.

339
15
IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using
Statistical Decision Making
Indra Kanta Maitra1 and Samir Kumar Bandyopadhyay2
1Department of Information Technology, B.P. Poddar Institute of Management and Technology, Kolkata, West Bengal,
India
2Department of Computer Science and Engineering, University of Calcutta, Salt Lake Campus, Kolkata, West Bengal, India
15.1
Introduction
Great fear and apprehension are related with the word cancer; although multidisci-
plinary scientiÔ¨Åc studies are undertaking best eÔ¨Äorts to combat this disease, the advent
of curative medicine for a perfect cure has not yet been discovered [1]. Around 5000
years ago, ayurvedic doctors treated patients with abnormal growths or tumors [23].
Two eminent ayurvedic classics, Charaka [29] and Sushrutasanhitas [3], deÔ¨Åned cancer
as inÔ¨Çammatory or non-inÔ¨Çammatory swelling, and mentioned them as either Granthi
(minor neoplasm) or Arbuda (major neoplasm). According to the modern medical point
of view, the deÔ¨Ånition of cancer is referred to a large number of conditions, character-
ized by abnormal, progressive, and uncontrolled cell division that destroys surrounding
healthy tissue. They have the ability to metastasize and spread throughout the body [23].
A lump or mass, called a tumor, is developed by most types of cancer, which are termed
after the organ of the body part where the tumor initiates. Breast cancer starts in breast
tissue, which is made up of milk-producing glands called lobules and ducts that link
lobules to the nipple. A tumor with malignancy has the capacity to spread beyond the
breast to other organs of the body via the lymphatic system and bloodstream [32].
15.1.1
Breast Cancer
Breast cancer is in the second and Ô¨Åfth positions related to occurrence and death,
respectively, among all types of cancer around the world. So, it is an important public
health problem in the world for women, specially aged women. Breast cancer is
growing throughout the world; about one million women are diagnosed with breast
cancer annually, and more than 400,000 die from it [8, 30]. The scenario of developing
countries is more serious, where the incident has increased as much as 5% per year
[9, 30]. As reported by Tata Memorial Hospital, the second most common cancer in
Indian females is breast cancer. The frequency is more in developed parts of urban areas
than in the countryside, and it is predominant in the advanced socioeconomic section
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

340
Hybrid Intelligence for Image Analysis and Understanding
due to rapid urbanization and westernization of lifestyle. Only the initial detection and
diagnosis are the means of control, but they comprise a major hurdle in India due to
absence of consciousness and lethargy among Indian females toward healthcare and
systematic checkup, and the unavailability of appropriate healthcare setups, especially
for breast cancer treatment.
15.1.2
Computer-Aided Detection/Diagnosis (CAD)
Early and eÔ¨Écient detection, followed by appropriate diagnosis, is the most eÔ¨Äective
way to have successful treatment and reduce mortality. A specialized breast screening is
performed to detect the presence of abnormalities such as tumors and cysts in women‚Äôs
breasts, whereas biopsy is the process to identify malignancies. Several screening tech-
niques are available for examination of the female breast, including commonly used
methods such as ultrasound imaging (i.e. a high-frequency band of sound waves that
investigates the breast); magnetic resonance imaging (MRI), which examines the breast
by the use of powerful magnetic Ô¨Åelds; and mammography, which yields X-ray images
of the breast; among others. Among all screening associated with clinical breast exam-
ination, digital mammography has been shown to be the most eÔ¨Écient and consistent
screening method for breast tumor detection at the nascent stage.
Computer technology has had a remarkable impact on medical imaging. CAD is a
comparatively young interdisciplinary subject containing components of artiÔ¨Åcial intel-
ligence and computer vision along commonly used methods with radiological image
processing. CAD techniques can reduce the time of medical image analysis and eval-
uation by radiologists substantially. The evaluation of medical images, however, is still
almost entirely dependent on human intervention, but in near future the scenario is
expected to change. Computers will be applied more often for image evaluation. CAD
is typically applied in areas to detect visible structures and segments more precisely
and economically. Today, CAD has been applied in routine clinical exercises and used
as a ‚Äúsecond opinion‚Äù in assisting radiologists in image interpretation, including breast
tumor detection.
15.1.3
Segmentation
In CAD, image segmentation is an important step, which segregates the medical image
into dissimilar non-overlapping regions such that each section is nearly homogeneous
and ideally resembles some anatomical part or region of interest (ROI). The precision
of CAD to identify the presence of abnormal masses on mammogram images involves
an accurate segmentation algorithm. This is regarded as one of the primary step toward
image analysis. The process of segmentation refers to the decomposition of a screen
into its constituent parts, so that the problem to be solved depends on each level of
subdivision. Segmentation is discontinued once the ROI of a particular application has
been segregated, and edge points of an image are obtained. The segmentation technique
can be generally categorized into Ô¨Åve main classes: threshold based, edge based, region
based, clustered based, and water-shaded based. The two main features of a gray-level
image are edge and region. The radiological images, speciÔ¨Åcally DICOM images, are
typically grayscale images. Segmentation algorithms for gray images primarily depend
on two main features (i.e. values of image intensity; discontinuity and similarity).

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
341
An initial approach is to decompose an image based on abrupt variations in intensity,
which is the primary feature of image edge detection. By deÔ¨Ånition, edges comprise a
process to identify signiÔ¨Åcant discontinuity (i.e., edges are substantial local variations of
intensity), so the discontinuities mean sudden alterations of pixel intensity of the image.
Thus, causes for variations of intensity are due to geometric and non-geometric events:
geometric events deal with discontinuity in entropy and/or color depth and texture (i.e.,
object margin) and discontinuity in plain and/or color, and non-geometric events are
direct reÔ¨Çections of light called specularity and inner reÔ¨Çection or shadow from other
objects or the same objects. It can also be accurately applied to deÔ¨Åne size, shape, and
spatial correlation between anatomical structures from the intensity distributions exhib-
ited by radiological medical images.
The latter approach is primarily based on dividing the image into regions that are sim-
ilar according to a set of speciÔ¨Åc conditions. The region-growing method is an example
of latter approach. The region-growing method checks the neighboring pixels within
one region having the same or similar value. Seeded region growing is a variation of the
commonly used methods region growing method depending on the similarity of pixels
within the region. In seeded region growing, seed(s) can be collected automatically or
by manual means. A set of seeds is applied by this method for partitioning an image into
dissimilar regions. Each seeded region is an associated constituent comprising one or
more points and is characterized by a set.
The proposed method described in this chapter is to identify the abnormalities (i.e.,
the presence of abnormal masses). The fundamental idea behind the research is to
segment out the abnormal region(s) from the entire breast regions. In the process of
segmentation, two diÔ¨Äerent approaches have been used. Intensity values are used to
generate edge from mammogram images to diÔ¨Äerentiate regions by boundaries within
breast ROI, whereas similarity and dissimilarity features are used by the commonly
used methods seeded region-growing algorithm to diÔ¨Äerentiate the anatomical regions
including abnormal region(s) by statistical decision making, which is described in
subsequent sections of this chapter.
15.2
Previous Works
Segmentation or abnormality detection is the primary step in mammographic CAD.
A brief review has been performed on alternative approaches proposed by diÔ¨Äerent
authors. The discussion is restricted in segmentation of mammographic masses,
describing their main features and highlighting the diÔ¨Äerences among normal and
abnormal masses. The key objective of the discussion is to mention methods in a nut
shell and, later, compare diÔ¨Äerent approaches with the proposed method. CAD being
an interdisciplinary area of research, people from diÔ¨Äerent disciplines like statistics,
mathematics, computer science, and medicine have contributed signiÔ¨Åcantly to enrich
the topic.
A substantial amount of research has been performed in order to enhance the
accuracy of CAD. As discussed earlier, the primary objective of mammogram CAD is
to segment out the abnormalities. Quite a few signiÔ¨Åcant works have been conducted
using diÔ¨Äerent methods to improve the segmentation accuracy and eÔ¨Äective identi-
Ô¨Åcation of image features. Among these automated and semi-automated methods,

342
Hybrid Intelligence for Image Analysis and Understanding
a dynamic weighted constant enhancement algorithm that combines adaptive Ô¨Åltering
and edge detection [25], an adaptive multilayer topographic region-growing algorithm
[35], a gray-level-based iterative and linear segmentation algorithm [6], a dynamic
programming approach [33], dynamic contour modeling [4], and so on are some of the
signiÔ¨Åcant approaches to segment mass lesions from surrounding breast tissues.
The spiculated masses have been detected by Kegelmeyer et al. [12] using orientation
of local edge and laws of texture features, but this is not applicable for the Ô¨Ånding of
nonspiculated masses. Karssemeijer et al. [11] applied a statistical analysis approach to
determine pixel orientations to isolate the stellate alterations on mammogram. Markov
random Ô¨Åelds are used by Comer et al. [7] and Li et al. [14] to diÔ¨Äerentiate regions based
on texture in a mammogram.
A fully automated method developed by Jiang et al. [10] comprises three steps. In the
Ô¨Årst step, the maximum entropy norms are applied in the ROI that is obtained after
correcting the background trend to enhance the initial outlines of a mass. After this, an
active-contour model is used to enhance the outlines that were formed earlier. Finally,
identiÔ¨Åcation of the spiculated lines attached to the mass boundary is performed with
the aid of a special line detector.
Kobatake et al. [13] proposed a system focused on the solution of two problems: Ô¨Årst,
how to identify tumors as apprehensive areas with a very low contrast to their back-
ground; and, second, how to isolate features that depict malignant tumors. For the initial
problem, they proposed a unique adaptive Ô¨Ålter called the iris Ô¨Ålter to identify abnormal-
ities, whereas malignant tumors are diÔ¨Äerentiated from other tumors using proposed
typical parameters.
Yang et al. [34] introduced a series of preprocessing steps that are designed to enhance
the intensity of a ROI, remove the noise eÔ¨Äects, and locate apprehensive masses using
Ô¨Åve texture features generated from the spatial gray-level diÔ¨Äerence matrix (SGLDM)
and fractal dimension. Finally, the mass extraction was performed using entropic thresh-
olding techniques coupled with a probabilistic neural network (PNN).
A two-step procedure was proposed by √ñzekes et al. [24] to specify the ROIs and clas-
sify ROI depending on a set of rules. Initially, pixel intensity of mammogram images is
applied and reads the pixels in eight directions using various thresholds. Finally, con-
nected component labeling (CCL) is used to label all ROIs, and two rules based on
Euclidean distance are applied to classify ROIs as true masses or not.
Campanini et al. [5] proposed a multiresolution over-complete wavelet representa-
tion that has classiÔ¨Åed the image with redundant information. The vectors of the massive
space acquired are then fed to a Ô¨Årst SVM classiÔ¨Åer. The detection task is categorized
into a two-class pattern recognition problem. At the initial stage, the mass is categorized
as suspect or not, by using this SVM classiÔ¨Åer, then the false candidates are discarded
with a second cascaded SVM. In order to reduce the number of false positives further, an
ensemble of experts is used to obtain the suspicious regions by using a voting strategy.
Rejani et al. [28] proposed a four-step scheme: mammogram enhancement using Ô¨Ålter-
ing, a top hat operation, DWT, and segmentation of the tumor area by thresholding,
extraction of features from the segmented tumor region(s), and SVM classiÔ¨Åer applied
to identify abnormalities. Martins et al. [22] proposed another SVM-based approach
for mass detection and classiÔ¨Åcation on digital mammogram. The K-means algorithm
is used to segment image, whereas texture of segmented structure is analyzed by the

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
343
commonly used methods co-occurrence matrix. The SVM is applied to classify masses,
and nonmasses, structure using shape and texture feature.
Here, diÔ¨Äerent automatic and semi-automatic methods of segmentation of mammo-
gram masses are presented and reviewed. Special emphasis is given to the approaches
and classiÔ¨Åcation methods. The approaches included in the chapter are statistical
analysis‚Äìbased, active-contour‚Äìbased, adaptive Ô¨Ålter‚Äìbased, neural network‚Äìbased,
threshold-based, and SVM classiÔ¨Åcation‚Äìbased techniques. The majority of the
research papers have not reported the accuracy estimation of their proposed methods.
In this chapter, only those research papers are selected that clearly stated the accuracy
estimation, so that later these can be compared with the proposed method.
15.3
Proposed Method
Mammography has been proven to be the most viable and eÔ¨Äective method to detect
early signs of breast abnormalities like benign and malignant masses. The masses are tis-
sues that are absent in normal breast anatomy, and they are called abnormal masses. The
density of these abnormal masses is diÔ¨Äerent, along with their variable size and shape.
Mammography is a type of radiography. The ionizing radiation produced by radiogra-
phy technology determines the internal structure of a person depending on the density
of body parts. Due to the heterogeneity of density, abnormal masses leave a unique
impression in the mammogram image. The proposed method is a set of sequential algo-
rithms to extract and analyze that impression to identify the abnormality (i.e., presence
of abnormal masses in a digital mammogram image). The input of the proposed method
is raw digital mammogram images, and the output is the decision whether there is any
abnormality present or not. These proposed decision-making systems consist of three
sequential distinct subsections, namely, preparation, preprocessing, and identiÔ¨Åcation
of abnormal region(s). The schematic diagram of the proposed method is depicted in
Figure 15.1.
15.3.1
Preparation
Medical images are challenging to interpret; thus, a preparation method is needed in
order to increase the image quality and make the segmentation outcome more precise.
Here, the eÔ¨Éciency of the decision making depends on the accuracy of segmentation.
The preparation process basically standardizes the image for further processing, and
standardization of input is the key to any image-processing algorithm. The preparation
phase consists of three diÔ¨Äerent steps, namely, image orientation, artifact removal, and
denoising.
The two most common mammographic views are mediolateral oblique (MLO) and
craniocaudal (CC). MLO is much accepted because its orientation along the horizontal
axis shows the entire gland of the breast, whereas CC shows only the central and inner
breast tissue. But in the MLO view, one nonbreast region (i.e., pectoral muscle) may be
present in the left or right upper corner of the image of the left and right breast, respec-
tively. Due to this heterogeneous orientation, a standardization algorithm is required to
transform the image, so as to place the pectoral muscle at the upper-left corner of the
mammogram. To perform left-oriented mammogram images, the right breast mam-
mogram needs to be Ô¨Çipped horizontally at 180‚àò, which is an exact mirror reÔ¨Çection of

344
Hybrid Intelligence for Image Analysis and Understanding
Mammogram Image (Input)
Preparation
Preprocessing
Image Enhancement Algorithm
Edge Detection Algorithm
Breast Contour Detection
Algorithm
Region of Interest (ROI)
Generation
Anatomical Segmentation of
Breast ROI
Colouring of Anatomical
Regions
Statistical Decision Making
Decision (Output)
Pectoral Muscle Detection
and Isolation Algorithm
Image Orientation
Artifacts Removal
Gaussian Smoothening
Figure 15.1 Schematic diagram of the proposed method.
the image. This process will allow the breast regions to be compared and analyzed in a
similar way, and thus reduce the algorithmic complexity to a great extent. The process
of image orientation starts with the identiÔ¨Åcation of mammogram, whether it repre-
sents the left or right breast regions. In the case of left, the algorithm will ignore the
orientation process, and in the case of right, it will Ô¨Çip the image horizontally [16].
Another additional complexity of mammogram image analysis is the presence of
patient-related and hardware-related artifacts. These artifacts provide high-intensity
regions on the mammogram and are inconsequential to the investigation of abnormal-
ities within the mammogram. The presence of such artifacts also changes the intensity

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
345
(a)
(b)
Figure 15.2 (a) The original mammogram image and (b) the prepared mammogram image (MIAS
mdb184.L). Source: Suckling (1994) [31]. Reproduced with permission of Mammographic Image
Analysis Society. [31]
levels of the mammogram image signiÔ¨Åcantly, which may aÔ¨Äect statistical analysis on
the image. An algorithm has been proposed to remove all such artifacts and markings
on the nonbreast region of the mammogram, and replace them with the background
color. The resulting image is free from any other object except the breast region [16].
The characteristics of high-intensity noise are high values of optical densities, such
as shadows that present them as horizontal strips or that are ghost images of previ-
ously performed mammography. These noises are embedded in the breast region of the
mammogram, thus resulting in loss of information from the breast region. These noises
also hinder the detection process to yield false results or negative detection. Such noise
must be removed from the image to provide accurate results in the detection process. In
this research, the well-known Gaussian Ô¨Ålter is used to remove such noise by blurring
these salt-and-pepper noises before performing edge detection or other processing on
the mammogram images. A Gaussian smoothening operator is applied to blur mam-
mogram images and remove unwanted noise. A Gaussian 2D convolution operator is
used as a kernel that is a hump-like structure. Here in the proposed research, a Ô¨Çexible
Gaussian smoothening algorithm is used where the size of kernel and value of deviation
(Œ©) can be adjustable depending on the image quality [17].
The above-mentioned preparation steps enhance the image quality and standardize
the mammogram image for those derived from diÔ¨Äerent formats, makes, and versions.
In addition to the preparation phase, the mammogram images are trimmed from both
left and right sides so that the additional background parts are removed from the image
without aÔ¨Äecting the breast region (Figure 15.2). It helps to reduce the execution time
for further analysis and decision-making algorithms.
15.3.2
Preprocessing
The preprocessing phase consists of mammogram image enhancement and edge
detection, isolation and suppression of pectoral muscle, contour determination, and
anatomical segmentation. All these processes are mandatory, distinct, and sequential in
nature. In order to improve the image quality and to make the resultant segmentation

346
Hybrid Intelligence for Image Analysis and Understanding
more accurate, the preprocessing steps are mandatory, so that the image is ready
for further processing. The two distinctive regions of a mammogram image are the
exposed breast region and the unexposed air-background (nonbreast) region, but the
principal feature on a mammogram remains as the breast contour, otherwise known
as the skin‚Äìair interface or breast boundary and the pectoral muscle. On partition-
ing the mammogram into breast and nonbreast regions and pectoral muscle, we
obtain the breast region. On suppression of both nonbreast and pectoral muscle
regions, the breast ROI is derived and further processing can be done. By obtaining
the breast ROI, anatomical regions need to be diÔ¨Äerentiated and partitioned, so that
abnormal region(s) among normal regions can be identiÔ¨Åed. The input of preprocessing
steps is prepared mammogram images, and the output is anatomical regions within the
breast ROI along with abnormal region(s), if present. So, it can be said that non-invasive
mapping of breast anatomy can be derived by using preprocessing.
15.3.2.1
Image Enhancement and Edge Detection
The mammogram image enhancement and edge detection are two distinct steps, but
here the edge detection algorithm is completely dependent on image enhancement due
to use of the homogeneity feature. In the proposed image enhancement method, homo-
geneity of the image will be substantially increased. In addition to that, the algorithm
will quantize color in gray scale up to the applicable level. The homogeneity enhance-
ment is followed by the process of edge detection from a medical image, which is a very
crucial step toward comprehending image features. Since edges often develop in those
locations demarcated by object boundaries, edge detection is widely used in medical
image segmentation where these images are segregated into areas that correspond to
separate objects. Here the objects are nothing but diÔ¨Äerent anatomical regions within
the breast ROI, and one or more regions are abnormal among these normal regions that
need to be identiÔ¨Åed. The outline of the proposed image enhancement and edge detec-
tion method is described in a nutshell. For further knowledge, the research paper on
medical edge detection by the authors may be consulted [20].
The proposed method consists of three parts: the Ô¨Årst part is to calculate the adaptive
threshold, which is the maximum distance threshold (MDT); the subsequent part is to
generate an enhanced image; and, Ô¨Ånally, the third part is to determine the edge map of
the original mammogram.
An initial objective of the proposed process is to determine an adoptive threshold
MDT value. The threshold is constant for a particular image, but it varies from image to
image depending on their intensity characteristic features like contrast, brightness, and
so on. In this research, a modiÔ¨Åed full and complete binary tree is used to store both
the intensity and frequency of the image. The primary objective to use such a tree is to
quantize the number of colors, yet preserve the full color palette at every tree level.
In this proposed method, color intensities and their frequencies are extracted from the
original image color space to generate a histogram. The extracted data are stored in the
leaf nodes of the proposed tree (i.e. each leaf node represents individual disjoint intensity
sequentially) (Figure 15.3). So, all the possible color of an image can be represented by
the leaf nodes (i.e., if the image contains 2n number of distinct colors, then the tree will
have 2n leafs at level n). The node structure of the said binary tree will contain pointers
for left and right child nodes, along with image data. The data will have two components
(i.e., color intensity and its frequency present in the image):

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
347
Figure 15.3 Full and complete binary tree.
Node={Node*Left, Node*Right, Int Intensity, Int Frequency}
The frequency of intensity of left child node f (L) and right child node f (R), whichever
is greater, will be the intensity (I) of the parent node. The frequency (Fq) for the node
will be the summation of f (L) and f (R).
Node.I =
{
f (R).I
if f (L).Fq < f (R).Fq
f (L).I
if f (L).Fq > f (R).Fq
(15.1)
Initially, the entire tree is constructed, but only the leaf has image data. In the next phase,
the intensity values of image data for all the intermediate nodes, including the root node,
are required to be calculated. As per the proposed algorithm, the parent node will hold
the intensity value of the child node that has a greater frequency among the two child
nodes. To achieve the same, the proposed tree has to be traversed in postorder and the
intensity value of the parent node is updated based on the comparison results of the child
nodes. The tree structure that is obtained by this procedure contains the histogram of
the original image with C gray shades at level n, where C = 2n and every subsequent
upper level contains n/2 number of gray shades (i.e., called the level-histogram).
In each level, half of the intermediate color bins are truncated depending on the con-
dition stated above. This intermediate truncation will generate diÔ¨Äerent bin distances
in the particular level of the tree (i.e., called the bin-distance). The average bin distance
(ABD) is the mean of these bin distances. The MDT calculation process Ô¨Årst segregates
the bins into two categories, namely, prominent bins and truncated bins. Prominent bins
are the points from where sharp changes of intensity values are recorded, whereas trun-
cated bins have an insigniÔ¨Åcant diÔ¨Äerence of intensity with their adjacent bins. Promi-
nent bins have a signiÔ¨Åcant role to determine edges of an image. The average bin distance
of the prominent bins generates the MDT.
In the second phase, the enhanced image will be generated from the original medical
image. The new intensity of a particular pixel has been mapped using the truncated his-
togram from the tree of a particular level. In the mapping process, a particular intensity
of an original image has been selected from the level histogram, and checking has been
performed regarding whether the obtained intensity is prominent or not. If the obtained
intensity is prominent, then it will be propagated to the image pixel, else the next higher
prominent intensity will be selected from the level histogram to propagate.
Finally, the process will scan the enhanced image in row major order. It will be started
from the leftmost pixel from the Ô¨Årst row and terminated at the rightmost pixel of the last
row. Here, two consecutive pixels are compared. If the absolute value of the diÔ¨Äerence
is greater than the MDT, then the corresponding pixel position of the horizontal edge

348
Hybrid Intelligence for Image Analysis and Understanding
map image will be set to 0 (i.e., black), else N (i.e., white).
f (h) =
r‚àë
i=0
c‚àë
j=0
Pi, j =
{
0
if ‚à£Pi, j ‚àíPi+k, j ‚à£> MDT
N
if ‚à£Pi, j ‚àíPi+k, j ‚à£< MDT
(15.2)
For vertical edge detection, the algorithm is similar to the above horizontal edge detec-
tion algorithm except it sets pixels vertically (i.e., in column major order rather than
horizontally).
In the previous two steps, horizontal edge map and vertical edge map images have
been generated. Now the horizontal and vertical edge maps are superimposed on each
other using the logical OR operation.
EdgeMap = f (h) ‚à®f (ùë£)
(15.3)
The superimposed output image will be the edge of the medical image (i.e., EdgeMapIm-
age). It is the Ô¨Ånal outcome of the method, which can be applicable to any mammogram
image (Figures 15.4 and 15.5).
15.3.2.2
Isolation and Suppression of Pectoral Muscle
The pectoral muscle, a nonbreast region in mammograms, acts like an additional
complexity for automated abnormality detection. The pectoral muscle region exhibits
a high-intensity region in the majority of MLO projections [27] of mammograms and
can inÔ¨Çuence the outcome of image-processing techniques. Intensity-based methods
can produce erroneous results when used to segment high-intensity regions such as
suspected masses or Ô¨Åbro-glandular discs because the pectoral muscle has the same
opacity as tumors. The authors have published more than one paper on the related
topic [18, 19].
Figure 15.4 Enhanced mammogram image (MIAS
mdb184.L). Source: Suckling (1994) [31]. Reproduced with
permission of Mammographic Image Analysis Society.

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
349
(a)
(b)
(c)
Figure 15.5 (a) Edge map of Level 1, (b) edge map of Level 2, and (c) edge map of Level 3
(MIAS mdb184.L). Source: Suckling (1994) [31]. Reproduced with permission of Mammographic Image
Analysis Society.
There are two diÔ¨Äerent alternative approaches to isolate pectoral muscle from a mam-
mogram; one is region based and another is edge based. Here the edge-based method is
described brieÔ¨Çy. The output of the edge detection algorithm is the input of the proposed
pectoral muscle isolation method.
As discussed in this chapter, the breast region of an image is diÔ¨Äerentiated from the
background using the edge detection algorithm. The breast region, which is demarcated
by breast boundary in between the breast region and the image background, contains
outline edges of various breast constituents. For further processing, it is of extreme
importance to extract the breast region by eliminating the pectoral muscle region. A
new edge-based method is proposed to detect, extract, and isolate the pectoral muscle
from the breast region.
As previously discussed, all the mammogram images are homogeneously oriented by
the preparation process, so that the pectoral muscle will be placed at the top-left corner
for both left and right breast mammogram images and its boundary line traverses from
the top margin downward toward the horizontal reference line (i.e., chest wall of the
mammogram image), forming an inverted triangle. Since it is a muscular structure, sev-
eral layers are present. These layers form several impressions in an edge map. Therefore,
more than one inverted triangles are in the edge map. The objective is to identify the
largest triangle, which is the actual border of pectoral muscle, and divide the pectoral
muscle from the breast region, as shown in Figure 15.6.
It is now needed to identify the outermost edge line that constitutes the edge of the
pectoral muscle. It is observed that the largest inverted triangle in the edge map start-
ing from the top margin and ending on the vertical baseline (i.e., the horizontal refer-
ence line of the breast region on the left side), is the pectoral region. It is important
to identify the rightmost pixel of the breast region on the right side and draw a verti-
cal line from the top margin to the bottom margin, parallel to the left vertical baseline.
Another line is drawn parallel to the top and bottom margins. The said line originates
from the two-thirds position of the horizontal reference line to the right vertical line
(Figure 15.7).

350
Hybrid Intelligence for Image Analysis and Understanding
(a)
(b)
(c)
Figure 15.6 (a) The edge map of a mammogram, (b) showing the layers of pectoral muscle, and
(c) showing inverted triangles marked by diÔ¨Äerent gray shades (MIAS mdb184.L). Source: Suckling
(1994) [31]. Reproduced with permission of Mammographic Image Analysis Society.
2/3
1/3
Figure 15.7 Pectoral muscle lies within gray-shaded
derived rectangular area (MIAS mdb184.L). Source:
Suckling (1994) [31]. Reproduced with permission of
Mammographic Image Analysis Society.
The pectoral muscle, barring a few mammograms, lies within the rectangle formed by
these straight lines (Figure 15.7). The search is limited to the obtained rectangle, thus
reducing the processing time instead of considering the entire mammogram image.
The scanning will be right to left, starting from the right vertical line at the Ô¨Årst row.
If a pixel with black intensity is obtained, that is demarcating the path of the edge. By
considering all the surrounding pixels in a clockwise priority, the pixel path is traversed
and the highest priority neighboring pixel is considered for further progress. The pix-
els that surrounded the edge pixel but are of lower priority are stored in a backtrack
stack in order to prioritize value, and they are to be used only if the traversal process
reaches a dead end. If a dead end is reached, pop from the backtrack stack for a lesser
priority pixel and continue with the traversal process. The pixels traversed are stored in
a plotting list to be used later for drawing the pectoral boundary. Traversal continues to
the next pixel till it reaches the left baseline or the bottom of the rectangle. If the bot-
tom of the rectangle is reached, the path is discarded, the plotting list is erased, and the
algorithm resumes by searching the next black pixel at the Ô¨Årst row from right to left.
If the horizontal reference line is reached that speciÔ¨Åes that the path stored in the plot-
ting list is the pectoral boundary, that is drawn in a new image and further processing is
terminated.

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
351
Figure 15.8 Isolated pectoral boundary (MIAS mdb184.L).
Source: Suckling (1994) [31]. Reproduced with permission of
Mammographic Image Analysis Society.
The pectoral boundary obtained (Figure 15.8) is further processed by the pectoral
boundary smoothing method. The pectoral boundary image obtained so far is not
smooth. A smooth pectoral boundary is obtained by taking pixels at a Ô¨Åxed discrete
interval and joining them by drawing a simple curve between the pixel positions to
obtain a smooth and enhanced pectoral boundary.
15.3.2.3
Breast Contour Detection
The breast contour is one of the primary characteristics within the breast region. The
extracted breast region limited by breast contour is important because it restricts the
search for abnormality within a speciÔ¨Åc region without unwanted inÔ¨Çuence from the
background of the mammogram. It also facilitates enhancements for techniques such
as comparative analysis, which includes the automated comparison of corresponding
mammograms. The breast boundary contains signiÔ¨Åcant information relating to the
symmetry and deformation between two mammograms.
The authors published a research article on a breast segmentation and contour detec-
tion method for mammographic images [15], which is an edge-based technique. The
brief idea of the contour detection algorithm is cited here.
The input mammogram image of the proposed contour detection algorithm is the
edge map after identiÔ¨Åcation and isolation of the pectoral muscle area. Breast contour or
boundary is nothing but the outermost edge line of the derived edge of the mammogram.
All the mammogram images are homogeneously oriented, and the skin‚Äìair interface is
placed at the right side of the image. So, the image is scanned from the right side of the
image to locate the rightmost point of the edge at the topmost row of the image, which is
the starting point of the processing. The algorithm considers all the neighboring pixels
except the pixel that is already traversed (Figure 15.9).
The priority of the neighboring pixel is clockwise. It will select the pixel with the high-
est priority within the edge. It stores the pixel traversed in a plotting list to be used
later for drawing the breast boundary. The other pixels that surround that pixel, but are
of lower priority, are stored according to their priority order in a backtrack stack to be
used only if the traversal process reaches a dead end. If a dead end is encountered, where

352
Hybrid Intelligence for Image Analysis and Understanding
6
7
8
1
2
3
4
5
(a)
(b)
Figure 15.9 The technique of traversing process. (a) Black cell represents the current pixel, gray cell is
representative of already traversed pixel, and the rest are the path for further traversing. (b) The
priority of selection of neighbor is clockwise. Source: Suckling (1994) [31]. Reproduced with permission
of Mammographic Image Analysis Society.
there are no pixels that have not already been traversed, it pops out from the backtrack
stack a lesser priority pixel, creates a new branch, and continues, with the traversal pro-
cess. The traversal continues to the next pixel till it reaches the baseline (i.e., the bottom
of the image or horizontal reference line, i.e., the chest wall, indicating the end of the
breast region). The plotting list contains the breast boundary pixels that are plotted on
a blank image to obtain the breast boundary for further processing.
The breast boundary image, obtained by implementing the proposed breast boundary
detection method, is further processed by the breast boundary smoothing method to
obtain the Ô¨Ånal image. The breast boundary image obtained so far is not smooth. But
it is important to get a contour curve that is smooth. The pixel information along with
contour is preserved by the previous algorithm in an array. The smoothing algorithm
picks up pixels from there with a discrete interval and joins them by drawing a simple
curve between two successive pixel positions to obtain a smooth and enhanced breast
boundary. The enhanced boundary is single pixel and continuous (Figure 15.10).
Figure 15.10 Detected breast contour (MIAS mdb184.L).
Source: Suckling (1994) [31]. Reproduced with permission of
Mammographic Image Analysis Society.

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
353
15.3.2.4
Anatomical Segmentation
The anatomical segmentation algorithm [21] of the breast ROI is applied on the MLO
view of mammogram images devoid of pectoral muscle area and background delineated
by breast contour. The principal idea of the proposed algorithm is to diÔ¨Äerentiate the
anatomical breast regions and separate each of the regions with boundary lines.
A clear understanding of breast anatomy and its impression on digital mammograms
is the prerequisite to discuss the technical aspects of segmentation of anatomical
regions. A mature woman‚Äôs breast can be divided into four regions: skin, nipple, and
subareolar tissues; the subcutaneous region, which contains fat and lymphatics; the
parenchyma region, a triangular shape between the subcutaneous and retromammary
regions, with the apex toward the nipple; and the retromammary region, which
consists of retromammary fat, intercostal muscles, and the pleural reÔ¨Çection [2]. On
mammogram images, breast masses, including both noncancerous and cancerous
lesions, appear as white regions. Fat appears as black regions on the images. All other
components of the breast, like glands, connective tissue, tumors, calcium deposits, and
so on, appear as diÔ¨Äerent shades of white on a mammogram.
Any anatomical regions of living beings are of closed structure and bounded by a
periphery, and this is also applicable for human breast. Breast comprises bounded
anatomical regions. Hence, the edge map indicates various closed structures within
the breast region that correspond to the diÔ¨Äerent anatomical regions of the breast,
namely, the fatty tissues, ducts, lobules, glands, irregular masses, and calciÔ¨Åcations.
The objective of the algorithm is to clearly identify these regions on the mammogram
image and erase all other unwanted edges, lines, and dots from the edge map for further
processing and analysis (Figure 15.11).
(a)
(b)
Figure 15.11 (a) Breast ROI and (b) Boundaries of anatomical regions within breast ROI (MIAS
mdb184.L). Source: Suckling (1994) [31]. Reproduced with permission of Mammographic Image
Analysis Society.

354
Hybrid Intelligence for Image Analysis and Understanding
The principal objective of the proposed algorithm is to traverse the edge map to iden-
tify and plot all the circular paths forming a closed structure. To achieve the objective, a
ladder-like structure is proposed that is delineated by the chest wall of the breast image
on the left and right sides by a vertical line passing through the rightmost pixel of the
breast ROI. Horizontally, the image is divided by parallel lines subsequently after speciÔ¨Åc
intervals. The detailing of the anatomy depends on the density of the horizontal lines. If
the number of horizontal lines is high, all the minute details will be recorded, but it will
increase the execution time of the algorithm and complexity of the anatomy, whereas
reduction of horizontal lines will miss some tiny structure that may have importance in
medical diagnosis. So, the optimal number of horizontal lines have to be applied on a
case-to-case basis under the direction of medical expert.
The traversal process of circular edge paths starts from the horizontal lines subse-
quently and is recorded in the plotting list. This list is plotted on another blank image if
the edge path is circular, ends on the last row of the image, or ends on the vertical line
representing the left boundary of the breast. The pixel search in horizontal lines is in a
left-to-right direction, and it repeats for all the subsequent horizontal lines from top to
bottom. So, the algorithm consists of two iterations: the outer one depends on the num-
ber of horizontal lines, whereas the inner one searches the starting pixel on the same
horizontal line to begin the traveling process. The deliverable of the proposed algorithm
is a new image plotted with closed traveling paths that are the boundaries of diÔ¨Äerent
internal regions within the breast ROI.
15.3.3
IdentiÔ¨Åcation of Abnormal Region(s)
The principal objective of the proposed method is to identify the region(s) with abnor-
mality among normal regions in the breast. Now the questions are: is what is that fea-
ture indicating the abnormality, and how can that be identiÔ¨Åed by a decision-making
system? From a medical point of view, masses are tissues that are absent in normal
breast anatomy and are called abnormal masses. But from the point of view of medical
image processing, density of abnormal masses is diÔ¨Äerent and at the same time their size
and shape are variable. Due to heterogeneity, such masses leave a unique high-intensity
impression in the mammogram image. The motivation behind the research is to reduce
the workload of the radiologist and reduce the percentage of false detection. It is not a
replacement for radiologists but can be an assistance tool for second opinions.
The decision-making system is composed of two parts, to initially diÔ¨Äerentiate the
regions depending on the intensity distribution using a modiÔ¨Åed seeded region-growing
algorithm (i.e. coloring of regions), and Ô¨Ånally a statistical model is used to take a stand
regarding the presence or absence of abnormalities. The inputs of the proposed algo-
rithm are mammogram images after preparation and output of the anatomical segmen-
tation algorithm containing boundary outlines of anatomical regions of the breast ROI
(i.e., the concluding result of preprocessing).
15.3.3.1
Coloring of Regions
The segmentation process performed on the edge map diÔ¨Äerentiates various regions
on the breast, depending on their intensity values. Each region has a diÔ¨Äerent intensity
value. The fatty tissues, glands, lobules, and ducts exhibit diÔ¨Äerent intensity values, and
thus can be segregated into diÔ¨Äerent regions. An abnormality, such as a mass, tumors,

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
355
or calciÔ¨Åcations, that may be present within the breast has noticeably higher intensity
values than the normal tissues of the breast. So, it is needed to categorize all the obtained
closed structures on the basis of their intensity values. The distribution of pixel intensi-
ties also varies within each segmented region, but the majority of the pixels have similar
intensity values. So, the respective arithmetic node value is calculated for each region
from the original mammogram and replaces those pixels in the region with the com-
puted mode values. To propagate the mode value, a seeded region-growing technique
is used. Each region within the mammogram is bounded by a single-pixel boundary
as obtained during the edge detection process followed by the process of anatomical
segmentation.
During this process, the segmented image is scanned to locate a region that is yet to
be colored. The scanning process starts from the Ô¨Årst row of the image, proceeds in row
major order, and terminates at the right most pixel of the last row. On Ô¨Ånding the seed
for a region, the coloring process is started for the region by Ô¨Årst comparing the pixel
intensity of that pixel location on the original mammogram image. For each pixel, the
four boundary pixels located north, east, west, and south of the pixel are also checked
to Ô¨Ånd out whether those are colored or the boundary pixel. If the pixels are not colored
and not boundary pixels, they again form the seed for further searching. A stack is used
to store the seeds to be investigated, while a list is used to store the pixels of the region
that have been included in the region and already traversed. All the pixel positions within
the list are then searched on the original image to get their intensity values to derive the
mode value. The pixel locations of each region are then substituted by the computed
mode value intensity (Figure 15.12).
15.3.3.2
Statistical Decision Making
The regions are heterogeneous in color intensity, but there is some degree of homo-
geneity present. The abnormal regions are present within these regions with some
Figure 15.12 Intensity distribution of regions after coloring
(MIAS mdb184.L). Source: Suckling (1994) [31]. Reproduced
with permission of Mammographic Image Analysis Society.

356
Hybrid Intelligence for Image Analysis and Understanding
asynchronous characteristics. The objective is to extract these characteristics‚Äô features
to prove the presence of abnormality. To identify the abnormalities, a statistical
decision-making system is applied to analyze the distribution of the colors domain
through a step-by-step elimination model.
The proposed SRG algorithm categorized and enumerated each region along with
their respective statistical mode value. This data set is used for further statistical analysis.
First, the arithmetic mean (ùúá) for the distribution is calculated to obtain the deviations
of each region.
ùúá= 1
M
h
‚àë
i=0
ùë§
‚àë
j=0
Imagei,j
where Ii,j > 0
(15.4)
where M is the number of pixels; and Imagei,j > 0 ‚àßImagei,j <Total Number of color.
Subsequently, the standard deviation of the data set is calculated.
ùúé=
‚àö
‚àö
‚àö
‚àö
‚àö
1
RegCount
RegCount
‚àë
i=0
(RegModi ‚àíùúá)2
(15.5)
where RegCount is the number of regions; and RegMod is the region‚Äôs mode value.
Then the Z score is calculated to normalize the distribution.
Zscore = (Imagei,j ‚àíùúá)‚àïùúé
(15.6)
The regions with negative Z values are truncated because they are insigniÔ¨Åcant and nor-
mal regions. So, only regions with a positive Z score values will be considered for further
processing. Now the truncated mean value (Tùúá) is calculated along with the standard
deviation (Tùúé) using the truncated data set.
Tùúá= 1
n
RegCount
‚àë
i=0
Zscorei
where Zscorei > 0
(15.7)
where n is the number of regions with positive Z-score values:
Tùúé=
‚àö
‚àö
‚àö
‚àö
‚àö1
n
n
‚àë
i=0
(Zscorei ‚àíTùúá)2
(15.8)
Finally, the 2Tùúéand 3Tùúéof the population are calculated on a truncated data set.
Now, the regions are categorized into four discrete levels according to their color
intensity. There are some regions with color values greater than truncated mean (Tùúá)
but less than the truncated mean (Tùúá) + Tùúélevel. Some have color values greater than
truncated mean (Tùúá) + Tùúébut less than the truncated mean (Tùúá) + 2Tùúélevel. There are
few regions whose color value is beyond the truncated mean (Tùúá) + 2Tùúélevel but within
the truncated mean (Tùúá) + 3Tùúélevel. There are rare regions, if present, with color val-
ues over the truncated mean (Tùúá) + 3Tùúélevel. According to the algorithm, the fourth
category is the most suspicious and uncommon in the frequency of the color region
domain. This category is marked by the algorithm as the abnormal region. The third
category is also apprehensive and uncommon, and the algorithm marked this category
as a suspected region. This category regions are mostly absent in normal mammograms.
Finally, an image is generated depicting the anatomical regions. The colorings of
regions are done through a color lookup table. The color lookup table selects the color

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
357
for a particular region, depending upon the decision generated by the decision-making
system as stated above. The algorithm highlighted the abnormal region(s) with a
very deep gray shade; the suspected region(s) are marked with gray shade for further
analysis by the medical experts. The highly dense regions are colored with a light gray
shade, and normal areas are demarcated by white (Figure 15.13). The color lookup table
(Table 15.1) describes the color scale of breast regions according to the decision-making
system.
The proposed seeded region-growing algorithm scans the image in row major order
according to the boundary deÔ¨Åned by the ROI. Assume that height is n and width is m.
So, the running time of the proposed SRG will be O(n*m); if n=m, then the running time
complexity will be O(n2). The proposed image mean value calculation will also run in
O(n2). The Z score calculation followed by the truncated mean calculation will work in
(a)
(b)
Figure 15.13 (a) Highlighted regions with abnormal masses and (b) boundary of abnormal regions
(MIAS mdb184.L). Source: Suckling (1994) [31]. Reproduced with permission of Mammographic Image
Analysis Society.
Table 15.1 Color lookup table
Categories
Intensity limit
Type
Color
Category 01
< ùúá
Normal
White
Category 02
< Tùúá
Normal
White
Category 03
< Tùúá+ Tùúé
Normal
White
Category 04
< Tùúá+ 2Tùúé
Dense
Light gray
Category 05
< Tùúá+ 3Tùúé
Suspected
Gray
Category 06
‚â•Tùúá+ 3Tùúé
Abnormal
Very deep gray

358
Hybrid Intelligence for Image Analysis and Understanding
linearly constant time depending on the number of regions present in the image. So, the
cumulative complexity of the method is O(n2).
15.4
Experimental Result
Three diÔ¨Äerent types of mammogram imaging techniques are most commonly used
in clinical diagnosis. The Ô¨Årst one is the traditional Ô¨Ålm mammography printed, and
the other two are computed radiography (CR) and digital radiography (DR) mammo-
gram systems that store images using electronic signals and return back responses to
the requesting machine. Hence, CAD can be deployed on the last two methods, and the
DICOM (digital imaging and communication machine) image format is used by both
CR and DR mammography. In CR 28 and in DR 212 or 216 grayscale color intensity val-
ues are used to represent the 8-bit or the 12/16-bit pixel information in the DICOM
image format.
Any gray scale image is basically a 2D array of pixel intensities where intensities range
from k=0 to n. In CR mammogram DICOM images, the gray shade ranges from k=0 to
255, whereas in DR it is k=0 to 212 or 216. So the intensity of the image is the primary
feature to determine the abnormality. The signiÔ¨Åcant variation is observed in CR and
DR mammogram images, at the same time that distinguishable variations of intensity
can also be marked in diÔ¨Äerent makes of mammographic machines of the same type as
well as diÔ¨Äerent versions of the same make.
The proposed abnormal masses detection algorithm has been extensively tested with
two well-known mammogram databases, namely, MIAS (Mammographic Image Analy-
sis Society) digital mammogram database with 322 images in the 8-bit category, whereas
in DR (i.e., the 16-bit category), the DEMS (Dokuz Eylul University Mammography
Set) database has been considered with 485 images. For experimental purposes, CR
images obtained from diÔ¨Äerent medical institutes have been considered, but due to eth-
ical issues and anonymity of the patients, the results are not published here. The MIAS
database is 8-bit .png images, whereas DEMS is 16-bit DICOM images.
The proposed algorithm is tested with MIAS, DEMS, and other benchmarked
databases of mammograms that are in the public domain. The experimental outcomes
are extremely encouraging. Some of the outputs of MIAS belonging to diÔ¨Äerent
categories are cited here for demonstration of correctness of the proposed algorithm.
15.4.1
Case Study with Normal Mammogram
The proposed algorithm is applied on MIAS Image 272.L with predominantly fatty
tissues, and according to the MIAS observation no abnormality is present. The
proposed algorithm agrees with the MIAS observation and detects no abnormalities
(Figure 15.14).
15.4.2
Case Study with Abnormalities Embedded in Fatty Tissues
The proposed algorithm is applied on MIAS Image 028.L with predominantly fatty tis-
sues, and according to the MIAS observation, an abnormality is present. The proposed
algorithm agrees with the MIAS observation and detects abnormalities (i.e., the pres-
ence of a mass) (Figure 15.15).

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
359
(a)
(b)
(c)
Figure 15.14 MIAS mdb272.L: (a) mammogram image, (b) segmented anatomical regions without
highlighted abnormality, and (c) derived image showing absence of boundary of abnormal region(s).
Source: Suckling (1994) [31]. Reproduced with permission of Mammographic Image Analysis Society.
(a)
(b)
(c)
Figure 15.15 MIAS mdb028.L: (a) mammogram image, (b) segmented anatomical regions with
highlighted abnormality, and (c) derived image showing boundary of abnormal region(s). Source:
Suckling (1994) [31]. Reproduced with permission of Mammographic Image Analysis Society.
15.4.3
Case Study with Abnormalities Embedded in Fatty-Fibro-Glandular Tissues
The proposed algorithm is applied on MIAS Image 001.R with predominantly
fatty-Ô¨Åbro-glandular tissues, and according to the MIAS observation, an abnormality
is present. The proposed algorithm agrees with the MIAS observation and detects
abnormalities (i.e., the presence of masses) (Figure 15.16).
15.4.4
Case Study with Abnormalities Embedded in Dense-Fibro-Glandular Tissues
The proposed algorithm is applied on MIAS Image 145.R with predominantly
dense-Ô¨Åbro-glandular tissues, and according to the MIAS observation, an abnormality

360
Hybrid Intelligence for Image Analysis and Understanding
(a)
(b)
(c)
Figure 15.16 MIAS mdb001.R: (a) mammogram image, (b) segmented anatomical regions with
highlighted abnormality, and (c) derived image showing boundary of abnormal region(s). Source:
Suckling (1994) [31]. Reproduced with permission of Mammographic Image Analysis Society.
(a)
(b)
(c)
Figure 15.17 MIAS mdb145.R: (a) mammogram image, (b) segmented anatomical regions with
highlighted abnormality, and (c) derived image showing boundary of abnormal region(s). Source:
Suckling (1994) [31]. Reproduced with permission of Mammographic Image Analysis Society.
is present. The proposed algorithm agrees with the MIAS observation and detects
abnormalities (i.e., the presence of masses) (Figure 15.17).
15.5
Result Evaluation
The benchmarked MIAS and DEMS databases contain diÔ¨Äerent types of case studies
that are divided into two distinct classes: one is normal, and another is abnormal. The
abnormality can also be subdivided into two categories: the presence of calciÔ¨Åcation

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
361
and the presence of a mass or masses. The masses can be well-deÔ¨Åned or circumscribed
masses, spiculated masses, and other ill-deÔ¨Åned masses. The calciÔ¨Åcation clusters are
very tiny regions like dots, whereas masses are well deÔ¨Åned and comparatively larger
structures. The determination of these abnormalities depends on the segmentation of
breast anatomy. The proposed ladder concept is the key deciding factor for determi-
nation of calciÔ¨Åcation and masses. If the number of segments is higher in the ladder
structure, the anatomy is more detailed and is able to identify the calciÔ¨Åcations. When
the number of segments is less, the anatomy of breast regions is clearer; at the same time,
unwanted information is completely eliminated from the ROI, but there is inability to
isolate the microcalciÔ¨Åcation clusters in breast anatomy. In both ways, masses can be
segregated eÔ¨Éciently. The decision-making system, deÔ¨Åned earlier in this chapter, can
mark both the high-intensity abnormalities (i.e., microcalciÔ¨Åcation clusters and masses)
proÔ¨Åciently depending upon segregation of the region by the algorithm.
The quantitative analysis of the proposed algorithm is performed in three parts. In the
Ô¨Årst part, statistical analysis is done to identify and segregate abnormalities, if present,
using the Z score analysis. In the subsequent part, ROC analysis is done to measure
the accuracy of identiÔ¨Åcation of abnormalities by the proposed method with MIAS and
DEMS observations. Finally, the accuracy of the segmented abnormal masses is calcu-
lated for the images in the MIAS and DEMS databases where abnormal masses have
been identiÔ¨Åed by the proposed method. In the next section, the accuracy of the abnor-
mal mass detection is compared with results obtained by other comparable methods
described in Section 15.2 previous work.
15.5.1
Statistical Analysis
The Z score analysis is used to identify the tumor region(s) and distinguish them from
other areas of the mammogram. This analysis is performed on mammogram images
in the MIAS and DEMS databases. The graph Z score analyses of the mammograms
that are reported in the Experimental Result section (Section 15.4) are shown in
Figures 15.18‚Äì15.20, and 15.21. An adjustment value of ¬±0.1 is considered for inference
in the 2ùúéand 3ùúélevels.
The mammogram images are considered in three broad categories, as already
mentioned in this chapter. The fatty mammogram images consist of the least number of
regions, whereas the fatty-glandular have higher and dense-glandular have the highest
number of regions. So, identiÔ¨Åcation of abnormalities becomes more challenging with
the latter two categories. For image mdb272.L, none of the regions exceeds the 3ùúélevel.
For image mdb028.L representing fatty, image mdb001.R representing fatty-glandular,
and image mdb145.R representing dense-glandular, the breast mammogram shows
abnormal regions, that is, equal to or above the 3ùúélevel (¬±0.1).
15.5.2
ROC Analysis
The ROC analysis has been conducted on both the MIAS and DEMS mammogram
databases. Here, the MIAS database is considered as the benchmark due to its clear
documentation regarding classiÔ¨Åcation, size, type, and ground truth (GT) of images by
their own radiologist. Among the 322 mammogram images, 251 mammogram images
of MIAS are classiÔ¨Åed by their radiologist as normal or containing tumor(s), and among
the 251 images, 207 are normal and 44 have abnormal masses. The confusion matrix

362
Hybrid Intelligence for Image Analysis and Understanding
Intensity Distribution of Breast Regions
248
238
228
218
208
198
188
178
168
158
148
138
128
1
26
51
76
101
126
151
176
201
226
251
276
301
326
351
376
401
Truncated Œº (TŒº)
TŒº+œÉ
TŒº+2œÉ
TŒº+3œÉ
Intensity Distribution
Figure 15.18 The Z score analysis graph for mammogram mdb272.L.
248
238
228
218
208
198
188
178
168
158
148
138
128
1
101 201 301 401 501 601 701 801 901 10011101120113011401150116011701180119012001210122012301
Truncated Œº (TŒº)
TŒº+œÉ
TŒº+2œÉ
TŒº+3œÉ
Intensity Distribution
Intensity Distribution of Breast Regions
Figure 15.19 The Z score analysis graph for mammogram mdb028.L.
(Table 15.2) is obtained after implementing the proposed algorithm on the said 251
images to measure the agreement of the proposed algorithm with the available manual
interpretation of the database used (see also Table 15.3).
From the total number of cases of 251, the number of correct detection is 241 with
an accuracy value of 96%, sensitivity is 97.6%, and speciÔ¨Åcity is 88.6%. The total positive
cases missed is 5, and negative cases missed is 5. The empiric ROC area obtained is
0.931. The empirical ROC curve is cited in Figure 15.22.

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
363
Intensity Distribution of Breast Regions
248
238
228
218
208
198
188
178
168
158
148
138
128
1
101
201
301
401
501
601
701
801
901
1001
Truncated Œº (TŒº)
TŒº+œÉ
TŒº+2œÉ
TŒº+3œÉ
Intensity Distribution
Figure 15.20 The Z score analysis graph for mammogram mdb001.R.
Intensity Distribution of Breast Regions
248
238
228
218
208
198
188
178
168
158
148
138
128
1
251
501
751
1001
1251
1501
1751
2001
2251
2501
Truncated Œº (TŒº)
TŒº+œÉ
TŒº+2œÉ
TŒº+3œÉ
Intensity Distribution
Figure 15.21 The Z score analysis graph for mammogram mdb145.R.
Performance evaluation based on the size of tumor obtained by the proposed algo-
rithm and the calculated mass size has been categorized as Table 15.4.
A near-accuracy result is observed in the third and fourth categories due to the large
size of well-deÔ¨Åned masses. In cases of smaller mass size, the accuracy decreases. For
the Ô¨Årst category, three cases are missed due to their smaller size. In case of the second
category, the intensity for two images shows a very low intensity level for the masses

364
Hybrid Intelligence for Image Analysis and Understanding
Table 15.2 Confusion matrix of response data
reported from testing
MIAS truth
Tumor
Normal
Proposed method
Tumor
39
5
Normal
5
39
Table 15.3 Observed operating points
FPF
0.0000
0.1136
1.0000
TPF
0.0000
0.9758
1.0000
1.0
0.5
0.0
0.0
0.5
1.0
ROC Curve
False Positive Fraction
True Positive Fraction
Figure 15.22 Empirical ROC curve for tumor
identiÔ¨Åcation.
Table 15.4 Accuracy measures based on size of mass detected by the proposed method
Size
<1.20cm2
1.21cm2‚àí1.80cm2
1.81cm2‚àí3.60cm2
>3.61cm2
MIAS
11
11
12
10
Proposed method
8
9
12
10
Accuracy
86.4%
90.9%
100%
100%
Sensitivity
90.9%
93.9%
100%
100%
SpeciÔ¨Åcity
72.7%
81.8%
100%
100%

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
365
and has merged with the adjacent regions, and hence went undetected by the proposed
method.
15.5.3
Accuracy Estimation
The performance evaluation of algorithms is done in detail, which endorses the accep-
tance of results. The quantitative measures are used to prove the accuracy of proposed
algorithms by matching the acquired results representing an abnormal mass or masses,
as a ‚Äúmask‚Äù with its corresponding GT image. The average results of diÔ¨Äerent qual-
ity measures using the MIAS and DEMS database for abnormalities are depicted in
Table 15.5. Most of the research articles ignored the boundary accuracy estimation, so
it is diÔ¨Écult to compare. It was found that Rabottino et al. [26] have a reported bound-
ary accuracy estimation of their research with a sensitivity value of 88.34%, which is less
than the sensitivity value of 96.06% achieved by the proposed algorithm.
Table 15.5 Quantitative measures applied to assess the proposed methods
Quantitative measures
Computation
Average result
Accuracy (percentage agreement)
(|TN|+|TP|)/(|TN|+|TP|+|FP|+|FN|)
0.9990
Dice similarity coeÔ¨Écient (DSC)
(2 √ó|TP| )/(2 √ó|TP|+|FP|+|FN| )
0.9164
Error rate
(|FP|+|FN|)/(|FP|+|FN|+|TP|+|TN|)
0.0009
Sensitivity / completeness (CM)
|TP|/(|TP|+|FN|)
0.9606
Correctness (CR)
|TP|/(|TP|+|FP|)
0.9521
SpeciÔ¨Åcity (true negative fraction / rate) |TN|/(|TN|+|FP|)
0.9521
False positive fraction / rate
1‚àíSpeciÔ¨Åcity
0.0478
Underestimation fraction (UEF)
|FN|/(|TN|+|FN|)
0.0004
Overestimation fraction (OEF)
|FP|/(|TN|+|FN|)
0.0005
Table 15.6 Comparative analysis of proposed method with others
Authors
Accuracy
Sensitivity
SpeciÔ¨Åcity
Kegelmeyer et al. [12]
‚Äì
100%
82%
Karssemeijer and Te Brake [11]
90%
‚Äì
‚Äì
Comer et al. [7]
100% (abnormal tissues)
‚Äì
‚Äì
58% (stellate lesions)
Li et al. [14]
‚Äì
90%
‚Äì
Kobatake et al. [13]
90.5%
‚Äì
‚Äì
Yang et al. [34]
86%
‚Äì
‚Äì
Campanini et al. [5]
‚Äì
80%
‚Äì
Jiang et al. [10]
66.4%
54.3%
78.3%
√ñzekes et al. [24]
88.37%
‚Äì
‚Äì
Martins et al. [22]
85%
‚Äì
‚Äì
Rejani et al. [28]
‚Äì
88.75%
‚Äì
Proposed method
96%
97.6%
88.6%

366
Hybrid Intelligence for Image Analysis and Understanding
15.6
Comparative Analysis
The comparative analysis is done intensively with the proposed method and other simi-
lar algorithms mentioned in the Previous Works section (Section 15.2) for identiÔ¨Åcation
of masses. Most of the researchers have not shared the accuracy estimation of their
proposed algorithms. Some authors have demonstrated the ways and measurements
of accuracy estimations but have used diÔ¨Äerent parameters to describe the accuracy
of their algorithms. The most frequently used parameters are accuracy, sensitivity, and
speciÔ¨Åcity. Table 15.6 depicts the comparative analysis of the data gathered from these
algorithms with the proposed one.
15.7
Conclusion
The proposed decision-making method comprises three distinct phases, namely, prepa-
ration, preprocessing, and isolation of abnormal region(s). Preparation is an essential
step toward standardization of medical images that are diÔ¨Écult to interpret. Prepro-
cessing is mandatory and discrete, and it comprises sequential steps like homogeneity
enhancement, edge detection, determination of ROI by eliminating nonbreast regions
like pectoral muscle, accurate estimation of breast contour, and diÔ¨Äerention of anatomi-
cal regions. Finally, isolation of abnormal regions is a process of segmentation to identify
regions with abnormality among other normal regions. Intensity distribution of each
region is heterogeneous, yet a degree of homogeneity is present. The proposed method
extracts that homogeneity feature of a particular region by using the arithmetic mode
value of pixel intensities and colors of that region with that determined mode value.
The abnormal regions show unusual intensity distribution among normal regions. The
proposed method used a Z score to determine that infrequent intensity distribution
and marks it as an abnormality. The proposed method has been tested with standard
mammographic databases comprising CR and DR images of diÔ¨Äerent categories and
showing comparable results with other related research. The ROC analysis suggests the
algorithmic accuracy is 96%, whereas sensitivity and speciÔ¨Åcity are 97.6% and 88.6%,
respectively. The false-positive (FP) and true-negative (TN) cases of mass detection are
under an acceptable threshold when the algorithm is tested with a standard data set,
whereas the accuracy estimation for boundary of mass shows an accuracy value of 99%,
sensitivity 96%, and speciÔ¨Åcity 95.2%. In conclusion, the proposed method can be incor-
porated to CAD for a mass screening program toward isolation of abnormalities due to
its algorithmic simplicity, eÔ¨Éciency, and accuracy.
Acknowledgments
The authors are obliged to Mammographic Image Analysis Society (MIAS) and Dokuz
Eylul University for their public mammographic data sets dedicated for research and
development. They are also thankful to Pradip Saha, MD, Radiology, for his continuous
participation in the proposed work and to Soma Chakraborty, MD, Radiology (Spe-
cialist in Mammography), for her expert opinions and comments. The authors are also

IdentiÔ¨Åcation of Abnormal Masses in Digital Mammogram Using Statistical Decision Making
367
appreciative to Sangita Bhattacharjee, Sumit Das, and Sanjay Nag for their contributions
related to algorithm development, coding, and documentation of the proposed work.
The authors are especially grateful to Sisir Chatterejee for his expert opinions regarding
mathematical and statistical modeling related to the proposed method.
References
1 Balachandran et al. (2005) Cancer ‚Äì an ayurvedic perspective. Pharmacological
Research, 51, pp. 19‚Äì30.
2 Bassett et al. (1989) Breast sonography: technique equipment and normal anatomy.
Seminars in Ultrasound CT and MR, 10 (2), pp. 82‚Äì89.
3 Bhishagratha (1991) Sushruta samhita. Choukhamba Orientalia, Varanasi.
4 Brake et al. (2001) Segmentation of suspicious densities in digital mammograms.
Medical Physics, 28, pp. 259‚Äì266.
5 Campanini et al. (2004) A novel featureless approach to mass detection in digital
mammograms based on support vector machines. Physics in Medicine and Biology,
49 (6), pp. 961‚Äì975.
6 Catarious et al. (2004) Incorporation of an iterative linear segmentation routine into
a mammographic mass CAD system. Medical Physics, 31, pp. 1512‚Äì1520.
7 Comer et al. (1996) Statistical segmentation of mammograms. Digital Mammogra-
phy, International Congress Series. Elsevier, Amsterdam, pp. 471‚Äì474.
8 Ferlay et al. (2004) GLOBOCAN 2002: cancer incidence mortality and prevalence
worldwide. IARC Cancer Base, 2 (5).
9 International Agency for Research on Cancer Working Group. (2002) The evalua-
tion of cancer preventive strategies. Handbooks of Cancer Prevention, Breast Cancer
Screening, vol. 7. IARC Press, Lyon, France.
10 Jiang et al. (2008) Automated detection of breast mass spiculation levels and evalua-
tion of scheme performance. Academic Radiology, 15 (12), pp. 5341‚Äì1544.
11 Karssemeijer et al. (1996) Detection of stellate distortions in mammograms. The
Institute of Electrical and Electronics Engineers Transactions on Medical Imaging, 15,
pp. 611‚Äì619.
12 Kegelmeyer et al. (1994) Computer-aided mammographic screening for spiculated
lesions. The Journal of Pathology, 191, pp. 331‚Äì337.
13 Kobatake et al. (1999) Computerized detection of malignant tumors on digital mam-
mograms. IEEE Transactions on Medical Imaging, 18 (5), pp. 369‚Äì378.
14 Li et al. (1995) Markov random Ô¨Åeld for tumor detection in digital mammography.
The Institute of Electrical and Electronics Engineers Transactions on Medical Imaging,
14, pp. 565‚Äì576.
15 Maitra et al. (2011) Accurate breast contour detection algorithms in digital mammo-
gram. International Journal of Computer Applications, 25 (5), pp. 1‚Äì13.
16 Maitra et al. (2011) Artefact suppression and homogenous orientation of digi-
tal mammogram using seeded region growing algorithm. International Journal of
Computer Information Systems, 3 (4), pp. 32‚Äì38.
17 Maitra et al. (2011) Automated digital mammogram segmentation for detection
of abnormal masses using binary homogeneity enhancement algorithm. Journal of
Computer Science and Engineering (IJCSE), 2 (3), pp. 416‚Äì427.

368
Hybrid Intelligence for Image Analysis and Understanding
18 Maitra et al. (2011) Detection and isolation of pectoral muscle from digital mam-
mogram: an automated approach. International Journal of Advance Research in
Computer Science, 2 (3), pp. 375‚Äì380.
19 Maitra et al. (2012) Technique for pre-processing of digital mammogram. Computer
Methods and Programs in Biomedicine Elsevier (CMPB), 107 (2), pp. 175‚Äì188.
20 Maitra et al. (2015) A tree based approach towards edge detection of medical image
using MDT. International Journal of Computer Graphics, 6 (1), pp. 37‚Äì56.
21 Maitra et al. (2011) Anatomical segmentation of digital mammogram to diÔ¨Äerentiate
breast regions. International Journal of Research and Reviews in Computer Science
(IJRRCS), 2 (6), pp. 1327‚Äì1330.
22 Martins et al. (2009) Detection of masses in digital mammograms using K-means
and support vector machine. Electronic Letters on Computer Vision and Image
Analysis, 8 (2), pp. 39‚Äì50.
23 Mehta, (2012) Ayurveda & Cancer. Available from pdayurvedatoday.com.
24 √ñzekes et al. (2005) Computer aided detection of mammographic masses on digital
mammograms. Istanbul Ticaret √úniversitesi Fen Bilimleri Dergisi, 4 (8) pp. 87‚Äì97.
25 Petrick et al. (1996) Automated detection of breast masses on mammograms using
adaptive contrast enhancement and texture classiÔ¨Åcation. Medical Physics, 23,
pp. 1685‚Äì1696.
26 Rabottino et al. (2008) Mass contour extraction in mammographic images for breast
cancer identiÔ¨Åcation. Exploring New Frontiers of Instrumentation and Methods for
Electrical and Electronic Measurements, pp. 1‚Äì6.
27 Rangayyan (2005) Biomedical image analysis. CRC Press, New Delhi.
28 Rejani et al. (2009) Early detection of breast cancer using SVM classiÔ¨Åer technique.
International Journal on Computer Science and Engineering, 1 (3), pp. 127‚Äì130.
29 Sharma (1981) Charaka samhita. Choukhamba Orientalia, Varanasi.
30 Stewart et al. (2003) World cancer report. IARC Press, Lyon, France.
31 Suckling et al. (1994) The Mammographic Image Analysis Society Digital Mammo-
gram Database. Exerpta Medica, International Congress Series 1069, pp. 375‚Äì378.
32 Tata Memorial Hospital. (2013) Available from tmc.gov.in
33 Timp et al. (2004) A new 2D segmentation method based on dynamic program-
ming applied to computer aided detection in mammography. Medical Physics, 31,
pp. 958‚Äì971.
34 Yang et al. (2005) A computer-aided system for mass detection and classiÔ¨Åcation in
digitized mammograms. Biomedical Engineering applications, Basis & Communica-
tions, 17 (5), pp. 215‚Äì229.
35 Zheng et al. (1995) Computer detection of masses in digitized mammograms using
single-image segmentation and a multilayer topographic feature analysis. Academic
Radiology, 2, pp. 959‚Äì966.

369
16
Automatic Detection of Coronary Artery Stenosis Using Bayesian
ClassiÔ¨Åcation and Gaussian Filters Based on DiÔ¨Äerential
Evolution
Ivan Cruz-Aceves1, Fernando Cervantes-Sanchez2, and Arturo Hernandez-Aguirre2
1CONACYT ‚Äì Centro de Investigaci√≥n en Matem√°ticas (CIMAT), A.C., Jalisco S/N, Col. Valenciana, Guanajuato, M√©xico
2Centro de Investigaci√≥n en Matem√°ticas (CIMAT), A.C., Jalisco S/N, Col. Valenciana, Guanajuato, M√©xico
16.1
Introduction
Automatic detection of coronary artery stenosis represents one of the most important
challenges in cardiology. In clinical practice, the specialists perform a visual examination
over the X-ray angiograms obtained by a procedure involving cardiac catheterization.
Subsequently, a manual detection of potential cases of blood vessel stenosis is carried
out. Two cases of coronary stenosis are illustrated in Figure 16.1.
In systems that perform computer-aided diagnosis, the detection of vessel stenosis is
commonly addressed in the steps of vessel detection, vessel segmentation, and classi-
Ô¨Åcation of stenosis. In the Ô¨Årst stage of vessel detection, the vessel-like structures are
enhanced while removing noise from the input image. The segmentation step is used
to discriminate vessel and nonvessel pixels from the enhanced image as white and black
regions, respectively. Finally, the classiÔ¨Åcation step to detect coronary stenosis is carried
out by selecting a set of independent features in order to apply a classiÔ¨Åcation strategy.
In the literature, the automatic detection and segmentation of blood vessels have
been addressed by using diÔ¨Äerent perspectives in the spatial image domain. Some of
the state-of-the-art methods are based on mathematical morphology [1‚Äì6] and the
Hessian matrix [7‚Äì15]. Another technique based on the convolution of a template with
the input image is the Gaussian matched Ô¨Ålter (GMF) proposed by Chaudhuri et al.
[16]. The GMF represents one of the most commonly applied detection techniques for
diÔ¨Äerent types of blood vessels.
The GMF assumes that the shape of blood vessels can be approximated by a Gaussian
curve as a matching template at diÔ¨Äerent orientations. The generation of this Gaussian
template is governed by the parameters of spread of the Gaussian function, the width and
length of the template, and the number of oriented Ô¨Ålters to form a directional bank. Due
to the eÔ¨Éciency and ease of implementation of the GMF, it has been applied successfully
in diÔ¨Äerent clinical studies [17‚Äì19]. The main disadvantage of the GMF is the selec-
tion of the optimal parameter values for each particular application. To detect blood
vessels in retinal fundus images, Cinsdikici and Aydin [20] proposed experimentally
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

370
Hybrid Intelligence for Image Analysis and Understanding
Figure 16.1 Two X-ray angiograms with detection of coronary stenosis performed by a cardiologist.
determined parameter values, and Al-Rawi et al. [21, 22] proposed to introduce a search
space for each parameter using an exhaustive global search and genetic algorithms as
search strategies. On the other hand, to detect coronary arteries, Kang et al. [23‚Äì25]
proposed diÔ¨Äerent empirical values for the spread of the Gaussian proÔ¨Åle and the num-
ber of directional Ô¨Ålters. Cruz et al. [26, 27] proposed a new search space for the GMF
parameters, and also an empirically determined objective function with an estimation
of distribution algorithm to perform the optimization task. In our previous work [28], a
comparative analysis of four nature-inspired algorithms was performed over the train-
ing stage of the GMF technique. In that analysis, the algorithm of diÔ¨Äerential evolution
(DE) obtained the highest detection performance over a training set of 40 angiograms.
In the present chapter, a novel method for the automatic detection of vessel stenosis
in X-ray coronary angiograms is introduced. The proposed method uses GMFs tuned
by DE for the detection of coronary arteries, and an iterative thresholding method for
the segmentation of the Ô¨Ålter response. To evaluate the performance of these two steps,
the receiver operating characteristic (ROC) curve and the accuracy measure have been
adopted. Finally, in the segmented coronary angiograms, the naive Bayes classiÔ¨Åer is
applied over a 3D feature vector obtained from the results of a second-order derivative
operator in order to identify potential cases of coronary stenosis.
The remainder of this chapter is organized as follows. In Section 16.2, the funda-
mentals of the GMFs, DE, and the naive Bayes classiÔ¨Åer are described in detail. In
Section 16.3, the proposed method consisting of the steps of coronary artery detection
and segmentation, along with the process to detect potential cases of coronary stenosis,
is analyzed. The experimental results are presented in Section 16.4, and conclusions are
given in Section 16.5.
16.2
Background
This section introduces the fundamentals of the GMFs for vessel detection, DE as an
optimization strategy, and the naive Bayes classiÔ¨Åer to identify coronary stenosis, which
are of interest in the present work.

Automatic Detection of Coronary Artery Stenosis Using Bayesian ClassiÔ¨Åcation
371
16.2.1
Gaussian Matched Filters
The GMF method is a template-based approach proposed by Chaudhuri et al. [16] for
detecting blood vessels. GMF works on the assumption that the shape of vessel-like
structures in the spatial image domain can be computed by a Gaussian curve as a tem-
plate. This Gaussian matching template can be deÔ¨Åned as follows:
G(x, y) = ‚àíexp
(
‚àíx2 + y2
2ùúé2
)
,
|y| ‚â§L‚àï2,
(16.1)
where L represents the length in pixels of the vessel segment to be enhanced, and ùúéis
a continuous parameter that represents the average width of the vessel-like structures.
Since the Gaussian curve has inÔ¨Ånitely long double-sided trails, they are usually trun-
cated at u = ¬±3ùúé, by introducing a discrete parameter T to determine a neighborhood
N, and the position in the Gaussian template where the curve trails will cut as follows:
N = {(u, ùë£), |u| ‚â§T, |ùë£| ‚â§L‚àï2}.
(16.2)
To detect vessel-like structures at diÔ¨Äerent orientations, the Gaussian kernel G(x, y) is
rotated by applying a geometric transformation in order to form a directional Ô¨Ålter bank
of ùúÖ= 180‚àò‚àïùúÉ, evenly spaced Ô¨Ålters in the range [‚àíùúã
2 , ùúã
2 ], as follows:
ùúÖ=
[
cos ùúÉi ‚àísin ùúÉi
sin ùúÉi
cos ùúÉi
]
,
(16.3)
where ùúÉi = (iùúã‚àïùúÖ), i = {1, 2, ‚Ä¶ , ùúÖ} is the angular resolution, and ùúÖthe number of
directional Ô¨Ålters. Finally, these directional Gaussian matching templates are convolved
with the input image, and for each pixel in the image, the maximum response over all
orientations is preserved to generate the Ô¨Ålter response.
Since the performance of the GMF depends on three discrete parameters (L, T, ùúÖ)
and one continuous parameter (ùúé), the selection of the optimal parameter values for the
GMF plays a vital role for each speciÔ¨Åc application. In order to illustrate the eÔ¨Äects of
the parameter values for the GMF, in Figure 16.2a and 16.2b, an X-ray angiogram along
with its ground-truth image are introduced. Figure 16.2d and 16.2g present diÔ¨Äerent
Gaussian templates, along with the obtained Ô¨Ålter response in Figure 16.2f and 16.2i,
respectively, using the angiogram in Figure 16.2a.
16.2.2
DiÔ¨Äerential Evolution
DE represents a stochastic real-parameter strategy proposed by Storn and Price [29, 30]
for solving numerical global optimization problems. Similar to evolutionary algorithms,
DE uses a set of Np potential solutions (also called individuals) X = {x1, x2, ‚Ä¶ , xNp},
which are iteratively improved using mutation, crossover, and selection operators, and
also evaluated according to a Ô¨Åtness function. The mutation step is used to generate a
mutant vector Vi,g+1 for each generation g based on the spatial distribution of the whole
population {Xi,g|i = 1, 2, ‚Ä¶ , Np} as follows:
Vi,g+1 = Xr1,g + F(Xr2,g ‚àíXr3,g),
r1 ‚â†r2 ‚â†r3 ‚â†i,
(16.4)

372
Hybrid Intelligence for Image Analysis and Understanding
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
0
0.2
0.4
0.6
0.8
1
2
4
6
8
10
12
Figure 16.2 (a) X-ray coronary angiogram. (b) Ground-truth image of angiogram in (a). (c) Gaussian
proÔ¨Åle of the method of Chaudhuri et al. [16]. Second row: Template using ùúé= 2.0, L = 9, T = 13, with
ùúÉ= 0‚àò, and ùúÉ= 45‚àò, respectively, and resulting Ô¨Åltered image in (f). Last row: Matching template using
ùúé= 2.0, L = 7, T = 15, with ùúÉ= 0‚àò, and ùúÉ= 45‚àò, respectively, and resulting Ô¨Åltered image in (i).
where r1, r2, and r3 correspond to the indexes of three diÔ¨Äerent individuals from the
population {1, ‚Ä¶ , Np} in the current generation g; and F is the mutation parameter,
also called the diÔ¨Äerentiation factor. In the second step, a crossover operator is applied
to create a trial vector Ui,g+1 as follows:
Ui,g+1 =
{
Vi,g+1,
if r ‚â§CR
Xi,g,
if r > CR
(16.5)
where r is a uniform random value on the interval [0, 1], which is generated to be com-
pared with the crossover rate, CR ‚àà[0, 1]. The CR parameter is used to control the

Automatic Detection of Coronary Artery Stenosis Using Bayesian ClassiÔ¨Åcation
373
diversity of solutions in the population, and the rate of information that is copied from
the mutant vector. If the random value r is bigger than the parameter CR, the current
individual Xi,g is conserved; otherwise, the mutant vector information Vi,g+1 is copied to
the trial vector Ui,g+1 to be used in the selection stage. In the last step of DE, a selection
operator is introduced to obtain the best one between the individual Xi,g and the trial
vector Ui,g+1 in order to replace the information of the current individual Xi,g in the next
generation Xi,g+1.
Xi,g+1 =
{
Ui,g+1,
if f (Ui,g+1) < f (Xi,g)
Xi,g,
otherwise
(16.6)
According to the above description, the DE technique can be implemented by the
following procedure:
1. Initialize the number of generations G and the population size Np.
2. Initialize the mutation factor F and crossover rate CR.
3. Initialize each individual Xi with random values within the search space.
4. For each individual Xi,g, where g = {1, ‚Ä¶ , G}:
A. Compute Vi,g+1 by the mutation operator (16.4);
B. Obtain Ui,g+1 using the crossover operator (16.5);
C. Update Xi,g+1, applying the selection operator (16.6).
5. Stop if the convergence criterion is satisÔ¨Åed (e.g., the number of generations).
16.2.2.1
Example: Global Optimization of the Ackley Function
In order to illustrate the implementation details of DE, the optimization of a mathemat-
ical function is introduced, which represents the main application of population-based
methods. In this section, the Ackley function in two dimensions is presented, which is
deÔ¨Åned in Equation 16.7 and shown in Figure 16.3. The range for each variable of the
X
‚àí30
‚àí25
‚àí20
‚àí15
‚àí10
‚àí5
0
5
10
15
20
25
30
Y
‚àí30
‚àí25
‚àí20
‚àí15
‚àí10
‚àí5
0
5
10
15
20
25
30
f(X,Y)
2
4
6
8
10
12
14
16
18
20
22
5
10
15
20
(a)
 4 
 6 
 8 
 10 
 12 
 14 
 16 
 18 
 20 
‚àí30
‚àí20
‚àí10
0
10
20
30
‚àí30
‚àí20
‚àí10
0
10
20
30
(b)
Figure 16.3 Ackley function in two dimensions. (a) Isometric view in X1 and X2, and (b) level plot of the
function, where the optimal value is located at X1 = 0 and X2 = 0.

374
Hybrid Intelligence for Image Analysis and Understanding
Ackley function is X1 ‚àà[‚àí32.768, 32.768], X2 ‚àà[‚àí32.768, 32.768], and the optimal value
is located on (X1 = 0.0, X2 = 0.0).
f (x) = ‚àí20 ‚ãÖexp
‚éõ
‚éú‚éú‚éú
‚éù
‚àí0.2 ‚ãÖ
‚àö
‚àö
‚àö
‚àö
‚àö1
n
n
‚àë
i=1
(x2
i )
‚éû
‚éü‚éü‚éü
‚é†
‚àíexp
(
1
n
n
‚àë
i=1
cos(2 ‚ãÖùúã‚ãÖxi)
)
+ 20 + exp.
(16.7)
Since DE is a real-coded strategy, the individuals can be randomly initialized in the
continuous domain of the particular function. To illustrate the optimization process
that DE follows to optimize a mathematical function, in Figure 16.4 a numerical example
using DE to minimize the aforementioned Ackley function is presented.
Figure 16.4 Numerical example for solving the 2D Ackley function using DE as an optimization
strategy.

Automatic Detection of Coronary Artery Stenosis Using Bayesian ClassiÔ¨Åcation
375
16.2.3
Bayesian ClassiÔ¨Åcation
To determine the relationship between a feature set and the class variable, the Bayes
theorem can be used as a modeling probabilistic strategy. The Bayes theorem has been
commonly applied for solving classiÔ¨Åcation problems under a statistical perspective,
which can be computed by applying the following formula:
P(Y|X) = P(X|Y)P(Y)
P(X)
(16.8)
where P(Y|X) and P(Y) are the posterior and prior probabilities for Y, respectively;
P(X|Y) is the likelihood; and P(X) is the evidence. During the training step, the pos-
terior probabilities P(Y|X) for each combination of X and Y variables must be learned
in order to be maximized for correct classiÔ¨Åcation. The process to estimate the posterior
probabilities is a challenging problem because it requires a large training set; otherwise,
to compute the class-conditional probabilities P(X|Y), the naive Bayes classiÔ¨Åer can be
applied.
The naive Bayes classiÔ¨Åer is used to estimate the class-conditional probability assum-
ing that the features are independent, according to a class label [31]. This independence
assumption can be deÔ¨Åned as follows:
P(X|Y = y) =
f‚àè
i=1
P(Xi|Y = y),
(16.9)
where f denotes the number of features of the set X = {X1, X2, ‚Ä¶ , Xf }. On the other
hand, the process to classify a test record is carried out by computing the posterior
probability for each given class Y, as follows:
P(Y|X) =
‚àèf
i=1 P(Xi|Y)P(Y)
P(X)
.
(16.10)
To work with categorical features, the naive Bayes classiÔ¨Åer estimates the rate of
training instances in a given class y with a particular feature value xi. For continuous
attributes, a Gaussian distribution has been commonly applied to represent the
class-conditional probability. This distribution is deÔ¨Åned by two diÔ¨Äerent parameters,
its mean (ùúá) and standard deviation (ùúé), which can be calculated over the training
records that belong to the class yj as follows:
P(Xi = xi|Y = yj) =
1
‚àö
2ùúãùúéij
exp
‚àí
(xi‚àíùúáij)2
2ùúé2
ij .
(16.11)
16.2.3.1
Example: ClassiÔ¨Åcation Problem
To illustrate the process carried out by the naive Bayes classiÔ¨Åer, a training data set for a
medical classiÔ¨Åcation problem is introduced in Table 16.1, where the class-conditional
probability for each one of the four attributes (one categorical and three continuous)
must be computed.
This problem presents a medical test for patients with risk of cardiovascular disease.
The test is performed using the following four attributes: antecedents of cardiovascular
disease, age of the patient, triglyceride level, and low-density lipoprotein (LDL) choles-
terol. The value ranges for the triglyceride level can be categorized as normal for less

376
Hybrid Intelligence for Image Analysis and Understanding
Table 16.1 Training records for predicting cardiovascular risk
Patient
Antecedents
Age
Triglyceride level
LDL cholesterol
Risk
1
no
30
150
100
low
2
yes
56
480
190
high
3
yes
60
520
170
high
4
no
32
100
90
low
5
no
24
120
80
low
6
no
48
600
200
high
7
no
52
580
220
high
8
yes
50
540
170
high
9
no
28
135
75
low
10
yes
54
400
210
high
than 150 mg/dL, high between 200 and 499 mg/dL, and very high for above 499 mg/dL.
The value ranges for the LDL cholesterol level can be categorized as optimal for less than
100 mg/dL, high between 160 and 189 mg/dL, and very high for above 189 mg/dL.
The class-conditional probabilities for the cardiovascular classiÔ¨Åcation problem are
presented here:
For the Antecedents attribute:
P(Antecedents=yes|low) = 0
P(Antecedents=no|low) = 1
P(Antecedents=yes|high) = 4
6
P(Antecedents=no|high) = 2
6.
For the Age attribute:
Class low: sample mean = 28.50, sample variance = 11.67
Class high: sample mean = 53.33, sample variance = 18.67.
For the Triglyceride level attribute:
Class low: sample mean = 126.25, sample variance = 456.25
Class high: sample mean = 520, sample variance = 5280.
For the LDL cholesterol attribute:
Class low: sample mean = 86.25, sample variance = 122.92
Class high: sample mean = 193.33, sample variance = 426.67.
To classify the corresponding label of a new record X = ( Antecedents = no, Age = 40,
Triglyceride level = 550, LDL cholesterol = 185), the posterior probabilities need to be
computed as follows:
P(X low) = P(Antecedents = no
|
low) √ó P(Age = 40
|
low) √ó P(Tri. level = 550
|
|low) √ó
P(LDL cholesterol = 185|low) = 5.5658e-110
P(X|high) = P(Antecedents = no|high) √ó P(Age = 40|high) √ó P(Tri. level = 550|high)
√ó P(LDL cholesterol = 185|high) = 2.3491e-8.

Automatic Detection of Coronary Artery Stenosis Using Bayesian ClassiÔ¨Åcation
377
According to the above procedure, since P(X|high) is higher than P(X|low), the predic-
tion of the processed test record reveals that the new patient has a risk of cardiovascular
disease.
To perform the naive Bayes classiÔ¨Åer in another classiÔ¨Åcation task, the process illus-
trated in the present example is similar. The posterior probabilities must be computed
according to a predeÔ¨Åned training set of features.
16.3
Proposed Method
The proposed method consists of three main steps. The Ô¨Årst step is used for vessel detec-
tion by applying GMFs trained by DE over a predeÔ¨Åned search space. The second step is
used to segment the GMF response by applying an automatic thresholding technique,
and Ô¨Ånally, in the segmented vessels, a Bayesian classiÔ¨Åer is applied to detect coronary
artery stenosis in X-ray angiograms.
16.3.1
Optimal Parameter Selection of GMF Using DiÔ¨Äerential Evolution
Commonly, the GMF parameter values have been experimentally or empirically
determined for each particular application. Chaudhuri et al. [16] proposed the set
of parameters as L = 9, T = 13, ùúé= 2.0, and ùúÖ= 12 for retinal blood vessels. Kang
et al. [23‚Äì25] proposed an average width of ùúé= 1.5 with ùúÖ= 6 directional Ô¨Ålters to
be applied on coronary angiograms. As part of a system to segment ophthalmoscope
images, Cinsdikici and Aydin [20] proposed a new angular resolution using ùúÉ= 10‚àò,
obtaining ùúÖ= 18 directional templates. Al-Rawi et al. [21] introduced an exhaustive
search over a new range for the variables as L = {7, 7.1, ‚Ä¶ , 11}, T = {2, 2.25, ‚Ä¶ , 10},
and ùúé= {1.5, 1.6, ‚Ä¶ , 3} to be applied in retinal fundus images. In this method, the
set of parameters with the highest area under the ROC curve in a training step was
applied over the test images. Later, Al-Rawi and Karajeh [22] proposed the use of
genetic algorithms to replace the exhaustive search, keeping the same search space for
retinal images. The GMF trained by genetic algorithms achieves better performance
than the methods with Ô¨Åxed values in terms of segmentation accuracy, and also better
performance than the exhaustive search in terms of computational time and number
of evaluations to obtain the best set of parameters.
Based on the suitable performance of the population-based method over a predeÔ¨Åned
range of parameters, in the present work, DE has been performed to obtain the optimal
set of parameters for blood vessel detection in X-ray coronary angiograms using as
a Ô¨Åtness function the area (Az) under the ROC curve. The search space for the GMF
parameters was deÔ¨Åned according to the aforementioned methods, and taking into
account the features of blood vessels in the angiograms, as can be consulted in our
previous work [28]. In the experiments, the search space was set as L = {8, 9, ‚Ä¶ , 15},
T = {8, 9, ‚Ä¶ , 15}, and ùúé= [1, 5], keeping constant the number of directional Ô¨Ålters as
ùúÖ= 12 and obtaining an angular resolution of ùúÉ= 15‚àò.
In the optimization process, the set of parameters that maximizes the Az value using a
training set is directly applied over the test set of angiograms. The ROC curve represents
a plot between the true-positive fraction (TPF) and false-positive fraction (FPF) of a
classiÔ¨Åcation system. The TPF corresponds to the rate of vessel pixels correctly detected

378
Hybrid Intelligence for Image Analysis and Understanding
by the method, and the FPF to the rate of nonvessel pixels incorrectly classiÔ¨Åed by the
method. The area Az can be approximated by applying the Riemann-sum method, and
it is deÔ¨Åned in the range [0, 1], where 1 is perfect classiÔ¨Åcation, and zero otherwise.
16.3.2
Thresholding of the Gaussian Filter Response
To discriminate the vessel-like structures from the background of the Gaussian Ô¨Ålter
response, a soft classiÔ¨Åcation strategy can be applied. The thresholding method intro-
duced by Ridler and Calvard (RC) [32] assumes that two Gaussian distributions with the
same variance are present in the gray-level histogram of an input image. The fundamen-
tal idea of the method can be deÔ¨Åned by the following:
T = ùúáB + ùúáF
2
(16.12)
where ùúáB is the mean of background pixels, and ùúáF is the mean of foreground pixels.
This method selects an initial global threshold based on the average gray level of the
image to form the two initial classes of pixels. Then, the means of both classes are com-
puted to determine the new global threshold value. This process is iteratively performed
until stability is achieved.
According to the above description, the RC method can be implemented as follows:
1. Select an initial threshold value (T).
2. Apply image thresholding using T.
3. Obtain two classes of pixels, B and F.
4. Compute the mean intensity ùúáB from B.
5. Compute the mean intensity ùúáF from F.
6. Calculate the new T value using Equation 16.12.
7. Repeat until the convergence criterion is satisÔ¨Åed (e.g., ŒîT < ùúñ).
After the thresholding is applied over the Gaussian Ô¨Ålter response, several isolated
regions and pixels can appear in the segmented image, aÔ¨Äecting the performance evalu-
ation. To eliminate these isolated regions, a length Ô¨Ålter is applied based on the concept
of connected components. Since the Ô¨Ålter is governed by the parameter of size (number
of connected components), this parameter value has to be experimentally determined.
In our experiments, the best value for the length Ô¨Åltering was established as size = 500,
as is illustrated in Figure 16.5.
To evaluate the performance of the segmentation stage, the accuracy measure has
been adopted, since it represents the most commonly used metric for binary classiÔ¨Å-
cation. The accuracy measure reveals the rate of correctly classiÔ¨Åed pixels (sum of true
positives and true negatives) divided by the total number of pixels in the input image, as
follows:
Accuracy =
TP + TN
TP + FP + TN + FN
(16.13)
16.3.3
Stenosis Detection Using Second-Order Derivatives
To detect potential cases of coronary artery stenosis over the segmented image, a search
strategy for local minima points can be implemented. This basic strategy is divided into
four diÔ¨Äerent steps. First, a skeletonization operator is applied over the vessel pixels
in order to determine the centerline of the coronary arteries. Second, the distance

Automatic Detection of Coronary Artery Stenosis Using Bayesian ClassiÔ¨Åcation
379
Figure 16.5 First row: Segmentation results using the Ridler and Calvard method. The remaining three
rows illustrate the results of length Ô¨Åltering using 100, 200, and 500 pixels as connected components,
respectively.
from vessel centerline pixels to the closer boundary pixels of the segmented artery is
computed using the Euclidean distance. Subsequently, vessel bifurcations are localized
over the morphological skeleton in order to separate the vessel segments in the image.
Finally, using the Euclidean distance computed above, a search strategy to detect local
minima skeleton pixels (second-order derivatives) is applied over the separate vessel
segments. Figure 16.6 illustrates the aforementioned procedure to detect potential
cases of stenosis.
It can be observed that the method based on second-order derivatives (local minima)
is highly sensitive to false positives. Accordingly, robust methods for classiÔ¨Åcation must
be applied; however, the potential cases obtained by this strategy can be used as input
for more sophisticated procedures or pattern recognition techniques.
16.3.4
Stenosis Detection Using Bayesian ClassiÔ¨Åcation
To apply a Bayesian classiÔ¨Åcation strategy to the problem of vessel stenosis detection, a
set of features must be selected. The features represent the independent variables that
are evaluated in order to separate the test records according to a corresponding label. In
the experiments, the naive Bayes classiÔ¨Åcation technique is applied over three features

380
Hybrid Intelligence for Image Analysis and Understanding
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Figure 16.6 (a) X-ray coronary angiogram. (b) Skeleton of segmented vessel. (c) Addition of skeleton
and boundary pixels. (d) Skeleton using normalized intensities as Euclidean distance. (e) Separation of
vessel segments using bifurcation pixels. (f, g) Detection of local minima points over Gaussian Ô¨Ålter
response and original angiogram, respectively. (h) Stenosis detection marked in a black circle by
cardiologist.
based on the histogram of the vessel width estimation. The three continuous features
are:
Sum of vessel widths: The sum of all the amplitudes according to the frequencies
Mean of the vessel width histogram: The mean value of the histogram of the vessel width
estimation
Standard deviation of the vessel width histogram: The standard deviation value of the
histogram of the vessel width estimation.
To illustrate a case of vessel stenosis and a case of no stenosis, Figure 16.7 presents
both cases along with their histogram of vessel width estimation.

Automatic Detection of Coronary Artery Stenosis Using Bayesian ClassiÔ¨Åcation
381
(a)
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
Vessel width estimation
Frequency
(b)
(c)
1
2
3
4
5
6
7
8
9
10
0
2
4
6
8
10
12
14
16
18
20
Vessel width estimation
Frequency
(d)
Figure 16.7 (a) Stenosis pattern of 30√ó30 pixels. (b) Histogram of vessel width estimation of pattern in
(a). (c) No stenosis pattern of 30√ó30 pixels. (d) Histogram of vessel width estimation of pattern in (c).
The feature selection was established taking into account the shape and distribution of
the histograms generated with the training set of 20 patterns. Since the stenosis problem
represents a binary classiÔ¨Åcation task, the accuracy measure consisting in the fraction
of correct predictions divided by the total number of data points has been adopted.
16.4
Computational Experiments
The computational simulations presented in this section were performed with an Intel
Core i3, 2.13 GHz processor, and 4 GB of RAM using MATLAB version 2013b. The
database of X-ray angiograms was provided by the Mexican Social Security Institute,
and it consists of 80 images from diÔ¨Äerent patients. To assess the performance of the
proposed method, 20 images are used as a training set and the remaining 60 angiograms

382
Hybrid Intelligence for Image Analysis and Understanding
as a testing set. For the classiÔ¨Åcation of vessel stenosis, 40 patterns of size 30 √ó 30 pixels
are used, where the training and testing sets are formed by 20 patterns each.
16.4.1
Results of Vessel Detection
Since the vessel detection step represents the most important task to enhance vessel-like
structures, a comparative analysis with diÔ¨Äerent state-of-the-art detection methods is
introduced. Because the method of Al-Rawi et al. [21] and the proposed method need
a training stage, the 20 angiograms of the training set were used for tuning the GMF
parameters, as was described in Section 16.3.1.
In Table 16.2, the vessel detection performance in terms of the Az value of Ô¨Åve
GMF-based methods is compared with that obtained by the proposed method using
the test set. The method of Kang et al. [23‚Äì25] presents the lowest detection perfor-
mance using the 60 angiograms of the test set. On the other hand, the methods of
Al-Rawi et al. [21], Cruz et al. [27], Chaudhuri et al. [16], and Cinsdikici et al. [20]
present similar performance; however, the proposed method shows superior vessel
detection in terms of area under the ROC curve. In order to illustrate the vessel
detection results, Figure 16.8 presents a subset of X-ray angiograms with the Gaussian
Ô¨Ålter response obtained by the comparative methods.
16.4.2
Results of Vessel Segmentation
In the second stage of the proposed method, the Gaussian Ô¨Ålter response is thresholded
in order to classify vessel and nonvessel pixels, where diÔ¨Äerent thresholding strategies
can be compared. In our experiments, to select the best one thresholding strategy for
coronary arteries, a comparative analysis in terms of segmentation accuracy was per-
formed.
Table 16.3 presents a comparison of Ô¨Åve thresholding methods over the test set of 60
angiograms. The methods of Kapur et al. [33] based on the entropy of the histogram and
the method of histogram concavity [34] obtain the lowest performance. The methods of
Pal and Pal [35] and RATS [36] obtain a suitable performance over the test set; however,
since the iterative RC method [32] achieves the highest segmentation accuracy, it was
selected for further analysis.
To perform a qualitative analysis of the segmented blood vessels, Figure 16.9 presents
a subset of angiograms with the corresponding ground-truth images. The results
Table 16.2 Comparative analysis of Az values with
the testing set, using the proposed method and Ô¨Åve
GMF-based methods of the state of the art
Detection method
Az value
Kang et al. [23‚Äì25]
0.918
Al-Rawi et al. [21]
0.921
GMF entropy [27]
0.923
Chaudhuri et al. [16]
0.925
Cinsdikici et al. [20]
0.926
Proposed method
0.941

Automatic Detection of Coronary Artery Stenosis Using Bayesian ClassiÔ¨Åcation
383
Figure 16.8 First row: Subset of X-ray angiograms. Second row: Ground-truth images. The remaining
six rows present the Gaussian Ô¨Ålter response of the methods of Kang et al. [25], Al-Rawi et al. [21], Cruz
et al. [27], Chaudhuri et al. [16], and Cinsdikici et al. [20], and the proposed method, respectively.

384
Hybrid Intelligence for Image Analysis and Understanding
Table 16.3 Comparative analysis of Ô¨Åve automatic
thresholding methods over the Gaussian Ô¨Ålter
response using the test set of X-ray angiograms
Thresholding method
Accuracy value
Kapur et al. [33]
0.467
Histogram concavity [34]
0.656
Pal and Pal [35]
0.857
RATS [36]
0.953
Ridler and Calvard [32]
0.962
acquired by the methods of Kapur et al. [33] and Rosenfeld et al. [34] present a low rate
of true-positive pixels and the presence of broken vessels, which leads to a low-accuracy
performace. The main issue of the method of Pal and Pal [35] is the presence of a high
rate of false-positive pixels, which decreases the accuracy measure. The RATS method
[36] shows many broken vessels while presenting a high rate of false-positive pixels.
Moreover, the RC method [32] shows a suitable detection of true-positive pixels while
avoiding broken vessels and obtaining a low rate of false-positive pixels, which is useful
to analyze the entire coronary tree for the blood vessel stenosis problem.
16.4.3
Evaluation of Detection of Coronary Artery Stenosis
To perform the last stage of the proposed method, the product between the vessel seg-
mentation result and the input X-ray angiogram is illustrated in Figure 16.10. This prod-
uct is useful for working with a wide range of features, which is highly desirable for the
classiÔ¨Åcation of vessel stenosis.
On the other hand, a subset of the patterns used for the vessel stenosis problem is
visualized in Figure 16.11. The previously mentioned three continuous features (Sum
of vessel widths, Mean of the vessel width histogram, and Standard deviation of the
vessel width histogram) were computed over the training and testing sets of vessel
patterns.
In Table 16.4, the classiÔ¨Åcation values obtained from the naive Bayes classiÔ¨Åer using
the test set of 20 patterns are shown. From the stenosis test records, the value pre-
sented in Bayes-stenosis must be bigger than the value shown in the column of Bayes-no
stenosis, and the opposite for the No-stenosis records. It can be observed that the test
records of No-stenosis-2 and No-stenosis-3 are incorrectly classiÔ¨Åed (Status = no), which
decreases the accuracy of the classiÔ¨Åcation system.
Finally, in Table 16.5, the confusion matrix obtained from the classiÔ¨Åcation stage is
presented. The overall accuracy using the confusion matrix over the test set of vessel
patterns can be computed as (10 + 8)‚àï(10 + 10) = 0.9.
In general, a classiÔ¨Åcation task involves a high number of features; however, the per-
formance obtained with the three features based on the histogram of vessel width esti-
mation (90%) can be appropriate for systems that perform computer-aided diagnosis in
cardiology.

Automatic Detection of Coronary Artery Stenosis Using Bayesian ClassiÔ¨Åcation
385
Figure 16.9 First row: Subset of X-ray angiograms. Second row: Ground-truth images. The remaining
Ô¨Åve rows present the segmentation results of the methods of Kapur et al. [33], histogram concavity
[34], Pal and Pal [35], RATS [36], and Ridler and Calvard [32], respectively.

386
Hybrid Intelligence for Image Analysis and Understanding
Figure 16.10 First column: Subset of X-ray angiograms. Second column: Ground-truth images. Third
column: Segmentation result obtained from the proposed method. Last column: Product between
segmentation result and input angiogram.
Figure 16.11 First row: Subset of patterns of no-stenosis cases. Second column: Subset of vessel
stenosis patterns.
16.5
Concluding Remarks
In this chapter, a new method for automatic coronary stenosis in X-ray angiograms has
been presented. In the detection step, Gaussian-matched Ô¨Ålters tuned by diÔ¨Äerential
evolution have shown to be more eÔ¨Écient than Ô¨Åve state-of-the-art GMF-based meth-
ods, achieving an Az = 0.941 with a test set of 60 angiograms. In the second stage of

Automatic Detection of Coronary Artery Stenosis Using Bayesian ClassiÔ¨Åcation
387
Table 16.4 Results of naive Bayes classiÔ¨Åer over the test set of 20 records
Test record
Bayes-stenosis
Bayes-no stenosis
Status
Stenosis-1
0.0003521702
2.34985126541991E-05
yes
Stenosis-2
0.0001364722
2.3984168239561E-05
yes
Stenosis-3
0.0011275668
2.0114369609354E-05
yes
Stenosis-4
0.0011395404
1.92517874458132E-05
yes
Stenosis-5
0.0011190882
2.02945979302175E-05
yes
Stenosis-6
0.0001185143
1.20243160639164E-05
yes
Stenosis-7
0.0004880505
1.40674998399305E-05
yes
Stenosis-8
0.0011145618
1.93816551985246E-05
yes
Stenosis-9
0.0003115148
1.30993013377647E-05
yes
Stenosis-10
0.0003339428
1.30571489091502E-05
yes
No-stenosis-1
3.45096065896438E-13
1.56155952678713E-05
yes
No-stenosis-2
0.0001173525
1.15647687625231E-05
no
No-stenosis-3
6.89615692721782E-06
1.55312740490901E-06
no
No-stenosis-4
3.17501486637572E-10
1.92568264381307E-05
yes
No-stenosis-5
5.86365100656046E-06
2.11708957418583E-05
yes
No-stenosis-6
1.83238102570245E-07
2.29001505011661E-05
yes
No-stenosis-7
3.21239037388808E-08
0.000020533
yes
No-stenosis-8
5.0374318162336E-10
1.72396409222254E-05
yes
No-stenosis-9
7.17381447380215E-06
2.23627091900428E-05
yes
No-stenosis-10
2.40013627206018E-12
1.48854193021198E-05
yes
Table 16.5 Confusion matrix for the test set of 20 records
Class
Stenosis
No stenosis
Total
Stenosis
10
0
10
No stenosis
2
8
10
Total
12
8
20
vessel segmentation, an iterative thresholding strategy has shown to be the most eÔ¨Écient
compared with four other thresholding methods, obtaining a segmentation accuracy of
0.962 with the test set of 60 angiograms. Finally, in the last step of vessel stenosis classiÔ¨Å-
cation, the naive Bayes technique applied over a 3D feature vector obtained an accuracy
of 0.90 with a test set of 20 patterns. According to the experimental results, the proposed
method (consisting of the application of GMFs for enhancement, thresholding for vessel
segmentation, and naive Bayes classiÔ¨Åer) to detect coronary stenosis has proven to be
appropriate for computer-aided diagnosis in cardiology.

388
Hybrid Intelligence for Image Analysis and Understanding
Acknowledgment
This research has been supported by the National Council of Science and Technology
of M√©xico (C√°tedras-CONACYT No. 3150-3097).
References
1 Eiho, S. and Qian, Y. (1997) Detection of coronary artery tree using morphological
operator. Computers in Cardiology, 24, 525‚Äì528.
2 Qian, Y., Eiho, S., Sugimoto, N., and Fujita, M. (1998) Automatic extraction of coro-
nary artery tree on coronary angiograms by morphological operators. Computers in
Cardiology, 25, 765‚Äì768.
3 Maglaveras, N., Haris, K., Efstratiadis, S., Gourassas, J., and Louridas, G. (2001)
Artery skeleton extraction using topographic and connected component labeling.
Computers in Cardiology, 28, 17‚Äì20.
4 Sun, K. and N. Sang (2008) Morphological enhancement of vascular angiogram with
multiscale detected by Gabor Ô¨Ålters. Electronic letters, 44 (2).
5 Bouraoui, B., Ronse, C., Baruthio, J., Passat, N., and Germain, P. (2008) Fully auto-
matic 3D segmentation of coronary arteries based on mathematical morphology. 5th
IEEE International Symposium on Biomedical Imaging (ISBI): From Nano to Macro,
pp. 1059‚Äì1062.
6 Lara, D., Faria, A., Araujo, A., and Menotti, D. (2009) A semi-automatic method
for segmentation of the coronary artery tree from angiography. XXII Brazilian
Symposium on Computer Graphics and Image Processing (SIBGRAPI), pp. 194‚Äì201.
7 Lorenz, C., Carlsen, I., Buzug, T., Fassnacht, C., and Weese, J. (1997) A multi-scale
line Ô¨Ålter with automatic scale selection based on the Hessian matrix for medical
image segmentation. Proceedings of the International Conference on Scale-Space
Theories in Computer Vision, Springer LNCS, 1252, 152‚Äì163.
8 Frangi, A., Niessen, W., Vincken, K., and Viergever, M. (1998) Multiscale vessel
enhancement Ô¨Åltering. Medical Image Computing and Computer-Assisted Interven-
tion (MICCAI‚Äô98), Springer LNCS, 1496, 130‚Äì137.
9 Wink, O., Niessen, W., and Viergever, M. (2004) Multiscale vessel tracking. IEEE
Transactions on Medical Imaging, 23 (1), 130‚Äì133.
10 Salem, N. and Nandi, A. (2008) Unsupervised segmentation of retinal blood vessels
using a single parameter vesselness measure. Sixth Indian Conference on Computer
Vision, Graphics and Image Processing, IEEE, 34, 528‚Äì534.
11 Wang, S., Li, B., and Zhou, S. (2012) A segmentation method of coronary
angiograms based on multi-scale Ô¨Åltering and region-growing. International Con-
ference on Biomedical Engineering and Biotechnology, pp. 678‚Äì681.
12 Li, Y., Zhou, S., Wu, J., Ma, X., and Peng, K. (2012) A novel method of vessel seg-
mentation for X-ray coronary angiography images. Fourth International Conference
on Computational and Information Sciences (ICCIS), pp. 468‚Äì471.
13 Jin, J., Yang, L., Zhang, X., and Ding, M. (2013) Vascular tree segmentation in
medical images using Hessian-based multiscale Ô¨Åltering and level set method.
Computational and Mathematical Methods in Medicine, 2013 (502013), 9.

Automatic Detection of Coronary Artery Stenosis Using Bayesian ClassiÔ¨Åcation
389
14 Tsai, T., Lee, H., and Chen, M. (2013) Adaptive segmentation of vessels from coro-
nary angiograms using multi-scale Ô¨Åltering. International Conference on Signal-Image
Technology and Internet-Based Systems, pp. 143‚Äì147.
15 M‚Äôhiri, F., Duong, L., Desrosiers, C., and Cheriet, M. (2013) Vesselwalker: Coronary
arteries segmentation using random walks and Hessian-based vesselness Ô¨Ålter. IEEE
10th International Symposium on Biomedical Imaging (ISBI): From Nano to Macro,
pp. 918‚Äì921.
16 Chaudhuri, S., Chatterjee, S., Katz, N., Nelson, M., and Goldbaum, M. (1989) Detec-
tion of blood vessels in retinal images using two-dimensional matched Ô¨Ålters. IEEE
Transactions on Medical Imaging, 8 (3), 263‚Äì269.
17 Hoover, A., Kouznetsova, V., and Goldbaum, M. (2000) Locating blood vessels in
retinal images by piecewise threshold probing of a matched Ô¨Ålter response. IEEE
Transactions on Medical Imaging, 19 (3), 203‚Äì210.
18 Chanwimaluang, T. and Fan, G. (2003) An eÔ¨Écient blood vessel detection algo-
rithm for retinal images using local entropy thresholding. Proc. IEEE International
Symposium on Circuits and Systems, 5, 21‚Äì24.
19 Chanwimaluang, T., Fan, G., and Fransen, S. (2006) Hybrid retinal image registration.
IEEE Transactions on Information Technology in Biomedicine, 10 (1), 129‚Äì142.
20 Cinsdikici, M. and Aydin, D. (2009) Detection of blood vessels in ophthalmoscope
images using MF/ant (matched Ô¨Ålter/ant colony) algorithm. Computer methods and
programs in biomedicine, 96, 85‚Äì95.
21 Al-Rawi, M., Qutaishat, M., and Arrar, M. (2007) An improved matched Ô¨Ålter for
blood vessel detection of digital retinal images. Computers in Biology and Medicine,
37, 262‚Äì267.
22 Al-Rawi, M. and Karajeh, H. (2007) Genetic algorithm matched Ô¨Ålter optimization
for automated detection of blood vessels from digital retinal images. Computer
methods and programs in biomedicine, 87, 248‚Äì253.
23 Kang, W., Wang, K., Chen, W., and Kang, W. (2009) Segmentation method based on
fusion algorithm for coronary angiograms. 2nd International Congress on Image and
Signal Processing (CISP), pp. 1‚Äì4.
24 Kang, W., Kang, W., Chen, W., Liu, B., and Wu, W. (2010) Segmentation method of
degree-based transition region extraction for coronary angiograms. 2nd International
Conference on Advanced Computer Control, pp. 466‚Äì470.
25 Kang, W., Kang, W., Li, Y., and Wang, Q. (2013) The segmentation method of
degree-based fusion algorithm for coronary angiograms. 2nd International Con-
ference on Measurement, Information and Control, pp. 696‚Äì699.
26 Cruz-Aceves, I., Hernandez-Aguirre, A., and Valdez-Pena, I. (2015) Automatic coro-
nary artery segmentation based on matched Ô¨Ålters and estimation of distribution
algorithms. Proceedings of the 2015 International Conference on Image Processing,
Computer Vision, & Pattern Recognition (IPCV‚Äô2015), pp. 405‚Äì410.
27 Cruz-Aceves, I., Cervantes-Sanchez, F., Hernandez-Aguirre, A., Perez-Rodriguez, R.,
and Ochoa-Zezzatti, A. (2016) A novel Gaussian matched Ô¨Ålter based on entropy
minimization for automatic segmentation of coronary angiograms. Computers and
Electrical Engineering.
28 Cruz-Aceves, I., Hernandez-Aguirre, A., and Valdez, S.I. (2016) On the performance
of nature inspired algorithms for the automatic segmentation of coronary arteries
using Gaussian matched Ô¨Ålters. Applied Soft Computing, 46, 665‚Äì676.

390
Hybrid Intelligence for Image Analysis and Understanding
29 Storn, R. and Price, K. (1995) DiÔ¨Äerential evolution ‚Äì a simple and eÔ¨Écient adap-
tive scheme for global optimization over continuous spaces, Tech. Rep. TR-95-012,
International Computer Sciences Institute, Berkeley, CA.
30 Storn, R. and Price, K. (1997) DiÔ¨Äerential evolution ‚Äì a simple and eÔ¨Écient heuristic
for global optimization over continuous spaces. Journal of Global Optimization, 11,
341‚Äì359.
31 Tan, P., Steinbach, M., and Kumar, V. (2006) Introduction to data mining. Pearson
Education, Upper Saddle River, NJ, 227‚Äì246.
32 Ridler, T. and Calvard, S. (1978) Picture thresholding using an iterative selection
method. IEEE Transactions on Systems, Man, and Cybernetics, 8, 630‚Äì632.
33 Kapur, J., Sahoo, P., and Wong, A. (1985) A new method for gray-level picture
thresholding using the entropy of the histogram. Computer Vision, Graphics, and
Image Processing, 29, 273‚Äì285.
34 Rosenfeld, A. and De la Torre, P. (1983) Histogram concavity analysis as an aid
in threshold selection. IEEE Transactions on Systems, Man, and Cybernetics, 13,
231‚Äì235.
35 Pal, N.R. and Pal, S.K. (1989) Entropic thresholding. Signal Processing, 16, 97‚Äì108.
36 Kittler, J., Illingworth, J., and Foglein, J. (1985) Threshold selection based on a simple
image statistic. Computer Vision Graphics and Image Processing, 30, 125‚Äì147.

391
17
Evaluating the EÔ¨Écacy of Multi-resolution Texture Features for
Prediction of Breast Density Using Mammographic Images
Kriti1, Harleen Kaur1, and Jitendra Virmani2
1Electrical and Instrumentation Engineering Department, Thapar University, Patiala, Punjab, India
2CSIR-CSIO, Sector-30C, Chandigarh, India
17.1
Introduction
Breast cancer refers to an uncontrolled growth of breast cells. It can start from the cells
present in the lobules (glands that produce milk) or ducts (passages to carry milk). Some-
times, the tumors developed in the breast break away and enter the lymph system, from
where they can spread to tissues throughout the body. This process is called metastasis
[1, 2].
Breast cancer can be broadly categorized as: (1) carcinomas and (2) sarcomas. Carci-
noma starts in the cells of the lining of the organ or tissue. This lining is known as the
epithelial tissue. The carcinoma developed in the breast tissue is called adenocarcinoma,
and it starts in the glandular tissue [3, 4]. Sarcoma is the type of cancer that develops in
the non-epithelial tissue or connective tissue like muscles, fat, blood vessels, cartilage,
bone, and so on [3].
Other types of breast cancer are:
1. Ductal carcinoma in situ (DCIS): This is the initial stage of breast cancer. It is a
noninvasive type of cancer that starts in the inner linings of the milk ducts but cannot
spread to other parts of the breast through the duct walls. DCIS can be diagnosed
either during a physical examination if the doctor is able to feel any lumps in the
breast or during mammography as clusters of calciÔ¨Åcation inside the ducts [3, 5].
2. Invasive ductal carcinoma (IDC): IDC is the most common type of cancer. This
cancer develops in the inner sides of the milk ducts, breaks through the duct walls,
and invades the fatty breast tissue from where it can metastasize to other organs
and body parts. Some of the early symptoms of IDC include swelling of the breasts,
skin irritation, redness or scaliness of the skin, and a lump in the underarm. IDC can
be diagnosed by physical examination or diÔ¨Äerent imaging procedures like mam-
mography, ultrasound, breast magnetic resonance imaging (MRI), biopsy, and so
on [3, 6].
3. Invasive lobular cancer (ILC): This is the second most common form of breast cancer.
It starts developing inside the milk-producing lobules and then metastasizes beyond
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

392
Hybrid Intelligence for Image Analysis and Understanding
the lobules to the breast tissue and other body parts. ILC is much more diÔ¨Écult to
locate on a mammogram compared to IDC as the cancer may spread to surrounding
stroma. One of the early signs of ILC is hardening of the breast instead of a lump
formation. There can also be some change in the texture of the skin. ILC can also be
diagnosed during a physical examination or by using some imaging modalities [3, 7].
Although breast cancer has a high mortality rate, the chances of survival are improved
if malignancy is detected at an early stage. The best tool used for early detection is mam-
mography [8‚Äì16]. A mammogram is an X-ray of the women‚Äôs breast and is used to check
for early signs of breast cancer. There are two types of mammography:
A. Screening mammography
B. Diagnostic mammography.
The screening mammography is used on women who do not experience any symptoms
of breast cancer like skin irritation, swelling of a part of a breast, pain in the breast,
discharge other than milk, and so on. The diagnostic mammography is used when the
patient complains of pain in the breast or some lump formation, or in case an abnormal-
ity is detected at the time of screening mammography. The diagnostic mammography
enables the radiologist to make an accurate diagnosis and helps in determining the loca-
tion of the abnormality, its type (e.g., calciÔ¨Åcation, circumscribed masses, etc.), and the
severity of the abnormality.
On the mammograms, the adipose (fatty) tissue and the Ô¨Åbroglandular tissue are dis-
played along with the present abnormalities. To describe the Ô¨Åndings on the mammo-
grams, the American College of Radiology developed a standard system called the Breast
Imaging Reporting and Data System ( BI-RADS). The categories are described as [17]:
1. Category 0: Additional imaging evaluation. If any abnormality is present, it may not
be clearly noticeable, and more tests are needed.
2. Category 1: Negative. No abnormalities found to report.
3. Category 2: Benign Ô¨Ånding. The Ô¨Ånding in the mammogram is noncancerous, like
lymph nodes.
4. Category 3: Probably benign Ô¨Ånding. The Ô¨Ånding is most probably noncancerous but
is expected to change over time, so a follow-up is required regularly.
5. Category 4: Suspicious abnormality. The Ô¨Åndings might or might not be cancerous,
so to Ô¨Ånd the exact nature of the Ô¨Ånding, the patient should consider taking a biopsy
test.
6. Category 5: Highly suggestive malignancy. The Ô¨Ånding has more than 95% chance of
being cancerous, and biopsy examination is highly recommended for the patient.
7. Category 6: Known biopsy-proven malignancy. The Ô¨Åndings on the mammogram
have been shown to be cancerous by a previous biopsy.
BI-RADS breast density classiÔ¨Åcation is divided into four groups [17]:
1. BI-RADS I: Almost entirely fatty breasts. Breasts contain little Ô¨Åbrous and glandular
tissue.
2. BI-RADS II: Scattered regions of Ô¨Åbroglandular density exists. A few regions of
Ô¨Åbrous and glandular tissue are found in the breast.
3. BI-RADS III: Heterogeneously dense breasts. The regions of Ô¨Åbrous and glandular
tissue are prominent throughout the breast. Small masses are very diÔ¨Écult to observe
in mammogram.

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
393
4. BI-RADS IV: Extremely dense breasts. The whole breast consists of Ô¨Åbrous and glan-
dular tissue. In this situation, it is very hard to Ô¨Ånd the cancerous region present
because the appearance of normal breast tissue and the cancerous tissues tends to
overlap.
For a standard mammography examination, the following views are considered:
mediolateral oblique view (MLO) and craniocaudal view (CC). The CC view is the
top view of the breast in which the entire breast is pictured. The MLO view is an
angled or oblique view of the breast. This view allows the largest amount of breast
tissue to be visualized. The MLO view is considered to be an important projection
as it picturizes most of the breast tissue. Pectoral muscle representation on the
MLO view is an important part to evaluate the position of the patient and the Ô¨Ålm
adequacy. The amount of pectoral muscle visible on the image helps in deciding the
amount of breast tissue incorporated into the image; this brings out the decisive
quality factor that is essential to decrease the number of false negatives and increase
the sensitivity of the mammography. Also, as most of the breast pathology happens
in the upper external quadrant, that area is very easily visible on MLO views [18].
High breast tissue density is considered to be a prominent indicator of development
of breast cancer [19‚Äì32]. Radiologists estimate the changes in density patterns by
visual analysis of mammographic images. Since this visual analysis is dependent only
upon the experience of the radiologist and can be ambiguous, there has been a lot of
interest among researchers to implement the CAD systems for classifying the breast
tissue as per its density. The CAD system design for breast density classiÔ¨Åcation
suggested by various researchers is shown in Figure 17.1. These CAD systems are
considered clinically signiÔ¨Åcant, keeping in view the atypical cases of various density
CAD System for Breast Density Classification
Benchmark
Data
Benchmark
Data
Collected by
Research Group
Collected by
Research Group
INbreast
Fatty / Dense glandular
Fatty / Fatty-glandular /
Dense-glandular
Fatty / Fatty-glandular
/ Dense-glandular /
Extremely dense
MIAS
DDSM
Using Digitized Film-Screen Mammograms
Using Full Field Digital Mammograms
Figure 17.1 CAD system for breast density classiÔ¨Åcation. Note: Shaded blocks indicate the steps
involved in present work.

394
Hybrid Intelligence for Image Analysis and Understanding
patterns exhibited on mammograms. Many researchers have developed diÔ¨Äerent CAD
systems that classify breast tissue as per its density into two classes [fatty (F) and
dense (D)] [33‚Äì50], three classes [fatty (F), fatty-glandular (FG), and dense-glandular
(DG)] [50‚Äì58], or four classes [fatty (F), fatty-glandular (FG), dense-glandular (DG),
and extremely dense] [40, 52, 57]. The breast density classiÔ¨Åcation on the MIAS
database, carried out by many researchers, is shown in Figure 17.2. Brief summaries
of the studies carried out on the MIAS database for two-class, four-class, and
three-class classiÔ¨Åcations of the breast tissue are depicted in Tables 17.1, 17.2, and 17.3,
respectively.
For four-class classiÔ¨Åcation of the MIAS database, a diÔ¨Äerent radiologists suggest a
diÔ¨Äerent number of images for diÔ¨Äerent BI-RADS (I to IV) categories. In [52], out of
322 images, 128 images were categorized as BI-RADS I, 80 images as BI-RADS II, 70
images as BI-RADS III, and 44 images as BI-RADS IV. In the study in [40], three experts
classiÔ¨Åed the data: Expert A classiÔ¨Åed 129 images for BI-RADS I, 78 for BI-RADS II, 70
for BI-RADS III, and 44 for BI-RADS IV. Expert B classiÔ¨Åed 86 images for BI-RADS I,
112 images for BI-RADS II, 81 images for BI-RADS III, and 43 images for BI-RADS IV.
Expert C classiÔ¨Åed 59 for BI-RADS I, 86 for BI-RADS II, 143 for BI-RADS III, and 34 for
BI-RADS IV density categories. In [57], images that fall under the category of fatty (106)
were considered to belong to the BI-RADS I category, fatty-glandular (104) as BI-RADS
II, while dense (112) was divided into BI-RADS III (95) and BI-RADS IV (37) density
categories.
All the related studies [33‚Äì60] for classiÔ¨Åcation of breast tissue have been carried out
either on segmented breast tissue (SBT) or on some selected region of interest (ROIs).
MIAS Database Breast Density Classification
Fatty
Classification as per
BI-RADS standard
evaluated by radiologist
(106)
(104)
(112)
3-class classification
Fatty -
glandular
Dense -
glandular
Fatty
Fatty (106)
Dense (216)
BI-RADS I
Fatty
Fatty -
glandular
Dense -
glandular
Extremely
dense
BI-RADS II
BI-RADS III
BI-RADS IV
(106)
(104)
(112)
2-class classification
4-class classification
Fatty -
glandular
Dense -
glandular
Figure 17.2 MIAS database breast density classiÔ¨Åcation.

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
395
Table 17.1 Description of studies carried out for classiÔ¨Åcation of tissue density as fatty or
dense on the MIAS database
Investigators
Description
Images
SBT/ROI
ClassiÔ¨Åer
Acc. (%)
Oliver et al. (2005) [38]
322
SBT
Bayesian
91.0
Mustra et al. (2012) [57]
322
512 √ó 384
Na√Øve Bayesian
91.6
Sharma et al. (2014) [45]
322
200 √ó 200
SMO-SVM
96.4
Kriti et al. (2015) [49]
322
200 √ó 200
kNN
95.6
Sharma et al. (2015) [46]
212
200 √ó 200
kNN
97.2
Abdul-Nasser et al. (2015) [59]
322
100 √ó 100
SVM
99.0
Kriti et al. (2016) [47]
322
200 √ó 200
SVM
94.4
Virmani et al. (2016) [48]
322
200 √ó 200
kNN
96.2
Kriti et al. (2016) [50]
322
200 √ó 200
SSVM
94.4
Note: SBT: segmented breast tissue; Acc.: accuracy; SSVM: smooth support vector machine;
SVM: support vector machine; SMO: sequential minimal optimization; kNN: k-nearest
neighbor classiÔ¨Åer.
Table 17.2 Description of studies carried out for classiÔ¨Åcation of tissue density as fatty,
fatty-glandular, dense-glandular, or extremely dense on the MIAS database
Investigators
Description
Images
SBT/ROI
ClassiÔ¨Åer
Acc. (%)
Bosch et al. (2006) [52]
322
SBT
SVM
95.4
Oliver et al. (2008) [40]
322
SBT
Bayesian
86.0
Mustra et al. (2012) [57]
322
512 √ó 384
IB1
79.2
Note: SBT: segmented breast tissue; Acc.: accuracy.
The extraction of SBT involves various preprocessing steps, including removal of
pectoral muscle and the background. These preprocessing steps can be eliminated
by extracting ROIs from the center of mammograms. Exhaustive experimentation
carried out in recent studies has indicated that the center region of the breast contains
maximum density information [45, 46, 61]. Out of the above studies, some of the studies
have been carried out on data sets of mammographic images collected by the individual
researchers, while others have been carried out on standard benchmark databases like
MIAS and DDSM. It is worth noting that the study in [48] reports two-class breast
density classiÔ¨Åcation using wavelet texture descriptors with a maximum accuracy of
96.2 % with a k-nearest neighbor kNN) classiÔ¨Åer and features derived using a db1 (Haar)
wavelet Ô¨Ålter. For developing an eÔ¨Écient CAD system, it is necessary that the database
is diversiÔ¨Åed and should contain all possible variations of density patterns belonging to
each class. The patterns within each class can be categorized as typical or atypical. The
sample images of typical cases present in the MIAS database are shown in Figure 17.3.

396
Hybrid Intelligence for Image Analysis and Understanding
Table 17.3 Description of studies carried out for classiÔ¨Åcation of tissue density as fatty,
fatty-glandular, or dense-glandular on the MIAS database
Investigators
Description
Images
SBT/ROI
ClassiÔ¨Åer
Acc. (%)
Blot et al. (2001) [51]
265
SBT
kNN
63.0
Bosch et al. (2006) [52]
322
SBT
SVM
91.3
Muhimmah et al. (2006) [53]
321
SBT
DAG-SVM
77.5
Subashni et al. (2010) [54]
43
SBT
SVM
95.4
Tzikopolous et al. (2011) [55]
322
SBT
SVM
84.1
Li (2012) [56]
42
SBT
KSFD
94.4
Mustra et al. (2012) [57]
322
512 √ó 384
IB1
82.0
Silva et al. (2012) [58]
320
300 √ó 300
SVM
77.1
Abdul-Nasser et al. (2012) [59]
322
100 √ó 100
SVM
85.5
Kriti et al. (2016) [50]
322
200 √ó 200
SVM
86.3
Kriti et al. (2016) [60]
322
200 √ó 200
SVM
87.5
Note: SBT: segmented breast tissue; Acc.: accuracy; DAG: directed acyclic graph; kNN: k-nearest neighbor;
KSFD:kernel self-optimized Ô¨Åsher discriminant.
(a)
(b)
(c)
Figure 17.3 Sample mammograms showing typical cases of breast tissue density: (a) typical F tissue
mdb078, (b) typical FG tissue mdb210, and (c) typical DG tissue mdb126.
The MIAS database also contains some atypical cases in which identiÔ¨Åcation of density
patterns is diÔ¨Écult. The sample images of these atypical cases are shown in Figure 17.4.
It is believed that the texture of the mammographic image gives signiÔ¨Åcant informa-
tion about the changes in breast tissue density patterns. In the various studies reported
above, texture information from the SBT or the ROIs is extracted using statistical and
signal processing‚Äìbased methods carried out on a single scale. However, feature extrac-
tion can also be done in the transform domain over various scales by using various
multiresolution schemes like wavelets. It is logical to compute texture features in the

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
397
(a)
(b)
(c)
Figure 17.4 Sample mammograms showing atypical cases of breast tissue density: (a) atypical F tissue
mdb095, (b) atypical FG tissue mdb029, and (c) atypical DG tissue mdb201.
transform domain as the scale over which feature extraction is carried out is an impor-
tant characteristic because the visual system of humans is adapted to process the images
in a multiscale way [62, 63].
17.1.1
Comparison of Related Methods with the Proposed Method
The work done in the present study can be directly compared with the studies in [50, 57,
58, 60], as these have been carried out on a Ô¨Åxed-size ROI for classiÔ¨Åcation of breast
tissue into three classes as per its density. The study proposed by [57] extracted the
ROIs of size 512 √ó 384 pixels. Texture features were extracted from GLCM along with
statistical features and histogram features derived from each ROI. A total of 419 fea-
tures was extracted. For the three density categories, an accuracy of 82.0% was achieved
using kNN as classiÔ¨Åer with a value of k equal to 1. In the method proposed by [58], the
authors extracted the ROIs of size 300 √ó 300 pixels. The method combined the statistical
features extracted from the image histogram with those derived from the co-occurrence
matrix. The ten-fold cross-validation scheme was used with SVM classiÔ¨Åer, and an accu-
racy of 77.18% was achieved. The method proposed in [59] extracted the ROIs of size
300 √ó 300 pixels from each image. The ULDP (uniform local directional pattern) was
used as a texture descriptor for breast density classiÔ¨Åcation. The SVM classiÔ¨Åer used
achieved an accuracy of 85.5% for the three-class classiÔ¨Åcation. In the study in [50], sta-
tistical features were extracted from each ROI of size 200 √ó 200 pixels. The accuracy of
86.3% was achieved by using only the Ô¨Årst four PCs with SVM classiÔ¨Åer. In the study
proposed by [60], the authors extracted ROIs of size 200 √ó 200 pixels from the center of
the breast. In the feature extraction module, Laws‚Äô mask analysis was used to compute
texture features from each ROI image. The principal components analysis (PCA)-SVM
classiÔ¨Åer was used for the classiÔ¨Åcation task, and an accuracy of 87.5% was achieved for
three-class breast density classiÔ¨Åcation.
In the present work, the potential of diÔ¨Äerent multiresolution feature descriptor vec-
tors ( FDVs) consisting of normalized energy values computed from diÔ¨Äerent subimages
has been evaluated using SVM and smooth SVM (SSVM) classiÔ¨Åers.

398
Hybrid Intelligence for Image Analysis and Understanding
17.2
Materials and Methods
17.2.1
Description of Database
In the present work, the algorithms for texture analysis have been tested on the mam-
mograms taken from the mini-MIAS database. The mini-MIAS database consists of
322 Ô¨Ålm-screen mammograms (digitized database). These images were obtained from
161 women in MLO view only. The original MIAS database is digitized at 50ùúápixel
edge. The mini-MIAS databse was generated by down-sampling the original database
to 200ùúápixel and clipping or padding it to a Ô¨Åxed size of 1024 √ó 1024 pixels. Out of 322
mammographic images, 106 images belong to the fatty class, 104 images belong to the
fatty-glandular class, and 112 images belong to the dense-glandular class. For further
details of the database, refer to [64]. The description of the MIAS database is shown in
Figure 17.5.
17.2.2
ROI Extraction Protocol
After performing various experiments, it has been asserted that the center of the breast
contains maximum density information (i.e., in the region where glandular ducts are
prominent) [45, 46, 61]; therefore, Ô¨Åxed-size ROIs (200 √ó 200 pixels) are extracted from
the central location as represented in Figure 17.6.
A few sample ROIs are shown in Figure 17.7.
17.2.3
WorkÔ¨Çow for CAD System Design
CAD systems involve computerized analysis of mammograms, which is then used by the
radiologists to validate their diagnosis as these systems tend to detect any lesions that
might be missed during subjective analysis [65‚Äì80]. The workÔ¨Çow for the prediction of
breast density is shown in Figure 17.8.
Mammographic Images of the MIAS
Database (322)
Fatty
Mammographic
Images (106)
Fatty-Glandular
Mammographic
Images (104)
Fatty-Glandular
ROIs (104)
Fatty ROIs (106)
Dense-glandular
ROIs (112)
Total ROIs
(322)
Dense-Glandular
Mammographic
Images (112)
Figure 17.5 Database description.

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
399
Pectoral Muscle
Original Image
Background
Extracted ROI
Figure 17.6 ROI extraction protocol mdb030.
(a)
(b)
(c)
(d)
(e)
(f)
Figure 17.7 Sample ROIs: (a) typical F ROI mdb078; (b) typical FG ROI mdb210; (c) typical DG ROI
mdb126; (d) atypical F ROI mdb095; (e) atypical FG ROI mdb029; and (f) atypical DG ROI mdb201.

400
Hybrid Intelligence for Image Analysis and Understanding
Image Database
ROI Extraction
Feature Extraction Module
FDV1
Fatty
Fatty-
glandular
Dense-
glandular
SVM Classifier
Decision of CAD system
SSVM Classifier
FDV7
set
Classification Module
2D-DWT applied to ROIs to obtain 7
subimages (one approximate and six
orientation selective detailed subimages)
Normalized energy from each subimage
calculated and used to form feature vector
Figure 17.8 Block diagram: workÔ¨Çow for prediction of breast density.
The CAD system design consists of two modules: (a) feature extraction and (b)
classiÔ¨Åcation.
17.2.3.1
Feature Extraction
In feature extraction, the texture information present in the image is transformed into
numerical values that can be further used in the machine learning algorithms for texture
analysis. For the analysis of texture, diÔ¨Äerent feature extraction techniques can be used.
These methods are described in Figure 17.9.
The feature extraction can be done on either a single scale or multiple scales. On
a single scale, the features are extracted after the spatial interactions between neigh-
borhood pixels are considered for analysis. The examples of such methods are GLCM,
GLRLM, NGTDM, SFM, and GLDS. To extract features on various scales, the trans-
form domain methods based on wavelet, Gabor, curvelet, NSCT, NSST, and ridgelet are
used. The scale over which feature extraction is carried out is an important character-
istic as the visual system of humans is adapted to process the images in a multiscale
way [62, 63, 81]. In the transform domain method of feature extraction, signals are con-
verted from the time domain to another domain so that the characteristic information
present in it within the time series that is not observable can be easily extracted. For the
present work, wavelet-based texture descriptors (computed from ten diÔ¨Äerent compact
support wavelet Ô¨Ålters) have been computed from each ROI for classifying the breast
tissue according to the density information using discrete wavelet transform (DWT).
17.2.3.1.1
Evolution of Wavelet Transform
Fourier transform: Fourier transform is the most widely used signal-processing opera-
tion used to convert any signal from time domain to its frequency domain. In Fourier
analysis, the signal is represented as a series of sine and cosine waves of varying ampli-
tudes and frequencies. Fourier transform can only provide 2D information about the

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
401
Feature Extraction Methods
Statistical Methods
First order statistics
Second order statistics
GLCM features
Higher order statistics
GLRLM features
Edge features
NGTDM features
SFM features
Gray level difference
statistics
Laws‚Äô texture
energy measures
FPS features
STFT
2D-DWT
2D-WPT
Wavelet based
features
Gabor features
NSCT features
NSST features
Curvelet features
Ridgelet features
Signal Processing Based
Methods
Transform Domain
Methods
Figure 17.9 DiÔ¨Äerent feature extraction techniques used in texture analysis. Note: GLCM: gray-level
co-occurence matrix; GLRLM: gray-level run length matrix; NGTDM: neighborhood gray tone
diÔ¨Äerence matrix; SFM: statistical feature matrix; FPS: Fourier power spectrum; STFT: short-time Fourier
transform; 2D-DWT: two-dimensional discrete wavelet transform; WPT: wavelet packet transform;
NSCT: non-subsampled countourlet transform; NSST: non-subsampled shearlet transform.
signal (i.e., diÔ¨Äerent frequency components present in it and their respective ampli-
tudes); it gives no information about the time at which diÔ¨Äerent frequency components
exist.
Short-Time Fourier Transform (STFT): In STFT, the signal is broken up into small
parts of Ô¨Åxed-size lengths, then Fourier transform is taken for each piece of signal.
The Fourier transform of each piece of signal provides the spectral information of each
time slice. Although STFT provides the time and frequency information simultaneously,
there exists the dilemma of resolution. As the window size is Ô¨Åxed, there is poor time
resolution for wide windows and poor frequency resolution for narrow windows.
The drawback of Fourier transform can be overcome by using the wavelet transform,
which gives 3D information about a signal i.e., diÔ¨Äerent frequency components
present in a signal, their respective amplitudes, and the time at which these frequency
components exist. In wavelet analysis, the signal is represented as a scaled and
translated version of the mother wavelet [82]. According to Heisenberg‚Äôs uncertainty
principle, there can be either high-frequency resolution and poor time resolution
or poor-frequency resolution and good temporal resolution. The wavelet analysis
solves the resolution problem occurring in STFT. In DWT, the basis function varies
both in frequency range and in spatial range. The wavelet transform is designed in
such a way that there is good frequency resolution for low-frequency components
and good time resolution for high-frequency components. DWT is considered to be
better than Fourier transform and STFT because in most of the signals occurring in
nature, the low-frequency content is present for longer duration and high-frequency
content occurs for short duration. On comparing with time, frequency, and Gabor
wavelet-based analysis, it is observed that wavelet analysis uses a time-scale region
rather than a time-frequency region to describe the signal, as shown in Figure 17.10.

402
Hybrid Intelligence for Image Analysis and Understanding
Amplitude
Frequency
Scale
Frequency
Time
Time
Amplitude
Time
Time Domain (Shannon)
(Infinite time resolution and zero
frequency resolution)
STFT (Gabor)
(Finite time and frequency
resolution)
Wavelet Analysis
(Good frequency resolution for low-
frequency components and high time
resolution for high-frequency
components)
Fourier Domain (Fourier)
(Infinite frequency resolution and zero
time resolution)
Figure 17.10 Time-domain, frequency-domain, STFT, and wavelet analysis of a signal.
Wavelet transform: The wavelets are waveforms of limited duration that have an aver-
age value of zero. These wavelets are basically little waves that are concentrated in time
and frequency around a certain point. DWT represents a windowing technique with
variable-sized regions. The whole idea of wavelets is to capture the incremental infor-
mation. Piecewise constant approximation inherently brings the idea of representing
the image at a certain resolution. Consider an image consisting of a number of shells in
which each shell represents the resolution level. The outermost shell and the innermost
shell represent the maximum and minimum resolution levels, respectively. The job of the
wavelet is to take out a particular shell. The wavelet translates at maximum resolution
and will take out the outermost shell. For next level of resolution, it will take out the next
shell, and so on. By reducing the resolution, the wavelet is just peeling oÔ¨Äshell by shell
using diÔ¨Äerent dilates and translates. DiÔ¨Äerent dilates correspond to diÔ¨Äerent resolution,

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
403
and diÔ¨Äerent translates track the given resolution level. The process of wavelet analysis
of an image is shown in Figure 17.11. A 2D DWT when applied to images can be seen as
two 1D transform functions applied to rows and columns of the image separately [83],
as shown in Figure 17.12. The 2D wavelet decomposition up to the Ô¨Årst level applied
to a mammographic image is shown in Figure 17.13. When this operation is applied
to an ROI, image it is passed through a set of complementary Ô¨Ålters, and at the out-
put one approximate and one detailed sub-image are produced (i.e., the input image
S1
S2
S3
Minimum Resolution Level
(Innermost Shell)
Maximum Resolution Level
(Outermost Shell)
Figure 17.11 Process of wavelet analysis of an image.
LPF on
Columns
Horizontal
Approximation
Original
Image
HPF on
Columns
Vertical
Detail (LH)
Horizontal
detail (HL)
Diagonal
detail (HH)
HPF on
Columns
Horizontal
Detail
HPF on
Rows
LPF on
Columns
Approximate
Image (LL)
Low blurred version of
original image
Vertically aligned
features are
prominent
Horizontally aligned
features are
prominent
Diagonally aligned
features are
prominent
LPF on
Rows
Figure 17.12 Wavelet transform of sample Lena image.

404
Hybrid Intelligence for Image Analysis and Understanding
LPF on
Rows
HPF on
Rows
HPF on
Columns
Original Image
Horizontal
Approximation
Horizontal
Detail
LPF on
Columns
Approximate
Image (LL)
Vertical
Detail (LH)
Horizontal
Detail (HL)
Diagonal
Detail (HH)
HPF on
Columns
LPF on
Columns
Figure 17.13 Wavelet transform of image F mdb132.
is decomposed into two subimages). There can be several levels up to which an image
can be decomposed. The wavelet transform at the next level is applied to the approxi-
mate sub-image so that it further produces two sub-images [84]. The process of wavelet
decomposition up to the second level is shown in Figure 17.14.
Original Image
L1
H1
HL1
HH1
LH1
LL1
LLL1
HLL1
HL1
HH1
LH1
LH1
HH1
HL1
HL2
LL2
LH2
HH2
Figure 17.14 Wavelet decomposition of an image up to the second level.

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
405
D1
(v)
D1
(d)
D1
(h)
D2
(h)
A2
D2
(d)
D2
(v)
(a)
(b)
Figure 17.15 (a) 2D wavelet decomposition of image up to the second level. (b) 2D wavelet
decomposition of sample image mdb132 using a Haar wavelet Ô¨Ålter up to the second level.
In the present work, the decomposition is done up to the second level, and one approx-
imate sub-image Aj and six orientation-selective detailed sub-images D(k)
j ,k=h,v,d are
generated. This wavelet representation of an image is depicted in Figure 17.15.
Selection of wavelet Ô¨Ålters: The choice of wavelet Ô¨Ålter used for feature extraction is
based on some properties that are signiÔ¨Åcant for texture description [81, 85‚Äì87]. The
properties that are considered for selecting an appropriate wavelet Ô¨Ålter include: orthog-
onality or biorthogonality, support width, shift invariance, and symmetry. The proper-
ties of diÔ¨Äerent wavelet Ô¨Ålters used are summarized in Table 17.4. From the Table 17.4,
it can be observed that the Haar Ô¨Ålter is the only Ô¨Ålter having the properties of orthog-
onality, symmetry, and compact support. Wavelet Ô¨Ålters that provide compact support
are desirable due to their ease of implementation. Compact support Ô¨Ålter function has
zero value outside the compact speciÔ¨Åed boundaries. For energy conservation at each
level of decomposition, orthogonality is a desirable property. To avoid any dephasing
while processing images, symmetry is required. For further details on the properties
of wavelet Ô¨Ålters, readers are directed to [81, 87]. In the present work, a Haar wavelet
Ô¨Ålter is used. A Haar wavelet is the Ô¨Årst and simplest wavelet, and it was proposed in
1909 by Alfred Haar. The Haar wavelet is discontinuous and resembles a step function.
There are two functions that play an important role in wavelet analysis:scaling function ùúô
Table 17.4 Properties of wavelet Ô¨Ålters used
Wavelet Ô¨Ålter Biorthogonal Orthogonal Symmetry Asymmetry Near symmetry Compact support
Db
√ó
‚úì
√ó
‚úì
√ó
‚úì
Haar
√ó
‚úì
‚úì
√ó
√ó
‚úì
Bior
‚úì
√ó
‚úì
√ó
√ó
‚úì
Coif
√ó
‚úì
√ó
√ó
‚úì
‚úì
Sym
√ó
‚úì
√ó
√ó
‚úì
‚úì

406
Hybrid Intelligence for Image Analysis and Understanding
(father wavelet) and wavelet function ùúì(mother wavelet). The Haar wavelet function is
deÔ¨Åned as:
ùúì(x) =
‚éß
‚é™
‚é®
‚é™‚é©
1,
0 ‚â§x < 0.5
‚àí1, 0.5 ‚â§x < 1
0,
otherwise
‚é´
‚é™
‚é¨
‚é™‚é≠
(17.1)
The Haar scaling function is deÔ¨Åned as ùúì(x) = ùúô(2x) ‚àíùúô(2x ‚àí1):
Œ¶(x) =
{
1, if 0 ‚â§x < 1
0, otherwise
}
(17.2)
The Haar wavelet function and scaling function are shown in Figure 17.16.
Selection of texture features: In the transform domain, while using multiresolution
methods, features like energy, mean, and standard deviation are frequently extracted
from the sub-images to be used in the classiÔ¨Åcation task [85‚Äì89]. In the present work,
after the generation of approximate and detailed sub-images, normalized energy of each
sub-image is calculated. A description of the FDVs used in the present work for the
classiÔ¨Åcation task is shown in Table 17.5.
t=0.5
t=1
t = 0
t = 0
t = 1
1
(a)
(b)
Figure 17.16 (a) Haar wavelet function ùúì(t) and (b) Haar scaling function ùúô(x).
Table 17.5 Description of FDVs
FDV
Wavelet energy signatures
FDV1
(NEA
2 , NEh
2, NEùë£
2, NEd
2, NEh
1, NEùë£
1, NEd
1)
FDV2
(NEh
1, NEùë£
1, NEd
1)
FDV3
(NEh
2, NEùë£
2, NEd
2, NEh
1, NEùë£
1, NEd
1)
FDV4
(NEh
1, NEùë£
1, NEd
1, NEd
2)
FDV5
(NEA
2 , NEh
1, NEùë£
1, NEd
1)
FDV6
(NEA
2 , NEh
2, NEùë£
2, NEd
2)
FDV7
(NEh
2, NEùë£
2, NEd
2)
Note: FDV: feature descriptor vector; NE: normalized
energy; A: approximate subimage; h: horizontal subimage;
v: vertical subimage; d: diagonal subimage.

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
407
The normalized energy is calculated as:
NEx
y =
‚à•subimage‚à•2
F
area(subimage)
(17.3)
where x: subimage; y: level of decomposition; and .F: Frobenius norm.
The Frobenius norm is also known as the Euclidean norm. It is the norm of matrix X
of dimension m √ó n deÔ¨Åned as the square root of the sum of the absolute squares of its
elements.
‚à•X‚à•F =
‚àö
‚àö
‚àö
‚àö
m
‚àë
i=1
n
‚àë
j=1
|xij|2
(17.4)
The Frobenius norm is independent of the manner in which the elements are placed
within the matrix.
(
e.g., if A1 =
[
3 4
0 0
]
and A2 =
[
3 0
0 4
])
. The Frobenius norm of both
the matrixes comes out as:
‚àö
(3)2 + (4)2 + (0)2 + (0)2 = 5.
NEx
y represents the normalized energy of a sub-image x at the y level of decomposition
(e.g., NEA
2 ) represents the normalized energy of an approximate sub-image at the second
level of decomposition.
17.2.3.2
ClassiÔ¨Åcation
In the classiÔ¨Åcation module, the classiÔ¨Åers like SVM and SSVM are used to classify the
mammographic images into F, FG, or DG classes based on the training instances. The
SVM classiÔ¨Åer is very popular and has been suggested by many researchers to provide
good accuracy for mammographic abnormality classiÔ¨Åcations [90‚Äì95] The features were
normalized between 0 and 1 before feeding them to the classiÔ¨Åer.
SVM classiÔ¨Åer: The SVM is a supervised machine learning algorithm used for both
classiÔ¨Åcation as well as regression problems. For linearly separable data, the role of
SVM is to Ô¨Ånd the hyperplane that classiÔ¨Åes all training vectors in two classes. Multiple
solutions exist for linearly separable data. The goal of SVM is to arrive at that opti-
mal hyperplane that segments the data in such a way that there is the widest margin
between the hyperplane and the observations. The margin is required to be maximum
in order to reduce test error, keep the training error low, and minimize the VC dimen-
sion. The samples that touch the decision boundary are called support vectors. In the
SVM algorithm, the instances are plotted in an n-dimensional space, where n is length
of the feature vector. The working of the SVM algorithm for the two-class problem is
shown in Figure 17.17. Consider the two-class problem and let g(‚Éóx) = ‚Éóùë§T‚Éóx + ùë§0:
g(‚Éóx) ‚â•1 ‚àÄ‚Éóx ùúñclass1
(17.5)
g(‚Éóx) ‚â§‚àí1 ‚àÄ‚Éóx ùúñclass2
(17.6)
For support vectors, the inequality becomes an equality.
Each training example‚Äôs distance from the hyperplane is:
‚Éóz = |g(‚Éóx)|
‚à•‚Éóùë§‚à•
(17.7)

408
Hybrid Intelligence for Image Analysis and Understanding
x2
x1
support vectors
wx+b =‚Äì1
wx+b= 0
(optimal hyperplane)
wx+b = 1
Margin(œÅ)
Figure 17.17 SVM classiÔ¨Åer for linearly separable data.
The total margin is computed by:
1
‚à•‚Éóùë§‚à•
+
1
‚à•‚Éóùë§‚à•
=
2
‚à•‚Éóùë§‚à•
(17.8)
So, the margin is given as:
ùúå=
2
‚à•‚Éóùë§‚à•
(17.9)
Minimizing the ‚Éóùë§will maximize the separability. Minimizing the ‚Éóùë§is a nonlinear opti-
mization task, solved by Karush-Kuhn Tucker (KKT) conditions, using a Lagrange mul-
tiplier ùúÜi
‚Éóùë§=
N
‚àë
i=0
= ùúÜiyi ‚Éóxi
(17.10)
N
‚àë
i=0
ùúÜiyi = 0
(17.11)
Figure 17.18 shows an example to illustrate the workings of the SVM algorithm. In this
example, x1 and x2 are two features, and only three values are taken to classify the
two-class problem. Point (1,1) and point (2,0) belong to class 1 such that g(1,1) and g(2,0)
are equal to ‚àí1, and the point (2,3) belongs to class 2 such that g(2,3) is equal to 1. From
the Ô¨Ågure, the weight vector ‚Éóùë§=(2,3)‚àí(1,1)=(a,2a), where a is any constant value.
For x=(1,1):
a + 2a + ùë§0 = ‚àí1
(17.12)
For x=(2,3):
2a + 6a + ùë§0 = 1
(17.13)

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
409
Figure 17.18 Example to illustrate the SVM
algorithm.
(2,3)
(2,0)
√ó1
√ó2
(1,1)
Weight vector
w=(2,3)‚Äì(1,1)
After solving the equations, a = 2
5 and ùë§0 = ‚àí11
5 . The value of the weight vector comes
out as: ‚Éóùë§= ( 2
5, 4
5); these are support vectors as they compose the weight value. The value
of g(x) after submission is found to be:
g(‚Éóx) = x1 + 2x2 ‚àí5.5
(17.14)
This is the equation of the hyperplane that classiÔ¨Åes the elements.
In some cases, the clear demarcation cannot be made between the instances of
two classes; then the algorithm applies kernel functions to map the nonlinear data
points from low-dimensional input space to a higher dimensional feature space. The
hyperplane is a linear function of vectors that are drawn from feature space instead of
original input space. For nonlinearly separable data, each input vector from input space
is mapped into higher dimensional space via some transformation ùúô‚à∂x ‚àí‚àí‚àí‚àí‚Üíùúë(x).
The value of kernel function provides inner or dot product of vectors in feature space.
K(xi, xj) = ùúë(xi)Tùúë(xj)
(17.15)
The basic role of the kernel is to map the nonlinearly separable data space into better rep-
resentation space where data can easily be separated. DiÔ¨Äerent kernels used in the SVM
algorithm are linear, sigmoid, polynomial, and Gaussian radial basis function (GRBF).
The linear kernel is represented as:
K(xi, xj) = xT
i xj
(17.16)
Equation of a polynomial kernel is:
K(xi, xj) = (1 + xT
i xj)P
(17.17)
where p is power of polynomial. Equation of Gaussian radial basis function kernel is:
K(xi, xj) = exp
(
‚àí
‚à•xi ‚àíxj‚à•2
2ùúé2
)
(17.18)

410
Hybrid Intelligence for Image Analysis and Understanding
Input Space
Feature Space
Figure 17.19 SVM for nonlinearly separable data.
The sigmoid kernel is expressed as:
K(xi, xj) = tanh(ùõΩ0xT
i xj + ùõΩ1)
(17.19)
An example of SVM classiÔ¨Åer for nonlinearly separable data is shown in Figure 17.19. In
the present work, a multiclass SVM classiÔ¨Åer is implemented using the LibSVM library
[96], and a grid search procedure is used for obtaining the optimum values of the clas-
siÔ¨Åer parameters [97‚Äì102].
SSVM ClassiÔ¨Åer: SSVMs can be used to classify highly nonlinear data space, for
example checkboard. For large problems, SSVM is found to be faster than SVM. For
multiclass implementation of SSVM classiÔ¨Åer, the SSVM toolbox has been used [103].
Some important mathematical properties like strong convexity and inÔ¨Ånitely often
diÔ¨Äerentiability make it diÔ¨Äerent from SVM. For further information, the readers are
directed to [104‚Äì106]. In the case of SSVM also, a grid search procedure is carried out
to obtain the optimum values for classiÔ¨Åer parameters.
17.3
Results
The design of the proposed CAD system for the prediction of breast density is shown in
Figure 17.20.
In the present work, rigorous experimentation was performed to obtain the highest
accuracy for the proposed CAD system design based on classiÔ¨Åcation performance of
the classiÔ¨Åers for predicting the breast density with FDVs derived using the ten compact
support wavelet Ô¨Ålters and the computational time required for prediction of 161 testing
instances. The experiments are described in Table 17.6.

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
411
Figure 17.20 Flowchart of proposed CAD system
design.
START
Load Input Mammogram
Crop ROI of size 200 √ó 200 pixels
Decompose ROI upto 2nd level using
2D-DWT and db 1 wavelet filter
 Compute FDV
Test FDV using SSVM classifier
END
Decision of CAD system
Table 17.6 Experiment descriptions
Experiment
Description
Experiment I
To determine the performance of diÔ¨Äerent FDVs using SVM classiÔ¨Åer
Experiment II
To determine the performance of diÔ¨Äerent FDVs using SSVM classiÔ¨Åer
Note: FDV: feature descriptor vector.
17.3.1
Results Based on ClassiÔ¨Åcation Performance of the ClassiÔ¨Åers (ClassiÔ¨Åcation
Accuracy and Sensitivity) for Each Class
17.3.1.1
Experiment I: To Determine the Performance of DiÔ¨Äerent FDVs Using SVM
ClassiÔ¨Åer
The experiment has been carried out to evaluate the classiÔ¨Åcation performance of all
seven FDVs (FDV1‚ÄìFDV7) derived from each of the ten wavelet Ô¨Ålters using the SVM
classiÔ¨Åer. The results are tabulated in Table 17.7.
It is observed that the maximum CA of 85.0% is achieved using FDV1 and FDV6
derived using the coif1 and db4 wavelet Ô¨Ålters, respectively. The texture descriptors
used in FDV6 are less than the descriptors in FDV1 and still give the same accuracy,
so FDV6 is considered to be the choice for obtaining maximum classiÔ¨Åcation accuracy.
The sensitivity values of FDV6 are 90.5%, 71.1%, and 92.8% for F, FG, and DG classes,
respectively.

412
Hybrid Intelligence for Image Analysis and Understanding
Table 17.7 ClassiÔ¨Åcation performance of SVM classiÔ¨Åer using diÔ¨Äerent FDVs
FDV (l)
CM
Max. CA (Ô¨Ålter)
Sen.F(%)
Sen.FG(%)
Sen.DG(%)
F
FG
DG
F
47
6
0
FDV1 (7)
FG
9
38
5
85.0 (coif1)
88.6
73.0
92.8
DG
0
4
52
F
37
13
3
FDV2 (6)
FG
4
40
8
78.2 (db1)
69.8
76.9
87.5
DG
0
7
49
F
43
7
3
FDV3 (3)
FG
2
42
8
83.2 (db1)
81.1
80.7
87.5
DG
1
6
49
F
42
8
3
FDV4 (4)
FG
11
36
5
74.7 (db1)
79.2
69.2
78.5
DG
6
6
44
F
46
6
1
FDV5 (4)
FG
11
37
4
84.4 (db1)
86.7
71.1
94.6
DG
0
3
53
F
48
5
0
FDV6 (4)
FG
10
37
5
85.0 (db4)
90.5
71.1
92.8
DG
0
4
52
F
44
8
1
FDV7 (3)
FG
4
37
11
80.7 (db1)
83.0
71.1
87.5
DG
1
6
49
Note: FDV: feature descriptor vector; l: length of FDV; CM: confusion matrix; F: fatty class; FG:
fatty-glandular class; DG: dense-glandular class; Max. CA: maximum classiÔ¨Åcation accuracy in % ge;
Sen.F: sensitivity for fatty class; Sen.FG: sensitivity for fatty-glandular class; Sen.DG: sensitivity for
dense-glandular class.
17.3.1.2
Experiment II: To Determine the Performance of DiÔ¨Äerent FDVs Using
SSVM ClassiÔ¨Åer
The experiment has been carried out to evaluate the classiÔ¨Åcation performance of all
seven FDVs (FDV1‚ÄìFDV7) derived from each of the ten wavelet Ô¨Ålters using the SSVM
classiÔ¨Åer. The results are tabulated in Table 17.8.
It is observed that the maximum CA of 89.4 % is achieved using FDV6 derived using
the db1 ( Haar) wavelet Ô¨Ålter. A comparable classiÔ¨Åcation accuracy of 88.2 % is achieved
from FDV1 derived from the db1 ( Haar) wavelet Ô¨Ålter. The sensitivity values of FDV6
are 86.7%, 86.5%, and 94.6% for F, FG, and DG classes, respectively.
17.3.2
Results Based on Computational EÔ¨Éciency of ClassiÔ¨Åers for Predicting 161
Instances of Testing Data Set
For both the CAD systems, the time taken for feature extraction is the same. Therefore,
the computational eÔ¨Éciency of the CAD systems has been compared with respect to the

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
413
Table 17.8 ClassiÔ¨Åcation performance of SSVM classiÔ¨Åer with diÔ¨Äerent FDVs
FDV (l)
CM
Max. CA (Ô¨Ålter)
Sen.F(%)
Sen.FG(%)
Sen.DG(%)
F
FG
DG
F
44
8
1
FDV1 (7)
FG
3
45
4
88.2 (db1)
83.0
86.5
94.6
DG
0
3
53
F
38
13
2
FDV2 (6)
FG
0
45
7
80.7 (db1)
71.6
86.5
83.9
DG
1
8
47
F
35
16
2
FDV3 (3)
FG
0
47
5
81.3 (db1)
66.0
90.3
87.5
DG
1
6
49
F
34
17
2
FDV4 (4)
FG
1
46
5
76.4 (db1)
64.1
88.4
76.7
DG
5
8
43
F
45
7
1
FDV5 (4)
FG
8
38
6
84.4 (db1)
84.9
73.0
94.6
DG
0
3
53
F
46
6
1
FDV6 (4)
FG
5
45
2
89.4 (db1)
86.7
86.5
94.6
DG
0
3
53
F
38
13
2
FDV7 (3)
FG
1
47
4
81.9 (db1)
71.6
90.3
83.9
DG
1
8
47
Note: FDV: feature descriptor vector; l: length of FDV; CM: confusion matrix; F: fatty class; FG:
fatty-glandular class; DG: dense-glandular class; Max. CA: maximum classiÔ¨Åcation accuracy in %
ge; Sen.F: sensitivity for fatty class; Sen.FG: sensitivity for fatty-glandular class; Sen.DG: sensitivity for
dense-glandular class.
time taken for predicting the labels for 161 instances of the testing data set, as shown in
Table 17.9.
From the table, it can be observed that the computational time required to predict the
labels of testing instances is less in case of SSVM as compared to SVM for both FDV1 and
FDV6. SSVM classiÔ¨Åer requires 2.0840 seconds to predict the labels of testing instances
for FDV6 and 2.0081 seconds to predict the labels of testing instances for FDV1.
17.4
Conclusion and Future Scope
From the experiments conducted, it can be concluded that the feature vector (consisting
of normalized energy values computed from all four sub-images obtained at the second
level of decomposition using 2D-DWT with a db1 (Haar) wavelet Ô¨Ålter along with SSVM
classiÔ¨Åer) yields an eÔ¨Écient three-class tissue density prediction system.

414
Hybrid Intelligence for Image Analysis and Understanding
Table 17.9 Comparison of computational time for prediction of testing instances
ClassiÔ¨Åer used
FDV
CA (%)
Computational time (s)
SVM
FDV1
85.0
10.3794
FDV6
85.0
12.1001
SSVM
FDV1
88.2
2.0081
FDV6
89.4
2.0840
Note: The above computations have been carried out on an Intel Core I3-2310 M, 2.10 GHz
with 3 GB RAM. FDV: feature descriptor vector; SSVM: smooth support vector machine;
SVM: support vector machine.
Image Database
ROI Extraction
Feature Extraction Module
2D-DWT applied to ROIs to obtain 7
subimages (one approximate and six
orientation selective detailed subimages)
Normalized energy from each approximate
subimage and detailed subimages of 2nd
level are calculated and used to form
feature vector set (FDV)
FDV
Classification Module
SSVM Classifier
Decision of CAD system
Fatty
Fatty-
glandular
Dense-
glandular
Figure 17.21 Proposed CAD system design.
The block diagram of the proposed CAD system design for predicting breast density
is shown in Figure 17.21.
There are certain cases where the density information cannot be clearly determined
solely on subjective analysis, so the CAD systems for breast density classiÔ¨Åcation come
into play. In the clinical environment, it is essential for a radiologist to correctly identify
the density type of the breast tissue, and then double-check the mammogram that has
been classiÔ¨Åed as dense to look for any lesions that might be hidden behind the dense
tissue by varying the contrast of the image. Using CAD systems for breast density clas-
siÔ¨Åcation, uncertainties that are present at the time of visual analysis can be removed,
and this also improves the diagnostic accuracy by highlighting certain areas of suspicion
that may contain any tumors obscured behind the dense tissue.

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
415
Following are the recommendations for future work:
1. The proposed system can be tested on a full-Ô¨Åeld digital mammogram (FFDM)
database like the INbreast database [107] consisting of DICOM images.
2. The proposed system can be tested for images acquired from MRI and other imaging
modalities.
3. The proposed system can be improved by incorporating an automatic ROI extraction
algorithm.
References
1 What is breast cancer. Available from http://www.nationalbreastcancer.org/what-is-
breast-cancer.
2 What is breast cancer (2015) Available from http://www.breastcancer.org/
symptoms/understand_bc/what_is_bc.
3 Types of breast cancers (2014) Available from http://www.cancer.org/cancer/
breastcancer/detailedguide/breast-cancer-breast-cancer-types.
4 Cancer health center. Available from http://www.webmd.com/cancer/what-is-
carcinoma.
5 DCIS-ductal carcinoma in situ (2015) Available from http://www.breastcancer.org/
symptoms/types/dcis.
6 IDC‚Äìinvasive ductal carcinoma (2016) Available from http://www.breastcancer.org/
symptoms/types/idc.
7 ILC‚Äìinvasive lobular carcinoma (2016) Available from http://www.breastcancer.org/
symptoms/types/ilc.
8 Heine, J.J., Carston, M.J., Scott, C.G., Brandt, K.R., Wu, F.F., Pankratz, V.S., Sellers,
T.A., and Vachon, C.M. (2008) An automated approach for estimation of breast
density. Cancer Epidemiology Biomarkers & Prevention, 17 (11), 3090‚Äì3097.
9 Cheddad, A., Czene, K., Eriksson, M., Li, J., Easton, D., Hall, P., and Humphreys,
K. (2014) Area and volumetric density estimation in processed full-Ô¨Åeld digital
mammograms for risk assessment of breast cancer. PLoS One, 9 (10), e110 690.
10 Colin, C., Prince, V., and Valette, P.J. (2013) Can mammographic assessments
lead to consider density as a risk factor for breast cancer? European Journal of
Radiology, 82 (3), 404‚Äì411.
11 Huo, Z., Giger, M.L., and Vyborny, C.J. (2001) Computerized analysis of
multiple-mammographic views: potential usefulness of special view mammo-
grams in computer-aided diagnosis. IEEE Transactions on Medical Imaging, 20
(12), 1285‚Äì1292.
12 Zhou, C., Chan, H.P., Petrick, N., Helvie, M.A., Goodsitt, M.M., Sahiner, B., and
Hadjiiski, L.M. (2001) Computerized image analysis: estimation of breast density on
mammograms. Medical physics, 28 (6), 1056‚Äì1069.
13 Oliver, A., Freixenet, J., Mart√≠, R., and Zwiggelaar, R. (2006) A comparison
of breast tissue classiÔ¨Åcation techniques, in Medical Image Computing and
Computer-Assisted Intervention‚ÄìMICCAI 2006, Springer, pp. 872‚Äì879.
14 Jagannath, H., Virmani, J., and Kumar, V. (2012) Morphological enhancement of
microcalciÔ¨Åcations in digital mammograms. Journal of the Institution of Engineers
(India): Series B, 93 (3), 163‚Äì172.

416
Hybrid Intelligence for Image Analysis and Understanding
15 Virmani, J. and Kumar, V. (2010) Quantitative evaluation of image enhancement
techniques, in Proceedings of International Conference on Biomedical Engineering
and Assistive Technology (BEATS-2010), NIT Jalandhar, India.
16 Yaghjyan, L., Pinney, S., Mahoney, M., Morton, A., and Buckholz, J. (2011) Mam-
mographic breast density assessment: a methods study. Atlas Journal of Medical
and Biological Sciiences, 1, 8‚Äì14.
17 American Cancer Society (2014) Understanding your mammogram report: birads
categories. Available from http://www.cancer.org/treatment/understandingy-
ourdiagnosis/examsandtestdescriptions/mammogramsand otherbreastimaging
procedures/mammograms-and-other-breast-imaging- procedures-mammo-report.
18 Mediolateral oblique view. Available from http://radiopaedia.org/articles/
mediolateral-oblique-view.
19 Wolfe, J.N. (1976) Breast patterns as an index of risk for developing breast cancer.
American Journal of Roentgenology, 126 (6), 1130‚Äì1137.
20 Wolfe, J.N. (1976) Risk for breast cancer development determined by mammo-
graphic parenchymal pattern. Cancer, 37 (5), 2486‚Äì2492.
21 Boyd, N., Martin, L., Chavez, S., Gunasekara, A., Salleh, A., Melnichouk, O., YaÔ¨Äe,
M., Friedenreich, C., Minkin, S., and Bronskill, M. (2009) Breast-tissue composition
and other risk factors for breast cancer in young women: a cross-sectional study.
Lancet Oncology, 10 (6), 569‚Äì580.
22 Boyd, N.F., Lockwood, G.A., Byng, J.W., Tritchler, D.L., and YaÔ¨Äe, M.J. (1998) Mam-
mographic densities and breast cancer risk. Cancer Epidemiology Biomarkers & Pre-
vention, 7 (12), 1133‚Äì1144.
23 Eng, A., Gallant, Z., Shepherd, J., McCormack, V., Li, J., Dowsett, M., Vinnicombe,
S., Allen, S., and dos Santos-Silva, I. (2014) Digital mammographic density and
breast cancer risk: a case-control study of six alternative density assessment meth-
ods. Breast Cancer Research, 16 (5), 439‚Äì452.
24 Boyd, N.F., Rommens, J.M., Vogt, K., Lee, V., Hopper, J.L., YaÔ¨Äe, M.J., and Paterson,
A.D. (2005) Mammographic breast density as an intermediate phenotype for breast
cancer. Lancet Oncology, 6 (10), 798‚Äì808.
25 Boyd, N.F., Martin, L.J., YaÔ¨Äe, M.J., and Minkin, S. (2011) Mammographic density
and breast cancer risk: current understanding and future prospects. Breast Cancer
Research, 13 (6), 223‚Äì235.
26 Boyd, N.F., Guo, H., Martin, L.J., Sun, L., Stone, J., Fishell, E., Jong, R.A., Hislop,
G., Chiarelli, A., Minkin, S. et al. (2007) Mammographic density and the risk and
detection of breast cancer. New England Journal of Medicine, 356 (3), 227‚Äì236.
27 Vachon, C.M., Van Gils, C.H., Sellers, T.A., Ghosh, K., Pruthi, S., Brandt, K.R., and
Pankratz, V.S. (2007) Mammographic density, breast cancer risk and risk prediction.
Breast Cancer Research, 9 (6), 1‚Äì9.
28 Martin, L.J. and Boyd, N. (2008) Potential mechanisms of breast cancer risk associ-
ated with mammographic density: hypotheses based on epidemiological evidence.
Breast Cancer Research, 10 (1), 1‚Äì14.
29 Warren, R. (2004) Hormones and mammographic breast density. Maturitas, 49 (1),
67‚Äì78.
30 Mousa, D.S.A., Brennan, P.C., Ryan, E.A., Lee, W.B., Tan, J., and Mello-Thoms,
C. (2014) How mammographic breast density aÔ¨Äects radiologists‚Äô visual search
patterns. Academic Radiology, 21 (11), 1386‚Äì1393.

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
417
31 √òster√•s, B.H., Martinsen, A.C.T., Brandal, S.H.B., Chaudhry, K.N., Eben, E.,
Haakenaasen, U., Falk, R.S., and Skaane, P. (2016) Bi-rads density classiÔ¨Åcation
from areometric and volumetric automatic breast density measurements. Academic
Radiology, 23 (4), 468‚Äì478.
32 Ekpo, E.U., Ujong, U.P., Mello-Thoms, C., and McEntee, M.F. (2016) Assessment of
interradiologist agreement regarding mammographic breast density classiÔ¨Åcation
using the Ô¨Åfth edition of the bi-rads atlas. American Journal of Roentgenology, 206
(5), 1119‚Äì1123.
33 Miller, P. and Astley, S. (1992) ClassiÔ¨Åcation of breast tissue by texture analysis.
Image and Vision Computing, 10 (5), 277‚Äì282.
34 Karssemeijer, N. (1998) Automated classiÔ¨Åcation of parenchymal patterns in mam-
mograms. Physics in Medicine and Biology, 43 (2), 365.
35 Bovis, K. and Singh, S. (2002) ClassiÔ¨Åcation of mammographic breast density using
a combined classiÔ¨Åer paradigm, in Medical Image Understanding and Analysis
(MIUA) conference, Portsmouth, Citeseer.
36 Wang, X.H., Good, W.F., Chapman, B.E., Chang, Y.H., Poller, W.R., Chang, T.S.,
and Hardesty, L.A. (2003) Automated assessment of the composition of breast
tissue revealed on tissue-thickness-corrected mammography. American Journal of
Roentgenology, 180 (1), 257‚Äì262.
37 Petroudi, S., Kadir, T., and Brady, M. (2003) Automatic classiÔ¨Åcation of mammo-
graphic parenchymal patterns: a statistical approach, in Engineering in Medicine
and Biology Society, 2003. Proceedings of the 25th Annual International Conference
of the IEEE, vol. 1, IEEE, vol. 1, pp. 798‚Äì801.
38 Oliver, A., Freixenet, J., and Zwiggelaar, R. (2005) Automatic classiÔ¨Åcation of breast
density, in Image Processing, 2005. ICIP 2005. IEEE International Conference on, vol.
2, IEEE, vol. 2, pp. 1258‚Äì1261.
39 Castella, C., Kinkel, K., Eckstein, M.P., Sottas, P.E., Verdun, F.R., and Bochud, F.O.
(2007) Semiautomatic mammographic parenchymal patterns classiÔ¨Åcation using
multiple statistical features. Academic Radiology, 14 (12), 1486‚Äì1499.
40 Oliver, A., Freixenet, J., Marti, R., Pont, J., Perez, E., Denton, E.R., and Zwiggelaar,
R. (2008) A novel breast tissue density classiÔ¨Åcation methodology. Information
Technology in Biomedicine, IEEE Transactions, 12 (1), 55‚Äì65.
41 V√°llez, N., Bueno, G., D√©niz-Su√°rez, O., Seone, J.A., Dorado, J., and Pazos, A.
(2011) A tree classiÔ¨Åer for automatic breast tissue classiÔ¨Åcation based on birads
categories, in Pattern Recognition and Image Analysis, Springer, pp. 580‚Äì587.
42 Chen, Z., Denton, E., and Zwiggelaar, R. (2011) Local feature based mammographic
tissue pattern modelling and breast density classiÔ¨Åcation, in Biomedical Engineer-
ing and Informatics (BMEI), 2011 4th International Conference, vol. 1, IEEE, pp.
351‚Äì355.
43 He, W., Denton, E., and Zwiggelaar, R. (2012) Mammographic segmentation and
risk classiÔ¨Åcation using a novel binary model based Bayes classiÔ¨Åer. Breast Imaging,
pp. 40‚Äì47.
44 Kutluk, S. and Gunsel, B. (2013) Tissue density classiÔ¨Åcation in mammographic
images using local features, in Signal Processing and Communications Applications
Conference (SIU), 2013 21st, IEEE, pp. 1‚Äì4.

Àá
418
Hybrid Intelligence for Image Analysis and Understanding
45 Sharma, V. and Singh, S. (2014) Cfs‚Äìsmo based classiÔ¨Åcation of breast density
using multiple texture models. Medical & Biological Engineering & Computing, 52
(6), 521‚Äì529.
46 Sharma, V. and Singh, S. (2015) Automated classiÔ¨Åcation of fatty and dense mam-
mograms. Journal of Medical Imaging and Health Informatics, 5 (3), 520‚Äì526.
47 Kriti, Virmani, J., Dey, N., and Kumar, V. (2016) PCA_PNN and PCA-SVM based
CAD systems for breast density classiÔ¨Åcation, in Applications of Intelligent Opti-
mization in Biology and Medicine, Springer, pp. 159‚Äì180.
48 Virmani, J. and Kriti (2016) Breast tissue density classiÔ¨Åcation using wavelet-based
texture descriptors, in Proceedings of the Second International Conference on Com-
puter and Communication Technologies, Springer, pp. 539‚Äì546.
49 Kriti and Virmani, J. (2015) Breast density classiÔ¨Åcation using Laws‚Äô mask texture
features. International Journal of Biomedical Engineering and Technology, 19 (3),
279‚Äì302.
50 Kriti, Virmani, J., and Thakur, S. (2016) Application of statistical texture features
for breast tissue density classiÔ¨Åcation, in Image Feature Detectors and Descriptors,
Springer, pp. 411‚Äì435.
51 Blot, L. and Zwiggelaar, R. (2001) Background texture extraction for the classiÔ¨Åca-
tion of mammographic parenchymal patterns, in Medical Image Understanding and
Analysis, pp. 145‚Äì148.
52 Bosch, A., Munoz, X., Oliver, A., and Marti, J. (2006) Modeling and classifying
breast tissue density in mammograms, in Computer Vision and Pattern Recognition,
2006 IEEE Computer Society Conference, vol. 2, IEEE, pp. 1552‚Äì1558.
53 Muhimmah, I. and Zwiggelaar, R. (2006) Mammographic density classiÔ¨Åcation
using multiresolution histogram information, in Proceedings of the International
Special Topic Conference on Information Technology in Biomedicine, Ioannina,
Greece, Citeseer.
54 Subashini, T., Ramalingam, V., and Palanivel, S. (2010) Automated assessment of
breast tissue density in digital mammograms. Computer Vision and Image Under-
standing, 114 (1), 33‚Äì43.
55 Tzikopoulos, S.D., Mavroforakis, M.E., Georgiou, H.V., Dimitropoulos, N., and
Theodoridis, S. (2011) A fully automated scheme for mammographic segmentation
and classiÔ¨Åcation based on breast density and asymmetry. Computer Methods and
Programs in Biomedicine, 102 (1), 47‚Äì63.
56 Li, J.B. (2012) Mammographic image based breast tissue classiÔ¨Åcation with kernel
self-optimized Ô¨Åsher discriminant for breast cancer diagnosis. Journal of Medical
Systems, 36 (4), 2235‚Äì2244.
57 Mu≈°tra, M., Grgi¬¥c, M., and Delac, K. (2012) Breast density classiÔ¨Åcation using mul-
tiple feature selection. AUTOMATIKA, 53 (4), 362‚Äì372.
58 Silva, W. and Menotti, D. (2012) ClassiÔ¨Åcation of mammograms by the breast
composition, in Proceedings of the International Conference on Image Processing,
Computer Vision, and Pattern Recognition (IPCV), The Steering Committee of
The World Congress in Computer Science, Computer Engineering and Applied
Computing (WorldComp), pp. 1‚Äì6.
59 Abdel-Nasser, M., Rashwan, H.A., Puig, D., and Moreno, A. (2015) Analysis of tis-
sue abnormality and breast density in mammographic images using a uniform local
directional pattern. Expert Systems with Applications, 42 (24), 9499‚Äì9511.

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
419
60 Kriti and Virmani, J. (2016) Comparison of CAD systems for three class breast
tissue density classiÔ¨Åcation using mammographic images, in Medical Imaging in
Clinical Applications: Algorithmic and Computer-Based Approaches (ed. N. Dey, V.
Bhateja, and E.A. Hassanien), pp. 107‚Äì130.
61 Li, H., Giger, M.L., Huo, Z., Olopade, O.I., Lan, L., Weber, B.L., and Bonta, I.
(2004) Computerized analysis of mammographic parenchymal patterns for assessing
breast cancer risk: eÔ¨Äect of ROI size and location. Medical Physics, 31 (3), 549‚Äì555.
62 Daugman, J.G. (1993) An information-theoretic view of analog representation in
striate cortex, in Computational Neuroscience, MIT Press, Cambridge, MA, pp.
403‚Äì423.
63 Virmani, J., Kumar, V., Kalra, N., and Khandelwal, N. (2013) Prediction of liver
cirrhosis based on multiresolution texture descriptors from b-mode ultrasound.
International Journal of Convergence Computing, 1 (1), 19‚Äì37.
64 Suckling, J., Parker, J., Dance, D., Astley, S., Hutt, I., Boggis, C., Ricketts, I.,
Stamatakis, E., Cerneaz, N., Kok, S. et al. (1994) The mammographic image analysis
society digital mammogram database, in Exerpta Medica. International Congress
Series, vol. 1069, pp. 375‚Äì378.
65 Doi, K. (2007) Computer-aided diagnosis in medical imaging: historical review,
current status and future potential. Computerized Medical Imaging and Graphics,
31 (4), 198‚Äì211.
66 Doi, K., MacMahon, H., Katsuragawa, S., Nishikawa, R.M., and Jiang, Y. (1999)
Computer-aided diagnosis in radiology: potential and pitfalls. European Journal of
Radiology, 31 (2), 97‚Äì109.
67 Giger, M.L., Doi, K., MacMahon, H., Nishikawa, R., HoÔ¨Ämann, K., Vyborny, C.,
Schmidt, R., Jia, H., Abe, K., and Chen, X. (1993) An ‚Äúintelligent‚Äù workstation for
computer-aided diagnosis. Radiographics, 13 (3), 647‚Äì656.
68 Kumar, I., Bhadauria, H., and Virmani, J. (2015) Wavelet packet texture descrip-
tors based four-class birads breast tissue density classiÔ¨Åcation. Procedia Computer
Science, 70, 76‚Äì84.
69 Virmani, J., Kumar, V., Kalra, N., and Khadelwal, N. (2011) A rapid approach for
prediction of liver cirrhosis based on Ô¨Årst order statistics, in Multimedia, Sig-
nal Processing and Communication Technologies (IMPACT), 2011 International
Conference, IEEE, pp. 212‚Äì215.
70 Zhang, G., Wang, W., Moon, J., Pack, J.K., and Jeon, S.I. (2011) A review of breast
tissue classiÔ¨Åcation in mammograms, in Proceedings of the 2011 ACM Symposium
on Research in Applied Computation, ACM, pp. 232‚Äì237.
71 Tourassi, G.D. (1999) Journey toward computer-aided diagnosis: role of image
texture analysis 1. Radiology, 213 (2), 317‚Äì320.
72 Hela, B., Hela, M., Kamel, H., Sana, B., and Najla, M. (2013) Breast cancer detec-
tion: a review on mammograms analysis techniques, in Systems, Signals & Devices
(SSD), 2013 10th International Multi-Conference, IEEE, pp. 1‚Äì6.
73 Liu, Q., Liu, L., Tan, Y., Wang, J., Ma, X., and Ni, H. (2011) Mammogram density
estimation using sub-region classiÔ¨Åcation, in Biomedical Engineering and Informat-
ics (BMEI), 2011 4th International Conference, vol. 1, IEEE, pp. 356‚Äì359.
74 Llobet, R., Poll√°n, M., Ant√≥n, J., Miranda-Garc√≠a, J., Casals, M., Mart√≠nez, I.,
Ruiz-Perales, F., P√©rez-G√≥mez, B., Salas-Trejo, D., and P√©rez-Cort√©s, J.C. (2014)
Semi-automated and fully automated mammographic density measurement and

420
Hybrid Intelligence for Image Analysis and Understanding
breast cancer risk prediction. Computer Methods and Programs in Biomedicine, 116
(2), 105‚Äì115.
75 Papaevangelou, A., Chatzistergos, S., Nikita, K., and Zografos, G. (2011) Breast den-
sity: Computerized analysis on digitized mammograms. Hellenic Journal of Surgery,
83 (3), 133‚Äì138.
76 Mustra, M., Grgic, M., and Delac, K. (2010) Feature selection for automatic breast
density classiÔ¨Åcation, in ELMAR, 2010 Proceedings, IEEE, pp. 9‚Äì16.
77 Cheng, H., Shan, J., Ju, W., Guo, Y., and Zhang, L. (2010) Automated breast cancer
detection and classiÔ¨Åcation using ultrasound images: a survey. Pattern Recognition,
43 (1), 299‚Äì317.
78 Virmani, J., Kumar, V., Kalra, N., and Khandelwal, N. (2011) Prediction of cirrho-
sis from liver ultrasound b-mode images based on laws‚Äô masks analysis, in Image
Information Processing (ICIIP), 2011 International Conference, IEEE, pp. 1‚Äì5.
79 Virmani, J., Kumar, V., Kalra, N., and Khandelwal, N. (2011) Prediction of cirrhosis
based on singular value decomposition of gray level co-occurence marix and a neu-
ral network classiÔ¨Åer, in Developments in E-systems Engineering (DeSE), 2011, IEEE,
pp. 146‚Äì151.
80 Virmani, J., Kumar, V., Kalra, N., and Khandelwal, N. (2014) Neural network
ensemble based CAD system for focal liver lesions from b-mode ultrasound.
Journal of Digital Imaging, 27 (4), 520‚Äì537.
81 Virmani, J., Kumar, V., Kalra, N., and Khandelwal, N. (2013) SVM-based character-
ization of liver ultrasound images using wavelet packet texture descriptors. Journal
of Digital Imaging, 26 (3), 530‚Äì543.
82 Sifuzzaman, M., Islam, M., and Ali, M. (2009) Application of wavelet transform
and its advantages compared to Fourier transform. Journal of Physical Sciences, 13,
121‚Äì134.
83 Li, X. and Tian, Z. (2006) Wavelet energy signature: comparison and analysis, in
Neural information processing, Springer, pp. 474‚Äì480.
84 Mohideen, S.K., Perumal, S.A., and Sathik, M.M. (2008) Image de-noising using
discrete wavelet transform. International Journal of Computer Science and Network
Security, 8 (1), 213‚Äì216.
85 Yoshida, H., Casalino, D.D., Keserci, B., Coskun, A., Ozturk, O., and Savranlar, A.
(2003) Wavelet-packet-based texture analysis for diÔ¨Äerentiation between benign and
malignant liver tumours in ultrasound images. Physics in Medicine and Biology, 48
(22), 3735.
86 Wan, J. and Zhou, S. (2010) Features extraction based on wavelet packet transform
for b-mode ultrasound liver images, in Image and Signal Processing (CISP), 2010
3rd International Congress, vol. 2, IEEE, pp. 949‚Äì955.
87 Tsiaparas, N.N., Golemati, S., Andreadis, I., Stoitsis, J.S., Valavanis, I., and Nikita,
K.S. (2011) Comparison of multiresolution features for texture classiÔ¨Åcation
of carotid atherosclerosis from b-mode ultrasound. Information Technology in
Biomedicine, IEEE Transactions, 15 (1), 130‚Äì137.
88 Chang, T. and Kuo, C.J. (1993) Texture analysis and classiÔ¨Åcation with
tree-structured wavelet transform. Image Processing, IEEE Transactions, 2 (4),
429‚Äì441.

Evaluating the EÔ¨Écacy of Multi-resolution Texture Features
421
89 Avci, E. (2008) Comparison of wavelet families for texture classiÔ¨Åcation by using
wavelet packet entropy adaptive network based fuzzy inference system. Applied Soft
Computing, 8 (1), 225‚Äì231.
90 Bazzani, A., Bevilacqua, A., Bollini, D., Brancaccio, R., Campanini, R., Lanconelli,
N., Riccardi, A., Romani, D., and Zamboni, G. (2000) Automatic detection of
clustered microcalciÔ¨Åcations in digital mammograms using an SVM classiÔ¨Åer, in
ESANN, pp. 195‚Äì200.
91 Kamra, A., Jain, V., Singh, S., and Mittal, S. (2016) Characterization of architectural
distortion in mammograms based on texture analysis using support vector machine
classiÔ¨Åer with clinical evaluation. Journal of Digital Imaging, 29 (1), 104‚Äì114.
92 de Oliveira, F.S.S., de Carvalho Filho, A.O., Silva, A.C., de Paiva, A.C., and Gattass,
M. (2015) ClassiÔ¨Åcation of breast regions as mass and non-mass based on digi-
tal mammograms using taxonomic indexes and SVM. Computers in Biology and
Medicine, 57, 42‚Äì53.
93 Rejani, Y. and Selvi, S.T. (2009) Early detection of breast cancer using SVM classi-
Ô¨Åer technique. arXiv preprint arXiv:0912.2314.
94 Campanini, R., Dongiovanni, D., Iampieri, E., Lanconelli, N., Masotti, M., Palermo,
G., Riccardi, A., and RoÔ¨Élli, M. (2004) A novel featureless approach to mass
detection in digital mammograms based on support vector machines. Physics in
Medicine and Biology, 49 (6), 961.
95 Guo, Q., Shao, J., and Ruiz, V. (2005) Investigation of support vector machine for
the detection of architectural distortion in mammographic images, in Journal of
Physics: Conference Series, vol. 15, IOP Publishing, p. 88.
96 Chang, C.C. and Lin, C.J. (2011) Libsvm: a library for support vector machines.
ACM Transactions on Intelligent Systems and Technology (TIST), 2 (3), 27.
97 Virmani, J., Kumar, V., Kalra, N., and Khandelwal, N. (2013) A comparative study
of computer-aided classiÔ¨Åcation systems for focal hepatic lesions from b-mode
ultrasound. Journal of Medical Engineering & Technology, 37 (4), 292‚Äì306.
98 Li, S., Kwok, J.T., Zhu, H., and Wang, Y. (2003) Texture classiÔ¨Åcation using the sup-
port vector machines. Pattern Recognition, 36 (12), 2883‚Äì2893.
99 Virmani, J., Kumar, V., Kalra, N., and Khandelwal, N. (2013) Svm-based charac-
terisation of liver cirrhosis by singular value decomposition of GLCM matrix.
International Journal of ArtiÔ¨Åcial Intelligence and Soft Computing, 3 (3), 276‚Äì296.
100 Azar, A.T. and El-Said, S.A. (2014) Performance analysis of support vector
machines classiÔ¨Åers in breast cancer mammography recognition. Neural Computing
and Applications, 24 (5), 1163‚Äì1177.
101 Virmani, J., Kumar, V., Kalra, N., and Khandelwa, N. (2013) Pca-svm based cad sys-
tem for focal liver lesions using b-mode ultrasound images. Defence Science Journal,
63 (5), 478.
102 Virmani, J., Kumar, V., Kalra, N., and Khandelwal, N. (2013) Characterization of
primary and secondary malignant liver lesions from b-mode ultrasound. Journal of
Digital Imaging, 26 (6), 1058‚Äì1070.
103 Lee, Y. and Mangasarian, O. (2015) SSVM toolbox. Available from http://research.cs
.wisc.edu/dmi/svm/ssvm/.
104 Lee, Y.J. and Mangasarian, O.L. (2001) SSVM: a smooth support vector machine for
classiÔ¨Åcation. Computational Optimization and Applications, 20 (1), 5‚Äì22.

422
Hybrid Intelligence for Image Analysis and Understanding
105 Purnami, S.W., Embong, A., Zain, J.M., and Rahayu, S. (2009) A new smooth sup-
port vector machine and its applications in diabetes disease diagnosis. Journal of
Computer Science, 5 (12), 1003.
106 Lee, Y.J., Mangasarian, O.L., and Wolberg, W.H. (2003) Survival-time classiÔ¨Åcation
of breast cancer patients. Computational Optimization and Applications, 25 (1-3),
151‚Äì166.
107 Moreira, I.C., Amaral, I., Domingues, I., Cardoso, A., Cardoso, M.J., and Cardoso,
J.S. (2012) Inbreast: toward a full-Ô¨Åeld digital mammographic database. Academic
Radiology, 19 (2), 236‚Äì248.

423
Index
1-level decomposition
55
2-level decomposition
55
2D PCA
266, 267, 271, 274, 275, 278, 279
3ùúémethod
356
a
ABD
347
Abnormal mass
340, 341, 343, 354, 358,
361, 365
Accuracy
315, 323, 332, 335, 341, 343,
361‚Äì363, 365, 366, 370, 378, 381,
382, 384, 387
Accuracy rate
65
Action bank representation
165, 167‚Äì169,
172
Activation function
131
Adaboost
191‚Äì194, 202
adaptive thresholding
326
age
103‚Äì106, 113, 114, 118‚Äì120
Ais
268
AlexNet
158
Algorithm
340‚Äì354, 356‚Äì363, 365, 366
Anatomical regions
353
Anatomical segmentation
353
Angiograms
369, 370, 377, 381, 382, 386
Anomaly detection
252
Area under the ROC curve (AUC)
68, 71
Artifacts
344
ArtiÔ¨Åcal neuron
130
authentication
315‚Äì317, 335
b
Back-propagation
136, 137
Band selection
263, 265, 271, 275, 276, 278
Basis vectors
55
BI-RADS
392, 394
biometric trait
335
Bonferroni‚ÄìDunn test
68
Breast cancer
297‚Äì301, 303, 304,
307‚Äì310, 339, 340, 391
Breast contour
351
Breast tumor
340
c
CA
411, 412
CAD
340, 341, 358, 366, 393‚Äì395, 398,
400, 410, 412, 414
CaÔ¨Äe
148, 150
Cancer
339, 353
CC
343
CC view
393
Ccd
235, 236
CCL
342
Chain rule
137
character recognition
25‚Äì27, 38
ClassiÔ¨Åcation
191, 369, 375, 376, 378, 379,
381, 382, 384, 387
classiÔ¨Åcation
26, 27, 32‚Äì34, 400, 407
Clonal selection algorithm
265, 266, 271,
276, 278, 279
clustering
283‚Äì285, 287, 288, 290‚Äì293
Cmos
235, 236
compactness
26, 29, 35, 37, 38
Confusion Matrix
70, 384
Contour Signature
86, 88, 89, 94, 95, 98
Control classiÔ¨Åer
68
Convolution
139
Convolution kernel
139
Hybrid Intelligence for Image Analysis and Understanding, First Edition.
Edited by Siddhartha Bhattacharyya, Indrajit Pan, Anirban Mukherjee, and Paramartha Dutta.
¬© 2017 John Wiley & Sons Ltd. Published 2017 by John Wiley & Sons Ltd.
Companion Website: www.wiley.com/go/bhattacharyya/hybridintelligence

424
Index
Convolution layer
142
Convolutional Neural network
127, 129,
141, 166, 167, 169, 170, 182
Coronary Stenosis
369, 370, 377, 378, 384,
386
Correlation coeÔ¨Écient
8
Cost function
136
CR mammogram
358
Critical diÔ¨Äerence
68
Crossover
7
Cuckoo search algorithm
306, 307,
310
Cuckoo search optimization algorithm
302‚Äì304, 306, 308, 310
d
Data sets
265, 276, 279
database
322, 331
Deep learning
127
Degrees of freedom
67
DEMS
358
Descriptors
188, 190, 191, 201
DICOM
340, 358
DiÔ¨Äerential Evolution
370, 371, 377,
386
Digitization
221, 222
Discrete wavelet transform
53, 54
distinctness
29, 35, 37, 38
dorsal vein
316, 321
DR mammogram
358
Dropout
147
DWT
401‚Äì403, 413
e
Edge detection
341, 342, 345, 346, 348,
349, 355, 366
Edge map
348
EFCM
27, 29, 35, 37, 38
Empirical measure Q(I)
8
Euclidean distance
5, 28‚Äì30, 34, 35, 38,
248, 249, 254
Evolutionary algorithm
167, 168,
170‚Äì172, 182, 183
f
F-ratio
90, 91, 93, 97, 98
False positive (FP)
366
False positive rate (FPR)
68
FAR
316, 323, 326, 335
FCM
283, 284, 287, 288, 290‚Äì293
FDVs
397, 406, 410‚Äì412
Feature enhancement
89, 90, 93, 96, 98
Feature extraction
26, 27, 31, 32, 35, 239,
247, 248, 265, 266, 271, 275, 278,
319, 326, 335, 396, 400
feature level fusion
335
Feature mining
247
Feature selection
247, 248, 251, 265, 266,
271
Feature Vector
319‚Äì321, 333, 370, 384,
387
Features
187, 190‚Äì193
Feed-forward network
135
Filter Response
370, 371, 378, 382
Ô¨Ånger vein
315, 316, 320‚Äì322, 331, 335
Forward propagation
137
Fourier transform
400, 401
frequency domain
318, 326, 331‚Äì333, 335
Friedman test
66
FRR
316, 323, 326, 335
Fully connected layer
146
fusion
316, 317, 320, 321, 326, 330, 335
Fuzzy c-means (FCM)
205, 208, 217, 224,
226, 227
Fuzzy c-means (FCM) algorithm
2, 4, 5, 9
Fuzzy KNN
266, 268, 269, 271, 277, 279
Fuzzy logic
318, 321, 335
g
Gabor Ô¨Ålter
316, 318‚Äì320, 323, 333, 335
Gaussian Ô¨Ålter
345
Gaussian Matched Ô¨Ålters
369‚Äì371, 377,
386
Gaussian radial basis function kernel
409
gender
103‚Äì106, 111, 113‚Äì120
Generations
373
Generic point
188
Genetic algorithms (GAs)
3, 6
Geometric moment
80, 86, 87, 94, 95, 97,
98
GIS
206
GoogleNet
159
Gradient descent
136
GT
361

Index
425
h
Haar
395, 405, 406, 412, 413
Haar wavelet transform
55, 56
hand vein images
316, 318, 320‚Äì323, 326,
330, 331, 335
Handedness
103‚Äì106, 113, 114, 117‚Äì120
handwriting
103‚Äì106, 108, 113, 118, 120
Hh component
56
Hiddden Morkov Model
316
Hierarchical and temporal machine
158
hierarchical method
319
High-pass Ô¨Ålter
55
Hl component
55
Homogeneity enhancement
346, 366
Horizontal projection
58
Hotspot
205‚Äì209, 215, 217, 218, 224,
226‚Äì229
HSI
83
Human action recognition
165‚Äì169, 182
Hybrid techniques
234
Hyper-parameters
143
Hyperspectral image
263‚Äì265, 271, 276,
279
Hyperspectral imaging
233, 235, 240, 256
i
Iinear mixing model
241‚Äì243
illumination
80‚Äì85, 92‚Äì94, 97, 99
Image analysis
263, 265, 283, 284, 287,
293, 319
Image classiÔ¨Åcation
234, 247, 248, 250,
251
Image Segmentation
1‚Äì3
Iman and Davenport test
67
Indiana dataset
271, 272, 274, 277, 279
ISODATA
207, 226, 227
k
K-means clustering
84, 93, 94, 98
Kappa statistics
68
kernel
409
Kernel function
66
Krawtchouk moment
80, 86‚Äì88, 94‚Äì99
l
L1 regularization
147
L2 regularization
147
LCS
86, 88, 89, 94, 95, 98
Lda
264, 278
LeNet
158
Leptourtic curve
62
Lh component
55
linear kernel
409
Ll component
56
longest run
27, 32‚Äì34, 38
Low-pass Ô¨Ålter
55
m
Mahalanobis distance
248, 252
Mahalanobish distance
84, 93, 94, 98
Malignancy
339
Mammographic Image Analysis Society
(MIAS)
300
Mammography
340, 343, 345, 358, 392,
393
Markov random Ô¨Åelds
342
MatConvNet
148, 150
MATLAB
148
Matthews correlation coeÔ¨Écient (MCC)
68, 70
Max pooling
145
MDT
346
Mean absolute error (MAE)
69
Medical images
340, 341, 343, 366
Membership degree
5
Mesokurtic curve
62
Metastasize
339
MfGa based FCM algorithm
4, 9, 12, 14,
18
MIAS
358, 394, 395, 398
MIAS database
300, 307, 310
Mixed pixel
240, 241, 244
MLO
343
MLO view
393
Model building time
65, 66
ModiÔ¨Åed Genetic (MfGa) Algorithm
6, 9
Moment coeÔ¨Écient of kurtosis
62
Moment coeÔ¨Écient of skewness
61
morphological
80, 81, 85, 94
Mother wavelet
54
MRI
340
Multi criteria evaluation (MCE)
207, 228,
229

426
Index
Multilayer self-organizing neural network
(MLSONN) architecture (contd.)
Multilayer self-organizing neural network
(MLSONN) architecture
4
Multilevel sigmoidal (MUSIG) activation
function
4
Multilingual environment
47, 48
Multimodal biometric
315
Multimodal biometric recognition
315‚Äì317
Multiple classiÔ¨Åers
66, 69, 75
Mutation
7
n
Naive Bayes
370, 375, 377, 379, 384, 387
Negatively skewed distribution
60, 61
Nemenyi test
68
Neural network
128
Neuron
130
non-uniform zoning
90, 96, 99
Nonlinear mixing model
241‚Äì243
normalized energy
397, 406, 407, 413
Null hypothesis
67
o
Object classiÔ¨Åcation
157
Object memorability
157
Optimization
370, 371, 373, 374, 377
Optimized MUSIG (OptiMUSIG)
activation function
4
OverÔ¨Åtting
147
p
palm vein
316, 321, 322, 331
Parallel feature fusion
80, 91‚Äì93, 97, 98
Parameter sharing
144
Pca
264‚Äì267
Pearson coeÔ¨Écient of skewness
60
Pectoral muscle
346, 350
Percentile coeÔ¨Écient of kurtosis
62
Percentile coeÔ¨Écient of skewness
61
Perceptron
128, 134, 135
Platykurtic curve
62
PNN
342
Polynomial
208, 210
polynomial kernel
409
Pooling layer
145
Population initialization
6
Positively skewed distribution
61
Post-hoc tests
66, 68
Precision
70
preprocessing
25‚Äì27, 31, 32, 316, 318
q
Quartile coeÔ¨Écient of skewness
61
r
Radiometric calibration
238
RADIUS
207, 228
Radon transform (RT)
57
Recall
70
Receiver Operating Characteristic
370,
377
Receiver operating characteristic (ROC)
curve
71
Recognition
187, 193, 194, 197, 199‚Äì201
RectiÔ¨Åed linear units
131
Recurrent neural networks
135
Region of interest
340‚Äì342, 346, 353, 354,
357, 361, 366
Regression analysis
207, 208, 210, 212
ResNet
159
RGB
82, 83, 85, 86
ROC analysis
361
Root mean square error (RMSE)
69
rotation
80, 81, 85, 92, 93, 97, 99
s
Sammon‚Äôs nonlinear mapping
35
Scaling function
54
score level fusion
320, 335
Script identiÔ¨Åcation
47, 48, 52
Segmentation
340‚Äì343, 345, 346, 351,
353‚Äì355, 361, 366
segmentation
80‚Äì85, 92, 93, 97
Selection
7
Semisupervised method
265, 271, 278
Sensitivity
362, 365, 366
Serial feature fusion
80, 91‚Äì93, 98
SGLDM
342
short time fourier transform
401
Sigmoid kernel
410
Skewness
59
Soft biometrics
103‚Äì106, 109, 111, 113,
114, 118, 120, 121

Index
427
Softmax function
132
Spatial
233, 236‚Äì238, 240, 245, 247, 255
spatial domain
318, 319, 333, 335
SpeciÔ¨Åcity
362, 366
Spectral
263, 265, 266, 272, 274
Spectral bands
237, 242, 245, 247‚Äì249,
253, 254
Spectral calibration
238
Spectral dimension
233
Spectral information
236, 237, 240, 245
Spectral Library
238, 245, 246, 251
Spectral resolution
233
Spectral response
236
Spectral signature
233, 239, 247‚Äì249, 253,
254
Spectral unmixing
240, 246
Spectroscopy
233, 234
SRG
341, 354, 355, 357
SSVM
397, 407, 410, 412, 413
SSVM toolbox
410
STAC
207, 227, 228
static hand gesture
79, 80, 82, 83, 85‚Äì89,
92, 94, 95, 97, 99
Statistical decision making
341, 355, 356
Statistical features
71
Statistical performance analysis
65
Statistical signiÔ¨Åcance tests
50
Stochastic gradient descent
138
Structure-based methods
50
Subpixel
255, 256
sum rule
318, 320, 326, 334
Supervised classiÔ¨Åcation
234, 247, 248,
250
Supervised method
265, 278
Support Vector Machine (SVM)
65, 131,
298, 303
SVM
277, 279, 298‚Äì300, 303, 307, 309,
397, 407‚Äì411, 413
SVM classiÔ¨Åer
301, 303, 310, 342, 343
t
Target identiÔ¨Åcation
233, 234, 236, 238,
240, 254‚Äì256
Tchebichef moment
80, 87, 88, 94‚Äì98
Texture-based features
50
Thresholding
377, 378, 382, 387
TNT-mips
207, 226, 227
Torch
148
Tracking
187, 188, 190, 193, 194, 196, 197,
199‚Äì202
True negative (TN)
366
True positive rate (TPR)
68
Tumor
339, 340, 342, 348, 353, 354, 361
Two-stage approach
75
u
uncertainty models
284
uniform zoning
90
unimodal
315, 316, 329, 332
Unsupervised method
264‚Äì266, 268
v
Vertical projection
58
Vessel Detection
369, 370, 377, 382
Vessel Segmentation
369, 384, 387
VGGNet
150, 151, 159
Visual appearance-based methods
50, 51
w
Wavelet coeÔ¨Écient
54
Wavelet decomposition tree
55
wavelet Ô¨Ålters
400, 405
Wavelet transform
401
y
YCbCr
82, 83, 93, 94
YIQ
82, 83
z
Z-score
356


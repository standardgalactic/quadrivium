Machine Learning Specialization 
Linear classiﬁers:  
Parameter learning 
Emily Fox & Carlos Guestrin 
Machine Learning Specialization 
University of Washington 
1 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
2 
Learn a probabilistic classiﬁcation model 
©2015-2016 Emily Fox & Carlos Guestrin 
Deﬁnite 
 
“The sushi & everything  
else were awesome!” 
“The sushi was good,  
 the service was OK” 
Not sure 
P(y=+1| x=          
           )  
             = 0.99 
P(y=+1| x=          
        )  
             = 0.55 
“The sushi & everything  
else were awesome!” 
“The sushi was good,  
the service was OK” 
Many classiﬁers provide a degree of certainty: 
 
 
   P(y|x) 
 Extremely useful in practice 
Output label 
Input sentence 

Machine Learning Specialization 
3 
A (linear) classiﬁer 
•  Will use training data to learn a weight  
or coeﬃcient for each word  
©2015-2016 Emily Fox & Carlos Guestrin 
Word 
Coeﬃcient 
Value 
ŵ0 
-2.0 
good 
ŵ1 
  1.0 
great 
ŵ2 
  1.5 
awesome 
ŵ3 
  2.7 
bad 
ŵ4 
-1.0 
terrible 
ŵ5 
-2.1 
awful 
ŵ6 
-3.3 
restaurant, the, we, … 
ŵ7, ŵ8, ŵ9,… 
  0.0 
… 
 … 

Machine Learning Specialization 
4 
Logistic regression model 
©2015-2016 Emily Fox & Carlos Guestrin 
-∞ 
+∞ 
Score(xi) = ŵTh(xi) 
0.0 
1.0 
0.0 
0.5 
P(y=+1|x,ŵ) =         1       .  
 
⌃ 
1 + e-ŵ h(x) 
T 

Machine Learning Specialization 
Quality metric for logistic regression: 
Maximum likelihood estimation 
5 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
6 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
ŵ 
h(x) 
x 
P(y=+1|x,ŵ) =         1       .  
 
⌃ 
1 + e-ŵ h(x) 
T 

Machine Learning Specialization 
7 
Learning problem 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
2 
1 
+1 
0 
2 
-1 
3 
3 
-1 
4 
1 
+1 
1 
1 
+1 
2 
4 
-1 
0 
3 
-1 
0 
1 
-1 
2 
1 
+1 
©2015-2016 Emily Fox & Carlos Guestrin 
Training data:  
N observations (xi,yi) 
Optimize 
quality metric  
on training 
data 
 ŵ 

Machine Learning Specialization 
MOVE TO  
HEAD SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
8 

Machine Learning Specialization 
9 
Finding best coeﬃcients 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
2 
1 
+1 
4 
1 
+1 
1 
1 
+1 
2 
1 
+1 
©2015-2016 Emily Fox & Carlos Guestrin 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 
3 
3 
-1 
2 
4 
-1 
0 
3 
-1 
0 
1 
-1 

Machine Learning Specialization 
10 
Finding best coeﬃcients 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
2 
1 
+1 
4 
1 
+1 
1 
1 
+1 
2 
1 
+1 
©2015-2016 Emily Fox & Carlos Guestrin 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 
3 
3 
-1 
2 
4 
-1 
0 
3 
-1 
0 
1 
-1 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
2 
1 
+1 
4 
1 
+1 
1 
1 
+1 
2 
1 
+1 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 
3 
3 
-1 
2 
4 
-1 
0 
3 
-1 
0 
1 
-1 

Machine Learning Specialization 
11 
Finding best coeﬃcients 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
2 
1 
+1 
4 
1 
+1 
1 
1 
+1 
2 
1 
+1 
©2015-2016 Emily Fox & Carlos Guestrin 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 
3 
3 
-1 
2 
4 
-1 
0 
3 
-1 
0 
1 
-1 
P(y=+1|xi,w) = 0.0 
P(y=+1|xi,w) = 1.0 
Pick ŵ that makes  

Machine Learning Specialization 
12 
Quality metric = Likelihood function 
©2015-2016 Emily Fox & Carlos Guestrin 
No ŵ achieves perfect predictions (usually) 
P(y=+1|xi,w) = 0.0 
P(y=+1|xi,w) = 1.0 
Positive data points 
Negative data points 
Likelihood ℓ(w): Measures quality of  
ﬁt for model with coeﬃcients w 

Machine Learning Specialization 
13 
Find “best” classiﬁer 
©2015-2016 Emily Fox & Carlos Guestrin 
#awesome 
#awful 
0 
1 
2 
3 
4 
… 
0 
1 
2 
3 
4 
… 
Maximize likelihood over all possible w0,w1,w2 
ℓ(w0=1, w1=0.5, w2=-1.5) = 10-4 
ℓ(w0=1, w1=1, w2=-1.5) = 10-5 
ℓ(w0=0, w1=1, w2=-1.5) = 10-6 
Best model:  
Highest likelihood ℓ(w) 
ŵ = (w0=1, w1=0.5, w2=-1.5)  

Machine Learning Specialization 
Data likelihood 
©2015-2016 Emily Fox & Carlos Guestrin 
14 

Machine Learning Specialization 
15 
Pick w to maximize: 
 
If model good, should predict: 
 
Quality metric: probability of data 
©2015-2016 Emily Fox & Carlos Guestrin 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
2 
1 
+1 
Pick w to maximize: 
 
If model good, should predict:  
 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 

Machine Learning Specialization 
16 
Maximizing likelihood  
(probability of data)  
Data 
point 
x[1] 
x[2] 
y 
Choose w to maximize 
x1,y1  
2 
1 
+1 
x2,y2 
0 
2 
-1 
x3,y3 
3 
3 
-1 
x4,y4 
4 
1 
+1 
x5,y5 
1 
1 
+1 
x6,y6 
2 
4 
-1 
x7,y7 
0 
3 
-1 
x8,y8 
0 
1 
-1 
x9,y9 
2 
1 
+1 
©2015-2016 Emily Fox & Carlos Guestrin 
Must combine into single  
measure of quality 

Machine Learning Specialization 
17 
Learn logistic regression model with  
maximum likelihood estimation (MLE) 
Data 
point 
x[1] 
x[2] 
y 
Choose w to maximize 
x1,y1  
2 
1 
+1 
x2,y2 
0 
2 
-1 
x3,y3 
3 
3 
-1 
x4,y4 
4 
1 
+1 
©2015-2016 Emily Fox & Carlos Guestrin 
P(y=+1|x[1]=2, x[2]=1,w) 
P(y=-1|x[1]=0, x[2]=2,w) 
P(y=-1|x[1]=3, x[2]=3,w) 
P(y=+1|x[1]=4, x[2]=1,w) 
ℓ(w) =  
P(y1|x1,w) 
P(y2|x2,w) 
P(y3|x3,w) 
P(y4|x4,w) 
N
Y
i=1
P(yi | xi, w)

Machine Learning Specialization 
MOVE TO  
FULL BODY SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
18 

Machine Learning Specialization 
Finding best linear classiﬁer  
with gradient ascent 
19 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
20 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
ŵ 
h(x) 
x 
P(y=+1|x,ŵ) =         1       .  
 
⌃ 
1 + e-ŵ h(x) 
T 

Machine Learning Specialization 
MOVE TO  
HEAD SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
21 

Machine Learning Specialization 
22 
Find “best” classiﬁer 
©2015-2016 Emily Fox & Carlos Guestrin 
#awesome 
#awful 
0 
1 
2 
3 
4 
… 
0 
1 
2 
3 
4 
… 
Maximize likelihood over all possible w0,w1,w2 
ℓ(w0=1, w1=0.5, w2=-1.5) = 10-4 
ℓ(w0=1, w1=1, w2=-1.5) = 10-5 
ℓ(w0=0, w1=1, w2=-1.5) = 10-6 
Best model:  
Highest likelihood ℓ(w) 
ŵ = (w0=1, w1=0.5, w2=-1.5)  
`(w) =
N
Y
i=1
P(yi | xi, w)

Machine Learning Specialization 
23 
Maximizing likelihood 
©2015-2016 Emily Fox & Carlos Guestrin 
max 
w0,w1,w2 
ℓ(w0,w1,w2) is a  
function of 3 variables 
Maximize function over all 
possible w0,w1,w2 
N
Y
i=1
P(yi | xi, w)
No closed-form solution è use gradient ascent  

Machine Learning Specialization 
MOVE TO  
FULL BODY SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
24 

Machine Learning Specialization 
Review of gradient ascent 
25 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
MOVE TO  
HEAD SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
26 

Machine Learning Specialization 
27 
Finding the max  
via hill climbing 
©2015-2016 Emily Fox & Carlos Guestrin 
Algorithm: 
 
while not converged 
 w(t+1) ß w(t) + η dℓ  
dw w(t) 

Machine Learning Specialization 
Convergence criteria 
For convex functions, 
optimum occurs when 
 
 
In practice, stop when 
©2015-2016 Emily Fox & Carlos Guestrin 
Algorithm: 
 
while not converged 
 w(t+1) ß w(t) + η dℓ  
dw w(t) 
28 

Machine Learning Specialization 
29 
Moving to multiple dimensions: 
Gradients 
©2015-2016 Emily Fox & Carlos Guestrin 
 Δ 
ℓ(w) = 
(w) = 

Machine Learning Specialization 
30 
Contour plots 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
31 
Gradient ascent 
©2015-2016 Emily Fox & Carlos Guestrin 
Algorithm: 
 
while not converged 
 w(t+1) ß w(t) + η   ℓ(w(t))  
  Δ 

Machine Learning Specialization 
MOVE TO  
FULL BODY SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
32 

Machine Learning Specialization 
Learning algorithm for  
logistic regression 
33 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
MOVE TO  
HEAD SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
34 

Machine Learning Specialization 
35 
Derivative of (log-)likelihood 
©2015-2016 Emily Fox & Carlos Guestrin 
@`(w)
@wj
=
⇣
1[yi = +1] −P(y = +1 | xi, w)
⌘
Diﬀerence between truth and prediction 
N
X
i=1
Sum over  
data points 
hj(xi)
Feature 
value 
⇣
1[yi = +1]
Indicator function: 

Machine Learning Specialization 
36 
Computing derivative 
x[1] 
x[2] 
y 
P(y=+1|xi,w) 
Contribution to 
derivative for w1 
2 
1 
+1 
0.5 
0 
2 
-1 
0.02 
3 
3 
-1 
0.05 
4 
1 
+1 
0.88 
©2015-2016 Emily Fox & Carlos Guestrin 
Total derivative: 
@`(w(t))
@wj
=
N
X
i=1
hj(xi)
⇣
1[yi = +1] −P(y = +1 | xi, w(t))
⌘
w0 
  0 
w1 
  1 
w2 
-2 
(t) 
(t) 
(t) 

Machine Learning Specialization 
37 
Derivative of (log-)likelihood: Interpretation 
©2015-2016 Emily Fox & Carlos Guestrin 
@`(w)
@wj
=
⇣
1[yi = +1] −P(y = +1 | xi, w)
⌘
Diﬀerence between truth and prediction 
N
X
i=1
Sum over  
data points 
hj(xi)
Feature 
value 
If hj(xi)=1: 
P(y=+1|xi,w) ≈ 1 
P(y=+1|xi,w) ≈ 0 
yi=+1 
yi=-1 

Machine Learning Specialization 
38 
Summary of gradient ascent 
for logistic regression 
©2015-2016 Emily Fox & Carlos Guestrin 
init w(1)=0 (or randomly, or smartly), t=1 
while ||    ℓ(w(t))|| > ε  
  for j=0,…,D 
 partial[j] = 
 
 wj
(t+1) ß wj
(t) + η partial[j] 
  t ß t + 1 
 
  Δ 
N
X
i=1
hj(xi)
⇣
1[yi = +1] −P(y = +1 | xi, w(t))
⌘

Machine Learning Specialization 
MOVE TO  
FULL BODY SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
39 

Machine Learning Specialization 
Choosing the step size η 
40 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
MOVE TO  
HEAD SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
41 

Machine Learning Specialization 
42 
Learning curve: 
Plot quality (likelihood) over iterations 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
43 
If step size is too small, can take a  
long time to converge 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
44 
Compare converge with  
diﬀerent step sizes 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
45 
Careful with step sizes that are too large 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
46 
Very large step sizes can even  
cause divergence or wild oscillations 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
47 
Simple rule of thumb for picking step size η 
•  Unfortunately, picking step size  
requires a lot of trial and error L 
•  Try a several values, exponentially spaced 
- Goal: plot learning curves to 
•  ﬁnd one η that is too small (smooth but moving too slowly) 
•  ﬁnd one η that is too large (oscillation or divergence) 
•  Try values in between to ﬁnd “best” η 
•  Advanced tip: can also try step size that decreases with 
iterations, e.g., 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
MOVE TO  
FULL BODY SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
48 

Machine Learning Specialization 
Deriving the gradient for  
logistic regression 
49 
©2015-2016 Emily Fox & Carlos Guestrin 
VERY 
OPTIONAL 

Machine Learning Specialization 
MOVE TO  
HEAD SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
50 

Machine Learning Specialization 
51 
Log-likelihood function 
•  Goal: choose coeﬃcients w maximizing likelihood: 
•  Math simpliﬁed by using log-likelihood – taking (natural) log: 
©2015-2016 Emily Fox & Carlos Guestrin 
`(w) =
N
Y
i=1
P(yi | xi, w)
``(w) = ln
N
Y
i=1
P(yi | xi, w)

Machine Learning Specialization 
52 
The log trick, often used in ML…  
•  Products become sums: 
 
•  Doesn’t change maximum! 
- If ŵ maximizes f(w): 
- Then ŵ maximizes ln(f(w)):  
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
Insert next title slide before Slide 52, 
around 4:55 in 
PL7_DerivingtheGradient_1stEdit   
©2015-2016 Emily Fox & Carlos Guestrin 
53 

Machine Learning Specialization 
Expressing the log-likelihood 
54 
©2015-2016 Emily Fox & Carlos Guestrin 
VERY 
OPTIONAL 

Machine Learning Specialization 
55 
Using log to turn products into sums 
 
 
•  The log of the product of likelihoods becomes the sum of the logs: 
©2015-2016 Emily Fox & Carlos Guestrin 
``(w) = ln
N
Y
i=1
P(yi | xi, w)

Machine Learning Specialization 
56 
Rewriting log-likelihood 
•   For simpler math, we’ll rewrite likelihood with indicators: 
©2015-2016 Emily Fox & Carlos Guestrin 
``(w) =
N
X
i=1
ln P(yi | xi, w)
=
N
X
i=1
[1[yi = +1] ln P(y = +1 | xi, w) + 1[yi = −1] ln P(y = −1 | xi, w)]

Machine Learning Specialization 
Insert next title slide before Slide 54, 
around 7:33 in 
PL7_DerivingtheGradient_1stEdit   
©2015-2016 Emily Fox & Carlos Guestrin 
57 

Machine Learning Specialization 
Deriving probability that y=-1 given x 
58 
©2015-2016 Emily Fox & Carlos Guestrin 
VERY 
OPTIONAL 

Machine Learning Specialization 
59 
Logistic regression model: P(y=-1|x,w) 
•  Probability model predicts y=+1: 
•  Probability model predicts y=-1: 
©2015-2016 Emily Fox & Carlos Guestrin 
P(y=+1|x,w) =         1       .  
 
1 + e-w h(x) 
T 

Machine Learning Specialization 
Insert next title slide before Slide 55, 
around 9:15 in 
PL7_DerivingtheGradient_1stEdit   
©2015-2016 Emily Fox & Carlos Guestrin 
60 

Machine Learning Specialization 
Rewriting the log-likelihood 
61 
©2015-2016 Emily Fox & Carlos Guestrin 
VERY 
OPTIONAL 

Machine Learning Specialization 
62 
Plugging in logistic function for 1 data point 
©2015-2016 Emily Fox & Carlos Guestrin 
P(y = +1 | x, w) =
1
1 + e−w>h(x)
P(y = −1 | x, w) =
e−w>h(x)
1 + e−w>h(x)
``(w) = 1[yi = +1] ln P(y = +1 | xi, w) + 1[yi = −1] ln P(y = −1 | xi, w)

Machine Learning Specialization 
Insert next title slide before Slide 56, 
around 16:56 in 
PL7_DerivingtheGradient_1stEdit   
©2015-2016 Emily Fox & Carlos Guestrin 
63 

Machine Learning Specialization 
Deriving gradient of log-likelihood 
64 
©2015-2016 Emily Fox & Carlos Guestrin 
VERY 
OPTIONAL 

Machine Learning Specialization 
65 
Gradient for 1 data point 
©2015-2016 Emily Fox & Carlos Guestrin 
``(w) = −(1 −1[yi = +1])w>h(xi) −ln
⇣
1 + e−w>h(xi)⌘

Machine Learning Specialization 
66 
Finally, gradient for all data points 
•  Gradient for one data point: 
•  Adding over data points: 
©2015-2016 Emily Fox & Carlos Guestrin 
hj(xi)
⇣
1[yi = +1] −P(y = +1 | xi, w)
⌘

Machine Learning Specialization 
MOVE TO  
FULL BODY SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
67 

Machine Learning Specialization 
Summary of logistic  
regression classiﬁer 
68 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
MOVE TO  
HEAD SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
69 

Machine Learning Specialization 
70 
©2015-2016 Emily Fox & Carlos Guestrin 
Training 
Data 
Feature 
extraction 
ML  
model 
Quality 
metric 
ML algorithm 
y 
ŵ 
h(x) 
x 
P(y=+1|x,ŵ) =         1       .  
 
⌃ 
1 + e-ŵ h(x) 
T 

Machine Learning Specialization 
MOVE TO FULL 
BODY SHOT 
©2015-2016 Emily Fox & Carlos Guestrin 
71 

Machine Learning Specialization 
72 
What you can do now… 
•  Measure quality of a classiﬁer using 
the likelihood function 
•  Interpret the likelihood function as 
the probability of the observed data 
•  Learn a logistic regression model with 
gradient descent 
•  (Optional) Derive the gradient 
descent update rule for logistic 
regression 
©2015-2016 Emily Fox & Carlos Guestrin 

Machine Learning Specialization 
73 
Simplest link function: sign(z) 
©2015-2016 Emily Fox & Carlos Guestrin 
-∞ 
+∞ 
z 
0.0 
1.0 
sign(z) =
⇢+1
if z ≥0
−1
otherwise
sign(z) 
But, sign(z) only outputs -1 or +1, 
no probabilities in between 

Machine Learning Specialization 
74 
Finding best coeﬃcients 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
2 
1 
+1 
4 
1 
+1 
1 
1 
+1 
2 
1 
+1 
©2015-2016 Emily Fox & Carlos Guestrin 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 
3 
3 
-1 
2 
4 
-1 
0 
3 
-1 
0 
1 
-1 
-∞ 
+∞ 
Score(xi) = ŵTh(xi) 
0.0 
1.0 
P(y=+1|xi,ŵ) 

Machine Learning Specialization 
75 
   Choose w to make 
            
Increase probability y=+1 when 
 
If model good, should predict 
 
Quality metric: probability of data 
©2015-2016 Emily Fox & Carlos Guestrin 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
2 
1 
+1 
   Choose w to make 
           
Increase probability y=-1 when 
 
If model good, should predict  
 
x[1] = #awesome 
x[2] = #awful 
y = sentiment 
0 
2 
-1 
P(y=+1|x,ŵ) =         1       .  
 
⌃ 
1 + e-ŵ h(x) 
T 

Machine Learning Specialization 
76 
Maximizing likelihood  
(probability of data)  
Data 
point 
x[1] 
x[2] 
y 
Choose w to 
maximize 
x1,y1  
2 
1 
+1 
x2,y2 
0 
2 
-1 
x3,y3 
3 
3 
-1 
x4,y4 
4 
1 
+1 
x5,y5 
1 
1 
+1 
x6,y6 
2 
4 
-1 
x7,y7 
0 
3 
-1 
x8,y8 
0 
1 
-1 
x9,y9 
2 
1 
+1 
©2015-2016 Emily Fox & Carlos Guestrin 
Must combine into single  
measure of quality 

Machine Learning Specialization 
77 
Learn logistic regression model with  
maximum likelihood estimation (MLE) 
•  Choose coeﬃcients w that maximize likelihood: 
•  No closed-form solution è use gradient ascent  
 
©2015-2016 Emily Fox & Carlos Guestrin 
N
Y
i=1
P(yi | xi, w)


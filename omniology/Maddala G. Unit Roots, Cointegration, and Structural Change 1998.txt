
Unit Roots, Cointegration, and Structural Change
Time series analysis has undergone many changes in
recent years with the advent of unit roots and
cointegration. Maddala and Kim present a comprehensive
review of these important developments and examine
structural change. The volume provides an analysis of
unit root tests, problems with unit root testing,
estimation of cointegration systems, cointegration tests,
and econometric estimation with integrated regressors.
The authors also present the Bayesian approach to these
problems and bootstrap methods for small-sample
inference. The chapters on structural change discuss the
problems of unit root tests and cointegration under
structural change, outliers and robust methods, the
Markov switching model, and Harvey's structural time
series model. Unit Roots, Cointegration, and Structural
Change is a major contribution to Themes in Modern
Econometrics, of interest both to specialists and
graduate and upper-undergraduate students.
G. S. MADDALA is University Eminent Scholar at the
Ohio State University and one of the most distinguished
econometricians writing today. His many acclaimed
publications include Limited Dependent and Qualitative
Variables in Econometrics (Cambridge, 1983) and
Econometrics (McGraw-Hill, 1977) and Introduction to
Econometrics (MacMillan, 1988, 1992).
IN-MOO KIM is Professor of Economics at Sung Kyun
Kwan University, Seoul, Korea.


UNIT ROOTS
COINTEGRATION
AND STRUCTURAL CHANGE
G. S. Maddala
The Ohio State University
In-Moo Kim
Sung Kyun Kwan University
CAMBRIDGE
UNIVERSITY PRESS

CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore, Sao Paulo
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
Information on this title: www.cambridge.org/9780521582575
© Cambridge University Press 1998
This publication is in copyright. Subject to statutory exception
and to the provisions of relevant collective licensing agreements,
no reproduction of any part may take place without the written
permission of Cambridge University Press.
First published 1998
Sixth printing 2004
A catalogue record for this publication is available from the British Library
ISBN 978-0-521-58257-5 hardback
ISBN 978-0-521-58782-2 paperback
Transferred to digital printing 2007

To my parents
G. S. Maddala
To Jong Han, Jung Youn, and So Youn
In-Moo Kim


Contents
Figures 
page xii
Tables 
xiii
Preface 
xvii
Part I Introduction and basic concepts 
1
1 
Introduction 
3
References 
6
2 
Basic concepts 
8
2.1 
Stochastic processes 
8
2.2 
Some commonly used stationary models 
11
2.3 
Box-Jenkins methods 
17
2.4 
Integrated variables and cointegration 
20
2.5 
Spurious regression 
28
2.6 
Deterministic trend and stochastic trend 
29
2.7 
Detrending methods 
32
2.8 
VAR, ECM, and ADL 
34
2.9 
Unit root tests 
37
2.10 Cointegration tests and ECM 
39
2.11 Summary 
41
References 
42
Part II Unit roots and cointegration 
45
3 
Unit roots 
47
3.1 
Introduction 
47
3.2 
Unit roots and Wiener processes 
49
3.3 
Unit root tests without a deterministic trend 
60
3.4 
DF test with a linear deterministic trend 
65
vn

viii 
Contents
3.5 
Specification of deterministic trends 
72
3.6 
Unit root tests for a wide class of errors 
74
3.7 
Sargan-Bhargava and Bhargava tests 
82
3.8 
Variance ratio tests 
86
3.9 
Tests for TSP versus DSP 
87
3.10 Forecasting from TS versus DS models 
89
3.11 Summary and conclusions 
92
References 
92
4 
Issues in unit root testing 
98
4.1 
Introduction 
98
4.2 
Size distortion and low power of unit root tests 
100
4.3 
Solutions to the problems of size and power 
103
4.4 
Problem of overdifferencing: MA roots 
116
4.5 
Tests with stationarity as null 
120
4.6 
Confirmatory analysis 
126
4.7 
Frequency of observations and power of unit root tests 
129
4.8 
Other types of nonstationarity 
131
4.9 
Panel data unit root tests 
133
4.10 Uncertain unit roots and the pre-testing problem 
139
4.11 Other unit root tests 
140
4.12 Median-unbiased estimation 
141
4.13 Summary and conclusions 
145
References 
146
5 
Estimation of cointegrated systems 
155
5.1 
Introduction 
155
5.2 
A general CI system 
155
5.3 
A two-variable model: Engle-Granger methods 
156
5.4 
A triangular system 
160
5.5 
System estimation methods 
165
5.6 
The identification problem 
173
5.7 
Finite sample evidence 
175
5.8 
Forecasting in cointegrated systems 
184
5.9 
Miscellaneous other problems 
187
5.10 Summary and conclusions 
191
References 
191
6 
Tests for cointegration 
198
6.1 
Introduction 
198
6.2 
Single equation methods: residual-based tests 
198

Contents 
ix
6.3 
Single equation methods: ECM tests 
203
6.4 
Tests with cointegration as null 
205
6.5 
Multiple equation methods 
211
6.6 
Cointegration tests based on LCCA 
222
6.7 
Other tests for cointegration 
226
6.8 
Miscellaneous other problems 
228
6.9 
Of what use are cointegration tests? 
233
6.10 Conclusions 
241
References 
242
7 
Econometric modeling with integrated regressors 
249
7.1 
1(1) regressors not cointegrated 
249
7.2 
1(1) regressors cointegrated 
250
7.3 
Unbalanced equations 
251
7.4 
Lagged dependent variables: the ARDL model 
252
7.5 
Uncertain unit roots 
254
7.6 
Uncertain unit roots and cointegration 
256
7.7 
Summary and conclusions 
258
References 
258
Part III Extensions of the basic model 
261
8 
The Bayesian analysis of stochastic trends 
263
8.1 
Introduction to Bayesian inference 
264
8.2 
The posterior distribution of an autoregressive parameter 
266
8.3 
Bayesian inference on the Nelson-Plosser data 
268
8.4 
The debate on the appropriate prior 
271
8.5 
Classical tests versus Bayesian tests 
277
8.6 
Priors and time units of measurement 
277
8.7 
On testing point null hypotheses 
278
8.8 
Further comments on prior distributions 
284
8.9 
Bayesian inference on cointegrated systems 
287
8.10 Bayesian long-run prediction 
290
8.11 Conclusion 
291
References 
292
9 
Fractional unit roots and fractional cointegration 
296
9.1 
Some definitions 
296
9.2 
Unit root tests against fractional alternatives 
298
9.3 
Estimation of ARFIMA models 
300
9.4 
Estimation of fractionally cointegrated models 
302

Contents
9.5
9.6
10
10.1
10.2
10.3
10.4
10.5
10.6
10.7
10.8
11
11.1
11.2
11.3
11.4
12
12.1
12.2
12.3
12.4
12.5
12.6
12.7
12.8
12.9
12.10
12.11
Part
13
13.1
13.2
13.3
Empirical relevance of fractional unit roots
Summary and conclusions
References
Small sample inference: bootstrap methods
Introduction
A review of the bootstrap approach
The AR(1) model
Bootstrapping unit root tests
The moving block bootstrap and extensions
Issues in bootstrapping cointegrating regressions
Miscellaneous other applications
Conclusions
References
Cointegrated systems with 1(2) variables
Determination of the order of differencing
Cointegration analysis with 1(2) and 1(1) variables
Empirical applications
Summary and conclusions
References
Seasonal unit roots and seasonal cointegration
Effect of seasonal adjustment
Seasonal integration
Tests for seasonal unit roots
The unobserved component model
Seasonal cointegration
Estimation of seasonally cointegrated systems
Empirical evidence
Periodic autoregression and periodic integration
Periodic cointegration and seasonal cointegration
Time aggregation and systematic sampling
Conclusion
References
IV Structural change
Structural change, unit roots, and cointegration
Tests for structural change
Tests with known break points
Tests with unknown break points
303
305
306
309
309
309
322
325
328
332
335
336
336
342
342
348
355
358
359
362
364
365
366
371
375
376
378
379
381
381
382
383
387
389
390
390
391

Contents 
xi
13.4 A summary assessment 
398
13.5 Tests for unit roots under structural change 
399
13.6 The Bayesian approach 
402
13.7 A summary assessment of the empirical work 
407
13.8 Effect of structural change on cointegration tests 
410
13.9 Tests for structural change in cointegrated relationships 
411
13.10 Miscellaneous other issues 
414
13.11 Practical conclusions 
416
References 
418
14 
Outliers and unit roots 
425
14.1 Introduction 
425
14.2 Different types of outliers in time series models 
425
14.3 Effects of outliers on unit root tests 
428
14.4 Outlier detection 
437
14.5 Robust unit root tests 
440
14.6 Robust estimation of cointegrating regressions 
445
14.7 Outliers and seasonal unit roots 
448
14.8 Conclusions 
448
References 
449
15 
Regime switching models and structural time series models 454
15.1 The switching regression model 
454
15.2 The Markov switching regression model 
455
15.3 The Hamilton model 
457
15.4 On the usefulness of the MSR model 
460
15.5 Extensions of the MSR model 
463
15.6 Gradual regime switching models 
466
15.7 A model with parameters following a random walk 
469
15.8 A general state-space model 
470
15.9 Derivation of the Kalman filter 
472
15.10 Harvey's structural time series model (1989) 
475
15.11 Further comments on structural time series models 
477
15.12 Summary and conclusions 
479
References 
479
16 
Future directions 
486
References 
488
Appendix 1 A brief guide to asymptotic theory 
490
Author index 
492
Subject index 
500

Figures
2.1 
Correlogram of an AR(2) model 
16
2.2 
Examples of two AR(1) processes with a drift 
23
2.3 
The variances of xt and yt 
24
2.4 
The autocorrelations of Xt and yt 
25
2.5 
Cointegrated and independent 1(1) variables 
27
2.6 
ARIMA(O,1,1) and its components 
31
3.1 
Random walk and step function 
53
8.1 
Marginal posterior distributions of p when p = 1 
272
xn

Tables
2.1 
Regression of integrated variables 
32
3.1 
Critical values for Dickey-Fuller tests 
64
3.2 
Asymptotic distributions of the t-ratios for different DGPs 
71
3.3 
Critical values for the Schmidt-Phillips LM test 
85
3.4 
Nelson and Plosser's results 
89
4.1 
Critical values of DFmax statistics 
112
4.2 
Critical values for the Elliott-Rothenberg-Stock DF-GLS
test 
114
4.3 
Critical values for the Hwang-Schmidt DF-GLS test (t-test) 116
4.4 
Critical values for the KPSS test 
122
4.5 
Quantiles of the LS estimator in an AR(1) model with drift
and trend 
143
6.1 
Critical values for the ADF ^-statistic and Zt 
200
6.2 
Critical values for the Za 
200
6.3 
Response surface estimates of critical values 
201
6.4 
Critical values for the Harris and Inder test 
210
6.5 
Quantiles of the asymptotic distribution of the Johansen's
LR test statistics 
213
6.6 
Critical values of the LCCA-based tests 
224
7.1 
Features of regressions among series with various orders of
integration 
252
8.1 
Posterior probabilities for the Nelson-Plosser data 
270
12.1 Critical values for seasonal unit roots in quarterly data 
369
12.2 Critical values for seasonal unit roots in monthly data 
371
13.1 Asymptotic critical values for the diagnostic test 
412
xin


Nothing is so powerful as an idea whose time has come.
Victor Hugo
The Gods love the obscure and hate the obvious.
Brihadaranyaka Upanishad
Undue emphasis on niceties is a disease to which persons with mathe-
matical training are especially prone.
G. A. Barnard, "A Comment on E. S. Pearson's Paper,"
Biometrika, 1947, 34, 123-128.
Simplicity, simplicity, simplicity! I say, let your affairs be as two or three,
and not a hundred or a thousand. Simplify, simplify.
H. D. Thoreau: Walden


Preface
The area of unit roots, cointegration, and structural change has been an
area of intense and active research during the past decade. Developments
have been proceeding at a fast pace. However, almost all the books are
technically oriented and do not bring together the different strands of
research in this area. Even if many new developments are going to take
place, we thought it is time to provide an overview of this area for the
benefit of empirical as well as theoretical researchers. Those who are
doing empirical research will benefit from the comprehensive coverage
of the book. For those who are doing theoretical research, particularly
graduate students starting on their dissertation work, the present book
will provide an overview and perspective of this area. It is very easy
for graduate students to get lost in the intricate algebraic detail of a
particular procedure and lose sight of the general framework their work
fits in to.
Given the broad coverage we have aimed at, it is possible that we have
missed several papers. This is not because they are not important but
because of oversight and/or our inability to cover too many topics.
To keep the book within reasonable length and also to provide access-
ibility to a broad readership, we have omitted the proofs and derivations
throughout. These can be found by interested readers in the papers
cited. Parts of the book were used at different times in graduate courses
at the University of Florida, the Ohio State University, Caltech, State
University of New York at Buffalo, and Sung Kyun Kwan University in
Korea.
We would like to thank Adrian Pagan, Peter Phillips, and an anony-
mous referee for many helpful comments on an earlier draft. Thanks are
also due to professor Chul-Hwan Kim of Ajou University, Young Se Kim
of Sung Kyun Kwan University, and Marine Carrasso of the Ohio State
xvn

xviii 
Preface
University for their helpful comments. Responsibility for any remaining
errors is ours. We would also like to thank Patrick McCartan at the
Cambridge University Press for his patience in the production of this
book.
G. S. Maddala
The Ohio State University, U.S.A.
In-Moo Kim
Sung Kyun Kwan University, Seoul, Korea

Parti
Introduction and basic concepts
This part consists of two chapters. Chapter 1 is just an outline of the
book. Chapter 2 introduces the basic concepts: stochastic processes; sta-
tionarity, the different kinds of commonly used stationary models (MA,
AR, ARMA), Box-Jenkins methods; integrated variables and cointegra-
tion; spurious regression; deterministic and stochastic trend; detrending
methods; VAR, ECM, ADL models; tests for unit root and cointegration.
All these topics are pursued in subsequent chapters and some of the
statements made here (regarding unit root and cointegration tests) are
qualified in subsequent chapters. The point here is to explain what all
these terms mean.


1
Introduction
During the last decade, the econometric literature on unit roots and
cointegration has literally exploded. The statistical theory relating to
the first order autoregressive processes where the autoregressive param-
eter is equal to one (unstable process) and greater than one (explosive
process) was developed by Anderson (1959), White (1958, 1959), and
Rao (1961) (see Puller (1985) for a review). However, the econometric
literature on unit roots took off after the publication of the paper by
Nelson and Plosser (1982) that argued that most macroeconomic series
have unit roots and that this is important for the analysis of macroeco-
nomic policies.
Similar is the story on cointegration. Yule (1926) suggested that re-
gressions based on trending time series data can be spurious. This prob-
lem of spurious regressions was further pursued by Granger and Newbold
(1974) and this also led to the development of the concept of cointegra-
tion (loosely speaking, lack of cointegration means spurious regression).
Again, the pathbreaking paper by Granger (1981), first presented at a
conference at the University of Florida in 1980, did not catch fire until
about five years later, and now the literature on cointegration has ex-
ploded. As for historical antecedents, Hendry and Morgan (1989) argue
that Prisch's concept of multicollinearity in 1934 can be viewed as a
forerunner of the modern concept of cointegration.
The recent developments on unit roots and cointegration have changed
the way time series analysis is conducted. Of course, the publication
of the book by Box and Jenkins (1970) changed the methods of time
series analysis, but the recent developments have formalized and made
systematic the somewhat ad hoc methods in Box and Jenkins. Moreover,
the asymptotic theory for these models has been developed.
Traditionally, the analysis of time series consisted of a decomposition

4 
Introduction
of the series into trend, seasonal, and cyclical components. We can write
the time series Xt as
xt = Tt + St + Ct
where Tt = trend, St= seasonal, and Ct = the remaining component
which we might call the cyclical component. The trend and seasonal
were first removed and then the residual was explained by an elaborate
model. The trend, which is a long-term component was considered to
be beyond the scope of explanation, and so was the seasonal. Attention
thus focused entirely on the short-term dynamics as described by Ct>
The earliest approaches to trend removal consisted of regressing the
time series on t (if the trend is considered linear) and a polynomial
of t (if the trend is considered to be nonlinear). Elaborate methods
have been devised for the removal of the seasonal. These are described
in Hylleberg (1992), but one common method used in the analysis of
seasonally unadjusted data was to use seasonal dummies (see Lovell,
1963).
By contrast to the regression methods for trend removal and sea-
sonal adjustment, the methods suggested in Box and Jenkins (1970)
consisted of removing the trend and seasonal by successive differencing.
Define
Axt = xt - xt-i
A2xt = AAxt = xt- 2xt-i + xt-2
= Xt - 
Xt-4
xt - xt_i2
A linear trend is removed by considering Axt, a quadratic trend by con-
sidering A2xt. With quarterly data the seasonal is removed by consid-
ering A±xt. With monthly data the seasonal is removed by considering
Thus, there are two approaches to the removal of trend and seasonal:
(i) regression methods,
(ii) differencing methods.
During the 1980s there were two major developments. The first was a
discussion of these two methods in a systematic way and deriving tests
to determine which is more appropriate for a given series. The second
major development was the argument that trend and seasonal contain
important information and that they are to be explained rather than

Introduction 
5
removed. Methods have been devised to estimate long-run economic
relationships. If we are considering two time series, say xt and yt, then
the trend in xt may be related to the trend in yt (common trends) and the
seasonal in xt may be related to the seasonal in yt (common seasonals).
Thus modeling the trend and the seasonal should form an integrated
part of the analysis of the time series, rather than a concentration on
the short-run dynamics between xt and yt.
Two questions also arose in this context. If one is also considering the
problem of determining long-run relationships, how long a time series
do we need to consider? Is it 20 years or 100 years and is having 240
monthly observations better than having 20 yearly observations? Also,
if we are considering a span of 100 years, would the parameters in the
estimated relationships be stable over such a long period? This is the
problem of structural change. These are the issues that will be covered
in this book. In the following chapters these problems are discussed in
detail.
The book is divided into four parts.
Part I: Introduction and basic concepts
This consists of this chapter and chapter 2:
Chapter 2 introduces the basic concepts of ARM A models, unit roots
and cointegration, spurious regression, Vector Autoregression (VAR),
and Error Correction Models (ECM).
Part II: Unit roots and cointegration
This consists of chapters 3 to 7:
Chapter 3 discusses the different unit root tests. It also has an intro-
duction to Wiener processes that will repeatedly be used in the rest of
the book.
Chapter 4 discusses issues relating to the power of unit root tests,
tests using stationarity as the null, tests for MA unit roots, LM tests for
unit roots, and other important problems with unit root testing.
Chapter 5 discusses the different methods of estimation of cointegrated
systems.
Chapter 6 discusses different tests for cointegration. These use no
cointegration as the null. The chapter also covers tests using cointegra-
tion as the null.
Chapter 7 discusses the issues that arise in modeling with integrated
regressors.

6 
Introduction
Part III: Extensions of the basic model and alternative ap-
proaches to inference
This consists of chapters 8 to 12:
Chapter 8 is on Bayesian analysis of unit roots and cointegration and
the Bayesian approach to model selection.
Chapter 9 is on fractional unit roots and fractional cointegration.
Chapter 10 is on bootstrap methods which are alternatives to asymp-
totic inference in the preceding chapters.
Chapter 11 extends the analysis of the previous chapters to the case
of 1(2) variables. It discusses issues of testing 1(2) versus 1(1) and 1(1)
versus 1(2) as well as modeling systems with 1(2), 1(1) and 1(0) variables.
Chapter 12 is devoted to the analysis of seasonal data, tests for sea-
sonal unit roots, tests for seasonal cointegration, and estimation of sea-
sonally cointegrated systems.
Part IV: Structural change
Chapters 13 to 15 are devoted to analysis of structural change. Specifi-
cally, they discuss the effects of structural change on unit root tests and
cointegration tests:
Chapter 13 discusses structural change and unit roots.
Chapter 14 is on outlier problems and robust estimation methods. It
discusses the effects of different types of outliers on unit root tests, and
robust estimation methods in the presence of outliers.
Chapter 15 discusses regime switching models and structural time
series models.
Finally chapter 16 presents some avenues for further research.
Throughout the book, emphasis is on the intuition behind the different
procedures and their practical usefulness. The algebraic detail is omitted
in most cases because interested readers can refer to the original work
cited. We have tried to emphasize the basic ideas, so that the book will
be of guidance to empirical researchers.
References
Anderson, T.W. (1959), "On Asymptotic Distributions of Estimates
of Parameters of Stochastic Difference Equations," Annals of
Mathematical Statistics, 30, 676-687.
Box, G.E.P. and G.M. Jenkins (1970), Time Series Analysis Forecasting
and Control, Holden-Day, San Francisco.

References 
7
Fuller, W.A. (1985), "Nonstationary Autoregressive Time Series,"
Handbook of Statistics, 5, North-Holland Publishing Co., Ams-
terdam, 1-23.
Granger, C.W.J. (1981), "Some Properties of Time Series Data and
Their Use in Econometric Model Specification," Journal of
Econometrics, 16, 121-130.
Granger, C.W.J. and P. Newbold (1974), "Spurious Regression in
Econometrics," Journal of Econometrics, 2, 111-120.
Hendry, D.F. and M.S. Morgan (1989), "A Re-Analysis of Confluence
Analysis," Oxford Economic Papers, 44, 35-52.
Hylleberg, S. (ed.)(1992), Modeling Seasonality, Oxford University
Press, Oxford.
Lovell, M.C. (1963), "Seasonal Adjustment of Economic Time Series
and Multiple Regression Analysis," Journal of American Statis-
tical Association, 58, 993-1010.
Nelson, C.R. and C.I. Plosser (1982), "Trends and Random Walks in
Macroeconomic Time Series," Journal of Monetary Economics,
10, 139-162.
Rao, M. M. (1961), "Consistency and Limit Distributions of Estimators
of Parameters in Explosive Stochastic Difference Equations,"
Annals of Mathematical Statistics, 32, 195-218.
White, J.S. (1958), "The Limiting Distribution of the Serial Correla-
tion Coefficient in the Explosive Case," Annals of Mathematical
Statistics, 29, 1188-1197.
(1959), "The Limiting Distribution of the Serial Correlation Coeffi-
cient in the Explosive Case II," Annals of Mathematical Statis-
tics, 30, 831-834.
Yule, G.U. (1926), "Why Do We Sometimes Get Nonsense Correlations
Between Time Series? A Study in Sampling and the Nature of
Time Series," Journal of Royal Statistical Society, 89, 1-64.

2
Basic concepts
The purpose of this chapter is to introduce several terms that will be
used repeatedly in the subsequent chapters, and to explain their mean-
ing. The terms included are: stationarity, ARMA models, integrated
variables, Box-Jenkins methods, unit roots, cointegration, determinis-
tic and stochastic trends, spurious regression, spurious periodicity and
trend, vector autoregression (VAR) models, and error correction models
(ECMs).
2.1 Stochastic processes
Prom a theoretical point of view a time series is a collection of random
variables {Xt}. Such a collection of random variables ordered in time is
called a stochastic process. The word stochastic has a Greek origin and
means pertaining to chance. If it is a continuous variable, it is customary
to denote the random variable by X(i), and if t is a discrete variable, it
is customary to denote them by Xt. An example of continuous random
variables X(t) is the recording of an electrocardiogram. Examples of dis-
crete random variables Xt are the data of unemployment, money supply,
closing stock prices, and so on. We will be considering discrete processes
only, and so we shall use the notation Xt or X(t) interchangeably.
The probability structure of the sequence of random variable {Xt}
is determined by the joint distribution of a stochastic process. The
question arises, however, since T (time) is commonly an infinite set,
whether we need an infinite dimensional distribution to define the prob-
ability structure of the stochastic process. Kolmogorov (1933) showed
that when the stochastic process satisfies certain regularity conditions
the stochastic process can be described by a finite dimensional distribu-
tion. That is, under certain conditions the probabilistic structure of the

2.1 Stochastic processes 
9
stochastic process {Xt} is completely specified by the joint distribution
F(Xtl,... 
,Xtn) for all values of n (a positive integer) and any subset
(£1,..., tn) of T. One of the regularity conditions is the symmetry, that
reshuffling the ordering of the index does not change the distribution.
The other is the compatibility that the dimensionality of the joint dis-
tribution can be reduced by marginalization.
Since the definition of a stochastic process by the joint distribution
is too general, it is customary to define the stochastic process in terms
of the first and second moments of the variable, Xt. Given that, for
a specific t, Xt is a random variable, we can denote its distribution
and density functions by F(Xt) and f(Xt) respectively. The parametric
family of densities are determined by the following the first and the
second moments:
Mean 
/it = E(Xt)
Variance 
of = var(Xt)
Autocovariance 7*1,t2 = cov(Xt1,Xt2)
The distribution of a stochastic process is characterized by the first and
the second moments, and they are both functions of t. Note that if
Xt follows a normal distribution, the distribution of Xt is completely
characterized by the first and the second moments, which is called a
Gaussian process.
The fact that the unknown parameters //£, of, 7t1? t2 change with
t presents us with a difficult problem. There are (finite but still) too
many parameters to be estimated. However, we have just a sample of
size 1 on each of the random variables. For example, if we say that
the unemployment rate at the end of this week is a random variable,
we have just one observation on this particular random variable in a
week. There is no way of getting another observation, so we have what
is called a single realization. This feature compels us to specify some
highly restrictive models for the statistical structure of the stochastic
process. Given a single realization, we need to reduce the number of
parameters and the question is how to reduce the number of parameters
Vu °h 7*i, t2-
Reducing the number of parameters to be estimated can be done by
imposing certain restrictions. Restrictions come in two forms:
(i) stationarity: restrictions on the time heterogeneity of the process,
(ii) asymptotic independence: restrictions on the memory of the pro-
cess.

10 
Basic concepts
These two restrictions reduce the numbers of parameters to be estimated
and also facilitate the derivation of asymptotic results. We shall discuss
stationarity here but omit discussion of asymptotic independence. This
can be found in Spanos (1986, chapter 8).
Stationarity A time series is said to be strictly stationary if the joint
distribution of Xtl,..., 
Xtn is the same as the joint distribution of
Xtl+T,.. .,Xtn+T for all ti,... , £n, and r. The distribution of the sta-
tionary process remains unchanged when shifted in time by an arbitrary
value r. Thus the parameters which characterize the distribution of the
process do not depend on t, but on the lag r. The concept of stationar-
ity is difficult to verify in practice because it is defined in terms of the
distribution function. For this reason the concept of stationarity defined
in terms of moments is commonly preferred.
A stochastic process {Xt,t E T} is said to be Ith-order stationary if
for any subset (£i, t2,..., tn) of T and any r the joints moments are
where l\ H 
\-ln < I. Let us take positive integers for h,l2,... 
,ln and
/. When I = 1, i.e., h = 1
E(Xt) = E(Xt+T) = V> (a constant)
the process {Xt} is said to be first-order stationary. When I = 2, the
possible cases are (/i = l,/2 = 0), (/i = 2, l2 = 0), and (l\ = l,/2 = 1).
According to three cases the process {Xt} has its joint moments as
follows
E(Xt) = E(Xt+T) = {i (a constant)
E{Xl) = E(Xt+T) = a2 
(a constant)
cov(Xtl,Xt2) 
= cov(Xtl+T,Xt2+T) = jtut2 = lr
where t\ — t2 = r. The mean and variance of Xt are constant and the
covariances of Xt depend only on the lag or interval r = t\ — t2, not on
t\ or t2. The process Xt is said to be second-order stationary.
Second-order stationarity is also called weak or wide-sense or covari-
ance stationarity. In modeling time series, second-order stationarity is
the most commonly used form of stationarity. This is partly due to the
fact that for a normal (or Gaussian) stationary process, second-order
stationarity is equivalent to strict stationarity. If Xt follow a multivari-
ate normal distribution, since the multivariate normal distribution is

2.2 Some commonly used stationary models 
11
completely characterized by the first and second moments, the two con-
cepts of strict stationarity and weak stationarity are equivalent (recall
the case of n = 2). For other distributions this is not so.
In order to see how stationarity reduces the number of parameters,
let us consider a Gaussian process {Xt} and the parameters 6t for the
subset (£i,...,£n) of T. Without the assumption of stationarity the
joint distribution of the process {Xt} is characterized by the vector of
parameters
6= [jj,ti,cov(Xti,Xtj)] 
for ij = l,...,ra
which is a n + n
 2
+ 
x 1 vector. By imposing stationarity, as we have
seen above, the vector of parameters is reduced to
6= [/i,a2,7r]
which is a (n + 1) x 1 vector. A sizeable reduction in the number of
the unknown parameters is resulted in by imposing stationarity. Note,
however, that even in the case of stationarity the number of parameters
(especially 7T) increases as r increases, i.e., as the size of T increases.
This is the reason why we need more restriction - the memory of the
process which is about the meaningful size of r regardless of the size of
T.
2.2 Some commonly used stationary models
We shall now discuss some commonly used stationary processes. We
shall denote the autocovariance function by acvf and the autocorrelation
function by acf.
2.2.1 Purely random process
This is a discrete process Xt consisting of a sequence of independent
identically distributed (iid) random variables. It has a constant mean
and constant variance. Its acvf is given by
and the acf is given by
1 ifr = O

12 
Basic concepts
A purely random process is also called a white noise. A white-noise
process is a second-order stationary process and has no memory. If Xt
is also assumed to be normal, then the process is strictly stationary.
2.2.2 Moving-average (MA) processes
Suppose that {et} is a purely random process with mean zero and vari-
ance a2. Then a process {Xt} defined by
Pl6t-1 + • • •
is called a moving-average process of order q and is denoted by MA(q).
Since the es are unobserved variables, we scale them so that po = 1.
Since E{et) = 0 for all t, we have E(Xt) = 0. And et are independent
with a common variance a2. Further, writing out the expressions for Xt
and Xt-r in terms of the es and picking up the common terms (since
the es are independent), we get
) 
for r > q
Also considering cov(Xt, Xt+T), we get the same expressions as for 7(—T).
Hence 7(—T) = 7(T). The ac/ can be obtained by dividing 7(7") by
var(Xt). For the MA process, p(r) = 0 for T > q, that is, they are zero
for lags greater than the order of the process. Since 7(7") is independent
of £, the MA(q) process is weakly stationary. Note that no restrictions
on the Pi are needed to prove the stationarity of the MA process.
To facilitate our notation we shall use the lag operator L. It is defined
by VXt = Xt-j for all j . Thus LXt = Xt-uL2Xt 
= Xt-2,L~lXt 
=
X^-fi, and so on. With this notation the MA(q) process can be written
as (since /?0 = 1 )
Xt = (1 4- PiL + p2L2 + • • • + pqLq)et 
= P(L)et
The polynomial in L has q roots and we can write
Xt = (1 - TTIL)(1 - n2L)... 
(1 - 7TqL)et
where TTI, TT2, ..., TT^ are the roots of the equation
After estimating the model we can calculate the residuals from et =
[/^(L)]-1^ provided that [/3(L)]-1 converges. This condition is called
the invertibility condition. The condition for invertibility is that \iti\ < 1

2.2 Some commonly used stationary models 
13
for all i.1 This implies that an MA(q) process can be written as an
AR(oo) process uniquely.
For instance, for the MA(2) process
Xt = (l + ftL + 
ftL2)et 
(2.1)
TTI and 7T2 are roots of the quadratic equation z2 + P\z + /?2 = 0. The
condition |-7r» | < 1 gives
-ft ± ^ft2 - 4ft
This gives the result that ft and ft must satisfy
ft + ft > -1
ft - ft > -1 
(2.2)
Iftl < 1
The last condition is derived from the fact that /?2 = 7TI7T2, the product
of the roots. The first two conditions are derived from the fact that if
Pi - 4/?2 > 0, then ft - 4/?2 < (2 + /?i)2 or (3\ - 4/?2 < (2 - /?i)2. Under
the condition (2.2) the MA(2) process (2.1) can be written as an AR(oo)
uniquely.
Moving-average processes arise in econometrics mostly through trend
elimination methods. One procedure often used for trend elimination is
that of successive differencing of the time series Xt. If we have
Xt = ao + ait + c*2t2 + et
where et is a purely random process, successive differencing of Xt will
eliminate the trend but the resulting series is a moving-average process
that can show a cycle. Thus the trend-eliminated series can show a cycle
even when there was none in the original series. This phenomenon of
spurious cycles is known as the Slutsky effect (Slutsky, 1937).
2.2.3 Autoregressive (AR) processes
Suppose again that et is a purely random process with mean zero and
variance a2. Then the process Xt given by
Xt = aiXt-i + a 2X t_ 2 + • • • + otvXt-v + et 
(2.3)
1 An alternative statement often found in books on time series is that the roots of
the equation 1 + f3\z + foz2 + • • • + (3qzq = 0 all lie outside the unit circle.

14 
Basic concepts
is called an autoregressive process of order p and is denoted by AR(p).
Since the expression is like a multiple regression equation, it is called
regressive. However, it is a regression of Xt on its own past values.
Hence it is autoregressive.
In terms of the lag operator L, the AR process (2.3) can be written
as
(1 - aiL - a2L2 
apLp)Xt = et 
(2.4)
or
1
Xt =
- a2L2 
apD>) l
1
et
(l-7riL)(l-7r 2L)--(l-7r pL)
where TTI, TT2, ..., irp are the roots of the equation
z
v 
-
v
~
x
0
The condition is that the expansion of (2.4) is valid and the variance of
Xt is finite, that is \TTI\ < 1 for all i.
To find the acvf, we could expand (2.3), but the expressions are messy.
An alternative procedure is to assume that the process is stationary and
see what acf are. To do this we multiply equation (2.3) throughout
by Xt-T, take expectations of all the terms and divide throughout by
var(Xt), which is assumed finite. This gives us
p(r) = aip{r - 1) + • + aTp(r - p)
Substituting r = 1,2,... ,p and noting p(r) = p(—r) we get equations to
determine the p parameters ai, a2,..., ap. These equations are known
as the Yule-Walker equations.
To illustrate these procedures we will consider an AR(2) process
Xt = Oi\Xt—i -h cx2Xt—2 ~\~ i
TTI and n2 are the roots of the equation
z2 — OL\Z — a2 = 0
Thus |7Ti| < 1 implies that

2.2 Some commonly used stationary models 
15
This gives
ai + OL2 < 1
OL\ — a2 
> 1
< 
1 
(2.5)
(The conditions are similar to the conditions (2.2) derived for the in-
vert ibility of the MA(2) process.)
In the case of the AR(2) process we can also obtain the p(r) recursively
using the Yule-Walker equations. We know that p(0) = 1 and
1 — oc2
Thus
2
p(2) = 
aip(l) + OL2p{$$) = -— 
h oc2
\ - 
OL2
= aip(2) + a2p(l) = -^-r1 
~ H
and so on.
As an example, consider the AR(2) process
Here OL\ = 1.0 and a2 = —0.5. Note that conditions (2.5) for weak
stationarity are satisfied. However, since a\ + 4a2 < 0 the roots are
complex and p(r) will be a sinusoidal function. A convenient method to
compute p(r) is to use the Yule-Walker equations
p[r) = p(r - 1) - 0.5p(r - 2)
Note that p(0) = 1 and p(l) = ai/(l - a2) •= 0.6666. We then have
the autocorrelations for r = 2,3,..., 13 recursively. This method can be
used whether the roots are real or complex. Figure 2.1 shows a plot of
this correlogram.
2.2.4 Autoregressive moving-average (ARMA) 
processes
We will now discuss models that are combinations of the AR and MA
models. These are called ARM A models. An ARMA(p, q) model is
defined as
Xt = axXt-i + a2Xt-2 + • • • + apXt-p + et 4- /?iet-i + • • • + Pqet-

16
Basic concepts
Lags
Fig. 2.1. Correlogram of an AR(2) model
where St is a purely random process with mean zero and variance a2.
The motivation for these methods is that they lead to parsimonious
representations of higher-order AR(oo) or MA(oo) processes.
Using the lag operator L, we can write this as
4>(L)Xt = 0{L)et
where <f)(L) and 0(L) are polynomials of orders p and q, respectively,
defined as
<t>{L) = 1 -
- ... - 
apLp
For stationarity we require that the roots of <j>(L) = 0 lie outside the unit
circle. For the invertibility of the MA component, we require that the
roots of 8(L) lie outside the unit circle. For instance, for the ARMA(2,2)
process these conditions are given by equations (2.2) and (2.5). The acvf
and acf of an ARMA model are more complicated than for an AR or
MA model.
We will derive the acf for the simplest case: the ARMA(1,1) process
Xt =
-i + et +

2.3 Box-Jenkins methods 
YJ
In terms of the lag operator L this can be written as
or
Xt 
~
= 
[1 + (a + 0)L + a(a + /?)£2 + a2(a + /?)L3 + • • •]£*
Since {£*} is a purely random process with mean zero and variance a2
we get
varyJit) 
= 
[i -\- ycx -\- /J) -\- ct {a, -f- p) 
-f-'' '\cr
= 11 + I - a
2 J 
I - a 2
Also
cov(Xt, Xt_i) 
= 
[(a + f3)+ a(a + (3)2 + a3(a + f3)2
(a + /?)(! + a/3)
= 
T^
Hence
= cov(XX) 
(
var(Xt) 
1 + (32 -
Successive values of p(r)can be obtained from the recurrence relation
p(r) = ap(r — 1) for r > 2. For the AR(1) process p(l) = a. It can
be verified that p(l) for ARMA(1,1) process is > or < a depending on
whether (3 > 0 or < 0, respectively.
2.3 Box—Jenkins methods
The Box-Jenkins method is one of the most widely used methodologies
for the analysis of time series data. The influential work of Box and
Jenkins (1970) shifted professional attention away from the stationary
serially correlated deviations from deterministic trend paradigm toward
the ARIMA(p,d,q) paradigm. It is popular because of its generality;
it can handle any series, stationary or not, with or without seasonal
elements, and it has well-documented computer programs. It is perhaps
the last factor that contributed most to its popularity. Although Box

18 
Basic concepts
and Jenkins have been neither the originators nor the most important
contributors in the field of ARMA models (for earlier discussion, see
Quenouille, 1957), they have popularized these models and made them
readily accessible to everyone, so much so that ARMA models are often
referred to as Box-Jenkins models.
The basic steps in the Box-Jenkins methodology consist of the follow-
ing five steps.
1. Differencing to achieve stationarity How do we conclude whe-
ther a time series is stationary or not? We can do this by studying the
graph of the correlogram of the series. The correlogram of a stationary
series drops off as r, the number of lags, becomes large, but this is not
the case of a nonstationary series. Thus the common procedure is to
plot the correlogram of given series yt and successive differences Ay,
A2?/, and so on, and look at the correlograms at each stage. We keep
differencing until the correlogram dampens.
2. Identification of a tentative model Once we have used the differ-
encing procedure to get a stationary time series, we examine the correlo-
gram to decide on the appropriate orders of the AR and MA components.
The correlogram of a MA process is zero after a point; that of an AR
process declines geometrically. The correlograms of ARMA processes
show different patterns (but all dampen after a while). Based on these,
one arrives at a tentative ARMA model. This step involves more of a
judgmental procedure than the use of any clear-cut rules.
3. Estimation of the model The next step is the estimation of the
tentative ARMA model identified in step 2. The estimation of AR mod-
els is straightforward. We estimate them by ordinary least squares (OLS)
by minimizing the error sum of squares, E£2. In the case of MA models,
we cannot write the error sum of squares Es2 as simply a function of
the observed ys and the parameters as in the model. What we can do
is to write down the covariance matrix of the moving-average error and,
assuming normality, use the maximum likelihood method of estimation.
An alternative procedure suggested by Box and Jenkins is the grid-search
procedure. In this procedure we compute it by successive substitution
for each value of the MA parameters and choose the set of values of the
parameters that minimizes the error sum of squares E£2. For ARMA
models, again the problem is with the MA component, either use ML

2.3 Box-Jenkins methods 
19
methods or use the grid-search procedure for the MA component. Ans-
ley (1979) provides an algorithm for the exact likelihood of the mixed
autoregressive moving-average process.
4. Diagnostic checking When an AR, MA, or ARMA model has
been fitted to a given time series, it is advisable to check that the model
does really give an adequate description of the data. There are two
criteria often used that reflect the closeness of fit and the number of
parameters estimated. One is the Akaike Information Criterion (AIC)
and the other is Schwartz Bayesian Information Criterion (BIC). If p is
the total number of parameters estimated, we have
AIC(p) = nlog<j2 + 2p
and
BIC(p) = n log <T2 + p log n
Here n is the sample size. If RSS = ^ £t ls ^ne residual sum of squares,
then a2 = RSS/(n — p). If we are considering several ARMA models,
we choose the one with the lowest AIC or BIC. The two criteria can lead
to different conclusions.
In addition, we have to check the serial correlation pattern of the resid-
uals. Box and Pierce (1970) suggest looking at not just the first-order
autocorrelation but autocorrelations of all orders of the residuals. They
suggest calculating Q = N^2™=1 p2., where pT is the autocorrelation of
lag r and N is the number of observations in the series. If the model
fitted is appropriate, they argue that Q has an asymptotic x2 distribu-
tion with m — p — q degrees of freedom, where p and q are, respectively,
the orders of the AR and MA components. Although the Q statistics
are quite widely used by those using time series programs (there is no
need to list here the hundreds of papers, books, and programs that still
use them), they are not appropriate in autoregressive models (or models
with lagged dependent variables). The arguments against their use, that
the OLS estimates of the coefficients of lagged dependent variables are
inconsistent under the presence of serially correlated errors, are exactly
the same as those against the use of the DW statistic in such a situation.
Despite this inappropriateness of the Q statistic, the discussion of the
Q statistic in the time series literature all concentrate only on the "low
power" of the Q statistics. For further discussion of modifications of the
Q statistics (these modifications are also inappropriate) and some alt-

20 
Basic concepts
ernative Lagrangian multiplier (LM) test statistics, see Maddala (1992,
pp. 540-542).
5. Forecasting Suppose that we have estimated the model with n
observations. We want to forecast xn+k- This is called a A;-periods
ahead forecast. First we need to write out the expression for xn+k-
And then replace all future values xn+j(0 < j < k) by their forecasts
and en+j(j > 0) by zero (since its expected value is zero). We also
replace all £n-j(j 
> 0) by the predicted residuals. For example, from
the ARMA(2,2) model
Xt =
we have the expression for the /c-period ahead forecast
When k = 2, we replace xn by the observed xn and #n+i by the forecasts.
The forecasted unknown error £n+2 and £n+i are replaced by zero, while
en is replaced by the predicted residual
it = Zt — Oi\Zt~\ — OL2Zt-2
where zt = (1 — OL\L — a ^ I / 2 ) " 1 ^ .
2.4 Integrated variables and cointegration
In time series analysis we do not confine ourselves to the analysis of
stationary time series. In fact, most of the time series we encounter are
nonstationary.
Consider the following processes
yt 
= yt-i + vt
The error terms Ut and vt are assumed to be normally independently
identically distributed with mean zero and unit variance, Ut,vt ~ tin
(0,1), i.e., a purely random processes. Both Xt and yt are AR(1) models.
The difference between two models is that yt is a special case of an Xt
process when p = 1 and is called a random walk model. It is also
referred to as an AR(1) model with a unit root since the root of the
AR(1) equation is 1 (or unit). When we consider the statistical behavior
of the two processes by investigating the mean (the first moment), the

2.4 Integrated variables and cointegration
21
variance and autocovariance (the second moments), they are completely
different. Although the two processes belong to the same AR(1) class,
Xt is a stationary process, while yt is a nonstationary process.
The two stochastic processes can be expressed as the sum of the initial
observation and the errors by successive substitution
Xt = 
pXt-i + Ut = p(pXt-2 + Ut-l) + Ut = . . .
= 
pfx0 + ut + put-i H 
h p*" 1^
t-i
= 
pfxo + ^2 9%ut-%
2=0
Similarly, in the unit root case
t-i
2=0
Since both series are expressed as the sum of the initial observation
and the errors, it can be said that the autoregressive model has been
transformed to the moving-average form.
Suppose that the initial observations are zero, x0 = 0 and yo = 0.
The means of the two processes are
E{xt) = 0 and E(yt) = 0
and the variances are
*-! . 
i
var(xt) = 22p2lvar(ut-.i) 
—> 
-
2=0 
^
where —• means converge to asymptotically (as t —> oo) and
t-i
var(yt) = ^2var(vt-i) 
= *
2=0
The autocovariances of the two series are
[
t - l 
t+T-l
2=0 
2=0
t+T-l
/ 
J
2=0
and
7^ = E(ytyt+T) = E
t-l
_ 2=0
t+r—1
2=0
= 
(*-r)

22 
Basic concepts
since the errors are assumed to be iin and cov(ut,us) = 0,t ^ s. The
means of Xt and yt are the same, but the variances (including autocovari-
ances) are different. The important thing to note is that the variances
and the autocovariance of yt are functions of t, while those of xt con-
verge to a constant asymptotically. Thus as t increases the variance of
yt increases, while the variance of Xt converges to a constant.
The above example shows that the two processes Xt and yt have dif-
ferent statistical properties. The variance of the stationary stochastic
process Xt converges to a constant, while the variance of the random
walk process yt increases as t increases. Now if we add a constant to the
AR(1) model, then the means of two processes also behave differently.
Consider the AR(1) processes with a constant (or drift) as follows
xt 
= a + pxt-i + ut, 
\p\ < 1
yt 
= a + yt-i + vt
The successive substitution yields
t 
t
i=0 
i=0
and
t-i
— 
4- / 4- \"^ 
(2 6)
i=0
Note that yt series contains a (deterministic) trend t. If the initial ob-
servations are zero, xo = 0 and yo = 0, then the means of two processes
are
E(yt) 
= at
but the variances and the autocovariances are the same as those derived
from the AR(1) model without the constant. By adding a constant to
the AR(1) processes, the means of two processes as well as the variances
are different. Both the mean and variances of yt are time varying, while
those of xt are constant.
To illustrate these properties, we generate the 150 observations of
xt and yt with a = 0.5 and p = 0.9. The innovations ut and vt are
generated by using a pseudo-random number generator. Figure 2.2 show
the typical shape of the two stochastic processes. If we slice the time
domain with some windows, say r = 30, we can find that the process

2.4 Integrated variables and cointegration
23
Stationary AR(1)
Random walk
1 0 
20 
40 
60 
80 
100 
120 
140 
160
Time
Fig. 2.2. Examples of two AR(1) processes with a drift
xt passes the mean of the xt process (E(xt) = 0) at least once, while
the process yt does not. The process xt has a force to converge toward
the mean (that is, it is mean-reverting) and randomly fluctuates around
the mean (no systematic changes). On the other hand, the process yt
increases systematically as t —> oo (sometimes systematically decreases)
and there is no force to move it toward its mean.
The variances of the two series are computed after the first 20 periods.
Figure 2.3 illustrates the variances of the two series after the first 20
periods. The variance of yt increases as t increases, while the variance
of xt converges to a constant (1/(1 — p) = 10) after t > 70.
Figure 2.4 shows the correlograms of Xt and yt. As we have seen
thus we can expect
7o
as r —> oo since \p\ < 1. For a nonstationary series yt, since

24
Basic concepts
Stationary AR(1)
Random walk
20 
40
140
160
Fig. 2.3. The variances of xt and
the values of pv
r will not come down to zero except for a very large value
of the lag.
Since the variance of the nonstationary series is not constant over
time (not covariance stationary or just nonstationary), the conventional
asymptotic theory cannot be applied for these series. One of the easiest
ways to analyze those series is to make those series stationary by differ-
encing. In our example, the random walk series yt can be transformed
to a stationary series by differencing once
&yt = yt- yt-i = (l— L)yt = et
where L is a lag operator. Since the error St is assumed to be indepen-
dently normal, the first difference of yt is stationary. The variance of
Ayt is constant over the sample period.
When the nonstationary series can be transformed to the stationary
series by differencing once, the series is said to be integrated of order 1
and is denoted by 1(1). If the series needs to be differenced k times to be
stationary, then the series is said to be I(k). In our example, yt is 1(1)
variable, since the series needs to be differenced once to be stationary,
while Xt is the 1(0) variable. The I(k) series (k ^ 0) is also called a
difference-stationary process (DSP). When AdXt is a stationary series

2.4 Integrated variables and cointegration
25
Stationary AR(1)
Random walk
' 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38
Lags
Fig. 2.4. The autocorrelations of xt and yt
that can be represented by an ARMA(p, q) model, we say that Xt is an
autoregressive integrated moving-average (ARIMA) process. Since the
number of differences is equal to the order of integration, Xt is denoted
as ARIMA(p, d, q) process.
Another important class is the trend-stationary process (TSP). Con-
sider the series
zt = a + 6t 4- et
(2.7)
The mean of zt is E(zt) = a + 6t and is not constant over the sample
period, while the variance of Zt is var(zt) = o2 and constant. Although
the mean of zt is not constant over the period, it can be forecasted
perfectly whenever we know the value of t and the parameters a and
6. In this sense it is stationary around the deterministic trend t and zt
can be transformed to stationarity by regressing it on time. Note that
both the DSP model equation (2.6) and the TSP model equation (2.7)
exhibit a linear trend, but the appropriate method of eliminating the
trend differs.
Most econometric analysis is based on the variance and covariance
among the variables. For example, the OLS estimator from the regres-
sion yt on xt is the ratio of the covariance between yt and xt to the

26 
Basic concepts
variance of Xt. Thus if the variances of the variables behave differently,
the conventional asymptotic theory cannot be applicable. When the
order of integration is different, the variance of each process behaves
differently. For example, if yt is an 1(0) variable and xt is 1(1), the
OLS estimator from the regression yt on Xt converges to zero asymp-
totically, since the denominator of the OLS estimator, the variance of
xt, increases as t increases, and thus it dominates the numerator, the
covariance between Xt and yt. That is, the OLS estimator does not have
an asymptotic distribution. (It is degenerate with the conventional nor-
malization of y/T.) We need to have the normalization of T rather than
that of y/T.
Cointegration
An important property of 1(1) variables is that there can be linear com-
binations of these variables that are 1(0). If this is so then these variables
are said to be cointegrated. The concept of cointegration was introduced
by Granger (1981). Suppose that we consider two variables yt and xt
that are 1(1). Then yt and xt are said to be cointegrated if there exists
a /? such that yt — j3xt is 1(0). This is denoted by saying that yt and xt
are CI(1,1). More generally, if yt is I(d) and xt is I(d), then yt and xt
are CI(d,b) if yt — (3xt is I(d — b) with b> 0. What this mean is that
the regression equation
yt = pxt + ut
makes sense because yt and Xt do not drift too far apart from each other
over time. Thus, there is a long-run equilibrium relationship between
them. If yt and xt are not cointegrated, that is yt — /3xt = ut is also 1(1),
then yt and xt would drift apart from each other over time. In this case
the relationship between yt and Xt that we obtain by regressing yt and
Xt would be spurious. We shall discuss spurious regression in the next
section.
To fix idea let us consider a simple system of equations
xt = ut, 
ut = ut-i 
+eu
yt + Oixt = vu 
vt = pvt-i + e2t
In this system of equation, Xt and yt are 1(1) variables regardless of
the value of p. If p = 1, then the linear combination yt + axt ~ ^(1)>
thus Xt and yt are two independent random walks. If \p\ < 1, then
yt + ocxt ~ ^(0), thus Xt and yt are cointegrated. In order to see how
two independent random walk variables and two cointegrated variables
drift apart from each other over time, we draw the typical shapes of

2.4 Integrated variables and cointegration
27
-
—
-
• M
Random walk X
Cointegrated Y
Random walk Y
4 ft
-
-
1 0 
20 
40 
60 
80 
100 
120 
140 
160 
180 
200
Time
Fig. 2.5. Cointegrated and independent 1(1) variables
two cases based on the 100 observations of xt and yt with a = — 1 and
p = 0.8,1. The errors ut and v* are generated from iin(0,1). Figure
2.5 shows the typical shapes of three stochastic processes, where xt is a
1(1) variable and yt is an independent 1(1) variable (when p = 1) and a
cointegrated 1(1) variable (when p = 0.8 and a = — 1). Two cointegrated
1(1) variables xt and yt (solid line and dashed line) show some tendency
for the two series not to drift too far apart (or move together), while
two independent 1(1) variables xt and yt (solid line and dotted line) do
not have such tendency.
The concept of cointegration can be extended for the higher order of
integrated variables. We can have 1(2) variables that are cointegrated
to produce an 1(1) variable. When dealing with 1(2) variables, different
types of cointegration can occur. Firstly, linear combinations of 1(2)
variables can be 1(1) or even 1(0) and, secondly, some linear combina-
tions of 1(1) variables, can cointegrate with first-differences of the 1(2)
variables to produce an 1(0) variable. In the modeling of demand for
money equations, if m< = log of nominal money, yt = log of income,
and pt = log of prices, it has been found that rat and pt are possibly
1(2) and real money (ra^ — Pt) and velocity (mt — Pt — yt) a r e possibly

28 
Basic concepts
2.5 Spurious regression
Consider two uncorrelated random walk processes
Vt = Vt-i + uu 
ut ~ iin(Q, al)
xt = Xt-i + Vt, vt ~ wn(0, cr^)
where itt and vt are assumed to be serially uncorrelated as well as mu-
tually uncorrelated. And consider the regression
Vt = Po + Pixt + et
Since yt and xt are uncorrelated random walk processes, we would expect
that the R2 from this regression would tend to zero. However, this is not
the case. The parameter (3i detects correlation and Yule (1926) showed
long ago that spurious correlation can persist even in large samples in
nonstationary time series. If the two time series are growing over time,
they can be correlated even if the increments in each series are uncorre-
lated. Thus, we have to be cautious when interpreting regressions with
1(1) variables.
This point was also illustrated in Granger and Newbold (1974) who
present some examples with artificially generated data where the errors
ut and vt were generated independently so that there was no relationship
between yt and xt, but the correlations between yt and yt-i> and xt and
Xt-i were high. The regression of y on x gave a high R2 but a low
Durbin-Watson (DW) statistic. When the regression was run in first
differences, the R2 was close to zero and the DW statistic was close to
2, thus demonstrating that there was no relationship between y and x
and that the R2 obtained was spurious.
Phillips (1986) shows that with two independent random walk pro-
cesses with no drift (like the ones we have considered) the least squares
regression
$
i
t 
(2.8)
leads to a divergent estimator fio and convergence of f}\ to a random
variable. By showing the limiting distributions of the OLS estimator and
test statistics, including t and DW statistics and R2, Phillips explains the
same results found by the simulation of Granger and Newbold (1974).
Entorf (1992) shows that the results are altered if we consider inde-
pendent random walk processes with drifts. Suppose that we have
yt = ay + yt-i + ut
xt = 
ax+xt-i+vt

2.6 Deterministic trend and stochastic trend 
29
then Entorf shows that in the regression (2.8)
where -^ denotes convergence in probability. Thus P\ converges to a
constant rather than a random variable. Entorf also finds that /3o has
a divergent distribution. Thus, the results from spurious regression de-
pend on whether we consider random walk processes with drifts or with
no drifts.
2.6 Deterministic trend and stochastic trend
As we have seen, integrated variables exhibit a systematic variation. But
the variation is hardly predictable, though the variation is systematic.
This type of variation is called a stochastic trend. On the other hand,
trends which are completely predictable (if we know the coefficient of
time) are known as deterministic trends. The specification of a deter-
ministic trend can be any functional form of time. For example, the
deterministic trend DTt can be any of following:
DTt = O(zero), c(constant), a + ^(linear form)
or
p
2^ Pit1 (polynomial time trend)
i=0
or
ao + Pot , t = l,...,ra
ai + Pit , t = m + l,...,T 
(segmented trend)
To fix ideas about the deterministic trend and the stochastic trend, let
us consider the following ARIMA(O,1,1) model with a drift (a constant
term)
Ayt = a + et + ^et-i
where et is assumed to be iid errors. Let y0 = eo = 0 so that yt can be
written by successive substitution as
yt =
yt-2 + et-i + jet-2) + et
t-i

30 
Basic concepts
t
Letting
DTt 
= 
at
STt 
= 
(1 + 7
Ct 
= 
--yet
we can rewrite yt as
yt = DTt + Zt = DTt + STt + Ct 
(2.9)
Here DTt is a deterministic trend in yt, and Zt is the noise function or
stochastic component of yt. The noise function Zt can be decomposed
as the sum of the stochastic trend STt and the cyclical component Ct.
The cyclical component is assumed to be a mean-zero stationary process.
The stochastic trend incorporates all random shocks (ei to et) that have
permanent effects on the level of yt. The sum of the deterministic trend
DTt and the stochastic trend STt is the overall trend and the permanent
component of yt.
If we denote the permanent component of yt as y\, then yt can be
rewritten as
yt = yPt+Ct
where the permanent component of yt is y\ = DTt 4- STt, the sum of
the deterministic trend and the stochastic trend. It can be shown that
the permanent component of yt is a random walk with drift such that
2 = 1
2 = 1
To see the typical shape of each component we generate the ARIMA
(0,1,1) series by setting a = 0.008 and 7 = 0.3. The innovations et are
generated from a normal distribution of (0, 0.0532). Figure 2.6 shows
the typical shape of series and each component. The solid line is the
graph of the generated ARIMA(0,1,1) series. The dashed line is the
deterministic trend DTt = 0.008t, the short-dashed line is the stochastic

2.6 Deterministic trend and stochastic trend
31
— ARIMA(O,1,1)
— Deterministic trend
- - Stochastic trend
Cyclical component
1 0 
20 
40 
60 
80 
100 
120 
140 
160
Time
Fig. 2.6. ARIMA(O,1,1) and its components
trend STt = l - 3 ^ i = 1 et, and the dotted line is the cyclical component
Ct = —0.3e£. The sum of two trend and the cyclical component is the
generated ARIMA(O,1,1) series.
The decomposition of the noise function Zt into STt and Ct as we have
seen in equation (2.9) can be extended to any ARIMA(p, l,q) model.
Beveridge and Nelson (1981) show that any ARIMA(p, 1, q) model can be
represented as a stochastic trend plus a stationary component. The noise
function Zt is assumed to be described by an autoregressive-moving-
average process
A(L)Zt = B(L)et
where A(L) and B(L) are polynomials in the lag operator L of order
p and q, respectively, and et is assumed to be a sequence of iid errors.
Suppose that the polynomial A(L) has a unit root, we can write
A{L) = (l-L)A*{L)
where A* (L) has roots strictly outside the unit circle. The first-difference
of the noise function is
- L)A*(L)Zt = A*(L)AZt = B(L)et

32
Basic concepts
Table 2.1. Regression of integrated variables
xt \ yt
Deterministic
Stochastic
Deterministic
Regression valid
Spurious regression
Stochastic
Spurious regression
Spurious regression
unless yt and xt
are cointegrated
AZt 
= 
A^
and
where i/>*(L) = (1-L)~1[
for both sides of (2.10) yields
zt =
(2.10)
. Applying the operator (1-L)" 1
Thus, any ARIMA(p, l,g) model, here the noise function Zt, can be
decomposed into a stochastic component STt and a cyclical component
ct.
In the previous sections we talked of spurious regressions arising from
trending variables. Whether such regressions are spurious or not depends
on the type of trend - whether it is deterministic or stochastic. Table 2.1
shows when the regressions of yt on xt are valid. In the case of random
walk processes with drifts, considered by Entorf, note that both yt and
xt can be expressed as a function of time plus a random walk process
with no drift.
2.7 Detrending methods
The method used for detrending depends on wether the time series is
TSP (trend-stationary process or a process that is stationary around a
trend) or DSP (difference-stationary process or a process that is sta-
tionary in first-differences). We shall be discussing problems of distin-
guishing between TSP and DSP in the next two chapters. The empirical
evidence on this is mixed. One summary conclusion that emerges from

2.7 Detrending methods 
33
the voluminous empirical work is that the evidence in favor of determin-
istic trends is stronger for real variables than for nominal variables.
There have been several papers that have studied the consequences
of underdifferencing or overdifferencing. If the time series is DSP and
we treat it as TSP, this is a case of underdifferencing. If the time series
is TSP, but we treat it as DSP, we have a case of overdifferencing.
However, the serial correlation properties of the resulting errors from
the misspecified processes need to be considered. For instance, if the
regression relationship is correctly specified in first differences, i.e.
Ayt = (3Axt + et
this implies that
yt = a + (3xt + ut
where ut = et+et-\-\ 
is serially correlated and nonstationary. On the
other hand, if the regression relationship is correctly specified in levels,
i.e.
yt = a + f)xt + vt
this implies that
Ayt = (3Axt +vt 
-vt-i
The errors follow a noninvertible moving-average process. The only
question is whether OLS estimation of this equation with first-order
MA errors leads us astray.
Plosser and Schwert argue that even if the MA coefficient is somewhat
underestimated, the sampling distribution of /? does not lead frequently
to incorrect conclusions, and hence "the cost of overdifferencing may
not be large when care is taken to analyze the properties of regression
disturbances." (1978, p. 643)
Nelson and Kang (1984) list several ways in which investigators would
be led to misleading results if they estimate underdifferenced relation-
ships. But these results hold if we do not correct for serial correlation
in the errors. Their results on pages 79-80 show that the consequences
of underdifferencing are not as serious if serial correlation in the errors
is taken into account. Plosser and Schwert (1978, p. 638) argue that
"the real issue is not differencing but an appropriate appreciation of the
role of the error term in regression models." This point is discussed
further in McCallum (1993), with reference to regression models with
lagged dependent variables. Note that in all this discussion we have
been considering DSP without trend and TSP with a linear trend.

34 
Basic concepts
There is also some discussion of detrending on the periodic properties
of the detrended series. Nelson and Kang (1981) argue that if the true
process is DSP (with errors not exhibiting any cycles) and trend removal
is done by regression on time (treating it as TSP) then the detrended
series exhibits spurious periodicity.
2.8 VAR, ECM, and ADL
The vector autoregressive (VAR) model is just a multiple time series
generalization of the AR model. The multiple time series generalization
of the ARMA model is the VARMA model but we shall not consider it
here. The VAR model has been popularized by Sims (1980) and it also
forms a starting point for the analysis of cointegrating regression. In
matrix notation, the VAR model for k variables can be written as
where Y{ = (ylt, y2t, - •., Vkt) and Ai, A 2,..., Ap are k x k matrices and
Ut is a A:-dimensional vector of errors. With E(ut) = 0 and E is positive
definite. More compactly the VAR model can be represented as
Yt = A(L)Yt + Ut
where L is the lag operator. Since the VAR model is nothing but the
stacked form of stationary AR(p) models and the regressors are the same
for all the equations, the estimation of the VAR model is straightforward.
The maximum likelihood estimator (MLE) reduces to the OLS estimator
for each equation in the VAR model. The MLE of E is also provided by
the OLS residuals Ut giving E = E(utu's).
The above results apply only for the unrestricted VAR model. In
practice it has been found that the unrestricted VAR model gives very
erratic estimates (because of high multicollinearity among the explana-
tory variables) and several restricted versions have been suggested. Also,
when some of the variables in Yt are 1(1), then one needs to use them in
first-differences. If some of these, 1(1) variables are cointegrated, then
this imposes further restrictions on the parameters of the VAR model.
These problem will be discussed in subsequent chapters.
The error correction model (ECM), first introduced into the econo-
metric literature by Sargan (1964) and popularized by Davidson et al.
(1978) has been a viable alternative to the VAR model. For some time
in the 1980s, the American econometricians were all estimating VARs
and the European econometricians were all estimating ECMs. There

2.8 VAR, ECM, and ADL 
35
are several interpretations of the error correction model and these are
discussed in Algoskoufis and Smith (1991). The main characteristic of
ECMs as compared with the VARs is the notion of an equilibrium long-
run relationship and the introduction of past disequilibrium as explana-
tory variables in the dynamic behavior of current variables. The recent
revival in the popularity of the ECMs has been based on the demonstra-
tion by Granger and Weiss (1983) that if two variables are integrated
of order 1, and are cointegrated, they can be modeled as having been
generated by an ECM.
The ECM links the realized value yt to its target value y% = f3'zt. In
its simplest form, it can be written as
Ayt = AiAy; + A2(yt*_i - yt-i)
where Ai > 0, A2 > 0 . The last term represents past disequilibrium.
The partial adjustment model is given by
Aj/t = Kvt - yt-i) = AA?/t* + Kvt-i - yt-i)
Thus the partial adjustment model corresponds to the ECM with Ai =
A2-
Another class of models often considered is the autoregressive dist-
ributed lag (ADL) model discussed in Hendry, Pagan, and Sargan (1984).
A general ADL model with p regressors, m lags in y, and n lags in each
of the p regressors is denoted by ADL(m, n;p). It is given by
t=l 
j=l 
i=0
Such models are also called dynamic linear regression (DLR) models.
Consider the simplest ADL(1,1;1) model
yt = a0 + «i2/t-i + A)#t + Pixt-i + et 
(2.11)
where it is assumed that et ~ iid(0, a2) and |c*i| < 1. We shall show the
connection between this and the ECM model. In long-run equilibrium
yt = yt-\ and xt = #t-i, then we can write
Thus, the long-run response to y of a change in x is given by

36 
Basic concepts
Now write the equation (2.11) as
Vt ~ 2/t-i = ao + (ai - l)yt-i + A) (a* ~ £t-i) + (A>
Writing /?0 + /?i = fc(l — OL\) we get
Ayt = a0 + (ai - l)(yt-i - kxt-i) + A) Ax* + et 
(2.12)
Note that (yt-i — kxt-i) is a last periods disequilibrium. Thus (2.12) is
the ECM that is implied by the ADL(1,1;1) model.
If we write (2.12) as
+ £t 
(2.13)
then clearly an estimate of the long-run response k is given by
k = <7i/7i
Equation (2.13) is known as the Bardsen (1989) transformation.
An alternative transformation of the same equation is that of Bewley
(1979) which is as follows: define A = (1 — ai)" 1 so that the long-run
response k = A(/?o + /?i). Now write the equation (2.11) as follows
(yt - otiyt) = «o - <*i(yt - 
yt-i)
or
(1 - ai)yt = a0- «i Ayt -h (/?o +
or
yt = Aa0 - Aai Ayt + A(/30 + Pi)xt - A/?i Axt 4- Aet 
(2.14)
In this equation the coefficient of xt is the long-run response. However,
since A^ is correlated with et (because it involves yt) OLS estimation
of this equation gives inconsistent estimates. Hence the equation has to
be estimated by instrumental variable (IV) methods. Note that (2.11),
(2.12), (2.13), and (2.14) are all equivalent transformations of the same
equation, and hence should give the same estimates of the long-run par-
ameter k. That (2.11) and (2.13) give the same estimates is fairly obvious
given that both the equations can be estimated by OLS and the param-
eters are simple linear transforms. What is not obvious is (the result is
proved in Bewley (1979, pp. 69-73) and Wickens and Breusch (1988))
that the IV estimation of (2.14) gives the same estimates and the same
standard errors as the OLS estimation of (2.11) provided we use the
explanatory variables in (2.11), that is, (yt-ij^tj^t-i) a s instrumental
variables.

2.9 Unit root tests 
37
One other point to note is that we can write equation (2.12) as
A> + ft - l)a;t-i + et 
(2.15)
or
Note that the estimate of the error correction term, namely (a\ — 1), is
the same in both (2.12) and (2.15). What we have done is, we assumed
the long-run response in the error correction term to be equal to 1,
and added Xt-i as an explanatory variable. This simple but important
result will be used when we come to cointegration tests based on ECM.
The generalization of these results to a general ADL(m, n\p) model are
straightforward except for some extra notation and will not be pursued
here.1
2.9 Unit root tests
As discussed earlier, the question of whether to detrend or to difference a
time series prior to further analysis depends on whether the time series
is trend-stationary (TSP) or difference-stationary (DSP). If the series
is trend-stationary, the data generating process (DGP) for yt can be
written as
Vt = 7o + 7i* + et
where t is time and et is a stationary ARM A process. If it is difference-
stationary, the DGP for yt can be written as
yt = a0 + yt-i + et
where et is again a stationary ARMA process. If the ets are serially
uncorrelated, then this is a random walk with drift a0.
Following Bhargava (1986), we can nest these two models in the fol-
lowing model
Vt = 7o + 71* + ut
ut 
= put-i + et
so that
yt = 7o + 7i* + p[yt-i ~ 7o - 7i(* - 1)] + et 
(2.16)
1 For details see chapter 2 of Banerjee et al. (1993).

38 
Basic concepts
where e* is a stationary process. If \p\ < 1, yt is trend-stationary. If
\p\ = 1, yt is difference-stationary. Equation (2.16) can be written as
yt = A) + /3it 4- pyt-i + e* 
(2.17)
or
Aj/t = A) + A* + (p ~ l)yt-i + et 
(2.18)
where /?0 = 7o(l — p)47ip and /?i = 7i(l — p) . Note that if p = 1, then
AL = 0. If we start with the model
Vt — 7o + ut
ut 
= 
put-i + et
then we get
yt = 7o(l - p) 4- pyt-i + et
or
Vt = Po + PUt-i +et
with /?o = 0 if p = 1. If we have a quadratic trend, then equation (2.16)
would be
yt = 7o + 7i* + 72*2 + p[yt-i - 7o - 7i(* - 1) - 72(* - I)2] + et
which can be written as
Vt = Po + Pit 4- /32*2 4- pyt-i 4 et 
(2.19)
where
A) = 
7o(l - p ) + (7i
and
02 = 72(1 - P )
Thus, if p = 1, then /32 = 0. We shall discuss the problems caused by
this in later chapters.
It is customary to test the hypothesis p = 1 against the one-sided
alternative \p\ < 1. This is called a unit root test. One cannot, however,
use the usual t-test to test p = 1 in equation (2.17) because under the
null hypothesis, yt is 1(1), and hence the ^-statistic does not have an
asymptotic normal distribution. The relevant asymptotic distribution,
based on Wiener processes, will be discussed in chapter 3.

2.10 Cointegration tests and ECM 
39
Equation (2.16) and (2.17) have been derived by considering the issue
of testing whether a series is difference-stationary or trend-stationary.
By contrast the tests for unit roots developed by Dickey (1976), Puller
(1976), and Dickey and Fuller (1979), commonly known as Dickey-Fuller
tests are based on a simple autoregression with or without a constant or
time trend. They are based on testing p = 1 in the equations
Vt = pyt~i+et 
(2.20)
Vt = A> + P2/t-i+et 
(2.21)
Vt = 
A) + 01* + «/t-i + et 
(2.22)
However, as we noted earlier, the Bhargava formulation implies that
in equation (2.21), /?o = 0 if p = 1 and in equation (2.22), (3\ = 0 if
p = 1. No such restrictions are imposed in the formulations in (2.21)
and (2.22). As a consequence, the parameters in equations (2.21) and
(2.22) have different interpretations under the null and the alternative
(see Schmidt and Phillips, 1992). For instance, in equation (2.21), under
the null hypothesis p = 1; (3Q represents the coefficient of trend, whereas
under the alternative yt is stationary around the level /?o/(l ~ p) (see
the discussion in section 1.6 earlier). Similarly, by successive substi-
tution, we can show that in equation (2.22), under the null hypoth-
esis p = 1, the parameters /?o and (3i represent coefficients of t and
t2 in a quadratic trend, whereas under the alternative they represent
the level and the coefficient of t in a linear trend. Because of these
problems, the Bhargava-type formulation is preferred in unit root test-
ing.
2.10 Cointegration tests and ECM
As we mentioned earlier (section 2.4), if a linear combination of 1(1)
variables is stationary or 1(0), then the variables are said to be coin-
tegrated. Suppose we have a set of k variables yt which are all 1(1)
and j3'yt = ut is 1(0), then (3 is said to be a cointegrated vector and
the equation /3fyt = ut is called the cointegrating regression. Note
that the elements of the cointegrating vector can be zero, but not all
of them. If there are two such vectors (3\ and /32 so that (3[yt = u\t
and f3f
2yt = u2t are both 1(0), then any linear combination of these vec-
tors is also a cointegrating vector because linear combinations of 1(0)
variables are 1(0). There is, thus, an identification problem. Unless
we bring in some extraneous information, we cannot identify the long-

40 
Basic concepts
run equilibrium relationship. These problems will be illustrated in later
chapters.
We shall discuss here the relationship between cointegration and ECM.
To fix ideas, consider the case of two variables yu and y2t that are both
1(1). In this case if we write yu = f3y2t + ut and ut is 1(0) then yu and
2/2t are cointegrated and (3 is the cointegration vector (for the case of
two variables, scalar). In the two variable case, if there is cointegration,
we can show that (3 is unique. Because, if we have yu = "yyn + Vt
where Vt is also 1(0), by substraction we have (f3 — 7)2/2* + Ut — Vt is
1(0). But Ut — vt is 1(0) which means {(3 — 7)3/2* is 1(0). This is not
possible since y2t is 1(1). Hence we should have (3 = 7. But, in the
case of more than two variables, the cointegration vector is no longer
unique.
How do we know whether yu and y2t are cointegrated? We can test
whether the error in the cointegrating regression is 1(1). Thus, the hyp-
othesis that Ut has a unit root is a hypothesis that there is no cointe-
gration. Tests for cointegration thus, have no cointegration as the null
hypothesis. On the other hand, if the null hypothesis is that there is
cointegration, this has to be based on stationarity as the null hypoth-
esis for Ut. These issues will be discussed in the chapter on tests for
cointegration.
The earliest cointegration test is the one suggested in Engle and
Granger (1987) which consists of estimating the cointegrating regres-
sion by OLS, obtaining the residuals Ut and applying unit root tests for
Ut. Since Ut are themselves estimates, new critical values need to be tab-
ulated. Several extensions of this test have been proposed and critical
values have been tabulated for these tests. Since they are all based on
Ut, they are called residual-based tests.
Another set of tests are those that are based on the ECM model.
These tests are based on what is known as the Granger representation
theorem which says that if a set of 1(1) variables are cointegrated, they
can be regarded as being generated by an ECM.
To illustrate these ideas, consider a simple two-equation model used
in Engle and Granger (1987)
2/it + PV2t = uu, uu = uitt-i + elt 
(2.23)
Vit + OLy2t = 
u2t, u2t = pu2,t-i + e2t, \p\ < 1 
(2.24)
where e\t and e^t are possibly correlated white-noise errors. The model
is internally consistent only if a ^ (3. The reason for this constraint

2.11 Summary 
41
is that if a = /?, it is impossible to find any values of y\t and y2t that
satisfy both equations. The reduced forms for yu and y^t are
a 
(3
Vit 
= 
aU>it 
ou2t
a — p 
a — p
1 
1
2/2t 
=
\
a — p 
a — p
These equations also make clear the fact that both y\t and y2t are driven
by a common 1(1) variable uu. This is known as the common trend
representation of the cointegrated system. Also equation (2.24) states
that a linear combination of yu and y^t is stationary. Hence yu and
y2t are cointegrated with a cointegration coefficient a. Note that if
p = 1, then u2t is also 1(1) and hence there is no cointegration. The null
hypothesis p = 1 is thus a test of the hypothesis of no cointegration.
The Engle-Granger test is thus based on equation (2.24).
There is also an autoregressive representation for this system. Equa-
tion (2.23) and (2.24) can be written as
As/it = P0yiit-i + af36y2,t-i + Vit
where 8 = (1 — p)/(a — /?) and r/it and r/2t are linear combinations of e\t
and e2t- If we write zt = yu + «2/2t, then equation (2.24) implies
zt = pzt-i + e2t
or
A^ = (p - l)zt-i + e2t
or
Ayit = -aAjfet + (p - l)^t-i + e2t 
(2.25)
This is in the form of an ECM where zt-\ represents past disequilibrium.
2.11 Summary
This chapter provides an introduction to the basic models that will be
discussed in detail in later chapters. Besides an introduction to ARM A
models and Box-Jenkins methods, the chapter introduces trend and dif-
ference stationarity (TSP and DSP), unit roots and cointegration, vector
autoregressive (VAR) models, and error correction models (ECMs).
The next two chapters will discuss problems concerning unit roots and
the subsequent two chapters will discuss cointegration.

42 
Basic concepts
References
Algoskoufis, G. and R. Smith (1991), "On Error Correction Models:
Specification, Interpretation, Estimation," Journal of Economic
Surveys, 5, 97-128.
Ansley, C.F. (1979), "An Algorithm for the Exact Likelihood of Mixed
Autoregressive Moving Average Process," Biometrika, 66, 59-
65.
Banerjee, A., J.J. Dolado, J.W. Galbraith, and D.F. Hendry (1993),
Cointegration, Error Correction, and the Econometric Analysis
of Non-Stationary Data, Oxford University Press, Oxford.
Bardsen, G. (1989), "The Estimation of Long-Run Coefficients from
Error-Correction Models," Oxford Bulletin of Economics and
Statistics, 51, 345-350.
Beveridge, S. and C.R. Nelson (1981), "A New Approach to Decompo-
sition of Economic Time Series into Permanent and Transitory
Components with Particular Attention to Measurement of the
'Business Cycle'," Journal of Monetary Economics, 7, 151-174.
Bewley, R. (1979), "The Direct Estimation of the Equilibrium Re-
sponse in a Linear Dynamic Model," Economic Letters, 3, 357-
361.
Bhargava, A. (1986), "On the Theory of Testing for Unit Roots in
Observed Time Series," Reiew of Economic Studies, 53, 137-
160.
Box, G.E.P. and G.M. Jenkins (1970), Time Series Analysis Forecasting
and Control, Holden-Day, San Francisco.
Box, G.E.P. and D.A. Pierce (1970), "Distribution of Residual Auto-
correlations in Autoregressive Integrated Moving Average Time
Series Models," Journal of the American Statistical Association,
65, 1509-1526.
Davidson, J.E.H., D.F. Hendry, F. Srba, and S. Yeo (1978), "Economet-
ric modeling of the Aggregate Time-Series Relationship Between
Consumer's Expenditure and Income in the United Kingdom,"
Economic Journal, 88, 661-692.
Dickey, D.A. (1976), "Estimation and Hypothesis Testing for Nonsta-
tionary Time Series," Ph.D. dissertation, Iowa State University.
Dickey, D.A. and W.A. Fuller (1979), "Distribution of the Estimators
for Autoregressive Time Series with a Unit Root," Journal of
the American Statistical Association, 74, 427-431.
Engle, R.F. and C.W.J. Granger (1987), "Co-integration and Error

References 
43
Correction: Representation, Estimation and Testing," Econo-
metrica, 55, 251-276.
Entorf, H. (1992), "Random Walk with Drift, Simultaneous Errors,
and Small Samples: Simulating the Bird's Eye View," Institut
National de la Statistique et des Etudes Economiques.
Fuller, W.A. (1976), Introduction to Statistical Time Series, John
Wiley, New York.
Granger, C.W.J. (1981), "Some Properties of Time Series Data and
Their Use in Econometric Model Specification," Journal of
Econometrics, 16, 121-130.
Granger, C.W.J. and P. Newbold (1974), "Spurious Regression in
Econometrics," Journal of Econometrics, 2, 111-120.
Granger, C.W.J. and A.A. Weiss (1983), "Time-Series Analysis of
Error-Correction Models," in S. Karlin, T. Amemiya, and L.A.
Goodman (eds.), Studies in Econometrics, 
Time Series and
Multivariate Statistics, Academic Press, New York.
Hendry, D.F., A.R. Pagan, and J.D. Sargan (1984), "Dynamic Specifi-
cation," in Z. Griliches and M.D. Intrilligator (eds.), Handbook
of Econometrics II, North-Holland, Amsterdam, 1023-1100.
Kolmogorov, A. (1933), Foundations of the Theory of Probability, pub-
lished in 1950 by Chelsea, New York.
Maddala, G.S. (1992), Introduction to Econometrics, 2nd ed, Macmil-
lan, New York.
McCallum, B.T. (1993), "Unit Roots in Macroeconomic Time Series:
Some Critical Issues," Economic Quarterly, Federal Reserve
Bank of Richmond, 79, 13-43.
Nelson, C.R. and H. Kang (1981), "Spurious Periodicity in Inappropri-
ately Detrended Time Series," Journal of Monetary Economics,
10, 139-162.
(1984), "Pitfalls in the Use of Time as an Explanatory Variable in
Regression," Journal of Business and Economics, 2, 73-82.
Phillips, P.C.B. (1986), "Understanding Spurious Regression in Econo-
metrics," Journal of Econometrics, 33, 311-340.
Plosser, C.I. and W.G. Schwert (1978), "Money, Income and Sunspots:
Measuring Economic Relationships and the Effects of Differenc-
ing," Journal of Monetary Economics, 4, 637-660.
Quenouille, M. (1957), The Analysis of Multiple Time Series, Charles
Griffin, London.
Sargan, J.D. (1964), "Wages and Prices in the United Kingdom: A
Study in Econometric Methodology," in P.E. Hart, G. Mills,

44 
Basic concepts
and J.K. Whitaker (eds.), Econometric Analysis for National
Economic Planning, Butterworth, London; reprinted in D.F.
Hendry and K.F. Wallis (eds.), Econometrics and Quantitative
Economics, Basil Blackwell, Oxford, 1984.
Schmidt, P. and P.C.B. Phillips (1992), "LM Test for a Unit Root
in the Presence of Deterministic Trends," Oxford Bulletin of
Economics and Statistics, 54, 257-287.
Sims, C. (1980), "Macroeconomics and Reality," Econometrica, 48, 1-
48.
Slutsky, E. (1937), "The Summation of Random Causes as the Source
of Cyclic Processes," Econometrica, 5, 105.
Spanos, A. (1986), Statistical Foundations of Econometric modeling,
Cambridge University Press, Cambridge.
Wickens, M.R. and T.S. Breusch (1988), "Dynamic Specification, the
Long Run and the Estimation of Transformed Regression Mod-
els," Economic Journal, 98, 189-205.
Yule, G.U. (1926),"Why Do We Sometimes Get Nonsense Correlations
Betweeen Time Series? A Study in Sampling and the Nature of
Time Series," Journal of the Royal Statistical Society, 89, 1-64.

Part II
Unit roots and cointegration
This part contains five chapters that form the core material that needs
to be understood to follow the rest of the book.
Chapter 3 gives a brief introduction to Wiener processes. We do not
go into these in great detail because we do not go into details of the
derivations of asymptotic distributions. Those interested in these can
refer to the source material. (Many empirical researchers do not need
the derivations.) We next discuss the importance of scaling factors in
the derivation of asymptotic distributions. Next we discuss the Dickey-
Fuller (DF) distribution and the DF tests, the ADF test and the problem
of selection of lag length (a problem that needs special attention). Next
we discuss the Phillips-Perron (PP) tests, Sargan-Bhargava tests, vari-
ance ratio tests, and finally forecasting problems.
Although often used, the ADF and PP tests are useless in practice
and should not be used. Some useful modifications of these tests are
discussed in chapter 4. The material covered in this chapter forms the
basis of all the modifications discussed in the next chapter.
Chapter 4 considers several issues in unit root testing. The reason why
there are so many unit root tests is that there is no uniformly powerful
test for the unit root hypothesis. We discuss several of the tests for
completeness. Some of them are not worth considering but they are all
promoted by the respective authors and the Nelson-Plosser data set is
used as a guinea pig for every new test suggested.
We first discuss the problems of size distortions and low power of unit
root tests and then some solutions to these problems. We also discuss
tests for stationarity as null, the oft-quoted KPSS test. We do not
recommend its use - it has the same low power problems as the ADF
and PP tests. It is discussed here because it is often referred to - as
useful for confirmatory analysis in conjunction with the ADF and PP
45

46 
// Unit roots and cointegration
tests. But we feel that such confirmatory analysis is an illusion (with two
tests that lack power). Some useful modifications of the ADF, PP, and
KPSS tests are discussed and these should be preferred. This chapter
also discusses in detail panel data unit root tests. The Levin-Lin test is
very often used (in fact over used) and we discuss useful modifications
of this test.
Chapter 5 is on estimation of cointegrated systems and inference on
the estimated coefficients. We first discuss the two-variable model and
the normalization issue (not usually discussed). Second, we discuss a
triangular system, the FM-OLS method, and several other methods in-
volving lags and leads. Third, discuss system estimation methods -
the Johansen procedure (widely used) and the Box-Tiao method (rarely
used). Fourth, we discuss identification problems since the system meth-
ods identify only the cointegration space but not the individual cointe-
grating vectors without more prior information. Fifth, we discuss several
Monte Carlo studies that have been conducted to study the different es-
timation methods but very few unambiguous conclusions emerge except
that the Engle-Granger two-step method should be avoided. Finally we
discuss several miscellaneous issues such as forecasting from cointegrated
systems, threshold cointegration, and so on.
Chapter 6 on tests for cointegration is complementary to chapter 5.
It discusses the commonly used residual-based tests (these should be
avoided), ECM-based tests, tests with cointegration as null, and tests
associated with system-based estimation methods (the Johansen and
Box-Tiao methods). The chapter also has a sceptical note on the use
of cointegration tests. One important problem discussed (often com-
pletely ignored) is the pre-testing problem - cointegration itself depends
on preliminary tests for unit roots and thus there is a question of the
appropriate significance levels to use in cointegration tests.
Chapter 7, the final chapter in this part, is on inferential procedures
in regression models involving 1(1) and 1(0) regressors, unbalanced equa-
tions, the problem of uncertain unit roots, and testing procedures under
uncertainty about unit roots and cointegration. Many of these problems
are not commonly discussed in books.

3
Unit roots
3.1 Introduction
In the previous chapter we discussed the econometric problems that
might occur when we run a regression with different orders of integrated
variables. To avoid this problem, the first thing we have to do is to iden-
tify the correct order of the integration of each variable. In the context
of ARIMA modeling, this identification is equivalent to determining the
parameter d in the ARIMA(p, d, q) model. The Box-Jenkins approach
discussed in the previous chapter suggested the use of visual inspection
of correlograms for determining the parameter d. The recent develop-
ment of unit root tests is nothing but the use of formal statistical tests
in place of the visual inspection of the correlogram.
The idea that the parameter d is equal to the number of unit roots
led Dickey and Fuller to replace the subjective inspection of the sample
autocorrelation function with a formal test of the unit root null hypoth-
esis. This test, known as the standard Dickey-Fuller (DF) test, is based
on independently and identically distributed (iid) errors. For a wide
class of errors which allows some heterogeneity and serial correlations
in errors, two approaches have been proposed to modify the standard
DF test. One is the parametric approach which suggests the augmented
Dickey-Fuller (ADF) test. The nonparametric approach leads to the
Phillips-Perron tests. The DF tests were applied to many US macroeco-
nomic time series by Nelson and Plosser (1982). The evidence presented
by them showed that most macroeconomic variables are well described
by the ARIMA models with one unit root. This was believed to have
important implications for understanding the sources and nature of the
business cycle although most macroeconomists now think that unit root
tests do not shed much light on the sources and nature of the business
47

48 
Unit roots
cycle. Since then an enormous number of studies on the unit root have
appeared. The results of Nelson and Plosser have been re-examined
using different methods and given different conclusions. In fact every
time a new method has been suggested, it has been tried on the Nelson-
Plosser data.
One thing we have to note is the importance of the specification of
the deterministic trend in the unit root testing procedures. If a time
series has a unit root, then it shows a systematic pattern in its move-
ment (though the movement cannot be predictable) which is named as
stochastic trend (see the section 2.6). Many macroeconomic variables are
more fruitfully modeled with both the deterministic trend and stochastic
trend. When we incorporate the misspecified deterministic trend into
the model which is also believed to have a stochastic trend, the sys-
tematic movement by the stochastic trend may be shadowed or overem-
phasized by the misspecified deterministic trend. Thus it is important
to determine the appropriate form of deterministic trends a priori be-
fore conducting the unit root tests. The critical values for the unit root
tests also differ according to the different form of deterministic trends.
Because of this fact, the unit root testing procedures are not as simple
as conventional testing procedures and require the use of appropriate
tables of critical values.
In most econometric applications a linear specification of the deter-
ministic trend is used and coefficients of the linear trend are assumed to
be constant over the sample period. Recently the importance of using a
flexible specification for the deterministic component has been empha-
sized. It is motivated by the view that the linear time trend hypothesis is
inappropriate for modeling the deterministic component of an economic
time series. From an econometric perspective, the appropriate represen-
tation of nonstationarity in macroeconomic time series is a vital issue
since misspecification of a random walk as a stationary process evolving
around a deterministic trend has major effects on the statistical analy-
sis of the data. Recent studies (Perron, 1989; Hamilton, 1989; Phillips,
1992, 1994), suggesting that the linear trend might not be an appro-
priate specification for some macroeconomic variables, will be discussed
later. In this chapter we confine ourselves to a discussion of the cases of
a zero intercept, a constant, and a linear trend.
In the following sections we shall first discuss Wiener processes which
are extensively used in the derivation of the several asymptotic distri-
butions in the subsequent chapters. The discussion here will be brief
and concerns the derivation of the most basic distributions. Since our

3.2 Unit roots and Wiener processes 
49
purpose is to present the basic results we shall not go into the details
of the derivation of the asymptotic distributions in this book. These
derivations can be found in the papers cited.
3.2 Unit roots and Wiener processes
A time series yt is said to have an autoregressive unit root if we can
write it as
yt = DTt + zt
zt = pzt-i + et
and
p=l
where et is stationary and DTt is a deterministic component. With
p — 1, Azt is stationary and Ayt is stationary around the change in the
deterministic part. In this case yt is said to be integrated of order 1
and is denoted as 1(1). Stationary series are said to be 1(0). When yt is
stationary and is thus 1(0), and DTt is a linear trend, then
yt = DTt + et
and the difference of yt is
Ayt = a + et- 
et-i
where a — DTt — DTt-\ is constant. Thus Ayt has a moving-average
unit root. MA unit roots arise if a stationary series differenced. This is
known as overdifferencing. Tests using 1(1) as the null hypothesis are
based on the unit autoregressive root as the null. Tests using 1(0) as the
null hypothesis are based on the unit moving-average root as the null.
In this chapter we shall consider the case of tests for the autoregressive
unit root, since these are the ones most common. Tests for the moving-
average unit root will be discussed in the next chapter.
As we have seen in chapter 2, if there is a unit root (from now on,
unit root means autoregressive unit root), that is p = 1, the variance
of the process yt increases over time. This implies that since the OLS
estimator of the autoregressive parameter p is the ratio of the covariance
of yt and yt-\ to the variance of yt and var(yt) —> oo as t —> oo, the
OLS estimator p no longer has an asymptotic normal distribution when
the true autoregressive parameter p = 1. Thus the t- or F-type test

50 
Unit roots
statistics based on the OLS estimator do not follow the conventional t-
or F-distribution.
In order to test the unit root null against the stationary alternative
hypothesis, first we need to derive the distribution of the OLS estimator
of the autoregressive parameter under the unit root null hypothesis HQ :
p = 1. This distribution turns out to be a function of Wiener processes
and hence we shall first give a brief introduction to the Wiener processes.
3.2.1 Wiener processes: some definitions
The Wiener process has been used in physics to describe the motion of a
particle that is subject to a large number of molecular shocks. It is also
referred to as a Brownian motion. Consider a small interval of length
At, and define AW as the change in W during At. Then the behavior
of a variable, W which follows a Wiener process can be described by two
relations:
(i) AW is related to At by the relation
where e ~ iV(0,1). Hence E(AW) = 0, var(AW) = At.
(ii) The values of AW for any two different short intervals are inde-
pendent. That is, the es are independently normally distributed.
Consider next the value of W during a long time period T. This can
be denoted by W(T) — W(0). This can be regarded as the sum of n
small intervals of length At. Hence T = nAt and
Since the e{ are IN(0,1), by assuming W(0) = 0, we have E(W(T)) = 0
and var(W(T)) = n - At = T. Now Let us consider the relationship
between a random walk and W(T). Let St ~ IN(0,1) and ST = X^=i £t-
Then ST is a random walk, since ST = St~i 4- £T-> or an 1(1) process
with independent increments. We have E(ST) = 0 and var(ST) = T, as
with the variable W(T).
As we will show later, the index t in the increasing range [0,T] can be
mapped into an index r in the fixed range [0,1] and the Wiener process
is defined over this interval as W(r). If W(r) is a Wiener process, then
for fixed r
W(r)~iV(0,r), 
0 < r < 1

3.2 Unit roots and Wiener processes 
51
Thus the Wiener process is like a continuous random walk defined on
the interval [0,1].
In differential equation form, the Wiener process is defined by
dW = eVdi
This is the basic Wiener process with a drift rate of 0 and variance rate of
1. The drift is said to be zero because E(W(T)) = 0. The variance rate
is said to be 1, because var(W(T)) = 1 multiplied by T. A generalized
Wiener process x is defined by
Ax = aAt + bAW
or in differential equation form
dx = adt + bdW
Substituting AW = sy/At we get
Ax = aAt + bey/At
Hence
A x - 
N(aAt,b2At)
As before, if we consider the behavior of x during a long period T and
defining n — Tj At we get
x(T) ~ N(aT, b2T)
Again, mapping the index t in the increasing range [0, T] to an index r
in the fixed range [0,1] we can see the generalized Wiener process
x(r) ~ N(ar,b
2r), 
0<r 
< 1
A generalized Wiener process, in which a and b are functions of the
underlying value of x at time £, is the Ito process. It is defined by
dx = a(x, t)dt + b(x, t)dW
where W is the basic Wiener process. Both the drift and variance of the
Ito process change over time.
Another process that is often used is the geometric Brownian motion.
The variable x is said to have a geometric Brownian motion if
AT
— = aAt + bAW
x

52 
Unit roots
where W is the basic Wiener process. These processes are used in the
description of the movement of stock prices. However, in the sequel we
shall be using only the basic Wiener process.
As mentioned earlier, let et ~ IN(0,1) and ST = Ylt=i£t, 
then
ST ~ JV(0,T) is an 1(1) process with independent increments. The
distribution of the standardized sums is closely related to the Wiener
process and this relationship is known as the functional central limit
theorem (FCLT). In the study of the asymptotic theory of integrated
processes, the distribution of the standardized sums ST/VT 
plays an
important role. Just as the central limit theorem (CLT) plays an impor-
tant role in the asymptotic theory of stationary processes, the functional
central limit theorem (FCLT) plays an important role in the asymptotic
theory of integrated processes.
The FCLT is based on the mapping of the increasing interval from 0
to T to a fixed interval [0,1]. Divide the interval [0,1] into T + 1 parts
of 0,1/T, 2/T,..., 1. Define a new index r in [0,1] corresponding to the
time index in [0, T) by the relation
Next we construct the step function from the standardized sum St by
defining
RT(T) = -±=S[Tr]
where [Tr] denotes the integer part of Tr, e.g., if T = 100 and r = 0.791,
then [Tr] = 79. Since r is the real number in [0,1], so is Tr.
Figures 3.1 shows how the standardized random walk can be approx-
imated by the step function RT(r). The first graph in figure 3.1 shows
this mapping for the sample size of 10. The other graphs in figures 3.1
show the mapping for sample sizes of 50, 100, and 200, respectively and
indicate that as T —» oo, the step function approaches to the random
walk rapidly. In other words, as T —> oo, Rr(r) becomes increasingly
dense on [0,1] and it converges weakly to the standard Wiener process
under some regularity conditions.
This is the idea behind the functional central limit theorem. The
result
RT(r) => W(r)
is known as Donsker's theorem (Donsker, 1951). Here => is used to sig-
nify weak convergence of the associated probablity measure. For details

0 
10 
20 
30 
40 
50 
60 
70 
80 
90 
100 
0 
20 
40
7=100 
7 = 200
Fig. 3.1. Random walk and step function

54 
Unit roots
and proof see Billingsley (1968). For a wide class of weakly dependent
and heterogenously distributed errors, some general results have been
established by McLeish (1975) and Herrndorf (1984).
The other important theorem used to derive the asymptotics for unit
roots is the continuous mapping theorem (CMT). If /(•) is a continuous
functional on [0,1], then a consequence of the FCLT is that
f(RT(r)) =» f(W(r))
For details and proof see Billingsley (1968) and Hall and Heyde (1980).
3.2.2 Some basic results on Wiener processes
We shall now give some basic results on the use of Donsker's theorem and
continuous mapping theorem (CMT) in the derivation of the distribution
associated with 1(1) processes.
Suppose that yt is a random walk and thus yt = yt-i + et where et
are iid(0,1). Assuming the variance to be 1 has the advantage that we
can drop the scaling parameter. We assume yo = 0 for simplicity. We
shall show some results in the form of a lemma. (In the following unless
otherwise noted, all summations are over t = 1 and T .)
Lemma 3.1
(i) 
-|= =» / W(r)dr 
where y = Z t i Vt/T
vT 
Jo
(ii) T- 2X>?^ f[W(r)]2dr
Jo
(iii) T"5y]tyt=> / rW(r)dr
Jo
r1
(iv) r-*y]tet=* / rdW(r)
Jo
T 
fl
(v) T^Y.Vt-iet^ 
I W(r)dW(r)
t=2 
Jo
Note the following things in the mapping
et =^ dW(r)
yt => W(r)

3.2 Unit roots and Wiener processes 
55
Proofs (i) Consider
RT(r) 
= 
V T = 7 T 
for 
- T - -
r < f
VT
where RT{T) is a step function with steps yi/y/T at i/T and is constant
between steps. Hence
T 
ri/T
RT(r)dr = V /
*
( i_ 1 ) / r
Note that
Since Yli=i Vi-i — Yli=i Vi w e n a v e
" ' 
— L/J(i-1)/T ~ 
rp 
rp 
— rp
(i-l)/T 
K 
}/ 
J- 
J- 
1
Using Donsker's theorem we have
RT(r) => W(r)
Hence we get
vT
(ii) We can show as before that
Since YA=I Vi-i — YA=I Vi a nd using the CMT we get
Spl^ => 
j\W(r)fdr
Proofs of results (iii), (iv), and (v) follow along similar lines and are
omitted.

56 
Unit roots
3.2.3 Relationships with normal distributions
Let et be JJV(O,1). Define yt = J2j=i e* . Then yT ~ JV(O,T) and hence
Note that W(l) = iV(0,1). We shall now show the relation between
Wiener processes and normal distributions in the form of a lemma as
follows:
Lemma 3.2
r1 
i
i) 
/ W(r)dr ~ JV(O, -)
Jo 
3
(ii) 
/ rdW(r) ~ N(0, \)
Jo 
<$
(iii)
r1 
i
(iv) 
/ (r - a)W(r)dr - iV(0, A), 
A = — (8 - 25a + 20a2)
Jo 
6°
f1 
2
v) 
/ rW(r)dr ~ N(0, —) 
z/ a = 0.
fW(r) 
andV(r) are two independe
\ I [W{r)fdr] 
\f W(r)dV(r)] - iV(0,
(
(vi) 
IfW(r) andV(r) are two independent Wiener processes, then
Proof (i) We note that
T
T(T + 1)
2 
'
T 
T(T+1)(2T-
1
If yt is a random walk, then
j/t = yt-i+e t, 
e t~JW(0,l), 
t = l,...,T
and 2/0 = 0- Consider T~^y = T~? ^ y t and yt = ^i + e2 -f • • • + et.
Thus, when we consider YjVtiwe 
n a v e Y Vt = ^ ei + (^~ l)e2 4- •
Hence var(J2vt) — Si^ 2* Since the es are IN(0,l), we have

3.2 Unit roots and Wiener processes 
57
where
f
l
)
(
2
T 
+ l) ^ 1
v = T-3 f-12 = T
6
Earlier we showed that T~iy => /J" W(r)dr. Hence J^ W ( r ) d r « JV(O, | ) .
(ii) Consider next T~^ Y2^et- 
Again this is iV(0, V), since e^ are
JiV(0,1) and
Thus
/ rdW(r) « AT(O i )
Jo 
^
(iii) Since
we have
X>-ie* = | [E^
2 - E^
2-! - E
e2] = I VT - E
e2]
Note that
VT = ei -h e2 + • • • + eT ~ JV(O, T)
Hence
and
Also
Hence
1 
r 
o
Since we showed that
1 
P1
T 
Jo
result (iii) follows. Proofs of results (iv), (v), and (vi) are omitted here.

58 
Unit roots
Note on (iii) In the general case where et are not independent, but
var(et) = o\
and
lim E(T~xy%) = a2(> 0)
T—•oo
we have
and
In this case we have
This equation is the basis of the Phillips-Perron test that will be dis-
cussed later. Note that if et ~ iid(0,cTg), then a2 = G\ and we have
When we consider a multiple (n-vector) time series and n > 1, Phillips
(1988b) shows that for n > 1, as T -• oo,
1 V yt-ie{ ^ / B(r)dB(r)f + fii
where 5(r) is vector Brownian motion with covariance matrix Q and
For the proof, see Phillips (1988b).
3.2.4 Scaling factors in asymptotic 
distributions
When we discuss the asymptotic distributions of regression coefficients
where the regressors are trending variables, it is important to remember
the scaling factors. For instance in the usual regression model
yt = pxt + uu 
ut ~ iid(0, a2) 
t = 1,2,..., T

3.2 Unit roots and Wiener processes 
59
we know that var((3) = cr2/^2x2. The usual assumption is that
1 ^—> o
— > xt
or
> xt 
> a constant
/ 
j 
t
In this case we consider the asymptotic distribution of \/T(/3 — /?),
which had a scaling factor \fT. Then the asymptotic distribution has a
finite variance.
What if Xt = tl As we have seen above
or
In this case we have to consider the asymptotic distribution of T3/2(/3 —
(3) which will have a finite variance. If xt = t2, we consider the asymp-
totic distribution oiT5^20—/3) and so on. For example, in the regression
yt = a + (3t + 7t2 + 6t3 + ut
we have to consider the asymptotic distributions of
VT(d-a), Ty/f0-0), 
T2Vftf--f), 
and T3Vf(6-6)
Note that we have

60 
Unit roots
In general
T
^2^ 
=O(Tj+1)
1
If Y^x2t = O(Tj+1), 
then the appropriate scaling factor is T^" 1)/ 2
to get the asymptotic distribution of corresponding coefficient of xt. In
the subsequent chapters we will use the appropriate normalization rules
when considering asymptotic distributions.
3.3 Unit root tests without a deterministic trend
S.S.I Some historical notes
Most of the papers about the asymptotic behavior of the OLS estimator
p have been concerned with the AR(1) model without the deterministic
trend
yt = pyt-i+et 
(3.1)
where {e*} is a sequence of independent normal random variables with
mean zero and variance cr2, i.e., et ~ IN(0,a2). 
We may rewrite (3.1)
as
t-i
Vt = ptyo + 
^p'et-i
2=0
The distribution of the successive ys is not uniquely determined by that
of the es alone. Since yt depends on yOy the distribution of y0 must also
be specified.
Three distributions have been commonly proposed for y0 as follows
(White, 1958; Phillips, 1987a):
(i) y0 = a constant with probability one.
(ii) t/o n a s a certain specified distribution,
(iii) yo — yr where T is the sample size.
Distribution (iii) is the so-called circularity condition that is used mainly
as a mathematical device to simplify distribution theory (Anderson,
1942). Distribution (ii) permits the greatest flexibility that allows for a
nonstationary series and includes (i) as a special case (Phillips, 1987a).
The assumption (i) is the commonly used condition (Dickey and Fuller,
1979; Evans and Savin, 1981). It can be shown that the asymptotic
distribution of normalized p does not depend upon yo if M ^ 1 (White,
1958). If |p| > 1, the asymptotic distribution depends upon yo, and as

3.3 Unit root tests without a deterministic trend 
61
yo/o~ increases the distribution of p becomes more bell-shaped (Evans
and Savin, 1981). Needless to say, the finite sample distribution of p
depends upon Z/Q.
Given n observations, the maximum likelihood estimator of p is the
least squares estimator
/5=(E2/t
2-1)-1E2/t2/t-i 
(3-2)
1 
1
Mann and Wald (1943) first proved that if |p| < 1, then
under the iid assumption of the errors. Rubin (1950) showed that p is a
consistent estimator for all values of p. White (1958) has shown that the
limiting distribution of \p\T(p — p)/(p2 — 1) is a Cauchy distribution by
inverting the limiting joint-moment generating function when \p\ > 1.
On the other hand, when p = 1, White was unable to invert the limit-
ing joint-moment generating function to obtain the limiting distribution.
However, based on Donsker's theorem, he represented the limiting distri-
bution of T(p—l)/\/2 as that of the ratio of two integrals defined on the
Wiener process. Later M. M. Rao (1961) obtained an expression of the
limiting distribution of T(p — l)/\/2 but it is not of an easily recogniz-
able form. However, the standardization of p as T(p — p)/y/2 by White
(1958) and M. M. Rao (1961) has been proved incorrect and the correct
standardization of p is T(p - p) (Phillips, 1987a). Fuller (1976) and
Dickey and Fuller (1979) derived the asymptotic distribution of p under
the assumptions of the iid errors and assumption of yo — 0- Phillips
(1987a) derived it under the more general assumption about the errors
and assumption (ii) of y$.
3.3.2 The Dickey-Fuller 
distribution
Using the results on Wiener processes in the previous sections, we shall
derive the distribution of p under the null hypothesis of a unit root, that
is, p = 1. From the equation (3.2), we have

62 
Unit roots
As we have seen in the section 3.2.2, the numerator in (3.3) converges
to a random variable
T ^ y V - i e t ^ / W(r)dW(r) 
(3.4)
t=i 
Jo
Interpreting (3.4) heuristically, averaging corresponds to integrating
(analogous to taking expectations). We construct the step function from
yt-i/VT 
which converges weakly to the Wiener process W(r). And then
etVT(= AytVr) 
is the innovation over which averaging occurs and cor-
responds to dW(r). The denominator has a nonzero asymptotic variance
and
T
2dr 
(3.5)
By the continuous mapping theorem, the limiting distribution of the
OLS estimator p when p = 1 is
Jo W(r)2dr
This is known as Dickey-Puller distribution. Although Fuller (1976) and
Dickey and Fuller (1979) did not provide the limiting distribution of p
with functional of Wiener processes, they first derived its distribution
and made the tables of this distribution. The functional form of Wiener
processes is due to Phillips (1987a, 1988b).
We can use the distribution (3.6) for testing the unit root null hy-
pothesis Ho : p = 1, that is
K = T{p- 1)
or we can normalize it with the standard error of the OLS estimator and
construct the ^-statistic testing p = 1. The test statistic is
*> " SE(p)
The asymptotic distribution of the t-type test statistic is given by
The proof follows from the fact that
rp-i)
C/5 
, 
: 
=r

3.3 Unit root tests without a deterministic trend 
63
The numerator is given by equation (3.6). The denominator can be
written as
1/2
Hence the distribution of the t-type statistic is given by (3.7). This distri-
bution is known as the Dickey-Fuller ^-distribution. The test procedure
using K or t^ for the unit root null hypothesis is called the Dickey-Fuller
(DF) test.
The asymptotic distribution of the ^-statistic under p — 1 is not
the standard t-distribution, and so conventional critical values are not
valid. The numerator of the distribution is skewed to the right, be-
ing a X2(l) minus its expectation, but the ratio is skewed to the left.
In practice, most of the ^-statistic outcomes will be negative and of-
ten very different from —2.0 even for large sample sizes. Thus, using
the conventional critical values can lead to considerable overrejection
of the null hypothesis of a unit root. Fuller (1976) provided the crit-
ical values of these statistics. Table 3.1 shows some of those critical
values.
The following example was given in Dickey and Fuller (1979). Gould
and Nelson (1974) investigated the stochastic structure of the veloc-
ity of money using the yearly observations from 1869 through 1960
given in Friedman and Schwartz (1963). They considered the following
models
yt-yi 
= 1.0044 (yt_i - Vl) + et, 
a1 = 0.0052 
(3.8)
(0.0094)
yt = 0.0141 + 0.970 yt-i + eu 
a2 = 0.0050 
(3.9)
(0.0176) (0.0199)
Model (3.8) assumes that it is known that no intercept enters the model
if ?/i is subtracted from all observations. Model (3.9) permits an intercept
in the model. The test statistics for the unit root null are for model
(3.8)
K = 91(1.0044 - 1) = 0.4004
t = (1.0044 - l)/0.0094 = 0.4681

64 
Unit roots
Table 3.1. Critical values for Dickey-Fuller tests
Sample size
K-test
1%
AR(1) without a drift
25
100
250
500
oo
AR(1) with
25
50
100
250
500
oo
AR(1) with
25
50
100
250
500
oo
-11.9
-13.3
-13.6
-13.7
-13.8
a drift
-17.2
-18.9
-19.8
-20.3
-20.5
-20.7
a drift anc
-22.5
-25.7
-27.4
-28.4
-28.9
-29.5
5%
-7.7
-7.9
-8.0
-8.0
-8.1
-12.5
-13.3
-13.7
-14.0
-14.0
-14.1
t-test
1%
-2.62
-2.60
-2.58
-2.58
-2.58
-3.75
-3.58
-3.51
-3.46
-3.44
-3.43
I a time trend
-17.9
-19.8
-20.7
-21.3
-21.5
-21.8
-4.38
-4.15
-4.04
-3.99
-3.98
-3.96
5%
-1.95
-1.95
-1.95
-1.95
-1.95
-3.00
-2.93
-2.89
-2.88
-2.87
-2.86
-3.60
-3.50
-3.45
-3.43
-3.42
-3.41
F-test
1%
7.88
7.06
6.70
6.52
6.47
6.43
10.61
9.31
8.73
8.43
8.34
8.27
(a)
5%
5.18
4.86
4.71
4.63
4.61
4.59
7.24
6.73
6.49
6.34
6.30
6.25
F-test
1%
8.21
7.02
6.50
6.22
6.15
6.09
(b)
5%
5.68
5.13
4.88
4.75
4.71
4.68
Notes: F-test (a) is for Ho : a = 0, p = 1 in yt — a + pyt •+- ut
and Ho : 8 = 0, p = 1 in yt = a + St + pyt + ut.
F test (b) is for Ho : a = 0,6 = 0, p = 1 in yt = a + St + pyt + ut.
Sources: Fuller (1976, p. 371) for the K-test and p. 373 for the t-test.
Dickey and Fuller (1981, p. 1063) for the F-test.
More detailed percentiles at the levels:
1%, 2.5%, 5%, 10%, 50%, 90%, 95%, 97.5%, and 99%
for the K-test and the t-test can be found in Fuller (1996,
pp. 641 and 642, respectively).
and for model (3.9)
K = 92(0.9702 - 1) = -2.742
t = (0.9702 - l)/0.0094 = -1.50
Thus using table 3.1 the null hypothesis of a unit root cannot be rejected
at the 5 percent level. They concluded that the logarithm of velocity is
consistent with the random walk model.

3.4 DF test with a linear deterministic trend 
65
3.3.3 Computation of critical values
It is worth noting that different methods have been used to compute
the critical values of the limiting distributions in the presence of a unit
root, since the standard DF test requires different critical values for the
different specifications of deterministic trend even with time invariant
parameters.
Evans and Savin (1981, 1984) provide the exact finite sample distri-
bution calculated by the method due to Imhof (1961) or the method
due to Pan (1968). Although the effects of a nonzero value of yo vanish
asymptotically, they are substantial in finite samples as demonstrated
by Evans and Savin (1981) (for p) and Nankervis and Savin (1985) (for
tfi in case of iid errors). Under the assumption of the nonzero yo, the
actual size of the test is well below the (asymptotic) nominal size.
For calculating the limiting distribution, several methods have been
developed. The most commonly used method is the simulation method;
that is, the integral functions in the limiting distribution are approxi-
mated by functions of partial sums of independent normal random vari-
ables. See appendix B of Zivot and Andrews (1992). Phillips (1977)
and Satchell (1984) considered an Edgeworth approximation but this
performs poorly. Nabeya and Tanaka (1990) proposed Fredholm ap-
proximation for computing the limiting distribution, which is based on
the relationship between the kernel of the characteristic function and the
Fredholm determinant. Perron (1991) suggested the continuous time ap-
proximation which is based on the Ornstein-Uhlenbeck diffusion process.
The approximations proposed by Nabeya and Tanaka (1990) and Per-
ron (1991) are based on the assumption p = 1 — c/T and exp(—c/T),
respectively, where c is a fixed constant. This is known as a local to unity
process. As noted by Chan and Wei (1987), this is similar to that in
approximating binomial distributions by Poisson distributions when the
number of trials is large but the probability of success is near 0. These
methods are useful to study the power of a test of unit root against near
unit root where c is small. More detailed discussion of all these issues is
in Tanaka (1996).
3.4 DF test with a linear deterministic trend
In the previous section we discussed the distribution of p under the
hypothesis p = 1 when both the data generating process (DGP) and
the estimated equation had no drift (constant term) or trend. We shall

66 
Unit roots
now discuss models with drift and trend. Here, the important thing to
remember is the difference between the maintained null hypothesis (or
the true DGP) and the estimating regressions that are used to test the
null hypothesis.
3.2^.1 Models with drift
We can consider two possible cases where yt are believed to be generated
by the following two DGPs:
(i) yt = 2/t-i + et
(ii) yt = a + yt-\ + et
For the given data yt when we estimate the regression
yt = a + pyt-i + et 
(3.10)
The asymptotic distribution of p will depend on whether the DGP is (i)
or (ii).
(i) Let us consider the first DGP. Dickey and Fuller (1979) derived
the asymptotic distribution of the standardized OLS estimates. The
asymptotic distribution can be expressed in terms of Wiener processes
as follows
where W* = W(r) — J W(r)dr is the "demeaned" Brownian motion. The
asymptotic distribution of the OLS estimates p in (3.11) is not the same
as the asymptotic distribution in (3.6). The corresponding ^-statistic
has the asymptotic distribution
P
The t-statistic for the constant term also follows the nonstandard asymp-
totic distribution, a functional of the Wiener processes.
When a constant term is used in the estimated regression and yt is
generated by the pure random walk (without a drift), then a different
table of critical values must be used. The critical values of T(p — 1) and
tp are tabulated by Fuller (1976) and those for t& are given in Dickey
and Fuller (1981). Part of the critical values are given in table 3.1.
Although the test statistics are for the unit root null hypothesis, a
maintained hypothesis on which the asymptotic distribution was derived

3.4 DF test with a linear deterministic trend 
67
is that the true value of a is zero. Thus naturally we could consider the
joint hypothesis of Ho : a = 0 and p = 0. Dickey and Puller (1981)
derived the asymptotic distribution of the F-statistics for the joint hy-
pothesis and tabulated the critical values by Monte Carlo methods. Part
of the critical values are given in table 3.1.
(ii) Now consider the second DGP, that is, a random walk with a drift.
Recall that the random walk with drift can be written as
Since yt has a deterministic trend inherently, yt is dominated by the
deterministic time trend asymptotically. Thus in the denominator of
the OLS estimator of p, J2t=i Vt-\-> t n e regressor yt_1 is asymptotically
dominated by the time trend a(t — 1). In large samples it is as if the
explanatory variable yt-i were replaced by the time trend a(t — 1). As
a result, the asymptotic distributions of the OLS estimate are asymp-
totically normal and the t- and F-statistics follow the standard t- and
F-distribut ions.
The above results have been studied by West (1988) who shows the
asymptotic normality of unit root test statistics. However, as we will
argue later, this result is of limited usefulness in the context of unit root
tests, although it illustrates the importance of the use of scaling factors.
The asymptotic normality result is as follows: under the unit root null
Ho : p = 1, yt has a linear trend, and assuming yo = 0 for simplicity we
have
t
= oct + St
As discussed earlier, we need to scale the coefficients properly. Thus we
consider the distribution of VT(a — a) and T3/2(p — 1). We have
where
and

68 
Unit roots
Consider the vector b. Since
]yt-iet =
by normalizing T~3/2 we have from the first term of the right-hand side
of equation
and from the second term of the right-hand side of equation
T"3/2 ] T etSt-i -> 0 in probability
As noted in the section 3.2.2
Thus
Hence
where
B=[a/2 
a2/3
The scaling factors are such that all terms with the stochastic component
St can be ignored asymptotically.
Consider the matrix A
Again the scaling factor T~3 ensures that
T~3 Y^ St-i -»• ° 
i n probability
and
T~3 ^ ( t - l)St-i -^0 
in probability
All we are left with is
Similarly
2 ^ 
2 ^ [ a ( i - 1 ) + 5 t_!

3.4 DF test with a linear deterministic trend
Thus the matrix A —> B and since b => N(0, o^B) we have
69
- a )
T^(p-l)
This gives us the result
Note that this result holds only if a ^ 0.
When the true DGP (not estimating regression) is
again we have, under p = 1
pyt-i + et
6t2
and the deterministic term at + <5£2 dominates 5t in the derivation of
the asymptotic distributions. In this case the scale factors for d, <S, and
p are T"1/2,T"3//2;, and T~5/2 respectively. The matrix B is now
B =
1 
1/2 
(3/3
1/2 
1/3 
/?/4
/3/4 /32/5
and hence
The reason why this result on asymptotic normality is not useful is
that the behavior of yt under the null and the behavior under the alter-
native are different. Consider the case
yt = a + pyt-i + et
If a ^ 0, then under Ho : p = 1, yt has a linear trend whereas under
Hi : \p\ < 1 it has no trend. Similarly, if we consider the model
yt = a + 8t + pyt-i + et
If S 7^ 0, then, under ii/o • P — 1, 2/t has a quadratic trend and, under
Hi : \p\ < 1, ?/£ has a linear trend. This problem does not arise with
the approach suggested by Bhargava (1986) whose formulation we have
discussed earlier in chapter 2 (section 2.9).

70
Unit roots
Models with linear trend
Now consider the more general estimating regression (not DGP) with a
constant term and a linear deterministic trend
yt =
pyt-i +et
In this case, even though yt-i would be asymptotically equivalent to a
time trend when a ^ 0, since the deterministic trend is incorporated as
a separate regressor into the estimating regression, the drift term in the
maintained DGP a turns out not to affect the asymptotic distribution.
Thus the asymptotic distribution of the OLS estimator p is invariant to
the value of a and the maintained hypothesis, Ho : p = 1 and 8 = 0.
Again we consider two DGPs:
(i) Random walk with drift only,
(ii) Random walk with linear trend.
(i) When the DGP is a random walk with drift, that is, p = 1 and 8 = 0,
the asymptotic distributions of p and the corresponding t-statistics are
as follows
JW(r)dW + A
T(p-l)
D
and
where
JW(r)dW-
(3.13)
(3.14)
D =
A = 12
I W(r)2dr - 12 ( f rW{r)dr
+ 12 / W(r)dr f rW(r)dr - 4 ( f W{r)dr
I rW{r)dr- - f W(r)dr\
[w(r)dr--W(l)] 
-W(l) 
[ W(r)dr
J 
2 
J 
J
The t-statistics for the OLS estimator a and 8 also follow the functionals
of the Wiener processes. Fuller (1976) tabulated the critical values. Part
of those critical values are given in table 3.1.
It is natural to consider the F-test of the joint hypotheses Ho : p = 1
and 8 = 0 and Ho : p = 1, a = 0 and 8 = 0. Dickey and Fuller (1981)
provide the asymptotic distributions of the F-statistics and tabulated

3.4 DF test with a linear deterministic trend 
71
Table 3.2. Asymptotic distributions of the t-ratios for different DGPs
Regression 
DGP 
Asymptotic distribution
No deterministic trend 
a = 8 = 0 
A function of Wiener processes
( a = 8 = 0 ) 
(equation 3.6)
With a drift 
a = 8 = 0 
A function of Wiener processes
( a ^ 0,8 = 0 ) 
(equation 3.12)
a ^ 0,8 = 0 
Normal distribution
With a linear trend 
a = 8 = 0 
A function of Wiener processes
(Q^0,^0) 
(equation 3.14)
a ^ 0,8 ^ 0 
Normal distribution
the critical values. Part of the critical values are given in table 3.1.
(ii) If the true DGP also has a linear trend, as shown earlier, the test
statistic of p has an asymptotic normal distribution.
For different DGPs, the asymptotic distributions of the t-ratio for the
unit root null from estimating regressions are summarized in table 3.2.
3.4-3 An illustrative example
The following example is given in Dickey and Fuller (1981). They applied
the above procedure for the logarithm of the quarterly Federal Reserve
Board Production Index 1950:1 to 1970:IV (110 observations). They
assume that the series is adequately represented by the model
yt = a + 8t + pyt-i + jSAj/t-i + et
where et are assumed to be iin(0,<72). The OLS estimates are
yt = 0.52 + 0.00121 - 0.119 yt-i + 0.498 Ayt-i, 
RSS = 0.056448
(0.15) (0.00034) 
(0.033) 
(0.081)
yt = 0.0054 + 0.447 Ayt-U 
RSS = 0.063211
(0.0025) (0.083)
yt = 0.511 Aj/t-i, 
RSS = 0.065966
(0.079)

72 
Unit roots
where RSS is the residual sum of squares. The number in parentheses
are the standard errors. The ^-statistic for the unit root null is
tp = -0.119/0.033 = -3.61
This leads to rejection of the unit root hypothesis at the 5 percent level,
since tp < —3.45. The F-statistic for the null hypothesis Ho : a = 0,6 =
0 and p = 1 is computed
F = (0.065966 - 0.056448)/3(0.056448/106) = 5.96
Since F > 4.88 in table 3.1, one can reject the unit root without a drift
at the 5 percent level. On the other hand, the F-statistic for the null
hypothesis HQ : 8 = 0 and p = 1 is computed
F = (0.063211 - 0.056448)/2(0.056448/106) = 6.34
Since F < 6.49 in table 3.1, one cannot reject the unit root with possible
drift at the 5 percent level. Thus there is some evidence to support the
unit root null hypothesis with possible drift at the 5 percent significant
level.
3.5 Specification of deterministic trends
It is hard to believe that the pure AR(1) model without the deterministic
trend describes well most of the macroeconomic variables. Almost all
macroeconomic variables usually show some tendency to increase over
time. For example GNP and consumption increase over time. This
suggests that it may be appropriate to incorporate the linear trend term
into the model. When we include the linear trend term into the model,
we can classify the time series into two important classes which imply the
different methods of eliminating the trend. These classes are the trend-
stationary process (TSP) and the difference-stationary process (DSP) In
order to identify which class macroeconomic series belong to, Nelson
and Plosser (1982) apply the above Dickey-Fuller tests for identifying
the classes of the macroeconomic series.
Besides the importance of the presence of the deterministic trend in
macroeconomic series, the functional form (or specification) of the deter-
ministic trend plays an essential role in the unit root testing procedure
and it is also closely related to the power and size of the unit root
tests. If we omit a trend variable, which is in the true DGP, from the

3.5 Specification of deterministic trends 
73
estimating regression, then the power of the t-test goes to zero as the
sample size increases (Perron, 1988). Thus it is important to include as
many deterministic regressors as there are deterministic components in
the trend function of the DGP. Otherwise the test will at best lose finite
sample power or at worst have power that goes to zero as the sample size
increases. On the other hand, it is desirable not to include extraneous
deterministic regressors. The power of a test decreases as additional
deterministic regressors are included.
Campbell and Perron (1991) argue that "the proper handling of de-
terministic trends is a vital prerequisite for dealing with unit roots."
Perron (1988) proposed a sequential testing strategy and argued that a
proper testing strategy should start from the most general trend spec-
ification and test down to more restricted specifications. However, in
the more general models which allow more than a simple linear trend,
such a sequential testing procedure cannot yet be applied given that the
distribution theory for the relevant statistics has not been derived.
Cochrane (1991a, p. 202) argued that Campbell and Perron's (1991)
advice is correct and sensible, but expressed some reservations:
One never knows the deterministic trends with great precision before analysis
begins. Economic theory does not give any guidance. ... "proper handling"
of deterministic trend is an impossible task. To a humble macroeconomist it
would seem that an edifice of asymptotic distribution theory that depends
crucially on unknowable quantities must be pretty useless in practice.
Recent research has demonstrated the importance of using a flexible
specification for the deterministic component. It is motivated by the
view that the linear time trend hypothesis is inappropriate for model-
ing the deterministic component of an economic time series. Various
models for the flexible deterministic trend have been proposed such as
the structural break model, the Markov switching model, and the Bayes
updating models. These will be discussed in later chapters.
Before discussing the application of the unit root tests for discrim-
inating the DSP and the TSP classes, it is important to consider the
possibility of the serial correlation of errors. The Dickey-Fuller test-
ing procedures, discussed so far, assume iid errors and yo = 0. These
are strong assumptions for the real world. In the next section we shall
discuss some modified testing procedures for unit roots under the as-
sumption of a wide class of errors where the serial correlation and some
heteroskedasticity of errors are allowed.

74 
Unit roots
3.6 Unit root tests for a wide class of errors
In general, the asymptotic distribution of the statistics we have con-
sidered so far, are not invariant to other parameters and depend on
the other regression parameters and the variances of errors. The most
important noninvariance shared by all regression models (discussed in
the previous section) is that the limiting (as well as finite sample) dis-
tributions of all the statistics considered depend upon the correlation
structure of the errors. More precisely, the limiting distributions de-
pend upon the ratio a2/a2, where
is the variance of the sum of errors and
is the variance of errors.
To derive the asymptotic distributions, Dickey and Fuller (1979, 1981)
assumed that the errors et were zzd(0, a2). It is easy to verify that this is
a sufficient condition for a2 — a2 (see section 3.2.2); it is not, however,
a necessary condition. This equivalence also holds, for example, with
errors that are martingale difference sequences under mild additional
conditions. Therefore, the limiting distributions obtained by Dickey
and Fuller are also valid in the presence of some heterogeneity in the
error sequence provided the errors are martingale differences. In general,
however, they cease to be appropriate when the errors are nonorthogonal
(or serially correlated) and thus a2 ^ a2.
When the errors et are correlated, there is a need to either change
the estimation method (adopt another regression model) or modify the
statistics to obtain consistent estimators and statistics. Dickey and
Fuller (1979) and Said and Dickey (1984) use the first approach of chang-
ing the estimating regressions using the parametric approach. Phillips
(1987a) and Phillips and Perron (1988) follow the second approach of
modifying the statistics using a nonparametric approach.
3.6.1 Changing the estimating equations: the ADF test
Consider the case in which the series of first-differences {Ayt} has a
stationary AR(p) representation with p known; i.e., (1 — L)a(L)yt = et,

3.6 Unit root tests for a wide class of errors
75
with a pth-order polynomial in L. Then, we can test the null hypothesis
of a unit root by estimating an autoregression of Ayt on its own lags
and yt-i using OLS
Ayt = a + pyt-i
et
(3.15)
This is known as the augmented Dickey-Fuller (ADF) regression. If
there is a unit root, under Ho : p = 1, the t-statistic for the unit root
null hypothesis follows the same DF distribution in the equation 3.12.
The same principle holds for the F-statistics.
The reason behind this result is that in a regression of 1(1) variable on
1(1) and 1(0) variables, the asymptotic distribution of the coefficient of
1(1) and 1(0) variables are independent. To see this, consider the DGP
yt = pyt-i + eu
et
Under the null of unit root HQ : p = 1, yt is 1(1) and Ayt =
Consider the estimating regression
yt = pyt-i + (3Ayt-i + et
is 1(0).
We will show that the asymptotic distributions of p and $ are indepen-
dent. We have to normalize the coefficients and write
yt =
Then
Tp 1
J
T*?2Vt-i
We have
Iv
f*E »?-!=• fw(r)dr
• 
Jo
et_! ^ / W(r)dW(r)
Jo
>2
t_x -> w ( e M ) - 1
Hence
T\/T
-$=[ W(r)dW(r) -» 0

76 
Unit roots
Thus the asymptotic distribution of Tp and VT/3 are independent. This
result can be used for showing that the distribution of p in the ADF
regression is the Dickey-Fuller distribution. Also the asymptotic distri-
bution of VT(J3 — (3) is normal.
This fact has been extended by Said and Dickey (1984) to the more
general case in which, under the null hypothesis, the series of first-
differences are of the general ARMA(p, q) form with p and q unknown.
They showed that a regression model, such as (3.15), is still valid for
testing the unit root null under the presence of the serial correlations of
errors, if the number of lags of Ayt introduced as regressors increases
with the sample size at a controlled rate T*. Essentially the moving-
average terms are being approximated by including enough autoregres-
sive terms.
Consider the model with the serial correlation in errors described by
yt = pyt-i + zt
and
zt = azt-i + et + /3et_i
where it is assumed that |a|<l,|/?|<l,yo = 0 and {et} is a sequence of
iid random variables. If \p\ < 1, then yt is a stationary ARIMA(2,0,l).
If p = 1 the series is an ARIMA( 1,1,1) process. We consider p = 1 as a
null hypothesis to be tested. Notice that
-/?)j(*t-j ~ 
azt-j-i)
3=0
and it follows that
yt - yt-i = (P- i)yt-i + (<*
Under the null hypothesis that p = 1 we see that zt = yt — yt-i- This
motivates us to estimate the coefficients by regressing the first-difference
Ayt on yt-i) Ayt~iy..., 
Ayt-k where fc is a suitably chosen integer. To
get consistent estimates of the coefficients it is necessary to let k be
a function T. We shall assume that Tl^k —> 0 and that there exist
c > 0,r > 0 such that ck > Txlr. In this case, the limiting distribution
of the ^-statistics of the coefficient on the lagged dependent variable yt-\
has the same Dickey-Fuller ^-distribution as when the errors are iid.

3.6 Unit root tests for a wide class of errors 
77
3.6.2 Choice of lag-length in the ADF test
It has been observed that the size and power properties of the ADF test
are sensitive to the number of lagged terms (k) used. See, for instance,
Schwert (1989) and Agiakoglou and Newbold (1992). Several guidelines
have been suggested for the choice of k. Ng and Perron (1995) examine
these in detail.
The guidelines are:
(i) Rule for fixing k at an arbitrary level independent of T
Ng and Perron do a detailed simulation study of this rule. For a
model with MA(1) parameter 0, they find that for 0 = —0.8, and
sample size T = 100, the size is 0.283 instead of 0.05 when k = 4.
The size is closer to the nominal size when k is increased to 10.
However, the size increases with sample size for negative values of
6. For instance, with 9 = —0.8 and k = 3, the exact size increases
from 0.455 to 0.598 as T increases from 100 to 500. The power, of
course, rises with sample size. As for AR errors, the size is close
to the nominal size if k > the true lag order of the AR processes,
but overparameterization results in loss of power. Power also rises
with sample size, although size distortions are qualitatively the
same. Overall, choosing a fixed k is not desirable.
(ii) Rule for fixing A; as a function of T
A rule commonly used is the one suggested by Schwert (1989)
which is to choose
k = Int{c(T/100)1/d}
Schwert suggest c = 12 and d = 4. The problem with such a rule
is that it need not be optional for all p and q in the ARMA(p, q).
(iii) Information based rules
The information criteria suggest choosing k to minimize an objec-
tive function that trades off parsimony against reduction in sum
of squares. The objective function is of the form
Ik = \og&l + k-Y
The Akaike information criterion (AIC) chooses CT = 2. The
Schwarz Bayesian information criterion (BIC) chooses CT = logT.
These are the most commonly used information criteria in econo-
metrics. (In chapter 8 we shall discuss the predictive information
criterion (PIC) suggested by Peter Phillips.) Ng and Perron ar-
gue that both AIC and BIC are asymptotically the same with

78 
Unit roots
ARMA (p, q) models and that both of them choose k propor-
tional to log T.
(iv) Sequential rules
Hall (1994) discusses two sequential rules in the context of pure
autoregressions. The first rule, called general to specific rule,
is to start with a large value of k (kmax)^ test the significance
of the last coefficient and reduce k iteratively until a significant
statistic is encountered. The other rule, called the specific to
general rule, is to start with a small k and increase k successively
until a nonsignificant coefficient is encountered. Hall showed that
the specific to general approach is not generally asymptotically
valid. He also found its performance to be inferior to that based
on the general to specific approach in ARMA models.
Ng and Perron (1995) compare AIC, BIC, and Hall's general to specific
approach through a Monte Carlo study using T = 100 and both AR and
MA processes. The major conclusions are:
(i) Both AIC and BIC choose very small values of k (e.g., k = 3).
This results in high size distortions, especially with MA errors,
(ii) Hall's criterion tends to choose higher values of k. The higher the
kmax is, the higher is the chosen value of k. This results in the
size being at the nominal level, but of course with a loss of power.
What this study suggests is that Hall's general to specific methods is
preferable to the others. DeJong et al. (1992) show that increasing k
typically results in a modest decrease in power but a substantial decrease
in size distortions. If this is the case the information criteria are at a
disadvantage because they result in a choice of very small values of k.
(For some opposite evidence arguing in favor of BIC compared with
Hall's method, see Stock (1994, p. 2781). However, he also says that
one should use either the Schwarz criterion or Hall's sequential method.)
3.6.3 Modifications of the test statistic: Phillips-Perron 
test
As we have seen in section 3.2.2, when the errors are not Ud, we have
, - l
T{p-:
where

3.6 Unit root tests for a wide class of errors 
79
Hence we can consider the modified test statistics
T 
-,-1
Jo
t=2
Since
W(r)2dr
/o
the adjusted test statistic has the Dickey-Fuller distribution.
The test statistics proposed by Phillips (1987a) and Phillips and Per-
ron (1988) are these modified test statistics, using the estimators s2 and
s?ri) which are consistent estimators of a\ and a2, respectively. The
consistent estimator of a2 is given by
t=l
Consistent estimation of a2 = liniT—oo T~1E((J2j=i 
ej)2) is more diffi-
cult. The problem is essentially equivalent to the consistent estimation
of an asymptotic covariance matrix in the presence of weakly dependent
and heterogeneously distributed observations This has been examined
by White and Domowitz (1984) and White (1984).
Under the stronger moment condition, Phillips (1987a) proved that
t = l 
T=lt=T + l
is a consistent estimator of a2. Howevers, s^ is not constrained to be
nonnegative. When there are large negative sample serial covariances,
Sj7 c a n take on negative values. Newey and West (1987) have suggested
a modification to variance estimators such as s^ which ensures that
they are nonnegative. The modification suggested is
t=l 
r=l 
t=r+l
where
They showed that s?nl is a consistent estimator of a2 under the same
conditions as s^i and that it is nonnegative by construction. Rather than

80 
Unit roots
using the first-differences et = yt — yt-i we may also use the residuals
et — yt — pyt_i from the least squares regression, since p —> 1 as T —> oo.
Inevitably the choice of Z will be an empirical matter. If we allow the
number of estimated autocovariances to increase as T —• oo but control
the rate of increase so that I = (^(T1/4), then s^i yields a consistent
estimator of a2. White and Domowitz (1984) and Perron (1988) provide
some guidelines for the selection of /. A preliminary investigation of the
sample autocorrelations of et = yt — yt-i will help in selecting an appro-
priate choice of /. Since the sample autocorrelations of first-differenced
economic time series usually decay quickly it is likely that in moderate
sample sizes quite a small value of / will be chosen. Another way to
approach the problem is to check the sensitivity of the results to various
values of /.
Phillips (1987a) and Phillips and Perron (1988) proposed the nonpara-
metric test statistics for the unit root null by using consistent estimates
of variances as follows (for simplicity of notation, we shall denote s^ by
(i) AR(1) without a drift
(ii) AR(1) with a drift
Zp 
= 
T ( p - l ) -
where y_x = £ f ~' yt/(T - 1).
(iii) AR(1) with a drift and a linear trend
zP = 
T(p-Q-
Z 
^
where Dx = det(X'X) 
and the regressors are X = 
(l,t,yt-i).

3.6 Unit root tests for a wide class of errors 
81
The limiting distributions of Zp and Zt are identical to those of K —
T(p — 1) and the t-statistics, respectively, when a2 = o2
e. Thus the
asymptotic critical values of the tests are the same as the asymptotic
critical values tabulated by Puller (1976) which are given in table 3.1.
Note that Phillips uses a for p and hence these tests are commonly
referred to as the Za and Zt tests. To maintain consistency of notation
with our previous discussion, we are defining them as the Zp and Zt
tests. The procedure of correcting test statistics has been widely used by
Phillips as well as several others in many other tests with nonstationary
time series.
3.6.4 A comparison of the two approaches
If the moving-average terms are important, the number of extra lags of
Ayt needed as regressors in the autoregressive correction may be quite
large. Furthermore, as shown by Schwert (1989), using Monte Carlo
simulations, the exact size of the test may be far from the nominal
size if the order of the autoregressive correction is not increased as the
sample size increases. An adjustment is essential because the effect of
the correlation structure of the residuals on the shape of the distribution
becomes more precise. This point highlights the fact that with impor-
tant MA components in the structure of the series {yt} a large number of
nuisance parameters may be needed in the estimation. Since we further-
more lose one effective observation for each extra lag of Ayt introduced,
this implies that the Said and Dickey approach may have substantially
lower power when MA terms are more important than if the errors were
iid.
On the other hand, several simulation studies have shown that Phillips
and Perron's nonparametric tests have serious size distortions in finite
samples when the data generating process has a predominance of neg-
ative autocorrelations in first-differences (Schwert, 1989; Phillips and
Perron, 1988; DeJong et al, 1992). This has often been taken to sug-
gest that the Phillips and Perron's nonparametric tests may be less re-
liable than the ADF tests when there is a predominance of negative
autocorrelations in first-differences. However, it should be noted that
Perron and Ng (1996) suggested useful modifications of the PP tests,
that solve this problem. These are discussed in section 4.3 of chapter
4.

82 
Unit roots
3.7 Sargan-Bhargava and Bhargava tests
The preceding tests for unit roots are in the framework of the DF tests.
We shall discuss here some alternative approaches. Sargan and Bhar-
gava (1983) suggest tests in the Durbin-Watson framework. They gen-
eralize the Durbin-Watson statistic (used for testing serial correlation)
to the problem of testing the hypothesis that the residuals from the
least squares regression follow a random walk. They also discuss the
Berenblutt-Webb statistic (which they argue is the preferred alternative)
and suggest computing the significance points using the Imhof routine.
This statistic is given by
_ e^e
where e and u are respectively the least squares residuals from the first-
difference equation and the levels equation respectively.
Bhargava uses the framework we discussed earlier in section 2.9 of
chapter 2, which, as argued there, is a better approach to unit root
testing than the Dickey-Fuller approach. He suggests some tests for unit
roots along the lines of the tests developed in Sargan and Bhargava. He
suggests four test statistics Ri,R2,Ni, 
and 7V2, but since the last two
are slight modification of the R\ and R^ we shall discuss only these here.
The statistics are defined as
ti 
=
where y = ^i=1 Vi/T. The simple random walk hypothesis is rejected for
large values of Ri. This statistic is the modified DW statistic in Sargan
and Bhargava.
One statistic that is related to Ri is the statistic MSB (discussed in
Stock, 1990) defined as
where s2 is an estimate of the variance of yt. For s2 Bhargava used
T~x ^2t=i(yt~y)2- But it is customary to use the Newey-West estimator
for s2 (discussed earlier in the context of Phillips-Perron tests). A better
estimator suggested by Perron and Ng is discussed in section 4.3 of
the next chapter. Perron and Ng used this in their study of the MSB
statistic.
Since the sum of squares of an 1(1) variables is OP(T2) but that of an

3.7 Sargan-Bhargava and Bhargava tests 
83
1(0) series is OP(T) the MSB statistic can be used to test for a unit root.
For an 1(0) series MSB is related to the PP tests by the relation
Zt = MSB • Zp
Note that MSB tends to zero. The unit root hypothesis is rejected in
favor of stationarity when MSB is smaller than some appropriate critical
value. Critical values with yt demeaned and detrended are given in Stock
(1990).
The second test statistic R2 suggested by Bhargava, is denned as
A
where
t=2
B = jfh^
-(T - t)Vl - (T - l)(y - (yi 4- yT)/2)f
Schmidt and Phillips (1992) derive Bhargava's R2 test statistic by
considering an LM (Lagrangian Multiplier) test or score test for p in the
model
yt = 7o +7i*+ ^t 
(3-16)
ut = put-! + et
As noted in section 2.9 of chapter 2, this model is equivalent to
yt = /?o + Pit + pyt-i + et 
(3.17)
where
A> = 7o(l-p)
and
This equation can also be written as
Ayt = Po + Pit + (P - l)2/t-i + et

84 
Unit roots
or
et 
(3.18)
Let St-i be the residual from a regression of yt-i on an intercept and t.
Then the estimator of </> from equation (3.18) is the same as an estimate
of (f) from the equation
Ayt = intercept + cj>St-i + et, t = 1,2,..., T 
(3.19)
If the least squares estimator of (j) from the equation (3.19) is 0, then
the usual coefficient test based on T</> and the corresponding ^-statistic
have the DF distribution (3.11) and (3.12) under the hypothesis 0 = 0.
To derive the LM statistic, Schmidt and Phillips (referred to as SP
hereafter) assume that the et are /7V(0,cr2) in (3.16). Under the hy-
pothesis p = 1 we can write
Vt = (7o + uo) + 71* + ei H 
1" e*
= 
6 + 7it + ei H 
h et
where 6 = 70 + wo-
Ayt = 71 + et
Hence the estimator of 71 which we shall refer to as 71 is given by
71 = mean of Ayt = T _ *
Also
6 = 2/i - 7i
Let
These are the residuals using estimates of the parameters in (3.16) under
the hypothesis p = 1.
Note that
St = 
yt-yi-{t-
1 
[(T-l)yt-(t-l)yT-(T-t)yi}
T - l
Hence So = ST = 0 and
-, T
- 
= " - —
1 t=l

3.7 Sargan-Bhargava and Bhargava tests 
85
Table 3.3. Critical values for the Schmidt-Phillips LM test
Sample size
25
50
100
500
coeff.
1%
-20.4
-22.8
-23.8
-25.3
test
5%
-15.7
-17.0
-17.5
-18.1
t-test
1%
-3.90
-3.73
-3.63
-3.59
5%
-3.18
-3.11
-3.06
-3.04
Source: Schmidt and Phillips (1992).
From these results SP note that the denominator B in Bhargava's R2
statistic mentioned earlier can be written as ^2(St — S)2.
SP show that (we shall omit the details) the LM test statistic is ob-
tained from the estimator 0 of 0 in the equation
Ayt = intercept 4- <f>St-i 4- error 
(3.20)
Then
is the statistic for the coefficient test and f the statistic for the cor-
responding t-test of the hypothesis (f> = 0 in (3.20). SP note that the
difference between the LM test statistics and the DF test statistics is
that the DF tests use St~i and the LM tests use St-i as a regressor in
the same equation (see equations 3.19 and 3.20). Since St is obtained
from a regression of yt on a time trend, if yt is 1(1), this is a spurious re-
gression and SP argue that for this reason the LM tests can be expected
to be more powerful than the DF tests. This they confirm by a Monte
Carlo study. Except when UQ = Uo/ae is large in absolute value, their
experiments reveal that the tests based on f and p are more powerful
than the DF tests.
SP derive the asymptotic distributions of p and f. The finite sample
distributions are complicated but they tabulate the percentiles by sim-
ulation. They are given in table 3.3. The lower tail critical values are
smaller than the corresponding lower tail values of the DF tests (tab-
ulated in table 3.1). SP also provide tables of critical values for the f
and p tests under polynomial trends of order 2, 3, and 4 (table IB of
the SP paper). For the case where e* are not iid, they suggest applying
the same type of corrections to the LM statistics as those we discussed
earlier for the PP tests.

86 
Unit roots
Finally SP show that p is related approximately to Bhargava's R2 test
statistic mentioned earlier by the relation
T
Hence in subsequent literature the LM tests are often referred to BSP
(Bhargava-Schmidt-Phillips) tests. Bhargava derived his tests as best
invariant tests and this guarantees invariance with respect to 70,71, and
ae in (3.16). Since St depends on only et, the LM test is also invariant
to 70,71, and ae. Under the alternative hypothesis the distribution of f
and p in still invariant to 70,71, and ae but depends on UQ = uo/cre.
3.8 Variance ratio tests
The measurement of the degree of persistence in a time series is another
way of evaluating the presence of a unit root. Cochrane (1988) proposes
a variance ratio (VR) statistic for this purpose. The VR is the variance
of the fcth difference of a time series divided by fc-times the variance of
its first-difference. Thus
where T4 = var(yt — yt-k)/k. 
It is straightforward to show that
can be written as the weighted sum of the correlations Vj between
and Ayt-j.
k
Cochrane uses the following estimator for the variance ratio
k ^ j t 
- yt-k)
var{yt-vt-i)
The second term is a bias correction factor. Anderson (1971) showed
that as T —> 00, k —> 00, and k/T —>• 0, this estimator has a limiting
normal distribution with mean VRk and variance AkVR^/^T.
Lo and MacKinlay (1988) derived the asymptotic distribution of VRk
for the case of a random walk null and normal iid innovations under the
assumption that k is fixed and T goes to infinity. They show that the
distribution is normal with mean 1 and variance 2(2k — l)(k — l)/3kT.
Note that when the null model is a random walk, VRk is 1 and with k
large, this expression agrees with the one derived by Anderson (1971).

3.9 Tests for TSP versus DSP 
87
There are several values of k for which the variance ratio test can be
used. In practice it is customary to consider VRk for different values of
k and consider a model rejected when at least some of the VR statistics
provide evidence against it. This method of testing penalizes the null
hypothesis and the correct critical values of the tests are not known
because the tests are correlated.
Cecchetti and Lam (1994) examine (i) the accuracy of the asymptotic
distributions of the VR test statistics in small samples and (ii) the conse-
quences of the use of multiple tests for different values of k. They argue
that there are substantial size distortions with the use of the asymptotic
approximations and more size distortions with the use of the sequential
testing procedure with different values of k. They suggest using a joint
test based on all the VRk statistics and obtaining the critical values of
these test statistics using Monte Carlo methods.
3.9 Tests for TSP versus DSP
Ever since the publication of the paper by Nelson and Plosser (1982),
macroeconomists have been interested in unit roots in time series. Nelson
and Plosser found that they could not reject the null of an autoregres-
sive unit root in 13 out of 14 US macroeconomic time series, in some
cases spanning over 100 years. The existence of a unit root was inter-
preted as having important implications for the theory of business cycles
and the persistence of the effect of real shocks to the economy. Though
some economists like Cochrane (1991b) argued that the evidence on unit
roots is empirically ambiguous and also irrelevant to the question of the
persistence of the effect of real shocks, the literature on unit roots kept
proliferating and more and more tests for unit roots have been appear-
ing.
Nelson and Plosser started with the problem of discrimination between
the trend-stationary process (TSP) and the difference-stationary process
(DSP) models
1 u± : yt — OL -f- ot T~ Ut
DSP: Ayt = a + ut
where ut is stationary. This is a model selection problem. They, how-
ever, approach it as a hypothesis testing problem, a test for a nested
hypothesis.
To test the hypothesis that a time series belongs to the DSP class

88 
Unit roots
against the alternative hypothesis that it belongs to the TSP class,
Nelson and Plosser employed the unit root tests described in the previ-
ous section. They started with the TSP model and the assumption of
the first-order serial correlation in errors
ut = put-i + et
This formulation for unit root testing has been advocated by Bhargava
(1986). This gives
yt = A) + Pit + p [yt-i - A) - Pi(t - 1)] + et
Thus the reduced form for this model can be written as
yt = a + 6t + pyt-i + et
where et is assumed to be im(0, cr2) and 6 = 0 if p = 1. They test the
unit root null hypothesis Ho : p = 1 and 6 = 0. (Recall that the value
of constant a does not affect the asymptotic distribution of the OLS
estimates p and 6 when the estimating regression includes t as an inde-
pendent variable (see section 3.4). If the null hypothesis is rejected, then
yt belongs to the TSP class. If the unit root null cannot be rejected, then
it belongs to the DSP class. After observing the sample autocorrelations
of the first-differences of US historical data, Nelson and Plosser assume
moving-average errors (the presence of the serial correlation in errors)
and incorporate the additional regressors Ayt~k into the model to cor-
rect the serial correlations in the errors. In other words they employed
the ADF approach.
Nelson and Plosser analyzed a set of 14 US macroeconomic time series.
They used the logarithm of the series, except for the interest rate series,
instead of level based on the fact that (Nelson and Plosser, 1982 p. 141):
The tendency of economic time series to exhibit variation that increases in
mean and dispersion in proportion to absolute level, motivates the trans-
formation to natural logs and the assumption that trends are linear in the
transformed data.
Table 3.3 shows part of their results. They found that all 14 series,
except the unemployment rate, were characterized by the DSP class.
It is often argued that the way we estimate the equation, whether in
levels or in first-differences, depends on whether the series is TSP or
DSP respectively. If it is TSP and we use first-differences, then we have
overdifferencing. If it is DSP and we estimate the equation in levels,

3.10 Forecasting from TS versus DS models 
89
Table 3.4. Nelson and Plosser's results
Series
Real GNP
Nominal GNP
Real per capita GNP
Industrial production
Employment
Unemployment rate
GNP deflator
Consumer prices
Wages
Real wages
Money stock
Velocity
Interest rate
Common stock prices
Sample size (T)
62
62
62
111
81
81
82
111
71
71
82
102
71
100
Lag length (k)
2
2
2
6
3
4
2
4
3
2
2
1
3
3
tp
-2.99
-2.32
-3.04
-2.53
-2.66
-3.55*
-2.52
-1.97
-2.09
-3.04
-3.08
-1.66
0.686
-2.05
Note: The values of tp are smaller than the critical value —3.45
at the 5% significance level.
Source: Nelson and Plosser (1982, table 5, p. 151).
then we have underdifferencing. There has been some debate in the
literature on the overdifferencing versus underdifferencing issue, arguing
that the former is a less serious error than the latter. However, it is more
important to take the serial correlation structure in both the models into
account, and when this is done the issue of over- versus underdifferencing
becomes a nonissue. See McCallum (1993), who suggests estimating the
equation in both levels and first-differences and choosing the one that
requires the smaller amount of correction to remove autocorrelation of
the residuals, and illustrates this with two samples.
3.10 Forecasting from TS versus DS models
One of the main issues that has been discussed in the literature on
unit roots is the use of unit root tests in deciding whether to use the
trend-stationary (TS) model or the difference-stationary (DS) model
for forecasting. Campbell and Perron (1991) argue that unit root test
procedures can be practically useful for forecasting even in small samples
where they have only a limited ability to distinguish between TS and
DS alternatives. They provide evidence for this through a Monte Carlo
study. They generated 5,000 samples of length 100 from the ARMA(1,1)

90 
Unit roots
process
xt = fat-i + ut + 0ut-i
For 0 = 1.0 they consider the cases 0 = (0.98, 0.95, 0.90, 0.80, 0.50). For
0 = 0 they consider the cases </> = (1.0, 0.98, 0.95, 0.90, 0.80, 0.50). For
each case they consider one-period ahead and 20-period ahead forecasts
from an autoregressive model in levels and an autoregressive model in
differences. In the former case a linear trend is estimated whereas in the
latter case the mean of the differenced data is estimated. For each model
a lag length k is chosen with the maximum lag fixed at kmax = 6 (The
optimal k is decided by starting with k = 6 and working down until the
last coefficient in the AR(fe) model is significant.) For each sample and
forecast horizon, out of sample mean squared errors of forecasts were
calculated using 25 draws of the DGP.
Campbell and Perron find that stationary forecasting models are su-
perior for all processes with <f> = 1 and 6 < —0.9, while unit root fore-
casting models are superior for all processes with 6 = 0 and <fi > 0.9
(one-period ahead forecast) or 0.95 (20-period ahead forecast). They
also consider a mixed strategy which is a pre-testing method: use the
levels model if the Said-Dickey and Phillips-Perron tests reject the unit
root at the 5 percent level and use the difference model if it does not.
The performance of this mixed strategy was found to be closer to that
of the better of the two individual strategies.
In his comment on the Campbell and Perron paper, Cochrane (1991a)
argues that the Monte Carlo study does not throw any light on the unit
root question. He presents his own Monte Carlo study which shows
that for the ARMA(1,1) model with </> = 0.95 and 0 = 0.5, the AR(1) in
differences provides better one-period ahead forecasts whereas the AR(1)
in levels provides the better forecasts for 20- and 50-period horizons.
Over the long-run stationarity asserts itself. He used only one lag to
separate the issues of choice of lag length from the unit root question.
However, the forecasting issue is separate from that of whether the
unit root tests can distinguish between the TS and DS processes. The
main question is: for most macroeconomic time series, is it better to fore-
cast using the trend-stationary (TS) model or the difference-stationary
(DS) model? One common presumption has been that it is better to
use the DS model because it is better in characterizing the uncertainty
associated with forecasting these series over long horizons (see Dickey,
Bell, and Miller (1986) and Phillips (1991) for this view). On the other
hand, Meese and Geweke (1984) studied the accuracy of forecasts gen-

3.10 Forecasting from TS versus DS models 
91
erated by a wide range of autocorrelated procedures for 150 quarterly
and monthly macroeconomic time series, and found that forecasts based
on detrended data were, on average, more accurate than those based on
differenced data.
Interval forecasts
Meese and Geweke (1984) considered only point forecasts but not the
standard errors of the forecasts. Sampson (1991), for instance, shows
that without parameter uncertainty, forecast variance for the unit root
model grows with the forecast horizon, whereas for the trend-stationary
model, it tends to a constant. On the other hand, with the estimated
parameters, forecast variance grows with the square of the variance for
both models. However, the leading terms in the expressions for the
forecast variances, as we shall presently see, are roughly proportional
to 12n2 /N3 for the trend-stationary model and 4n2/N for the unit root
model, where n is the forecast horizon and N is the length of the time
series on which the estimation is based. Thus the leading terms are in
the ratio 1/N2 and hence the forecast variances are much higher for
the unit root model than the trend-stationary model. In Sampson's
empirical example N = 41 and n = 60. This is too long a forecast
horizon to be of any practical interest. Usually the forecast horizon
beyond which predictability ceases occurs at relatively small values such
as n = 8 ~ 10 periods.
Sampson considered a general stationary process for the errors but to
simplify matters we shall consider iid errors. Consider the two models
TS 
yt = fi + (3t + ut, 
ut~ m(0, a2
u)
DS 
Ayt=/jL + vt, 
vt~in(0,<rl)
For the TS model we have
\ NV*(ji-ri 1 
[ 
2 / 
4 
-6
The rate of convergence of J3 to (3 is faster in the TS model than in the
unit root model. The n-period ahead forecast error is given by
(// - jl) + (N + n)0 - (3) + uN+n
The variance of this is
-i 
I 
- " " V" ' 
' 
' " / 
i
N 
N2 
TV3
For the unit root model, the variance is a complicated expression.

92 
Unit roots
3.11 Summary and conclusions
This chapter discusses the basic unit root tests: the Dickey-Fuller (DF),
the augmented Dickey-Fuller (ADF) and the Phillips-Perron (PP) tests.
Some other tests are also discussed: Sargan-Bhargava tests and variance
ratio tests.
Although often used, the DF, ADF, and PP tests lack power against
meaningful alternatives and should not be used any more. In the next
chapter we shall discuss useful modifications of these tests. As for vari-
ance ratio tests, which are also commonly used, using the asymptotic
distributions results in substantial size distortions in small samples, it is
best to obtain small sample critical values using Monte Carlo methods.
If more than one variance ratio test statistic is used, it is best to apply
a joint test with critical values obtained by Monte Carlo methods.
We discussed the Sargan-Bhargava tests here because they form the
basis of other tests discussed in subsequent chapters. Finally we also re-
view the controversy surrounding the issue of forecasting from detrended
versus differenced data.
References
Agiakoglou, C. and P. Newbold (1992), "Empirical Evidence on
Dickey-Fuller type tests," Journal of Time Series Analysis, 13,
471-483.
Anderson, R.L. (1942), "Distribution of the Serial Correlation Coeffi-
cient," Annals of Mathematical Statistics, 13, 1-13.
Anderson, T.W. (1971), The Statistical Analysis of Time Series, Wiley,
New York.
Ansley, C.F. (1979), "An Algorithm for the Exact Likelihood of Mixed
Autoregressive Moving Average Process," Biometrika, 66, 59-
65.
Bhargava, A. (1986), "On the Theory of Testing for Unit Roots in
Observed Time Series," Review of Economic Studies, 53, 369-
384.
Billingsley, P. (1968), Convergence of Probability Measures, John Wi-
ley, New York.
Box, G.E.P. and CM. Jenkins (1970), Time Series Analysis Forecasting
and Control, Holden-Day, San Francisco.
Box, G.E.P. and D.A. Pierce (1970), "Distribution of Residual Auto-
correlations in Autoregressive Integrated Moving Average Time

References 
93
Series Models," Journal of the American Statistical Association,
65, 1509-1526.
Campbell, J.Y. and P. Perron (1991), "Pitfalls and Opportunities:
What Macroeconomists Should Know about Unit Roots," in
O.J. Blanchard and S. Fisher (eds.),NBER Macroeconomics An-
nual, The MIT Press, 141-201.
Cecchetti, S.G. and P.S. Lam (1994), "Variance-Ratio Tests: Small-
Sample Properties with an Application to International Output
Data," Journal of Business and Economic Statistics, 12, 177-
186.
Chan, N.H. and C.Z. Wei (1987), "Asymptotic Inference for Nearly
Nonstationary AR(1) Processes," Annals of Statistics, 15, 1050-
1063.
Cochrane, J.H. (1988), "How Big is the Random Walk in GNP?" Jour-
nal of Political Economy, 96, 893-920.
(1991a), "Comment on Campbell and Perron," in O.J. Blanchard
and S. Fisher (eds.),NBER Macroeconomics Annual, The MIT
Press, 201-210.
(1991b), "A Critique of the Application of Unit Root Tests," Journal
of Economic Dynamics and Control, 15, 275-284.
Davidson, J.E.H. (1981), "Problems with the estimation of moving-
average processes," Journal of Econometrics, 19, 295-310.
DeJong, D.N., J.C. Nankervis, N.E. Savin, and C.H. Whiteman (1992),
"The Power Problems of Unit Root Tests for Time Series with
Autoregressive Errors," Journal of Econometrics, 53, 323-343.
Dickey, D.A. and W.A. Fuller (1979), "Distribution of the estimators
for autoregressive time series with a unit root," Journal of the
American Statistical Association, 74, 427-431.
(1981), "Likelihood Ratio Statistics for Autoregressive Time Series
With a Unit Root," Econometrica, 49, 1057-1072.
Dickey, D.A., W.R. Bell, and R.B. Miller (1986), "Unit Roots in Time
Series Models: Tests and Implications," American Statistician,
40, 12-26.
Donsker, M.D. (1951), "An Invariance Principle for Certain Probabil-
ity Limit Theorems," Memoirs of the American Mathematical
Society, 6, 1-12.
Evans, G.B.A. and N. E. Savin (1981), "Testing for Unit Roots: 1,"
Econometrica, 49, 753-779.
(1984), "Testing for Unit Roots: 2," Econometrica, 52, 1241-1269.

94 
Unit roots
Friedman M. and A.J. Schwartz (1963), A Monetary History of the
United States 1867-1960, Princeton University Press, Princeton,
NJ.
Fuller, W. A. (1976), Introduction to Statistical Time Series, New York,
Wiley.
(1996), Introduction to Statistical Time Series, 2nd edn, New York,
Wiley.
Gould J.P. and C.R. Nelson(1974), "The Stochastic Structure of the
Velocity of Money," American Economic Review, 64, 405-417.
Hall, A. (1992), "Testing for a Unit Root in Time Series Using In-
strumental Variable Estimation with pretest Data-Based Model
Selection," Journal of Econometrics, 54, 223-250.
(1994), "Testing for a Unit Root in Time Series with pretest Data-
Based Model Selection," Journal of Business and Economic
Statistics, 12, 461-470.
Hall, P. and C.C. Heyde (1980), Martingale Limit Theory and Its Ap-
plications, Academic Press, New York.
Hamilton, J.D.(1989), "A New Approach to the Economic Analysis of
Nonstationary Time Series and the Business Cycle," Economet-
rica, 57, 357-384.
Herrndorf, N. (1984), "A Functional Central Limit Theorem for Weakly
Dependent Sequences of Random Variables," Annals of Proba-
bility, 12, 141-153.
Imhof, J. P. (1961), "Computing the Distribution of Quadratic Forms
in Normal Variables," Biometrika, 48, 419-426.
Lo, A. W. and A.C. MacKinlay (1988), "Stock Market Prices Do Not
Follow Random Walks: Evidence From Simple Specification
Tests," Review of Financial Studies, 1, 41-66.
Maddala, G.S. (1992), Introduction 
to Econometrics, 
2nd edn,
Macmillan, New York.
Mann, H.B. and A. Wald (1943), "On Stochastic Limit and Order
Relationships," Annals of Mathematical Statistics, 14, 217-277.
McCallum, B.T. (1993), "Unit Roots in Macroeconomic Time Series:
Some Critical Issues," Economic Quarterly, Federal Reserve
Bank of Richmond, 79, 13-43.
McLeish, D.L. (1975), "Invariance Principles for Dependent Variables,"
Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebi-
ete, 32, 165-178.
Meese, R. and J. Geweke (1984), "A Comparison of Autoregressive

References 
95
Univariate Forecasting Procedures for Macroeconomic Time Se-
ries," Journal of Business and Economic Statistics, 2, 191-200.
Nabeya, S. and K. Tanaka (1990), "A General Approach to the Limit-
ing Distribution for Estimators in Time Series Regression With
Nonstable Autoregressive Errors," Econometrica, 58, 145-163.
Nankervis, J.C. and N.E. Savin (1985), "Testing the Autoregressive
Parameter With the t-statistic," Journal of Econometrics, 27,
143-161.
Nelson, C. R. and C. I. Plosser (1982), "Trends and Random Walks in
Macroeconomic Time Series," Journal of Monetary Economics,
10, 139-162.
Newey, W.K. and K.D. West (1987), "A Simple Positive Semi-Definite
Heteroskedasticity and Autocorrelation-Consistent Covariance
Matrix," Econometrica, 55, 703-708.
Ng, S. and P. Perron (1995), "Unit Root Tests in ARMA Models With
Data-Dependent Methods for the Selection of the Truncation
Lag," Journl of American Statistical Association, 90, 268-281.
Pan, J. J. (1968), "Distribution of the non-circular serial correla-
tion coefficient," American Mathematical Society and Institute
of Mathematical Statistics, Selected Translations in Probability
and Statistics, 7, 281-292.
Perron, P. (1988), "Trends and Random Walks in Macroeconomic Time
Series: Further Evidence from a New Approach," Journal of
Economic Dynamics and Control, 12, 297-332.
(1989), "The Great Crash, the Oil Price Shock, and the Unit Root
Hypothesis," Econometrica, 57, 1361-1401
(1991), "A Continuous Time Approximation to the Stationary First-
Order Autoregressive Model," Econometric Theory, 7, 236-252.
Perron, P. and S. Ng (1996), "Useful Modifications to Some Unit Root
Tests with Dependent Errors and Their Local Asymptotic Prop-
erties," Review of Economic Studies, 63, 435-465.
Phillips, P. C. B. (1977), "Approximations to Some Finite Sample Dis-
tributions Associated with a First-order Stochastic Difference
Equation," Econometrica, 45, 463-485.
(1987a), "Time Series Regression with a Unit Root," Econometrica,
55, 277-301.
(1987b), "Towards a Unified Asymptotic Theory for Autoregression,"
Biometrika, 74, 535-547.
(1988a), "Regression Theory for Near-Integrated Time Series,"
Econometrica, 56, 1021-1043.

96 
Unit roots
(1988b), " Weak Convergence to the Matrix Stochastic Integral
Jo BdB1'," Journal of Multivariate Analysis, 24, 252-264.
(1991), "To Criticize the Critics: An Objective Bayesian Analysis of
Stochastic Trends," Journl of Applied Econometrics, 6, 333-364.
(1992), "Bayesian Model Selection and Prediction with Empirical
Applications," Cowles Foundation Discussion Paper No. 1023.
(1994), "Bayes Models and Forecasts of Australian Macroeconomic
Time Series," in C. Hargreaves (ed.),Nonstationary Time Series
Analysis and Cointegration, Oxford University Press, Oxford,
chapter 3.
Phillips, P.C.B. and P. Perron (1988), "Testing for a Unit Root in Time
Series Regression," Biometrika, 75, 335-346.
Quenouille, M. (1957), The Analysis of Multiple Time Series, Charles
Griffin, London.
Rao, M.M. (1961), "Consistency and Limit Distributions of Estimators
of Parameters in Explosive Stochastic Difference Equations,"
Annals of Mathematical Statistics, 32, 195-218.
Rubin, H. (1950), "Consistency of maximum likelihood estimates in the
explosive case," in T. C. Koopmans (ed.) Statistical Inference
in Dynamic Economic Models, Wiley, New York, 356-364.
Said, S.E. and D.A. Dickey (1984), "Testing for Unit Roots in
Autoregressive-Moving Average Models of Unknown Order,"
Biometrika, 71, 599-607.
Sampson, M. (1991), "The Effect of Parameter Uncertainty on Fore-
cast Variances and Confidence Intervals for Unit Root and Trend
Stationary Time-Series Models," Journal of Applied Economet-
rics, 6, 67-76.
Sargan, J.D. and A. Bhargava (1983), "Testing Residuals from Least
Squares Regression for Being Generated by the Gaussian Ran-
dom Walk," Econometrica, 51, 153-174.
Satchell, S. E. (1984), "Approximation to the Finite Sample Distribu-
tion for Nonstable First Order Stochastic Difference Equations,"
Econometrica, 52, 1271-1289.
Schmidt, P. and P.C.B. Phillips (1992), "LM Tests for a Unit Root
in the Presence of Deterministic Trends," Oxford Bulletin of
Economics and Statistics, 54, 257-287.
Schwert, G.W. (1989), "Tests for Unit Roots: A Monte Carlo Investiga-
tion," Journal of Business and Economic Statistics, 7, 147-159.
Stock, J.H. (1990), "A Class of Tests for Integration and Cointegra-
tion," manuscript, Harvard University.

References 
97
(1994), "Unit Roots, Structural Breaks, and Trends," in R.F. Engle
and D.L. McFadden (eds.), Handbook of Econometrics, vol. IV,
North-Holland, Amsterdam.
Tanaka, K. (1996), Time Series Analysis: Non-Stationary and Non-
Invertible Distribution Theory, John Wiley, New York.
West, K.D. (1988), "Asymptotic Normality, When Regressors Have a
Unit Root," Econometrica, 56, 1397-1417.
White, H. (1984), Asymptotic Theory for Econometricians, Academic
Press, New York.
White, H. and I. Domowitz (1984), "Nonlinear Regression with Depen-
dent Observations," Econometrica, 52, 143-162.
White, J.S. (1958), "The Limiting Distribution of the Serial Correla-
tion Coefficient in the Explosive Case," Annals of Mathematical
Statistics, 29, 1188-1197.
Zivot, E. and D.W.K. Andrews (1992), "Further Evidence on the Great
Crash, the Oil Price Shock, and the Unit Root Hypothesis,"
Journal of Business and Economic Statistics, 10, 251-270.

4
Issues in unit root testing
4.1 Introduction
In the preceding chapter we introduced some tests (ADF and Phillips-
Perron tests) routinely used in testing for unit roots. There are, however,
several problems with these tests - particularly their low power. In this
chapter we discuss these problems and some solutions and alternatives
that have been suggested, but are not wide spread in practice. The main
issues that need to be discussed are:
(i) The low powers of unit root tests: what can be done to improve
this?
(ii) The problem of over differencing and moving-average (MA) unit
roots.
(iii) Use of instrumental variable (IV) methods (IV tests).
(iv) Using stationarity as null - rather than unit root as null.
(v) Devising more powerful tests,
(vi) Does frequency of observation matter (monthly versus quaterly
data, quaterly versus annual data, and so on)?
(vii) Can we increase the sample size and power by using panel data
(panel data unit root tests)?
(viii) What are the consequences of uncertainty about unit roots?
(ix) What are the appropriate significance levels to use on unit root
tests? Isn't a unit root test a pre-test?
(x) Is 1(1) the only type of nonstationarity? What about tests for
1(0) versus I(d) with d > 0 but d / 1.
(xi) What about a multivariate approach to unit root testing instead
of the often used univariate approach?
These are the problems we shall be discussing in this chapter.
98

4.1 Introduction 
99
These issues will not be discussed in sequence but the following sec-
tions address these issues. The sections are organized around the theme
of the poor power of unit root tests, and what to do about the problem.
We shall discuss a large number of unit root tests just so that we are
not guilty of omitting them, although we do not believe that many of
them are useful. The large number of unit root tests is a consequence
of the fact that there is no uniformly powerful test of the unit root
hypothesis (see Stock, 1994b). We shall, therefore, give an overview of
the chapter and a clarification of the tests discussed. Then we shall
outline what tests we think are important and what are not.
In section 4.2 we discuss in detail the evidence on size distortions and
low power of the commonly used unit root tests such as the ADF and
PP (Phillips-Perron) tests. This gives a diagnosis of the problem. The
next question is what to do about it. This is discussed in section 4.3.
One of the major problems noted is the presence of MA components
in the errors which cannot be taken into account without very long auto-
regressions. Hence, in section 4.3.1 we discuss unit root tests in ARMA
models. Another solution is to use instrumental variables. Hence, in
sections 4.3.2 and 4.3.3 we discuss unit root tests based on instrumental
variable regressions.
The remaining parts of section 4.3 are devoted to modifications of the
PP tests (by Perron and Ng) and ADF test (by Elliott, Rothenberg, and
Stock), tests based on weighted symmetric estimators (Fuller), and tests
based on reverse and forward regressions (Leybourne).
All these tests provide improvements over the PP and ADF tests.
There is, as yet, no comprehensive study comparing all these tests. But
our guess is that such studies will not provide any clearcut evidence in
favor of one or the other. The only clearcut evidence that would emerge
(which is already there in the several separate studies comparing them
with the existing tests) is that they clearly dominate the ADF and PP
tests. Hence, ADF and PP tests, still often used, should be discarded in
favor of these tests.
Given that the major problem with tests based on autoregressions are
high MA roots, tests have been developed for MA unit roots. These
are discussed in section 4.4. Another issue that has been raised is: why
do we assume the unit root as the null hypothesis? This problem does
not arise in the Bayesian approach (see chapter 8) where the null and
alternative are on an equal footing. Tests using stationary as null are
discussed in section 4.5. These tests, however, have the same type of
problems as the ADF and PP tests. They have also been suggested as

100 
Issues in unit root testing
useful for confirmatory analysis that is in conjunction with the ADF or
PP tests. This issue is discussed in section 4.6 where questions have
been raised about the usefulness of such confirmatory analysis.
The rest of the chapter is devoted to discussions of other tests that
have been suggested to solve the poor power problem of unit root tests,
by increasing the number of observations, by increasing the frequency
of observations (section 4.7), and using panel data (section 4.9). Some
other problems discussed are: uncertain unit roots and the pre-testing
problem (section 4.10) and unbiased estimation as a solution to unit root
testing (section 4.12).
The chapter thus discusses several unit root tests but they follow a
logical order as outlined above. We also discuss some other issues raised
in unit root testing.
4.2 Size distortion and low power of unit root tests
Schwert (1989) first presented Monte Carlo evidence to point out the size
distortion problems of the commonly used unit root tests. He argued
that the distribution of the Dickey-Fuller tests is far different from the
distribution reported by Dickey and Fuller if the underlying distribution
contains a moving-average (MA) component. He also suggests that the
Phillips-Perron (PP) tests suffer from size distortions when the MA
parameter is large, which is the case with many economic time series
as noted by Schwert (1989). The test with the least size distortion is
the Said and Dickey (1984) high-order autoregression t-test. Schwert
argues that it is important to consider the correct specification of the
ARIMA process before testing for the presence of unit roots. Whereas
Schwert complained about the size distortion of unit root tests, DeJong
et al. complained about the low power of unit root tests. DeJong et al.
(1992a) argued that the unit root tests have low power against plausible
trend-stationary alternatives. DeJong et al. (1992b) consider Monte
Carlo experiments to study the size and power of the ADF test and
the PP tests and argue that the ADF test displays size distortions in
the presence of negatively correlated MA errors and the PP tests suffer
from serious size distortions with plausibly correlated MA or AR error
structures. They argue that the PP tests have very low power (generally
less than 0.10) against trend-stationary alternatives but the ADF test
has power approaching one-third and thus is likely to be more useful
in practice. They conclude that tests with higher power need to be

4-2 Size distortion and low power of unit root tests 
101
developed. Similar problems about size distortions and low power were
noticed by Agiakoglou and Newbold (1992).
There is one other aspect to these size distortions and low power
problems. The Beveridge and Nelson (1981) decomposition (discussed
in chapter 2) showed that a series with a unit root can be decomposed
into a pure random walk with drift and a stationary component. A
trend-stationary series can also have trivially such decomposition with
the variance of the random walk component zero. The effect of the
variances of the errors in the randon walk and stationary components
on the ADF and PP tests was studied by Liu and Praschnik (1993)
who show that the size distortions are sensitive to the ratio of these
variances but that, of the tests they considered, the ADF test was the
least sensitive.
Cochrane (1991) uses the Beveridge-Nelson decomposition to argue
that since the random walk component can have arbitrarily small vari-
ance, tests for unit roots or trend stationarity have arbitrarily low power
in small samples. Also, Cochrane shows that there are unit root pro-
cesses whose likelihood function and autocorrelation functions are ar-
bitrary close to those of any given stationary process and vice versa.
Cochrane, thus, argues that inference on unit roots or trend stationarity
can be fragile.
The poor power problems is not unique to the unit root tests. Cochrane
argues that any test of the hypothesis 8 = 0O has arbitrarily low power
against alternatives 6o — s in small samples, but in many cases the dif-
ference between 6$ and 8o — e would not be considered important from
the statistical or economic perspective. But the low power problem is
particularly disturbing in the unit root case because of the discontinuity
of the distribution theory near the unit root. Cochrane complained that:
the results of unit root tests do not necessarily answer one important question
namely: which distribution theory provides a better small sample approxima-
tion? (Cochrane, 1991, p. 283)
He points out that the borderline cases are not a technical curiosity but
occur in a lot of empirical examples.
Mention must be made of a recent paper by Gonzalo and Lee (1996),
who complain about the repetition of the phrase "lack of power unit
root tests." They show numerically that the lack of power and size
distortions of the Dickey-Fuller tests for unit roots are similar to and in
many situations even smaller than the lack of power and size distortions
of the standard student t-tests for stationary roots in an autoregressive

102 
Issues in unit root testing
model. But arguments like this miss the important point. There is no
discontinuity of inference in the latter case but there is in the case of unit
root tests. Thus, the consequences of lack of power are vastly different
in the two cases.
Blough (1992) follows the same arguments of Cochrane but also presents
Monte Carlo results. He formulates the unit root hypothesis not as a
point hypothesis but as an interval hypothesis. He starts from the model
y t = m t + S y l t + (1 - 8 ) y 2 u 
0 < 8 < l
where rat is a deterministic process, yu is a random walk, and y2t is a
purely nondeterministic stationary process, i.e.
Ayu = en 
and 
y2t = a(L)e2t
The errors (£it,e2t) are iid with mean zeros and covariances
rf cr12
and a(L) is a square summable lag polynomial of possibly infinite order
with a(0) = 1.
It is possible to construct a variety of processes by varying 8. Call these
yt(8). For any 8 > 0 the asymptotic properties of statistics generated
from yt(8) are dominated by the random walk component yu. However,
for finite samples, yt(8) will behave like y2t when 8 is small.
Blough defines the generic null hypothesis of a unit root as the hy-
pothesis that 8 > 0. Thus, for a generic null of unit root versus generic
stationary alterntive, we have
Ho : 0 < 8 < 1
# i : 8 = 0
For this test he shows that the power against any stationary alternative
can be no greater than the level of the test.
Blough argues from this that the explanation of unit root tests by
the standard power/level criteria should proceed with caution. Unit
root tests are often used to provide information about the distributions
of statistics for subsequent inference. The asymptotic distributions of
such statistics depend on whether the underlying processes have unit
roots. But their finite sample distributions must be nearly the same for
a given stationary process and for a unit root process nearby. Hence
the diagnostic value of unit root tests lies not in establishing a unit root

4-3 Solutions to the problems of size and power 
103
but in establishing what asymptotic theory provides good finite sample
approximation. As one example Blough provides limited Monte Carlo
evidence that suggests that probabilities of spurious regression match
up quite well with the rejection probabilities of low-order ADF tests. In
this example the interest is not in rejection of acceptance of a unit root
per se but in the spurious regression.
We shall discuss this issue of using unit root tests for subsequent
inference in a later section of this chapter.
4.3 Solutions to the problems of size and power
There have been several solutions to the problems of size distortion and
low power of the ADF and PP tests mentioned in the previous section.
Some of these are modifications of the ADF and PP tests and others are
new tests. We shall review them first and then outline some guidelines
for choice.
4'S.I LR tests in ARM A models
Since the criticism of the tests based on AR models has been that they
have size distortions and low power against meaningful stationary alter-
natives in the presence of large MA components, it would be desirable
to look at tests for unit roots in ARM A models. Yap and Reinsel (1995)
develop likelihood ratio (LR) tests for a unit root in an ARMA model
and present limited simulation evidence on their performance in the con-
text of an ARMA (1,1) model. Since Schwert (1987) argues that many
economic time series can be characterized by an ARMA (1,1) model,
this evidence is of practical interest.
Consider the ARMA model
3=1 
3=1
where et ~ iid(O, erf). Consider the case where yt is stationary and
(f>(L) = 0 has one unit root and all other roots are outside the unit
circle. We can write this equation in first differences as
p - l 
q
Ayt = cyt-i + 2_^ (/>jAyt-j + et —
3=1 
3=1
where c = —(1 — Y?j=i fa)-

104 
Issues in unit root testing
In the presence of a unit root, c = 0. Let 5 denote the residual sum
of squares (RSS) in the Gaussian estimation of the unconstrained model
and 5o denote the RSS under the restricted model c = 0. Under Ho :
c = 0, Yap and Reinsel show that the LR test statistics is asymptotically
distributed as
T(50 - S) ^ Jo W(r)dW(r)
which is the Dickey-Fuller distribution. This implies that the addition
of MA terms to a unit root nonstationary AR model does not alter
the asymptotic distribution of the LR test statistics for the hypothesis
Ho : c = 0 that one would obtain in a pure AR model.
Yap and Reinsel discuss other Wald type statistics for testing the
hypothesis Ho : c = 0 and show that their asymptotic distribution is the
same as that of the LR test statistic. This does not mean that the finite
sample distributions are the same. To investigate this they consider
a limited Monte Carlo study of an ARMA (1,1) model. Their results
suggest that overall the LR test performs better than the other tests (in
terms of size and power properties) in the ARMA (1,1) model when a
substantial MA component {6\ positive) is present.
4*3.2 IV tests in ARMA models
One other set of unit root tests that have been suggested in the context
of ARMA models is the instrumental variable (IV) tests starting with
the paper by Hall (1989). Hall suggests an IV estimation in an ARMA
(1, q) model and Pantula and Hall (1991) extend this to an ARMA (p, q)
model. Hall suggests using yt-k(k > q) as an instrumental variable. The
IV estimator is therefore defined as
]C VtVt-k
P= ^
Based on this Hall defines the coefficient test (if-test) and the t-test
corresponding to the similar test statistics in the Dickey-Fuller regres-
sion. He proves that the asymptotic distribution of these statistics are
the corresponding DF distributions.
In the case of the ^-statistic the factor ^yt-iyt-k/T2 
need not be
positive. In this case Hall's t-statistic is not defined. This can happen
if (i) the autoregressive coefficient is small, (ii) k is large, or (iii) T is

4-3 Solutions to the problems of size and power 
105
small. Li (1995) defines a modified t-statistic by noting that
1 
T 
1 
T
t=k 
t=k
H 
o / 
yt—liV't—1 "I" ut—2 ""h * * • ~h ut—k)
The second term converges in probability to zero. Hence Li defines a
modified t-statistics by substituting J2t=kyt-i f°r S*lfc2/t-fc2/t-i- Li
also investigates the powers of both the coefficient test and the modified
t-test (which was not done in Hall's paper) and finds that the power
functions of the IV tests are not monototic functions of the autoregres-
sive parameter when there is a negative moving-average component.
The tests developed in Hall (1989) and Pantula and Hall (1991) follow
the Dickey-Fuller famework. Lee and Schmidt (1994) use the Bhargava
approach to develop IV unit root tests (see section 3.7 of chapter 3 for
the difference in the two approaches).
For the model with trend, define the residuals
ut = (yt - yi) - jr—J;U/T - yi)
Then the Bhagava test, discussed in Schmidt and Phillips (1992) as an
LM test and denoted as the BSP test depends on the regression
Aut = (j)Ut-i 4- error, 
t = 2,..., T
The IV test developed in Lee and Schmidt uses Ut-k a s a n instrumental
variable where k > q is the order of the MA component as in Hall.
They derive the asymptotic distributions of the coefficient test and t-
test. They also suggest a modified coefficient, the modification being
similar to the one in Li that we discussed ealier viz substituting Yl^t-i
for y^ut-iUt-k- Thus the test statistic used is
Lee and Schmidt conduct detailed Monte Carlo studies to compare the
size and power of the BSP coefficient and £-tests in the autoregression
with no MA error, and the IV tests and corrected IV tests developed in
their paper. They conclude that the modified IV test has good power
and is the best choice. The simulation results were all for q = 1 and
k = 2, using larger values of k resulted in a loss of power.

106 
Issues in unit root testing
In practice one major problem with all these IV tests is the choice of
k. Clearly overestimating k results in a loss of power. The optimal k is
q + 1. The IV methods avoid the need to estimate the MA parameters
but one needs to have information on the orders of the processes. Hall
(1995) discusses methods of determining the orders of p and q in the
ARMA (p, q) model using the autocovariances of the residuals from the
IV regression. There is the problem that the properties of these autoco-
variances themselves depend on whether the true process is stationary
or has a unit root, and there is the dilemma that these are being used
as a basis of model selection prior to performing a unit root. Hall sug-
gests on the basis of simulation evidence that it is better to use the IV
residuals assuming the process is stationary. Detailed results from Hall
are omitted here. Hall and Hassett (1990) gives an earlier discussion of
this issue.
The importance of correctly specifying the order of p and q in the IV
tests has been emphasized in Pantula and Hall (1991) who show that
if the model is overspecified, then the empirical levels are higher than
the nominal levels in moderate-sized samples. On the other hand, if the
model is underspecified, the IV estimators are inconsistent.
4*3.3 Other IV based tests - Durbin-Hausman 
tests
Choi (1992a) suggests some tests based on what he calls pseudo-IV esti-
mators. The IV estimator is called a pseudo-IV estimator because it is
based on using the current value yt rather than the lagged values yt-k->
used in the previous discussion as instruments. Thus, in the model
yt = ayt-i + ut
the OLS estimator is Y^Vt-iVt/ YlVt-i- The pseudo-IV estimator that
Choi considers is
This is the estimator we would get if we considered the reverse regression
Vt-i = oc*yt + uf.
and used OLS. We shall see later that this is related to tests based on
symmetric estimators and reverse Dickey-Fuller tests.
Choi considers the difference (aIV — QLOLS) standardized by the usual
variance estimators of 6LOLS. He calls this the Durbin-Hausman test for

4-3 Solutions to the problems of size and power 
107
a unit root and derives the asymptotic distributions of similarly defined
statistics in the three DF-type regression models
yt =ayt-\ + ut
yt = /i + ayt-i + ut
yt = fi + (3t + ayt-i + ut
and labels them respectively as DHSi, DHS2, and DHS3.
The Durbin-Hausman test (or Hausman's specification error test) is
based on the difference between two estimators both of which are con-
sistent under the null and one of which is not consistent under the al-
ternative. It also requires the difference to diverge under the alternative
for test consistency. The variance of the difference of the two estimators
is equal to the differences in the variances if one of the two estimators is
efficient. This follows from the result in Rao (1973) that the covariance
between an efficient estimator and the difference between an efficient
and inefficient estimator is zero. Also, as is well known, the Hausman
specification test is not the same as a hypothesis test.
The test statistics that Choi considers are not standardized by using
the correct variance of (aIV — aOLs)- They are standardized by using
the arbitrary variance, var(aOLs)> Thus they cannot be called Durbin-
Hausman tests. This may explain the lack of widespread use of these
tests.
Choi argues that his DHS tests display better power properties in
finite samples than the Dickey-Fuller tests, especially DHS2 and DHS3.
Also the limiting distributions are easier to derive, and serial correlation
corrections are easier to make. These advantages might be outweighed
by the fact that the tests are not really Durbin-Hausman tests because
of the arbitrary normalization factors used.
4' 3.4 Modifications of the Phillips-Perron 
(PP) tests
We have discussed likelihood ratio based tests and IV tests in the pres-
ence of MA errors. We shall discuss one other alternative: modification
of the PP tests.
The PP tests were orginally designed to take care of MA errors as well.
However, as discussed earlier, they suffer from serious size distortions
when there are negative MA errors. The DF test does not have such
serious size distortions but it is less powerful than the PP tests. (Of
course, in all these cases one can consider the size adjusted power or

108 
Issues in unit root testing
the power when the test is performed using whatever critical value that
makes the size correct).
Perron and Ng (1996) suggest modifications of the PP tests to correct
this problem. They use methods suggested by Stock (1990) to derive
modifications of the Zp and Zt statistics (see chapter 3, section 3.6.3).
The modified Zp and Zt statistics are
T ^ 
2
2
Convergence of p at rate T ensures that Zp and MZP are asymptotically
equivalent. Defining
they note that Zt = MSB • Zp. Hence they define the modified Zt statis-
tics by
MZt = MSB • MZp
If we write the model as
yt = pyt-i +v*
then for the computation of the PP test statistics we need estimates of
two error variances o\ = var(ut) and a2 = liniT_+00 T~1E(Sjr) where
ST = YlJ=i ur For an estimate of cr^ they use
For the estimate of a2 they suggest using an autoregressive estimator
defined as
where
t=k+l 
j=l
and bj and {e^} are obtained from the autoregression
^y*-J 
+ 
etk
Perron and Ng argue that this autoregressive spectral density estima-
tor works best and that the modifications of the PP tests suggested still

4-3 Solutions to the problems of size and power 
109
have problems with other estimators of a2. They use local asymptotic
analysis to explain why the other estimators yield no improvement even
with the modified statistics.1
In addition to the MZ^ and MZ$ statistics, Perron and Ng also inves-
tigate the size and power properties of the MSB statistic defined earlier,
but with the above estimate of a2. Critical values for the demeaned and
detrended case for this statistic were taken from Stock (1990). Perron
and Ng show that the modified PP tests are able to maintain good power
while correcting the size distortion problems in the presence of negative
MA errors common in most macroeconomic time series.
The importance of these modifications of the PP tests from the prac-
tical point of view is that they do not have to specify the order of the
MA component as the IV tests discussed earlier. Also, MA can arise in
time series models with additive outliers (outlier problems in time series
are discussed in chapter 14), and the advantage of the modified PP tests
is that one does not need to identify the outliers before applying the
unit root tests.
4*3.5 Forward and reverse Dickey-Fuller 
regressions
Leybourne (1995) suggests tests based on forward and reverse Dickey-
Puller regressions. Since these are related to the tests by Choi (1992a)
discussed earlier, we shall go through these in detail. The results in Ley-
bourne are more transparent than in Choi and also, as argued earlier, the
modification of the tests in Choi, being based on the Durbin-Hausman
tests, is rather questionable.
Consider the following DF regression
yt = a + pyt-i +ut, 
t = 2,..., T
Let p be the estimate of p and RSS the residual sum of squares from a
least squares estimation of this equation. Then
G 
= T-3
and the DF ^-statistic which we denote by DFf can be written as (using
the expression from the estimation of a linear regression)
DFf 
= 
(p-l)[var(j>)]-1/2
1 Local asymptotic power involves investigating power for alternatives pT = 1 + c/T.
For details and references to work in this area, see Stock (1994b, pp. 2270-2271).

110 
Issues in unit root testing
v(yuyt-i) 
-var(yt-i)
= 
*
Now consider the reverse DF regression. To avoid confusion, define
zt = 2/T+i-t (* = 0,1,2, ...,T + 1), i.e., z\ = J/T, ^2 = VT-I and so on,
and the reverse regression
zt = a
The DF ^-statistic from the reverse regression is defined DFr by the
same expression as DFf with zt replacing yt and a* replacing a.
Noting that var(zt) = var(yt-i), var(zt-i) = var(yt), and cov(zt,zt-i)
cov(yt, yt-i)) it can be seen that p* from the reverse DF regression is the
same as the pseudo-IV estimator that Choi considers. However, its mo-
tivation from the reverse DF regression is more transparent than from
the IV perspective.
Leybourne shows after some algebra that the difference between the
forward and reverse DFf statistic is given by
DFr = DFf - A
where
A = &~1[var(yt-1)]~1/2[var(yt) - var(yt-i)] = cr~xQ
t=2
We might note that the difference Choi considers is the difference be-
tween the coefficient statistics, not the t-statistics. Thus A is an end-
effect measure that depends on yx — yi> If yt is /(I), as under the null
of a unit root, this difference will diverge. Note that a requirement for
the Durbin-Hausman test is that the difference diverge under the alter-
native (see Choi, 1992a, p291). Here the difference diverges under the
null.
Under the unit root null
Using this and simplifying the expression in Q Leybourne shows that
A =* D'1'2 \w(l)2 - 2W(1) f W(r)dr]

4-3 Solutions to the problems of size and power 
111
where
D= [ W(r)2dr-([ 
W{r)drf
Jo 
Jo
Since
1/2 \W{lf 
1
*D 
[—2 
2
we get
rvFfi)2 
i 
f1
DFr ^ D~l/2 
—— 
h W(l) / W(r)di
L 2 
2 
Jo
Under the stationary alternative, DFf and DFr have the same distribu-
tion which is the distribution for DFf.
The above expressions are for the case of a model with drift. In a
model with time trend the expressions carry through but they involve
demeaned and detrended Brownian motions. Again DFr = DFf — X.
In the case where the error follows an AR(p) process, we have to con-
sider the ADFf and ADFr statistics. These can be shown to have the
same distributions (under the null of a unit root) as the DFf and DFr
distributions respectively.
Having derived the asymptotic distribution of DFf — DFr, it is not
clear why Leybourne did not consider a test based on this. Instead he
considers a test statistic DFmax which is defined as
DFmax = Max(DFf, DFr)
The asymptotic distribution of this statistic is very difficult to derive.
Leybourne obtains critical values of this test statistic by simulation using
a model with normally distributed errors. The critical values do not vary
much with sample size (see table 4.1).
Leybourne investigates the power of the DFmax test and finds that
there is a 15 percent increase in power relative to DFf test. He also
investigates the robustness of the results to obtain other error distribu-
tions and also gives an empirical application based on the Nelson-Plosser
data arguing that the DF^ax test shows less evidence of a unit root than
the DFf test.
4 »3.6 Weighted symmetric 
estimators
Consider the process
yt = a + pyt-i +1H, t = 2,..., T

112 
Issues in unit root testing
Table 4.1. Critical values of DFmax statistics
Nominal size
1.00
0.05
0.01
_2
-2
- 3
Model with
25
.15
.50
.25
50
-2.14
-2.48
-3.17
Sample
drift
200
-2.13
-2.44
-3.06
size
Model with drift
and trend
25
-2.89
-3.26
-3.99
50
-2.87
-3.22
-3.84
200
-2.53
-3.12
-3.72
Source: Leybourne (1995, p. 565).
The idea behind the symmetric estimators is that if a normal stationary
process satisfies this equation, then it is also satisfies the equation
yt = a + pyt+i + ut
This symmetry leads one to consider an estimator of p that minimizes
Q(p) = yZwt(Vt ~ PVt-i)2 + YVl - wt)(yt ~ pyt+i)2 
(4.1)
where yt = yt —T"1 ]Cj=i Vj- For wt — 1 we get the OLS estimator. For
wt = 0.5 we get the simple symmetric estimator considered in Dickey,
Hasza, and Fuller (1984). For Wt = (t — 1)/T we get the weighted sym-
metric (WS) estimator suggested in Park and Fuller (1995). A detailed
discussion of WS estimators in nonstationary models can be found in
chapter 10 of Fuller (1996).
For the model with drift (but no trend), with yt defined as deviations
from the mean, the WS estimator of p is
HWS —
and the corresponding t-statistic is
where a2
ws = Q(pws)/(T 
- 2) and <?(•) is denned in (4.1). The WS
estimator can exceed 1 in some models, particularly those for which the
ML estimator is < 1 but very close to 1. The symmetric estimator treats
the beginning and last observation the same way. Note that under the
null of a unit root yr — yi diverges.

4-3 Solutions to the problems of size and power 
113
The limiting distributions of the WS estimators are given by
T(pws -1)=> D~lN
LWS 
=^r U 
iV
where
- f1 
2 if1 
r
Jo 
[Jo 
\
the expression we get in the DF distributions and
N = 
W^ 
1 - W(l) [ W(r)2dr
2 
J[ W(r
Jo
-I
1 
r /•!
2
W(r)2dr + 2 / W(r)dr
The expressions for the case of a model with linear trend are more com-
plicated. They are given in Fuller (1996, p. 571) and will not be repro-
duced here.
Pantula et al. (1994) present Monte Carlo evidence in the case of the
model with drift (but no trend) to show that the WS estimators are
considerably more powerful than the DF statistics and that they have
the best power properties among the estimators they investigated.
4.3.7 DF-GLS test
Elliott, Rothenberg, and Stock (1996), which we shall denote by ERS,
first derive the asymptotic power envelope for point optimal tests of a
unit root in the autoregressive representation of a Gaussian time series
under various specification of the trend. King (1987) defines a point
optimal test as a test that optimizes power at a predetermined point
under the alternative. These tests are second best when uniformly most
powerful (UMP) tests do not exist. We shall show later why UMP tests
do not exist for the unit root hypothesis.
After developing the asymptotic power envelope, ERS propose a fam-
ily of tests whose power functions are tangent to the power envelope at
one point and are never too far below the envelope. They call this test
PT(0.5). They then suggest the Dickey-Fuller GLS (DF-GLS) test as
one that has the limiting power function close to that of the Pj-(0.5)
test.

114 
Issues in unit root testing
Table 4.2. Critical values for the Elliott-Rothenberg-Stock DF-GLS
test
Sample size
50
100
200
oo
Critical values
1%
-3.77
-3.58
-3.46
-3.48
5%
-3.19
-3.03
-2.93
-2.89
10%
-2.89
-2.74
-2.64
-2.57
Note: The model is with linear trend
and c = -13.5.
Source: Elliott, Rothenberg, and
Stock (1996, table 1, p. 825).
Let yt be the process we consider. The DF-GLS t-test is performed
by testing the hypothesis ao = 0 in the regression
Ayf = aO2/jLi + ^Ayf^ 
+ • • • + apAyf_p + error
where yf is the locally detrended series yt. The local detrending depends
on whether we consider a model with drift only or a linear trend. The
latter is one most commonly used. In this case we have
yf =yt-Po- 
Pit
where (/?o?/5i) a r e obtained by regressing y on z where
y = [yu (1 - aL)y2,..., (1 - aL)yT]
z = [zi, (1 - aL)z2,..., (1 - 6LL)ZT)
and
The a that produces the asymptotic power depends on the significance
level used e. This is very inconvenient but ERS argue that fixing c = — 7
in the model with drift and c — —13.5 in the linear trend case, the
limiting power function of the PT tests and the DF t-test applied to the
locally trended data are within 0.01 of the power envelope for 0.01 <
e < 0.10. They provide critical values for this DF-GLS test. They are
given in table 4.2.
Hwang and Schmidt (1996) suggest another type of DF-GLS test.

4-3 Solutions to the problems of size and power 
115
They start with the Bhargava-type specification (see section 2.9 of chap-
ter 2)
Vt = 7o + 7i* + ut
ut — aut-i + et
which gives
Ayt = A) + Pit + (a - l)2/t-i + et
Consider two residuals from this equation et obtained by estimating this
equation with a = 1 and e\ obtained by estimating this equation using
a value a = a*.
The Bhagava test which is derived in Schmidt and Phillips as an LM
test (see the section 3.3 of chapter 3) is based on the regression
Aet = <t>et-i + error
and testing <\> = 0. They suggest choosing a value of a so as to optimize
the power at a predetermined point for the alternative (as in point op-
timal tests). They suggest a value a* = 0.85 for annual data. Note that
this implies a half life of four years. (A half life is defined as n where
(a*)n = 0.50 - it is the time taken to achieve a 50 percent adjustment
to shocks.) Hence with quarterly data if we take a* = 0.95, this implies
a half life of arround 14 quarters or 3.5 years.
Hwang and Schmidt present (using simulations) the critical values for
this test for a* in the range (0, 1) and sample sizes 50 - 500. They also
present the powers of this test for different true values of a. The tables
are too numerous to be discussed here. Some important numbers for
practical use with the DF type t-test are given in table 4.3. The critical
values are for a* = 0.85 (for use with annual data) and a* = 0.95 (for
use with quarterly data).
Hwang and Schmidt argue that their GLS test is more powerful than
the DF test or the Bhargava-Schmidt-Phillips LM test. There is no
comparison with the ERS test. The limitation of the Hwang and Schmidt
procedure is that, it does not have an asymptotic justification. It is only
relevant for the exact same model used to generate the small sample
critical values. The procedure is not of general applicability.

116 
Issues in unit root testing
Table 4.3. Critical values for the Hwang-Schmidt DF-GLS test (t-test)
Sample size
50
100
200
500
1%
-3.85
-3.91
-3.93
-3.93
a* = 0.85
5%
-3.26
-3.32
-3.67
-3.39
Critical
10%
-2.95
-3.03
-3.09
-3.11
values
1%
-3.57
-3.62
-3.73
-3.82
a* = 0.95
5%
-2.97
-3.07
-3.20
-3.32
10%
-2.68
-2.81
-2.93
-2.04
Source: Hwang and Schmidt (1996).
4.4 Problem of overdifferencing: MA roots
Let us consider the simple moving-average model
yt = dt + uu 
Aut = (1 - 6L)et
where et is 1(0) and dt is a deterministic component. The case |0| = 1
is known as the noninvertible MA process. If 9 = 1 we have a unit MA
root and Ut = et + u$ — eo- In this case with UQ = eo, Ut is an 1(0)
process. If \9\ < 1, then (1 — 9L) is invertible and Ut is 1(1). Thus a test
of the hypothesis 9 = 1 versus |0| < 1 is a test of the null hypothesis
that yt is 1(0) versus yt is 1(1).
MA unit roots can occur if a stationary series is differenced, i.e., there
is overdifferencing. For this reason the occurrence of a maximum of the
likelihood function near or at 9 = 1 can be interpreted as evidence of
overdifferencing (see, for instance, Plosser and Schwert, 1977). However,
Sargan and Bhargava (1983) point out that the occurrence of a maxi-
mum of the likelihood fucntion at 9 = 1 is insubstantial evidence for
overdifferencing because the likelihood function can have a local maxi-
mum at 9 = 1 with relatively high probability, even when the true 9 is
less than 1. Kang (1975) showed that the likelihood function is station-
ary at 9 = 1 and simulation studies on the behavior of the ML estimator
of 9 reveal that the global maximum can occur at 9 = 1 even if the true
9 is in the invertible region. Anderson and Takemura (1986) also proved
that the probability that a noninvertible process is estimated when the
true process is invertible and is O(T~n) where n is a positive integer.
This problem is known as the pile-up problem in the literature on MA
models.
Sargan and Bhargava prove that the ML estimate of a noninvertible

4-4 Problem of overdifferencing: MA roots 
117
MA(q) model is T-consistent, showing the same speed of consistency as
the least squares estimator of a unit root AR process. They also show
that the limiting probability that the MLE is exactly 1 when 9 = 1 is
0.6575. One other point made by them is that if 9 = 1, the MLE 9 is
not normally distributed and hence the LR test cannot be applied to
test the hypothesis 9 = 1. On the other hand, the MLE of an invertible
MA process is normally distributed.
Shephard (1993) establishes an approximate sampling theory for the
MLE of an MA process which will be accurate even in a strictly nonin-
vertible case. He does this by considering a local level model which is an
unobserved components model represented as follows
yt = fit + £t> 
£t ~ Hd(0, o2)
fit = fit-i -h r7t, 
rjt ~ iid(0, qa2)
£t and r)s are independent for all t and s. Interest centers in the parameter
q which is also called the signal-noise ratio. This is an important model
studied (among several others) by Leybourne and McCabe (1989) and
Harvey (1989). For this model, even when the random component of
yt is dominent (i.e. q ^ 0), there is a nonzero probability that the
estimated value of q using ML is zero. Shephard and Harvey (1990)
suggest assuming a diffuse prior for fio and integrating it out.
It should be noted that the unobserved components (UC) model is
very much related to the MA root model. They are alternative param-
eterizations of the same model. See Stock (1994b, p. 2789) who says:
In general for suitable choices of initial conditions, all MA models have UC
representation and all UC models have MA representation.
This correspondence is useful because the work on UC models can be
used to study tests for MA unit roots.
Choi and Yu (1997) consider a method of testing the unit root MA
hypothesis by using the framework for unit root tests based on AR
models. This is done by aggregating the series yt as follows: define
St = ^2i—i V% with y\ = 0, then St = ctSt-i + yt with a = 1. Hence
a test of the hypothesis 9 = 1 (which as discussed earlier implies that
yt is 1(0)) is equivalent to the joint hypothesis a = 1 and yt is 1(0).
Thus the unit root testing framework for AR models can be used on this
aggregated equation. Choi and Yu argue that this is an equivalent way
of testing for a unit MA root but the advantage is that the likelihood
function for the AR(1) model with fixed initial conditions is easier to
handle than the likelihood function for the unconditional MA(1) model.

118 
Issues in unit root testing
Noting this analogy, any of the unit root tests discussed earlier can be
used to test a = 1 on St. However, Choi and Yu, develop what they call
SBDH tests (Sargan-Bhargava-Durbin-Hausman tests - although as we
noted earlier the Durbin-Hausman terminology has some objections).
For the unit root hypothesis, the LM test statistic is (see chapter 3)
[ 
Ta2 
J
where a2 = 5Zt=2 3/?/^"- The SBDH statistic they suggest is
S B D H = % ^
To allow for more general error structures, they modify these test
statistics along the lines of Phillips and Perron (1988) so that the asymp-
totic distributions of these tests are free of nuisance parameters. The
modified test statistics are
and
SBDH0 =
&2T2
where
a* 
* 
1
c(n)k(n/r), 
c{n) =
n=—r
Z-2 t
and k(n/r) is a lag window. <3^/2TT is the spectral density estimate of yt
at the zero frequency. These tests can, however, be used for zero mean
processes only.
Choi and Yu show that under the null the asymptotic distribution of
LM° and SBDH0 are given by
I2
W(r)dW(r)dr\
/o 
J
and
W(r)2dr
r1
/
Jo

4-4 Problem of overdifferencing: MA roots 
119
They tabulate the empirical size and power of these tests by simulation.
They also extend these tests to models with time trends and to tests for
the hypotheses
1(1) versus I(jfc), 
k = 2
and
1(2) versus I(fc), 
A; = 3
The asymptotic distributions of the test statistics for these latter tests
are more complicated. Using these tests one can test the null hypotheses
1(0), 1(1), and 1(2) sequentially.
Note, however, that there have been some objections raised in the
literature on this procedure of testing up and it has been argued that
testing down is a better approach (see Dickey and Pantula, 1987). This
is similar to the issue of specific to general versus general to specific
approach to dynamic econometric modeling, the latter advocated in the
so-called LSE methodology.
Breitung (1994) suggests three simple tests of the MA unit root hy-
pothesis. The first test makes use of the fact that in models with MA
unit roots, the spectral density is close to zero at frequency zero. Thus,
the MA unit root hypothesis can be tested by testing the significance of
the spectral density at frequency zero. Breitung finds that this test is
less powerful than the other tests for the MA unit root.
The second test is based on a comparison of the variance of the inte-
grated series under the null and the alternative. Let zt = Ylj=i Vj ^e
the partial sum process for yt. If yt has an MA unit root, the variances
of yt and zt are of the same order of magnitude, whereas if yt does not
have a MA unit root then the variance of the partial sum diverges at a
rate of T. Thus, the variance difference (VD) statistic is based on the
difference between the variance of the original and the integrated series.
To integrate the process Breitung considers a slightly different concept
than the partial sum method. For details, see Breitung (1994).
The third statistic he considers is the Tanaka (1990) statistic. This
statistic essentially depends on a variance comparison of the double in-
tegrated series with a single integrated series.
Breitung finds that the powers of the three tests: spectral density
test, VD test, and Tanaka's test, have powers converging at the rate
of 7ll/4
jj?3/4
) and T respectively. Results from his Monte Carlo ex-
periments suggest that the Tanaka test performs best if the alternative
is close to the null whereas the VD statistic performs better for more

120 
Issues in unit root testing
substantial violations of the null. Breitung (1994, p. 361), however,
suggests some useful modifications of the Tanaka test statistic.
We shall see in the next section how the Tanaka test statistic is re-
lated to other tests using stationary as null. Since, as argued at the
beginning of this section, under the hypothesis of an MA unit root the
series is stationary whereas it is nonstationary if the MA root 6 is less
than 1 in absolute value, tests for the MA unit root as null and tests
for stationarity as null are related. We shall therefore look at tests of
stationarity as null.
4.5 Tests with stationarity as null
There have been several tests for stationarity as null, although these are
not as numerous as tests using unit AR root as null. Some of these are:
Tanaka (1990), Park (1990), Kwiatkowski, Phillips, Schmidt, and Shin
(1992), Saikkonen and Luukkonen(1993), Choi (1994), Leybourne and
McCabe (1994), and Arellano and Pantula (1995). Most of these papers
also present an analogy between tests for an MA unit root and tests
for stationary as null, although the models they start with are slightly
different from the ones we discussed in the preceding section. We shall
now discuss these tests, though not in their chronological order.
4.5.1 KPSS test
Kwiatkowski, Phillips, Schmidt, and Shin (1992), which is often referred
to as KPSS, start with the model
where et is a stationary process and (t is a random walk given by
Ct = Ct-i + uu 
ut ~ iid(0, o-2
u)
The null hypothesis of stationarity is formulated as
Ho : a2
u = 0 or £t is a constant
This is a special case of a test for parameter constancy against the
alternative that the parameters follow a random walk. This problem
was discussed by Nabeya and Tanaka (1988) for a regression model
Vt = Ptxt + izt + et

4-5 Tests with stationarity as null 
121
Thus the KPSS model is a special case with x± = 1 and zt = t.
The Nabeya-Tanaka test statistic for this hypothesis is given by
where et are the residuals from the regression of yt on a constant and
a time trend, <jg is the residual variance from this regression (residual
sum of squares divided by T) and St is the partial sum of et defined by
T
For testing the null of level stationarity instead of trend stationarity
the test is constructed the same way except that et is obtained as the
residual from a regression of yt on an intercept only. The test is an
upper tail test.
The asymptotic distribution of the LM test statistic has been derived
in Nabeya and Tanaka. However, this is valid only if the errors are iid.
KPSS consider the case of a general error process and hence modify
the test statistic as in Phillips (1987) and Phillips and Perron (1988).
They then derive the asymptotic distribution of the modified statistic
and tabulate the critical values by simulation.
When the errors are iid the denominator of the LM statistic a2 con-
verges to a2. However, when the errors are not iid the appropriate de-
nominator of the test statistic is an estimate of a2 not a2, where a2 is
the long-run variance defined by
a2 = lim T^EiSl)
A consistent estimator of a2 is s^i which is given by
E
t = l 
T=l 
t=T+l
Ew" E
Here wT\ is an optimal weighting function that corresponds to the choice
of a spectral window. KPSS use the Bartlett window, as suggested by
Newey and West (1987)
For consistency of 5^, it is necessary that / —> oo as T —• oo. The rate
/ = o{Tx^2) is usually satisfactory under both the null and the alternative
(see section 3.6.3 for the Newey-West estimator).

122 
Issues in unit root testing
Table 4.4. Critical values for the KPSS test
Tests
Test A:
Test B:
Intercept only
Linear trend
Critical values
0.10
0.347
0.119
0.05
0.463
0.146
0.01
0.739
0.216
Source: Kwiatkowski, Phillips, Schmidt,
and Shin (1992, table 1, p. 166).
Note, however, that the PP tests have been found to have low power
(see section 4.2) and that the problem could be corrected by changing
this spectral window (see section 4.3 for a discussion). Anyway, we shall
proceed with the way KPSS formulated the test. The critical values for
the KPSS test are given in table 4.4.
We shall comment later on the size distortions and power properties
of the KPSS test.
4-5.2 Leybourne and McCabe test
Leybourne and McCabe (1994) suggest a modification of the KPSS test
which can be viewed as an analogue of the ADF test whereas the KPSS
test is an analogue of the Phillips-Perron test. They argue that inference
from the KPSS test can be very sensitive to the value of the lag I that
is used in the computation of s^, and that the alternative they suggest
is more robust to lag specification (the lag being the order of the AR
as in the ADF test) and also more powerful than the KPSS test. They
show that their test is consistent to order T whereas the KPSS test is
consistent to order T/l or T2/3 since / is of order T1/3. Also, in the
test they suggest the null and alternative hypotheses are based on more
realistic models than are encountered in practice.
The test suggested by Leybourne and McCabe is based on a gener-
alization of the local level (or unobserved components) model (Harvey,
1989, chapter 2). The null hypothesis is that the process is a station-
ary ARIMA(p, 0,0) process and the alternative hypothesis is that it is
an ARIMA(p, 1,1) process with a positive MA(1) coefficient. The test
involves choosing p but they argue that inference using a value of p > po
(the true value) does not have much of an effect on the inference.

4-5 Tests with stationarity as null 
123
The model is
$(L)yt = at + f3t + et
at = at-i+r]t, 
a0 = a, 
£ = 1,2,...,T
where et ~ iid(0,a2), rjt ~ izd(O, cr^), St and rjt are independent, and
$(L) = 1 — (j)\L — (j>2L2 
4>PLP is a pth order AR polynomial with
roots outside the unit circle. They call this the structural model. This
model can be shown to be second-order equivalent to the ARIMA (p, 1,1)
process, which will be referred to as the reduced-form model
*(L)(1 - L)yt = (3 + (1 -0L)uu 
0 < 6 < 1
where ut ~ iid(0^al) and o\ = a^/8. Define A = cr^/of. Then 9 is
related to o~2 by the relation
6 
2
Thus if cr^ = 0, then 0 = 1, and the model collapses to a stationary
AR(p) process. If <J^ is very close to zero, then 6 is very close to 1, and
yt is almost stationary.
The test for stationarity in this model is
Ho : tf = 0 versus Hi : a* > 0
Under #1 the model is an ARIMA model with an MA characteristic
that closely resembles many economic time series, as argued by Schwert
(1987). This is a more realistic alternative than a pure random walk
with iid errors. In fact, the condition 6 > 0 excludes such processes.
Another feature of the model is that p, the order of the AR component,
is > 0. This ensures that the short-run and long-run effects of rjt are
different which is a realistic assumption.
Leybourne and McCabe deal with the structural model because the
inferential procedures fall in the category of a linear model with an
intercept that follows a random walk. The earlier work by Leybourne
and McCabe (1989) suggests a test statistic of the form e'Ve where e is
a T x 1 vector with the £th element et and V is a T x T matrix with
the (i, j)th element equal to the minimum of i nd j . The procedure they
suggest for constructing it consists of the following steps:
(i) Estimate the ARIMA(p, 1,1) model by ML
p
Ayt = (3 + ^2 <t>i&Vt-i + ut- Out-i

124 
Issues in unit root testing
to get fa.
(ii) Construct y% as
(iii) Regress y$ on an intercept and a time trend to get the residuals
it-
Then the test statistic is
sp = a~2T~2sfVi
where a2 = e'e/T is a consistent estimator of a2.
In the case of (3 = 0, it is a residual from a regression of yfc on intercept
alone. The corresponding test statistic is denoted as sa. Leybourne and
McCabe show that the asymptotic distribution of sa and sp are the
same as the corresponding statistics derived by KPSS and thus one can
use the critical values from the KPSS paper (tabulated earlier). The
important difference between the two tests is that the test by Leybourne
and McCabe accounts for autocorrelation in a parametric fashion by
including the lagged term in yt in the initial model specification. Thus,
the test is an analogue of the ADF test. The KPSS test, on the other
hand, starts with the basic model and then modifies the test statistic
nonparametrically by changing the estimate of a2 to s|^, the same way
that the PP tests adjust the DF tests.
Leybourne and McCabe apply their tests to make inferences on unit
roots using the 11 US macroeconomic time series studied by Schwert
(1987). In three cases (PI, PPI, and Wage), the KPSS test did not
reject stationarity, whereas the test by Leybourne and McCabe showed
unambiguously that these series contained unit roots.
4 '5.3 Some other tests
Park (1990) suggests tests based on the idea that if a variable follows a
unit root process, then OLS standard errors are usually inappropriate
and tend to indicate that unrelated variables have a statistically signifi-
cant relationship. His test statistics add one or more spurious variables
to the regression of yt on a constant and trend, and test the significance
of these spurious variables. His J test is defined by
T _ RSSj - RSS2
~ 
~2
STl

4.5 Tests with stationarity as null 
125
where RSSi is the residual sum of squares from a regression of yt on a
constant and trend, RSS2 is the residual sum of squares with the super-
fluous regressors added, and s^t is the Newey-West estimator, defined
earlier in the KPSS test. Under the null hypothesis, this statistic has a
X2 distribution with d.f. equal to the number of superfluous regressors.
The choice of the superfluous regressors is arbitrary. Some of the can-
didates are polynomial trends and pseudo-random walks. The power of
the test appears to vary greatly but using two or more variables seem
to give better discriminatory power than using just one.
Amano and van Norden (1992) compare the size and power of the
KPSS test and Park's test and find that, as with conventional unit root
tests, these tests also suffer from size distortion and loss of power for
certain data generation processes. Both tests also appear sensitive to
the specification of the truncation parameter.
Bierens and Guo (1993) suggest some tests which are based on the
Cauchy distribution. They argue that these tests have better asymptotic
properties than Park's test. These test statistics, however, have not been
used in practice.
Choi (1994) provides a test for stationarity within the framework of
testing for an MA unit root. We have discussed the tests proposed by
Choi and Yu (1997) in the previous section and so will not go through
this test in detail.
Arellano and Pantula (1995) also suggest a test based on testing for
an MA unit root. They note the pile-up problem that we discussed in
the previous section and also the fact that the distribution of the MLE
of the moving-average parameter 6 when 0 = — 1 is not known. They
also note that when analyzing simulated data with noninvertible MA
processes that the SAS program gave ML estimates that were unstable
and the standard errors were unrealistic. Arellano and Pantula, instead
consider the one-step Gauss-Newton estimator of 9 starting with an
initial value 0 — — 1, and derive the asymptotic distribution of the test
statistics suggested. Since there is no comparison with the tests we have
described in the econometric literature and also the illustrative example
is not of interest to us, we shall not go into details of these tests here.
4'5.4 Some general comments
There is no comparison of all these tests. However, it appears that
the most fruitful approach is to start with the unobserved components

126 
Issues in unit root testing
model as in Leybourne and McCabe. These tests for stationarity have
been suggested as useful for two purposes:
(i) for confimatory analysis - this we discuss in the next section and
(ii) as tests where it makes sense to test stationarity as null (e.g., in
discussion of convergence in the literature on growth).
As for the problem of deciding between 1(0) and 1(1) the most fruitful
approach is to analyze it in the Bayesian framework. This we shall
discuss in chapter 8.
4.6 Confirmatory analysis
It has been suggested (see, e.g., KPSS, p. 176 and Choi, 1994, p. 721)
that the tests using stationarity as null can be used for confirmatory
analysis, i.e., to confirm our conclusions about unit roots. However, if
both tests fail to reject the respective nulls or both reject the respective
nulls, we do not have a confirmation. The situation is similar to the
tests of nonnested hypotheses.
There is also the problem of which tests to use for each of the hypothe-
ses: unit root null and stationarity as null. As we noted in the previous
section, the KPSS and Leybourne-McCabe tests gave conflicting results
on the test for stationarity as null for three of 11 series.
In spite of these limitations, it is generally agreed that using both tests
together is better than using either test alone. Amano and van Norden
(1992) perform a Monte Carlo study and conclude that the joint testing
approach gives the most reliable results when the joint test indicates
that the data are stationary or that the data have a unit root for small
samples and large k (the truncation lag). They used the KPSS-PP com-
bination but the conclusions were basically unaltered by the KPSS-ADF
combination. Note that the PP test they considered was the original PP
test, and not the one with modifications noted in section 4.4 earlier.
Henricsson and Lundback (1995) apply this confirmatory analysis to
a study of purchasing power parity (PPP) theory and conclude that
the absence of PPP cannot be rejected (using the KPSS test) and the
presence of PPP can be rejected (using the DF test). This, they argue,
is confirmation that PPP does not hold and that the rejection of PPP
using the DF test is not a consequence of the low power of unit root
tests. We shall discuss this issue of testing PPP later when we discuss
panel data unit root tests.
KPSS (p. 175) also present results from confirmatory analysis applied

4-6 Confirmatory analysis 
127
to 14 time series (the Nelson-Plosser data analyzed by several investi-
gators). A rough summary of these findings is that for 12 of the 14
series (except unemployment rate and industrial production) we cannot
reject the null hypothesis of a unit root. With the KPSS test, we can-
not reject the null of trend stationarity at the usual critical levels for
six series: real per capita GNP, employment, unemployment rate, GNP
deflator, wages, and money. We might note that some of these results
are strange because many other studies have actually argued (as we will
note in a subsequent chapter) that money and prices are 1(2). Also, Ley-
bourne and McCabe (although they investigate a different series, that
analyzed by Schwert) came up with the conclusion that monetary base,
CPI, and wages show clear evidence of unit roots (the null of stationary
is rejected).
In any case KPSS conclude on the basis of the KPSS test and the
Dickey-Fuller tests that there is confirmation in the case of four se-
ries: unemployment which is stationary and consumer prices, real wages,
velocity, and stock prices which all have unit roots. Three more se-
ries: real GNP, nominal GNP, and the interest rate, probably have unit
roots, though the evidence against the trend-stationary hypothesis is
only marginally significant. (The result on interest rates is puzzling.
As John Cochrane remarked: "Interest rates now are the same as in
Babylonian days. How can there be a unit root in interest rates?")
Finally, for six series - real per capita GNP, employment, unemploy-
ment rate, GNP deflator, wages, and money - it is not possible to reject
either the unit root hypothesis or the trend-stationary hypothesis. KPSS
conclude that the data are not sufficiently informative to distinguish be-
tween these hypotheses. In the case of one series, industrial production,
there is evidence against both the hypotheses and so KPSS argue that it
is not clear what to conclude. Many of the conclusions from the confir-
matory analysis of KPSS are contraditory to commonsense. What this
suggests is that it is important to use better tests of both the hypotheses
in confirmatory analysis.
Burke (1994) does a detailed Monte Carlo study to determine the
usefulness of CDA (confirmatory data analysis). He investigates several
important issues through simulation studies, choosing model structures
similar to macroeconomic time series in the US. For the unit root test
he selects the ADF test with lag selection based on the AIC criterion.
For the stationary tests he uses the KPSS test and the SBDH tests in
Choi and Yu that we discussed in section 4.4. (He investigated Park's

128 
Issues in unit root testing
test but finds it less useful for CD A.) The major issues investigated in
the study are:
(i) How often does confirmation occur?
(ii) How many of these confirmations are correct?
(iii) How do these results change if the size of the tests is increased
from 5 percent to 10 percent.
Regarding issue (iii) he arrives at the conclusions that using the 10
percent significant level gives better results than using the 5 percent
significance level. It is not possible to summarize the numerous results
in Burke's paper. The important conclusions are:
(i) Joint rejections are relatively infrequent, but joint nonrejections
are far more common.
(ii) Even if confirmation occurs this may not be correct.
For instance, for a stationary model, the stationarity test gave correct
inference on 70 percent of occasions, the unit root test on only 43 percent
of occasions. Using CD A there was confirmation on only 50 percent of
cases and there was a 61 percent chance of the confirmation to be correct.
Under nonstationarity, the situation was better. In the corresponding
case to that described earlier, the stationarity test gave correct inference
in 71 percent of occasions, the unit root test on 81 percent. CDA allowed
inference in 73 percent of cases, with a 91 percent chance that this infer-
ence is correct. In an experiment with trend- and difference-stationary
models considered by Rudebusch (1992), Burke found that, when the
true model is trend stationary, the proportion of confirmations is 50-60
percent and about half of these are correct. When the true model is
difference-stationary, the proportion of confirmations is 60-65 percent
of which about 82 percent are correct.
The overall conclusion is that if the true model is stationary, the
proportion of correct confimations is low. It is thus, more important
to consider better unit root tests and stationary tests (as discussed in
section 4.3-4.5) than to use confirmatory analysis with defective tests.
The arbitrary choice of significance levels for the respective tests is also
a point of concern. This problem can be mitigated with the Bayesian
approach discussed in chapter 9.

4-7 Frequency of observations and power of unit root tests 
129
4.7 Frequency of observations and power of unit root tests
Are unit root tests based on quarterly data more powerful than those
based on the corresponding yearly data? Shiller and Perron (1985) and
Perron (1989) find using Monte Carlo experiments that over a substan-
tial range of values, power depends more on the span of the data rather
than on the number of observations (see Shiller and Perron, p. 381).
Perron (1991) showed analytically that with fixed alternatives tests for
a unit root are only consistent when the time span rises with the num-
ber of observations. Diebold et al. (1991) also argue that long data
spans are important for identifying mean reversion in slowly decaying
processes.
When we consider time aggregation and skip sampling problems we
need to keep in mind the distinction between flow data and stock data.
If we are considering skip sampling at intervals of m periods, with a
stock variable yt, and sample values y$, we have
yl = yt for t = m, 2ra, 3m,...
In the case of a flow variable, however, it is a problem of time aggregation
and we have
%* = (l + L + L2 + .-. + L m- x)yt
Interest rates and currency rates fall in the former category whereas
consumption and GNP fall in the latter category. Shiller and Perron
(1985) and Perron (1989) discuss the skip sampling problem with stock
data. On the other hand, Choi (1992b) considers the problem of time
aggregation that is relevant for flow data. He finds, in his simulation
study, that using the data generated by aggregating subinterval data
results in lower powers of unit root tests. Thus, using quarterly data is
better than using annual data, and using monthly data is better than
using quarterly data. He also finds that for the aggregated data, the PP
tests are more powerful than the ADF test. Note that time aggregation
produces a MA residual in the aggregated series. For example, if the
quarterly model is
yt = pyt-i +V*, t = 1,2,..., T
then the yearly model is
xs = p4xs-i +v8, 
5 = 1,2,..., T/4
where
Xs = 2/4,8 + 2/4,s-l + 2/4,8-2 + 2/4,s-3

130 
Issues in unit root testing
and vs is a moving-average of ut.
Choi starts with a quarterly AR(1) model to generate the data on yt-
He then generates the yearly data using time aggregation and applies
unit root tests to the quarterly data and yearly data. The sampling
frequency is thus fixed.
Ng (1995) also does a Monte Carlo study starting from a continuous
time model. She studies the effect of varying the sampling frequency as
well as the total time span S of the data. She finds that with flow data:
(i) Increasing the frequency of observation while keeping S fixed in-
creases power but at a diminishing rate.
(ii) Power decreases if the frequency of observation is increased but
S is decreased.
(iii) Power increases if S is increased even if the total number of obser-
vation is kept constant (by reducing the frequency of observation).
Since time aggregation results in MA errors one would think that IV
tests would be appropriate but the power functions of the IV test are
not monotonic as S increases. With the ADF test and Phillips Za and
Zt tests power increases with increases in S.
Overall the conclusion on time aggregation with flow data is that in-
creasing the time span S increases the power of the commonly used ADF
and PP tests (though not of the IV tests) but increasing the frequency of
observation with S fixed also increases power, i.e., using quarterly rather
than yearly data does help in increasing power. With skip sampling the
results in Shiller and Perron and Perron suggest that increasing the fre-
quency of observation with the time span S fixed does not increase the
power of unit root tests. However, Choi and Chung (1995) re-examine
these studies with the ADF and PP tests (not examined earlier by Shiller
and Perron and Perron). They also consider the null of a unit root and
a unit root with drift (not considered earlier) and also relax the assump-
tion of taking the critical value as zero. The results now show that with
the ADF tests, using data with high sampling frequency can provide
significant improvement in finite sample power. But in the case of PP
tests sampling frequencies do not significantly affect the power of the
tests. For low frequency data, the PP test appears to be more powerful
than the ADF test.
Thus, with skip sampling too, there is an improvement in power by
using the frequency of observations if one uses the ADF test. Thus,
using quarterly data is better than using yearly data, and monthly data
than quarterly data. As an empirical example, Papell (1997) finds that

4-8 Other types of nonstationarity 
131
the evidence against the unit root in real exchange rate data is stronger
for monthly than for quarterly data. The data are on exchange rates
(not flow data) and thus fall into the category of skip sampling.
4.8 Other types of nonstationarity
The preceding discussion was in terms of I(d) models where d was as-
sumed to take on the values 1 or 0, the model with d = 1 being the
model with persistence of shocks. However, persistence of shocks can
also be modeled with d > 0 but < 1. This problem of fractional unit
roots is discussed in chapter 9.
Another cause for nonstationarity is the randomness of the coefficients
of autoregressions. Some references in this area are Cramer (1991),
Nicholls and Quinn (1982), McCabe and Tremayne (1995), and Ley-
bourne, McCabe, and Tremayne (1996).
There has also been some discussion about some economic variables
being 1(2), that is, having double unit roots. Examples of this are money
supply and prices. Hence questions arise as to how to test for more than
one unit root. There are two approaches: the first, a bottom-up approach
is the one discussed in Choi and Yu (1997) suggests testing sequentially:
1(0) versus 1(1) and 1(1) versus 1(2) if the first hypothesis is rejected.
The second approach suggested in Dickey and Pantula (1987) is a top-
down approach. First test 1(2) versus 1(1) and if the hypothesis of 1(2)
is rejected, then test 1(1) versus 1(0). This procedure is appropriate
for tests of nonstationarity as null, whereas the bottom-up approach is
appropriate for tests of stationarity as null. As discussed earlier, we can
also think of using both the tests together for confirmatory analysis but
we shall not pursue this here.
Dickey and Pantula consider the case of three unit roots which is
rarely the case with economic variables. But their procedure can be
easily adapted for tests of double unit roots. For completeness, we shall
review their procedure here.
The standard DF unit root tests assume that there is at most one unit
root in the series. Dickey and Pantula (1987) proposed t*- and F-tests
that compare a null hypothesis of k unit roots with an alternative of
k — 1 unit roots. Consider
v
Vt =
where {e^} is a sequence of iid random variables with mean 0 and vari-

132 
Issues in unit root testing
ance a1 = 1 and y_p+i = • • • = t/o = 0. They considered the AR model
with p = 3 and three possible unit roots. Let mi,m 2, and ms denote
the roots of the characteristic equation
m3 - ftm2 - (32m - /?3 = 0
The above AR(3) model can be written as
xt = Oiyt-i + 02zt-i + 03wt-i + et
where zt = yt — yt-i,wt 
= zt — zt-i 
and xt — wt — wt-i. 
Note that
ZtiWt, and Xt are the first-, second-, and third-differences of the process
yt, respectively. Consider the hypotheses
(i) no unit root
Ho : |rai| < 1
or
HQ : 6\ < 0 and some restrictions on 62 and 8s
For example, the restrictions on 62 and #3 are
-12 < 02 + 20i < 0, - 2 < 03 < 0
(ii) one unit root
Hi : mi = 1, |ra2| < 1
or
-#0 : 0i = 0,02 < 0 and some restrictions on 02 and 03
The restrictions on 02 and 03 are 0 < 4 + 02 + 203, - 2 < 03 < 0.
(iii) two unit roots
H2 : rrii = ra2 = 1, |m3| < 1
or
Ho : 0i = 02 = 0,03 < 0
(iv) three unit roots
H% : mi = rri2 = ms = I
or
Ho • 0i — 02 — 03 — 0

4-9 Panel data unit root tests 
133
Dickey and Pantula showed that applying F-statistics for testing from
the higher number of unit roots to the lower number of unit roots is valid,
while the reverse order of applying F-statistics is not valid. Based on
the asymptotic distributions of F-statistics derived by Pantula (1986),
they suggest testing the hypotheses sequentially in the order H%, H2,
and Hi :
(i) If H3 is rejected by the F-test, then go to step (ii); otherwise
conclude that H% is true,
(ii) If H2 is rejected by the F-test, then go to step (iii); otherwise
conclude that H2 is true,
(iii) If Hi is rejected by the F-test, then conclude that Ho is true;
otherwise conclude that Hi is true.
The empirical percantiles of the asymptotic distributions of the F-statistics
can be found in Pantula (1986).
Dickey and Pantula proposed alternative testing procedures based on
t-test statistics. However, the ^-statistics from the regression of Xt on
2/t_i, 2?t-i, and wt-i has different asymptotic distributions depending on
the number of unit roots present. And thus they argue that a sequential
procedure based on these statistics is not consistent. They suggest to
use alternative £*-statistics:
(i) for H3 against H2-, obtain the t*-statistic from the regression Xt
on 
Wt-i,
(ii) for H2 against Hi, use the t*-statistic in the regression Xt on Zt-i
and Wt-i for testing the coefficient of Zt-i is 0,
(iii) for Hi against iJo, use the t*-statistic in the regression xt on
yt-i,Zt-i 
and Wt-i for testing the coefficient of yt-i is 0.
Their results of a Monte Carlo power study show that the procedure
based on t* is more powerful than that based on F in most cases.
4.9 Panel data unit root tests
The principle motivation behind panel data unit root tests is to increase
the power of unit root tests by increasing the sample size. An alternative
route of increasing the sample size by using long time series data, it
is argued, causes problems arising from structural changes. However,
it is not clear whether this is more of a problem than cross-sectional
heterogeneity, a problem with the use of panel data.
It is often argued that the commonly used unit root tests such as ADF,

134 
Issues in unit root testing
Za, and Zt are not very powerful, and that using panel data you get a
more powerful test. For instance, in the case of tests for the purchasing
power parity (PPP) hypothesis, one can test the hypothesis that the real
exchange rate has a unit root. A nonrejection of this hypothesis implies
that PPP does not hold, even in the long run. Several studies fail to
reject the null of unit root, whereas panel data unit root tests as in Oh
(1996) and Wu (1996) reject the null. However, an alternative solution
to the low power problem is to use tests that have been demonstrated
to have better power. Cheung and Lai (1995) use the test developed by
Elliott et al. (1996) and find greater support for the PPP hypothesis. In
the following sections, we shall review the different panel data unit root
tests and then argue that they test a different hypothesis and thus, do
not really solve the problem of low power which is the basic motivation
in the first place. We shall also suggest a simpler test due to Fisher
(1932).
4' 9.1 The different panel data unit root tests
Breitung and Meyer (1994) (to be denoted BM) suggest a simple panel
data unit root test valid for fixed T and N —> oo. They consider the
model
Vi,t = «2/»,t-i + (1 - a)Vi + e»,t
eitt ~ «n(0,a2), i = 1,2,..,N,t = l, 2,...,T
Regressing y^i on y^t-i ignoring ^ gives an estimate a with asymptotic
bias
/ . 
N 
(l-a)S2
where S2 — N~1^2fi2. 
When a = 1, the bias vanishes. Hence the
unit root hypothesis can be tested using the t-statistic for HQ : a = 1.
However, since under the alternative \a\ < 1 the OLS estimate a is
biased, the test leads to a loss of power. BM suggest estimating the
equation
(Vi,t ~ 2/*,o) = <x(yitt-i - 2/i,o) + uiit
Denoting the estimator by a they show that
l i o o a = -(a + 1)

4-9 Panel data unit root tests 
135
Again, under the unit root hypothesis the bias disappears. BM suggest
other modifications to handle higher-order autoregressions, time trends,
and time effects.
As explained in Levin and Lin (1993a, 1993b), the approach taken
by BM cannot be used to analyze the influence of individual specific
effects or serial correlation on the appropriate critical values at which to
evaluate the t-tests. Also, it cannot be extended to allow for heteroge-
nous error distributions. For these reasons there are not many empirical
applications of these tests.
Quah (1994) considers the simple model
y%,t = otyitt-i 4 e»,t, 
sit ~ wd(0, a2)
and considering testing a = 1. Quah (1992) considers the model
Vi,t = M + «2/i,t-i 4 siit, 
sit ~ nd(0, a2)
and testing a = 1. These cases are not very interesting from the empirical
point of view. As pointed out by Levin and Lin, Quah's methodology
cannot be extended to the case of individual and time specific effects.
Levin and Lin (1993a) conduct an exhaustive study and develop unit
root tests for the model
2/*,t = otyitt-i 4 <$o + Stf 4- rji 4 vt + eitU 
£it ~ nd(0, cr2)
The model incorporates a time trend as well as individual and time
specific effects. Initially, they assume the iid errors, but they showed
that under the serial correlation of errors the test statistics have the
same limiting distributions by the inclusion of lagged first-differences of
yij like the parametric correction of the ADF test.
Levin and Lin consider the following six models
(i)
(ii)
(iii)
(iv)
(v)
(vi)
yi,t
y»,t
yi,t
y»,t
yi,t
yi,t
= ayi
= otyi
= ayi
= 
0LVi
= otyi
= otyi
,t-i
,t-i
,t-i
,t-i
,t-i
,t-i
4-
+
+
+
4
Vt
Vi
Vi
) + eitU
i 4- Sit 4-
• + £*,*,
. + £»,*>
,o 4- 77* 11
a =
Ho
Si,t,
Ho
Ho
+ e
= 1
:a
1
: a
: o;
it
= 1
= 
l , i
§o = 0
: = l,fil=0
7» = 0 for all i
For models (i) to (iv), they show that
- l ) => AT(0,2)

136 
Issues in unit root testing
For model (v), if y/N/T -> 0, then
Ty/N(& - 1) + 3VN =» iV(0,10.2)
VT25ta + Vl.875iV =» iV(0,845/112)
In model (vi), both intercept and time trend vary with individuals.
In the empirical applications, Oh(1996) uses only models (i) to (v).
Wu (1996) uses the complete model with trend, and individual and time
specific effects but uses the distributions derived for model (v). Pa-
pell (1997) uses model (v) with lagged first-differences of yi^t added but
computes his own exact finite sample critical values using Monte Carlo
methods and finds them 3-15 percent higher than those tabulated in
Levin and Lin (1993a).
Levin and Lin argue that in contrast to the standard distributions of
unit root test statistics for a single time series, the panel test statistics
have limiting normal distributions. However, the convergence rates are
faster as T —• oo (superconsistency) than as N —•» oo. The paper by
Levin and Lin (1993b) provides some new results on panel data unit
root tests. However, since the models discussed are the same, we shall
not go into details.
The major limitation of the Levin-Lin tests is that a is the same
for all observations. Thus, if we denote by OLI the value of a for the
ith. cross-section unit then the Levin-Lin test specifies the null Ho and
alternative Hi as
HQ : Oi\ — OL2 — • - • = &N = a = 1
Hi : ai = ct2 = - - - = &N = a < 1
Im, Pesaran, and Shin (1996, to be denoted IPS) relax the assumption
that ai = <*2 = • • • = &N under Hi. The basic idea of the test is simple.
Take the model (iv) in Levin and Lin and substitute c^ for a. Essentially
what we have is a model with a linear trend for each of the N cross-
section units. Thus, instead of pooling the data, we use separate unit
root tests for the N cross-section units. Consider the t-test for each
cross-section unit based on T observations. Let £^,i = 1,2,..., JV denote
the t-statistics for testing unit roots, and let E(ti) = [i and var(U) = a2.
Then
1—^ =» JV(O, i)

4-9 Panel data unit root tests 
137
The problem is computing /x and a2. This they do by Monte Carlo
methods and tabulate them for ready references (table A, B, C of their
paper).
The important thing to note is that the IPS test is a way of combining
the evidence on the unit root hypothesis from the N unit root tests
performed on the N cross-section units. Note that implicit in the test is
the assumption that T is the same for all cross-section units and hence
E(ti) is the same for all i. Thus, we are considering only balanced panel
data.
R. A. Fisher (1932) suggested a method of combining the evidence
from several independent tests. Suppose there are N unit root tests as
in IPS. Let Pi be the observed significance level (p-value) for the ith
test. The (-2^1ogP*) has a \2 distribution with d.f. 2N. This is an
exact distribution. This test often known as the P\ test mentioned in
Maddala (1977, section 4.12) has not received much attention.
The advantage of P\ test is that it does not require a balanced panel
as in the case of the IPS test. The disadvantage is the the p-value has to
be derived by Monte Carlo methods. Another advantage is that it can
also be carried out for any unit root test. The IPS test is easy to use
because there are ready tables available in the paper by Im et al. (1996)
for E(ti) and var(U). However, these are valid only for the ADF test.
The paper by Maddala and Liu (1996) presents the results on the
different panel data unit root tests based on real exchange rate data for
131 countries divided into six regional groups. The results are different
for the different tests. In general the Fisher test rejects the unit root null
more often than the others. It is difficult to judge which of these tests
is better, without a Monte Carlo study comparing their performance.
Maddala and Wu (1996) compare the powers of the LL, IPS, and
Fisher tests. They find that in a variety of situations the Fisher test is
more powerful than the IPS test which in turn is more powerful than
the LL tests.
One drawback of both the IPS and Fisher tests is that they depend
on the assumption that there is no cross-country correlation among the
errors. This assumption is almost always violated in practice. Maddala
and Wu find that the Fisher test is more robust than the IPS test to
the violation of this assumption. They suggest bootstrap methods (see
chapter 10) to obtain the critical values for the Fisher test.

138 
Issues in unit root testing
4' 9.2 The hypotheses of interest
Whether it is the use of panel data unit root tests in investigations of
PPP or growth and convergence, the first question is: what are the
hypotheses of interest? Consider the case of PPP and test of the long-
run validity of the PPP hypothesis in the form of testing for a unit root
in the real exchange rate. One may be interested in testing whether
the hypothesis of the PPP holds for the Japanese Yen against US dollar
(JY/US) exchage rate. In this case, what is of relavance are the data
on this exchange rate. It is no use to be told that we reject the validity
of the PPP even in the long run for the JY/US but that if we throw
in a large number of countries and use the panel data unit root test,
we do not reject the PPP hypothesis for the JY/US exchange rate. The
hypotheses under consideration in the two cases are different. This is the
major problem with the studies such as those of Oh (1996), McDonald
(1996), and Wu (1996).
On the other hand, one may be interested not in the validity of the
PPP for any particular exchange rate but as a general hypothesis. One
has the results of unit root tests for a number of exchange rates and the
null hypothesis of a unit root is rejected at different significance levels
for the different exchange rates. What one needs is a summary picture
of the conflicting evidence. The question is how does one do it? The
panel data unit root test of Levin and Lin does not help us answer this
question, the IPS test does, but under the limitations mentioned earlier.
A more important issue is not a test of the hypothesis of the validity of
the PPP but an estimate of the time it takes for deviations from the PPP
to correct themselves. We may be interested in this for each particular
exchange rate or as a general summary for all the exchange rates. For
this purpose the appropriate procedure is to use the panel data to get
improved estimates for the autoregressive parameter in the equation for
each of the real exchange rates. This issue has been discussed in Maddala
(1991) and Maddala and Hu (1995). The procedure is based on Bayesian
and empirical Bayesian methods discussed in the statistical literature.
Again, one can talk about the time it takes for deviations from PPP
to correct themselves - for each exchange rate or for a set of exchange
rates. The question is: what is a summary statistic? The answer is
that it is not a simple average of the time for each exchange rate but
a weighted average, or alternatively a simple average of the shrinkage
estimates obtained from the panel data. In any case panel data unit

4-10 Uncertain unit roots and the pre-testing problem 
139
root tests do not provide answers to all these problems. Thus, although
theoretically appealing, they are of limited value in practice.
4.10 Uncertain unit roots and the pre-testing problem
The unit root tests that we have discussed are always a prelude to further
inference. In this researchers have to face the problem of uncertainty
about the unit root. As Cochrane (1991) argued the important question
is not whether there exists a unit root and to classify time series into
the unit root or no unit root categories but to outline the appropriate
inferential procedures (confidence interval statements and so on). The
confidence intervals are different under the unit root and no unit root
cases. Under the unit root case we use the nonstandard distributions and
in the no unit root case we use the normal distributions. Which of these
should researchers apply? This problem does not arise in the Bayesian
framework, as we shall discuss in chapter 8. See also Stock (1994a), who
suggests a Bayesian-type approach. For confidence interval statement,
see Stock (1991).
Watson (1994, p. 2568) discusses a conservative inference which he
argues is widely used in applications. The procedure is as follows: let
W be the statistic of interest. Let CJJ and Cjy be the critical values
under the unit root (17) and no unit root (N) hypotheses, i.e., P(W >
Cu\U) = P(W > CN\N) = a. Then reject the unit root hypothesis if
W > max(Cu,Cjsr)- Do not reject it if W < rmn(C[/,Cjv)- However, a
problem arises if min(Cu,CN) < W < max(Cu,CN)- In this case he
says you have to look at the data and see which hypothesis is plausible.
By construction, the size of the test is < a.
This procedure, however, is just another procedure to classify the time
series into the unit root or no unit root categories as the procedures we
discussed earlier. It should therefore be compared with say the confir-
matory analysis we discussed earlier. It does not address the problem of
taking the uncertainty about unit roots into account in further inference
like confidence interval statements. Under this procedure we use the
standard normal distribution if we reject the unit root hypothesis and
the nonstandard distribution if we do not. The pre-testing problem still
remains.
The properties of this pre-test estimator from a decision theoretic
point of view are discussed in Lahiri and Paul (1996). Based on the
analysis of risk functions, Lahiri and Paul conclude that OLS is a better
choice for almost all economic time series following the AR(1) model,

140 
Issues in unit root testing
unless we have a strong belief that the series behaves like stock price
series, i.e., the autoregressive parameter p is equal to 1. In this case
nonstandard inference is used. The pre-test estimator was never found
to be optimal.
In the statistical literature, there is considerable discussion of the pre-
testing problem but the only consensus that has come out all this is
that one should not use the usual 5 percent significance levels but one
should use higher (say 25 percent) significance levels in pre-tests. A
good discussion of this is in the series of papers in Communications in
Statistics, A: Theory and Methods. 1976, under the title: "For What
Use Are Tests of Hypotheses and Tests of Significance?"
The pre-testing problem is all the more important in the discussion
of estimation and tests of cointegration relationship (chapters 5 and 6).
They all start with the hypothesis that the variables under considera-
tion are 1(1). We shall discuss the consequences of the violation of this
assumption in the forthcoming chapters. Given that all this analysis
depends on a unit root pre-test, the question of what significant levels
to use for unit root tests, if these tests are a prelude to cointegration
analysis, is a very important one for which there is no definite answer
yet.
4.11 Other unit root tests
Besides the commonly used ADF and PP tests, there are many other
unit root tests in the literature, but these have not been used much in
practice.
Burridge and Guerre (1996) observe that the number of level crossings
in a variable would be larger if there was a unit root than there would
be otherwise. They use this to derive a unit root test. However, they
find that this nonparametric test is more sensitive to the shape of the
distribution of the errors than the DF tests.
Bierens (1993) devises unit root tests based on higher-order sample
autocorrelations. Box and Jenkins observe that for a stationary process
the sample autocorrelation function tails off quickly. In practice, it hap-
pens quite often that unit root tests do not reject the null hypothesis of
a unit root although the sample autocorrelation function tails off rather
quickly. Bierens argues that the Box-Jenkins approach to determining
the degree of differencing on the basis of the behavior of the sample
autocorrelation function is misleading.
Abadir (1995b) notes that nonstationarity causes the limiting distri-

4-12 Median-unbiased estimation 
141
butions of the Wald (W) and Lagrange multiplier (LM) statistics to
become different from each other. He uses this divergence to devise a
new test for unit roots.
None of these tests have found much applicability. Note that they
all test the unit root hypothesis by looking at different consequences of
unit roots. The variance ratio tests discussed earlier in section 3.8 also
fall in this category but they have been used often in the literature on
macroeconomics and finance.
4.12 Median-unbiased estimation
Consider the AR(1) model with drift and trend as follows
yt = n + 0t + ayt-i+et 
(4.2)
where et are assumed to be iidN(0,a2). 
The least squares (LS) esti-
mator in this model exhibits substantial biases, especially when the AR
parameter a approaches 1. For the AR parameter a the bias of the LS
estimator tends to be downward and for the coefficient on the time trend
(3 the bias tends to be upward. Those biases are quite large as the AR
parameter gets close to 1.
Numerous papers deal with this bias of the LS estimator in the AR
processes. For stationary AR processes without a time trend Hurwicz
(1950), Marriott and Pope (1954), and Kendall (1954) established the
mean-bias of the LS estimator. Based on these results Orcutt and
Winokur Jr. (1969) constructed approximately mean-unbiased estimates
of the AR parameter in stationary models. Le Breton and Pham (1989)
calculated the exact and asymptotic biases of the LS estimator in sta-
tionary, unit root, and explosive AR(1) models without intercepts.
When the parameter space is bounded or when the distributions of es-
timators are skewed and/or kurtotic, the concept of median-unbiasedness
is often more useful than that of mean-unbiasedness. In the classical
normal linear regression model with fixed regressors the LS estimator is
median-unbiased. But, in the autoregressive model, the LS estimator is
not median-unbiased. As we mentioned in chapter 3, the LS estimators
of the AR models with unit roots have asymmetric distributions. In
this case there is no unambiguous measure of the center of the distri-
bution. The median may be a preferred measure to the mean because
the median is less sensitive to the tails of the distribution. In this sense
Rudebusch (1992) and Andrews (1993) proposed median-unbiased esti-
mators of the AR parameters. Since Rudebusch's (1992) procedure is

142 
Issues in unit root testing
open to the question of the existence and uniqueness of his estimator,
we will discuss Andrews' (1993) procedure for the AR(1) model and its
extention to the AR(p) model by Andrews and Chen (1994).
Andrews (1993) proposed a bias correction procedure for the LS esti-
mator in the AR(1) model allowing stationary and unit root cases. Since
the LS estimator is downwardly biased, the probability that the LS es-
timator OLLS underestimates the true AR parameter is more than half.
Andrews proposed to use the corrected LS estimator, median-unbiased
estimator, which satisfies the condition that the probability of underes-
timation equals the probability of overestimation.
Andrews (1993) considered three models: AR(1) without drift, AR(1)
with drift, and AR(1) with drift and trend. Since in the AR(1) models
the distribution of OLLS depends only on a and is monotone in a (a
proof of this property is given in appendix A of Andrews, 1993), one can
compute the quantiles of the distribution of the LS estimator according
to the values of a. Then the median-unbiased estimator is the value of
a at which the LS estimate is equal to a median (0.5 quantile of the
distribution of the LS estimator).
Andrews (1993) provides the 0.05, 0.5, 0.95 quantiles of the distribu-
tion of the LS estimator according to the values of a G (—1,1] for three
different AR(1) models. To compute the quantiles of the distribution of
the LS estimator he used the Imhof (1961) algorithm. To understand
his procedure let us discuss how to use his table. For the AR(1) model
with drift and trend, the quantile of the LS estimator with the sample
size 60 (T + 1 = 60) are given as table 4.5. To get the median-unbiased
estimator we use the second column which shows the 0.5 quantiles of
the LS estimator (medians) corresponding to the different values of a.
If the LS estimate OLLS is greater than 0.853, then the median-unbiased
estimate OLMU is 1. If OLLS is less than —0.997, then OLMU is —1. For
any case where the value of OLLS is between —0.997 and 0.853, the cor-
responding value of a from table 4.5 is the value of OLMU- Suppose that
with the sample size of 60 we have the LS estimate of a as 0.8, then
the 0.5 quantile value which corresponds to 0.8 is given as 0.9. Thus
OLMU = 0-9.
Andrews (1993) also provides the 0.05 and 0.95 quatiles of the dis-
tribution of the LS estimates of three models which can be used to
construct the confidence intervals for the parameter a. For example
consider constructing a 90 percent two-sided confidence interval for a
in the AR(1) model with drift and trend. As in the above example,
when OLLS = 0.8, the lower bound for the 90 percent CI can be found in

4.12 Median-unbiased estimation 
143
Table 4.5. Quantiles of the LS estimator in an AR(1) model with drift
and trend
Quantiles (T + 1 = 60)
a 
O05 
O5 
0.95
-0.999 
-1.010 
-0.997 
-0.945
0.70 
0.417 
0.624 
0.773
0.80 
0.515 
0.715 
0.846
0.85 
0.562 
0.758 
0.880
0.90 
0.607 
0.799 
0.912
1.0 
0.666 
0.853 
0.956
Source: Andrews (1983, table III).
0.95 quatiles of the OILS- When we look at the 0.95 quantiles, we can
find that a = 0.74 yields the value of the 0.95 quantile of 0.8 (utilizing
a linear interpolation). Thus the lower bound for the 90 percent CI is
0.74. For the upper bound we use the 0.05 quantiles column. The 0.05
quatile column shows that if OLLS — 0.8, then the upper bound should
be 1. Thus the median-unbiased 90 percent CI for a is (0.74,1).
Andrews showed that the median unbised estimator can be used to
compute the impulse response functions and to construct unbiased model
selection procedures for determining whether the series belongs to the
TSP or the DSP class. Andrews also showed that the median-unbiased
estimates of a and the corresponding exact confidence intervals for a
are quite robust to the nonnormal distribution of et.
Andrews applied the median-unbiased correction procedure for the
eight real exchange rate series for the sample period 73:01 to 88:07 which
has been analyzed in Schotman and van Dijk (1991a) and for two se-
ries, the velocity of money and industrial production, of the Nelson and
Plosser data. The AR(1) model with drift is used for the real exchange
rate series and the AR(1) with drift and trend is used for two series
from the Nelson and Plosser (1982) data. The six real exchange rate
series are of the US dollar against the currencies of Prance (FR), West
Germany (WG), Japan (JP), Canada (CA), United Kingdom (UK), and
The Netherlands (NL). The two series are of the German Dmark against
the currencies of France and The Netherlands. Andrews finds that for

144 
Issues in unit root testing
each of the US dollar exchange rates except the UK/US, the median-
unbiased estimate of a is 1.00. For the UK/US it is 0.995. For the
FR/WG and NL/WG exchange rates, the median-unbiased estimates
are 0.968 and 0.965, respectively. He finds that the magnitudes of the
differences between the LS estimates and the median-unbiased estimates
are small (between 0.017 and 0.022), but they are large in terms of their
implications for the persistence of the time series. He also finds that the
residuals (yt — otMuyt-i) for the eight real exchange rate series reveal
evidence of nonnormality for some of the series. But in spite of nonnor-
mal errors the median-unbiased estimates of a were unchanged except
for the FR/WG and NL/WG series for which the estimates differed by
at most 0.001 and 0.002. The confidence intervals were unchanged.
Andrews and Chen (1994) extended the exact median-unbiased cor-
rection method of Andrews (1993) to the AR(p) model. In the AR(1)
case the distribution of QLLS depends only on a and is monotone in a.
But in the AR(p) case for p > 1, the distribution of OLLS depends on
a and some nuisance parameters, i.e., the coefficients of the high-order
term yt-j for 1 < j < p. Andrews and Chen proposed an iterative pro-
cedure that jointly estimates a and the nuisance parameters and yields
an approximately median-unbiased estimate of a. Once they obtained
the approximately median-unbiased estimator of a, the rest of the proce-
dures is the same as those in Andrews (1993). The method for obtaining
a median-unbiased estimator of a is extended to generate approximate
confidence intervals for a and to an approximately unbiased model se-
lection procedure for determining whether a data series belongs to TSP
class or DSP class.
Andrews and Chen applied their procedure to the Nelson and Plosser
data set. They found that for three series out of 14 the median-unbiased
estimates are equal to 1.0, for seven series the estimates are 0.96 or large,
and for three series the estimates are 0.89 or large. These results indi-
cate that the median-unbiased estimates of a show less persistence than
the results of the ADF tests performed by Nelson and Plosser (1982)
(as we have seen the ADF tests suffered from low power), but show con-
siderably more persistence than the results by the Bayesian estimation
methods. (The Bayesian estimation methods will be discussed in chapter
8.) When they use the extended Nelson and Plosser data compiled by
Schotman and van Dijk (1991b), eight out of the 14 series, including all
of the norminal variables except money stock, have the median-unbiased
estimates equal to 1.0.
Abadir (1995a) proposed a minimum mean square error (MSE) esti-

4-13 Summary and conclusions 
145
mat or for the unit root case which is almost unbiased, while the max-
imum likelihood estimator is not. He showed that the bias of the min-
imum MSE estimator is virtually zero because of its asymptotic order
of T~2 unlike the MLE's T~l bias. However, since Abadir's conclusions
are based on the AR(1) model without drift, it is open to the question
whether his conclusions hold for AR(p) processes with drift and trend.
4.13 Summary and conclusions
The purpose of this chapter is to discuss several problems with the unit
root tests mentioned in the previous chapter, and to outline modifica-
tions of those unit root tests. We discuss several other unit root tests -
many of which we do not recommend, but they are all discussed for the
sake of completeness and also because they are often used. The reason
there are so many unit root tests is that there is no uniformly powerful
test for the unit root hypothesis (as discussed in Stock, 1994b).
We first start with the size distortions and poor power problem of unit
root tests. We then discuss solutions to these problem. The solutions
discussed are unit root tests for ARM A models (IV tests), modifications
of the PP (Phillips-Perron) tests, modifications of the ADF test (DF-
GLS test), and tests based on weighted symmetric estimators. Which
of these should be preferred is a question for which there is no clear-cut
answer, but it is better to use one of these than the ADF and PP tests
discussed in the previous chapter. It is time now to completely discard
the ADF and PP tests (they are still used often in applied work!).
We next discuss tests for MA unit root and tests with stationarity as
the null. In the latter category a test often referred to is the KPSS test.
We do not recommend it. It has the same poor power properties as the
ADF test. It should be avoided. The stationarity tests, it is claimed,
can be used in conjunction with the ADF test for confirmatory analysis.
But this is an illusion (as shown by Monte Carlo studies).
We next discuss problems of time aggregation and median-unbiased
estimation as an alternative to unit root testing. Finally, we discuss
panel data unit root tests which have been suggested as a solution to
the poor power problem. But these tests are for an entirely different
hypothesis. So it is meaningless to assert that they solve the problem of
poor power of unit root tests. They solve a different problem. We discuss
the Levin-Lin test (very widely used) and point out its limitations. We
also discuss the alternative Im-Pesaran-Shin (IPS) test and suggest an

146 
Issues in unit root testing
alternative: the Fisher test. In all these cases it is important to ask the
basic question: what is the point in the panel data unit root tests?
In fact, it is important to ask the question (rarely asked): why are
we interested in testing for unit roots? Much of this chapter (as is
customary) is devoted to the question "How to use unit root tests?v
rather than "Why unit root tests?"
One answer is you need the unit root tests as a prelude to cointegration
analysis (discussed in the next two chapters). But if this is the case, then
the unit root tests are pre-tests and it is not clear what significance levels
should be used. If the pre-testing literature is of any guidance we can
say that the 1 percent or 5 percent significance levels should not be used.
Much higher significance levels (say 25 percent) are appropriate.
The other argument is that if there is a unit root then inference is non-
standard. Confidence intervals constructed with the usual asymptotic
normal distribution are not valid. But just because you fail to reject the
unit root null at the 1 percent or 5 percent significance level, does not
mean that the unit root null is valid. There is a nonzero probaiblity that
it is not valid. Hence it is not clear that nonstandard inference is any
more appropriate than standard inference. Note that this problem does
not arise in Bayesian inference (discussed later in chapter 8) because
the posterior probabilities of the unit root model and stationary model
are taken into account. As Cochrane (1991, p. 283) remarked, " the
results of unit root tests do not necessarily answer the important ques-
tion, namely which distribution theory provides a better small sample
approximation." The problem is, neither does by itself. One needs to
combine both.
In summary, it is high time we asked the question: Why all this unit
root testing rather than keep suggesting more and more unit root tests
and use the Nelson-Plosser data as a guinea pig for every new unit root
test suggested.
References
Abadir, K.M. (1995a), "Unbiased Estimation as a Solution to Testing
for Random Walks," Economics Letters, 47, 263-268.
(1995b), "A New Test for Nonstationary Against the Stable Alter-
native," Econometric Theory, 11, 81-104.
Agiakoglou, C. and P. Newbold (1992), "Empirical Evidence on
Dickey-Fuller type tests," Journal of Time Series Analysis, 13,
471-483.

References 
147
Amano, R.A. and S. van Norden (1992), "Unit Root Tests and the
Burden of Proof," Working paper no. 92-7, Bank of Canada.
Anderson, T.W. and A. Takemura (1986), "Why do Non-invertible
Estimated Moving Averages Occur?" Journal of Time Series
Analysis, 7, 235-254.
Andrews, D.W.K. (1993), "Exactly Median-Unbiased Estimation of
First Order Autoregressive / Unit Root Models," Econometrica,
61, 139-165.
Andrews, D. W. K. and D. Chen (1994), "Approximately Median-
Unbiased Estimation of Autoregressive Models," Journal of
Business and Economic Statistics, 12, 187-204.
Arellano, C. and S. G. Pantula (1995), "Testing for Trend Stationarity
versus Difference Stationarity," Journal of Time Series Analy-
sis, 16, 147-164.
Beveridge, S. and C.R. Nelson (1981), "A New Approach to Decompo-
sition of Economic Time Series into Permanent and Transitory
Components with Particular Attention to Measurement of the
'Business Cycle'," Journal of Monetary Economics, 7, 151-174.
Bierens, H.J. (1993), "Higher Order Sample Autocorrelations and the
Unit Root Hypothesis," Journal of Econometrics, 57, 137-160.
Bierens H.J. and S. Guo (1993), "Testing Stationatiry and Trend Sta-
tionarity against the Unit Root Hypothesis," Econometric Re-
views, 12, 1-32.
Blough, S.R. (1992), "The Relationship Between Power and Level for
Generic Unit Root Tests in Finite Samples," Journal of Applied
Econometrics, 7, 295-308.
Breitung, J. (1994), "Some Simple Tests of the Moving-Average Unit
Root Hypothesis," Journal of Time Series Analysis, 15, 351-
370.
Breitung, J. and W. Meyer (1994), "Testing for Unit Roots in Panel
Data: Are Wages on Different Bargaining Levels Cointegrated?"
Applied Economics, 26, 353-361.
Burke, S.P. (1994), "Confirmatory Data Analysis: The Joint Appli-
cation of Stationarity and Unit Root Tests," Discussion paper
No.20, University of Reading, UK.
Burridge, P. and E. Guerre (1996), "The Limit Distribution of Level
Crossings of a Random Walk, and a Simple Unit Root Test,"
Econometric Theory, 12, 705-723.
Cheung, Y. and K. Lai (1995), "Lag Order and Critical Values of the

148 
Issues in unit root testing
Augmented Dickey-Fuller Test," Journal of Business and Eco-
nomic Statistics, 13, 277-280.
Choi, I. (1992a), "Durbin-Hausman Tests for a Unit Root," Oxford
Bulletin of Economics and Statistics, 54, 289-304.
(1992b), "Effects of Data Aggregation on the Power of Tests for a
Unit Root," Economics Letters, 40, 397-401.
(1994), "Residual-Based Tests for the Null of Stationary with Appli-
cation to US Macroeconomic Time Series," Econometric The-
ory, 10, 720-746.
(1995), "Sampling Frequency and the Power of Tests for a Unit Root:
A Simulation Study," Economics Letters, 49, 131-136.
Choi, I. and B. Chung (1995), "Sampling Frequency and the Power of
Tests for a Unit Root: A Simulation Study," Economics Letters,
49, 131-136.
Choi, I. and B.C. Yu (1997), "A General Framework for Testing I(m)
against /(m + A;)," Journal of Economic Theory and Economet-
rics, 3, 103-138.
Cochrane, J.H. (1991), "A Critique of the Application of Unit Root
Tests," Journaal of Economic Dynamics and Control, 15, 275-
284.
Cramer, H. (1991), "On Some Classes of Non-Stationary Processes,"
Proceedings of the Fourth Berkeley Symposium on Mathematical
Statiatics and Probability, vol. II, University of California Press,
Berkeley.
DeJong, D.N., J.C. Nankervis, N.E. Savin, and C.H. Whiteman
(1992a), "The Power Problems of Unit Root Tests in Time Se-
ries with Autoregressive Errors," Journal of Econometrics, 53,
323-343.
(1992b), "Integration versus Trend Stationarity in Time Series,"
Econometrica, 60, 423-433.
Dickey, D.A., D.P. Hasza, and W.A. Fuller (1984), "Testing for Unit
Roots in Seasonal Time Series," Journal of American Statistical
Association, 79, 355-367.
Dickey D.A. and S.G. Pantula (1987), "Determining the Order of Dif-
ferencing in Autoregressive Processes," Journal of Business and
Economic Statistics, 5, 455-461.
Diebold, F.X. and G. Rudebusch (1991b), "On the Power of Dickey-
Fuller Tests Aganist Fractional Alternatives," Economics Let-
ters, 35, 155-160.

References 
149
Elliott, G., T.J. Rothenberg, and J.H. Stock (1996), "Efficient Tests
for an Autoregressive Unit Root," Econometrica, 64, 813-836.
Fisher, R.A. (1932), Statistical Methods for Research Workers, 4th ed,
Oliver and Boyd, Edinburgh.
Fuller, W.A. (1996), Introduction to Statistical Time Series, 2nd ed,
Wiley, New York.
Gonzalo, J. and T.H. Lee (1996), "Relative Power of t-Type Tests for
Stationary and Unit Root Processes," Journal of Time Series
Analysis, 17, 37-47.
Hall A. (1989), "Testing for a Unit Root in the Presence of Moving
Average Errors," Biometrika, 76, 49-56.
(1995), "Residual Autocorrelation and Unit Root Tests Based on
Instrumental Variable Estimators from Time Series Regression
Models," Journal of Time Series Analysis, 16, 555-569.
Hall, A. and K. Hassett (1990), "Instrument Choice and Tests for a
Unit Root," Economics Letters, 35, 161-165.
Harvey, A.C. (1989), Forecasting, Structural Time Series Models and
the Kalman Filter, Cambridge University Press, Cambridge.
Henricsson, R. and E. Lundback (1995), "Testing the Presence and
the Absence of PPP: Results for Fixed and Flexible Regimes,"
Applied Economics, 27, 635-641.
Hurwicz, L. (1950), "Least-squares Bias in Time Series," in T.C. Coop-
mans (ed.),Statistical Inference in Dynamic Economic Models,
Wiley, New York.
Hwang, J. and P. Schmidt (1996), "Alternative Methods of Detrending
and the Power of Unit Root Tests," Journal of Econometrics,
71, 227-248.
Im, K.S., M.H. Pesaran, and Y. Shin (1996), "Testing for Unit Roots
in Heterogeneous Panels," Discussion paper, University of Cam-
bridge.
Imhof, J.P. (1961), "Computing the Distribution of Quadratic Forms
in Normal Variates," Biometrika, 48, 419-426.
Kang, K.M. (1975), "A Comparison of Estimators of Moving Aver-
age Processes," unpublished manuscript, Australian Bureau of
Census.
Kendall, M.G. (1954), "Note on Bias in the Estimation of Autocorre-
lation," Biometrika, 41, 403-404.
King, M.L. (1987), "Towards a Theory of Point Optimal Testing,"
Econometric Reviews, 6, 169-218.

150 
Issues in unit root testing
Kwiatkowski D., P.C.B. Phillips, P. Schmidt, and Y. Shin (1992),
"Testing the Null Hypothesis of Stationary against the Alter-
native of a Unit Root," Journal of Economertics, 54, 159-178.
Lahiri, K. and M. Paul (1996), "Relative Performance of Pre-Test Es-
timators in the Presence of a Unit Root," 1996 Proceedings of
the Section on Bayesian Statistical Science, American Statisti-
cal Association, Washington DC, 238-243.
Le Breton, A. and D.T. Pham (1989), "On the Bias of the Least Squares
Estimator for the First Order Autoregressive Process," Annals
of the Institute of Statistical Mathematics, 41, 555-563.
Lee, J. and P. Schmidt (1994), "Unit Root Tests Based on Instrumen-
tal Variables Estimation," International Economic Review, 35,
449-462.
Levin, A. and C-F. Lin (1993a), "Unit Root Test in Panel Data:
Asymptotic and Finite Sample Properties," Discussion paper
93-23, UCSD.
(1993b), "Unit Root Test in Panel Data: New Results," Discussion
paper 93-56, UCSD.
Leybourne, S.J. (1995), "Testing for Unit Roots Using Forward and Re-
verse Dickey-Fuller Regressions," Oxford Bulletin of Economics
and Statistics, 57, 559-571.
Leybourne S.J. and B.P.M. McCabe (1989), "On the Distribution of
Some Test Statistics for Parameter Constancy," Biometrika, 76,
169-177.
(1994), "A Consistent Test for a Unit Root," Journal of Business
and Economic Statistics, 12, 157-166.
Leybourne, S.J., B.P.M. McCabe, and A.R. Tremayne (1996), "Can
Time Series be Differenced to Stationarity?" Journal of Busi-
ness and Economic Statistics, 14, 435-446.
Li, H. (1995), "The Power of Modified Instrumental Variables Tests for
Unit Roots," Biometrika, 82, 660-666.
Liu, P.C. and J. Praschnik (1993), "The Size of the Nonstationary
Component and Its Effect on Tests for Unit Roots," Statistical
Papers, 34, 83-88.
Maddala, G.S. (1977), Econometrics, McGraw-Hill, New York.
(1991), "To Pool or Not to Pool: That is the Question," Journal of
Quantitative Economics, 7, 255-264.
Maddala, G.S. and W. Hu (1995), "The Pooling Problem," in L. Matyas
and P. Sevestre (eds.),Econometrics of Panel Data, 2nd ed,
Kluwer, New York.

References 
151
Maddala, G.S. and P.C. Liu (1996), "Panel Data Unit Root Tests:
What do They Test?" Paper presented at the International
Conference on Panel Data, Amsterdam.
Maddala, G.S. and S. Wu (1996), "A Comparative Study of Panel Data
Unit Root Tests and a Simplified Test," Paper presented at the
Econometric Society Meetings, New Orleans, Jan. 1997.
Marriott, F.H.C. and J.A. Pope (1954), "Bias in the Estimation of
Autocorrelations," Biometrika, 41, 390-402.
McCabe, B.P.M. and A.R. Tremayne (1995), "Testing a Time Series
for Difference Stationarity," Annals of Statistics, 23, 1015-1028.
McDonald, R. (1996), "Panel Unit Root Tests and Real Exchange
Rates," Economics Letters, 50, 7-11.
Nabeya, S. and K. Tanaka (1988), "Asymptotic Theory of a Test for
the Constancy of Regression Coefficients Against the Random
Walk Alternative," Annals of Statistics, 16, 218-235.
Nelson, C.R. and C.I. Plosser (1982), "Trends and Random Walks in
Macroeconomic Time Series," Journal of Monetary Economics,
10, 139-162.
Ng, S. (1995), "Testing for Unit Roots in Flow Data Sampled at Dif-
ferent Frequencies," Economics Letters, 47, 237-242.
Nicholls, D.F. and B.G. Quinn (1982), Random Coefficient Autoregres-
sive Models: An Introduction, Springer Verlag, New York.
Oh, K.Y. (1996), "Purchasing Power Parity and Unit Root Tests Using
Panel Data," Journal of International Money and Finance, 15,
405-418.
Orcutt, G.H. and H.S. Winokur, Jr. (1969), "Firts Order Autoregres-
sion: Inference, Estimation, and Prediction," Econometrica, 37,
1-14.
Pantula, S.G. (1986), "Determining the Order of Differencing in Au-
toregressive Processes," unpublished manuscript, Dept. 
of
Statistics, North Carolina State University.
(1991), "Asymptotic Distributions of Unit Root Tests When the Pro-
cess is Nearly Stationary," Journal of Business and Economic
Statistics, 9, 63-71.
Pantula S.G. and A. Hall (1991), "Testing for Unit Roots in Autore-
gressive Moving Average Models," Journal of Econometrics, 48,
325-353.
Pantula, S. G., G. Gonzalez-Farias, and W. A. Fuller (1994), "A Com-
parison of Unit-Root Test Criteria," Journal of Business and
Economic Statistics, 12, 449-459.

152 
Issues in unit root testing
Papell, D.H. (1997), "Searching for Stationarity: Purchasing Power
Parity under the Current Float," Journal of International Eco-
nomics, Forthcoming.
Park, H.J. and W.A. Fuller (1995), "Alternative Estimators and Unit
Root Tests for the Autoregressive Process," Journal of Time
Series Analysis, 16, 415-429.
Park, J.Y. (1990), "Testing for Unit Roots and Cointegration by Vari-
able Addition," in T.B. Fomby and G.F. Rodes (eds.),Advances
in Econometrics, vol. 8, JAI Press, 107-133.
Perron, P. (1989), "Testing for a Random Walk: A Simulation Ex-
periment of Power When the Sampling Interval Is Varied," in
B. Raj (ed.), Advances in Econometrics and modeling, Kluwer
Academic Publishers.
(1991), "Test Consistency with Varying Sampling Frequency,"
Econometric Theory, 7, 341-368.
Perron, P. and S. Ng (1996), "Useful Modifications to Some Unit Root
Tests with Dependent Errors and Their Local Asymptotic Prop-
erties," Review of Economic Studies, 63, 435-465.
Phillips, P.C.B. (1987), "Time Series Regression with a Unit Root,"
Econometrica, 55, 277-301.
Phillips, P.C.B. and P. Perron (1988), "Testing for a Unit Root in Time
Series Regression," Biometrika, 75, 335-346.
Plosser, C.I. and G.W. Schwert (1977), "Estimation of a Non-invertible
Moving Average Process: The Case of Over-differencing," Jour-
nal of Econometrics, 6, 199-224.
Quah, D. (1992), "International Patterns of Growth; I : Persistence in
Cross Country Disparities," LSE working paper.
(1994), "Exploiting Cross Section Variation for Unit Root Inference
in Dynamic Data," Economics Letters, 44, 9-19.
Rao, C.R. (1973), Linear Statistical Inference and Its Applications, 2nd
ed, Wiley, New York.
Rudebusch, G. (1992), "Trends and Random Walks in Macroeconomic
Time Series: A Re-examination," International Economic Re-
view, 33, 661-680.
Said, S.E. and D.A. Dickey (1984), "Testing for Unit Roots in
Autoregressive-Moving Average Models of Unknown Order,"
Biometrika, 71, 599-607.
Saikkonen, P. and R. Luukkonen (1993), "Testing for a Moving Av-
erage Unit Root in ARIMA Models," Journal of the American
Statistical Association, 88, 596-601.

References 
153
Sargan, J.D. and A. Bhargava (1983), "Maximum Likelihood Estima-
tion of Regression Models with First Order Moving Average
Errors When the Root Lies on the Unit Circle," Econometrica,
51, 799-820.
Schmidt, P. and P.C.B. Phillips (1992), "LM Tests for a Unit Root
in the Presence of Deterministic Trends," Oxford Bulletin of
Economics and Statistics, 54, 257-287.
Schotman, P. and H.K. van Dijk (1991a), "A Bayesian Analysis of the
Unit Root in Real Exchange Rates," Journal of Econometrics,
49, 195-238.
(1991b), "A Bayesian Routes to Unit Roots," Journal of Applied
Econometrics, 6, 387-401.
Schwert, G.W. (1987), "Effects of Model Specification on Tests for Unit
Roots," Journal of Monetary Economics, 20, 73-103.
(1989), "Tests for Unit Roots: A Monte Carlo Investigation," Journal
of Business and Economic Statistics, 7, 147-159.
Shephard, N. (1993), "Distribution of the ML Estimator of an MA(1)
Model and a Local Level Model," Econometric Theory, 9, 377-
401.
Shephard, N. and A.C. Harvey(1990), "On the Probability of Esti-
mating a Deterministic Component in the Local Level Model,"
Journal of Time Series Analysis, 11, 339-347.
Shiller, R. and P. Perron (1985), "Testing the Random Walk Hypoth-
esis: Power versus Frequency of Observation," Economics Let-
ters, 18, 381-386.
Stock, J.H. (1990), "A Class of Tests for Integration and Cointegra-
tion," manuscript, Harvard University.
(1991), "Confidence Intervals of the Largest Autoregressive Root in
Macroeconomic Time Series," Journal of Monetary Economics,
28, 435-460.
(1994a), "Deciding Between 1(1) and 1(0)," Journal of Econometrics,
63, 105-131.
(1994b), "Unit Roots, Structural Breaks, and Trends," in R.F. Engle
and D.L. McFadden (eds.),Handbook of Econometrics, vol. IV,
Elsevier, Amsterdam.
Tanaka K. (1990), "Testing for a Moving Average Unit Root," Econo-
metric Theory, 6, 433-444.
Watson, M.W. (1994), "Vector Autoregressions and Cointegration," in
R.F. Engle and D.L. McFadden (eds.), Handbook of Economet-
rics, vol. IV, Elsevier, North-Holland.

154 
Issues in unit root testing
Wu, Y. (1996), "Are Real Exchange Rates Stationary? Evidence from
a Panel Data Set," Journal of Money, Credit, and Banking, 28,
54-63.
Yap, S.F. and G.C. Reinsel (1995), "Results on Estimation and Testing
for a Unit Root in the Non-stationary ARMA Model," Journal
of Time Series Analysis, 16, 339-353.

5
Estimation of cointegrated systems
5.1 Introduction
In chapter 2 we introduced the concept of cointegration. In this chapter
we shall discuss different estimation methods for cointegrated systems.
In the next chapter we shall discuss tests for cointegration.
The methods of estimation fall into two categories:
(i) single equation methods,
(ii) system methods.
This classification is similar to the classification that we usually adopt in
the estimation of simultaneous equations models. In the single equation
methods we are interested in estimating only one particular cointegrating
(CI) vector. In the system methods, we also determine the number of
cointegrating vectors. In the following sections we shall discuss:
(i) the different methods of estimation when the number of CI vec-
tors is known,
(ii) estimation of the number of CI vectors,
(iii) inference on the coefficients (elements) of the CI vectors.
For the last problem we also face identification problems, the nature
of which is somewhat different from the identification problems we en-
counter in the usual simultaneous equation models.
5.2 A general CI system
Consider a VAR (vector autoregressive) model
Yt = A1Yt-1 
+ --- + AkYt-k 
+ Ut, 
t = l,2,...,T 
(5.1)
where Yt is an n-vector of 1(1) variables.
155

156 
Estimation of cointegrated systems
We shall consider some subcases of (5.1) before we discuss the general
VAR model. These are:
Case (i) n = 2 and no dynamics. We have only two 1(1) variables and
we can write the CI equation as
Vit = PV2t + tH
where ut is 1(0). This is normalized with respect to yu. We shall
discuss the issue of normalization later.
Case (ii) n > 2 but the model is a triangular system as discussed, inter
alia in Phillips (1991)
Vit = /3'2y2t + ult
where y2t is a vector of all the 1(1) variables other than y\t and
U2t is a vector of the same dimension. u\t is 1(0) because we are
considering a CI relationship and u2t is 1(0) because y2t is 1(1).
We shall discuss the several single equation estimation methods with
reference to this equation and then move on to the system methods.
However, since it is easier to highlight the main problems with reference
to the simplest methods, we start with a two-variable model first.
5.3 A two-variable model: Engle-Granger methods
Consider two variables yu and y2t which are both 1(1) and the cointe-
gration relationship
Vit = 0V2t + ut 
(5.2)
where Ut is 1(0). This two-variable model, with some modifications, is
often used in practice (e.g., in discussions of the tests of purchasing power
parity (PPP) theory and the Fisher effect on interest rates). Hence it is
important to consider it in detail.
There are several issues related to this simple model. They are:
(i) Uniqueness of /?. This has been discussed in section 2.10 of chapter
2. As noted there this uniqueness does not carry over to models
with more than two variables. In these models the CI vector need
not be unique.

5.3 A two-variable model: Engle-Granger methods 
157
(ii) Superconsistency of j3. Stock (1987) shows that the OLS estimator
(3 of (3 converges to its true value at the rate T (superconsistency)
instead of the usual rate y/T (consistency). This is because ^ v\t
is OP(T2) if 2/2t is 1(1). This superconsistency properly also holds
in the case of several 1(1) variables, where we consider the OLS
estimation of the CI vector.
(iii) Although (3 is superconsistent, Banerjee et al. (1986) and Baner-
jee et al. (1993, pp. 214-230) show through Monte Carlo studies
that there can be substantial small sample biases and suggest
estimating the long-run parameter j3 by estimating a dynamic re-
gression rather than the static regression (5.2). We shall discuss
details of this along with the small sample properties of the other
estimators.
(iv) As noted in section 2.10 of chapter 2, the fact that yu and y2t
are cointegrated implies that they are related by an ECM. The
ECM measures short-run dynamics whereas the CI relationship
(5.2) gives the long-run relationship. The Engle-Granger two-step
procedure involves estimation of j3 by ordinary least squares using
the static regression (5.2), substituting this for (3 in the ECM and
then estimating the ECM by least squares. The superconsistency
of /3 assures that the two-step estimators of the parameters of the
ECM have the same asymptotic distribution as the ones that one
would get if (3 is known.
(v) Another issue is that of normalization. Note that equation (5.2)
is normalized with respect to yu. If we normalize the equation
with respect to y^t and write
2/2t = «2/it + vt
then the least squares estimator a of a will also be superconsis-
tent and since a = 1/(3, we can get a superconsistent estimator of
(3 as I/a. If the R2 from the equation is very close to 1, then we
will have /3 ~ I/a. Hendry (1986) argues that this is the case with
most economic relationships. However, this will not be the case
in many instances. Ng and Perron (1997) study the normaliza-
tion problem in two-variable models. They show that the least
squares estimator can have very poor finite sample properties
when normalized in one direction but can be well behaved when
normalized in the other. This occurs when one of the variables is
a weak random walk or is nearly stationary. They suggest to use
as regressand, the variable that is less integrated. As a practical

158 
Estimation of cointegrated systems
matter they suggest ranking the variables by the spectral density
at frequency zero of the first-differenced series.
As an empirical illustration they consider the estimation of the
Fisher equation relating interest rates to expected inflation rates.
The Fisher equation is defined as
(1 - T)i = 7re + r
where r is the real interest rate, i is the nominal interest rate,
r is the marginal tax rate, and TT€ is the expected inflation rate
(unobserved). If we use the observed inflation rate ir for 7re, this
introduces an errors in variables bias, but if i and ?r are 1(1)
and they are cointegrated, we can get consistent estimates of the
parameters.
Ng and Perron do several unit root tests. They conclude that
the interest rate series is unambigously 1(1), but the evidence on
7T is mixed. They report results using TT and i as regressands and
argue that the results from a regression of TT on i is the one to be
trusted.
(vi) Although the least squares estimator (3 is superconsistent, its
asymptotic distribution depends on nuisance parameters arising
from endogeneity of the regressor and serial correlation in errors.
To see this, we shall write equation (5.2) as
V\t = PU2t + ult
Ay2t = u2t 
(5.3)
where u\t and u2t are 1(0) since y2t is 1(1). Then endogeneity can
be modeled by assuming that
cav(u\t, u2t) 
= 0"i2 iit = s
= 
0 otherwise
Serial correlation can be introduced in u\t and u2t. The alterna-
tive estimation methods that we will be discussing in the following
sections differ in the way these two problems of endogeneity of
the regressors and serial correlation in the errors are handled.
(vii) If u\t and u2t in (5.3) are independent and thus a\2 = 0, there
is no simultaneity. If uu are first-order autocorrelated with cor-
relation p, the DW statistic computed from the residuals of (5.3)
converges to the usual 2(1 — p) as with regressions of stationary

5.3 A two-variable model: Engle-Granger methods 
159
variables. Of course, if p = 1, then y\t and t/2t are not cointe-
grated and the DW statistic should be close to zero. This is the
reasoning behind the use of the DW statistic as a test for cointe-
gration, as suggested by Sargan and Bhargava (1983). We shall
discuss this test along with other tests for cointegration in the
next chapter.
(viii) In the case where there is no endogeneity and no serial correlation
in the errors u\t and U2t, the t-statistic for testing the hypothesis
(3 = 0 has the standard normal distribution asymptotically,
(ix) The asymptotic distribution of $ is also normal if in (5.3) we
assume that Ay2t has a drift, that is Ay2t = /JL + U2t-> and we
assume no endogeneity and no serial correlation in the errors u\t
and U2t- In this case
As we discussed in section 3.4 of chapter 3, t\i will dominate the
term St- Also
Now J2 vlt w i l 1 b e dominated by /i2 £ t2 and thus will be Op(r3/i2/3)
(see section 3.2). Hence
Also, ^2y2tUit will be dominated by fi^tuu 
(see section 3.2)
and
r3/2V«ult =* / rdW(r) =* 
A
JO
where o\ = var(uit). Hence we get the result
The result here is similar to the asymptotic normality of unit root
test statistics we discussed in section 3.4.1 and is subject to the
same criticism noted there.

160 
Estimation of cointegrated systems
5.4 A triangular system
Consider now a system consisting of more than two 1(1) variables. In this
case we can have more than one CI vector. The single equation methods,
however, assume that there is only one CI vector and concentrate on the
estimation of one CI vector. We shall discuss these methods in terms of
the triangular system
Vit = fi'yvt + uit
where y2t is the set of all the 1(1) variables other than y\t. We assume
that each element of y2t has one unit root and that there are no CI
relationships among the regressors y2t> Denote u't = (uit,u2t). We as-
sume that {ut} is strictly stationary with mean 0 and contemporaneous
covariance matrix
It is assumed that £ > 0.
The cumulative sums of ut follow a multivariate random walk. Fol-
lowing the same arguments as in section 3.2 of chapter 3, these partial
sums have a limiting Brownian motion or Wiener process W(r) which
we shall partition as
w=\Wl
corresponding to the partition of ut into u\t and «2t- The covariance
matrix of this is called the long-run covariance matrix which we denote
by ft.
t = l 5 = 1
This is the sum of all covariances backwards and forwards of ut and
us. It can be decomposed into a contemporaneous variance and sums of
autocovariances. Thus
n = s + A + A'
where E = £(u 0^), A = T,ZiE(uout) 
a n d A' = E S i ^ K ^ o ) - T h e
least squares estimator J3 of (3 is given by
where y\ is the vector of observations on y\t and Y2 is the matrix of

5.4 A triangular system 
161
observations on y2t- The least squares estimator /3 is superconsistent
but its asymptotic distribution depends on nuisance parameters arising
from the endogeneity of the regressors and serial correlation in the er-
rors. Specifically, the asymptotic distribution is given by (see Park and
Phillips, 1988)
T0-0) 
=» (J W*W£)([J W2d(W1.2+u>i2n£W2)\+6>)
where W±.2 = W± — c^fi^ 1^? 0 is partitioned as
[ 
2\ ^22
and
t=o
There are two different sets of procedures to deal with the endogeneity
and serial correlation problems. The procedures are similar to those
discussed in chapter 3 in connection with the PP and ADF unit root
tests. The PP tests modify the test statistic and the ADF test mod-
ifies the estimating equation. Similarly, the fully modified OLS (FM-
OLS) of Phillips and Hansen (1990) applies nonparameteric corrections
to the OLS estimator /3. The other methods like Saikkonen's dynamic
OLS (DOLS), Phillips and Loretan's nonlinear least squares (NLS), and
Stock and Watson's dynamic GLS (DGLS), add leads and lags to the
estimating equation. We shall discuss the FM-OLS method first and
then the other methods.
5.4.1 The FM-OLS method
The FM-OLS procedure (Phillips and Hansen, 1990) eliminates the nui-
sance parameters in the following way. First modify yn using the trans-
formation
and the error u\t also by
u~lt = u l t -

162 
Estimation of cointegrated systems
This is a correction for endogeneity. Next we construct a serial correla-
tion correction term <v which is a consistent estimator of
where uft = u\t — c^fJiiA^t- The FM-OLS estimator combines these
two corrections to the least squares estimator and is given by
As for (l they suggest the Newey-West estimator we discussed in chapter
3 in connection with the Phillips-Perron tests. However, as commented
in chapter 4, the modifications suggested by Perron and Ng to the PP
tests can also be implemented in this case.
There is also the question of normalization we discussed earlier with
the two-variable model. Ng and Perron (1997) argue that the normal-
ization problems apply to the FM-OLS method as well and that the
guidelines using less integrated variables as regressands would seem ap-
propriate.
Phillips and Hansen (1990) present simulation evidence to argue that
the FM-OLS performs well. However, we shall discuss this after going
through the other single equation and system estimation methods, where
it is argued that some other estimators perform better.
5.4-2 Adjusting estimating equations
The FM-OLS method is similar in spirit to the PP unit root tests in
that it starts with the OLS estimator and applies corrections to it to
take care of the endogeneity and serial correlation problems. The other
methods are similar to the ADF test in that they modify the estimating
equations. The methods we shall discuss are those suggested by Hendry;
Saikkonen; Phillips and Loretan; Stock and Watson; and Inder. We shall
discuss these in turn.
Single equation ECM (SEECM): Hendry's dynamic regression
method
In the case of the Engle-Granger two-step procedure we discussed ear-
lier, Banerjee et al. (1986) present simulation results and argue that
ignoring lagged terms in a static equation like (5.2) may lead to sub-
stantial biases in the estimation of (3 in finite samples. They propose

5.4 A triangular system 
163
to estimate the long-run parameters by an unrestricted error correc-
tion model (ECM) incorporating all the dynamics. Hendry has ad-
vocated this method in several papers. Broadly speaking the method
involves adding lags of Ayu and Ay2t to equation (5.2) and estimat-
ing the resulting equation. The idea is to start with a sufficiently large
number of these lags and progressively to simplify it. Hendry's PC-
GIVE computer program can be used to implement this. Ericsson et al.
(1990) describe the computer program and also Hendry's methodology.
Saikkonen's dynamic ordinary least squares (DOLS)
This estimator was suggested by Saikkonen (1991), but the term DOLS
was used by Stock and Watson (1993) who generalize it to systems with
higher orders of integration. Saikkonen's method involves estimating
yit = fiyit +
j
where k\ and k2 are selected to increase at an appropriate rate with T.
The procedure involves adding leads and lags of Ay2t but not of yn.
Phillips and Loretan's nonlinear least squares (NLS)
Phillips and Loretan (1991) suggest a dynamic equation of the form
Vit =
The procedure is in the spirit of the ECM and included lagged values of
the equilibrium error. By comparison, Hendry's method which involves
adding lags of Ay\t and Ay2t with no restrictions on the coefficients
and merely designed to mop up the dynamics is more ad hoc. However,
since /? occurs in a nonlinear way, the equation has to be estimated by
nonlinear least squares methods. Phillips and Loretan (1991, pp. 427-
428) report that in their simulation study, the nonlinear formulation did
create some problems in estimation.
Stock and Watson's dynamic generalized least squares (DGLS)
Stock and Watson (1993) suggest correcting for serial correlation in
Saikkonen's method by using GLS. They suggest getting the estimated
residuals using Saikkonen's method to construct the covariance matrix
of the errors and then estimating the equation by GLS. Both Saikkonen's

164 
Estimation of cointegrated systems
and Stock and Watson's procedures thus start with the OLS estimation
of the model (5.4).
Inder's fully parametric least squares (FPLS)
Inder (1995) starts with the VAR model in equation (5.1) and derives
the implied single equation
Vit = P'v2t + A(L)Aylt + B(L)Ay2t + vt 
(5.5)
where A{L) and B(L) are polynomials in the lag operator L. Then after
suitable truncation of the lag polynomials A(L) and B(L) his suggestion
is to regress yu on y2t, leads and lags of Ay2t, and lags of Ayit. The
leads and lags of Ay2t eliminate any effect of endogeneity and the lags
of Ayu capture remaining autocorrelation in the stationary component
of the regression. Inder shows how this is linearization of the nonlinear
equation of Phillips and Loretan which is
Vit = P'v2t + Ai(L)(yit - f3'y2t) + B1(L)Ay2t + vt
There are two problems yet to be resolved. The first is that the presence
of Ayu in equation (5.5) means that equation (5.5) cannot be estimated
by ordinary least squares. Hence, one has to use the instrumental vari-
able method suggested in Bewley (1979). The other problem is that of
choosing the lag length. This problem is common to all the procedures
discussed above.
Selection of the lag length
In all these procedures that we have discussed, there is the problem of the
selection of the appropriate lag length in the truncation of the lag poly-
nomials A(L) and B(L). Inder suggests starting with a model with only
the current values of Ayu and Ay2t, regressing the residuals vt on two
extra lags of Ayu and Ay2t and two leads of Ay2t, and checking TR2 us-
ing a x2 distribution for its significance. This is the LM test for the joint
significance of the extra regressors. If this is rejected, increase one of
the lag and lead truncation values by one and repeat until the diagnostic
test is not rejected. However, this procedure of going to a more general
model from a restricted model often termed specific to general has been
often criticized. See the discussion in section 3.6.2 of chapter 3. As dis-
cussed there, Hall's general to specific method is preferable to other rules
for selection of the truncation lag. Although there is no corresponding
discussion of lag selection as in the case of the ADF test, we conjecture
that the same rules apply in the estimation of the CI regressions. As

5.5 System estimation methods 
165
noted earlier, the above methods are in the spirit of the ADF test for unit
roots, i.e., modifying the estimating equation adding lags (and leads).
5.5 System estimation methods
We have discussed the single equation methods, where we assume the
existence of one CI vector, which is of interest. Since all these methods
are associated with least squares estimation, there are the normalization
problems, which we talked about earlier. Ng and Perron (1997) argue
that their rules derived for the two-variable model, apply to the FM-OLS
method as well, but do not comment on their applicability to the other
methods discussed earlier by Hendry, Saikkonen, Phillips, and Loretan,
and others.
By contrast in the system estimation methods the problem of normal-
ization does not appear (except where noted later) and also the number
of CI vectors is not fixed a priori but determined in the course of esti-
mation.
The system estimation methods we shall discuss are:
(i) Johansen's procedure (this is the most commonly used),
(ii) Box-Tiao procedure,
(iii) Methods depending on principal components.
5.5.1 The Johansen procedure
Johansen's procedure applies maximum likelihood to the VAR model,
assuming that the errors are Gaussian.
(i) Start
Yt = Aiyt-i + • • • + AkYt.k + Uu 
t = 1,..., T
where Yt is an n-vector of 1(1) variables,
(ii) Write it as
AYt = B1Yt.1 + B2AYt.1 + • • • + BkAYt.k+1 
+ Ut
where B1 = -I + £* = 1 A{ and Bj = - £ t j A< for j = 2,..., k.
(iii) Since Alt, • • •, AY^-fc+i are all 1(0) but Yt-i is 1(1), in order that
this equation be consistent, B\ should not be of full rank. Let its
rank be r. Write

166 
Estimation of cointegrated systems
where a is an n x r matrix and /?' is an r x n matrix. Then (3'Yt-\
are the r cointegrated variables, j3' is the matrix of coefficients
of the cointegrating vectors and a has the interpretation of the
matrix of error correction terms.
(iv) Since our interest is in a and /?' we eliminate B2,..., 
Bk first. To
do this we proceed as follows. Regress Alt on A7 t-i,. •., Al^-fc+i
Get the residuals. Call them ROt. Regress lt_i on these same
variables. Get the residuals. Call them R\t. Now our regression
equation is reduced to
Rot = a(3'Rlt + ut
This is a multivariate regression problem. Define
$00 So:
as the matrix of sums of squares and sums of products of Rot
and Rit. (Each of these matrices is of order n x n.) Johansen
(1991) shows that the asymptotic variance of fi'Rit is /3'En/J, the
asymptotic variance of Rot is EQO and the asymtotic covariance
matrix of (5'R\t and Rot is /3'Eio where EQO, ^IO> and En are the
population counterparts of SOOJSIO? and Su.
(v) We shall maximize the likelihood function with respect to a hold-
ing P constant and then maximize with respect to /3 in the second
step. We get
Note that a1 is an r x n matrix and the conditional maximum of
the likelihood function is given by
/T = \Soo - S
Maximization of the likelihood function with respect to /3 implies
minimization of this determinant with respect to (3.
(vi) We shall use the identity
x 
\A-BC^B'\-\C\
lC~BA 
B] = 
\A\
with C = SOo,A = 13'Su(3, and B = (3'SW. We then have to
minimize
\/3'Snf3\

5.5 System estimation methods 
167
But
\X'(A1-A2)X\
\X'A1X\
is given by the maximum characteristic root of the equation \A2 —
XAi\ = 0. Thus, substituting A\ = Su and A<i = SIQSQQSOI
we get the maximum of the likelihood function by solving the
eigenvalue problem
|Sio<% $01 — XSu\ = 0
or finding the eigenvalue of
\S^S10S^Soi-XI\=0 
(5.6)
But the roots of this equation are the r canonical correlations
between Ru and Rot- That is we seek those linear combinations
of Yt-i that are highly correlated with linear combinations of AYt
after conditioning on the lagged variables AYt-i,..., 
AYt-k+i-
(vii) Note that if the eigenvalues of A are A^, the eigenvalues of (/ — A)
are (1 — A^). Hence if A^ are the canonical correlations given
by solving equation (5.6), then (1 — A^) are the eigenvalues of
(/ — Sn Si0S00 Soi).
(viii) Since the value of the determinant of a matrix is equal to the
product of its eigenvalues, we have
- l
I^H ~ ^10^00 I
Again using the determinant identity in (vi) we get this equal to
|*Soo — SoiS^ S
\Soo\
Hence
i ^ T = |Soo|-n?=i(l-A<) 
(5.7)
Note that this result corresponds to the result for the normal
multiple regression model
LmliT = constant • (y'y)(l - R2)
y'y is replaced by the generalized variance |5oo| a n (l 1 — R2 is
replaced by
niU(i-Ai)
where A^ are the canonical correlations.

168 
Estimation of cointegrated systems
(ix) To determine the number of CI vectors Johansen suggests two
tests: the trace test and the maximum eigenvalue test. We shall
discuss these in the next chapter.
The number of cointegrating relationships in the system, r, is chosen
in the procedure by LR tests described in chapter 6, which make use of
the fact that if there are r cointegrating vectors, then the (n — r) smallest
eigenvalues of equation (5.6) are zero. The corresponding r eigenvectors
are chosen as cointegrating vectors. This way of imposition of n — r
restrictions in the system yields an asymptotically efficient and optimal
estimator of the cointegrating vectors.
The Johansen procedure is called reduced rank regression. This was
introduced by Anderson (1951) in the context of independent variables
and has been applied by Ahn and Reinsel (1988) for stationary processes
and Ahn and Reinsel (1990) for nonstationary processes. By reduced
rank regression of xt and yt corrected for zt we mean the following
regression: regress Xt on zt and yt on Zt to form residuals Ut and Vt
respectively and then regress Ut on vt. This is the procedure used in
step (iv) described earlier.
There are two very minor differences between the Johansen and Ahn
and Reinsel approaches. The first is in the normalization used. The
second is in the computational algorithms used. Johansen used partial
canonical correlation analysis, while Ahn and Reinsel suggest an algo-
rithm based on iterated least squares. Both methods yield the same final
results. Because the differences are minor, we shall not elaborate on the
Ahn and Reinsel method here.
5.5.2 The Johansen procedure with trends
In the preceding discussion, the model had no constant term or trend.
This is an unrealistic assumption in practice. But before we discuss the
Johansen procedure with trends, we shall clarify the difference between
stochastic and deterministic cointegration.
Stochastic versus deterministic cointegration
The presence of deterministic trends in the 1(1) variables, has led to
two definitions of cointegration: stochastic cointegration and determin-
istic cointegration (see Ogaki and Park, 1997 and Campbell and Perron,
1991).
A vector of 1(1) variables Yt are said to be stochastically cointegrated

5.5 System estimation methods 
169
with cointegrating rank r, if there are r linearly independent combina-
tions of the variables Yt that are 1(0). These combinations may have
nonzero deterministic trends.
The variable Yt are said to be deterministically cointegrated with coin-
tegrating rank r, if the r combinations of Yt that are 1(0) are stationary
with no deterministic trends. That is, under deterministic cointegration,
the same cointegrating vectors that eliminate stochastic nonstationarity
and also eliminate deterministic nonstationarity. The Engle and Granger
(1987) concept of cointegration is that of deterministic cointegration.
Johansen (1992a) and Perron and Campbell (1993) have modified the
original Johansen method to accommodate the possibility of stochastic
cointegration and trend stationarity. Though these modifications are
slightly different from each other, the modifications are simply to in-
clude time trend as an additional regressor in the Johansen estimation
method. The significance tables for the test statistics, however, are dif-
ferent. These differences will be discussed in chapter 6 (section 6.5.1).
5.5.3 The Box-Tiao method
The Box-Tiao (1977) procedure (to be referred to as BT) is an alter-
native way of deriving cointegrating vectors using canonical correlation
analysis. Bossaerts (1988) first proposed the BT procedure as an alter-
native to the Engle-Granger methods of estimating cointegrating vectors
and applied the procedure to investigate common nonstationary compo-
nents of US stock prices.
BT noted that if one finds the canonical correlations between yt and
2/t-i? the most predictable components would be nearly or actually non-
stationary and the least predictable components would reflect stable
stationary processes. But what we consider in the BT procedure is
canonical correlations in the levels and hence Bewley and Yang (1995)
call this method levels canonical correlation analysis. LCCA.
The idea in Johansen method is to find linear combinations of Yt-\
that are most highly correlated with Alt (which is 1(0)) on the argument
that 1(0) and 1(1) variables are uncorrelated. The idea behind the BT
method is to find linear combinations of Yt-i that are least correlated
with Yt (which is 1(1)). Because the dependent variable in the Johansen
procedure is 1(0) and the dependent variable in the BT method is 1(1),
the distribution theory is more complicated in the BT procedure than
in Johansen's method. However, this distribution theory is developed in

170 
Estimation of cointegrated systems
Bewley and Yang (1995) and this should result in more usage of the BT
procedure as modified by Bewley et al. (1994).
Bewley et al. (1994) modify the original BT procedure to allow for
deterministic trends and other variables explaining short-run dynamics.
As in the Johansen procedure, these variables have to be factored out
first by estimating regressions of Yt and Yt-\ on these variables, and
the residuals computed. If we call these residuals Rot and Ru then the
equation to be estimated is
Rot = BRlt + ut 
(5.8)
What we are concerned with is the canonical correlations between Rot
and Rit- These are obtained as the eigenvalues of the same equation
as (5.6) except that Rot and Ru (and hence Soo and Soi) are defined
differently in the BT method than in the Johansen method. In the
BT method the residuals Rot are obtained from a regression of Yt on
the deteministic components and short-run dynamics. In the Johansen
method Rot are obtained from a regression of Al* on these variables.
Also, in the Johansen procedure we pick the larger eigenvalues, and
consider the corresponding eigenvectors as cointegrating vectors. In the
BT procedure we pick the smallest eigenvalues and consider the corre-
sponding eigenvectors as cointegrating vectors.
The asymptotic theory for the BT procedure has only recently been
developed in Bewley and Yang (1995). Until now this has been a ma-
jor drawback of the BT procedure. However, some Monte Carlo studies
compared the performance of the BT and Johansen's methods. Gonzalo
(1994) compared the two. He concluded that the asymptotic distribu-
tions of non-ML estimators contain terms that create systematic biases
in finite samples and hence concluded that the Johansen estimator is
better than the others he considered (including the BT estimator).
Bewley et al. (1994) extend this comparison between the BT and
Johansen estimators taking into account kurtosis and dispersion of the
small sample distributions. They find that the BT estimator performs
better than the Johansen estimator (i.e., it exhibits less kurtosis and less
dispersion) when the adjustment to equilibrium is slow, the sample size
is small, and the disturbances are not highly correlated (this correlation
is a measure of the endogeneity of the 1(1) regressor in the cointegrating
regression). The relative advantage of the BT estimator declines as the
sample size increases, and as the correlation in the disturbances (degree
of endogeneity) increases in absolute value. Bewley et al. argue that

5.5 System estimation methods 
171
there are many interesting cases in which the performance of the BT
estimator is relatively good.
The asymptotic theory of the BT estimator, derived in Bewley and
Yang (1995) leads to tests for cointegration similar to those in the Jo-
hansen procedure. We shall discuss these in chapter 6 and present a
comparison of the two testing procedures.
5.5.4 Principal components and cointegration
Consider a set of n variables 2/1,2/25 •• -->Vn with covariance matrix E.
Consider a linear combination d = c'y of the ys. We want to determine
c so that it has maximum variance. Obviously we need some constraint
on the cs. A constraint often imposed is c'c = 1 . The variance of d is
then c'Ec. Thus, the problem is to maximize c'Ec subject to c'c = 1 or
to maximize
c'Ec - \(c'c - 1)
On differentiating with respect to c and equating the derivatives to zero
we get
Ec-Ac = 0 or (E-AJ)c = 0 or |E - A/| = 0
Thus, the problem is to find the eigenvalues of E. Order the eigen-
values as Ai > A2 > • • • > An. Let the corresponding eigenvectors be
ci, C2, • • •, cn. Then d\ — c[y, d2 = c'2y,..., dn = c'ny are the principal
components. We have var(d\) = \\,var(d2) 
= A2..., etc. where d\
is the first principal component and has the highest variance and d2 is
the second principal component and has the second highest variance,
and so on. Also, the principal components are orthogonal, given the
orthogonal properties of the cs. Also note that the generalized variance
The relationship of principal components to cointegration is now clear.
Since the variance of a CI relationship is smaller compared to that of
an 1(1) variable, the principal components corresponding to the smaller
eigenvalues give the CI vectors and those corresponding to the larger
eigenvalues give the common stochastic trends. The principal compo-
nent approach was first used by Stock and Watson (1988) in the context
of testing for common trends. Its asymptotic properties have not been
published except in Gonzalo (1994) who derived the asymptotic distri-
bution for a particular data generating process. Harris (1997) works out
the asymptotic distributions of the principal components estimator. He

172 
Estimation of cointegrated systems
shows that it is consistent but asymptotically inefficient and suggests a
modified principal components estimator. The modifications are similar
to those used in FM-OLS, except that the adjustments made are to the
data (and not to the estimator). The adjusted data are used to com-
pute the principal components. Harris argues that the results from the
Johansen procedure are sensitive to the number of lagged terms used
in the VAR model. The pricipal components method, by contrast, does
not suffer from this problem.
We shall not go into the details of Harris' paper. But given that the
canonical correlation and principal components methods are commonly
in use in multivariate analysis and that the idea behind principal com-
ponents is an intuitively appealing one for cointegration, it is quite likely
that the principal components method will gain more acceptance in the
future.
5.5.5 Stock and Watson method
If there are n variables and there are k cointegrating relationships, this
means that there are k linear combinations that are 1(0) and n — k linear
combinations that are 1(1). These are the common stochastic trends.
The Stock and Watson (1988) procedure is:
(i) Pick the autoregressive order p.
(ii) Compute the eigenvectors of YlVty't-) that isi d° a principal com-
ponent analysis of yt-
(iii) Using the m principal components with largest variance, that
is, the largest eigenvalues, fit a vector autoregression to the dif-
ferences. If Pt is the vector of m principal components, then
estimate
APt = Ai APt_i + ... + V - i APt-p+i + et
Let Ai,..., Ap-i be the estimates of the coefficient matrices,
(iv) Compute
Ft = Pt- i i P t - i - ... - ip-iPt-p
(v) Regress AFt on Ft-\ getting the coefficient matrix B.
(vi) Compute the eigenvalues of i?, normalize, and compare with the
tables in Stock and Watson (1988). Rejecting the null of m com-
mon trends in favor of (m — q) common trends increases the num-
ber of cointegrating vectors by q.

5.6 The identification problem 
173
5.5.6 Some problems with the Johansen procedure
The Johansen procedure depends on the assumption that the errors are
independent normal. The procedure is very sensitive to this assump-
tion. When the errors are not independent normal (see Huang and
Yang, 1996), it has been found that the Johansen method has a greater
probability (than least squares methods) of rejecting the null of no coin-
tegration even when there are no cointegrating relations.
Another limitation discussed by Phillips (1994) and mentioned earlier
is that the Johansen procedure produces more outliers than the other
procedures.
The study by Gonzalo and Lee referred to in the next chapter also
shows that the Johansen LR tests tend to find spurious cointegration in
some situations of practical interest (see section 6.5.3 for other limita-
tions of the Johansen procedure).
In summary, the major drawbacks of the Johansen procedure are:
extreme sensitivity to departures from the underlying distributions of the
error terms, tendency to find spurious cointegration, and high variance
and high probability of producing outliers. By comparison, the least
squares methods are more robust. This is not a new conclusion. The
same was observed in the case of simultaneous equations estimation
methods regarding FIML and least squares procedures.
5.6 The identification problem
The word identification has somewhat different meanings in the statis-
tical and econometric literature. For instance Box and Jenkins talk of
identification of the appropriate orders of p and q in ARMA(p, q) mod-
els, or p, d, and q in ARIMA(p, d, q) models. This is different from the
terminology in econometrics. In the following we shall use the word
identification as is customary in econometrics.
It is a well known fact that linear combinations of CI vectors are also
cointegrated. Thus, when the Johansen procedure shows the existence of
r CI vectors, one needs to impose some conditions arising from economic
theory to give meaning to the CI vectors. Since the CI vectors constitute
long-run relationships, these conditions would help identify the long-run
relationship.
Johansen (1995) and Johansen and Juselius (1994) argue that there
are two sets of conditions to identify the long-run and short-run rela-
tionships. Intuitively, this makes sense because there can be several

174 
Estimation of cointegrated systems
short-run dynamic paths to reach the same long-run equilibrium and
one needs some identifying conditions to choose among these different
paths.
The basic starting point of the whole literature on cointegration was
that economic theory had very little to say about short-run economic
behavior and all economic theories are about long-run behavior. If this
is the case we can get very little help from economic theory to identify
short-run relationships. However, if economic theory does provide infor-
mation to identify the short-run relationships, then it is intuitively clear
that this information would also identify the long-run relationships.
Hsiao (1997), on the other hand, argues that there is only one set of
conditions that identify both the short-run and long-run relationships
and these conditions are the same as those for dynamic simultaneous
equation models. But if there exists a set of prior restrictions to identify
the short-run relationships, then this same set of restrictions is sufficient
to identify the corresponding long-run relationships. On the other hand,
if there exist a set of prior restrictions to identify the long-run relation-
ships, these are not sufficient to identify the short-run relationships.
The main reasons why there is a seeming conflict between the ap-
proaches of Johansen and Juselius (hereafter denoted as JJ) and Hsiao
are as follows: Hsiao assumes that all the relevant economic informa-
tion is in the structural equation system and so we can incorporate all
our knowledge of the system in restrictions on this structural equation
system. Since the structural equations define both the short-run and
long-run behavior, the restrictions for identification are the same for
both the short-run and long-run behavior. In the discussion by Hsiao
the number of CI vectors is known.
The Johansen method, on the other hand, starts with a VAR model
in the 1(1) variables and first determines the number of CI vectors. The
approach is a-theoretical. Cointegration is a purely statistical concept
and the CI vectors need not have any economic meaning. That is why
JJ (1994, p. 8) distinguish between three concepts of identifiction:
(i) generic identification which is related to a linear statistical model,
(ii) empirical identification which is related to the estimated param-
eter values,
(hi) economic identification which is related to the economic inter-
pretability of the estimated coefficients of an empirically identi-
fied structure.
JJ, thus do not introduce economic theory until much later in the anal-

5.7 Finite sample evidence 
175
ysis. The question is: which of these two approach is more useful in
practice. As argued earlier, economic theories are supposed to be about
long-run behavior and there is very little that can be said about short-
run behavior. In fact the VAR approach arose from Sims' criticism that
the identification conditions imposed under the Cowles foundation ap-
proach are ad hoc.
Even if one adopts the Cowles foundation structural approach, the
JJ long-run identification conditions are still useful, because they tell us
which structural relationships, if any, can be estimated using the super-
consistency property of CI relationships (or which structural parameter
estimates converge at a faster rate). If we are primarily interested in the
long-run behavior, then it does not matter if some short-run parameters
are not identified. Thus a condition for identification of the long-run
relationship is useful.
Some other discussions of the identification problems are in Campbell
and Shiller (1988), Boswijk (1992), and Wickens (1996).
Campbell and Shiller make the important point that error correc-
tion models (those implied by cointegrating systems) do not necessarily
reflect partial adjustment. They can also arise because one variable
forecasts another. Boswijk has a detailed discussion of identification
problems taking into account the relationship between exogeneity and
cointegration. Wickens argues that if the VECM corresponds to a com-
plete structural system then the long-run structural and reduced-form
coefficients can be shown to be linear transformations of the cointegrat-
ing vectors. But as mentioned earlier, the Johansen procedure starts
with a VECM with no structural interpretation. Identifying information
is introduced much later - after determining the number of CI vectors.
5.7 Finite sample evidence
There have been many Monte Carlo studies that investigate the finite
sample performance of the estimates obtained by the different estimation
methods discussed in the previous sections. Practioners in this area are
faced with two questions:
(i) What estimation method should we use?
(ii) Given the choice of the estimation method, what significance lev-
els, p-values, etc. should we use for inference on the estimated
coefficients?

176 
Estimation of cointegrated systems
The Monte Carlo studies to be reviewed here are concerned with question
(i). Issues concerning question (ii) will be discussed in the next chapter
and in chapter 10 on bootstrap methods.
We shall first review some of the Monte Carlo studies and then outline
some points on which there is a concensus.
Banerjee, Dolado, Hendry, and Smith (1986)
This is the first Monte Carlo study that investigates the relevance for
econometric practice of asymptotic theory in this area. The main prob-
lem considered in this paper is whether the Engle-Granger methods
of estimating the cointegrating vector through a static regression gives
good results. Another problem they investigate is that of contrasting
inferences drawn from static and dynamic regressions.
The DGP they consider is: the same as that in Engle-Granger (1987)
which is as follows
%t + Vt = vu 
vt(l — p\V) = e\t
yt + 2xt = ut, 
ut{l- 
p2L) = e2t
and
" 0 W 
al 
0
0 M 0 °l
The reduced form consists of the two regressions
e2t
yt 
=
— p2L
1 - p2L 
1 - piL
If A is the coefficient of xt in a static regression of yt on xt, then
1
plim X = —1 — -u
where
Note that as p\ —> 1, plim A = — 2 and as p2 —> 1, plim A = —1. If
|pi| < 1 and |p21 < 1? both xt and yt are 7(0) variables. If p\ = 1 and
p2 = 1, both Xt and yt are 7(1) variables, but not cointegrated. If p\ = 1
and \p21 < 1, then both variables are 7(1) and y and x are cointegrated.
We have in this case plim A = — 2 .

5.7 Finite sample evidence 
111
The parameter values Banerjee et al. consider in their Monte Carlo
study are
T 
= (33,66,99,150,199)
s 
= ^1/(72 = (16,8,4,2,1,0.5)
p2 
= (0.6,0.8,0.9)
Although plim A = — 2 and A is superconsistent, these authors find that
for small T and large values of p2 and s, the biases are very large. For
example if T = 33, s = 2, p2 = 0.9, the average bias is 0.589. If T = 66,
the bias reduces to 0.468.
Banerjee et al. next consider dynamic models and compare the es-
timates of the cointegrating coefficient from static and dynamic regres-
sions. The main conclusion is that the biases in the estimates of the
long-run parameter are larger in the static model than in the dynamic
model.
These points are pursued in Banerjee et al. (1993, chapter 7) where
they plot the biases in the estimates of the long-run parameter. The
conclusion is that the biases increase with p2 and s. They conclude that
some nonparametric corrections as in Phillips and Hansen (1990) and
in Phillips and Loretan (1991), or additional dynamics as in Hendry's
ECM are necessary to correct the biases in the estimates from static
regressions. The issue then is, which of these is better? The following
studies try to answer this question.
Phillips and Hansen (1990)
The Monte Carlo study of Banerjee et al. (1986) found that super-
consistency of OLS in cointegrating regressions was misleading in small
samples. The study implies that asymptotic theory seems to provide a
poor approximation in sample sizes typical of economic data. Phillips
and Hansen (1990) argue that this is not true. Superconsistency in itself
does not give any information on the sampling distribution. The asymp-
totic distribution theory, they argue, is useful even in small samples in
choosing between different estimators and test statistics. They illustrate
this through a simulation study. They compare the OLS, the FM-OLS
and Hendry's dynamic regression (see section 5.4).
The DGP they used was the triangular system
yit 
= 
(3y2t + uit

178 
Estimation of cointegrated systems
Ult
0.3 -0.4 1 
I" 1
021 
0.6 J 
[ (721
, E)
They set j3 = 2, T = 50 and let #21 &nd 021 vary. The parameters chosen
were (0.8,0.4,0) for 02i and (-0.8,-0.4,0.4,0.8) for o2\- They report
means and standard deviations of ft — (3 and £A. All simulations used
30,000 replications. Their conclusion is that overall, the FM-OLS and
ECM methods work well and improve over the OLS. In general, OLS
is the most biased estimator. The FM-OLS method shows a small but
persistent bias and seems generally preferable to Hendry's method.
Phillips and Hansen note that the relatively inferior behavior of the
dynamic regression method may have been caused by an insufficient
number of lagged variables. Banerjee et al. (1993, pp. 248-251) pursue
this argument and show that the superiority of the FM-OLS method
is due to the particular DGP used in this study and depends on some
parameter values. In general no clear ranking emerges.
Hansen and Phillips (1990)
The DGP used in this Monte Carlo study is the same as that used by
Banerjee et al. (1986) and Engle and Granger (1987). The purpose
of the study is to explore the small sample properties of OLS and IV
estimators in cointegrating regressions. The authors consider the un-
adjusted, bias corrected, and fully modified versions of the OLS and
IV estimators in order to evaluate the asymptotic theory developed in
Phillips and Hansen (1990).
The fully modified estimators have some advantages over the unad-
justed and bias corrected estimators. Bias corrections are complicated
by the fact that they depend on preliminary coefficient estimates which
are themselves biased.
The DGP used in this exercise was decomposed to highlight the prob-
lems into three categories: signal-noise ratio, serial correlation, and en-
dogeneity. The DGP was
yt-2xt 
= uu 
(l-pL)ut 
= elt
-yt + 3xt = vu 
(1-L)vt 
= e2t
KXHi?)]

5.7 Finite sample evidence 
179
\P\ < i, 
\e\ < I
The signal-noise ratio is given by a (we shall comment on this later),
serial correlation by p and endogeneity by 6. The general conclusion of
the study was that the signal-noise ratio (<J) appeared to be the critical
factor, not the degree of long-run endogeneity. If a is very high, the bias
problem is negligible and OLS works well. In this case the fully modified
(FM) estimates will permit inference in a conventional fashion. If a is
low, the FM-OLS method does not perform well and IV methods are
necessary.
Phillips and Loretan (1991)
Phillips and Loretan report results of simulations for the following pro-
cedures:
(i) OLS,
(ii) FM-OLS,
(iii) Linear ECM or Hendry type estimator,
(iv) Their own estimator (PL) based on nonlinear regression that in-
cludes leads of
(For a discussion of these estimators, see section 5.4.2.)
The DGP they use is identical to that used in Phillips and Hansen
(1990) described earlier. They set /? = 2.0 and T = 50 and allow 02i
and cr2i vary. They consider the values (0.8,0.4,0, —0.8) for 02i and
(—0.85,-0.5,0.5) for G2\> The model has only one regressor and the
time series dynamics are generated by MA(1).
The major finding of the Phillips and Loretan simulation study is
that FM-OLS and single equation ECM estimates are both substan-
tially better than OLS. The performance of the ECM estimator is itself
substantially improved by (i) adding more lags, (ii) including leads of the
differences of the regressor variable when these are not strongly exoge-
nous, and (iii) most importantly by formulating the ECM nonlinearly
in the parameters. However, Phillips and Loretan find that there are
substantial size distortions in inferences based on ECM estimates.
Inder (1993)
Inder argues that the favorable results obtained by Phillips and Hansen
(1990) and Phillips and Loretan (1991) for the FM-OLS method are due
to the particular DGP they used in their simulation. From his Monte
Carlo study he shows that estimates which include the dynamics (like the

180
Estimation of cointegrated systems
single equation ECM) are much more reliable, even if the dynamic struc-
ture is overspecified. Also even if the ^-statistics based on the FM-OLS
method are asymptotically valid, they do not have good finite sample
properties.
Inder's DGP includes a lagged dependent variable. He argues that
in this case FM-OLS yields almost no improvement over OLS, and that
the semi-parametric correction is insufficient to remove the autocorrela-
tion in the errors when the DGP includes a lagged dependent variable.
These problems do not arise with the unrestricted ECM (Hendry type)
whose performance is uniformly better. This argument is pursued in
Inder (1995) where he argues for a linear model with leads and lags and
shows how it performs better than the nonlinear model in Phillips and
Loretan (1991). These papers argue for Hendry style dynamic regres-
sion models. In all these methods there is the question of selection of
lag length as discussed in section 5.4.2.
Entorf (1992)
This paper illustrates the finite sample problems of regression models
with 1(1) variables. In particular, it emphasizes the role of drifts.
Consider the cointegrated system as follows
=7y + et, 
et ~ wd(0, crj)
= 7* 4- vu 
vt ~ iid(0, <r%)
Then, if it is estimated by OLS, as T —> oo
and
lx
Oy \ W(r)dr - -^ax / V(r)dr
J0 
lx 
Jo
N L
where W(r) and V(r) are independent standard Wiener processes. The
last expression is derived using the result (see chapter 3)
f W(r)dr ~ N (o, 1

5.7 Finite sample evidence 
181
In the presence of drifts we can write
2/t = 2/o + lyt + et + et-i + • • • + ei
xt = x0+ 7xt + vt+ vt-i H 
\-vi
Thus it is clear that (3 = ^x/ly and since /3 can be shown to be consistent
we get the required result. Entorf shows that if a time trend is included,
that is, if the estimated equation is
yt = a + J3xt + 6 + ut
then
and (I converges to a random variable as in the case of a spurious re-
gression with no drifts.
Entorf investigates the rate of convergence of the OLS estimators to
those predicted by asymptotic theory. He takes ay = ax — 1 and (3 =
^y/^x = 2.0. He considers j x = 0.05, 0.25, 0.75 with the corresponding
values of 7^ = 0.10, 0.50, 1.50. For the smallest drift the asymptotic
results are not valid even for samples of size 170 or higher. For the
largest value of drift the asymptotic results are valid for samples as
small as 30. Thus, the magnitude of drifts plays an important role.
The behavior of the DW statistic has been commonly suggested as a
guide to ensure that a spurious regression has not been estimated. En-
torf investigates the small sample distribution of the DW statistic. He
finds that for more than five regressors the 95 percent fractile of the DW
distribution is larger than two. Thus even regressions with DW values
of about two do not necessarily ensure that we do not estimate spurious
regressions.
Gonzalo (1994)
Gonzalo considers five estimators from:
(i) Engle-Granger static regression model,
(ii) the nonlinear least squares method by Phillips and Loretan,
(iii) the Johansen procedure,
(iv) principal components method,
(v) the Box-Tiao method (see section 5.5.3) due to Bossaerts (1988)
and Bewley and Orden (1994).

182 
Estimation of cointegrated systems
He derives the asymptotic distributions for these estimators. Prom the
Monte Carlo study he shows that the Johansen procedure performs bet-
ter than the others even when the errors are not normally distributed or
when the dynamics are unknown and we overparameterize by including
additional lags in the ECM. Needless to say, these conclusions are not
in agreement with many other studies we have reviewed.
It should be noted that Gonzalo considered, in his Monte Carlo study,
the procedure in Johansen (1988) which does not have a constant term.
The method in Johansen (1991) which includes a constant is more em-
pirically relevant. Also, Gonzalo was the first to derive the asymptotic
distribution of the principal component estimator but in a limited model.
The paper by Harris (1997) does this for a more general model.
Bewley et al (1994)
These authors compare, by Monte Carlo studies the performance of the
Box-Tiao method compared to the Johansen method and conclude that
in a variety of situations, the distributions of the Box-Tiao estimator
are less dispersed than those of the Johansen estimator.
Cappuccio and Lubian (1994)
These authors argue that consistent estimation of the long-run covari-
ance matrix via VAR prewhitening results in a considerable improvement
in the performance of the FM-OLS estimator and it performs better than
the Johansen estimator. They find that even in samples of 200, the em-
pirical distribution of the Johansen estimator is affected by the Cauchy
like factor noted by Phillips (1994) when the error correction mechanism
exhibits strong serial correlation.
Eitrheim (1992)
This is an ambitious study that examines the effects on the Johnsen
estimator of several model misspecifications such as:
(i) wrong order of the VAR model,
(ii) nonnormality of the errors,
(iii) time aggregation and skip sampling,
(iv) measurement errors.
The study concludes that provided the cointegration rank r is correctly
specified, the Johansen estimator is robust to these specification errors.
The problems considered in this study are very extensive and more de-
tailed Monte Carlo studies are needed to examine each of the issues.

5.7 Finite sample evidence 
183
Hargreaves (1994)
In his Monte Carlo study with a four equation system with one coin-
tegrating vector and two cointegrating vectors, Hargreaves (1994) finds
that:
(i) The Johansen estimator had the best median bias but the worst
variation.
(ii) Suppose we estimate a single cointegrating vector when in fact
there were two cointegrating vectors. If the cointegrating vec-
tor we are estimating has a much lower variance than the other
cointegrating vector which is ignored, then one will do fairly well.
In the opposite case one will do badly.
(iii) If one is unsure of the cointegrating dimensionality and is try-
ing to estimate any one cointegrating vector, then the FM-OLS
estimator is the best.
(iv) The Johansen estimator is the best only if the model is well spec-
ified without highly autocorrelated cointegrating errors.
Phillips (1994)
The paper by Phillips is not a Monte Carlo study but it is helpful in
understanding the results of several Monte Carlo studies that report
that the Johansen estimator, although almost unbiased, has a very high
variance. Phillips derives same exact finite sample distributions and
characterizes the tail behavior of ML estimators of the cointegrating
coefficients in VECM models. He shows that the distribution of the Jo-
hansen estimator has Cauchy-like tails and hence has no finite moments
of integer order. The ML estimation in Phillips' triangular representa-
tion, by contrast, has a distribution that has matrix ^-distribution type
tails with finite integer moments of order T — n+r where T is the sample
size, n the number of variables in the system, and r is the dimension of
the cointegration space.
These results of Phillips help to explain the fact that several Monte
Carlo studies found more extreme outliers with the Johansen method
than with other asymptotically efficient methods.
Summary of the Monte Carlo studies
It can be easily seen that there are only a few points on which there
is an agreement in all these Monte Carlo studies. The unambiguous
conclusions we can draw are that estimation of the long-run parameters

184 
Estimation of cointegrated systems
by static regressions is to be avoided and that the Johansen estima-
tor (though showing less bias than the other estimators) exhibits large
variation.
Among single equation methods, where estimation of a single CI vector
is of concern, on balance a linear model with leads and lags appears to
be the best choice. Among systems methods, where we also estimate the
number of CI vectors, there are two methods available: the Johansen
method and the Box-Tiao (BT) method. The BT estimator performs
better (has less dispersion and less kurtosis) when the adjustment to
equilibrium is slow, sample size is small, and disturbances are not highly
correlated (the correlation is a measure of the endogeneity of the 1(1)
regressors in the CI regression). When these conditions are not met, the
Johansen method should be used.
There is one issue that makes it difficult to compare the results from
the different studies and from different parameter combinations in the
same study. Li and Maddala (1995) suggest that in the choice of the
parameters in the DGP for Monte Carlo studies, attention should be
paid to the signal-noise ratio (SNR). There is the question of what
should be an appropriate measure of SNR in cointegrated systems and
this is an issue that needs careful study. Earlier, we noted that Hansen
and Phillips (1990) argued that SNR was a critical factor in their Monte
Carlo study.
Li and Maddala (1995) suggest denning SNR as the ratio of the inno-
vation of 1(1) errors to the variance of the 1(0) regression errors. They
present SNRs for a few of the Monte Carlo studies discussed earlier and
show that there is a large variation in SNR between the studies. More
detailed study of the role of SNR in comparing the results of the different
Monte Carlo studies is needed.
5.8 Forecasting in cointegrated systems
In section 3.10 of chapter 3, we discussed issues related to forecast-
ing from single equations. We shall now turn to multiple equations,
the VAR system. One important contribution of cointegration tests is
in the modeling of VAR systems, whether they should be in levels or
first-differences or both, with some restrictions. For this purpose the
cointegration relationships need not have any economic interpretation.
They are of value in determining the restrictions of the VAR system
which should be of value in forecasting. We shall now discuss whether
and when the cointegration restrictions improve the forecasting.

5.8 Forecasting in cointegrated systems 
185
If a set of nonstationary variables satisfies a cointegration relation,
simple first-differencing of all the variables can lead to econometric prob-
lems. In the general VAR system with n variables, if all the variables
are stationary, using an unrestricted VAR in levels is appropriate. If
the variables are all 1(1) but no cointegration relation exists, then appli-
cation of an unrestricted VAR in first differences is appropriate. If, in
addition, there are r cointegrating relationships, then we need to model
the system as a VAR in the r stationary combinations and (n — r) dif-
ferences of the 1(1) variables.
Engle and Yoo (1987) compare the mean squared forecast errors for
cointegrated system with the forecasts generated by unrestricted VAR.
This may not be a valid comparison because it is well known that un-
restricted VARs do not give good forecasts. What we need is a com-
parison of forecasts from the cointegrated system (which is essentially a
restricted VAR) with those from other restricted VARs such as Bayesian
VARs, or alternative forms of error correction models (besides those that
result from the cointegration relationships). It is, however, interesting
to note that in the Engle and Yoo study, for forecast horizons of up to
five, the unrestricted VAR did better than the Engle-Granger two step
method applied to the ECM. It is only for longer horizon forecasts that
the latter was better. Note that the VAR in levels is not appropriate if
all the variables are 1(1).
Engle and Yoo consider a two-variable system with the autoregressive
representation
l_0.6L 
-0.8L 1 [" xt 1 _ 
... [ / 100 
0
-0.1L 
1 - 0.8L J |_ yt \ ~ 6u 
et~n 
[ >y Q 
1OQ
The corresponding ECM representation is
with a = 2. The long-run relationship is x — 2y. One hundred replica-
tions were computed in each case with 100 observations used for fitting
the model and 20 observations used for post-sample forecasting.
Given that one is not often interested in forecasting over long hori-
zons, the Engle and Yoo study actually suggests that one does not gain
much by imposing the cointegration restrictions on the VAR. Actually,
the Engle-Yoo study uses the information that there is one cointegrating
relationship. In practice we would first conduct a test whether cointe-

186 
Estimation of cointegrated systems
gration exists or not. Also one should compare the forecasts from the
VAR in first-differences (no cointegration).
Clements and Hendry (1995) do another Monte Carlo study with the
Engle-Yoo bivariate DGP. Since the conclusions in these studies are
sensitive to the particular DGP used, they try several DGPs. They also
design an efficient Monte Carlo experiment using antithetic variates and
control variates (for a discussion of these methods, see Hendry, 1984).
The sample sizes were 50 and 100 and the forecast horizons were 1, 5,
10, and 20 periods ahead. In addition to the unrestricted VAR (UV)
and the Engle-Granger two-step method (EG), Clements and Hendry
also consider the ML method (that incorporates the restrictions in the
ECM) and also VAR in differences (DV). Clements and Hendry evaluate
the different forecasting methods for two sets of variables. The first is
the original set of 1(1) variables, say xt = (#itj#2t). The other is a set
of 1(0) variables, say wt = (wu,W2t) where w\t = x\t + ax2t is the
cointegrating equation and W2t = Ax2t>
Engle and Yoo conjectured that the ML method (which imposes cross-
equation constraints) would dominate the UV and EG methods. Clements
and Hendry find that this is the case only if we consider Xt (not wt).
Engle and Yoo also conjecture that the DV method (which is clearly
misspecified in the presence of cointegration) performs worse than the
UV method. Clements and Hendry find that this is not the case if we
are forecasting Xf. The DV method is in fact almost as good as the ML,
and it outperforms the UV method at long horizons. However, when
it comes to forecasting Wt, the DV method is much worse than the UV
method.
Given that we are not usually interested in long forecast horizons, the
following conclusions emerge:
(i) The Engle-Yoo study compares UV and EG methods. For fore-
cast horizons of five or less, UV is better.
(ii) Clements and Hendry consider the UV, EG, ML, and DV meth-
ods. For shorter (n < 10) horizons and forecasting xt, the ranking
in decreasing order is ML, EG, UV, and DV. For forecast hori-
zons of ten or higher, the ranking is ML, EG, DV, and UV. For
forecasting wt, ML and UV are about the same, the EG method
is slightly worse than UV, and the DV method is substantially
worse than UV.
All this suggests that the DV method is not often appropriate, and,
except for long horizons, one might just as well use the UV method (un-

5.9 Miscellaneous other problems 
187
restricted VAR). This means that one need not impose the cointegration
restrictions on the VAR models for the purpose of forecasting.
Some other studies make the opposite argument. In a Monte Carlo
study, Ohanian (1990) reports that even wrongly imposing unit roots
produces better forecasts than models that do not impose unit roots.
In an empirical study LeSage (1990) argues that even in the absence of
cointegration, the ECM and Bayesian VAR models with error correction
terms added produce good forecasts, especially at longer horizons (eight
to twelve months ahead). Thus he argues in favor of wrongly imposing
cointegration restrictions.
Reinsel and Ahn (1992) consider forecasting in a Monte Carlo ex-
periment of a four-variate AR(2) model with two unit roots (and thus
two cointegration relationships). As a measure of forecast performance,
they use the trace of the prediction mean square error matrix esti-
mated from post-sample prediction errors. They consider predictions
with d = 0,1,2,3, and 4 imposed where d is the number of unit roots.
The extreme cases d = 0 and d = 4 gave very poor forecasting perfor-
mance. In cases d = 1 and d = 3, the forecasting performance was fairly
good (compared with the correct model d = 2) with the model d = 1
providing slightly better short-run forecasts relative to the model d = 3
which provides better long-run forecasts. Thus imposing too much coin-
tegration and too little cointegration is bad. But moderate departures
are alright.
The preceding discussion gives a survey of the different methods of
forecasting from cointegrated systems. However, none of these papers
give an overall set of guidelines to be used by time series practioners.
Such guidelines with an empirical illustration are given in Joutz, Mad-
dala, and Trost (1995). This involves checking the data for the presence
of unit roots and cointegration, estimating the implied long-run rela-
tionships by dynamic models and then specifying and testing the VAR
model.
5.9 Miscellaneous other problems
5.9.1 Models with exogenous variables
In the Johansen procedure and other procedures discussed in the ear-
lier sections, we considered closed-form models where all variables de-
pended on one another (or in standard econometric terminology, all
variables are endogenous. In fact this is one of the problems in the

188 
Estimation of cointegrated systems
different approaches to identification discussed in section 5.6). In a lot of
economic models, however, certain variables can be treated as weakly
exogenous for the estimation of long-run relationships among other
variables. The estimation of the long-run relationships can be conducted
conditional on these variables. When it is known that certain variables
are weakly exogenous, neglecting this information can result in a loss
of power. This problem has been discussed in Johansen (1992b) and
Urbain (1992, 1993). It is shown that if a given variable is weakly ex-
ogenous for the long-run parameters (i.e., the parameters a and /3 in the
notation of section 5.5.1), the cointegrating vectors of interest must not
appear in the generating model of that variable, i.e., the variable must
not be error correcting. Tests for weak exogeneity, as well as Wald, LR,
and LM tests for testing cointegration in these models are discussed in
the next chapter. Kleibergen (1995) uses these procedures for the esti-
mation of import demand models and shows how treating oil price as
weakly exogenous decreases the long-run memory of the inflation rate.
5.9.2 Cointegration and Granger causality
"When I say a word, it is supposed to mean
exactly what I want it to mean,
nothing more, nothing less."
The mad hatter in Alice in Wonderland.
Suppose that there are two time series Xt and yt. When the past and
present values of yt provide some useful information to forcast xt+i at
time £, it is said that yt Granger causes Xt. Granger causality has been
used in the context of rational expectations, definition of super exo-
geneity, and econometric modeling strategy. Many testing procedures
for Granger causality have been proposed; in practice a commonly used
testing procedure for Granger causality is testing the significance of the
coefficients of lagged yt, which are used as the explanatory variables for
Xt in the regression context.
A better term for Granger causality is precedence. But like the Mad
Hatter in Alice in Wonderland. Granger has chosen the word causality
for precedence. Although he has amply made it clear in several pa-
pers what exactly the term means, some of the literature on Granger
causality has used results from causality tests to infer causality, as it
is commonly understood. An example that drives home what exactly

5.9 Miscellaneous other problems 
189
Granger causality implies is the statement: The weatherman}s prediction
about rain (Granger) causes the rain.
Many of the studies conducted on Granger causality were conducted
in a strictly bivariate framework, even though Granger himself warned
that omission of relevant other variables could result in spurious causal-
ity (see Granger, 1969, p. 429). We fully concur with the summary
assessment by Adrian Pagan (1989) of the work on Granger causal-
ity:
There was a lot of high powered analysis of this topic, but I came away from a
reading of it with the feeling that it was one of the most unfortunate turnings
for econometrics in the last two decades, and it has probably generated more
nonsense results than anything else during that time.
Is there any relationship between Granger causality and cointegration?
The answer to this question has been discussed in Granger (1988). As
we have discussed, cointegration is concerned with long-run equilibrium.
On the other hand, Granger causality is concerned with short-run fore-
castability. These two different concepts can be considered in an error
correction model (ECM). Suppose that xt, yt are both 1(1) variables and
they are cointegrated such that zt — x± — (3yt is 1(0). When x — (3y = 0
can be considered as a long-run (steady-state) equilibrium, zt can be
interpreted as the extent to which the system is out of equilibrium. As
we have seen in section 2.8, this cointegrated system can be written in
the form of ECM as follows
Axt = 7i2t_i + lagged (Axu Ayt) + elt
Ayt = 72*t-i + lagged (Axt, Ayt) + e2t
where one of 71,72 7^ 0 and £it,£2t are finite-order moving-averages.
In the ECM, we can find that Axt or Ayt (or both) must be Granger
caused by zt-\ which is itself a function of xt-\,yt-i- 
Thus, either xt+i
is Granger caused by yt or yt+i by xt- This implies that for a pair of
series to have an attainable equilibrium, there must be some Granger
causation between them to provide the necessary dynamics. Note that if
the lagged Axt and Ayt have nonzero coefficients then there is causality
in both directions.
Of what use is this relationship between Granger causality and coin-
tegration? Granger shows that this does not change causality tests.
However, when estimating cointegrated relationships, it would be of use
in the normalization issue (discussed earlier in section 5.3).

190 
Estimation of cointegrated systems
5.9.3 Threshold cointegration
In the previous section, we assumed that if two 1(0) variables yt and Xt
are cointegrated with cointegrating coefficient /?, then the disequilibrium
error zt = yt — (3xt is stationary or mean-reverting. The literature on
cointegration and the corresponding error correction models is that this
adjustment to long-run equilibrium is continuous. However, in many
instances, due to transaction costs or institutional constraints (for in-
stance, dividend adjustments, or exchange rate behavior in the presence
of target zones) this adjustment is not continuous, but takes place only
if the equilibrium error zt reaches a certain threshold 8. For instance,
Zt could be a random walk if \zt\ < #, but zt could follow a stationary
autoregressive process if \zt\ > 0. Such models are called threshold coin-
tegration models and are analyzed in Balke and Fomby (1997) (although
no empirical illustration is provided).
Since globally the system is a cointegrated system, Balke and Fomby
suggest first estimating the cointegration relationship between yt and xt
using Engle-Granger procedure (i.e., using OLS) and then testing for
the existence of threshold behavior in the estimated equilibrium error
%t — Vt + fat- Balke and Fomby present Monte Carlo evidence to argue
that this procedure works well.
However, considering the large biases in the estimation of long-run
parameters by using static regression (as discussed in section 5.7) it
might be desirable to use a dynamic regression at the first stage to get
estimates of the cointegrating parameter ft and the equilibrium error
Zt. Thus, further modifications to the Balke-Fomby method need to be
investigated.
5.9.4 Aggregation in cointegrated systems
When we talk of aggregation, we have to distinguish between temporal
aggregation and cross-sectional aggregation. Temporal aggregation oc-
curs when a variable is generated say, over a month but is only observed
quarterly or yearly. The question is what model will temporally aggre-
gated data obey. The answer will depend on whether the variable is a
stock or flow. Cross-sectional aggregation occurs when we have a number
of micro variables and we aggregate them to get a macro variable.
Granger (1990) discusses the effects of aggregation on cointegration.
He shows that integration and cointegration are not affected by temporal
aggregation. As for cross-sectional aggregation, its effect on cointegra-

5.10 Summary and conclusions 
191
tion is complex. It is possible to have cointegration at the aggregate level
and not at the disaggregate level and it is possible to have cointegration
at the micro level but not at the macro level. There are special condi-
tions, as investigated by Gonzalo (1993) under which cointegration at
the micro level implies cointegration at the macro level and vice versa.
These conditions depend on some common factor assumptions rather
than the homogeneity of the micro units.
5.10 Summary and conclusions
This chapter discussed several single equation methods and system meth-
ods for the estimation of CI relationships. The advantage with the sys-
tem methods is that they also determine the number of CI relationships.
In practice this creates an identification problem because linear combina-
tions of CI vectors are themselves cointegrated. This necessitates some
identification conditions to be imposed so as to give meaningful economic
interpretation to the estimated cointegrated vectors. The issues in this
regard, relate to the stage of analysis at which these extra identifying
conditions are imposed.
This chapter also presents a review of several Monte Carlo studies
done on the different single equation and system methods for the esti-
mation of CI vectors. Although very few unambiguous conclusions follow
from these studies, we can say that, in general, among single equation
methods, an equation with leads and lags is to be preferred. The choice
between the two system methods (Johansen and Box-Tiao) depends on
a number of factors. In practice, only the Johansen method has been
widely used.
Of critical importance (in both the single equation and system meth-
ods) is the selection of lag length. This has not received as much atten-
tion as necessary. It is customary to choose the lag length using some
information criteria (AIC or BIC), or sequentially increasing the num-
ber of lags (as done in Inder, 1995) but a preferable approach is to use
a general to specific modeling strategy starting with a large number of
lags, as described in section 3.6.2 of an earlier chapter.
References
Ahn, S.K., and G.C. Reinsel (1988), "Nested Reduced-Rank Autore-
gressive Models for Multiple Time Series," Journal of the Amer-
ican Statistical Association, 83, 849-856.

192 
Estimation of cointegrated systems
(1990), "Estimation for Partially Non-Stationary Autoregressive
Models," Journal of the American Statistical Association, 85,
813-823.
Algoskoufis, G. and R. Smith (1991), "On Error Correction Models:
Specification, Interpretation, Estimation," Journal of Economic
Surveys, 5, 97-128.
Anderson, T.W. (1951), "Estimating Linear Restrictions on Regression
Coefficients for Multivariate Normal Distributions," Annals of
Mathematical Statistics, 22, 327-351.
Balke, N.S. and T.B. Fomby (1997), "Threshold Cointegration," Inter-
national Economic Review, 38, 627-646.
Banerjee, A., J.J. Dolado, D.F. Hendry and G.W. Smith (1986),
"Exploring Equilibrium Relationships in Econometrics Through
Static Models: Some Monte Carlo Evidence," Oxford Bulletin
of Economics and Statistics, 48, pp. 253-278.
Banerjee, A., J.J. Dolado, J.W. Galbraith, and D.F. Hendry (1993),
Cointegration, Error Correction, and the Econometric Analysis
of Non-Stationary Data, Oxford University Press, Oxford.
Bewley, R. (1979), "The Direct Estimation of the Equilibrium Re-
sponse in a Linear Dynamic Model," Economic Letters, 3, 357-
361.
Bewley, R. and D. Orden (1994), "Alterative Methods for Estimating
Long-run Responses with Applications to Australian Import De-
mand," Econometric Reviews, 13, 179-204.
Bewley, R. and M. Yang (1995), "Tests for Cointegration Based on
Canonical Correlation Analysis," Journal of American Statisti-
cal Association, 90, 990-996.
Bewley, R., D. Orden, M. Yang, and L.A. Fisher (1994), "Comparison
of Box-Tiao and Johansen Canonical Estimators of Cointegrat-
ing Vectors in VEC(l) Models," Journal of Econometrics, 64,
3-27.
Bossaerts, P. (1988), "Common Nonstationary Components of Asset
Prices," Journal of Economic Dynamics and Control, 12, 347-
364.
Boswijk, H.P. (1992), Cointegration, Identification and Exogeneity,
Tinbergen Institute Thesis Publishers, Amsterdam.
Box, G.E.P. and G.C. Tiao (1977), "A Canonical Analysis of Multiple
Time Series," Biometrika, 64, 355-365.
Campbell, J.Y. and P. Perron (1991), "Pitfalls and Opportunities:
What Macroeconomists Should Know about Unit Roots," in

References 
193
O.J. Blanchard and S. Fisher (eds.), NBER Macroeconomics
Annual, The MIT Press, 141-201.
Campbell, J.Y. and R. Shiller (1988), "Interpreting Cointegrated Mod-
els," Working paper No. 2568, NBER.
Cappuccio, N. and D. Lubian (1994), "Estimating Long-Run Equilib-
ria in Finite Samples," manuscript, University of Padova, Italy
(revision of paper presented at Econometric Society Meetings,
Uppsala, Sweden, 1993).
Clements, M.P. and D.F. Hendry (1995), "Forecasting in Cointegrated
Systems," Journal of Applied Econometrics, 10, 127-146.
Eitrheim, O. (1992), "Inference in Small Cointegrated Systems: Some
Monte Carlo Results," Presented at the Econometric Society
Meeting in Brussels.
Elliott, G. (1995), "On the Robustness of Cointegration Methods When
Regressors Almost Have Unit Roots," Discussion paper 95-18,
UCSD.
Elliott, G. and J.H. Stock (1994), "Inference in Time Series Regres-
sion When the order of Integration of a Regression is Unkown,"
Econometric Theory, 10, 672-700.
Engle, R.F. and C.W.J. Granger (1987), "Cointegration and error cor-
rection: representations, estimation and testing," Econometrica,
55, 252-276.
Engle, R.F. and B.S. Yoo (1987), "Forecasting and Testing in Co-
integrated Systems," Journal of Econometrics, 35, 143-159.
(1991), "Cointegrated Economic Time Series: An Overview with
New Results," in R.F. Engle and C.W.J. Granger (eds.), Long
Run Economic Relationships: Reading in Cointegration, Oxford
University Press, Oxford, chapter 12.
Entorf, H. (1992), "Random Walks with Drift, Simultaneous Equation
Errors, and Small Samples: Simulating the Birds-Eye View,"
INSEE Discussion paper No. 9222.
Ericsson, N.R., J. Campos, and H.A. Tran (1990), "PC GIVE
and David Hendry's Econometric Methodology," Revista de
Econometria, 10, 7-47.
Escribano, A. and D. Pena (1994), "Cointegration and Common Fac-
tors," Journal of Time Series Analysis, 15, 577-586.
Gonzalo, J. (1993), "Cointegration and Aggregation," Ricerche Eco-
nomiche, 47, 281-291.
(1994), "Five Alternative Methods of Estimating Long Run Equilib-
rium Relationships," Journal of Econometrics, 60, 203-233

194 
Estimation of cointegrated systems
Granger, C.W.J. (1969), "Investigating Causal Relations by Econo-
metric Models and Cross-Spectral Methods," Econometrica, 37,
424-438.
(1988), "Some Recent Developments in a Concept of Causality,"
Journal of Econometrics, 39, 199-211.
(1990), "Aggregation of Time Series Variables: A Survey," in T.
Barker and M.W. Pesaran (eds.), Disaggregation in Economic
modeling, Ratlege, New York.
Hansen, B.E. and P.C.B. Phillips (1990), "Estimation and Inference in
Models of Cointegration: A Simulation Study," in T.B. Fomby
and G.F. Rhodes (eds.), Advances in Econometrics: Cointegra-
tion, Unit Roots and Spurious Regression, vol. 8, JAI Press.
Hargreaves, C. (1994), "A Review of Methods of Estimating Coin-
tegrating Relationships," in C. Hargreaves (ed.), Nonstation-
ary Time Series Analysis and Cointegration, Oxford University
Press, Oxford, chapter 4.
Harris, D. (1997), "Principal Components Analysis of Cointegrated
Time Series," Econometric Theory, 13, 529-557.
Harris, R. (1995), Using Cointegration Analysis in Econometric mod-
eling, Prentice Hall, Harvester Wheatsheaf, England.
Hendry, D.F. (1984), "Monte Carlo Experimentation in Econometrics,"
in Z. Griliches and M.D. Intrilligator (eds.), Handbook of Econo-
metrics, vol. II, Elsevier, Amsterdam.
(1986), "Econometric modeling with Cointegrated Variables: An
Overview," Oxford Bulletin of Economics and Statistics, 48,
201-239.
Hsiao, C. (1997), "Cointegration and Dynamic Simultaneous Equations
Model," Econometrica, 65, 647-670.
Huang, B.N. and C.W. Yang (1996), "Long-run Purchasing Power Par-
ity Revisited: A Monte Carlo Simulation," Applied Economics,
28, 967-974.
Inder, B. (1993), "Estimating Long-Run Relationships in Economics,"
Journal of Econometrics, 57, 53-68.
(1995), "Finite Sample Arguments for Appropriate Estimation of
Cointegrating Vectors," Monash University.
Johansen, S. (1988), "Statistical Analysis of Cointegration Vectors,"
Journal of Economic Dynamics and Control, 12, 231-254.
(1990), 
"Maximum Likelihood Estimation and Inference 
on
Cointegration-with Applications to the Demand for Money,"
Oxford Bulletin of Economics and Statistics, 52, 169-210.

References 
195
(1991), "Estimation and Hypothesis Testing of Cointegration Vectors
in Gaussian Vector Autoregressive Models," Econometrica, 59,
1551-1580.
(1992a), "Determination of Cointegration Rank in the Presence of a
Linear Trend," Oxford Bulletin of Economics and Statistics, 54,
383-397.
(1992b), "Cointegration in Partial Systems and the Efficiency of Sin-
gle Equation Analysis," Journal of Econometrics, 50, 389-402.
(1994), "The Role of the Constant and Linear Terms in Cointegration
Analysis of Non-Stationary Variables," Econometric Reviews,
13, 205-229.
(1995), Likelihood Based Inference on Cointegration in the Vector
Autoregressive Model, Oxford university press, Oxford.
Johansen, S. and K. Juselius (1990), "Maximum Likelihood Estima-
tion and Inference on Cointegration-with Applications to the
Demand for Money," Oxford Bulletin of Economics and Statis-
tics, 52, 169-210.
(1994), "Identification of the Long-run and Short-run Structure: an
Application to the IS-LM Model," Journal of Econometrics, 63,
7-37.
Joutz, F., G.S. Maddala, and R.P. Trost (1995), "An Integrated
Bayesian Vector Autoregression and Error Correction Model
for Forecasting Electricity Consumption and Prices," Journal
of Forecasting, 14, 287-310.
Kleibergen, F. (1995), "The Influence of Exogenous Variables on Long-
Run Parameters: An Application to Price Level and Import
Demand Models," Report TI 95-44, Tinbergen Institute, Rot-
terdam.
LeSage, J.P. (1990), "A Comparison of the Forecasting Ability of ECM
and VAR Models," The Review of Economics and Statistics, 72,
664-671.
Li, H. and G.S. Maddala (1995), "Estimating Long-run Relationship in
Economics: Some Comments and Further Results," Manuscript,
Ohio State University.
Ng, S. and P. Perron (1997), "Estimation and Inference in Nearly Un-
balanced Nearly Cointegrated Systems," Journal of Economet-
rics, 79, 53-81.
Ogaki, M. and J. Park (1997), "A Cointegration Approach to Estimat-
ing Preference Parameters," Journal of Econometrics, Forth-
coming.

196 
Estimation of cointegrated systems
Ohanian, L.E. (1990), "Evaluating Trends in Aggregate Time Series:
A Monte Carlo Based Forecasting Approach," in T.B. Fomby
and G.F. Rhodes (eds.), Advances in Econometrics Series: Co-
integration, Spurious Regression, and Unit Roots, vol. 8, JAI
Press, Greenwich, 323-342.
Pagan, A.R. (1989), "20 Years after: Econometrics 1966-1986," in B.
Cornet and H. Tulkens (eds.), Contributions to Operations Re-
search and Econometrics, The MIT Press, Cambridge, MA.
Park, J.Y. and P.C.B. Phillips (1988), "Statistical Inference in Regres-
sions with Integrated Processes: Part I," Econometric Theory,
4, 468-497.
Perron, P. and J.Y. Campbell (1993), "A Note on Johansen's Cointe-
gration Procedure When Trends Are Present," Empirical Eco-
nomics, 18, 777-789.
Phillips, P.C.B. (1991), "Optimal Inference in Cointegrated Systems,"
Econometrica, 59, 283-306.
(1994), "Some Exact Distribution Theory for Maximum Likelihood
Estimators of Cointegrating Coefficients in Error Correction
Models," Econometrica, 62, 73-93.
Phillips, P.C.B. and B.E. Hansen (1990) "Statistical Inference in In-
strumental Variables Regression with 1(1) Processes," Review
of Economic Studies, 57, 99-125.
Phillips, P.C.B. and M. Loretan (1991), "Estimating Long-Run Eco-
nomic Equilibria," Review of Economic Studies, 58, 407-436.
Reinsel, G.C. and S.K. Ahn (1992), "Vector Autoregressive Models
with Unit Roots and Reduced Rank Structure: Estimation,
Likelihood Ratio Test, and Forecasting," Journal of Time Series
Analysis, 13, 353-375.
Saikkonen, P. (1991), "Asymptotically efficient estimation of cointe-
grating regressions," Econometric Theory, 7, 1-21.
Sargan, J.D. and A. Bhargava (1983), "Testing Residuals from Least
Squares Regression for Being Generated by the Gaussian Ran-
dom Walk," Econometrica, 51, 153-174.
Stock, J.H. (1987), "Asymptotic Properties of Least Squares Estima-
tors of Cointegrating Vectors," Econometrica, 55, 1035-1056.
Stock, J.H. and M.W. Watson (1988), "Testing for Common Trends,"
Journal of the American Statistical Association, 83, 1097-1107.
(1993), "A Simple Estimator of Cointegrating Vectors in Higher Or-
der Integrated Systems," Econometrica, 61, 783-820.

References 
197
Urbain, J.D. (1992), "On Weak Exogeneity in Error Correction Mod-
els," Oxford Bulletin of Economics and Statistics, 54, 187-208.
(1993), "Exogeneity in Error Correction Models," Lecture Notes in
Economics and Mathematical Systems, Springer Verlag, Berlin.
Watson, M.W. (1994), "Vector Autoregressions and Cointegration," in
R.F. Engle and D.L. McFadden (eds.), Handbook of Economet-
rics, vol. IV, Elsevier, Amsterdam.
Wickens, M.R. (1996), "Interpreting Cointegrating Vectors and Com-
mon Stochastic Trends," Journal of Econometrics, 74, 255-271.
Wickens, M.R. and T.S. Breusch (1988), "Dynamic Specification, the
Long-Run and the Estimation of Transformed Regression Mod-
els," Economic Journal, 98, 189-205.
Xiao, Z. and P.C.B. Phillips (1996), "Efficient De-trending in Cointe-
grating Regression," Manuscript, Cowles foundation, Yale Uni-
versity.

6
Tests for cointegration
6.1 Introduction
In the previous chapter we discussed methods of estimation in cointe-
gration systems. In this chapter we shall discuss tests for cointegration.
Corresponding to the single equation and multiple equation methods of
estimation considered in the previous chapter, we have tests for cointe-
gration in single equation and system frameworks. Also, corresponding
to the tests for unit roots discussed in chapter 4 where we considered the
case of unit root as null and stationarity (no unit root) as null, we have
tests for cointegration with no cointegration as null and cointegration as
null.
There is one additional factor to be considered with regard to tests for
cointegration in systems of multiple equations. Here we are interested
in finding how many cointegrating relationships there are among the
variables. In other words, we are interested in testing hypotheses about
the rank of the cointegration space. In the following sections we shall
start with tests in a single equation framework and then consider system
methods.
6.2 Single equation methods: residual-based tests
The residual-based tests were the earlier tests for cointegration and were
discussed in Engle and Granger (1987). Consider the set of (k + 1)
variables yt which are 1(1). If there exists a vector 8 such that 6'yt is
1(0), then 8 is the cointegrating vector. Since 6 is determined only up to a
multiplicative constant, we shall normalize the first variable in yt to have
coefficient 1. Thus, if we write 6 as (1, —(3f)f and partition yt conformably
into (t/it,2/2t)> then y\t — /3fy2t is the cointegrating relationship. The
198

6.2 Single equation methods: residual-based tests 
199
residual-based tests consider the equation
V\t = P'y2t + ut 
(6.1)
If Ut has a unit root, then y\t — fi'y2t is not a cointegrating relationship.
Thus, a test for a unit root in Ut is a test that the variables yt are not
cointegrated. In practice (3 is not known and (6.1) is estimated, say by
OLS, and unit root tests are applied to the estimated residual ut.
It is common to apply the ADF test and Phillips-Perron Za and Zt
tests to ut. In practice any of the tests discussed in chapters 3 and 4 can
be applied. However, the critical values are not the same because we are
applying the tests to Ut, not Ut. The critical values will depend on
(i) the number of regressors in 2/2t hi equation (6.1) and
(ii) whether a constant and/or a time trend is included in (6.1).
6.2.1 Critical values for residual-based tests
Engle and Yoo (1987) and Phillips and Ouliaris (1990) have tabulated
critical values for the ADF t-statistic. (The critical values for the Zt are
the same as the ADF t-statistic.) Phillips and Ouliaris also tabulated
critical values for Za. Since the asymptotic distributions differ according
to different trend variables in cointegrating regression, the critical values
consist of three parts corresponding to the different trend variables in
the cointegrating regressions as follows:
(a) 
ylt 
= f3fy2t + ut
(b) 
ylt 
= a + (3'y2t + ut
(c) 
ylt 
= a + 6t
Tables 6.1 and 6.2 report part of tables I and II in Phillips and Ouliaris
(1990), omitting minus signs for simplicity.
Critical values in tables 6.1 and 6.2 are based on asymptotic distribu-
tions. For small sample sizes (and for all the sample sizes) MacKinnon
(1991) provides an approximation formula for computing critical values
for all sample sizes T. MacKinnon estimated response surface regres-
sions which approximate the critical values remarkably well. He found
a functional form for the response surface regressions after considerable
experiments as follows
Cfc(p, Tk) = (3oo+ PiT^1 + (32T~2 + ek
where Ck(p, Tk) is the critical values for a test at the p percent level with

200 
Tests for cointegration
Table 6.1. Critical values for the ADF t-statistic and Zt
N
i—i
2
3
4
5
Regression
1%
3.39
3.84
4.30
4.67
4.99
5%
2.76
3.27
3.74
4.13
4.40
(a)
10%
2.45
2.99
3.44
3.81
4.14
Regression
1%
3.96
4.31
4.73
5.07
5.28
5%
3.37
3.77
4.11
4.45
4.71
(b)
10%
3.07
3.45
3.83
4.16
4.43
Regression
1%
4.36
4.65
5.04
5.36
5.58
5%
3.8
4.16
4.49
4.74
5.03
(c)
10%
3.52
3.84
4.20
4.46
4.73
Source: Phillips and Ouliaris (1990, p. 190).
Table 6.2. Critical values for the Za
N
1
2
3
4
5
Regression
1%
22.83
29.27
36.16
42.87
48.52
5%
15.64
21.48
27.85
33.48
38.09
(a)
10%
12.54
18.18
23.92
28.85
33.80
Regression
1%
28.32
34.17
41.13
47.51
52.17
5%
20.49
26.09
32.06
37.15
41.94
(b)
10%
17.04
22.19
27.58
32.74
37.01
Regression
1%
35.42
40.34
47.36
53.61
58.16
5%
27.09
32.22
37.73
42.46
47.38
(c)
10%
23.19
27.78
33.16
37.74
42.32
Source: Phillips and Ouliaris (1990, pp. 189-190).
the sample size Tk and /3s are parameters to be estimated. The response
surface regressions were estimated by feasible GLS. Table 6.3 shows the
GLS estimates of (3s for three cases (a), (b), and (c). The estimates
of /?oo provide asymptotic critical values directly, while values for any
finite T can be computed by using the estimates of all three parameters.
Note that k denotes the number of variables included in a cointegrating
regression, while N in tables 6.1 and 6.2 denotes the number of regressors
included in a cointegrating regression. Thus, k = N + 1. The estimates
for k = 1 provide the critical values of the DF tests for a unit root. For
example, at p = 5 percent level the critical values can be approximated
by
C(T) = -3.4126 - 4.039T-1 - 17.83T"2
If T -+ oo, C = -3.4126. If T = 100, C = -3.4547. Similarly C =
—3.5005 when T = 50. These approximated critical values are remark-
ably close to the corresponding critical values in table 3.1 of chapter 3

6.2 Single equation methods: residual-based tests
201
Table 6.3. Response surface estimates of critical values
k
1
2
3
4
5
6
Regression
(a)
(b)
(c)
(b)
(c)
(b)
(c)
(b)
(c)
(b)
(c)
(b)
(c)
Size
1%
5%
10%
1%
5%
10%
1%
5%
10%
1%
5%
10%
1%
5%
10%
1%
5%
10%
1%
5%
10%
1%
5%
10%
1%
5%
10%
1%
5%
10%
1%
5%
10%
1%
5%
10%
1%
5%
10%
-2.5658
-1.9393
-1.6156
-3.4336
-2.8621
-2.5671
-3.9638
-3.4126
-3.1279
-3.9001
-3.3377
-3.0462
-4.3266
-3.7809
-3.4959
-4.2981
-3.7429
-3.4518
-4.6676
-4.1193
-3.8344
-4.6493
-4.1000
-3.8110
-4.9695
-4.4294
-4.1474
-4.9587
-4.4185
-4.1327
-5.2497
-4.7154
-4.4345
-5.2400
-4.7048
-4.4242
-5.5127
-4.9767
-4.6999
ft
-1.960
-0.398
-0.181
-5.999
-2.738
-1.438
-8.353
-4.039
-2.418
-10.534
-5.967
-4.069
-15.531
-9.421
-7.203
-13.790
-8.352
-6.241
-18.492
-12.024
-9.188
-17.188
-10.745
-8.317
-22.504
-14.501
-11.165
-22.140
-13.641
-10.638
-26.606
-17.432
-13.654
-26.278
-17.120
-13.347
-30.735
-20.883
-16.445
ft
-10.04
0.0
0.0
-29.25
-8.36
-4.48
-47.44
-17.83
-7.58
-30.03
-8.98
-5.73
-34.03
-15.06
-4.01
-46.37
-13.41
-2.79
-49.35
-13.13
-4.85
-59.20
-21.57
-5.19
-50.22
-19.54
-9.88
-37.29
-21.16
-5.48
-49.56
-16.50
-5.77
-41.65
-11.17
0.0
-52.50
-9.05
0.0
Source: MacKinnon (1991).

202 
Tests for cointegration
(which is computed by a Monte Carlo method), —3.41, —3.45, and —3.50,
respectively.
The critical values of the ADF tests for cointegration can also be
computed by the response surface estimates. For example, the regression
(c) with k = 2 and T = 500, the critical value at the 5 percent level is
approximated by C = —3.799 which is very close to the corresponding
critical value of table 6.1 (N=l at the 5 percent level), —3.8.
By using these critical values, one can reject the null hypothesis of no
cointegration if the computed value of the statistic is smaller than the
appropriate critical value. For example, for a regression with a constant
term and two explanatory variables (N = 2), we reject the null of no
cointegration at the 5 percent level if the computed value of ADF t-
statistic and Zt is less than —3.77 or the computed value of Za is less
than -26.09.
MacKinnon (1994) expands the response surface methodology to cal-
culate both asymptotic and finite sample critical values and p-values for
some commonly used unit root and cointegration tests. (The method-
ology can be extended to Johansen's tests and ECM tests discussed in
later sections.) The programs are available on internet. MacKinnon says
that
interested readers should ftp to qed.econ.queensu.ca and then go to the
directory pub/uroot. The directory contains the source codes for urcdist,
a compiled version for DOS-based PCs with at least 4MB memory and a
numeric coprocessor and 12 zipped files containing the estimated response
surface coefficients. The urcdist program is run interactively and prompts
the user for input.
6.2.2 Other residual-based tests
Besides these three tests, various residual-based tests have been pro-
posed. Engle and Granger (1987) suggested to use the Durbin-Watson
statistic from the cointegrating regression. However, Campbell and Per-
ron (1991) argued that it should not be used as the basis of a test of the
null hypothesis of no cointegration versus the alternative hypothesis of
cointegration (Campbell and Perron, 1991, rule 19, p. 177). The rea-
son is that under the null hypothesis of no cointegration, the asymptotic
distribution of the Durbin-Watson statistic depends on nuisance param-
eters such as the correlations among the first-differences of the variables
included in the regression.
Hansen (1990) suggested working with the estimated residuals from a

6.3 Single equation methods: ECM tests 
203
Cochrane-Orcutt (C-O) version of the static regression that allows for
AR(1) errors
L + et
He showed that under the null hypothesis of no cointegration, the C-0
estimate of the cointegrating vector converges to a constant, not to a ran-
dom variable as does the OLS estimator, and also has the usual Dickey-
Fuller distribution. Hansen reported some Monte Carlo comparisons
which indicate that C-0 tests are uniformly more powerful than Stock-
Watson, Johansen, and Phillips-Ouliaris tests. He also shows that the
C-0 tests can be extended to allow for more general serial correlation
in the residual Ut.
6.3 Single equation methods: ECM tests
The residual-based tests discussed in the previous section have been
found to have low power, although they are very often used. See Kremers
et al. (1992), Zivot (1994), and Banerjee (1995). The problem is with
the estimation of the static regression (6.1) in the first step. Kremers et
al. argue that the low power of the residual-based tests is due to ignoring
equation dynamics in (6.1) and concentrating on error dynamics (which
they call imposing a common factor restriction). This problem is now
well known in the estimation of dynamic models. This problem in its
most elementary version is as follows: Consider the two models:
Model 1 : yt = (3xt + uu 
v>t = put-\ + et
Model 2 : yt = aiyt-i 4- ot2xt + a^xt-\ + et
Model 1 has no equation dynamics but has error dynamics. Model 2
is one with equation dynamics but no error dynamics. But model 1
is the same as model 2 with the restriction a\a2 + a3 = 0. If this re-
striction (known as common factor restriction) is violated, then model
1 is invalid and the observed error dynamics is spurious - it is a conse-
quence of ignoring equation dynamics. This argument also carries over
to cointegrating systems.
To avoid this problem Kremers et al. suggest using ECM tests in-
stead of the residual-based tests. The ECM tests are based on the error
correction model implied by the cointegrated system. The Granger rep-
resentation theorem says that there is an error correction representation
for every cointegration relationship. For simplicity, let us assume that

204 
Tests for cointegration
y2t in equation (6.1) is of dimension 1 and hence (3 is a scalar. The ECM
representation is then
i) + lagged(Ayit, A^t) 4- £u
i) + lagged(Ayu, Ay2t) 4- e2t (6.2)
with OL\ and/or a2 nonzero. If a\ = a2 = 0, then we have no cointe-
gration. One can apply tests for cointegration without estimating /?, as
follows: write (6.2) as
-l + V2,t-i) + lagged(Ayit, Ay2t) + jiy2j-i 
+ eu
+ 2/2,t-i) + lagged(A2/it, Ay2t) + j2y2,t-i 
+ e2t (6.3)
where 71 = —a\(j3 + 1) and 72 = —a2(f3 + 1).
Kremers et al. consider a conditional ECM for yu and a marginal
unit root precess for y2t. This is also the system adopted in Banerjee et
al. (1996). In our notation the DGP they consider is
As/it = -y'Ay2t + «i(yi,t-i - P'y2,t-i) + ^lt
Ay2t = u2t 
(6.4)
where
(
Wit \ 
•• \( 
0 \ 
/ 
0i
I ^^ zza I 
I , I
The F-test for a\ = 0 is a test for the null of no cointegration. Kre-
mers et al. actually consider a case where the cointegrating vector (3
is known. Their purpose, however, is to demonstrate how the residual-
based tests are based on the common factor restriction and ignore equa-
tion dynamics, and how the ECM-based tests are more powerful than
the residual-based tests if the common factor restriction does not hold.
The paper by Kremers et al. is based on the restrictive assumption of
a known cointegrating vector. This is relaxed in the paper by Banerjee
et al. (1996). They write equation (6.4) as
Ayu = 7'Ay2t + &iyi,t-i + 8'y2,t-i + ^it 
(6.5)
where 0 = —aij3. The noncointegrating restriction a\ = 0 implies 6 = 0.
They suggest estimating equation (6.5) by OLS and testing that the
coefficient of yij-i is equal to zero. The ECM test is thus based on
&i or its t-ratio. Banerjee et al. derive the asymptotic distributions of
T&i and the ^-statistic based on &i, and suggest using the t-test. They
also tabulate critical values for models with a constant and with a time

6.4 Tests with cointegration as null 
205
trend. However, note that a\ = 0 also implies 6 = 0 in (6.5) and this is
not taken into account in the test by Banerjee et al. (1996).
The ECM-based test derived in Banerjee et al. is based on the as-
sumption that 7/2t are strictly exogenous (note the specification of the
covariance matrix for uu and U2t)- Other tests based on exogeneity
assumptions are discussed in Boswijk (1992) and Boswijk and Pranses
(1992). In spite of the fact that the residual-based tests have low power,
the ECM tests have not been found to be used in many applications.
This is because of the availability of system based tests (which are dis-
cussed in section 6.5 later) and also the nonavailability of ready tables
of critical values for use with the ECM tests.
Zivot (1994) considers a more general framework that relaxes the re-
strictive assumptions in Kremers et al. and Banerjee et al. For an
unknown cointegrating vector he suggests a two-step ECM test, which
is implemented as follows: using the static regression (6.1) get an esti-
mator (3 for p. This estimator, as is well-known, is superconsistent. Next
substitute /? for /? in equation (6.4), and apply the ECM test by testing
the significance of 6L\ using a standard t-test.
Zivot considers another ECM test which is a one-step test and is
similar to the ECM test considered in Boswijk (1992, 1994). Write (6.4)
as
Aylt = 7fAy2t + H'yt-i + ult
where II' = 7(1,—/?') and y't = {y\t,y2t)- The no cointegration restric-
tion 7 = 0 implies II = 0. Now estimate this equation by OLS and test
the significance of ft using a standard Wald test. Zivot performs Monte
Carlo experiments comparing the power of these ECM tests with the
power of the Engle-Granger residual-based ADF t-test, and finds them
more powerful.
6.4 Tests with cointegration as null
The tests considered in the previous sections are for the null hypothesis
of no cointegration. These are based on tests for a unit root hypothesis
in the residuals of the cointegrating regression. In chapter 4 we discussed
unit root tests with stationarity as the null hypothesis. Correspondingly
there are tests with cointegration as the null, although they have not
been widely used, nor does anyone talk of using them for confirmatory
analysis (as we discussed in chapter 4). We shall consider four tests here:
(i) the Leybourne and McCabe test which is based on an unobserved

206 
Tests for cointegration
components model, (ii) the Park and Choi test which is based on testing
the significance of superfluous regressors, (iii) the Shin test which is a
residual-based test, and (iv) Harris and Inder test.
We shall review these tests here. There is as yet no study comparing
the power of these tests. So no practical conclusions can be offered as to
their relative merits. Nor is there any study investigating the power of
these tests. So we do not even know whether their power properties are
poor as for the unit root tests discussed in chapter 4. Harris and Inder
(1994) compare the power of their test with that of the DF but note that
the power comparison is between that for two tests which have their null
and alternative reversed (and thus invalid). They do not compare the
power of their test with that of the other tests for cointegration as null,
which is the valid comparison. Nor has anyone made a study (as done
by Burke quoted in chapter 4) on using these tests in conjunction with
tests for no cointegration, for confirmatory analysis.
6.4.I Leybourne and McCabe (1993)
Leybourne and McCabe (1993) proposed the test for the null hypothesis
of cointegration against the alternative hypothesis of no cointegration.
They consider the cointegrating regression
Vit = PfV2t + ut
where the error ut can be written
ut 
= at + eu 
£t~ IN(0, a2)
at = at-i + rju 
rjt ~ 7JV(0, a2)
Then the null hypothesis of cointegration is
and the alternative hypothesis of no cointegration is
ffi : <J% > 0
Leybourne and McCabe suggest the (conditionally) locally best in-
variant (LBI) test of HQ against Hi based on the statistic
where e is the vector of OLS residuals from the cointegrating regres-
sion under if0, V is a T x T matrix with its ijth element equal to the

6.4 Tests with cointegration as null 
207
minimum of i and j(i,j = 1,..., T). For the estimate of the variance,
of, they proposed to use the Newey-West type estimator considered
in Phillips and Perron (1988) (see chapter 3). They derived the asymp-
totic distribution of the LBI test statistics, which is a function of Wiener
processes and depends only on the number of variables in y2t and a2
e.
Leybourne and McCabe performed a small Monte Carlo experiment
which showed that the test has approximately the correct sizes under
HQ for both samples of T=100 and 200. However, comparisons of the
relative robustness or power of the LBI test with other tests for the null
of no cointegration, are not attempted because of the differing natures
of the null and alternative hypotheses.
6.4-2 Tests using superfluous regressors
Park and Choi (1988) and Park (1990) propose tests for cointegration
which have the advantage that they can be formulated either with a null
of no cointegration or with a null of cointegration. Their basic idea is
to introduce superfluous regressors into the cointegrating regression and
test for the coefficients of the added superfluous regressors.
Consider the following version of the cointegrating regression with
added regressors
Vit = 0V2t + 7i«it + 72*2* + et 
(6.6)
The added variable su is a vector of q nonstationary deterministic func-
tions that are of a higher order than the variables in the cointegrating
regression. For example, if 1 and t are included in the cointegrating
regression, su could include the regressors {t2, £3,..., tq+1}. The vector
S2t is a p-element vector containing variables that are integrated of order
one.
Testing procedures are based on the comparison between the residual
sum of squares (RSS) from the original cointegrating regression and
the RSS from the cointegrating regression with superfluous regressors
su and S2t- Park (1990) proposed the J\ test statistic for the null of
cointegration
— RSS2
w
where RSSi is the RSS from the regression after nonparametric trans-
formation for independence of nuisance parameters and RSS2 is the
RSS from the transformed regression with superfluous regressors. The

208 
Tests for cointegration
denominator is the normalizing variance corresponding to the test statis-
tics. Park shows that under the null hypothesis of cointegration, the J\
statistic has limiting %2 distribution with the degree of freedom equal to
the number of the superfluous regressors. But under the hypothesis of
no cointegration, it diverges to infinity. Thus when J\ is large, the test
rejects the null hypothesis of cointegration. Asymptotic critical values
can be found in his paper.
For a choice of the superfluous regressors, st, it is required that the
stochastic components of the regressors {st} and {y2t} be not cointe-
grated. Park suggests that the most irrelevant time trend would be the
best choice. Likewise, any 1(1) process which is completely independent
of the processes in the model, as for instance, a computer-generated
pseudo random walk would serve well. Park also illustrated the case
where the lagged dependent variable is added as a regressor.
6.4*3 Residual-based test
Shin (1994) extends the KPSS test which tests the null of stationarity
against the alternative of unit root (see chapter 4) to a multivariate
context of testing the null of cointegration against the alternative of no
cointegration. He extensively uses the parameteric correction procedure
of estimation of cointegrating regression (see section 5.4.2 of chapter 5
for details). When the elements of the error vector in the cointegrating
regression are contemporaneously correlated, which causes the endogene-
ity problem in single equation estimation methods, as we have seen in
section 5.4.2, the effect of this correlations can be eliminated by adding
Ay2t to the regressor set. The parametrically corrected cointegrating
regression is
yit = P'y2t+ Yl bjAy2,t-j+et 
(6.7)
(For reader's convenience, this equation is duplicated from the equation
(5.4) in section 5.4.2.)
Based on the residual it from this equation, Shin proposed the follow-
ing test statistic for the null of cointegration
where St = Y2i=i &A an<^ &e a r e semiparametric consistent estimators of
the long-run variance of et (see chapter 3). The asymptotic distribution

6.4 Tests with cointegration as null
of the test statistic C is shown to be
r1
c^ 
/ g2
Jo
where
209
The standard Brownian motions W\ and W% are independent and cor-
respond to scalar variable yu and m-vector variable y<it, respectively.
The limiting distributions are affected by the inclusion of deterministic
components in the cointegrating equation. The asymptotics carry in a
similar fashion with the only modification being that the standard Brow-
nian motions are substituted by demeaned and detrended Brownian mo-
tions (see chapter 3 and also Park and Phillips, 1988). The asymptotic
distributions depend on ra, the dimension of the cointegrated system.
Critical values are given for m=l to 5 in Shin (1994, pp. 100-101).
6.4>4 Harris and Inder test
Harris and Inder (1994) also suggest a test for the null of cointegration.
They use nonparameteric correction procedure for estimation of cointe-
grating regression, while Shin uses the parametric correction procedure.
But both the tests are the same in the sense that they use the same test
statistics, the LM test statistic. Their procedure is as follows: consider
the problem whether the 1(1) variables yt and xt are cointegrated. First,
apply OLS to the regression
Vt = Mo +
+ ut
and obtain residual Ut. Form it = [ut, Axf
t]' and calculate the estimated
covariance matrices
1
f
(zt-k2't + ztz't-
L*=i
and
t=k+l
I 
T

210 
Tests for cointegration
Table 6.4. Critical values for the Harris and Inder test
N
1
2
3
4
5
10%
0.2335
0.1617
0.1203
0.0929
0.0764
5%
0.3202
0.2177
0.1590
0.1204
0.0972
Source: Harris and Inder (1994).
where w(k, 1) = 1 — k/(l + 1). Then calculate
yt = yt -
and
and re-estimate the cointegrating regression using the fully modified
estimator
where e^ = [0, Ik}'- Obtain the residuals uf from this regression and
construct the test statistic
where
2 = 1
and
A 2 
^ 
/v 
A — 1 ~
^1-2 — ^11 ~~ ^12^22 ^21
The critical values for this test, which depend only on the number of
regressors n (excluding the constant) provided by Harris and Inder are
given in table 6.4.
In practice it might be desirable to obtain the critical values by Monte
Carlo methods (since the critical values of table 6.4 are based on the
asymptotic distrbution).

6.5 Multiple equation methods 
211
6.5 Multiple equation methods
The results from single equation methods depend on what variable is
used for normalization of the cointegration relationship (see section 5.5.3).
Also the single equation methods do not enable us to test how many coin-
tegration relationships there are. For this reason it is common practice
to use system methods. The most popular of these system methods is
the Johansen method which is based on canonical correlation methods.
(We shall discuss limitations of this method in the next section.) The
modification of the Box-Tiao procedure by Bewley and Orden discussed
in chapter 5 (section 5.5.3) is also based on canonical correlations. We
shall discuss the relative merits of the two procedures in the next sec-
tion, after discussing two methods based on principal components and
other methods.
6.5.1 Methods based on canonical correlations:
Johansen Tests
The Johansen procedure has been discussed in chapter 5. The procedure
leads to two test statistics for cointegration. The first, called the trace
test, tests the hypothesis that there are at most r cointegrating vectors.
The second called the maximum eigenvalue test, tests the hypothesis
that there are r + 1 cointegrating vectors versus the hypothesis that
there are r cointegrating vectors. Johansen and Juselius (1990) suggest
that the maximum eigenvalue test may be better.
Trace test
Result (5.7) in section 5.5.3 shows that the maximum of the likelihood
is given by
where A* are the roots of the determinant equation (5.6). The LR test
statistic for the hypothesis of at most r cointegrating vectors is
Xtrace = ~T ^ 
ln(l - A,)
i=r+l
where Ar+i,..., An are the (n — r) smallest eigenvalues of the determinant
equation (5.6) presented in chapter 5. The asymptotic distribution of

212 
Tests for cointegration
this statistic is given by the trace of the stochastic matrix
/ (dW)W ([ 
WW'dr) 
[ W(dW)f 
(6.8)
Jo 
\Jo 
) 
Jo
where W is an (n — r) dimensional Brownian motion. In case there are
a constant and/or a trend term in the VAR model we start with, (6.8)
is changed to
r1 
~ f r1 ~ ~ 
\ - 1 r1 ~
/ (dW)W' / WW'dr 
I W(dW)f 
(6.9)
Jo 
\Jo 
J Jo
where W is the demeaned or detrended Brownian motion.
Maximum eigenvalue test
To test the null hypothesis of r cointegrating vectors versus the alterna-
tive of (r + 1) cointegrating vectors the LR test statistic is
^max = — Tln(l — Ar+i)
The asymptotic distributions of this statistic are given by the maximum
eigenvalue of the stochastic matrix (6.8) and (6.9) according to the dif-
ferent specifications of deterministic trends in the VAR model.
Detailed tables of critical values for these tests are provided in Osterwald-
Lenum (1992) who tabulates them for systems of order n < 11, and num-
ber of cointegrating vectors 0 to n (0 implying all linear combinations are
1(1) and n implying that all the variables are stationary). He also tab-
ulates them for different cases of constant and/or trend terms included.
The paper gives the 50, 80, 90, 95, 97.5, and 99 percent quantiles, as
well as the mean and variance of the test statistics.
As discussed in chapter 5 (section 5.5.2), Johansen (1992) and Perron
and Campbell (1993) extended the Johansen tests to include trends (and
thus cover the case of stochastic cointegration). The tests considered
earlier are tests for deterministic cointegration (see section 5.5.2 for a
discussion of deterministic and stochastic cointegration).
Table 6.5 provides part of critical values of the Johansen's LR tests in
Osterwald-Lenum (1992). As we have seen in equations (6.8) and (6.9)
the asymptotic distributions of test statistics are different according to
the different specifications of deterministic trends in the VAR model.
Moreover, when we construct the ECM from the VAR model, the de-
terministic terms in the ECM may differ from those in the VAR model.
When there are deterministic cointegration relationships among vari-
ables, deterministic trend terms in the VAR model will not be present

6.5 Multiple equation methods
213
Table 6.5. Quantiles of the asymptotic distribution of the JohansenJs
LR test statistics
n — r
1
2
3
4
5
6
7
8
9
10
11
1
2
3
4
5
6
7
8
9
10
11
90%
2.86
9.52
15.59
21.58
27.62
33.62
38.98
44.99
50.65
56.09
61.96
2.86
10.47
21.63
36.58
55.44
78.36
104.77
135.24
169.45
206.05
248.45
Case I
95%
3.84
11.44
17.89
23.80
30.04
36.36
41.51
47.99
53.69
59.06
65.30
3.84
12.53
24.31
39.89
59.46
82.49
109.99
141.20
175.77
212.67
255.27
99%
90%
Case II
95%
Maximum eigenvalue
6.51
15.69
22.99
28.82
35.17
41.00
47.15
53.90
59.78
65.21
72.36
6.51
16.31
29.75
45.58
66.52
90.45
119.80
152.32
187.31
226.40
269.81
2.69
12.07
18.60
24.73
30.90
36.76
42.32
48.33
53.98
59.62
65.38
3.76
14.07
20.97
27.07
33.46
39.37
45.28
51.42
57.12
62.81
68.83
Trace test
2.69
13.33
26.79
43.95
64.84
89.48
118.50
150.53
186.39
225.85
269.96
3.76
15.41
29.68
47.21
68.52
94.15
124.24
156.00
192.89
233.13
277.71
99%
test
6.65
18.63
25.52
32.24
38.77
45.10
51.57
57.69
62.80
69.09
75.95
6.65
20.04
35.65
54.46
76.07
103.18
133.57
168.36
204.95
247.18
293.44
90%
7.52
13.75
19.77
25.56
31.66
37.45
43.25
48.91
54.35
60.25
66.02
7.52
17.85
32.00
49.65
71.86
97.18
126.58
159.48
196.37
236.54
282.45
Case III
95%
9.24
15.67
22.00
28.14
34.40
40.30
46.45
52.00
57.42
63.57
69.74
9.24
19.96
34.91
53.12
76.07
102.14
131.70
165.58
202.92
244.15
291.40
99%
12.97
20.20
26.81
33.24
39.79
46.82
51.91
57.95
63.71
69.94
76.63
12.97
24.60
41.07
60.16
84.45
111.01
143.09
177.20
215.74
257.68
307.64
Source: Osterwald-Lenum (1992).
in the ECM. On the other hand, if there are stochastic cointegration
relationships, deterministic trend terms appear in the ECM. But in this
case there are two possibilities. One is the case that deterministic trend
enters only via the error correction term (so there is no separate drift
term in the ECM). The other is the case that a drift term enters as
an independent term in the ECM. Thus the critical values in table 6.5
should be used according to the following cases (in the following, Dt is
a column vector of zero-mean seasonal dummies and et ~ A/"(0, A)):
Case I: There is no drift in the ECM model.
fc-i
AXt = J2 r» AX*_i + apXt-i + 9Dt + et
This is the case of the deterministic cointegration.

214 
Tests for cointegration
Case II: There is a separate drift in the the ECM model.
fc-i
AXt = ]T Ti AXt-i + oifi'Xt-x + fJL + * A + £t
i=l
This is the case of the stochastic cointegration. A (separate)
drift term in the ECM implies that the first-differenced variables
in the ECM do not have the same mean. And thus the level
variables may have different growth patterns (though they have
common stochastic growth pattern).
Case III: There is no (separate) drift in the ECM, but a constant enters
only via the error correction term.
fc-i
AXt = ^ r< AXt^ + a(/3', A>)(*t-i, 1/ + * A + et
This model implies that the first-differenced variables in the
ECM have a common mean. For example, models of the term
structure of interest rates are expected to have such a property.
Besides these three cases, critical values for the ECM with a trend are
also given by Osterwald-Lenum.
The question of what deterministic variables should be included in
the ECM is closely related to the null hypothesis of unit root tests. As
we have seen in chapter 3, when the null hypothesis of unit root tests
(or DGP) is a random walk with tk trend term, one should include
at least tk+1 trend term in the estimating regression. Thus when one
considers the ECM with the presence of drift and/or trend, one should
check whether the appropriate specification of the deterministic trends
has been used in the regression for unit root tests.
6.5.2 Finite sample evidence on the Johansen tests
There have been several Monte Carlo studies on the finite sample evi-
dence on the Johansen tests. We shall review these here and then present
a summary assessment of these studies.
Podivinsky (1990)
Podivinsky (1990) investigates the empirical sizes of Johansen's test by
Monte Carlo simulation. He finds that the tabulated critical values based
on the asymptotic distribution may be inappropriate when applied to
sample sizes of 100 or smaller. The second problem investigated is that

6.5 Multiple equation methods 
215
of underspecifying or overspecifying the number of variables in the sys-
tem. The author investigates the powers of the Johansen test, the ADF
test, and the CRDW (cointegrating regression Durbin-Watson) test in
these cases. The main conclusion is that, all the tests can be misleading
if too few potential variables are included in the analysis (see Maddala,
1992, p. 601, for a discussion of the consequences of omitted variables
in tests for cointegration). On the other hand, overspecification of the
number of variables in the system has less serious consequences, par-
ticularly when combined with Johansen's procedure for testing linear
restrictions on the elements of a cointegrating vector.
Boswijk and Franses (1992)
Boswijk and Franses (1992) investigate the effects of dynamic specifica-
tion on the size and power of three cointegration tests: (i) the residual-
based ADF test proposed by Engle and Granger, (ii) the ECM-based
Wald test by Boswijk, and (iii) Johansen's test. They find:
(i) An insufficient lag length p can lead to substantial size distortions
of the null hypothesis. On the other hand, overparameterization
results in a loss of power.
(ii) The overrejection of the null of no cointegration suggests the pos-
sibility of spurious cointegration. In a way the idea of cointegra-
tion arose as a means of safeguarding against spurious regressions.
This Monte Carlo study suggests that this attempt is only partly
successful.
(iii) The Wald test is more powerful than ADF test. It is also more
powerful than Johansen's test in this case because the condition-
ing variable is weakly exogenous. Johansen's LR test does not
exploit this feature.
Eitrheim (1992)
This is an extensive Monte Carlo study on Johansen's trace and maxi-
mum eigenvalue tests and the ML method of estimating the cointegrating
vectors. The author also discusses the effects of several model misspeci-
fications: (i) wrong order in the VAR model, (ii) ignoring nonnormality
(e.g., if the errors are of the ARCH type or are serially correlated),
(iii) effect of temporal aggregation or systematic sampling on inference
about cointegration, (iv) measurement errors: the nonstationary latent
variables are measured with errors. Several aspects of the test statis-

216 
Tests for cointegration
tics and the estimators of the short-run and long-run parameters were
studied but some of the main conclusions are:
(i) The trace test and maximum eigenvalue test have, as expected,
low power against near cointegration alternatives,
(ii) The Monte Carlo results corroborate the theoretical analysis in
Johansen (1991) with respect to the proposed sequence of tests
starting with Ho : r = 0, Hi : r < 1 and stopping at the first
nonrejection where r is the number of cointegrating vectors. The
Monte Carlo results indicate that in the near cointegration case
and in small samples, r may be underestimated.
Reimers (1992)
This is a Monte Carlo study of the Johansen LR test. The main conclu-
sion is that the Xmax test statistic should be corrected for the number
of estimated parameters to obtain satisfactory size properties in finite
samples. This is accomplished by multiplying the test statistic by a fac-
tor (T — np)/T where T is the number of observations, n is the number
of variables in the VAR system, and p is the lag length of the VAR. This
suggestion has been implemented by Gregory (1994) in his Monte Carlo
study.
Cheung and Lai (1993)
Cheung and Lai examine the finite sample sizes of Johansen's (1991)
LR tests for cointegration and their robustness to lag length specifica-
tion and nonnormal errors. (The 1991 paper by Johansen includes a
constant term, but the 1988 paper does not. Gonzalo (see chapter 5,
section 5.7) considered Johansen's 1988 paper.) Cheung and Lai use
response surface methodology similar to that of MacKinnon (see section
6.2). Unlike the study by Reimers they adjust the critical values rather
than the test statistics. Their findings are:
(i) The response surface estimation shows that the finite sample bias
of Johansen's tests is a positive function of T/(T — np) where T is
the sample size, n is the number of variables and p is the number
of lags in the VAR. Since T/(T - np) is > 1 for any finite T, the
tests are always biased toward finding cointegration too often if
asymptotic critical values are used.
(ii) If a low-order VAR model is used both the trace and maximum

6.5 Multiple equation methods 
217
eigenvalue tests are seriously biased toward spuriously finding
cointegration.
(iii) They find the AIC and BIC criteria not satisfactory in the selec-
tion of the lag length. But this is not surprising. See the discus-
sion in chapter 3 (section 3.6.2) on the selection of lag length.
(iv) Regarding nonnormality, skewness in innovations has a statis-
tically significant effect on the test sizes of both the Johansen
tests, but less so for the trace test. On the other hand, the trace
test appears to be more robust to excess kurtosis (in the innova-
tions) than the maximum eigenvalue test. Overall, the trace test
is found to be more robust to nonnormality than the maximum
eigenvalue test.
Toda (1994, 1995)
Toda (1995) investigates the performance of the Johansen (1991) tests
(i.e., including a constant term rather than the tests in Johansen (1988)
that do not include a constant term). These tests are for deterministic
cointegration. Toda (1994) investigates tests for stochastic cointegration
and the trace test statistic only. In his 1995 paper, Toda finds that the
test performance is very sensitive to how close to unity the stationary
roots of the process are. Even 100 observations are not sufficient to
detect the true cointegration rank if a stationary root is close to unity,
say 0.8 or higher. In this situation the cointegration rank is underesti-
mated. Toda also finds that the test performance is very sensitive to the
correlation between the innovations that drive the stationary and non-
stationary components of the cointegrated process. If this correlation is
zero, the tests may not perform well, even if the root of the stationary
component is not close to unity. Toda suggests that a sample size of
at least 300 is needed to see satisfactory performance of the Johansen
test. However, such conclusions about the sample size depend on the
error variances in the system (see the discussion on signal-noise ratios
in chapter 5, section 5.7).
Toda (1994) investigates tests for stochastic cointegration as discussed
in Johansen (1992) and Perron and Campbell (1993). In this paper
Toda considers both the trace and maximum eigenvalue test statistics.
His conclusions are similar to those in Toda (1995) for deterministic
cointegration discussed earlier, except that the tests from the modified
procedures perform worse than the tests for deterministic cointegration.
Among the trace tests and maximum eigenvalue tests, neither dominate

218 
Tests for cointegration
the other except that the latter are better when the power of the tests
is low. This study is rather discouraging with regard to Johansen-type
tests for stochastic cointegration.
A summary assessment of the Monte Carlo studies
In addition to these studies that concentrate on the Johansen tests, there
are other studies that analyze the Johansen tests along with other single
equation tests (see, e.g., Gregory, 1994 and, Haug, 1996). They both
come to the conclusion that no test dominates the others and hence it
is important for empirical researchers to report the results from several
tests for cointegration. However, this raises the issue of what to con-
clude in case of conflicting results from the several tests. Combination
of tests results (as in the Fisher test discussed in chapter 4 in connec-
tion of panel data unit root tests) is not possible since the tests are not
independent.
Gregory concludes from his Monte Carlo study that the ADF, Za, Zt
tests appear to be most reliable in terms of size and power. The LR tests
of Johansen (as well as the Stock and Watson test) tend to overreject the
null. (Note that these two sets of tests are from different categories as we
shall discuss later.) The paper by Haug comes to different conclusions.
It concludes that the Johansen and Juselius Xmax test and the ADF test
reveal least size distortion. First of all, the two tests are not comparable
- the Xmax test is for the number of cointegrating vectors and the ADF
test is a single equation test for testing the existence of a cointegration
relationship. Anyway the conclusion that the Xmax test exhibits the
least size distortion is in conflict with the conclusion by Gregory and
several others.
A major drawback of these studies is that when considering single
equation tests like the ADF, Za, Zt, etc., the ECM-based tests discussed
in section 6.3 are completely excluded. The ECM-based tests have been
shown to have superior power properties than the ADF, Za, and Zt
tests. Thus, any conclusions about the latter tests are almost useless.
We also remarked in section 6.2 that any of the unit root tests can
be used to test for cointegration. Thus, given more powerful unit root
tests (such as the DF-GLS and modified Phillips-Perron tests discussed
in chapter 4) one should no longer use the ADF, Za, and Zt tests for
cointegration. Of course, when using the new tests for unit roots, one
cannot use the significance levels tabulated, because the residuals used
in the cointegration tests are estimated errors. However, it is always
best to get the appropriate significance levels by Monte Carlo methods

6.5 Multiple equation methods 
219
(or bootstrap techniques as described in chapter 10) rather than from
the asymptotic distributions.
Furthermore, it is important to separate out single equation tests
and system tests when reporting the results from Monte Carlo stud-
ies. Gregory does this but Haug does not. It is important to bear in
mind that when talking about cointegration tests, we are referring to
three categories of tests:
(i) Single equation tests - these test whether a given set of 1(1) vari-
ables are cointegrated.
(ii) Multiple equation tests such as Johansen's Xmax and trace tests
- these are tests for the number of cointegrating vectors.
(iii) Tests for significance of the coefficients of the estimated cointe-
grating vector - these tests apply both in the single equation as
well as the multiple equation contexts.
The Monte Carlo studies should separate out the tests in these three
categories.
Xiao and Phillips (1996) extend the Elliot-Rothenberg-Stock proce-
dure we discussed in chapter 4 to the case of cointegration but they
do this for tests in category (ii) (which is, of course, the more diffi-
cult problem). What we were mentioning earlier is to use that test for
tests in category (i) instead of the ADF, Za, and Zt tests for cointegra-
tion.
The Monte Carlo studies on the Johansen tests (tests in category
(ii)) discussed earlier emphasize different characteristics of these tests.
There are no conflicts in the conclusions. We can summarize the results
as follows:
(i) Tabulated critical values based on asymptotic distributions may
be inappropriate if sample sizes are 100 or smaller (Podivinsky).
(ii) All tests can be misleading if too few variables are included (Po-
divinsky) .
(iii) Insufficient lag length can lead to substantial size distortions.
Overspecification, on the other hand, leads to loss of power (Boswijk
and Franses).
(iv) The Johansen Amax test statistic should be corrected for the num-
ber of estimated parameters. This is done by multiplying it by
(T — np)/T, where T is the number of observations, n is the
number of variables, and p is the number of lags.

220 
Tests for cointegration
(v) If a low-order VAR model is used both the trace and Amaa; statis-
tics are seriously biased toward spuriously finding cointegration.
There appears to be scope for improvement with the Johansen tests by
using efficient detrending procedures, as discussed in Xiao and Phillips
(1996). More discussion of tests in category (iii) will be provided in
chapter 10.
6.5.3 Further comments on the Johansen tests
The Johansen procedure is the most commonly used system method in
cointegration analysis. It has been built into several econometric soft-
ware packages widely used (e.g., Microfit, PC-GIVE, E-VIEWS) and has
a seperate software package CATS in RATS. The Monte Carlo studies
reviewed in the previous section discuss some problems in this procedure.
The main problems are sensitivity to misspecification of the lag length,
and substantial size distortions in the tests for the second and subse-
quent cointegrating vectors when the ratio of data points to the number
parameters is small (of the order of 5 or less). See Toda (1994) and Ho
and S0rensen (1996). We shall discuss a few more of the problems noted
in the literature.
The Johansen procedure identifies the cointegration space and not the
cointegrating vectors because linear combinations of cointegrating vec-
tors are themselves cointegrated. Thus, some further identifying condi-
tions need to be imposed. We have discussed this problem in chapter 5
(section 5.6).
Gonzalo and Lee (1995) show analytically and numerically that in
some situations of practical interest the Johansen LR tests tend to find
spurious cointegration with probability approaching one asymptotically.
They consider two situations in which this occurs: the first is one where
the variables have long memory and they are not pure 1(1) processes, but
they are difficult to differentiate from 1(1) processes with standard unit
root tests (we consider these in chapter 9). The second corresponds to
1(1) variables whose VAR representation has a singular or near singular
covariance matrix. They argue that the Engle-Granger is more robust
in these situations than the Johansen LR tests and recommend using
both tests in order to avoid or discover a pitfall. However, in practice, it
is almost always the case that the two procedures give different results,
but it has been noted that the difference is not necessarily due to this
particular pitfall. In fact, as argued in section 6.3 on ECM-based tests,

6.5 Multiple equation methods 
221
the Engle-Granger tests have been found to have very low power. The
appropriate course of action is to apply tests for these situations and
devise powerful cointegration tests to take account of these problems.
The study by Gonzalo and Lee considers the problems with the
Johansen procedure if the variables are not 1(1) because of long mem-
ory. The problems actually arise for all cointegration tests, not just the
Johansen tests. Elliott (1995a) raises another issue: the problem that
the variables considered are not 1(1) but close to unit root processes.
Again as he clearly indicates this is a problem with all cointegration
tests, not just the Johansen tests, because they all apply unit root tests
(which have low power) in the first step and then assuming that the
variables are 1(1) proceed with the cointegration tests.
Elliott (1995a) shows, analytically, using local to unity asymptotic
distribution theory that the estimates of the cointegrating vector when
there is a large root in the explanatory variables which is not equal
to one but close to one, remain consistent, but the tests for the coef-
ficients of the cointegration vector tend to overreject the true null hy-
potheses. Elliott shows that the extent of this overreject ion depends
on the extent of the departure of the largest root in the regressor from
one and on the degree of simultaneity between the errors in the equa-
tion of interest and the explanatory variables. For any deviation of
the largest root in the explanatory variable from one, the size of the
test approaches one as the degree of simultaneity increases. Elliott
argues that this can be a very important problem in empirical work.
For plausible parameter values he shows that the actual size can be 50
percent when the nominal size is 5 percent. For many economic vari-
ables the confidence intervals for the largest root include the unit root
but they do include many stationary roots as well. For instance, for
the US GNP quarterly data from 1948:1 to 1992:1, a 95 percent confi-
dence interval for the largest root is [0.885, 1.03]. Since the value 1.0
is in this interval we would not reject the unit root hypothesis. But
we would not reject many stationary alternatives either. Elliott and
Stock (1994) show that pre-tests to distinguish 1(0) from 1(1) and then
preceding in subsequent analysis is to use standard normal asymptotic
theory if 1(0) is accepted and nonstandard theory if 1(1) is accepted
results in nominal sizes of up to 30 percent for a nominal test of 10
percent. These problems and solutions to this problem that they pro-
pose are discussed in section 7.5 of the next chapter. The problem
with cointegration tests is this two-step procedure that is universally
followed.

222 
Tests for cointegration
We shall not go into the asymptotic distributions that Elliott derives.
Elliott considers pre-tests in reference to the model
Vit = «i +P2/i,t-i +£it
2/2t = a2+72/i,t-i +^2t
The problem is testing hypotheses about 7 when p = 1 -f c/T is close to
unity but not equal to 1.
This is the model analyzed by Elliott and Stock (1994), which we
discuss in greater detail in section 7.5 of the next chapter. For this
model he derives:
(i) the asymptotic distribution of T(j — 7),
(ii) the asymptotic distribution of the t-statistic ty=7,
(iii) the asymptotic sizes of the tests
for all the efficient methods of estimation (discussed in chapter 5). The
conclusions have been stated earlier. Elliott shows that the problem
can be seen as an omitted variable bias problem. In the absence of
any dynamics in the errors, the estimation of 7 involves estimating the
equation (by taking conditional expectation of e^t given en)
y2t = a + 72/M-i + 0(1 - L)ylt - </>(p - l)yi,t-i + Vt
where (j) = cov(en,£2t)/var(en)- 
If we impose p = 1 when it is not,
we are omitting the last term in this equation and this is the source of
the bias. This depends on (j> and (p — 1). If 0 ~ 0, then even if p is
not close to 1, the problem is not severe. Elliott argues that in practice
the problem is not so severe if the simultaneous equation bias is not
large. Nevertheless, it is always the best to use a more efficient method
of estimation than OLS.
The problems discussed by Elliott refer not to tests for cointegration
directly (which we have been discussing) but to tests on the coefficients
of regressions when there is uncertainty whether they are cointegrating
regressions. In the example considered by Elliott it is not certain whether
7 is a cointegrating vector or not.
6.6 Cointegration tests based on LCCA
As described in chapter 5 (section 5.5.3) the Box-Tiao method levels
canonical correlation analysis (which Bewley terms LCCA) has been
used to compute cointegration vectors by Bossaerts (1988) and Bewley

6.6 Cointegration tests based on LCCA 
223
and Orden (1994). Bossaerts suggested that the canonical variates ob-
tained by this method could be directly tested for cointegration. If zt
is the canonical variate corresponding to the minimum eigenvalue, then
one estimates
zt = pzt-i+et 
(6.10)
and tests the hypothesis p = 1. Bossaerts conjectured that the signifi-
cance tables in Fuller (1976) could be used for this test, and Bewley and
Orden conjectured that the critical values depended on the number of
variables in the system.
Bewley and Yang (1995) to be referred to as BY examine these con-
jectures and develop four tests for cointegration based on the levels
canonical cointegration. As discussed in chapter 5 (section 5.5.3) the
eigenvalues are obtained from equation (5.6) as in the Johansen method
except that, as noted there, the definition of Rot and Ru (hence Soo and
Soi) are defined from the levels regression. The four tests developed by
BY are:
(i) The Dickey-Fuller type t-test based on ip.
(ii) The Dickey-Fuller type coefficient test based on T(p — 1). Both
of these are for the equation (6.10).
(iii) The minimum eigenvalue test based on
^min = T{\ — Ai)
where Ai is the minimum eigenvalue.
(iv) The trace test based on
Tests (iii) and (iv) are the analogues of Johansen's maximum eigenvalue
and trace tests, respectively. However, the hypotheses they test are
different.
BY show that the test statistics have nonstandard asymptotic distri-
butions and provide critical values for each of the tests for models up to
six variables. These tables also show that Bossaerts conjecture that one
could use the Dickey-Fuller tables is not correct. The asymptotic distri-
butions and hence the critical values depend on the number of variables
in the system.
BY perform a Monte Carlo study to examine the small sample per-
formance of these tests. The model used is the bivariate model
yt - bxt = ut 
ut = put-i + eiu 
\p\ < 1
xt-ayt 
= vt 
vt = 
vt-i+e2t

224 
Tests for cointegration
Table 6.6. Critical values of the LCCA-based tests
n
2
4
6
T
75
150
250
75
150
250
75
150
250
t
1%
4.26
4.16
4.13
5.49
5.29
5.21
6.58
6.22
6.11
p
5%
3.65
3.59
3.57
4.90
4.74
4.68
5.96
5.70
5.60
T(p
1%
27.85
29.46
30.21
42.62
46.34
47.96
54.75
61.24
64.36
-1)
5%
21.64
22.53
22.93
35.89
38.43
39.52
48.21
53.08
55.19
T ( l -
1%
45.03
52.34
56.15
60.62
77.81
86.04
69.19
96.73
111.35
Ai)
5%
36.50
41.01
43.11
54.09
66.32
72.14
64.87
86.64
97.38
Source: Bewley and Yang (1995, table 1, p. 992).
where en and S2t have zero means and covariance matrix
1 
9a
8a 
a2
The crucial parameter in the system are p and 8. BY observe that for this
model, Johansen's maximal root is (asymptotically) 
(1—p)/[2—(l+p)92].
Hence as 9 —> 1, the power of Johansen's Xmax test statistic, Tln(l —
^max) approaches unity. On the other hand, the LCCA minimal root
can be expressed asymptotically as Xmin = P2- This does not involve 9
and hence the power of the LCCA-based test is insensitive to 9 in finite
samples.
BY find that for approximately |0| < 0.5 each of the LCCA tests
has greater power than the Johansen tests or the Engle-Granger test. If
\8\ > 0.5, then the Johansen test dominates. Also the LCCA trace test is
as powerful as the Xmin test for 9 close to zero, but its power deteriorates
rapidly as 8 —> 1. On the basis of the simulation experiment BY argue
that the LCCA trace test is not recommended. But each of other tests
perform well compared with the two Johansen tests and the Engle-
Granger test except when the disturbances generating the cointegrating
equation are highly correlated with those generating the common trends.
BY present critical values for the four tests at significance levels 1
percent, 5 percent, and 10 percent for sample sizes T = 75, 100, 150,
200, 250, and 500, and the number of variables n = 2, 3, 4, 5, 6 (table 1,
p. 992 of their paper). Part of their table is presented in table 6.6. We
have omitted the trace test because BY said it was not recommended.

6.6 Cointegration tests based on LCCA 
225
Note, however, that these critical values in BY are for the model with
no constant term as in Johansen (1988). Yang and Bewley (1996), how-
ever, provide critical values for the tests in the case of drift. Johansen has
different tests for models with constant term (Johansen, 1991) and mod-
els with trend (Johansen, 1992). The tables from Osterwald-Lenum pre-
sented earlier are for these three cases. Thus, when referring to Johansen
tests, it is important to keep in mind which of the Johansen tests one is
talking about.
Another difference between the Johansen tests and the tests in BY
is that in the Johansen procedure the tests determine the number of
cointegrating vectors. The tests in BY test one eigenvector at a time
sequentially. First the eigenvector corresponding to the smallest eigen-
value Ai is tested for a unit root. Then the dimension of the system is
reduced and the roots are recalculated and the procedure is repeated.
Bewley and Yang (1996) pursue this comparison further and investi-
gate the properties of the LCAA-based tests by comparing them with
Johansen's LR tests, examining size distortions in tests for the second
and subsequent cointegrating relationships and the impact of misspeci-
fying the lag length in each of these situations. Since the LCCA-based
tests have also been found to suffer from size distortions, they propose
some simple adjustments to improve their small sample performance.
The main conclusions of this study are:
(i) The LCAA-based tests are less sensitive to misspecification of the
lag length than the Johansen LR tests.
(ii) The size distortion problem in the LCAA tests can be mitigated
by applying a correction factor [(T — p)/T]0'75 where T is the
sample size and p is the lag length. Note that a factor of [(T —
p)/T] was suggested by Reinsel and Ahn that was also used in
the Reimers (1992) and Cheung and Lai (1993) papers reviewed
in the previous section..
(iii) In an empirical example in a typical macroeconomic setting where
all evidence from exploratory data analysis points to the existence
of cointegrating relationships, the LR tests reject cointegration
whereas the LCCA-based tests detect cointegrated relationships.
As noted earlier, in situations where the degree of correlation
between the innovations in the cointegrated relationship and in-
novations in the process driving the stochastic trend are weakly
correlated, the LCCA-based tests are more powerful than the

226 
Tests for cointegration
Johansen tests. This appears to be the case in many practical
situations.
We shall not go into more details of this study.
6.7 Other tests for cointegration
In the preceding sections we discussed residual-based tests and tests
based on canonical correlations. We shall now discuss some other tests.
6.7.1 Tests based on principal components
The principal component method is another multivariate technique that
depends on eigenvalues of a matrix like the canonical correlation method.
Simply stated it identifies the linear combination of a set of n variables
y = (2/1,...,2/n) that has the highest (or lowest) variance subject to a
normalization condition. Let the covariance matrix of y be E. Then
variance of a linear combination c'y is c'Ec. Maximizing this with respect
to c subject to a normalization rule c'c — 1 gives c as the eigenvector of
|E — cl\ = 0. If Ai, A2,..., An are the eigenvalues of E and ci, c2,..., cn are
the corresponding eigenvectors, then the c* are mutually orthogonal and
var^y) = X{. If we order the A^ in descending order, Ai > A2 > • • • >
An, then c[y is the linear combination that has the highest variance.
c'2y is the linear combination orthogonal to this with the next highest
variance, and so on. c'ny is the linear combination with the largest
variance.
The idea in using the principal component approach to cointegration
is that 1(1) variables have much higher variance than 1(0) variables.
Thus, the principal components corresponding to the lowest A^ give the
cointegrating vectors and those corresponding to the largest A; give the
common stochastic trends. The procedure of finding the minimum roots
of a covariance matrix is similar to the LCCA method we discussed in
the preceding section.
Stock and Watson (1988), Phillips and Ouliaris (1988), and Harris
(1997) use the principal component approach to derive tests for cointe-
gration. The last paper is the most recent and the most comprehensive
one. But we shall leave the details to interested readers because going
through the paper will involve intricate detail.

6.7 Other tests for cointegration 
227
6.7.2 Other tests
Some other tests suggested in the literature are those by Saikkonen
(1992), Kleibergen and van Dijk (1994), and Horvath and Watson (1995).
Saikkonen's tests are system tests. He derives the tests using an au-
toregressive approximation to the DGP (data generating process). He
derives Wald tests for cointegration that follow the usual %2 distribu-
tions under the null, assuming the number of cointegrating vectors and
the normalization rules to be known. He also derives tests for these as-
sumptions. These tests by Saikkonen have not been used in application.
Even Gregory (1994) has Saikkonen's paper in the references but does
not include them in his Monte Carlo study. Hence we shall not discuss
them in detail here. But the other two have different approaches. They
are both system tests starting with the VAR system as in the Johansen
procedure. The Horvath and Watson method is based on some prior
information on some of the cointegrating vectors.
Consider the LR matrix B\ in the VAR model we discussed in sec-
tion 5.5.1 (chapter 5) in connection with the Johansen procedure. The
Johansen procedure first determines the rank of the cointegration space
and introduces any identifying information later. The framework con-
sidered by Kleibergen and van Dijk (to be referred to as KVD) differs
from Johansen's and considers the identification of the cointegration
space and that of the different cointegrating vectors in a single step us-
ing parametric identifying restrictions. For conformity with the KVD
notation, we shall use n for the LR matrix B\. Also, we shall write
II = (3a (instead of B\ — a(3' used in chapter 5).
First partition Yt into two components Y\t and Y2t of dimensions r
and n — r. Then perform a triangular decomposition of the matrix n.
The parametric restriction imposed on n to ensure identification are
Ir 
0
-f32 
In-r
and
an ai2
so that
n =
a = • o
OL22
where (32 is (n — r) x r, an is r x r, a\2 is r x (n — r), and a22 is
(n — r) x (n — r). The number of cointegrating vectors can be tested by

228 
Tests for cointegration
testing the significance of c*22 for different values of r, the cointegrating
rank. If a22 = 0, then
and the error correction coefficients are given by
a* = a = (an,ai2)
If 0:22 7^ 0, then (3 is no longer interpretable as a cointegrating vector.
Thus, testing for cointegration is performed by testing whether #22 = 0
for different values of r. Partition II into
I" n n 
II12
L n 2i n 2 2
If Iln is of full rank, then a and (3 are exactly identified. We have
the relationships: &n = Iln, &12 = ^12, $2 = —fbiftn, and o;22 =
-n2injf1
1ni2 + II22. KVD construct the Wald tests to test a22 = 0. We
shall not go into the details of this and refer readers to the KVD paper.
The Horvath and Watson paper is concerned with the rank test for
cointegration rank when some of the CI vectors are known. This sort of
situation often arises in the case of foreign exchange markets.
There have been very few applications of the KVD testing procedure
and the Horvath and Watson test. The KVD procedure is illustrated
in Kleibergen et al. (1995). The Horvath and Watson procedure is
illustrated in Edison et al. (1997). Edison et al. find the critical values
from the asymptotic distributions given in Horvath and Watson paper
to be too low compared with the critical values they derive by Monte
Carlo methods. This suggests that in all the applications of the several
tests we have discussed, it is better to derive the critical values by Monte
Carlo methods rather than from the asymptotic distributions.
6.8 Miscellaneous other problems
6.8.1 Effect of time 
disaggregation
In chapter 4 (section 4.7) we discussed the effects of time aggregation
on the power of unit root tests. We shall now discuss this problem with
reference to cointegration tests.
Hooker (1993) first argued through Monte Carlo experiments that
temporal disaggregation (using monthly versus quarterly data or quar-
terly versus yearly data) yields significant increases in the power of ADF

6.8 Miscellaneous other problems 
229
cointegration tests and that this result is contrary to the findings regard-
ing ADF unit root tests that showed increases to power by increasing
the time span of the data and not by increasing the number of obser-
vations by time disaggregation. Lahiri and Mamingi (1995), however,
argue that their Monte Carlo experiments yielded no increase in power
by time disaggregation, thus contradicting Hooker's results and reiter-
ating the conclusions reached regarding the unit root tests. Hakkio and
Rush (1991), which we shall discuss in section 6.9, also present Monte
Carlo evidence that time disaggregation does not improve the power of
residual-based cointegration tests.
In yet another study, Hu (1996) investigates the effect of skip sampling
and time disaggregation on the power of Johansen's trace and maximum
eigenvalue tests. She finds that although there are power gains when
switching to high frequency data to gain more observations with a fixed
time span, the power gains are much more significant when data with
a longer time span are used. It also confirms the findings reported in
chapter 4 (section 4.7) that the length of the sample time span is more
important than the number of observations within a fixed time span.
The study by Hu considers both the cases of skip sampling and time
aggregation. Also it studies the more powerful Johansen tests than the
residual-based tests studied earlier. It also provides critical values for
skip sampling and time aggregation for the Johansen tests.
Although the gains in power by increasing the number of observations
with a fixed time span are not as great as those coming from longer time
spans, it is not true that one should not use high frequency data. In
practice data over long time spans may not always be available and even
if they are, there is the problem of structural changes (discussed later in
chapter 13). Thus, in practice one should use the highest frequency data
available (although there are the problems of seasonal roots which we
discuss in chapter 12). Further discussion of time aggregation problems
is in chapter 12 (section 12.10).
6.8.2 The pre-testing problem
The pre-testing problem arises in the discussion of cointegration tests
at several stages. First, one has to check for unit roots before starting
cointegration tests. Thus, the unit root tests are pre-tests and it is not
clear what effect this has on the significance levels used for the subse-
quent cointegration tests. Also, it is customary to use the conventional
5 percent and 1 percent significance levels for the unit root tests. (Who

230 
Tests for cointegration
started this convention is a good question. It is R.A. Fisher who sug-
gested it in an obscure paper in 1923 and since he is the father of modern
statistics, it has been blindly followed ever since.)
There is a substantial statistical literature on the pre-testing problem
and although there is no definite answer to the question of what signif-
icance levels should be used for pre-tests, there is a concensus that the
5 percent and 1 percent significance levels are the wrong ones to use.
The significance levels used should be much higher, say 25 percent (this
is the rule suggested by the statistician I.J. Good). If this is done, we
would reject the unit root null more often than we do at the 5 percent
level, and then a discussion of cointegration is out of the question.
A further problem is that just because we failed to reject the null of
unit root at the 5 percent level does not mean that we have a unit root
process. There is a nontrivial probability that it is a stationary process.
Also, the root can be close to one but not exactly one. As argued by
Elliott (1995a), and as discussed in section 6.5.3 earlier, this has the
consequence that the tests on the coefficients of the cointegrating vector
have substantial size distortions.
However, the problem is not just one of size distortion of the tests.
There is the problem of bias in the estimated coefficients as well. The
problem of pre-test bias has been investigated by Kim (1995), in a typical
triangular cointegration system. He follows a decision theoretic approach
and evaluates the different estimators by comparing their risk functions.
He considers three estimation methods: OLS, 2SLS, and FIML, and
three different cases: stationary but near-integrated, unit root, and ex-
plosive but near-integrated series. His conclusions are that the FIML
estimator of a cointegrating vector shows high risk in an explosive but
near-integrated series, but it turns out to be the best estimator in sta-
tionary but near-integrated and unit root cases. The OLS estimator
shows high risk in all three cases. The 2SLS estimator shows consis-
tently low risks for all three cases. (A decision theoretic approach was
also used by Lahiri and Paul to evaluate pre-test estimators in AR(1)
models (see section 4.10 of chapter 4).)
The pre-testing problem does not arise in a Bayesian context because
the respective posterior probabilities of unit roots and stationarity are
taken into account. The Bayesian approach is discussed in chapter 8.
The second problem where pre-testing occurs is in the context of sys-
tem procedures (as in the Johansen method or the Box-Tiao method).
First, we use the tests to determine the number of cointegrating vectors
and then we use tests on the significance of the coefficients of the esti-

6.8 Miscellaneous other problems 
231
mated cointegrating vectors. This is a two-step testing procedure. The
significance levels used at the second stage are affected by the signifi-
cance levels used in the first stage.
Elliott (1995b) suggests a one-step procedure as an alternative to solve
the pre-testing problem caused by the two-step procedure. He suggests
testing the coefficients of the implied error correction model. However,
his Monte Carlo studies indicate that this one-step procedure is not
always better than the two-step procedure particularly when there is
strong prior information on the number of cointegrating vectors (or when
the first test strongly favors one number for the cointegrating vectors).
He, therefore, suggests using both the one-step and two-step procedures,
but this creates problems when they give conflicting results.
The issues relating to pre-testing biases in cointegrating tests are far
from resolved. A Bayesian investigation of this problem would be very
fruitful but this has not been done yet.
6.8.3 Using cointegration concepts to test for unit roots
Tests for unit roots usually precede tests for cointegration. So it looks
paradoxical that one should use the concepts of cointegration to derive
a test for unit roots. However, this is what is done in Hansen (1995).
He derives a unit root test that is related to tests for cointegration.
It is often said that "unit root tests have low power," but this may be
due to the fact that tests for unit roots depend on univariate time series
and do not take account of information in related series. Hansen shows
that taking account of this information results in considerable increase
in the power of unit root tests. He suggests a covariate-adjusted Dickey-
Fuller (CADF) t-statistic and tabulates its critical values. A GAUSS
program that calculates the test statistics and critical values is available
from Bruce Hansen.
The test that Hansen derives is based on the t-ratio of yt-i in a
transformed equation of the AR(1) model
Ayt = 72/t-i + ut
It is assumed that there is a set variables xt which is 1(1) so that Axt is
1(0). Assume that (Axt,ut) are iid with mean zero and E(Ax^) = o2
x,
cov(Axt,ut) = axu. Then we can write the AR(1) model as
Ayt = 72/t-i + b'Axt + et
where et = ut — b' Axt and b = Gxuja\ is the regression coefficient of ut

232 
Tests for cointegration
on Axt. Note that 7 retains the same meaning in this equation as in the
AR(1) model. However, var(et) < var(ut) unless oxu = 0 in which case
the two variances are equal. Hence, Hansen argues that the parameter
7 can be more precisely estimated (at least in large samples) from the
transformed equation. Note, however, that the variance of 7 from the
transformed equation is var(et)/var(yt-i) 
where yt-\ is the residual
from a regression of yt-i and Ax*. Thus, var(yt-i) < var(yt-i). Hence
it is not exactly obvious that the variance of 7 from the transformed
equation is smaller. Hansen argues that this transformed equation is
similar to that considered by Kremers et al. (1992) discussed in section
6.3, but those authors consider it as a test for cointegration and not for
univariate unit roots.
The derivation of the test statistic for 7 in the transformed equation
(particularly for leads and lags in the errors) is complicated but Hansen
shows that the ^-statistic has a distribution that is a convex combina-
tion of the DF distribution and the standard normal, depending on a
parameter p2 that can be computed from the data. Hansen provides 1
percent, 5 percent, and 10 percent critical values of the test statistic for
values of p2 between 0 and 1, for the cases: standard, demeaned, and
detrended. His GAUSS computer program gives the details.
Hansen shows that there is a considerable gain in the power using the
CADF t-statistics when testing for unit roots. As is usual he illustrates
it using the Nelson-Plosser data (the common guinea pig for every new
unit root test).
The idea of using covariates in unit root testing is a good one, but the
fact that only 1(1) variates can be considered is very restrictive. These
variables are judged to be 1(1) by some earlier tests that are based on
single-variable unit root tests. Also, there is the issue of why unit root
tests are not included in this whole discussion. Why not jointly test for
unit roots in yt and xt instead of a test for unit roots conditional on Xt
being 1(1). The panel data unit root tests discussed in section 4.9 fall
into this category of joint tests. Thus, there are several other issues in
the use of covariates in unit root testing, although the argument about
considering other variates makes sense. The basic issue is how best to
use the information in yt and xt to analyze whatever problem you are
interested in. Starting from a VAR is the common route.
The use of unit root tests using a single variable does not make eco-
nomic sense in many situations and can sometimes give misleading re-
sults. An example of this is testing the purchasing power parity (PPP)
theory based on unit root testing in real exchange rates. Steigerwald

6.9 Of what use are cointegration tests? 
233
(1996) illustrates this point. He shows how considering nominal ex-
change rates and prices as jointly endogenous and incorporating dynam-
ics in the equations produces dramatically different results compared to
those based on tests for unit roots on real exchange rates.
6.9 Of what use are cointegration tests?
We have discussed several tests for cointegration. However, several
Monte Carlo studies about the power of these tests show that in general
these tests are not powerful. Also as noted by Elliott (1995a) there is
the problem of substantial size distortions if the variables under consid-
eration are not really 1(1) but have roots close to unity.
Campbell and Perron (1991) argue that it may be difficult to distin-
guish processes that exhibit cointegration from those that do not, and
more so to estimate precisely the exact number of cointegrating rela-
tionships. In fact if the goal of cointegration tests is to uncover the true
long-run relationships, this argument is very disconcerting. In some
applications the goal is not to uncover the true number of cointegrating
relationships per se, but rather to have a useful guide in imposing restric-
tions on VAR models and ECM models that may lead to more efficient
estimation and improve forecasting performance. For this purpose the
testing procedures described in this chapter can serve a useful purpose,
although recent studies show that the VECM model, the VAR with coin-
tegrating restrictions does not provide much evidence of improvement
in forecasting. See chapter 5 (section 5.8).
Another issue is how to interpret the results of cointegration analysis.
If there is only one cointegration relationship, then it may be easy to
interpret it as a long-run relationship. However, if we find the number
of cointegrating vectors to be greater than one, this creates problems
of interpretation, and we need to bring in more economic theory. We
discussed the identification problems in chapter 5 (section 5.6).
In spite of the limitations mentioned above, many empirical studies
take cointegration very seriously and aim to test economic theories using
cointegration techniques. We shall review some of these studies and see
what has been accomplished. We shall start with the work on (see
Maddala, 1991)
(i) tests of the market efficiency hypothesis (MEH),
(ii) tests of the long-run demand for money,
(iii) tests of the purchasing power parity (PPP) theory.

234 
Tests for cointegration
6.9.1 Testing the MEH
If the prices in two markets are cointegrated this implies that it would
be possible to forecast one from the other. This, in turn, implies that the
markets are not efficient. The MEH thus implies absence of cointegration
(or the non-rejection of the no-cointegration null).
In the case of the foreign exchange markets, the MEH for the spot
rates implies that the spot rates for the different currencies should not
be cointegrated. The MEH applied to the forward market implies that
the forward rates for the different currencies are not cointegrated and
that each of the spot rates is cointegrated with its own forward rate.
MacDonald and Taylor (1989) consider monthly data on the log of the
exchange rates for the period January 1973 to December 1985 for nine
currencies (Australian dollar, British pound, Canadian dollar, Danish
krone, Dutch guilder, French franc, German mark, Italian lira, and
Japanese yen). They use the ADF test and find that the logs of all
the spot rates are 1(1) and that there is no strong evidence of cointe-
gration among the different spot rates (in pairwise cointegration tests).
Hakkio and Rush (1989) consider German and UK spot and forward
rates for the period July 1975 to October 1986 (on a monthly basis).
They find that neither the two spot rates nor the two forward rates are
cointegrated, thus, suggesting market efficiency. In addition both the
German future spot and current forward rates, as well as the UK future
spot and current forward rates appeared to be cointegrated. This again
suggests market efficiency.
Both the papers, however, dealt with only bivariate tests of cointe-
gration. It is intuitively obvious that they suffer from the usual omitted
variable problem. Suppose that x, y, and z are three 1(1) variables
which are cointegrated so that x = fiiy + fcz + u where u is 1(0). If
now we run the regression x = (3\y + v, then, since v = foz + u is
1(1), we will observe that x and y are not cointegrated. Thus, a bivari-
ate cointegration test will not reject the hypothesis of no-cointegration.
Such bivariate tests do not tell us whether markets are efficient or not.
The Johansen (1988) procedure that tests for all possible cointegrating
relationships among a set of variables is the appropriate one to use to
test the MEH. Baillie and Bollerslev (1989) examined the logarithms
of daily exchange rates for seven currencies for the period March 1,
1980 to January 28, 1985 and found that both the Engle and Granger
two-step cointegration test and the Johansen test showed evidence of
cointegration. The data were on the exchange rates versus the US dol-

6.9 Of what use are cointegration tests ? 
235
lar for the UK, West Germany, Prance, Italy, Switzerland, Japan, and
Canada. The rejection of the hypothesis of no-cointegration is partly
due to the fact that they had a large sample of 1,245 (daily) observa-
tions.
Sephton and Larson (1991) provide convincing evidence that the is-
sue of whether or not a set of spot rates are cointegrated depends on
the time period considered. They show that the results from the Jo-
hansen procedure are sensitive to the time period considered. However,
it is not just the Johansen procedure that gives such results. The in-
stability can be observed with even the Engle-Granger two-step pro-
cedure. It is not at all surprising that tests for market efficiency are
sensitive to structural changes because it has been noted that tests for
unit roots are sensitive to structural changes. (This is discussed in chap-
ter 13.) Note that in tests for cointegration the null hypothesis is one
of no-cointegration. Since the significance levels used are the same (5
percent) no matter what the sample size is, the null hypothesis (of no-
cointegration) is likely to be rejected if the number of observations is
very large. This is just a statistical artifact. Bayesians have always ob-
jected to the idea of using a constant significance level irrespective of
the sample size. Baillie and Bollerslev have 1,245 observations covering
a period of five years. If we were to be analyzing the same period with
monthly data, we would have only 60 observations, and the hypoth-
esis of no-cointegration may not be rejected at the usual significance
levels.
This point has been noted by Layton (1993). He argues that MacDon-
ald and Taylor (1989) and Baillie and Bollerslev (1989) came to different
conclusions due to the different sampling frequencies used. Since Bail-
lie and Bollerslev used a multivariate cointegration testing framework
while MacDonald and Taylor employed bivariate tests, one possible ex-
planation for the difference in results could rest on the issue of technique.
However, Layton argues that the contradictory results are due to the fact
that Baillie and Bollerslev used daily data and MacDonald and Taylor
used monthly data. Layton argues that foreign exchange markets display
well-known varying degree of temporally correlated volatility depending
on how frequently the data are sampled. He substantiates his argu-
ments by considering the data on the Australian dollar sampled daily,
weakly, and monthly. He tests the data for ARCH effects and finds that
the evidence for cointegration becomes weaker as the ARCH effect gets
stronger.

236 
Tests for cointegration
6.9.2 Testing long-run equilibrium relationships: the long-run
demand for money
One other application of cointegration is to test the existence of long-
run relationships. One argument sometimes made is that cointegration
is about long-run economic relationships, and one needs really long time
series (not in the number of observations but in time span) to use coin-
tegration techniques (see, for instance, Hakkio and Rush (1991) for such
an argument). This is not a meaningful argument for several reasons. If
the variables are nonstationary, then existence of a long-run equilibrium
economic relationship implies cointegration. But not all cointegrating
relationships need have meaning in the sense of long-run economic re-
lationships. Cointegration is a purely statistical concept. It is part of
a-theoretical econometrics. Cointegrating relationships need not have
any economic interpretation. Whether or not a series is long enough to
establish a cointegrating relationship depends purely on the statistical
properties of the series and whether the cointegrating relation has any
economic interpretation can never be answered without any relevance to
the question being asked.
How long the long run is depends on the speed of adjustment of the
particular markets considered. This was the conventional wisdom in old
style econometrics before cointegration. There is no reason to discard
this. This is also an additional reason why the analysis of short-run
dynamics is very important because that is where we get estimates of
speeds of adjustment. For financial markets with rapid speed of adjust-
ment, the long run is indeed short. For goods markets the speeds of
adjustment are perhaps slow for some commodities and fast for others
(there is not much empirical evidence on which commodities adjust fast
which do not). Since the speeds of adjustment in the goods markets
are the ones that are relevant for testing the PPP theory, it is likely
that we would need a long time series (certainly not 100 years!) to
apply the cointegration techniques. For testing the MEH certainly the
data considered in the relevant studies are long enough for cointegration
techniques to be applicable. Even for testing the PPP theory, a time se-
ries of 10-15 years can be considered adequate assuming some plausible
speeds of adjustment in the traded goods markets. Thus, contrary to
the claims in Hakkio and Rush (1991) that we have to wait some years
to have data of adequate time span to use cointegration techniques, the
problem with testing the PPP is not the lack of a long series of data, but
rather the lack of a meaningful hypothesis to start with. For almost all

6.9 Of what use are cointegration tests ? 
237
the problems analyzed by using cointegration, the data are long enough
for cointegration techniques to be applicable.
Another argument why a long time span does not help in estimating
cointegrating vectors is that a long time series is more likely to have
quite a few structural changes, and under parameter changes, a single
cointegrating relationship cannot be considered meaningful. There is
abundant evidence to show that ignoring structural breaks leads to mis-
leading inference on both unit roots and tests for cointegrating relation-
ships. (This is discussed in chapter 13.)
What sort of results have been obtained on long-run economic rela-
tionships using cointegration techniques? The range of applications has
been really wide, from the long-run demand for money, PPP theory and
the permanent income hypothesis to US military expenditures and the
dollar. For instance, Grilli and Beltratti (1989) use quarterly data for
the period 1951:1 to 1986:111 and cointegration techniques to establish a
significant relationship between US military expenditures (real) and the
real exchange rate.
We shall consider two sets of studies - one on the long-run demand
for money and the other on PPP theory. The studies discussed are
not the most interesting in the respective areas, but we have chosen
them because they illustrate some problems in the use of multivariate
cointegration techniques. Given the limitations of bivariate cointegra-
tion relationships mentioned earlier, we shall consider only multivariate
cointegration techniques. (The issue is similar to the use of simple ver-
sus multiple regression.) A commonly used method for this purpose is
the Johansen method (1988). As discussed in chapter 5, this is a flexible
technique that allows one to test for the number of cointegrating vectors,
get the ML estimates of the coefficients, and so on.
Johansen and Juselius (1990) estimate demand for money functions
for Denmark and Finland using quarterly data. For the Danish data the
sample was 1974:1 to 1987:111 (55 observations). For the Finnish data the
sample was 1958:1 to 1984:111 (67 observations). For the Danish data
the order of cointegration was one which simplified the interpretation
of the cointegrating vector as a long-run demand for money function.
But for the Finnish data there were three cointegrating vectors and this
caused problems of interpretation.
The study on demand for money for UK by Cuthbertson and Taylor
(1990) - to be referred to as CT - presented similar problems. The
variables they considered were: the log of nominal M3 (m), the log of
GDP deflator (p), the log of real GDP (y), three month treasury bill rate

238 
Tests for cointegration
(RB), and the own rate on money (RM) which is the maximum of the
seven-day deposit rate and the rate on high interest checking accounts.
They found that m and p were 1(2) but m—p was 1(1) as were y, RB, and
RM. Hence, subsequent analysis was conducted in terms of these four
variables. The Johansen procedure suggested two cointegrating vectors.
The hypothesis of income homogeneity for the two vectors could not
be rejected, but the joint hypothesis of income homogeneity and equal
and opposite signs for the interest rates (arguing that it is the interest
rate differential that is the determinant of the demand for money) was
rejected. The two cointegrating vectors were
(m - p)t = yt- O.URBt + 0.034RM*
and
(m - p)t = yt- 0.36RBt + 0MRMt
CT, however, rejected the second vector saying that the interest elas-
ticities were too high to be plausible and argued that the first equation
represents the long-run demand for money.
Whenever we have more than one cointegrating vector, we have seri-
ous problems of interpretation. In the above example, since any linear
combination of two cointegrating vectors is also cointegrating, there are
an infinite number of elasticities one can generate. Thus, identification
of the demand for money function requires some extraneous informa-
tion. This is not surprising, because, as mentioned earlier, cointegration
techniques are purely statistical in nature, and they belong to the area
of a-theoretical econometrics, much the same way as VAR models do.
An alternative route is not to ask how many cointegrating relations
there are (and what they are) but to test whether a prespecified relation
derived on the basis of economic theory is a cointegrating relationship.
One such candidate in the above example is
(m-p)t=yt-f3(RBt-RMt)
Looking at the second cointegrating vector that CT estimated, it appears
that one would not reject the hypothesis of cointegration. CT claim
that this hypothesis was rejected, but this is surprising. One possible
explanation is that the restriction was tested for both the vectors jointly,
which is not a meaningful hypothesis to test.
This example on the demand for money by CT highlights the prob-
lems one faces when there are more than one cointegrating vector. It
also illustrates the arbitrary way a cointegrating vector was picked up

6.9 Of what use are cointegration tests ? 
239
and exhibited as the long-run demand for money. Podivinsky (1990)
suggests two more problems with the application of the Johansen pro-
cedure: the first, that his tabulated critical values may be inappropriate
when applied to sample size of 100 or smaller; second, that the tests
can be misleading in the presence of omitted variables. Hence, it is
important to include all the plausible variables in the analysis from the
start.
6.9.3 Cointegration and the purchasing power parity
(PPP) theory
The literature on testing the PPP theory is enormous, and in recent
years many of these studies have used the cointegration methods. All
this work, however, is like looking for a black cat in a dark room in which
no cat (black or white) exists. There has been discussion of whether PPP
holds during some time periods (e.g., in the 1920s, in the ninteenth cen-
tury), or some countries (Latin American countries with high inflation
rates), or in one of the exchange rate regimes (fixed exchange rate pe-
riods as opposed to flexible exchange rate periods), and so on. One
argument for devoting so much attention to this theory is that it is one
of the equations in almost all models of exchange rate behavior, and it
continues to be used even in spite of overwhelming evidence against it.
The PPP constrains the exchange rate between two countries to be
proportional to the ratio of the price levels in the two countries. The
PPP theory is closely related to the law of one price which states that
the price of a commodity is the same everywhere in the world. Apart
from the fact that commodities with the same name are not the same ev-
erywhere, and transportation costs also account for differences in prices,
even arbitrage that should bring prices to equality is not instantaneous.
It takes time for economic agents to react to price differences. More-
over, the law of one price does not apply to nontraded goods. Finally,
even if PPP held exactly for each of the traded commodities, the PPP
theory can be violated because of differences in the weights given to the
different commodities in the construction of price indexes in the two
countries. Thus, PPP cannot be considered to be a sensible hypothe-
sis. As Learner (1991) argues, testing a sharp hypothesis like the PPP
at a constant significance level is meaningless to start with. The more
interesting question is not whether PPP holds exactly in the long run,
whatever the term long run means (100 years?) but as to how fast ar-
bitrage eliminates price differences, and for which commodities. The

240 
Tests for cointegration
interesting questions are how far do exchange rates deviate from the
price ratio and for how long. As a theory of the determination of ex-
change rates, the price ratio is only one of the determining variables and
the question is how important is it compared to other variables, such as
money supplies, interest rate differentials, and so on.
The empirical work on PPP using the Johansen procedure illustrates
the problems of obtaining cointegrating relationships when some impor-
tant variables are omitted from the analysis. Johansen and Juselius
(1992) - to be referred to as JJ - analyze the exchange rate for UK
using quarterly observations from 1972:1 to 1987:111. The variables they
considered were: the log of wholesale price index for UK (pi), the log of
trade-weighted foreign price index (P2), the log of the effective exchange
rate (ei2), three month treasury bill rate in UK (ii, and three month
Euro-dollar interest rate (22). They test the PPP which says
P1-P2- 
ei2 = 0
and the uncovered interest parity (UIP) which says
JJ find that there are two cointegrating vectors, one of which can be
identified as (0,0,0, —1,1) indicating that the interest rate difference is
stationary. The hypothesis that the vector (1,-1,-1,0,0) is a cointe-
grating vector is rejected; thus, the PPP theory is rejected. But a vector
of the form (1, — 1, —1, a, b) turns out to be a cointegrating vector. This
can be viewed as describing a modified PPP that includes interest rates.
This they interpret as saying that evidence for PPP can be found only
if we allow for the interaction between the goods and asset markets.
A similar study by Juselius (1991) considers the exchange rate between
Denmark and Germany for the period 1972:1 to 1987:111. This study finds
three cointegrating vectors and support for both PPP and UIP (because
of pegged exchange rates within certain ranges until 1983). Liu (1992)
studies the PPP for nine Latin American countries arguing that because
of the high inflation rates, exchange rates for these countries versus the
US dollar are more likely to be influenced by the relative price level. He
tests a weaker version of the PPP theory, i.e., ei2 = flo+/3iPi-\-f32P2 with
/?i negative and /?2 positive (but not necessarily unity in magnitude).
He finds support for this theory for almost all the countries considered
when the Johansen procedure is used. The interesting thing is that the
estimates of the cointegrating vectors were quite different when they
were estimated by OLS as compared to the Johansen ML procedure.

6.10 Conclusions 
241
There has been some criticism of the Johansen procedure in stud-
ies with exchange rate data. The first criticism is that it assumes ho-
moskedastic errors, which is rarely the case with exchange rates because
of volatility clustering. Pranses et al. (1992) discuss GARCH errors but
only in relation to Engle-Granger tests of cointegration. The second
problem with the Johansen procedure is that it assumes that the errors
are orthogonal across equations. This is also an assumption that is not
likely to be valid with exchage rate data. Abuaf and Jorion (1990) ar-
gue the importance of relaxing this assumption in their study of PPP
(they do not use cointegration methods). For these reasons, Moore and
Copeland (1995) suggest using the Phillips-Hansen method (in prefer-
ence to the Johansen method) arguing that this method is valid under a
wide range of distributional assumptions of the errors. They show that
the conclusions are sensitive to the estimation method used. They also
argue that in this case there is no obvious advantage to be gained by
the fact that the Johansen procedure can accommodate more than one
cointegrating vector.
There have also been several arguments given about using panel data
methods to increase the power of cointegration tests. We discussed the
problems of panel data unit root tests in chapter 4 (section 4.9). Pedroni
(1995) has advanced similar arguments in favor of panel cointegration
methods. Much of the criticism of panel unit root tests also carries over
to panel cointegration tests.
A more fruitful approach to the PPP theory is to analyze the de-
terminants of the deviations from PPP. Koedijk and Schotman (1989)
characterize the pattern of deviations from PPP through the use of prin-
cipal component analysis.
6.10 Conclusions
This chapter discusses tests for cointegration. It would be useful to
distinguish between three types of cointegration tests:
(i) Single equation tests - tests to determine whether a number of
variables are cointegrated.
(ii) Multiple equation tests - tests to determine the number of coin-
tegrating vectors. Johansen's trace test and maximum eigenvalue
test fall in this category.
(iii) Tests for significance of the coefficients of the estimated cointe-

242 
Tests for cointegration
grating vector. These are discussed in greater detail in chapter
10 on bootstrap methods.
Some Monte Carlo studies combine the different tests but it is important
to separate out the results into these different categories because the
results are comparable only for tests within a category.
We have discussed several tests in this chapter. Many are listed here
for completeness. Although tests associated with the Johansen proce-
dure are discussed very often, tests based on levels canonical correlation
analysis (LCCA) are rarely discussed. These are discussed in section
6.6.
A number of cointegration tests have been discussed. Not all are
useful. There have been many Monte Carlo studies comparing the size
and power characteristics of these tests. Not many definitive conclusions
emerge from the Monte Carlo studies. The conclusions that we can
draw are summarized in section 6.5.2 and need not be repeated here.
The Monte Carlo studies do not cover the ECM-based tests discussed in
section 6.3.
An important issue that arises in connection with cointegration tests
is the pre-testing problem. This has not received any attention until
very recently and there is no satisfactory solution as yet. This is an
important issue that needs further study.
References
Abuaf, N. and P. Jorion (1990), "Purchasing Power Parity in the Long
Run," Journal of Finance, 45, 157-174.
Baillie, R.T. and T. Bollerslev (1989), "Common Stochastic Trends in
a System of Exchange Rates," Journal of Finance, 44, 167-181.
Banerjee, A. (1995), "Dynamic Specification and Testing for Unit Roots
and Cointegration," in K. D. Hoover (eds.), Macroeconometrics,
Kluwer, Boston, chapter 12.
Banerjee, A., J.J. Donald, and R. Mestre (1996), "ECM Tests for Coin-
tegration in a Single Equation Framework," Manuscript, Oxford
University.
Bewley, R. and D. Orden (1994), "Alterative Methods for Estimating
Long-run Responses with Applications to Australian Import De-
mand," Econometric Reviews, 13, 179-204.
Bewley, R. and M. Yang (1995), "Tests for Cointegration Based on

References 
243
Canonical Correlation Analysis," Journal of American Statistical
Association, 90, 990-996.
(1996), "On the Size and Power of Systems Tests for Cointegration,"
Manuscript, University of New South Wales, Sydney.
Bossaerts, P. (1988), "Common Nonstationary Components of Asset
Prices," Journal of Economic Dynamics and Control, 12, 347-
364.
Boswijk, H.P. (1992), "Cointegration, Identification, and Exogeneity,"
Ph.D. Dissertation, Tinbergen Institute, No. 37, Tinbergen In-
stitute Research Series.
(1994), "Testing for an Unstable Root in Conditional and Structural
Error Correction Models," Journal of Econometrics, 63, 37-60.
Boswijk, H.P. and P.H. Franses (1992), "Dynamic Specification and
Cointegration," Oxford Bulletin of Economics and Statistics, 54,
369-381.
Campbell, J.Y. and P. Perron (1991), "Pitfalls and Opportunities:
What Macroeconomists Should Know About Unit Roots," in
O.J. Blanchard and S. Fisher (eds.), Macroeconomics Annual,
The MIT Press, Cambridge, MA, 141-201.
Cheung, Y. and K. Lai (1993), "Finite Sample Sizes of Johansen's
Likelihood Ratio Tests for Cointegration," Oxford Bulletin of
Economics and Statistics, 55, 313-328.
Cuthbertson, K. and M.P. Taylor (1990), "Money Demand, Expecta-
tions and the Forward Looking Model," Journal of Policy Mod-
eling, 12, 289-315.
Edison, H.J., J.E. Gagnon, and W.R. Melick (1997), "Understand-
ing the Empirical Literature on Purchasing Power Parity: the
Post-Bretton Woods Era," Journal of International Money and
Finance, 16, 1-17.
Eitrheim, O. (1992), "Inference in Small Cointegrated Systems: Some
Monte Carlo Results," Presented at the Econometric Society
Meeting in Brussels.
Elliott, G. (1995a), "On the Robustness of Cointegration Methods
When Regressors Almost Have Unit Roots," Discussion paper
95-18, Department of Economics, University of California at San
Diego.
(1995b), "Tests for the Correct Specification of Cointegrating Vectors
and the Error Correction Model," Discussion paper 95-42, De-
partment of Economics, University of California at San Diego.

244 
Tests for cointegration
Elliott, G. and J.H. Stock (1994), "Inference in Time-Series Regres-
sion When the Order of Integration of a Regressor is Known,"
Econometric Theory, 10, 672-700.
Engle, R. and C.W.J. Granger (1987), "Cointegration and error cor-
rection: representations, estimation and testing," Econometrica,
55, 252-276.
Engle, R.F. and B.S. Yoo (1987), "Forecasting and Testing in Co-
Integrated Systems," Journal of Econometrics, 35, 143-159.
Franses, P.H., P. Kofman, and J. Moser (1992), "GARCH Effects on
a Test of Cointegration," Working paper TI 93-76, Tinbergen
Institute.
Fuller, W. A. (1976), Introduction to Statistical Time Series, New York,
Wiley.
Gonzalo, J. and T.H. Lee (1995), "Pitfalls in Testing for Long-Run Re-
lationships," Working paper No. 38, Department of Economics,
Boston University.
Gregory, A.W. (1994),"Testing for Cointegration in Linear Quadratic
Models," Journal of Business and Economic Statistics, 12, 347-
360.
Grilli, V. and A. Beltratti (1989), "US Military Expenditures and the
Dollar," Economic Enquiry, 27, 737-44.
Hakkio, C.S. and M. Rush (1989), "Market Efficiency and Cointegra-
tion: An Application to the Sterling and Deutschemark Ex-
change Rates," Journal of International Money and Finance, 8,
75-88.
(1991), "Cointegration: How Short is the Long Run," Journal of
International Money and Finance, 9, 75-88.
Hansen, B.E. (1990), "A Powerful Simple Test for Cointegration Us-
ing Cochrane-Orcutt," Working paper No. 230, University of
Rochester.
(1995), "Rethinking the Univariate Approach to Unit Root Testing,"
Econometric Theory, 11, 1148-1171.
Harris, D. (1997), "Principal Components Analysis of Cointegrated
Time Series," Econometric Theory, 13, 529-557.
Harris, D. and B. Inder (1994), "A Test of the Null of Cointegration,"
in C. Hargreaves (ed.), Non-Stationary Time Series Analysis
and Cointegration, Oxford University Press, Oxford, 133-152.
Haug, A.A (1996), "Tests for Cointegration: A Monte Carlo Compari-
son," Journal of Econometrics, 71, 89-115.

References 
245
Ho, M.S. and B.E. S0rensen (1996), "Finding Cointegration Rank in
High Dimensional Systems Using the Johansen Test: An Illu-
stration Using Data Based Monte Carlo Simulation," The Re-
view of Economics and Statistics, 78, 31-43.
Hooker, M.A. (1993), "Testing for Cointegration: Power versus Fre-
quency of Observation," Economics Letters, 41, 359-362.
Horvath, M.T.K. and M.W. Watson (1995), "Testing for Cointegration
When Some of the Cointegrating Vectors Are Known," Econo-
metric Theory, 11, 984-1014.
Hu, W. (1996), "Time Aggregation and Skip Sampling in Cointegration
Tests," Statistical Papers, 37, 225-234.
Johansen, S. (1988), "Statistical Analysis of Cointegration Vectors,"
Journal of Economic Dynamics and Control, 12, 231-254.
(1991), "Estimation and Hypothesis Testing of Cointegration Vectors
in Gaussian Vector Autoregressive Models," Econometrica, 59,
1551-1580.
(1992), "Determination of Cointegration Rank in the Presence of a
Linear Trend," Oxford Bulletin of Economics and Statistics, 54,
383-397.
Johansen, S. and K. Juselius (1990), "Maximum Likelihood Estimation
and Inference on Cointegration - with Applications to the De-
mand for Money," Oxford Bulletin of Economics and Statistics,
52, 169-210.
(1992), "Testing Structural Hypotheses in a Multivariate Cointegra-
tion Analysis of the PPP and UIP for UK," Journal of Econo-
metrics, 53, 211-244.
Juselius, K. (1991), "Long-Run Relations in a Well Defined Statistical
Model for the Data Generating Process: Cointegration Analysis
of the PPP and UIP Relations between Denmark and Germany,"
in J. Gruber (ed.), Econometric Decision Models: New Methods
of modeling and Application, Springer Verlag.
Kim I.M. (1995), "Pretest Bias in a Cointegrated System," The Korean
Economic Review, Sung Kyun Kwan University, 22, 115-140.
Kleibergen, F. and H.K. van Dijk (1994), "Direct Cointegration Testing
in Error Correction Models," Journal of Econometrics, 63, 61-
103.
Kleibergen, F., J.D. Urbain, and H.K. van Dijk (1995), "A Cointegra-
tion Study of Aggregate Imports Using Likelihood Based Test-
ing Principles," Tinbergen Institute paper No. TI 95-169.

246 
Tests for cointegration
Koedijk, K. and P. Schotman (1989), "Dominant Real Exchange Rate
Movements," Journal of International Money and Finance, 8,
517-531
Kremers, J.J., N.R. Ericsson, and J.J. Dolado (1992), "The Power of
Co-integration Tests," Oxford Bulletin of Economics and Statis-
tics, 54, 325-348.
Lahiri, K. and N. Mamingi (1995), "Testing for Cointegration: Power
versus Frequency of Observation: Another View," Economics
Letters, 49, 121-124.
Layton, A.P. (1993), "A Suggested Reconciliation of Recent Cointegra-
tion Testing of Foreign Exchange Market Spot Rates," Applied
Financial Economics, 3, 353-356.
Learner, E. (1991), "The Interplay of Theory and Data in the Study of
International Trade," in M. Nerlove (ed.), Issues in Contempo-
rary Economics, volume 2: Macroeconomics and Econometrics,
Macmillan, IEA.
Leybourne, S.J. and B.P.M. McCabe (1993), "A Simple Test for Coin-
tegration," Oxford Bulletin of Economics and Statistics, 55, 97-
103.
Liu, P.C. (1992), "Purchasing Power Parity in the Latin Ameri-
can Countries: A Cointegration Analysis," Weltwirtschaftliches
Archiv, 128, 662-680.
MacDonald, R. and M.P. Taylor (1989), "Foreign Exchange Market
Efficiency and Cointegration," Economics Letters, 29, 63-68.
MacKinnon, J.G. (1991), "Critical Values for Co-Integration Tests,"
in R.F. Engle and C.W.J. Granger (eds.), Long-Run Economic
Relationships, Oxford University Press, 267-276.
(1994), "Approximate Asymptotic Distribution Functions for Unit
Root and Cointegration Tests," Journal of Business and Eco-
nomic Statistics, 12, 167-176.
(1995), "Numerical Distribution Functions for Unit Root and Coin-
tegration Tests," Discussion paper No. 918, Queens University.
Maddala, G.S. (1991), "On the Use of Cointegration Tests," Paper
presented at the Bank of Korea, Seoul, August, 1991.
(1992), Introduction to Econometrics, 2nd ed., Macmillan, New York.
Moore, M.J., and L.S. Copeland, (1995), "A Comparison of Johansen
and Phillips-Hansen Cointegration Tests of Forward Market Ef-
ficiency: Baillie and Bollerslev Revisited," Economics Letters,
47, 131-135.

References 
247
Osterwald-Lenum, M. (1992), "A Note with Fractiles of the Asymptotic
Distribution of the Maximum Likelihood Cointegration Rank
Test Statistics: Four Cases," Oxford Bulletin of Economics and
Statistics, 54, 461-72.
Park, J.Y. (1990), "Testing for Unit Roots and Cointegration by Vari-
able Addition," in T.B. Fomby and G.F. Rodes (eds.), Advances
in Econometrics, vol. 8, JAI Press, 107-133.
Park, J.Y. and B. Choi (1988), "A New Approach to Testing for a Unit
Root," CAE Working paper No. 88-23, Cornell University.
Park, J.Y. and P.C.B. Phillips (1988), "Statistical Inference in Regres-
sions with Integrated Processes: Part I," Econometric Theory,
4, 468-497.
Pedroni, P. (1995), "Panel Cointegration: Asymptotic and Finite Sam-
ple Properties of Pooled Time Series Tests with an Application
to the PPP Hypothesis," Mnuscript, Indiana University.
Perron, P. and J.Y. Campbell (1993), "A Note on Johansen's Cointe-
gration Procedure When Trends Are Present," Empirical Eco-
nomics, 18, 777-789.
Phillips, P.C.B. (1994), "Some Exact Distribution Theory for Maxi-
mum Likelihood Estimators of Cointegrating Coefficients in Er-
ror Correction Models," Econometrica, 62, 73-93.
Phillips, P.C.B. and S. Ouliaris (1988), "Testing for Cointegration Us-
ing Principal Components Methods," Journal of Economic Dy-
namics and Control, 12, 205-230.
(1990), "Asymptotic Properties of Residual Based Tests for Cointe-
gration," Econometrica, 58, 165-193.
Phillips, P.C.B. and P. Perron (1988), "Testing for a Unit Root in Time
Series Regression," Biometrika, 75, 335-346.
Podivinsky, J.M. (1990), "Testing Misspecified Cointegrating Relation-
ships," Working Paper No. 19, Monash University, Australia.
Reimers, H.E. (1992), "Comparisons of Tests for Multivariate Cointe-
gration," Statistical Papers, 33, 335-359.
Saikkonen, P.,(1992), "Estimation and Testing of Cointegrated Systems
by an Autoregressive Approximation," Econometric Theory, 8,
1-27.
Sephton, P.S. and H.K. Larson (1991), "Tests of Exchange Market
Efficiency: Fragile Evidence from Cointegration Tests," Journal
of International Money and Finance, 10, 561-570.

248 
Tests for cointegration
Shin, Y. (1994), "A Residual-Based Test of the Null of Cointegra-
tion Against the Alternative of No-Cointegration," Econometric
Theory, 10, 91-115.
Steigerwald, D.G. (1996), "Purchasing Power Parity, Unit Roots and
Dynamic Structure," Journal of Empirical Finance, 2, 343-357.
Stock, J.H. and M.W. Watson (1988), "Testing for Common Trends,"
Journal of the American Statistical Association, 83, 1097-1107.
Toda, H.Y. (1994), "Finite Sample Properties of Likelihood Ratio Tests
for Cointegrating Ranks When Linear Trends are Present," Re-
view of Economics and Statistics, 76, 66-79.
(1995), "Finite Sample Performance of Likelihood Ratio Tests for
Cointegration Ranks in Vector Autoregression," 
Econometric
Theory, 11, 1015-1032.
Xiao, Z. and P.C.B. Phillips (1996), "Efficient Detrending in Cointe-
grating Regression," Manuscript, Cowles Foundation, Yale Uni-
versity.
Yang, M. and R. Bewley (1996), "On Cointegration Tests for VAR
Models with Drift," Economics Letters, 51, 45-50.
Zivot, E. (1994), "Single Equation Conditional Error Correction Model
Based Tests for Cointegration," Discussion paper 94-12, Depart-
ment of Economics, University of Washington, Seattle.

Econometric modeling with
integrated regressors
In the previous chapter we discussed estimation methods for cointegrated
systems. In the models considered, the variables were all known to be
1(1). Also, in the Phillips triangular system, the number of cointegrating
vectors is known. In the Johansen method, on the other hand, the
number of cointegrating vectors is estimated (it is not assumed known).
In this chapter we will consider estimation of models with a mixture
of 1(1) and 1(0) variables as well as problems of inference when there is
uncertainty about the unit roots, that is, when we do not know whether
a variable is 1(1) or 1(0). First we will deal with the case where we know
the classification of the variables into the 1(1) and 1(0) categories. Next,
we consider problems where this classification is unknown. Problems
with 1(2) variables are treated in chapter 11.
The asymptotic theory relevant for the discussion of the problems
discussed here is rather lengthy and hence we shall summarize only the
main results. Details on the asymptotic theory can be found in the
papers cited.
7.1 1(1) regressors not cointegrated
For simplicity of exposition, we shall start with a model with one 1(1)
and 1(0) regressor. The model is
yt = <x + Pxt+'yzt+et 
(7.1)
where et ~ 1(0), zt ~ 1(0), Xt ~ 1(1) and zt and St are uncorrelated.
This model has been studied in Park and Phillips (1988, 1989) and Sims
et al. (1990). The main results are as follows:
249

250 
Econometric modeling with integrated regressors
(i) Note that *yzt + £t is 1(0). Hence f3 can be consistently estimated
ignoring zt. Let /3 be the estimator of ft ignoring zt in (7.1).
(ii) Suppose we estimate (7.1) by OLS. Let (/3,7) be the estimators
of (3 and 7. Then the distributions of J3 and 7 are independent.
This follows from the differences in the normalization factors and
has been discussed in section 3.6.1 of chapter 3.
(iii) The distribution of ft is of the same form as that of /3. However,
it is not true that the asymptotic distributions of T(J3 — (3) and
T((3 — (3) are the same because the errors are different.
The fact that the asymptotic distribution of the coefficients of the
1(1) and 1(0) regressors are independent and that the distribution the-
ory for the coefficients of the 1(0) regressors is standard, can be used
conveniently to determine the lag length in an autoregressive model.
For example, suppose we consider the AR model
yt = Piyt-i + thyt-2 + Psyt-s + et
where yt is 1(1) and we are interested in testing the hypothesis /?3 = 0,
i.e., the lag length is 2 rather than 3. We can write this equation as
yt = Piyt-i + (P2 + Ps)yt-2 - P^yt-2 + et
Now Ayt-2 is an 1(0) variable and hence the distribution of VT((3s — (3S)
will follow the standard normal distribution theory. Hence hypothesis
tests of the Wald type based on the asymptotic normal distribution can
be used in testing the hypothesis (3s = 0 by estimating a regression equa-
tion regressing yt on 2/^—1? 2/t—2? and Ayt-2. 
This sort of reformulation
can be done for testing any one lag coefficient in a model with k lags.
7.2 1(1) regressors cointegrated
We shall next consider the case where the 1(1) regressors are cointe-
grated. Consider again for simplicity
y = a + (3\X\ + (32x2 + /?3#3 + jzt + et 
(7.2)
As before, we assume zt and et are independent and 1(0). Suppose x2
and xs are cointegrated, i.e., x2-\-8xs = w\$ 1(0). We can write equation
(7.2) as
y = a + /?ixi + (/3s - (328)x3 + (32w + jzt + et 
(7.3)
It would appear that all the parameters in (7.3) can be consistently
estimated. The regression of x2 on X3 gives an estimator 8 of 6 which is

7.3 Unbalanced equations 
251
superconsistent and hence the estimators of the parameters in (7.3) with
8 substituted for 6 would give consistent estimators of all the parameters.
However, as discussed in Wooldridge (1992) it is not true that all the
parameters in (7.3) can be consistently estimated unless w is uncorre-
lated with et. In general, the error term et has to be uncorrelated with
all stationary linear combinations of the regressors.
As for tests for cointegration when some regressors are cointegrated,
note that we apply tests for CI using equation (7.3) rather than (7.2).
7.3 Unbalanced equations
An unbalanced equation is one in which the regressand is not of the same
order of integration as the regressors or any linear combination of the
regressors. A requirement in order to obtain a meaningful estimation
with integrated variables is balance in the orders of integration of the
variables on the left-hand side and right-hand side of the regression
equation. Consider a regression where the dependent variable is 1(0)
and the explanatory variables are 1(1). Pagan and Wickens (1989, p.
1002) argue that the disturbance will also be 1(1) if there is only one
regressor, and that to achieve an 1(0) disturbance, there must be at least
two 1(1) regressors.
Banerjee et al. (1993) study the features of regression among series
with various orders of integration. Table 7.1 shows some of their results
based on the model
yt = a + (3xt + ut, 
ut ~ im(0,1)
The last column of table 7.1 indicates the probability of rejecting the
null hypothesis of (3 = 0 at the conventional significance level. Case (ii)
is spurious regression discussed in section 2.5 of chapter 2. But spurious
regression can also arise in a regression of an 1(2) variable on another
independent 1(2) variable as in case (iii) or regression of 1(2) on 1(1) and
vice versa as in cases (vi) and (vii). When an 1(0) series is regressed on
an 1(1) series as in case (iv), Banerjee et al. argue that the only way in
which OLS can make the regression consistent and minimize the sum of
squares is to drive the coefficient of the 1(1) variable to zero.
In the Monte Carlo simulations reported in Banerjee et al., they obtain
the histogram for the regression coefficient of an 1(2) series regressed on
an 1(1) series, and show that the empirical distribution of this regression
coefficient is long-tailed and peaked, i.e., distinctly nonnormal. They

252 
Econometric modeling with integrated regressors
Table 7.1. Features of regressions among series with various orders of
integration
Case 
y 
x 
Pr[t(/? = 0) > 2]
(i) 
1(0) 
1(0) 
0.0493
(ii) 
1(1) 
1(1) 
0.7570
(iii) 
1(2) 
1(2) 
0.9406
(iv) 
1(0) 
1(1) 
0.0458
(v) 
1(1) 
1(0) 
0.0486
(vi) 
1(2) 
1(1) 
0.8530
(vii) 
1(1) 
1(2) 
0.8444
Source-. Banerjee et al. (1993, table 3.1, p. 79).
also report the results of several other Monte Carlo studies on unbal-
anced equations (pp. 164-8). Marmol (1996) derives analytically the
distributions of the regression coefficients, the Durbin-Watson statistic
and the correlation coefficient. He shows that the Monte Carlo results of
Banerjee et al. (1993) agree with his analyzed results. Haldrup (1994)
also contains some asymptotic results on regression with 1(2) and 1(1)
variables.
Should one estimate unbalanced regressions? Of course not, if it can
be avoided. But if it has to be done, one has to be careful in their
interpretation and use appropriate critical values. Banerjee et al. (1993,
p. 166) say:
The fact that a regression is unbalanced may not be a matter of concern,
for example, ADF statistics are computed from models that are unbalanced.
They are nonetheless valid tools of inference as long as the correct critical
values are used.
Perhaps Banerjee et al. are more optimistic than others, regarding un-
balanced equations.
7.4 Lagged dependent variables: the ARDL model
Models with lagged dependent variables and serially correlated errors
have a long history in econometrics. The problem of biases in the OLS
estimation of lagged dependent variable models with serially correlated
errors have been studied extensively. The extension of these models
to the case of 1(1) regressors can be analyzed within the framework
of regressions with cointegrated regressors (discussed in the previous

7.4 Lagged dependent variables: the ARDL model 
253
sections) for which asymptotic theory has been developed in Park and
Phillips (1989) in a general framework. However, from the practical
point of view it would be interesting to look at the expressions for the
asymptotic biases in the least squares estimates.
Maekawa et al. (1996) investigate the asymptotic properties of the
OLS estimator, the DW test statistic, and Durbin's h-test statistic in
the following model
yt = otyt-i + 0zt + lit, \OL
zt = zt-i +et
where vt ~ 7iV(0,cr^), et ~ IN (0,(7%) and they are independent. This
model was considered earlier by Maddala and Rao (1973) with stationary
zt, i.e., zt was specified by
zt = Xzt-i +et, \X\ < 1
Thus the expressions for the asymptotic biases derived there could be
expected to hold with A = 1. Maekawa et al. find that this is indeed the
case. Define
2
and
1 
[ 
(1 + apK2
Then they show that
plim a = a + 7
plim $ = (3 + 6
plim p = p — 7
plim DW = 2(1 - p + 7)
where
7=P 
a n d 
6 = 
-
^
These expressions agree with those derived in Maddala and Rao (1973)
substituting A = 1 (0 = 1 in their notation). If p = 0, then a,/3,
and p are consistent. Also, DW—> 2. Under the assumption p = 0 the
distributions of y/T(a — a) and VT(l3 — (3) are asymptotically normal.

254 
Econometric modeling with integrated regressors
Maekawa et al. show that in this model, if p ^ 0 then Durbin's two-
step procedure produces consistent estimates. An alternative procedure
is to use the Bewley and Wickens-Breusch procedure discussed in chapter
2.
7.5 Uncertain unit roots
Throughout the preceding discussion we assumed that we knew whether
a variable belonged to the 1(0) or 1(1) category. However, in practice
there is some uncertainty on this issue. Elliott and Stock (1994) examine
this issue. Let Xt be a variable about which there is some uncertainty
whether it is 1(1). We are ultimately interested in the coefficient of Xt-i
in a regression of yt on Xt-i- The motivation for this problem is that
in the finance literature, it is often considered with yt denoting excess
stock returns and xt denoting dividend yields. A common empirical
observation is that lagged dividend yield is a significant explanatory
variable for excess stock returns. However, there are some questions
about the validity of this inference if xt is 1(1).
The model that Elliott and Stock consider is
xt = Vi + pxt-i +Vit
Vt = /i2+7^t-i +^2t
Let 8 = corr(rjit1 Tj2t)- This is a measure of endogeneity of the regressor
Xt-\. There are two approaches to follow:
(i) Just proceed with testing 7 = 0 ignoring the unit root problem.
(ii) Apply a pre-test for a unit root in xt. If the unit root null is
rejected, then the 1(0) normal distribution is used. If the unit root
null is not rejected, then the 1(1) distribution (under endogeneity)
is used.
With procedure (i), as theory predicts, there were no appreciable size
distortions when 8 = 0, even at high values of p. However, when 8 was
high and p was high, there were substantial size distortions. For example,
when 8 = —0.9,p = 0.95 and T=50, they found rejection rates under
1 percent in the left tail and 22 percent in the right tail when using a
standard normal (5 percent, 95 percent) critical values.
As for procedure (ii), Elliott and Stock consider a first-stage one-sided
DF t-test to test for unit root in xt. At the second stage they used an
equal tailed 10 percent nominal significance level, i.e., (5 percent, 95

7.5 Uncertain unit roots 
255
percent) quantiles of the 1(0) and 1(1) distributions depending on the
outcome of the DF t-test. With 6 = 0 both distributions are normal so
that the same critical values (±1.645) were used whether or not the DF
t-test rejected the unit root null. In this case, as expected, there were no
size distortions, e.g., for 8 = —0.9 and p = 0.9, the empirical size was 30
percent even for T = 100. Elliott and Stock suggest a Bayesian solution
to this pre-testing problem based on a Bayesian mixture approximation.
We shall discuss this in chapter 9 when we talk of the Bayesian approach
to unit roots.
What this suggests is that both the procedures (i) and (ii) result
in substantial size distortions, when 8 is high. The fact that the true
significance levels are higher than the nominal levels means that if as
was observed 7 was found to be significant at the 5 percent level, it is
not significant at the 5 percent level but at a much higher level (say
even 30 percent). Thus the evidence on the predictive ability of lagged
dividends in predicting excess stock returns is weaker than the values
suggested by the significance of the usual t-values.
Cavanagh, Elliott, and Stock (1995) investigate further the magni-
tudes of size distortions in the second-stage test of the hypothesis 7 = 0
for different values of c in the local to unity model p = 1 + c/T. They
first derive the asymptotic sizes of the pre-test procedure as a function
of c and 8. These asymptotic sizes for different values of c and 6 are
tabulated in table 1 of their paper. For 8 small, say 8 < 0.4 in the
demeaned case and 8 < 0.3 in the detrended case, the asymptotic size
distortions of the two-step procedure are small and are possibly negligi-
ble for empirical work. However for 8 = 0.9 in the detrended case, the
rejection rate for the two-stage test with nominal size of 5 percent is
37 percent when c = 20 and the maximal rejection rate (empirical size)
over all values of c is 64 percent. Cavanagh et al. argue that these size
distortions also persist when alternatives to the DF t-test are used. The
alternatives they consider are those suggested by Stock and by Phillips
and Ploberger for choosing between 1(0) and 1(1) specifications, which
depend on Bayesian methods. These are discussed in chaper 9.
Cavanagh et al. discuss alternatives to the two-step procedure that
exhibit large size distortions. They suggest several bounds tests which
provide conservative critical values and confidence intervals. Their sum-
mary finding is that the bounds procedures correct the size distortions
but at a cost of substantial power loss. Of the different bounds tests
they considered the sup-bounds procedure was the best. This procedure
is as follows: denote by £7 the t-test statistic for testing 7 = 0. For

256 
Econometric modeling with integrated regressors
ease of notation we shall denote it by t. Let dt,c,a be the asymptotic
critical value for the test 7 = 0 at significance level a, for the value c in
p = l + C/T. Let
da(L) = inf dt, c,a
and
da(M) = supdt^a
c
(L denotes lower limit and M denotes maximum). If t $(da(L), 
di-a(M))
a test of 7 = 0 with asymptotic level 2a will reject the null regardless
of the value c. Values of t within the conservative acceptance region
(da(L), di-a(M)) but outside the acceptance region (dt>a(L), dtii-a(M))
constitute an indetermined region. Cavanagh et al. tabulate (in table
2 of their paper) the upper and lower bounds (da(L),di_a(M)) for se-
lected values of p and asymptotic significance levels 2a = 5 percent and
10 percent.
7.6 Uncertain unit roots and cointegration
In sections 7.1 and 7.2 we discussed models where there was certainty
about the cointegrating relationships among the 1(1) variables. Kita-
mura and Phillips (1997) develop instrumental variable methods that
are applicable where there is uncertainty about the order of integration
in the variables, as well as uncertainty regarding the number and lo-
cation of the cointegrating relationships. They suggest FM-GMM and
FM-GIVE estimators. These are generalizations of the FM-IV estimator
to the GMM (generalized methods of moments) and the GIVE (general-
ized instrumental variable estimator) used with stationary regressors in
econometric work. The FM-OLS and FM-IV estimators in the Phillips
and Hansen paper (see chapter 5) were developed for the case where all
the variables were 1(1). Phillips (1991, 1995) proved that the FM pro-
cedure can be applied to models with cointegrated regressors, models
with 1(1) and 1(0) regressors, and even to models with only stationary
regressors, without losing the good asymptotic properties of the FM
method. This idea is utilized in the development of the FM-GMM and
FM-GIVE methods. We shall not reproduce the detailed expressions for
these methods here. A practical guide to the fomulas for empirical imple-
mentation is described in detail in the paper by Kitamura and Phillips.
The reason why instrumental variables are introduced is to allow for
endogeneity of the regressors (especially the stationary regressors).

7.6 Uncertain unit roots and cointegration 
257
Kitamura and Phillips (1995) present results of a simulation study to
show the practical usefulness of the FM-GMM and FM-GIVE estima-
tors. The asymptotic distributions of the estimators for the coefficients
of the stationary regressors are standard normal distributions, and the
asymptotic distributions of the estimators of the coefficients of the non-
stationary regressors are mixed normal distributions.
However, although in the estimation stage one can ignore the issues
of unit roots and cointegration relationships among the regressors, when
it comes to hypothesis testing we still need to know which asymptotic
distributions to use - the standard normal or the mixed normal. The
uncertainty about the unit roots becomes important again at the hy-
pothesis testing stage. This problem is discussed in Kitamura (1994),
who develop a test statistic which has a standard normal distribution no
matter where unit roots are located. The test statistic is always asymp-
totically x2 distributed with degrees of freedom equal to the number of
restrictions. Kitamura also presents results of a simulation study sup-
porting the theoretical results.
The bounds procedures by Cavanagh et a/., discussed in the previ-
ous section, were also designed to make (conservative) inferences in the
presence of uncertainty about unit roots. However, these authors noted
substantial loss of power by using these bounds tests. In the case of the
vector autoregression model, Phillips (1995) also develops conservative
bounds procedures. He considers the FM-VAR estimation method and
shows that its optimal properties do not depend on the number and
location of unit roots or cointegration relationships among the 1(1) vari-
ables. He shows that when testing hypotheses using the Wald statistics,
if q is the number of restrictions tested then the x2 distribution is an
upper bounds variable and therefore the usual x2 critical value can be
used to construct tests that have conservative size.
The tests that Kitamura constructs are not based on bounds proce-
dures. His test involves the difference of the restricted minimized value
of the GMM objective function and the unrestricted minimized value of
the GMM objective function using the score vectors obtained from the
FM-GMM procedure as weights. It is thus a likelihood ratio type statis-
tic but not exactly. (It depends on the difference rather than the ratio
of two quantities.) The details of Kitamura's test are very intricate and
will not be discussed here. Readers are referred to Kitamura's paper.
Kitamura also has a computer program to calculate the test statistic.
One other paper that considers the problem of inference under un-
certainty about unit roots and cointegration relationships is the paper

258 
Econometric modeling with integrated regressors
by Toda and Yamamoto (1995). They consider the problem of hypo-
thesis tests involving restrictions on the parameters of a VAR when
there is uncertainty about the number of unit roots and cointegration
relationships. They suggest estimating the VAR model in levels, choos-
ing the lag length k by using the usual procedures for determining the
lag length. Having determined the lag length k, they suggest estimating
a (k + dmax^h. order VAR where dmax is the maximal order of integra-
tion that we suspect might occur in the process. The coefficient matrices
of the last dmax lagged vectors in the model are ignored and we can test
linear or nonlinear restrictions on the first k coefficient matrices using
standard asymptotic theory.
We, thus, have different procedures for making inference under uncer-
tainty about unit roots and cointegrating relationships in the context of
a VAR model. Practical applications of these procedures and compara-
tive studies of the different procedures will be forthcoming in the near
future.
7.7 Summary and conclusions
This chapter deals with problems in regressions with 1(1) and 1(0) vari-
ables, and unbalanced equations. We next discuss the role of lagged
dependent variables in a regression with 1(1) regressors.
We also discuss the important problems of uncertain unit roots and
estimation and testing procedures under uncertainty about unit roots
and cointegration. The usual procedure is to apply a unit root test and
if the null is not rejected at the conventional 1 percent or 5 percent sig-
nificance levels, then to assume (with certainty) that a unit root exists.
The uncertainty about unit roots is completely ignored. Thus, discussion
of the consequences of uncertainty about unit roots and cointegration is
very important
References
Banerjee, A., J.J. Dolado, J.W. Galbraith, and D.F. Hendry (1993),
Cointegration, Error Correction, and the Econometric Analysis
of Nonstationary Data, Oxford University Press, Oxford.
Cavanagh, C.L., G. Elliott, and J.H. Stock (1995), "Inference in Models
with Nearly Nonstationary Regressors," Econometric Theory,
11, 1131-1147.

References 
259
Elliott, G. and J.H. Stock (1994), "Inference in Time-Series Regre-
ssion When the Order of Integration of a Regressor is Known,"
Econometric Theory, 10, 672-700.
Haldrup, N. (1994), "The Asymptotics of Single-Equation Cointegra-
tion Regression with 1(1) and 1(2) Variables," Journal of Econo-
metrics, 63, 153-181.
Kitamura, Y. (1994), "Hypothesis Testing in Models with Possi-
bly Non-Stationary Processes," Manuscript, University of Min-
nesota.
Kitamura, Y. and P.C.B. Phillips (1995), "Efficient IV Estimation
in Non-Stationary Regression: An Overview and Simulation
Study," Econometric Theory, 11, 1095-1130.
(1997), "Fully Modified IV, GIVE, and GMM Estimation with Pos-
sibly Non-Stationary Regressors and Instruments," Journal of
Econometrics, 80, 85-124.
Maddala, G.S. and A.S. Rao (1973), "Tests for Serial Correlation in Re-
gression Models with Lagged Dependent Variables and Serially
Correlated Errors," Econometrica, 47, 761-774.
Maekawa, K., T. Yamamoto, Y. Takanehi, and M. Katanaka (1996),
"Estimation in Dynamic Regression with an Integrated Pro-
cess," Journal of Statistical Planning and Inference, 49, 279-
303.
Marmol, F. (1996), "Nonsense Regressions Between Integrated Pro-
cesses of Different Orders," Oxford Bulletin of Economics and
Statistics, 58, 525-536.
Pagan, A.R. and M.R. Wickens (1989), "A Survey of Some Recent
Econometric Methods," The Economic Journal, 99, 962-1025.
Park, J.Y. and P.C.B. Phillips (1988), "Statistical Inference in Regres-
sions with Integrated Processes: Part I," Econometric Theory,
4, 468-497.
(1989), "Statistical Inference in Regressions with Integrated Pro-
cesses: Part II," Econometric Theory, 5, 95-131.
Phillips, P.C.B. (1991), "Time Series Regression with Cointegrated Re-
gressors," Yale University.
(1995), "Fully Modified Least Squares and Vector Autoregreesion,"
Econometrica, 63, 1023-1078.
Sims, C.A., J.H. Stock, and M.W. Watson (1990), "Inference in Linear
Time Series Models with Some Unit Roots," Econometrica, 58,
113-144.

260 
Econometric modeling with integrated regressors
Toda, H.Y. and T. Yamamoto (1995), "Statistical Inference in Vector
Autoregressions with Possibly Integrated Processes," Journal of
Econometrics, 66, 225-250.
Wooldridge, J.M. (1992), "Notes on Regression with Difference-
Stationary Data," Manuscript, Michigan State University.

Part III
Extensions of the basic model
This part consists of five chapters that consider various extensions of the
basic models discussed in part II.
Chapter 8 discusses the Bayesian approach to unit roots and coin-
tegration. In chapters 4 and 7 we mentioned the problems created by
uncertain unit roots. The Bayesian approach where the posterior proba-
bilities of a unit root and stationarity are taken into account, is a natural
solution to this problem. The chapter discusses the different priors and
their merits and defects in the analysis of unit roots and cointegration.
It also discusses the Bayesian model selection approach.
Chapter 9 is on fractional unit roots and fractional cointegration.
These models are alternative ways of modeling persistence in economic
time series. The chapter discusses different estimation methods and the
empirical relevance of fractional integration.
Chapter 10 is on the bootstrap approach. Almost all the literature on
unit roots and cointegration is asymptotic. However, it has also been
found that the estimators suggested are biased, and the tests exhibit
substantial size distortions. The bootstrap method helps in correcting
these two problems. This chapter discusses the different methods of
bootstrap data generation and bootstrap-based tests.
Chapter 11 is on cointegrated system with 1(2) variables. It has been
observed that some variables like prices are more appropriately charac-
terized as 1(2) processes rather than 1(1) processes. This chapter dis-
cusses problems of analysis with 1(2) variables and mixtures of 1(1) and
1(2) variables.
Chapter 12 is on seasonal unit roots and seasonal cointegration. Most
economic time series at the quarterly and monthly level exhibit seasonal
patterns. Thus it is important to extend the unit roots and cointegration
concepts discussed in part II to seasonal data. It is also true that seasonal
261

262 
777 Extensions of the basic model
patterns are not constant over time. In this case one considers the
periodic models and extends the concepts of unit roots and cointegration
to periodic models. The chapter also discusses an alternative model for
changing seasonal pattern - the evolving seasonal model.

8
The Bayesian analysis of stochastic trends
The recent intensive research in unit root econometrics has focused on
two implications. One is the economic implication about the effect of
shocks on the economy. In models with unit roots, shocks have persistent
effects that last forever, while in the case of traditional trend-stationary
models, shocks only have a temporary effect. The other, statistical impli-
cation is that the presence of unit roots in the data greatly complicates
statistical inference. As we have discussed in the previous chapters,
the OLS estimators and the corresponding statistics have nonstandard
asymptotic distributions under the presence of unit roots, while they
have the standard distributions without unit roots.
The Bayesian approach has cast considerable doubt on the statistical
implications of the unit root in economic time series. Sims (1988) argued
that because the asymptotic distribution theory changes discontinuously
between the stationary and the unit root cases, classical hypothesis test-
ing based on asymptotic theory cannot deliver reasonable procedures for
inference based on the discontinuous asymptotic theory. He argued that
the simple flat prior Bayesian theory is both a more convenient and a
logically sounder starting place for inference than classical hypothesis
testing. In reply to Sims' criticism, Phillips (1991) argued that the rea-
son for the discrepancy between the results obtained by Bayesian and
classical methods on the existence of unit roots in US macroeconomic
time series is due to the use of the flat prior in the Bayesian analysis
and that the flat prior is not a satisfactory representation of uninfor-
mativeness. He suggested an ignorance prior (which is Jeffreys' prior),
which showed that most US macroeconomic time series have a unit root
under the Bayesian analysis as well. The special issue of the Journal
of Applied Econometrics (1991, volume 6, number 4) is devoted entirely
to this debate. There have subsequently been other special issues of
263

264 
The Bayesian analysis of stochastic trends
journals devoted to this topic. (See the issue on Bayesian inference in
Econometric Theory, 1994, volume 10 and also Journal of Econometrics,
1995, volume 69, number 1.)
In this chapter we shall begin with a review of Bayesian methods for
time series analysis. After reviewing the main points of the debate in
1991, we consider the recent developments in Bayesian inference on unit
roots and cointegration as well as on the model selection approach.
8.1 Introduction to Bayesian inference
Most researchers use classical methods not because these methods are
superior to Bayesian methods, but because most of the econometric
literature is in the classical tradition and because, the classical methods
were the first ones (in many cases) and the only ones they were taught.
The basis of all Bayesian inference is Bayes theorem: posterior prob-
ability varies with prior probability times likelihood. It is extended in
the context of distributions as: posterior distribution p(0\y) varies with
prior distribution TT(6) times the likelihood function L(y\6)
ex n(8)L(y\e)
Bayesian methods take the data as given, but regard the true parameter
as random, while classical methods view the true parameter of interest as
unknown and fixed, and study the behavior of its estimator in repeated
samples. All Bayesian inference is based on the given sample and the
posterior distribution. If many parameters are involved, one has to take
the joint posterior distribution of all these parameters and integrate
out all but the one of interest to get the marginal distribution of this
parameter. Then inferences about this parameter can be made from this
marginal distribution.
Given a particular model, the sample information is given in the like-
lihood function L(y\6). In Bayesian methods we have an additional
ingredient: the prior distribution TT(0). There can be various specifica-
tions of the prior distribution about 0, while the form of the likelihood
function depends on the probability distribution underlying the data. In
practice, however, only a few candidates are actually useful and used.
The most commonly used forms of prior distribution are a conjugate
prior and a diffuse prior. A conjugate prior is a prior that when com-
bined with the likelihood function yields a posterior distribution that
has the same functional form as the prior. The advantage of this prior
is that every time a new sample is observed, the revision of opinions

8.1 Introduction 
to Bayesian 
inference 
265
about 6 can be performed by the same analytical procedure. Raiffa and
Schlaifer (1961) contains a detailed discussion of conjugate priors.
When there is not any prior notion about #, what is suggested is a
diffuse or noninformative prior. When we know nothing a priori about
the parameters of the model, we let the variances of the prior distribution
of 8 increase without limit. The prior distribution is uniform over an
unspecified range and is given by
TT(6) OC a constant
This prior is also called an improper prior or a flat prior because it does
not integrate to 1.
Jeffreys (1961) suggested that for location parameters p(0) be taken
proportional to a constant and for scale parameters, that the prior be
taken as proportional to their inverses. For instance, for a sample from
a normal population N(n,a2) 
the diffuse prior is
7T(/X,(7)CX- 
( 8 . 1 )
a
In essense it is log a that has a uniform distribution. This prior is
invariant to transformations of the form <j> = an, since da = nan~1 and
thus
dcf) 
da
(f) 
a
This invariance is important because some parameterize the model in
terms of the standard deviation and others in terms of the variance a2.
Jeffreys also suggested a general invariant prior in a multiparameter
case. He suggests (Jeffreys, 1961, p. 179) that the pdf for the parameter
vector be taken as
p(0) oc |J(60|1/2 
(8.2)
where
is Fisher's information matrix for 6. In this case the prior pdf will be
invariant in the sense that if we parameterize the model in terms of
7] = F{6) with a one to one correspondence between 77 and 0, and take
our prior for 77 as
TTfa) OC 
1/2
then the posterior distribution of 77 will be consistent with the posterior

266 
The Bayesian analysis of stochastic trends
distribution of 9 starting with the prior (8.2). We shall omit the proof
and other properties of invariant prior distributions here (see Zellner,
1971, pp. 48-50).
Often the results obtained by Bayesian methods with the use of diffuse
priors coincide with the results obtained by the classical procedures (Jef-
freys, 1961; Lindley, 1965; Zellner, 1971). However, they differ sharply
in the case of nonstationary time series, which is the source of the de-
bate between the two groups. The difference between the two groups is
closely related to the concept of a noninformative prior in the analysis
of time series, i.e., which prior is noninformative in the analysis of time
series. Sims (1988) used a flat prior as a noninformative prior, as did
Zellner (1971, pp. 186-188). On the other hand, Phillips (1991) argued
that a flat prior in the analysis of time series is informative and proposed
an ignorance prior or Jeffreys' invariant prior as a noninformative prior
for time series models. The basic argument of Phillips is that the autore-
gressive parameter p in the equation yt = pyt-i + ut is not on the same
footing as (3 in the regression model yt = fixt + Ut because the sample
is more informative about p in the different ranges. When \p\ is large,
the data would be more informative about p. Thus in treating all values
of p as equally likely, the flat prior unwittingly carries information that
downweights large values of p. Flat priors, instead of being noninforma-
tive are in fact informative by effectively downplaying the possibility of
unit root and explosive alternatives.
8.2 The posterior distribution of an autoregressive parameter
Consider the simple AR(1) model
Vt =pyt-i+£t
Conditioning on the initial value yo, the Gaussian likelihood follows the
density
L(y\p,a,y0) = (2.)-
Assume a flat prior for (p, a)
7r(p, O) OC - , 
- 1 < p < 1, a > 0
a

8.2 The posterior distribution of an autoregressive parameter 267
The joint posterior distribution for (p, a) is then given by
P(P,a\y,yo) oc a-™ exp 
^
Let p be the OLS estimator of p so that p = YlVtUt-i/ 
YlVt-i- 
Then
we can write
where Q = J^t/t-i an<^ R = Y^^t 
1S ^ n e residual sum of squares with
it = yt — pyt_i. The joint posterior distribution can be written as
p(p,cr|y,2/0) o^cr 
exp 
——^
L 
LG 
J
The marginal posteriors are obtained after integrating out the other
parameters and are given by
, y0) oc a~T exp ( -
The marginal posterior distribution for p is a univariate t-distribution, p
is symmetrically distributed about the OLS estimate, and the variance
of p is R/(T — 3)<2, which decreases as Q increases. The marginal pos-
terior distribution for a is an inverted gamma-2 distribution (Raiffa and
Schlaifer, 1961; Zellner, 1971).
This framework has been used widely in the analysis of time series.
Thornber (1967) and Zellner (1971) used this framework and empha-
sized its applicability for stationary and nonstationary cases. Geweke
(1988) used the same approach in a cross-country applied study but used
a restricted domain in addition to the flat prior. Sims (1988) and Sims
and Uhlig (1991) also use this framework, although in the latter paper
the model is even simpler because a is assumed to be known for compu-
tational convenience. Schotman and van Dijk (1991b) employ a similar
approach in studying real exchange rate data. They modify a flat prior
so that p has a flat prior in a proper subset of the stationary interval
and they assign a discrete prior probability mass to p = 1 (values of p
in the explosive range being excluded).
The Bayesian methods focus on the posterior distribution of the true
parameter by taking the data and estimator as given, while the classical
methods concentrate on the behavior of the estimators in repeated sam-
ples viewing the true parameter of interest as unknown and fixed. Thus,

268 
The Bayesian analysis of stochastic trends
under the presence of unit roots, while the classical methods are based on
the asymmetric nonstandard distribution, for example the Dickey-Fuller
distribution (see chapter 3) of the OLS estimators of autoregressive pa-
rameters, the Bayesian methods use the symmetric standard (posterior)
distribution of true autoregressive parameters. The difference between
the two approaches is well described by comparing the two distributions,
p(p\p = 1) and p(p\p = 1) (Sims and Uhlig, 1991).
Sims and Uhlig (1991) compute the joint posterior distribution for p
and p under a flat prior by Monte Carlo methods. For computational
simplicity they assume that a2 = 1 and known. They show the asymmet-
ric sampling distribution of p and the symmetric posterior distribution
of p. Because of the asymmetry of the distribution of p, Sims and Uhlig
argue that the classical p-values, which the classical unit root tests rely
on, are quite misleading. Suppose we observed p = 1 and tried compar-
ing the p-values of the null hypotheses p = 0.98 and p = 1.02 by classical
procedures. Sims and Uhlig compute the p-values by computing the area
under the curve to the right of the observed p. They show that the p-
value for p = 0.98 given an observed p = 1 is 0.033, while the p-value for
p = 1.02 given p = 1.02 is 0.245. Thus we can reject Ho : p = 0.98 at
the 5 percent level, while easily accepting Ho : p = 1.02. They conclude
that the classical methods based on the asymmetric distribution of the
OLS estimator p can mislead, giving too much credence to large p values
and the simple flat-prior Bayesian method is a more convenient and a
logically sounder starting place for inference than classical hypothesis
testing for unit roots.
8.3 Bayesian inference on the Nelson-Plosser data
The above Bayesian methods have been applied to the Nelson and Plosser
data by DeJong and Whiteman (1991) and Phillips (1991) and the re-
sults are strikingly different from those obtained by Nelson and Plosser
(1982).
We shall begin to extend the above analysis to a model with a linear
deterministic trend and a richer dynamic structure such as
yt = a + 6t + <&(L)yt + eu 
et ~ iin(0, a
2)
where $(L) = ^2i=1 4>{Ll. This formulation includes the empirical speci-
fications used in Nelson and Plosser (1982), where k < 6, and the model
used by DeJong and Whiteman (1991), where k = 3. It is convenient to

8.3 Bayesian inference on the Nelson-Plosser data 
269
employ the following alternative parameterization
fc-i
yt = a + St + pyt-i + Yl &^Vt-1 + £t
where p = X^=i $%• Let V be the matrix of observations on (1, £, Ayt-i> • ••»
Ayt-fc+i) a nd let 7 = (a,<5,/3i, ...,/?fc_i) be the corresponding vector of
parameters. Then the joint posterior density for (p, a*, 7) corresponding
to the flat prior 7r(p, 7, <r) oc l/<7 is
oca T x
exp 
--
2a2
Integrating out both 7 and a directly from the joint posterior distri-
bution density yields the marginal posterior density for p
P(p\y,y0) oc (Rv + (j>- 
p)2Qv)-{T-
where Ry = Ylt=i ^t *s the residual sum of squares with
fc-i
and y-i represent the observation vector of (yt-i).
DeJong and Whiteman (1991) applied the above procedure with k = 3
to the same data used by Nelson and Plosser (1982). They focused on
the posterior distribution of the dominant root, which is the largest
autoregressive root, defined such that
A = maxj\\j\
where
In this setting, trend stationarity (TS) corresponds to A < 1, and differ-
ence stationarity (DS) is the special case A = 1. They employ a truncated
flat prior on the autoregressive coefficients such that
TT(0) = g(6)iA(0)

270 
The Bayesian analysis of stochastic trends
Table 8.1. Posterior probabilities for the Nelson-Plosser data
Series
Real GNP
Nominal GNP
Real per capita GNP
Industrial production
Employment
Unemployment rate
GNP deflator
Consumer prices
Wages
Real wages
Money stock
Velocity
Interest rate
Stock prices
T
62
62
62
111
81
81
82
111
71
71
82
102
71
100
p(A > 0.975)
0.003
0.020
0.003
0.001
0.004
0.002
0.010
0.196
0.018
0.003
0.005
0.592
0.617
0.040
pF(p > 0.975)
0.005
0.063
0.004
0.003
0.014
0.000
0.029
0.528
0.046
0.005
0.025
0.204
0.892
0.059
pj(p > 0.975)
0.019
0.141
0.016
0.192
0.060
0.087
0.062
0.652
0.100
0.021
0.044
0.642
0.998
0.278
Note: PF and pj denote results from a flat prior and from Jeffreys' prior.
Sources: DeJong and Whiteman (1991, p. 231) and Phillips (1991, p. 358).
where g(8) = I/a is a diffuse prior, %A{0) is an indicator function for
9 C A and the set A is given by
A = {((5, A) : 0 < 6 < 0.016,0.55 < A < 1.055}
With this truncated prior the integrals required for obtaining the poste-
rior distributions cannot be evaluated analytically. Thus they adopted
the Monte Carlo integration developed by Kloek and van Dijk (1978)
and Geweke (1989).
The second column of table 8.1 shows part of their results. Based
on the estimated posterior probabilities of the dominant root, they in-
fer that evidence in support of a stochastic trend is present for only
two series (velocity and bond yields) and they deem the evidence to be
marginal in the case of a third series (consumer prices).
Sowell (1991) criticized the methodology of DeJong and Whiteman
(DJW) arguing that
(i) in DJW, analysis is restricted to AR(3) models, while it is not in
Nelson and Plosser (1982),
(ii) there is no formal justification for the arbitrary rule that the
posterior mass associated with values of (A > 0.975) is support
for a unit root hypothesis,

8.4 The debate on the appropriate prior 
271
(iii) the question of the correct model for the (deterministic) trend
behavior remains unanswered.
Regarding the criticism (ii), Phillips (1991) provides the posterior prob-
abilities under the flat prior of p rather than the dominant root A. The
fourth column of table 8.3 shows that the results of Phillips (1991) sup-
port a similar inference, although they are slightly different especially
for consumer prices.
8.4 The debate on the appropriate prior
Sims (1988) suggested the simple flat-prior Bayesian theory for testing
the unit root hypothesis. As mentioned earlier, Phillips (1991) raised
the important point that the autoregressive time series model is different
from the usual regression model and the flat prior used in the latter is
not applicable to the time series model. However, the question is about
the appropriateness of Jeffreys' prior that Phillips suggests (in fact any
priors in general) for the time series models. In the debate in the Journal
of Applied Econometrics (1991, volume 6, number 4) the main points
raised were as follows (the debate is long but a few points stand out):
Phillips (1991) argued that flat priors ignore the way in which the
coefficients influence the amount of information contained in the sample
and suggested the use of Jeffreys' prior in equation (8.2). Phillips called
this an ignorance prior. With the simple AR(1) model, the ignorance
prior is given by
with
, 
= 
JT 
1 
l-p2T 
/ l f o y 2 i _ ^
•*• 00 
•* 
o 
* 
o 
-< 
o 
I 
I )fl-^
v " 
i - p2 
i - p2 i - p2 "•" V CT y 
1 - ^
The first issue raised was the appropriateness of Jeffreys' prior. The
first objection is that the prior depends on the sample size T. It was
argued by Learner (1991) and Kim and Maddala (1991) that it favors
high values of p (see figure 8.1). However, it was pointed out that this
is the case with the simple autoregressive model
yt = pyt-i + et

272
The Bayesian analysis of stochastic trends
2
0.03 -
0.01 
-
Flat Prior
Fig. 8.1. Marginal posterior distributions of p when p = 1
but not in the model with a trend and intercept
yt = n + (3t + pyt-i +st
This was shown through Monte Carlo studies by Phillips and also ana-
lytically by Schotman and van Dijk (SVD) who argue (1991a, p. 388):
Paradoxically the ignorance prior proposed by Phillips downweights the unit
root hypothesis relative to a flat prior in a model with trend and intercept.
Thus, the conclusions that follow from Jeffreys' prior are very sensitive
to the way the model is formulated. One issue raised by SVD is that of
alternative parameterization of the unit root model, the structural form
and the reduced form
uu
yt = /i + f3t
et
et
with et ~ 11(1(0, a2). The parameters in the two equations are related as

8.4 The debate on the appropriate prior 
273
For the structural form, Jeffreys' prior is proportional to |/^^|1//2, where
(
x'x/a2 
0 
0 
\
0 
Ipp 
0 
(8.3)
0 
0 
2T/a2 J
with
Ipp = ao(p) + 
-2 
-j—^-
T 
l-p2T
Ignoring the term due to the initial condition yo by assuming yo — 7
and using (8.3), Jeffreys' prior then becomes
P(p, 7, S, a) oc (det(Iee))1/2 a a~3(l - p)2a0(p)1/2 
(8.4)
The last proportionality sign can be verified by direct calculation of
the determinant of the matrix X X. Equation (8.4) has the convenient
property that the priors on all elements of 9 are independent. Note that
the prior (8.4) does not depend on 7 and 8. These parameters have flat
priors. Also, the prior (8.4) drops to zero as p —+ 1. SVD plot Jeffreys'
priors for the autoregressive model with (i) no constant and no trend,
(ii) constant only, and (iii) constant and time trend. In case (i) Jeffreys'
prior gives considerable weight to values of p > 1. In cases (ii) and (iii)
the prior has value zero at p = 1. They argue that this perhaps explains
the bias toward stationarity in models with a fitted intercept and time
trend.
Phillips, in his reply to the comment by SVD, argued that the prob-
lems with Jeffreys' priors noted by SVD arose from the fact that the
reduced form of the unit root structural model has one less parameter
under the null and this produces degeneracy. However, when we con-
dition the likelihood function on the initial value, this degeneracy does
not arise. To see this argument, let us consider the model with no trend.
The structural model is
yt = fj, + uu 
ut = put-i + £u £t ~ im(0, a2)
The reduced form is
yt = a + pyt-i + et

274
The Bayesian analysis of stochastic trends
with a = /i(l — p) so that when p = 1, /i disappears from the reduced
form. One way to avoid this degeneracy is to write the model conditional
on UQ. We then have the reduced form
Vt 
\ ^ ( l - p j + pj/t-i+et, 
t = 2,...,T
Now when p = 1, /i does not disappear from the model. It is retained in
the first observation and contributes to the level of the series. This seem-
ingly innocuous inclusion of the initial value has an enormous impact on
the Bayesian analysis of the unit root hypothesis.
We now write down the log-likelihood function
/, o~\ data,
oc-Tlog(<r)
J2(yt - Mi - P) - pyt-i)2
The information
where
matrix
A
B
C
D
2
involves Uo and
he =
' A 
B
B C
= 
Uo(l + p — p
UQ (I- 
p
2 T
a2
= 
(T-
{l-p2
- 2)/^2
2
is of the form
0 "
0
D _
T)/a2
y ao(p)
where ao(p) is defined earlier. If we assume uo = 0, in which case yo =
we get Jeffreys' prior as
= 0) oc a
where
ai(p) = l + (T
Note that SVD had (1—p)2 instead of ai(p). As before, the joint Jeffreys'
prior does not depend on /x, but one can think of the prior for \x as
with the priors for a and p as
TT(O-) OC - 
and 
n(p) oc [ao{p)]1/2
a

8.4 The debate on the appropriate prior 
275
The prior for /x conditional on a and p is flat but not degenerate at
p — 1. The above analysis assumes u$ = 0. Since u$ is not known, we
can consider the distribution of uo and get the unconditional likelihood
function. We can assume uo ~ iV(0, cr2) which implies that the process
started at time 0, or assume that the process started s periods before
and then derive the distribution of u$. These alternatives are discussed
in Uhlig (1994a). Further analysis of the test of the unit root hypoth-
esis using Jeffreys' priors and conditioning on tto, as well as alternative
distributions of UQ, can be found in Zivot (1994). The important point
to note is that conditioning on the initial value plays a very important
role in the Bayesian inference of the autoregressive model.
Lubrano (1995) also emphasizes the importance of initial observations.
He shows that with a structural model and an adequate treatment of
the first observation, a Bayesian unit root test can produce results which
are more or less in accordance with classical results. Lubrano uses the
structural model
yt = /i + St + ut
Ut = pUt-i + St, 
St ~ Md(0, CT2)
as in Schotman and van Dijk (1991a).
Many authors consider t/o fixed and define the likelihood function
conditional of y$. Lubrano considers a random initial condition with the
distribution of yo given by
yo~N 
[ft,
a2
1-p*
that is the equilibrium distribution of ut under \p\ < 1. Zellner (1971, p.
88) shows that in the general stationary case it does not matter whether
yo is treated as fixed or random, but Lubrano shows that it makes a
difference in the unit root case, when there is a constant term in the
model (for details refer to his paper).
This initial distribution is not valid for \p\ > 1. To allow for the
possibility of small values of |p| greater than 1, Lubrano modifies the
distribution of y0 as
y0 ~ N
9(1 -

276 
The Bayesian analysis of stochastic trends
where
{
0 
if \p\ > VI+ v
(1 - p2 + v)2/4v 
if V T ^ < |p| < y/TTv
and v is a small positive quantity. He suggests v = 0.5 so that the
interval for p is in [—1.225,1.2251] which is wide enough.
As for the prior, one can consider a mass point at p = 1 as done in
Schotman and van Dijk (1991a) and compare the posterior odds. The
other alternative Lubrano considers is a diffuse prior which is obtained as
a limit of an informative prior. For this he considers a Beta distrbution
TT(P\V) OC (Vl + v + p)p~1(y/l + v - p)9"1, 
p > 0, q > 0
This is an informative prior in a finite range. Setting p = 1 and q = 0
he obtains the non-informative limit
ft(;p\v) OC (y/l 
+ V — p)~X
Its shape is similar to that of Jeffreys' prior but it does not depend on T.
This prior allows for explosive values of p in the case 1 < p < y/1 + i>, but
does not depend on T. Finally, Lubrano compares his results on posterior
odds with those from Phillips' prior (Jeffreys' prior), the SVD prior, and
with those from the ADF test using the extended Nelson-Plosser data
(up to 1988), and shows that his results are more in line with those of
the classical results. Of the 14 series, the ADF test rejects the unit root
null in five series, with the posterior odds, if we reject the unit root
null if the posterior probability p(p > l\y) is < 0.05, the Lubrano prior
rejects the unit root null in six series, Phillips' prior rejects it in seven
series, and the SVD prior rejects it in eight series (see table 1, p. 99 of
Lubrano's paper).
Apart from the issue of initial observations, there is another issue
that arises when estimating models with several lags. The usual models
estimated include lagged values of Ayt so that the model is
fc-i
yt = p + fit + pyt-i 4
The use of Jeffreys' prior discussed earlier does not take into account
the correlation between the long-run dynamics (captured by p) and the
short-run dynamics (captured by fa). Zivot and Phillips (1994) show
that the correct application of Jeffreys' principle to this model yields a
prior which is difficult to work with numerically and a resulting posterior

8.5 Classical tests versus Bayesian tests 
277
which is improper. They suggest a variant of Phillips' prior (1991) which
yields a proper posterior and also allows for interaction between the
long-run and short-run parameters. However, this is an approximation
to the correct prior and still ignores the same off-diagonal elements of
the information matrix. It also depends on some arbitrary constant e
and they recommend in practice to try different values of e. Because of
these limitations we shall omit the details here.
8.5 Classical tests versus Bayesian tests
Of course, in problems like this, there is the question of the comparability
of classical significance levels and Bayesian posterior odds. The problem
is whether a classical p-value can be viewed as a posterior probability of
the null hypothesis. In some cases where one is testing a one-sided null
hypothesis against a one-sided alternative hypothesis such an interpreta-
tion is possible (see Berger, 1985, pp. 147-148, Casella and Berger, 1987,
and the references there). This interpretation is not possible for tests of
point null against one-sided or two-sided alternatives (see Berger, 1985,
pp. 148-151, Berger and Sellke, 1987, and the references there). How-
ever, some approximate equivalence is discussed in Hodges (1992). On
the other hand, Andrews (1994) derives some new results and discusses
the relationship of his results to earlier ones. He shows that for certain
priors, the Bayesian posterior odds test is equivalent in large samples to
classical Wald, LM, and LR tests for some significance levels and vice
versa. This result is more general for the purpose of the discussion here.
For our purpose the ealier references are sufficient.
8.6 Priors and time units of measurement
There were also some general questions raised about the use of priors in
general. Learner argues that one should not be stuck with a rigid prior
irrespective of the frequency of the time series and the variable being
considered. Learner (1991, p. 371) argues:
Do we have the same ideas about p if y is nominal GNP, real wages, the saving
rate, the Dow Jones average ...? I don't. Do you have the same ideas about p
if the time unit is centuries, decades, years, days and seconds. I don't...
Sims (1991, p. 425) discusses the last point regarding analysis of the
same problem at different frequencies. Suppose we are analyzing the
same problem with monthly data and annual data. If a univariate AR

278 
The Bayesian analysis of stochastic trends
specification is exactly correct at the monthly interval with a coefficient
p, it remains correct for the annual data with coefficient p12. If the prior
pdf applies to (j) = p12 for the annual data, the implied prior for p for
the monthly data is
Thus, if we have a flat prior for <fi for annual data, our corresponding
prior for monthly data is p11, a convex upward-sloping function of p. A
prior exp(—0) at the annual level, implies that the prior for monthly
data is proportional to p11 exp(—p12). Sims plots this prior. It starts
rising from p = 0.6, reaches its peak at p = 1, but falls off to zero for
p = 1.2. There is one prior that remains invariant to the choice of the
time unit of measurement - that is a flat prior on log(p) over p > 0 (i.e.,
a prior pdf of 1/p).
8.7 On testing point null hypotheses
Given that the unit root null hypothesis p = 1 is a point null hypothesis,
we cannot use a continuous prior density for p, since any such prior will
give p = 1, a prior probability zero, and hence a posterior probability
zero. The problems associated with the testing of point null hypotheses
in the Bayesian framework have been reviewed in Berger and Delampady
(1987). DeJong and Whiteman (1991) use intervals like P(A < 0.975)
where A is the largest root, in their posterior probability calculations.
Their procedure has been criticized on the grounds that they are not
really testing the unit root null and that many hypotheses in economics
(like the market efficiency hypothesis and the permanent income hy-
pothesis) are point null hypotheses. Thus, it is important to consider
a Bayesian approach to the unit root null (see Schotman and van Dijk,
1991a, pp. 392-393, for this argument). Whether any economic theories
really hinge on testing precise hypotheses is a debatable issue, but it is
often argued that one needs a nonzero prior for the null to do posterior
odds calculations for the unit root hypothesis. Phillips and Ploberger
(1994) argue that this is not the case and this approach has been fol-
lowed up by Phillips (1995). As we shall argue the approach in Phillips
and Ploberger (1994) is some sort of predictive approach. An earlier
discussion of the Bayesian predictive approach is in Geisser and Eddy
(1979).

8.7 On testing point null hypotheses 
279
8.7.1 The posterior odds approach
Using posterior distribution
In the classical approach the null hypothesis and the alternative hypoth-
esis are not on the same footing. The null hypothesis is on a pedestal
and it is rejected only if there is overwhelming evidence against it. This
is usually implicit in the use of the 5 percent and 1 percent significance
levels. In the Bayesian approach the null and alternative hypotheses
are on the same footing. There are several ways of comparing hypothe-
ses using Bayesian methods, but the most common one is the posterior
odds ratio. If we are interested in two hypotheses H\ and #2, then the
posterior odds ratio K12 based on data y is given by
= P{H1\y) = 
P(y\H1)-P{Hl)
12 
P(H2\y) 
P(y\H2) • P(H2)
If #i and 62 are the relevant parameters under H\ and H2 respectively,
then, since
P{y\Hi) =
the posterior odds ratio is calculated as
P(H1)JP(61\y,H1)d01
12
P(H2)JP(e2\y,H2)de2
where P(6i\y, Hi),i = 1,2, is the posterior distribution of Oi and is given
by
^ ) 
oc
Thus, the posterior odds ratio depends on the prior probabilities for
the hypotheses, the prior distributions for the parameters 6i under the
respective hypotheses, and the likelihood functions under the two hy-
potheses. Although the decision of which hypothesis to accept should
be based on an explicit loss function, the common procedure is to accept
Hi if K\2 > 1 and to accept H2 otherwise.
The posterior odds approach, although straightforward, has implica-
tions on the testing of point null hypotheses. Implied in the prior odds
for the two hypotheses is an assumption about the prior probability for
the null. For instance, suppose we are interested in testing the hypoth-
esis p = 0 in the model
yt = pxt 4- uu 
ut = put-i + Su £t ~ iin(0, a2)

280 
The Bayesian analysis of stochastic trends
Let
Hi : Model with no serial correlation in the errors.
H2 : Model with serial correlation in the errors.
If we specify the prior odds P(Hi)/P(H2) = 1 arguing that we do not
know which model is correct, implicitly we are specifying a prior prob-
ability P(p = 0) = 1/2. Also note that the prior probabilities for the
parameters and the prior probabilities for the hypotheses are interre-
lated.
Sims (1988) proposed the posterior odds ratio test, which follows the
lines of Learner's (1978) general version of the idea embodied in the
Schwarz's Bayesian information criterion (BIC). Since the Schwarz cri-
terion's asymptotics depend on the distribution of the estimate converg-
ing at the same rate for all true parameter values in a neighborhood of
the null hypothesis, it is not applicable directly for choosing the model
between the stationary and the unit roots models. Sims shows that the
criterion would be
r = 2 log ( ^ r ) - log(^) + 21og(l - 2-1/*)
where s is the number of periods per year (e.g., 12 for monthly data)
and a is the prior probability (weight) on the interval (0,1), probability
1 — a on p = 1. Since the first and the last terms of r are constant for
a given data and prior, a small r would be an evidence against the unit
root. Based on this fact, if t2 > r, we reject the unit root hypothesis,
otherwise, we do not. Sims suggests a = 0.8 at which the odds (between
the stationarity and unit root) are approximately even.
Using predictive distribution
Another approach to model choice is through predictive distributions
rather than posterior distributions. A discussion of predictive methods
for model selection from the classical point of view can be found in the
paper by Wei (1992). In the Bayesian framework, the predictive density
of future observations ?/*, is defined as P(y*\ data y). Using the laws of
probability, we see that
P(y*\y) = J L(y*\0,y)P(6\y)de
where L(y*\6,y) is the likelihood function of the future observations and
P(0\y) is the posterior density of 6.

8.7 On testing point null hypotheses 
281
The predictive odds ratios for the two models are given by
= 
P(y*\y,H1)-P(H1)
12 
P(y*\y,H2).P(H2)
But
P(y*\y,Hi) =
Dropping Hi in the conditioning variables, we get
= 
JP(y*\y,61)-P(e1\y)de1
12 
JP(y*\y,e2)-P(62\y)d62' 
P(H2)
where P(6i\y) is the posterior distribution of 8i given y (under Hi).
The predictive density in the Bayesian language has been re-interpreted
in classical language as a Bayes model or Bayesian frame of reference
and used to derive classical tests for unit roots by Phillips and Ploberger
(1994) and by Phillips (1994a, 1994b, 1995) for model selection and pre-
diction. In essense this work amounts to the use of predictive densities
and predictive posterior odds and the differences between the two ap-
proaches are minor.
The fundamental arbitrariness of the posterior odds approach with
noninformative priors is discussed in Learner (1978, section 4.5). On
the other hand, the predictive odds approach does not suffer from this
problem. Initial noninformative priors of the form P(/3,a2) oc o~2 are
applied to each model over periods 1 to m(m < n) to produce proper
posterior densities for (/3, a2). These are then used as priors to evaluate
proper predictive densities for periods (m + 1) to n. The ratio of two
such predictive densities is unambiguously defined.
8.7.2 Model selection approach
Predictive approach
Geisser and Eddy (1979) put forth two model selection criteria based
on predictive sample re-use (PSR) methods. These are, however, suit-
able for independently distributed variates such that Xj has density
f(xj\8k,Mk) where M& is the kth model. The first criterion, termed
PSR quasi-likelihood (PSRQL) selects the model that maximizes
N

282 
The Bayesian analysis of stochastic trends
where 0k^ is the MLE of 0k with Xj omitted. The second criterion,
termed PSR quasi-Bayes (PSRQB) selects the model that maximizes
N
3=1
where x^ denotes that Xj has been deleted and fp denotes the predictive
density, which is calculated as
fp(Xj\x{j),Mk) = j 
f(xj\0k,Mk)dP(9k\x(j),Mk)
where P(0k\x^,Mk) 
is the posterior density of 0k based on x^ and
usually a diffuse prior on 9k (see Geisser and Eddy for details).
Posterior information criterion (PIC)
Given a number of regression models, the Bayesian information criterion
(BIC) chooses the model that minimizes
log erf + k
n
where G\ is the estimate of a2 the error variance with k regressors and
n is the sample size.
Wei (1992) suggests the Fisher information criterion (FIC) for model
choice. This suggests choosing the model that minimizes
FICk = n&l + a2
K In \Ak
where Ak = X'X for the model with k parameters and o\ and G\ are
variance estimates of a1 based on the model with k parameters and the
full set of K parameters respectively. Compared with AIC and BIC
that have a penalty term that is based on the dimension of the selected
model, the FIC has a term that is proportional to the logarithm of the
statistical information in the model with k parameters. The redundant
information, by introducing a spurious variable, is used to represent its
penalty. The stimulation studies by Wei (1992), however, are for models
with exogenous regressors. He shows that FIC performs very well in
selecting the true model, but the performance of BIC is very close to
that of FIC.
Phillips and Ploberger, in a number of unpublished papers, the pub-
lished versions of which are Phillips and Ploberger (1994, 1996) and
Phillips (1994a, 1994b, 1995), suggested an alternative criterion for

8.7 On testing point null hypotheses 
283
model choice: the posterior information criterion (PIC). This minimizes
c1K\Ak/a2
K\V2 
exp[-(l/2aK)p'kAjk]
where C\K is a constant depending on K, the maximum number of re-
gressors, a\ is the estimate of the error variance from the regression
model with the largest number of regressors, K, Ak = (X'kXk) where
Xk is the matrix of observations on the k regressors, and /3fc is the esti-
mate of the coefficient vector with k explanatory variables.
Unlike the BIC which depends on the number of regressors k and
the residual variance &k, the PIC depends explicitly on the data matrix
Xk- This is an advantage of the PIC. The motivation behind the PIC
is complicated and described in the papers by Phillips and Ploberger.
However, the procedure is not completely Bayesian because of the way
the residual variance is estimated (from the largest model). Richard
in his comment on Phillips (1995) objects to this and suggests using
o\ for the /cth model and constructing Bayesian posterior odds. When
viewed in the light of Bayesian posterior odds, the limitations of PIC
as a criterion of model choice are clear. For more detailed discussion of
this point, see the paper by Phillips (1995), the comments, and Phillips'
reply.
However, Phillips discusses another criterion, PICF, which is an ex-
tension of PIC based on predictive densities. As argued earlier predictive
posterior odds do not suffer from the same problems as posterior odds
with noninformative priors. But the predictive odds have to be cal-
culated as discussed earlier, using a subsample to get proper posterior
distributions and then using these as priors for computing predictive
densities of the remaining observations. One can do this sequentially for
the fcth observation (k = m + 1, m + 2,..., n), using the predictive odds.
Phillips uses his PIC and PICF criteria to do such sequential model
choice where you are not confined to a single model for the whole period.
This can also be done using the Bayesian predictive odds sequentially.
Koop (1994) uses a Bayesian likelihood ratio (BLR) criterion for choos-
ing between 1(0) and 1(1) models, which is also due to Phillips and
Ploberger. This criterion is described in section 2 of Phillips' paper,
"The Long-run Australian Consumption Function Reexamined: An
Empirical Excercise in Bayesian Inference" (September, 1991). It's mo-
tivation is the same as that for PIC.

284 
The Bayesian analysis of stochastic trends
For the model
fc-i
yt = V + fit + pyt-i +
where ut ~ 7iV(0, a2), the BLR criterion is
BLR =
where <r2 is the OLS estimator of <J2, p is the OLS estimator of p, y_i is
the vector of yt lagged once, and Q = I — X(XfX)~1Xf 
with X as the
matrix of observations on the explanatory variables 1, £, Ayt-i, •••, ^yt-k+
With equal prior odds, we decide in favor of the hypothesis p = 1 if
BLR < 1.
8.8 Further comments on prior distributions
In the preceding sections we discussed the controversy over the flat prior
and Jeffreys' prior in the case of time series models, the importance of
taking account of the initial conditions in Bayesian inference, and the
posterior odds approach to the problem of testing the unit root hypoth-
esis. We shall now discuss further results on priors and on Bayesian
model selection methods.
The paper by Kass and Wasserman (1996) gives a critical survey (with
an annotated bibliography) of the different methods of generating prior
distributions for Bayesian inference. Section 3.5.1 of their paper dis-
cusses the problem of priors for autoregressive models, using the Berger-
Bernardo method, which is a method of generating noninformative pri-
ors. See Berger and Bernardo (1992). This method leads to the prior
7T(p) OC ( V / W 2 ~ ) - 1 
if 
\p\ < 1
To allow for the possibility of explosive roots Berger and Yang (1994)
suggested the prior
f h
y
1 
if H < I
This is the Berger-Bernardo prior in the range \p\ and the prior outside
this range is obtained by mapping p —* 1/p. This is actually a proper
prior integrating to 1. It gives equal probability to |p| < 1 and \p\ > 1.

8.8 Further comments on prior distributions 
285
Another class of priors is the maximal data information priors (MDIP)
suggested by Zellner, starting with Zellner (1971). See Zellner (1996)
for a historical review and an account of recent developments, including
Zellner and Min (1993). The basic idea is to choose the prior that
maximizes the difference in the information provided by the data and
the information provided by the prior density using Shannon's measure
of information. Define
= -jf(y\6)logf(y\0)dy
This is the information in f(y\6). Zellner and Min (1993) suggest choos-
ing the prior that maximizes the difference
G= 
f I(0)n(0)dO - f 7r(6)log7r(6)d6
The first term is the average information in f(y\0) and the second term
is the information in the prior density. This method yields the prior
TT(0) OC exp[J(0)]
This prior, however, is not invariant to reparameterization. For the
simple AR(1) model
yt = pyt-i + eu 
et ~ IN(0, a2)
the MDIP prior is given by
which converges to zero as p —> 1.
Uhlig (1994b) discusses the behavior of the different priors. Since the
paper by Uhlig summarizes a number of important issues regarding the
Bayesian approach to unit roots and cointegration, we shall quote some
points made.
(i) Consider a VAR model in Yt, the regression parameters being
denoted by B and the covariance matrix of errors by E. Denote
E"1 by H. Then given the data Yt, the conditional likelihood
function as a function of B and H is proportional to a Normal-
Wishart density function. This is true regardless of whether there
are unit roots, cointegrating vectors, explosive roots or not. (The
Normal-Wishart distribution specifies that the precision matrix
H follows a Wishart distribution and that conditional on iJ, the

286 
The Bayesian analysis of stochastic trends
matrix B in its column vectorized form vec(B) follows a multi-
variate normal distribution. See Zellner (1971) for the use of the
Normal-Wishart distribution in Bayesian inference.) If we con-
dition on H as well, the likelihood function is proportional to a
normal distribution. Hence conventional t and F statistics and
their p-values are meaningful in summarizing the shape of the
likelihood function regardless of whether there are unit roots or
not.
(ii) Whereas the conditional likelihood function is viewed as a func-
tion of the data given the parameters may not be standard (the
classical perspective), the conditional likelihood function is viewed
as a function of the parameters given the data is standard (the
Bayesian perspective). This is the central message of Sims and
Uhlig (1991).
(iii) If the prior is given by a Normal-Wishart density, the posterior is
also given by a Normal-Wishart density (see Zellner, 1971). This
is the reason why the Normal-Wishart prior is popular. However,
in contrast with the case of exogenous regressors, the Normal-
Wishart prior has the disadvantage that in AR models it can
be informative about some properties of the models. This was
pointed out by Phillips (1991). The argument is that some pa-
rameter regions are packed denser than the others. It makes
sense to reparameterize the model so that the parameter values
are evenly packed. This amounts to a prior proportional to \I\1^2
which is Jeffreys' prior. However, this prior also has some prob-
lems which have been discussed in section 8.4 earlier.
(iv) For the univariate AR(1) model, Uhlig plots the different priors.
This shows that the major differences are in the explosive region
\p\ > 1. Hence he concludes that conditional on non-explosive
roots, the differences between the prior suggested by Phillips
(1991) and a flat prior is small and will usually not matter in
practical applications. The difference becomes large once explo-
sive roots are taken seriously. If explosive roots are taken se-
riously, then it is best to report the results from several priors
discussed earlier.
(v) Pre-testing for unit roots or trend stationarity and proceeding
as if one is sure about the conclusions of the pre-test can be
misleading in calculating the uncertainty of n-step ahead forecasts
and also in answering macroeconomic questions, in general. The
Bayesian methods take the uncertainty about the presence of a

8.9 Bayesian inference on cointegrated systems 
287
unit root into account. But the tails of the predictive densities
can be sensitive to the prior treatment of explosive roots.
8.9 Bayesian inference on cointegrated systems
There have been many procedures for Bayesian tests for cointegration
and Bayesian analysis of cointegrated systems. First, we shall discuss
tests for cointegration.
Bayesian tests for cointegration
The simplest of the Bayesian tests for cointegration is to apply Bayesian
unit root tests (discussed earlier) to the residuals from an estimated
cointegrating regression. This procedure is not fully Bayesian because
the estimation of the cointegrating regression is not done by Bayesian
methods. The Bayesian methods are brought in at the end.
Koop (1994) presents an alternative approach based on the number
of nonstationary roots in a VAR system.1 In a cointegrated system
with k variables which are 1(1), if there are r cointegrating vectors, then
this leaves (k — r) series that are unit root series. Thus, cointegration
is present if the number of unit roots in the VAR is less than the total
number of unit roots in the univariate series. Koop defines cointegration
in a different way so that the problems of testing a point null hypothesis
(discussed earlier) do not arise. He defines cointegration to be present
if the number of nonstationary roots driving the VAR is less than the
number of nonstationary roots in the univariate series.
Koop's procedure runs as follows: let A^ be the number of nonsta-
tionary roots in the univariate models and XM be the number of non-
stationary roots in the multivariate model (VAR model). Suppose that
the Bayesian analysis gives
P(\u = m\y) = 0.90, 
P(XU = m- l\y) = 0.10
and
P(AM = m\y) = 0.05, P(XM = m-l\y) = 0.25, P(XM = m-2\y) = 0.70
Then there is cointegration if XM < Xu. Hence the probability that
there is cointegration
P(XM < Xu\y) = (0.90)(0.25 + 0.70) + (0.10)(0.70) = 0.925
1 Koop (1991) also discusses a Bayesian approach to cointegration. Since this paper
is based on unit root tests that do not take into the developments in Bayesian unit
root testing since 1991, we shall omit the discussion of this paper here.

288 
The Bayesian analysis of stochastic trends
i.e., there is 92.5 percent probability that there is cointegration. The
posterior probabilities P(Xu\y) can be calculated using the univariate
methods described earlier. For calculating the posterior probabilities
P(AM|y), Koop starts with the Geisser prior discussed in Zellner (1971,
pp. 224-233 and 396-399)
For this prior the posterior density P(<j)\y) is a generalized student t.
Koop does n repeated draws from this posterior density and calculates
the number of eigenvalues greater than or equal to 1 in absolute value.
The frequency distribution gives the probabilities P(XM = m) as n the
number of draws increases. Using this procedure, Koop finds that there
are no common trends in stock prices or exchange rates across countries,
but that for any given country, spot and forward exchange rates are
cointegrated.
There is, however, one major problem with Koop's procedure. This is
the use of the Geisser (noninformative) prior for the VAR model. This
prior was suggested for the standard multivariate regression model. The
VAR model is not a standard regression model. Earlier we mentioned
the criticism of Phillips against the use of flat priors, appropriate for
the standard regression model, for the AR(1) model, arguing that the
AR(1) model is not a standard regression model. The same criticism
applies here in the multivariate case. The problem of appropriate priors
for the VAR model remains to be studied.
Bayesian analysis of cointegration
There are three papers concerned with the Bayesian estimation of cointe-
grated systems: Kleibergen and van Dijk (1994), Bauwens and Lubrano
(1996), and Geweke (1996). Since the Bayesian analysis is notation
heavy and involves a lot of integration (analytical as well as numerical)
we shall not go into the details here. Readers interested in this area can
refer to the original articles. Of more concern to us is the priors used
and the appropriate priors for cointegration analysis. The discussion on
priors for unit root models in the preceding sections is relevant here. An
important point we mentioned earlier is to note that the VAR model
used in cointegration analysis is not the usual multivariate model with
exogenous regressors and hence priors appropriate for this latter model
are not appropriate for the cointegration model.

8.9 Bayesian inference on cointegrated systems 
289
Geweke (1996) considers the model
Y = XA + ZB + E
where Y, X, and Z are respectively n x L, n x p, and nxfc matrices, A
and 5 are respectively p x L and k x L matrices of parameters, and E
is an n x L matrix of errors. Z is supposed to be of full rank but X is
supposed to be of reduced rank q (q < p or L).
Geweke develops Bayesian methods for the analysis of this model and
also gives procedures for determining q. Since this method is not di-
rectly concerned with the cointegration model, we shall not discuss it in
detail. The main issue we are concerned with is the identification of the
cointegrating vectors in the Johansen procedure.
As for the priors, Geweke uses the inverted Wishart prior for E, the
covariance matrix of the errors Es, and normal priors AT(0, E~2) for each
of the elements of A. This is the Normal-Wishart prior that we discussed
in section 8.8 (Uhlig's comments). It is a diffuse prior which Kleibergen
and van Dijk criticize.
The paper by Kleibergen and van Dijk (1994) argues that diffuse priors
should not be used in cointegration models. The resulting posteriors may
be nonintegrable. They suggest the use of Jeffreys' priors for which this
problem does not arise. But Jeffreys' prior need not be unique. They
suggest four different versions of Jeffreys' prior, only some of which would
be useful in practice.
The paper by Bauwens and Lubrano (1996) considers the identification
problem in terms of the representation considered by Johansen. The
matrix £?, in section 5.5.1 is denoted by II in the Bauwens and Lubrano
paper. As done there they parameterize it as
where a and (3 are nxr matrices of rank r. II is identified but a and (3
are not because (aH~1)(H^/) is also equal to II, where H is any r x r
matrix of rank r.
The first result in the Bauwens and Lubrano paper is that (2nr —
r2) elements of (a,/3) are identified. As II is identified and II has n2
elements, the resulting (n — r)2 elements are determined by the rank
condition rank(II) = r.
The matrices a and (3 must be subject to a total of r2 restrictions
but they have 2nr elements. Bauwens and Lubrano use r normalization
restrictions on (3. This leaves r(r — 1) restrictions. They do this by im-
posing linear restrictions on /3. To do the Bayesian analysis, they note

290 
The Bayesian analysis of stochastic trends
that the model is a linear model conditional on /3. Hence they say that
the traditional noninformative prior (the Geisser prior discussed in Zell-
ner, 1971) can be used. This results in the overall prior |ft|"(n+1)/2P(/?)
where Q is the covariance matrix of the errors.
However, note that this prior for a multivariate regression (with ex-
ogenous regressors) is not valid for VAR models. We shall omit the
details of the subsequent Bayesian analysis which can be found in the
Bauwens and Lubrano paper.
The important thing to note is the use of the Geisser prior which is
not valid for a VAR model. The argument is the same as that of Phillips
(1991) as against the flat prior for the AR(1) model. The appropriate
priors for VAR models and hence for cointegration models is still an open
issue. There is a long discussion of appropriate priors for the AR(1)
model which we reviewed earlier. There is no corresponding detailed
discussion in the case of VAR models.
One other issue is: what new insights are being gained by the use of
Bayesian analysis? This is not entirely clear in these papers. Of course,
they are all interested in tackling the computational problems first. But
at some point it is important to get to the issue of what more we are
learning. The usual argument is that Bayesian analysis gives us finite
sample results.
One important area where Bayesian analysis should be of help is in
the pre-testing problem, which as we discussed in chapter 6 arises often
in the analysis of cointegrated systems. There is no work on this yet.
We have given only a brief review of the studies on Bayesian analysis of
cointegrated systems. Our purpose is to see what priors are being used
and point out their limitations. The computational problems are not
easy and the papers have made important contributions in this respect.
But more attention needs to be devoted to the priors.
8.10 Bayesian long-run prediction
The Bayesian approach is specially useful in studying the uncertainty
about forecasts and impulse response functions. Uhlig (1994b) demon-
strates convincingly that pre-testing for unit roots or trend stationarity
and then proceeding as if one is sure about the conclusion of this pre-test
can be misleading with regard to n-step ahead forecasts. By contrast, in
the Bayesian approach, the uncertainty about the underlying coefficients
is taken into account in computing the predictive density.
Uhlig also demonstrates that the predictive density and, in particular,

8.11 Conclusion 
291
its tails can be sensitive to prior treatment of explosive roots. Hence sen-
sitivity analysis should be performed with alternative priors if explosive
roots are to be taken seriously.
Koop et al. (1995) also show how Bayesian long-run forecasts can be
very sensitive to apparently innocuous assumptions made in the prior.
In the case of impulse response functions, the problems are more seri-
ous. Koop et al. (1994) show that apart from this sensitivity, impulse
responses at some forecast horizons may have posterior density func-
tions that have no finite moments. Hence it is important that the entire
posterior density be studied rather than just the posterior mean and
posterior variance.
8.11 Conclusion
The initial Bayesian analysis of the unit root model was by Sims (1988)
who used a flat prior for the autoregressive model. Subsequently Phillips
criticized this and suggested Jeffreys' prior, which itself has some prob-
lems. We discuss in detail the problems with the different priors sug-
gested (section 8.2 to 8.4). Further comments on priors for unit root
models along with the clarification of several issues by Uhlig are dis-
cussed in section 8.8.
One advantage of the Bayesian approach is the symmetric way the null
and the alternative hypotheses are treated (compared with the classical
approach where the null hypothesis is on a pedestal). The posterior odds
approach is useful for comparing models, but there are problems with
it if uninformative priors are used. We also discuss how the predictive
odds approach does not have these problems.
The unit root hypothesis is a point null hypothesis. The problems
created by this and the solutions offered have been discussed in this
chapter. One has no problem evaluating Ho : \p\ < 1 versus H\ \ \p\ > \
because the posterior probabilities of these regions can be computed once
the posterior distribution of p has been determined. But the posterior
probability of p — 1 is zero with any continuous posterior distribution
for p.
We also discuss the criteria for model selection suggested by Phillips
and Ploberger: PIC, PICF, and BLR. A complete Bayesian approach is
that of predictive odds ratios, which can also be used for model selection
using sequential data. This would enable us to allow for different models
for different spans of the data rather than consider a single model over
the entire time span.

292 
The Bayesian analysis of stochastic trends
Finally, we consider some issues concerning the Bayesian analysis of
cointegrated systems. One major conclusion is that one should not use
diffuse priors for the Bayesian analysis of cointegrated systems. Other
priors need to be investigated. Also, there is the issue that needs to
be investigated as to the additional insights the Bayesian analysis gives
that we do not get from classical analysis.
References
Andrews, D.W.K. (1994), "The Large Sample Correspondence Between
Hypothesis Tests and Bayesian Posterior Odds Tests," Econo-
metrica, 62, 1207-1232.
Bauwens, L. and M. Lubrano (1996), "Identification Restrictions and
Posterior Densities in Cointegrated Gaussian VAR Systems," in
T.B. Fomby (ed.), Advances in Econometrics, vol. 11B, JAI
press, Greenwich, CT.
Berger, J.O. (1985), Statistical Decision Theory and Bayesian Analysis,
2nd edn, Springer Verlag, New York.
Berger, J.O. and J.M. Bernardo (1992), "On the Development of the
Reference Prior Method," in J. M. Bernardo, J.O. Berger, A.P.
Dawid, and A.F.M. Smith (eds.), Bayesian Statistics 4: Pro-
ceedings of the Fourth Valencia International Meeting, Claren-
don Press, 35-60.
Berger, J.O. and M. Delampady (1987), "Testing Precise Hypotheses,"
Statistical Science, 2, 317-352.
Berger, J.O. and T. Sellke (1987), "Testing a Point Null Hypothesis:
The Irreconciliability of p values and Evidence," Journal of the
American Statistical Association, 82, 112-122.
Berger, J.O. and R. Yang (1994), "Non-Informative Priors and
Bayesian Testing for the AR(1) Model," Econometric Theory,
10, 461-482.
Casella, G. and J.O. Berger (1987), "Reconciling Bayesian and Fre-
quentist Evidence in the One-Sided Testing Problem," Journal
of the American Statistical Association, 82, 106-111.
DeJong, D.N. and C.H. Whiteman (1991), "Reconsidering Trends and
Random Walks in Macroeconomic Time Series," Journal of
Monetary Economics, 28, 221-254.
Geisser, S. and W.F. Eddy (1979), "A Predictive Approach to Model
Selection," Journal of the American Statistical Association, 74,
153-160.

References 
293
Geweke, J. (1988), "The Secular and Cyclical Behavior of Real GDP
in Nineteen OECD Countries, 1957-1983," Journal of Business
and Economic Statistics, 6, 479-486.
(1989), "Bayesian Inference in Econometric Models Using Monte
Carlo Integration," Econometrica, 57, 1317-1339.
(1996), "Bayesian Reduced Rank Regression in Econometrics," Jour-
nal of Econometrics, 75, 127-146.
Hodges, J. (1992), "Who Knows What Alternative Lurks in Significance
Tests?" in J.M. Bernardo et al. (eds.),, Bayesian Statistics, vol.
3, Oxford University Press, Oxford.
Jeffreys, H. (1961), Theory of Probability, 3rd ed, Oxford University
Press, London.
Kass, R.E. and L. Wasserman (1996), "The Selection of Prior Distri-
butions by Formal Rules," Journal of the American Statistical
Association, 91, 1343-1370.
Kim, I.M. and G.S. Maddala (1991), "Flat Priors versus Ignorance
Priors in the Analysis of the AR(1) Model," Journal of Applied
Econometrics, 6, 375-380.
Kleibergen, F. and H.K. van Dijk (1994), "On the Shape of the Likeli-
hood/Posterior in Cointegration Models," Econometric Theory,
10, 514-551.
Kloek, T. and H.K. van Dijk (1978), "Bayesian Estimates of Equation
System Parameters: An Application of Integration by Monte
Carlo," Econometrica, 46, 1-19.
Koop, G. (1991), "Cointegration Tests in Present Value Relationships,"
Journal of Econometrics, 49, 105-139.
(1994), "An Objective Bayesian Analysis of Common Stochastic
Trends in International Stock Prices and Exchange Rates,"
Journal of Empirical Finance, 1, 343-364.
Koop, K., J. Oseiwalski, and M. Steel (1994), "Posterior Properties of
Long-Run Impulse Responses," Journal of Business and Eco-
nomic Statistics, 12, 489-492.
(1995), "Bayesian Long-Run Prediction in Time Series Models,"
Journal of Econometrics, 69, 61-80.
Learner, E. (1978), Specification Searches, Wiley, New York.
(1991), "Comment on 'To Criticize the Critics'," Journal of Applied
Econometrics, 6, 371-373.
Lindley, D.V. (1965), Introduction to Probability and Statistics from a
Bayesian Viewpoint, 2 vols., Cambridge University Press, Cam-
bridge.

294 
The Bayesian analysis of stochastic trends
Lubrano, M. (1995), "Testing for Unit Roots in a Bayesian Frame-
work," Journal of Econometrics, 69, 81-109.
Nelson, C.R. and C.I. Plosser (1982), "Trends and Random Walks in
Macroeconomic Time Series," Journal of Monetary Economics,
10, 139-162.
Phillips, P.C.B. (1991), "To Criticize the Critics: 
An Objective
Bayesian Analysis of Stochastic Trends," Journal of Applied
Econometrics, 6, 333-364.
(1994a), "Bayes Models and Forecasts of Australian Macroeconomic
Time Series," in C. Hargreaves (ed.), Non-Stationary Time Se-
ries Analysis and Cointegration, Oxford University Press, 53-
86.
(1994b), "Model Determination and Macroeconomic Activity,"
Fisher-Schultz Lecture, European Meetings of the Econometric
Society, Mastricht.
(1995), "Bayesian Model Selection and Prediction with Empirical
Applications," with comments by F.C. Palm and J.F. Richard
and Reply by P.C.B. Phillips, Journal of Econometrics, 69, 289-
365.
Phillips, P.C.B. and W. Ploberger (1994), "Posterior Odds Testing for
a Unit Root with Data-Based Model Selection," Econometric
Theory, 10, 774-808.
(1996), "An Asymtotic Theory of Bayesian Inference for Time Se-
ries," Econometrica, 64, 381-412.
Raiffa, H. and R. Schlaifer (1961), Applied Statistical Decision Theory,
Graduate School of Business Administration, Harvard Univer-
sity.
Schotman, P.C. and H.K. van Dijk (1991a), "On Bayesian Routes to
Unit Roots," Journal of Applied Econometrics, 6, 387-401.
(1991b), "A Bayesian Analysis of the Unit Root in Real Exchange
Rates," Journal of Econometrics, 49, 195-238.
Sims, C.A. (1988), "Bayesian Skepticism on Unit Root Econometrics,"
Journal of Economic Dynamics and Control, 12, 463-474.
(1991), "Comments on To Criticize the Critics by P.C.B. Phillips,"
Journal of Applied Econometrics, 6, 423-434.
Sims, C.A. and H. Uhlig (1991), "Understanding Unit Rooters: A He-
licopter Tour," Econometrica, 59, 1591-1599.
Sowell, F. (1991), "On DeJong and Whiteman's Bayesian Inference
for the Unit Root Model," Journal of Monetary Economics, 28,
255-263.

References 
295
Thornber, H. (1967), "Finite Sample Monte Carlo Studies: An Au-
toregressive Illustration," Journal of the American 
Statistical
Association, 62, 801-808.
Uhlig, H. (1994a), "A Note on Jeffreys' Prior When Using the Exact
Likelihood Function," Econometric Theory, 10, 633-644.
(1994b), "What Macroeconomists Should Know About Unit Roots:
A Bayesian Perspective," Econometric Theory, 10, 645-671.
Wei, C.Z. (1992), "On Predictive Least Squares Principles," Annals of
Statistics, 20, 1-42.
Zellner, A. (1971), An Introduction to Bayesian Inference in Econo-
metrics, Wiley, New York.
(1996), "Past and Recent Results on Maximal Data Information Pri-
ors," Technical Report, Graduate School of Business, University
of Chicago.
Zellner, A. and C. Min (1993), "Bayesian Analysis, Model Selection
and Prediction," in W.T. Grandy Jr. and P.W. Millioni (eds.),
Physics and Probability: Essays in Honor of E. T. James, Cam-
bridge University Press, Cambridge, 195-206.
Zivot, E. (1994), "A Bayesian Analysis of the Unit Root Hypothesis
within an Unobserved Components Model," Econometric The-
ory, 10, 552-578.
Zivot, E. and P.C.B. Phillips (1994), "A Bayesian Analysis of Trend
Determination in Economic Time Series," Econometric Reviews,
13, 291-336.

9
Fractional unit roots and fractional
cointegration
In the preceding chapters we assumed that the time series is l(d) with d
being 0 or 1 or some greater integer. The model with d = 1 corresponds
to a model with persistence of shocks. However, persistence or long
memory can also be modeled with d > 0 but < 1. The introduction of
autoregressive fractionally integrated moving average (ARFIMA) model
by Granger and Joyeux (1980) and Hosking (1981) allows us to replace
the discrete choice of unit root versus no unit root with a continuous
parameter estimate of the long memory component of time series. Like-
wise, if two series yt and Xt are I(d), they are said to cointegrated if
yt + flxt is 1(6) with b < d, and we allow integer values for b and d. Typ-
ically d = 1 and b = 0. In the case of fractional cointegration, suppose
that yt and xt are both l(d) and yt + (3xt is l(b) with b < d. If b = 0,
then the long memory components in yt and xt are common and we say
that yt and xt are fractionally cointegrated. If b > 0, then yt + (3xt has
a long memory component left in it.
In this chapter we shall discuss the issues related to these problems.
The paper by Granger (1980) gives a motivation behind the l(d) process.
Granger shows that the sum of a large number of stationary AR(1)
processes with random parameters can result in a process with long
memory. Thus it makes sense to consider l(d) processes while analyzing
aggregate date.
9.1 Some definitions
A time series Xt follows an ARFIMA (p, d, q) process if
- L)dxt = /i + e(L)eu 
et - iid(0, a2)
296

9.1 Some definitions 
297
where $(L) = l-<f>1L 
<\>VI?, 0(L) = \-BxL 
6qL^ and /i
can be any deterministic function of time. If xt is trend-stationary, then
d = 0. If Xt is difference-stationary, then d = 1 (or some greater integer).
The ARFIMA model generalizes the ARIMA model by allowing the
differencing parameter d to take on any real value, rather than restricting
it to the integer domain. The fractional differencing term (1 — L)d can
be written as an infinite order MA process using the binomial expansion
T(k - d)Lk
= £
where the gamma function is defined as
/»O
T(g) = /
Jo
In order to see the role of the differencing parameter d on the stationarity
and memory of the process Xt (see chapter 2 for basic concepts), we need
to know the covariances of the ARFIMA process Xf. Some algebra pro-
vides the corresponding autocovariance functions 7T and autocorrelation
functions pr in the form of
<72r(l - 2d)
7 0 
~ 
r(l - d)r(l - d)
o
IT 
=
r(d)r(i - d)r(r + I - d)'
and
- d)T{r + d)
Pr ~ r(d)r(r + I - cO
The process xt is both stationary and invertible if the roots of
and ©(L) are outside the unit circle and d < |0.5|. But, since as r —>
CXD, p r oc T
2d~1 (Hosking, 1981), the autocorrelations do not have a
finite sum, that is, the ARFIMA processes with 0 < d < 0.5 displays
long memory. In other words, these processes exhibit more persistence
with the autocorrelation function decaying much slower than for the
corresponding 1(0) series with similar ARMA parameters.
For 0.5 < d < 1.0, the process Xt is nonstationary with a noninvertible
ARMA representation, since the variance of the process 70 does not have
a finite sum. However, even though the series is nonstationary, as we can
see in Hosking's formula, the autocorrelation function still decays to zero.

298 
Fractional unit roots and fractional cointegration
This implies that the memory of the process is finite and that given a
shock the process tends to revert to its mean, that is, the process is mean-
reverting. For d > 1.0 the process is not mean-averting, and a shock to
the process causes the process to deviate away from its starting point.
Thus the memory property of an ARFIMA process depends crucially on
the value of d and its autocorrelation functions decays at a much slower
rate than those for the corresponding 1(0) series.
9.2 Unit root tests against fractional alternatives
The low power of the standard unit root tests under the presence of
MA errors and the near unit root case has been well documented (see
chapter 4). When the DGP is a fractionally integrated process, what
will be the effect of ARFIMA alternatives on the standard unit root
tests? Sowell (1990) derived the asymptotic distribution of the Dickey-
Fuller test statistics in the case of fractionally integrated processes and
conjectured that in finite samples it would be difficult to discriminate
between unit root processes 1(1) and fractionally integrated processes
I(d) with d < 1. Diebold and Rudebusch (1991b) confirm this in a Monte
Carlo study. Hassler and Wolter (1994) provide analytical arguments as
well as Monte Carlo evidence and argue that the ADF test is even less
powerful when the alternative is a fractionally integrated process and
that the Phillips-Perron tests behave similarly.
In line with Sowell (1990) and Diebold and Rudebusch (1991b), Smith
and Yadav (1994) investigate the performance of various unit root tests
against ARFIMA alternatives by Monte Carlo experiments. They in-
vestigate the power of the ADF test (see chapter 3 for the ADF test),
Cochrane's variance ratio test (Cochrane, 1988, see section 3.8), and
different versions of variance ratio tests proposed by Lo and MacKinlay
(1988). The ADF test is based on a regression
Ayt = a + pyt-x + ] T Pj^Vt-j + eu 
et ~ iid(0, a
2)
where the lagged terms Ayt-i? •••? Ayt-q were included to whiten the
serial correlations of errors (see chapter 3 for details). The variance
ratio test proposed by Cochrane (1988) uses a weighted average of the
sample autocorrelation coefficients (for details, see section 3.8). The
null hypothesis of the unit root is that the variance ratio is one. Lo and
MacKinlay (1988) suggest using different versions of the variance ratios,

9.2 Unit root tests against fractional alternatives 
299
under homoskedastic errors
-1/2
where
Mq = — 
±
with m = q(T — q + 1)(1 — g/T) and under heteroskedastic errors
Z{qy =
where
„ _ VTMq
3 = 1
z2i=iiyi 
— y%-q — ®)
Lo and MacKinlay (1988) show that the asymptotic distributions of
both variance ratios, z(q) and z(q)* follow standard normal distributions
under the unit root null, HQ : z(q)(= z(q)*) = 1. Note that since the
above asymptotic result also requires T/q —» oc, Lo and MacKinlay
suggested to use the empirical critical values in their paper.
Smith and Yadav (1994) investigate the power of these tests by gen-
erating time series with a range of integration parameters, d = 0.7, 0.8,
0.9, 1, 1.05, 1.1, 1.3 and yo — 0. They found that the Cochrane test
and the two tests of Lo and MacKinlay perform similarly, with high
power against fractional alternatives. The performance of the ADF test
was the worst. Whether or not the errors are normally distributed has
little effect on the performance of the test. When ARCH errors are in-
troduced, there was a marked fall in power for all the tests. Especially,
the Cochrane test and the Lo and MacKinlay (homoskedastic) test show
substantial size bias under the presence of ARCH errors.
There have been other studies discussing the power of other unit root
tests against fractional alternatives. For instance, Lee and Schmidt
(1996) discuss the KPSS test (see chapter 4 for the KPSS test). They
show that the KPSS test is consistent against stationary long memory
alternatives, that is I(d) processes for \d\ < 0.5, and, hence, it can be
used to distinguish short memory and long memory stationary processes.

300 
Fractional unit roots and fractional cointegration
Their results, however, show that a rather large sample size (e.g., T =
1000) is necessary to distinguish reliably between a long memory process
and a short memory process.
Crato and de Lima (1996) consider several MA unit root tests (see
chapter 4 for a discussion of MA unit root tests). They characterize the
asymptotic behavior under fractionally integrated processes (as Sowell
did for the DF test). By means of simulation they also compared the
finite sample power of the MA unit root tests and contrast them with AR
unit root tests. Their conclusion is that, overall, overdifferencing tests
(MA unit root tests) performed worse than underdifferencing tests (AR
unit root tests). However, the MA unit root tests seemed to recognize
better the nonstationarity of ARFIMA models with d > 0.5.
There is as yet no discussion of the power of the modified PP tests
and the DF-GLS test (discussed in chapter 4) against fractional alter-
natives. The studies we have discussed consider the sensitivity of the
unit root tests to fractional alternatives. They do not derive new tests
in the presence of fractional alternatives. This is done in the papers
by Robinson (1994) and Gil-Alana and Robinson (1997). We shall not
go into the details of these tests here except to note that this is the
proper approach to fractional unit roots rather than the investigation of
how sensitive the existing unit root tests are to fractional alternatives.
(These papers are not referred to in the survey paper by Baillie, 1996.)
9.3 Estimation of ARFIMA models
The estimation procedures suggested for ARFIMA models fall into three
categories:
(i) two step procedures (Geweke and Porter-Hudak, 1983),
(ii) approximate ML (Li and McLeod, 1986 and Fox and Taqqu,
1986),
(iii) exact ML (Sowell, 1992a).
For the asymptotic properties of the exact and approximate ML esti-
mates, see Beran (1994a, 1994b).
Geweke and Poter-Hudak (1983, hereafter denoted by GPH) proposed
a semi-nonparameteric procedure to estimate a differencing parameter
d. The procedure is a two-step procedure. First estimate d. Then use
this and estimate the AR and MA components. This two-step procedure
has been widely used in practice. GPH (1983) showed that for frequen-
cies near zero, d can be consistently estimated from the least squares

9.3 Estimation of ARFIMA models 
301
regression
)) = c-d 
2
where Wj = 2TTJ/T, n = (j(T) < T, and /(m/) is the periodogram of X
at frequency Wj defined by
(9.1)
2TTT
With a proper choice of n, the asymptotic distribution of d depends on
neither the order of the ARMA component nor the distribution of the
error term of the ARFIMA process. It is suggested to set n = y/T and to
use the known variance of ?7j,7r2/6, to compute the estimated variance
of d.
The joint ML estimation procedures of the order of fractional integra-
tion and the ARMA parameters were proposed in a time-domain context
by Li and McLeod (1986) and Sowell (1992a) and in a frequency-domain
context by Fox and Taqqu (1986). Assuming normality, the likelihood
function of the ARFIMA model is given by
L = (2TT)- T/ 2|E|- 1/ 2 exp(-X/E"1X/2) 
(9.2)
where E is the T xT covariance matrix of X and is a function of d, 0^s,
and fas, and a2. Li and McLeod (1986) showed that the ML estimator,
which is obtained by maximizing (9.2) with respect to the parameter
vector (3 = (d,6i,fa) and a2, is consistent and asymptotically normal.
Fox and Taqqu (1986) suggested a frequency-domain method to es-
timate ARFIMA models. The frequency-domain estimators can be ob-
tained by minimizing the following variance with respect to the param-
eter vector (3 = (d, 0^, fa)
T-l
where /(A) is the periodogram defined in (9.1) and /(A, (3) is proportional
to the spectral density of X at frequency A. It can be shown that, for
ARFIMA models

302 
Fractional unit roots and fractional cointegration
Fox and Taqqu (1986) showed that the distribution of the frequncy-
domain estimator J3 can be approximated by
where
The properties of these estimators were discussed by Yajima (1985,
1988).
Diebold and Rudebusch (1989) applied the two-step estimation proce-
dure of GPH to US macroeconomic series and found post-war real output
series have fractional roots. Sowell (1992b) applied the ML procedure
to the same data and showed that the two-step estimation procedure in
Diebold and Rudebusch (1989) might bias upwards the estimate of the
fractional differencing parameter.
9.4 Estimation of fractionally cointegrated models
Fractionally cointegrated models are discussed in Dueker and Startz
(1995) and Baillie and Bollerslev (1994). In the usual framework of
cointegration we have two series yt and xt which are 1(1) and the linear
combination of yt and Xt which is 1(0). What if this linear combination
is I(d) where 0 < d < 1. This is the case considered by Baillie and
Bollerslev (1994) which we shall discuss in the next section. But is this
the case of fractional cointegration?
Dueker and Startz (1995) argue that a broader definition of fractional
cointegration is that there exists an I(d — b) linear combination of I(d)
series with b > 0. Under this definition a continuous measure of cointe-
gration can provide more information than the I(l)/I(0) framework.
Previous studies on fractional cointegration have only considered the
fractional integration of the residuals from a cointegrating regression.
Thus, they were conditional on d = 1. For instance Baillie and Bollerslev
(1994) perform a battery of tests that fail to reject unit roots in the
nominal exchange rates they study. They estimate a cointegrating vector
by OLS and then estimate the fractional order of integration of the
cointegrating residuals. They get an estimate of d — b = .89 which is
significantly less than 1. But this test is conditional on d = 1. Cheung
and Lai (1993) also test that the hypothesis test (d—b) is less than unity,
conditional on d = 1.

9.5 Empirical relevance of fractional unit roots 
303
Dueker and Startz argue that it is important to estimate d and (d — b)
simultaneously. They consider a bivariate ARFIMA model and adopt
the ML procedure for multivariate ARFIMA models described in Sowell
(1989) to estimate d and (d — b) simultaneously. They apply this to an
example of the long-run relationship between three month and one year
treasury bill rates. They find that they can reject both the hypotheses
of nocointegration and unit cointegration.
9.5 Empirical relevance of fractional unit roots
Several empirical studies on the dynamic properties of the US macro-
economic time series showed that these series are fractionally cointe-
grated. See Diebold and Rudebusch (1989, 1991a) on output and con-
sumption, Porter-Hudak (1990) on money supply, and Shea (1991) and
Backus and Zin (1993) on interest rates. Many of these studies use the
GPH approach. However, as argued by Sowell (1992a) and Smith et al.
(1993), the ML method is more efficient than the GPH approach. Crato
and Rothman (1994) who use the ML approach seem to get higher esti-
mates of d. In the following sections we shall examine briefly the evidence
on fractional integration in foreign exchange rates, and in stock prices.
For more references and detail, see Baillie (1996).
9.5.1 Exchange rates dynamics
Cheung (1993) estimated ARFIMA (p,d,q) models for five nominal
dollar spot rates - British pound (BP), Deutsche mark (DM), Swiss
franc (SF), French franc (FF), and Japanese yen (JY) - which are end-
of-week exchange rates from January 1974 to December 1989 taken from
the Chicago Mercantile Exchange Yearbooks. The d parameter estimate
of FF is close to 0.5, those of DM, SF, and JY ranged from 0.2 to 0.3,
and that of BP is around 0.15 according to various n = T0A5,T0-5, and
T0'55. These estimates show evidence of long memory in exchange rate
changes though the result for the BP is marginal. The unit root hy-
potheses for the exchange rates are rejected in favor of the long memory
alternatives. However, impulse response function analysis indicates that
persistence in exchange rates can be difficult to detect. Furthermore,
the ARFIMA model does not outperform the random walk model in
out-of-sample forecast.
Several empirical studies using the post-1973 float exchage rates ar-
gue that the behavior of nominal exchange rates is well approximated

304 
Fractional unit roots and fractional cointegration
by a unit root process - a martingale. On the other hand, Baillie
and Bollerslev (1989) provide some evidence supporting a hypothesis
of cointegration among exchange rates. These two different emipirical
evidences imply two incompatible economic perspectives on exchange
rate dynamics. The martingale hypothesis comes from an efficient mar-
kets perspective, as does the cointegration hypothesis from a common
trends perspective. Cointegration implies that there exist one or more
long-run relationships among exchange rate levels, deviations from which
tend to be eliminated over time and are therefore useful in predicting
future exchange rate changes, whereas nothing is useful for predicting
future exchange rate changes if exchange rates evolve as a vector mar-
tingale.
Diebold, Gardeazabal, and Yilmaz (1994), hereafter DGY, compare
the out-of-sample forecasting performance of VAR models with cointe-
gration and of a martingale for the same data set used by Baillie and
Bollerslev (1989). They found that the null hypothesis of no cointegra-
tion cannot be rejected for a system of nominal dollar spot exchange
rates, 1980-1985, and that no improvements in forecasting performance
are obtained from the use of cointegrated VARs. DGY also showed that
when a drift is included into the model estimated by Baillie and Boller-
slev (1989), the evidence for cointegration vanishes. They argued that
evidence of cointegration arises only under the assumption that drift is
known to be absent.
Baillie and Bollerslev (1994) in their reply to DGY (1994) agreed
with the findings of DGY that on allowing for an intercept in the coin-
tegrating regression, the test statistics do not reject the null hypothesis
of no cointegration. On the other hand, they found that the process
of the error correction term exhibits long memory characteristics with
long-term cycles in their autocorrelations. In the original framework of
cointegration, the error correction term is presumed to be 1(0) with 1(1)
processes. The idea of cointegration, however, only requires that the
error correction term be stationary. BB suggested to consider the error
correction term to possibly be fractionally integrated in the sense that
the deviations from the cointegrating relationship possesses long mem-
ory, according to which the effect of a shock declines at a slower rate
than the usual exponential decay associated with the autocorrelation
functions for the class of covariance stationary and invertible ARMA
processes. Because of the long memory of the error correction term,
they argued that any improvement in forecasting errors may only be
apparent several years into the future.

9.6 Summary and conclusions 
305
9.5.2 Long memory in stock prices
Long memory in stock prices was discussed first in Lo and Mckinlay
(1988) and Lo (1991). The paper by Lee and Robinson (1996) contains
a detailed review of these and other studies on long memory in stock
prices and provides a semi-parametric estimation method.
9.6 Summary and conclusions
In this chapter we presented a brief overview of fractional unit roots. Our
purpose has been to emphasize the major points. Work on fractional unit
roots is exploding as is evident from the long survey by Baillie (1996).
This work falls in the following categories:
(i) To investigate the sensitivity of the different unit root tests to
fractional alternatives. This work is not very useful as most of
the tests have been found to be sensitive. What is more important
is development of new tests that can be used in the presence of
fractional roots. Examples are Robinson (1994) and Gil-Alana
and Robinson (1997). More work is needed in this direction.
(ii) Estimation of ARMA models with fractional roots (ARFIMA
models). Here a majority of applications have used the Geweke
and Poter-Hudak (GPH) two-step method. Recent work has
shown that Sowell's ML method is the preferred choice. More
work is needed in this direction.
(iii) Estimation of fractionally cointegrated models needs further study.
The paper by Dueker and Startz (1995) is a beginning. Until now
fractional cointegration has been confined to an examination of
fractional integration of the residuals from a cointegrating regres-
sion.
(iv) There is as yet not much discussion of the forecasting performance
of fractionally integrated (and cointegrated) models.
(v) Koop et al. (1997) present a Bayesian analysis of the ARFIMA
model. The advantage of the Bayesian approach is that in predic-
tion problems, the estimation uncertainty is taken into account.
Also, in the presentation of impulse response functions we can
plot the whole density instead of just presenting a point estimate
and its standard error.

306 
Fractional unit roots and fractional cointegration
References
Backus, D.K. and S.E. Zin (1993), "Long-Memory Inflation Uncer-
tainty: Evidence from the Term Structure of Interest Rates,"
Journal of Money, Credit, and Banking, 25, 681-700.
Baillie, R.T. (1996), "Long Memory Processes and Fractional Integra-
tion in Econometrics," Journal of Econometrics, 73, 5-59.
Baillie, R.T. and T. Bollerslev (1989), "Common Stochastic Trends in
a System of Exchange Rates," Journal of Finance, 44, 167-181.
(1994), "Cointegration, Fractional Cointegration, and Exchange
Rate Dynamics," Journal of Finance, 49, 737-745.
Beran, J. (1994a), Statistics for Long Memory Processes, Chapman and
Hall, New York.
(1994b), "Maximum Likelihood Estimation of the Differencing Pa-
rameter for Invertible Short and Long Memory Autoregressive
Integrated Moving Average Models," Journal of Royal Statisti-
cal Society, B, 57, 659-672.
Cheung, Y. (1993), "Long Memory in Foreign Exchange Rates," Jour-
nal of Business and Economic Statistics, 11, 93-101.
Cheung, Y. and K. Lai (1993), "A Fractional Cointegration Analysis of
Purchasing Power Parity," Journal of Business and Economic
Statistics, 11, 103-112.
Cochrane, J.H. (1988), "How Big is the Random Walk in GNP," Jour-
nal of Political Economy, 96, 893-920.
Crato, N. and P.J.F. de Lima (1996), "On the Power of Underdifferenc-
ing and Overdifferencing Tests Against Nearly Non-ststionary
Alternatives," Manuscript, Johns Hopkins University.
Crato, N. and P. Rothman (1994), "Fractional Integration Analysis of
Long-Run Behavior for US Macroeconomic Time Series," Eco-
nomics Letters, 45, 287-291.
Diebold, F.X. and G. Rudebusch (1989), "Long Memory and Persis-
tence in Aggregate Output," Journal of Monetary Economics,
24, 189-209.
(1991a), "Is Consumption Too Smooth? 
Long Memory and the
Deaton Paradox," Review of Economics and Statistics, 74, 1-9.
(1991b), "On the Power of Dickey-Fuller Tests Aganist Fractional
Alternatives," Economics Letters, 35, 155-160.
Diebold, F.X., J. Gardeazabal, and K. Yilmaz (1994), "On Cointegra-
tion and Exchange Rate Dynamics," Journal of Finance, 49,
727-735.

References 
307
Dueker, M. and R. Startz (1995), "Maximum Likelihood Estimation of
Fractional Cointegration with an Application to the Short End
of the Yield Curve," Working paper, October 1995, University
of Washington.
Fox, R. and M.S. Taqqu (1986), "Large Sample Properties of Parameter
Estimates for Strongly Dependent Stationary Gaussian Time
Series," Annals of Statistics, 14, 517-32.
Geweke, J. and S. Porter-Hudak (1983), "The Estimation and Appli-
cation of Long Memory Time Series Models," Journal of Time
Series Analysis, 4, 221-38.
Gil-Alana, L.A. and P.M. Robinson (1997), "Testing of Unit Root and
Other Non-stationary Hypotheses in Macroeconomic Time Se-
ries," Journal of Econometrics, 80, 241-268.
Granger, C.W.J. (1980), "Long Memory Relationships and the Ag-
gregation of Dynamic Models," Journal of Econometrics, 14,
227-238.
Granger, C.W.J. and R. Joyeux (1980), "An Introduction to Long-
Memory Models and Fractional Differencing," Journal of Time
Series Analysis, 1, 15-39.
Hassler, U. and J. Wolter (1994), "On the Power of Unit Root Tests
Against Fractional Alternatives," Economics Letters, 45, 1-5.
Hosking, J.R.M. (1981), "Fractional Differencing," Biometrika, 68,
165-176.
Koop, G., E. Ley, J. Oseiwalski, and M.F.J. Steel (1997), "Bayesian
Analysis of Long Memory and Persistence Using ARFIM A Mod-
els," Journal of Econometrics, 76, 149-169.
Lee, D. and P.M. Robinson (1996), "Semiparametric Exploration of
Long Memory in Stock Prices," Journal of Statistical Planning
and Inference, 50, 155-174.
Lee, D. and P. Schmidt (1996), "On the Power of the KPSS test of Sta-
tionarity Against Fractionally Integrated Alternatives," Journal
of Econometrics, 73, 285-302.
Li, W.K. and A.I. McLeod (1986), "Fractional Time Series modeling,"
Biometrika, 73, 217-221.
Lo, A.W. (1991), "Long-Term Memory in Stock Prices," Econometrica,
59, 1279-1314.
Lo, A.W. and A.C. MacKinlay (1988), "Stock Market Prices Do Not
Follow Random Walks: Evidence from a Simple Specification
Test," The Review of Financial Studies, 1, 41-66.

308 
Fractional unit roots and fractional cointegration
Porter-Hudak, S. (1990), "An Application of the Seasonal Fractional
Difference Model to the Monetary Aggregates," Journal of
American Statistical Association, 85, 338-344.
Robinson, P.M. (1994), "Efficient Tests of Non-stationary Hypotheses,"
Journal of American Statistical Association, 89, 1420-1437.
Shea, G.S. (1991), "Uncertainty and Implied Variance Bounds in Long-
Memory Models of the Interest Rate Term Structure," Empirical
Economics, 16, 287-312.
Smith, A.A., F. Sowell, and S.E. Zin (1993), "Fractional Integration
with Drift: Estimation in Small Samples," Mimeo, Carnegie
Mellon University.
Smith, J. and S. Yadav (1994), "The Size and the Power of Unit Root
Tests Against Fractional Alternatives: A Monte Carlo Investi-
gation," Discussion paper No. 419, University of Warwick.
Sowell, F. (1989), "Maximum Likelihood Estimation of Fraction-
ally Integrated Time Series Models," unpublished manuscript,
Carnegie-Mellon University.
(1990), "The Fractional Unit Root Distribution," Econometrica, 58,
495-506.
(1992a), "Maximum Likelihood Estimation of Stationary Univariate
Fractionally Integrated Time Series Models," Journal of Econo-
metrics, 53, 165-188.
(1992b), "Modeling Long-Run Behavior with the
Fractional ARIMA Model," Journal of Monetary Economics,
29, 277-302.
Yajima, Y. (1985), "On the Estimation of Long Memory Time Series
Models," Australian Journal of Statistics, 27, 303-320.
(1988), "On the Estimation of a Regression Model with Long Mem-
ory Stationary Errors," Annals of Statistics, 16, 791-807.

10
Small sample inference: bootstrap methods
10.1 Introduction
The inferential procedures that we discussed in the previous chapters are
all based on asymptotic theory. The Monte Carlo results presented in
chapter 5 (section 5.7) throw light on the small sample behavior of the
different estimation methods, but once an estimation method is chosen,
there is still the question of appropriate inference on the parameters
estimated. The relevant procedures for this are again asymptotic.
Some methods for obtaining small sample results analytically like
Edgeworth expansions involve a lot of tedious algebra and are also ap-
plicable only in some special cases. The bootstrap method initiated by
Efron (1979) provides a viable alternative. Another alternative is to
use the Bayesian methods (discussed in chapter 8) but they are based
on a different philosophy. Reviews of the bootstrap methods discussed
here can be found in Jeong and Maddala (1993), Vinod (1993), Li and
Maddala (1996), and Horowitz (1997).
10.2 A review of the bootstrap approach
The bootstrap method is a resampling method. Several resampling
methods were in use earlier but they were disparate. Efron made the
resampling method a widely applicable technique. For a history of the
resampling approach going back to early papers by Barnard (1963) and
Hartigan (1969), see Hall (1992).
Let (2/1,2/2?#' • ?2/n) De a random sample from a distribution charac-
terized by a parameter 8. Inference about 0 will be based on a statistic
T. The basic bootstrap approach consists of drawing repeated sam-
ples (with replacement) of size m (which may or may not be equal
309

310 
Small sample inference: bootstrap methods
to n, although it usually is) from (2/1,2/2? *'• >2/n)« Call this sample
(2/1 > 2/2 5 •*• >2/m)- This ^s the bootstrap sample. We do this 5 times.
5 is the number of bootstrap replications. For each bootstrap sample
we compute the statistic T. Call this T*. The distribution of T* is
known as the bootstrap distribution of T. We use this bootstrap dis-
tribution to make inferences about 8. Under some circumstances (to be
described later) the bootstrap distribution enables us to make more ac-
curate inferences than the asymptotic distribution of T. The bootstrap
method described here is the simplest one that is valid for iid obser-
vations. Needless to say that when the iid assumption is not satisfied
this method needs to be modified. These modifications in the context
of time series models are described later.
Early work on the application of bootstrap methods merely consisted
of using the bootstrap distribution to get standard errors. The standard
error of T* was used as a better estimate of the small sample standard
error of T than that given by the asymptotic distribution of T. In some
complicated models the derivation of asymptotic standard errors (SEs)
is extremely complicated and in this case the bootstrap procedure for
computation of the SEs were the only ones available. See Veall (1988) for
this sort of argument. However, as remarked by Hartigan (1986, p. 76)
in these cases one should use the bootstrap standard error to construct
a pivotal statistic and then bootstrap this again for the construction of
confidence intervals or testing of hypotheses. In those cases where the
asymptotic SEs were available, the bootstrap SEs were justified as a
finite sample approximation. However, the standard error is of interest
only if the small sample distribution of T is known to be normal. In most
econometric applications this is not the case. Thus, using the bootstrap
distribution to get standard errors in this situation is useless. Confi-
dence interval estimation and hypothesis testing can be based directly
on the bootstrap distribution. For many early applications on the use of
just bootstrap standard errors in econometrics, see Jeong and Maddala
(1993, p. 580). It is not necessary to quote these misapplications here.
Even if the asymptotic and bootstrap standard errors are the same in
any given example, confidence interval statements from the bootstrap
distribution will be different from the confidence interval based on the
asymptotic distribution if the bootstrap distribution is skewed. For the
autoregressive model yt = pyt-i +£t based on Wolfer's sunspot numbers
for 1770-1889, Efron and Tibshirani (1986) got p = 0.815 with asymp-
totic SE = 0.0.53. The bootstrap SE based on 1,000 replications was
0.055 agreeing nicely with the asymptotic results. However, the boot-

10.2 A review of the bootstrap approach 
311
strap distribution was skewed to the left. We shall now list some results
for confidence interval estimation, hypothesis testing, and methods of
generation of bootstrap samples.
10.2.1 Confidence intervals
Many of the results on the properties of bootstrap confidence intervals,
that we shall quote now, have been derived using Edgeworth expansions.
The use of Edgeworth expansions to study the properties of bootstrap
methods started with the papers by Singh (1981) and Bickel and Preed-
man (1980). They considered the distribution of the sample mean with
known variance. For the case of unknown variance and the distribution
of the studentized mean, see Abramovich and Singh (1985); see Hall
(1992, p. 151) for a history of these methods. As Hall points out (in
appendix V) there are some limitations to the results obtained from
Edgeworth expansions. However, many of the conclusions that have
been derived from the Edgeworth expansions have been substantiated
in the Monte Carlo studies on models of interest to econometricians.
Hence, we shall state the main results.
Suppose 9 is a consistent estimator for #, and 6* is the bootstrap esti-
mator of 9. We consider the following methods to construct confidence
intervals:
(i) the asymptotic distribution of 9.
The two-sided confidence interval is 9 ± zaSE(9) where SE(§)
is the asymptotic standard error of 9 and za is the (100 — a)
percentile from the standard normal distribution. This interval
is a symmetric interval.
(ii) Use the bootstrap distribution of 6*.
The two-sided (100 —2a) confidence interval for 9 is (9 — Zi_a,9 +
z*) where z* is the 100a percentile of the distribution of 9* — 9.
This is a two-sided equal-tailed interval. It is often nonsymmetric.
This is called the percentile method. The nominal coverage of
this interval is (100 — 2a) and the difference between the actual
coverage and nominal coverage is called the coverage error. To
improve on the coverage error, Efron (1987) suggested two other
percentile intervals (see (iii) and (iv) below).
(iii) Bias-corrected (BC) percentile interval.

312 
Small sample inference: bootstrap methods
(iv) Accelerated bias-corrected (BCa ) percentile interval.
In addition there are the following other methods.
(v) The percentile-t or bootstrap-t methods (see Hall, 1988b, 1992).
This is the percentile method based on the bootstrap distribution
of the t-statistic
where s is a y/n consistent estimator of the standard error of
\fn(Q — 6). This procedure is often referred to as studentization.
t is said to be asymptotically pivotal. A pivotal statistic is one
whose distribution is independent of the true parameter 0. Har-
tigan (1986) stressed the importance of using a pivotal statistic
(see also Beran, 1987, 1988).
The bootstrap-t method requires a stable estimate of the stan-
dard error while the BCa method involves calculation of the
so-called acceleration constant which depends on an estimate of
skewness. In some cases like the Johansen procedure for cointe-
grated systems, the bootstrap-t has been found to perform poorly.
This is because of the unstable variance of the Johansen estima-
tor. On the other hand the bootstrap-t is more often used than
the BCa in econometric work because it is easier to compute.
However, Efron suggests (in his comment on Maddala and Li,
1996) that the BCa is not necessarily more difficult. A program
for computing the BCO interval described in the appendix of Efron
and Tibshirani (1993) written in language S can be obtained by
sending an e-mail to: statlib@lib.stat.cmu.edu with the mes-
sage: "send bootstrap.funs from S."
These procedures for the construction of confidence intervals
are all reviewed in DiCiccio and Romano (1988) and Hall (1988b,
1992) and we shall not repeat the details. The coverage errors
have been shown to be (^(n"1/2) for (i), (ii), and (iii) and C^n"1)
for (iv) and (v) (where n is the sample size).
These results apply to coverage errors in each trial. Hall (1988a)
shows that under quite general circumstances, the symmetric
bootstrap-t confidence interval has coverage error O(n~2). 
It
is also proved that symmetric intervals are not necessarily any
longer than equal-tailed intervals. In fact, Hall shows that in the
case of the slope parameter in a simple regression, symmetric con-
fidence intervals with coverage between 68 percent and 97 percent

10.2 A review of the bootstrap approach 
313
tend to be shorter than their equal-tailed counterparts. (For the
99 percent confidence level, the symmetric confidence interval is
longer than the two-sided interval.) These results are based on
the so-called small sample asymptotics (that is Edgeworth expan-
sions) and hold for skew distributions.
The important conclusion that follows from these results is
that, using the simple percentile method (ii) cannot be expected
to produce an improvement over the asymptotic result (i). Many
econometric applications, as we shall see, are based on (ii).
Some particular cases are of interest for us. Hall (1989) shows
that in a regression model y = a + fix + e, in the case of slope
parameters, (3, the bootstrap-^ produces two-sided intervals with
coverage error O(n~2) and one-sided intervals with coverage error
O(n~~3/2). For the constant term a or the conditional mean (a-\-
/3xo), the bootstrap-^ has coverage error O(n~1) only.
(vi) Beran's pivotal method.
This was introduced in Beran (1987) and studied in Beran (1988,
1990). This is an iterative bootstrap method that involves a boot-
strap within a bootstrap. Hence it is computationally intensive.
This method applied to the symmetrical bootstrap-^ leads to a
coverage error O(n~3) (see Hall, 1988a).
Hall (1988b) argues in favor of the bootstrap-£ method in pref-
erence to Efron's BCa. He shows that the difference between the
bootstrap-^ and the BCa limits is O(n~3/2). But more impor-
tantly, the BCa method involves tedious analytical corrections,
which the bootstrap methods are designed to avoid. In econo-
metric work where the models are more complicated than those
considered in these papers, the bootstrap-^ is easier to use than
the BCa method, and hence is the preferable one. Rilstone (1993)
compares the BCa method with bootstrap-^ in a Monte Carlo
study and finds that the bootstrap-t performs better.
In addition to these methods there are other methods discussed in Di-
Ciccio and Romano (1990). These are the automatic percentile method,
as an alternative to the BCa that does not require explicit calculation
of the acceleration constant, nonparameteric tilting, Tibshirani's (1988)
variance stabilized bootstrap-^ and a couple of others. The nonparame-
teric tilting method was introduced by Efron (1981a). These methods,
though promising, have not found widespread acceptance.
Kilian (1996) suggests a different bias corrected confidence interval.

314 
Small sample inference: bootstrap methods
He suggests what he calls bootstrap after bootstrap. This is motivated as
follows: let 0{x) be the initial estimator of 9 which we use in generating
bootstrap samples. Let the mean of the bootstrap estimator §(x*) be
denoted by 0*. Then the bias corrected estimator is
Obcix) = §(x) + (0(x) - 0*)
Killian's idea is that if we bootstrap 0&c we will get better confidence
intervals than if we bootstrap 0. Thus, we use the first bootstrap to
get bias correction and then another bootstrap to get the confidence
interval.
Note that the term bias correction in the literature on bootstrap con-
fidence intervals refers to bias in the coverage probability and not a
correction of the bootstrap estimator for bias which is what Killian's
method involves. However, Killian shows that his method works very
well in his application, compared with the percentile method. More de-
tailed studies are necessary to compare it with Efron's procedure as well
as the bootstrap-1.
Several objections have been raised to the bootstrap-£ method. These
are:
(i) It produces bad results if the estimate of the variance is poor.
(ii) The bootstrap-^ method is not invariant to transformations.
If it produces a confidence interval (a, b) for 9, and / is a monotone
increasing function of 9, then the percentile-t does not, in general, give
[/(a),/(6)] as a confidence interval for f(0). This is quite troublesome
in many econometric applications where Wald tests for nonlinear hy-
potheses are often used and the Wald test statistic depends on how
the nonlinear constraint is formulated. For instance HQ : /?i = — fi^1
and HQ : /?i/?2 = — 1 a r e equivalent formulations. By contrast, the
likelihood-ratio test and Rao's score test (LM test) are invariant to the
different parameterizations. Gregory and Veall (1985) show that there
are substantial size distortions with the use of the Wald test statistic,
and these differ considerably under different formulations. This size dis-
tortion problem and the problem of invariance with the Wald statistic
have been emphasized by many others. Horowitz and Savin (1992) argue
that the bootstrap-based critical values for the Wald test solve the size
distortion problem and that in some cases the power of the Wald test is
higher than that of the LR test. Hence, the invariance of the Wald test
need not be much of a concern. Jeong and Maddala (1996) also correct
the size distortions in the Wald statistic using bootstrap methods.

10.2 A review of the bootstrap approach 
315
10.2.2 Hypothesis testing
In the statistical literature, hypothesis testing using critical values from
bootstrap distributions has received less attention than construction of
confidence intervals. (In the econometric literature, it is perhaps the
reverse.) There are a few exceptions. Beran (1988) shows that if the
asymptotic distribution of the test statistic is pivotal under the null,
then under some regularity conditions the size of the bootstrap test has
an error of a smaller order than the size of the asymptotic theory test.
Hinkley (1988) discusses briefly bootstrap tests of significance and this
is followed by a more detailed discussion in Hinkley (1989). Hall and
Wilson (1991) and Hall (1992) provide two general guidelines in hypoth-
esis testing, which we shall discuss below. These guidelines have been
violated in econometric practice but with good reasons. In the econo-
metric literature the importance of using pivotal statistics in hypothesis
testing is discussed in Horowitz (1994).
The familiar duality between hypothesis testing and construction of
confidence intervals is maintained under bootstrap methodology as well.
Given this duality, what we said earlier regarding the importance of
(asymptotically) pivotal statistics in the construction of confidence in-
tervals and the orders of approximation for the different methods applies
to hypothesis testing as well. Thus, it is important to apply significance
tests using (asymptotically) pivotal statistics. Otherwise, one cannot
expect much of an improvement over the asymptotic results. This is
confirmed, for instance, by the conflicting results in Veall (1986) who
finds that the performance of bootstrap is no better than that of asymp-
totic theory, and Rayner (1991) and Rilstone (1993) who come to the
opposite conclusion. The former bootstraps the coefficients J3 and the
latter two bootstrap (3/SE(f3) which is pivotal. Other examples of this
will be provided in the following sections.
Two important issues concerning hypothesis testing using bootstrap
methods relate to the questions about
(i) what test statistic to bootstrap and
(ii) how to generate the bootstrap samples.
We have said that it is important to use a pivotal (or asymptotically
pivotal) statistic in hypothesis tests. But there is another issue that has
been brought up by Hall and Wilson (1991). Suppose that we want to
test the hypothesis Ho : 0 — 0o versus Hi : 0 ^ 0O. Given an estimator
0 of 0, the usual test procedure would be based on T = 0 — 0Q and the

316 
Small sample inference: bootstrap methods
significance level and p-values are obtained from the distribution of T
under Ho. A direct application of the bootstrap procedure would suggest
using the bootstrap distribution of T* = 6* — 9$ instead of the distri-
bution of T (0* is the value of 9 from the bootstrap sample). However,
Hall (1992, section 3.12) discusses the bad behavior of the power of this
test arguing that T* does not approximate the null hypothesis when the
sample comes from a distribution with parameter 9 far away from 9Q.
Hall and Wilson, therefore consider another bootstrap procedure based
on the empirical distribution of (9* — 9). They compare this with the
test procedure based on T and T*.
Hall and Wilson propose two guidelines for hypothesis testing. The
first suggests using the bootstrap distribution of (9*—9) but not (9*— 0Q).
The second guideline suggests using a properly studentized statistic, that
is (0* - 0)/<r* but not (0* - §)/& or (0* - 0), where a* is the estimate
of <7 from the bootstrap sample.
Van Giersbergen and Kiviet (1994) discuss these two rules in the con-
text of hypothesis testing in regression models. To simplify the exposi-
tion, we shall discuss the case of a simple regression
y = f3x + e, e ~ iid(0,a2)
Let f3 and a be the OLS estimators of (3 and a respectively and e the
OLS residuals. Let i* be the bootstrap residuals obtained by resampling
i. The null hypothesis to be tested is Ho : (3 = /?o versus Hi : f3 ^ /?o-
Consider two sampling schemes for the generation of the bootstrap
samples
S1:y*=0x + e*
S2:y*=f3ox + e*
Both use e* but they differ the way y* is generated. For each sampling
scheme consider two t-statistics
TnT0) = {$*-$)/**
T2:T(/3o) = 0*-&>)/&*
Thus four versions of the t-statistic can be defined. Hall and Wilson
consider sampling scheme Si only and suggest using 7\ only. They
do not consider sampling scheme S2 which is the appropriate one for
statistic T2. Van Giersbergen and Kiviet suggest, on the basis of a
Monte Carlo study of an AR(1) model, the use of T2 under sampling
scheme S2 in preference to T\ under S\. The main conclusions of the
paper are:

10.2 A review of the bootstrap approach 
317
(i) Inference based on T2 under Si does not just have low power but
in fact has size close to zero. Similarly T\ under S2 does not work
and should not be used. The resampling scheme should mimic
the null distribution of the test statistic to be bootstrapped.
(ii) Using T\ under S\ and T2 under 52 are equivalent in nondynamic
models. This equivalence extends to the multiparameter case if
one bootstraps the appropriate F-statistic. However, in dynamic
models this equivalence breaks down in finite samples. The Monte
Carlo results suggest that it is better to use T2 under S2.
(iii) The limiting distributions of T\ under Si and T2 under S2 are
identical even with dynamic models. The conclusion that T2 un-
der S2 is better is based on small sample performance.
One can also propose another resampling scheme
S3 : y* = fox + el
where £Q is the bootstrap sample from So = y — fox (after centering).
Note that in both Si and S2 we use resampling based on the OLS resid-
uals i. If the null HQ : (3 = fo is true but the OLS estimator (3 gives
a value of j3 far from fo, the empirical distribution of the residuals will
suffer from a poor approximation of the distribution of the errors under
the null. The intuition behind S3 is as follows. If the null hypothesis
is true, so = y — fox is exactly the true distribution of the regression
errors. Hypothesis testing based on this will give (approximately) the
correct test size. If the null is not true, then £0 is different from the true
distribution of the errors. Hypothesis testing will give proper power
depending on how far the null is away from the true value of /?. Thus,
using T2 under S3 is better than using T\ under Si or T2 under S2 which
are identical as shown in van Giersbergen and Kiviet. The idea of us-
ing restricted regression errors for resampling has been used in Li and
Maddala (1997) and Nankervis and Savin (1996).
As we shall discuss in section 10.4, for the unit root model, test statis-
tic T2 under sampling scheme S2 or S3 are the only ones that can be
justified on asymptotic grounds. It is not appropriate to use T\ under
Si.

318 
Small sample inference: bootstrap methods
10.2.3 Generation of bootstrap samples: residual-based
versus direct methods
In the preceding section we discussed some issues about the generation
of bootstrap samples and how this is related to the hypothesis test under
consideration. The resampling methods were all residual-based methods.
We shall now discuss issues related to direct methods of resampling data.
For the case of random regressors (which he calls the correlation model
as opposed to the regression model) Freedman (1981) suggests resam-
pling the pair (y, X), which have a joint distribution with E(y\X) = X/3.
In this model the pairs {y^Xi) are assumed to be iid. Given a sample
of size n we can compute J3, the least squares estimator of /?. Denote
this by P(n). Then y/n0(n) — j3) is asymptotically normal with mean 0
and a certain covariance matrix which we shall denote by E. Let m be
the size of the bootstrap sample and denote the bootstrap estimator by
/3*(m). Freedman (1981, p. 1226) shows that under some conditions, as
m —> oo and n —> oo
What this result shows is that the bootstrap distribution based on
direct sampling is useful as an approximation of the asymptotic distri-
bution of y/n(J3(n) — (3). In cases where the asymptotic distribution can
be derived, this result is not of much use except as a consolation that
using the bootstrap is alright. In this case the bootstrap method can
not be expected to improve upon the asymptotic result without using
a pivotal method. However, in the case of heteroskedasticity of an un-
known form, this direct method even without pivoting is useful. In fact,
in this case, even if the model is the usual regression model with fixed
Xs (as opposed to the model with random Xs) the direct method of
sampling (yi,Xi) is useful because it gives an estimate of the correct co-
variance matrix E (see Jeong and Maddala, 1993, pp. 577-578). On the
other hand, if one uses White's heteroskedasticity-consistent covariance
matrix estimator, one can obtain asymptotically valid pivotal statistics
in the presence of heteroskedasticity of an unknown form. In practice,
this would be the preferred approach. An alternative method to handle
heteroskedasticity of an unknown form is the wild bootstrap suggested
by Hardle and Mammen (1990).
Efron (1981b) uses the direct method of resampling the data in a
problem involving censored data. He bootstraps the data (xi,di) where
di = 1 if Xi is not censored and d\ = 0 if Xi is censored. His model

10.2 A review of the bootstrap approach 
319
does not have covariates. Some applications of this direct method to
censored regression models (that is models with covariates) are reviewed
in Jeong and Maddala (1993, pp. 591-594). The problem here is one of
nonlinearity of an unknown form. But the basic question is: what do you
do after generating the bootstrap sample? Suppose you have the data
on a latent variable yi which is observed as a dummy variable di{— 1 or
0) and a set of covariates X{. We draw the bootstrap sample (d*,X*).
What do we do next? If we are going to estimate a logit model, then
we can also estimate the logit model with the actual data, compute the
generalized residuals, and resample the generalized residuals to generate
bootstrap samples. In this case the direct bootstrap method does not
make use of all the information as does the method based on generalized
residuals. It is only if some general semiparametric method of estimation
is used that the direct bootstrap method would be useful. We cannot
go into a detailed discussion of the appropriate bootstrap methods for
the censored regression model here, but we will argue that the above
considerations carry over to time series models as well.
The direct method of sampling the data, rather than the residuals,
has not gone unnoticed in econometrics. The earliest use is in Kiviet
(1984) which we will discuss later. Veall (1987, p. 205) considers the
direct bootstrap approach but rejects it on grounds that it does not
embody all the information used in the residual-based approach. Li and
Maddala (1996) actually use the direct sampling approach combined
with the moving block method (discussed in section 10.4) to data vectors
(yi, Xi) in cointegrating regression models and find its performance worse
than that of the residual-based bootstrap. Here we shall review some
problems with the use of the direct bootstrap approach in econometric
applications once we move out of the framework of the usual regression
model. These problems have not received sufficient attention.
Consider first the lagged dependent variable model. Freedman and
Peters (1984) consider a q variable model and introduce the recursive
bootstrap method. The model is
where Yt and Xt are (1 x q) and (lxp) matrices, respectively, and B and
C are (q x q) and (p x q) matrices, respectively. The St are assumed to be
iid(0, E) and Yo is assumed to be known. The B, C are first estimated
using a system method (that takes care of contemporaneous correlation
among the errors et), and the residuals it = Yt — Yt-\B — XtC are
calculated. Bootstrap samples e^ are generated. Since the entire vector

320 
Small sample inference: bootstrap methods
of residuals is resampled, this preserves the contemporaneous correlation
structure of the errors. The bootstrap sample Yt* is generated in a
recursive fashion, assuming Xt, and Yo are given, using the equation
Now re-estimate the model using (Yt*,Xt, and Yo). This method was
used in an empirical application with q = 10 and T = 18. It is shown
that the conventional asymptotic standard errors are substantially (by
about 40 percent-50 percent) below the bootstrap standard errors. The
empirical results, of course, merely say that the asymptotic and boot-
strap standard errors are different. There is no way of telling without a
Monte Carlo study how biased each is. Freedman and Peters argue that
the problem lies in the estimation of the covariance matrix £ in the con-
ventional procedure, which can be poor with such a limited sample. It is
also worth noting that Preedman and Peters concentrated on the stan-
dard errors and did not consider bootstrapping the students ^-statistic,
which on asymptotic grounds, is expected to give better confidence in-
tervals, as argued earlier.
Suppose we abstract from the multiple equation issue and concentrate
on the estimation of a simple dynamic equation
Vt = A) + Piyt-i + foxt + foxt-i + et 
(10.1)
For this model Kiviet (1984) presents the results of a Monte Carlo study
and finds that the bootstrap does not improve on conventional asymp-
totic inference. Even though this paper is out of date (and was never
published) there are a few points in this paper worth noting which are
useful for further work. Kiviet considered two error structures: one in
which et are JiV(0,l) and the other in which et are 7L(0,l) where L is
the Laplace distribution. Also the problem may have been caused by
the use of the percentile method which is known to give no improvement
over asymptotic results. The bootstrap-£ bootstrap as discussed earlier,
should do better. One suspects from Kiviet's Monte Carlo study that
the substantial discrepancies observed by Preedman and Peters between
the asymptotic and bootstrap standard errors may have been caused by
the high dimensionality of their system (q = 10). However, one needs to
investigate this by a Monte Carlo study.
It is interesting to note that Kiviet considers the recursive method
suggested by Preedman and Peters (1984) but he also considers a direct
resampling scheme whereby the data vector (yt,yt-i>xtixt-i) 
ls boot-
strapped. This did not give any improvement over asymptotic results

10.2 A review of the bootstrap approach 
321
either. What this suggests is that the important issue is not residual
sampling versus direct sampling of data but one of percentile versus
bootstrap-^. In fact this dynamic model was analyzed by van Giersber-
gen and Kiviet (1993) using the bootstrap-^ with substantial improve-
ment over the asymptotic results (although an iterative bootstrap-t that
they suggest, did better), as far as the confidence interval construction
is concerned.
There are, in fact, more problems with the direct method in this
case. Suppose the errors et in (10.1) are AR(1). Then there is a serial
correlation bias in the OLS parameters in (10.1). Suppose that the in-
formation that et in (10.1) are AR(1) is used in the estimation of the
model from the bootstrap sample, then this information is incorporated
in the residual method but is not used in the direct method of gener-
ation of the bootstrap sample by resampling the data. Thus, we feel
that the direct method ignores some important information utilized in
the residual-based sampling in the generation of bootstrap samples of
dynamic models.
Our basic argument is that whatever information about the error term
is used in the estimation of the model from the bootstrap samples, should
also be used in the generation of the bootstrap samples. This is done
in the residual-based bootstrap generation but not in the method of
bootstrapping the data. This issue is not important in the case of models
with no lagged dependent variables, where one can bootstrap the data
and use White's heteroskedasticity-consistent covariance matrix to get
asymptotic pivotal statistics and thus get asymptotic refinements using
bootstrap methods. But in the case of lagged dependent variables with
serially correlated errors, there is also a problem of consistency of the
resulting estimators if the order of serial correlation is not properly taken
into account.
Turning next to the application of the direct method to the cointe-
grated regression models, the problems are similar. Suppose that yt and
Xt are 1(1) and we have the regression equation
yt = 0xt + ut 
(10.2)
Suppose we bootstrap the data (yt,xt) and estimate equation (10.2) by
OLS. If Ut is also 1(1), then it is well known that equation (10.2) is a
spurious regression. But there is no way of knowing this if we use the
direct bootstrap method, without first testing whether (10.2) is indeed
a cointegration relationship.
Suppose we initially apply cointegration tests to equation (10.2) and

322 
Small sample inference: bootstrap methods
make sure that equation (10.2) is a meaningful regression relationship.
This implies that yt and xt are 1(1) and Ut is 1(0). (An 1(0) variable is
stationary and an 1(1) variable is stationary in first differences.) But the
direct bootstrap method does not use the information that ut is 1(0).
This point is discussed further in section 10.6. In fact, in Li and Maddala
(1997) the direct method was used in several Monte Carlo studies of the
comparison of the moving block procedure with asymptotic methods
and it was found that the performance of the direct method was slightly
worse than the one based on bootstrapping the residuals, although the
direct method provided an improvement over asymptotic results. It
should be noted that Li and Maddala considered bootstrapping a t-
statistic in both cases and hence that is not an issue here.
We have reviewed several cases where the direct method of bootstrap-
ping the data has been used. We have also outlined some problems in the
use of the direct method as compared to bootstrapping residuals. These
problems have not been appreciated in econometric work and it is often
thought that the direct method, which is the simplest, is universally
applicable.
10.3 The AR(1) model
The simplest AR(1) model is given by
yt = pyt-i +et
with ?/o — 0? Ut ~ iid(0,cr2) and — oo < p < oo. We can divide p into
three regions. When \p\ < 1 the process {yt} is stationary. When \p\ = 1
the process is unstable. This is known as the unit root model. When
\p\ > 1 the process is explosive. The unit root and explosive cases are
discussed in the next section. Here we shall discuss the stationary case.
The AR(1) model with intercept is
yt = a + pyt-i + eu 
et ~ iid(0, a2)
However, since the distribution of the OLS estimator p of p is invariant
to a and cr2, we might as well set a = 0 and a2 = 1 and consider the
model
Vt = PVt-i + eu 
et ~ nd(0,1)
Dufour (1990) and Andrews (1993) have developed exact inference pro-
cedures for the AR(1) parameter but these depend on the normality
assumption of the errors. Hence bootstrap methods which are robust to

10.3 The AR(1) model 
323
distributional assumptions of the errors hold promise. The procedure
for the generation of the bootstrap samples is the recursive procedure
described in the previous section. However, after computing the least
squares residuals Ut and the bootstrap residuals tt£, we use po but not p
in generating y\. This is sampling scheme 52 discussed in section 10.2.2.
The first bootstrap application of this model is by Rayner (1990). He
uses the bootstrap-^ (or the bootstrap-^ method) and is concerned with
the problem of testing the hypothesis Ho : p = po- He does not examine
the percentile method, but examines the student statistic
t = (P - Po)/SE(p)
He considers approximating the distribution of this ^-statistic by the
bootstrap distribution of t* which is defined as
t* = (P* - Po)/SE(p*)
where p* and SE(p*) are the values of p and SE(p) computed from the
bootstrap sample. This violates the Hall and Wilson rule which says
that p but not po should be used in t*. However, Rayner modifies the
sampling rule by using p0 but not p in generating y* (see section 10.2.2),
that is, he uses the ^-statistic T2 with sampling rule S2).
Rayner finds that the use of the student-t approximation is not satis-
factory, particularly for high values of p and that the bootstrap-t per-
forms very well in samples of sizes five to ten, even when mixtures of
normal distributions are used for the errors. He also argues that the
bootstrap approximates the power well for samples as small as five to
ten. Finally, for the starting value yo> he says, it is important to use a
random starting value (from the equilibrium distribution of yt).
Rayner's argument that the bootstrap sample generation should be
consistent with the null hypothesis being tested is correct (see the dis-
cussion in section 10.2.2). So is his use of the bootstrap-^ instead of
the percentile method. However, his conclusion about the sample sizes
(as small as five to ten) at which the performance of the bootstrap is
good is very surprising. So is his conclusion regarding the robustness
of the procedure to departures from normality with small sample sizes.
Subsequent studies have failed to confirm these conclusions.
Van Giersbergen and Kiviet (1994) do a Monte Carlo study with the
same model as Rayner. In addition to the bootstrap-^ they also consider
the usual percentile method and an iterative bootstrap-t. The sampling
scheme used was T\ and S\ in the case of percentile and bootstrap-t
methods and T2 and S2 in the case of the iterative bootstrap-^ method.

324 
Small sample inference: bootstrap methods
As expected, the percentile method did not perform any better than the
asymptotic theory. The bootstrap-t method performed much better but
even for a sample size n = 40, the bootstrap-^ gave a significance level of
0.08 compared to a nominal significance level of 0.05. In their study, the
iterative bootstrap-^ did better giving a significance level of around 0.05
even for samples of size n = 20. As far as robustness to nonnormality
is concerned, they investigated a shifted \2 distribution (shifted to have
mean zero), truncated ^-distributions, and ARCH disturbances. The
performance of all the methods deteriorated, particularly with ARCH
disturbances, but the iterative bootstrap-t performed best. As men-
tioned in section 10.2.3, these authors also discuss an AR(1) model with
an exogenous regressor and again find the performance of the iterative
bootstrap-£ the best. Due to space limitation, we shall not discuss their
iterative bootstrap-£ method. Details of this method can be found in
van Giersbergen and Kiviet (1993).
There are a few differences in the way the bootstrap sample is gen-
erated in the van Giersbergen-Kiviet study. They condition the sample
generation on y0, unlike the case in Rayner's study. They also adjust
the least squares residuals by centering and scaling (see the description
in the previous section).
Nankervis and Savin (1996) also replicate Rayner's study using Rayner's
method of treating y0. They consider three models: (i) the AR(1) model,
(ii) a linear trend model with AR(1) errors, and (iii) the unit root model.
First Nankervis and Savin use the test statistic T2 with sampling
scheme S3 (see section 2.2) but not 52 as done by Rayner. Thus, their
results are not exactly comparable to Rayner's. They find that the
bootstrap-/; test has the correct level for a sample size of ten when the
error distribution is normal but it suffers substantial size distortions
when the errors follow a mixture of normal or lognormal distributions,
although the distortions are less than with the usual t-test. However, for
samples of size 50, the bootstrap-t test has the correct size even for these
nonnormal distributions. Also, the empirical power of the bootstrap-t
test is identical to that of the usual t-test assuming normality. Thus,
there is no loss of power in using the bootstrap-^ test even when the
errors have a normal distribution. We shall skip more details, since this
is a very restricted model.
In summary, the bootstrap-^ test performs well in having the correct
size and good power, for samples of size 50 (lower if errors are normal).
The results presented by Rayner are not likely to be extended to non-
normal errors because the sample sizes he considers are very low (five

10.4 Bootstrapping unit root tests 
325
to ten). The major differences between the three studies by Rayner,
van Giersbergen-Kiviet, and Nankervis-Savin, are in the treatment of
2/o and the way that any information of the form | p | < 1 is exploited
in the construction of confidence regions. Van Giersbergen and Kiviet
(1996) extend these methods to bootstrapping a stable AD model.
10.4 Bootstrapping unit root tests
The literature on unit root testing is enormous and it is not possible
for us to consider all the issues in bootstrapping the different unit root
test procedures. The issues relate, among other things, to devising more
powerful tests than the standard Dickey-Fuller test and its extensions,
considering stationarity as null versus unit root as null, and using the
Bhargava (1986)-type structural approach instead of the Dickey-Fuller
type reduced-form approach. The conflict between the structural versus
reduced form approaches has also been noted in the Bayesian approach
to unit root testing and Peter Phillips argues in favor of the Bhargava
approach. For an exposition of Phillips' arguments and some recompu-
t at ions, see Zivot (1994).
In spite of its defects, we shall follow the Dickey-Fuller reduced-form
approach in what follows. This is because we wish to concentrate on
the major issues pointed out in section 10.2 relating to the type of test
statistics to use and the type of sampling scheme to be used. There is
some confusion in the econometric literature that needs to be cleared up.
For instance, Basawa et al. (1991a) prove that sampling scheme Si is not
appropriate in the unit root case. Basawa et al. (1991b) use test statistic
n(p* — 1) with sampling scheme S3. Ferretti and Romo (1994) establish
the result that test statistic n(p* - 1) with sampling scheme £2 can also
be used in bootstrap tests of unit roots. Note that this is similar to the
procedure used by Rayner (1990) for the stationary AR(1) model, except
that it is not in the pivotal form. We have compared both the Basawa et
al. (1991b) and Ferretti and Romo schemes of getting bootstrap samples
and did not notice any difference, although more detailed investigation
of this is under way.
Consider an AR(1) unit root model
yt = pyt-i+eu 
p = l, * = 1,2,...,T 
(10.3)
where yo = 0, €t ~ iid(0,o~2). In this case, the OLS estimator p of
p is a function of the Wiener processes (for readers' convenience, the

326 
Small sample inference: bootstrap methods
followings are duplicated from section 3.3.2 of chapter 3)
The Dickey-Fuller coefficient test (or if-test) and £-test statistics have
the following limiting distributions
and
tp 
=> 
77^ 
(1U.0)
(J(W(r))2dr)1/2
To construct the bootstrap test corresponding to (10.5), we start with
the OLS estimation of (10.3), compute it and get the bootstrap sam-
ple €%. We now generate y^ using e^ and p by a recursive procedure.
What Basawa et al. (1991a) show is that the limiting distribution
of (Er=i2/t*-i)1/2(P* ~ P) is n o t (10.4). The limiting distribution of
T(p* — p) turns out to be random and does not coincide with (10.5)
even if the error distribution is assumed to be normal. However, for
the sampling scheme 53, which is resampling the restricted residuals
(yt — Vt-i) after centering, Basawa et al. (1991b) show that the lim-
iting distribution of (ElLi 2/t*-i)1/2(P* ~ 1) is (1°-4) and the limiting
distribution of T(p* - 1) is (10.5).
As an alternative, consider the resampling scheme 52, that is the un-
restricted OLS regression errors, are used to generate e^ and the pseudo
data y% are generated using the null H$ : p = 1. The asymptotic valid-
ity of this bootstrap method is established in Ferretti and Romo (1994).
They show that the limiting distribution of (EtLi 2/t-i)1^2(P* ~~ 1) is
(10.4) with the resampling scheme S^-
It is important to note two points. First Basawa et al. (1991a) do not
prove the invalidity of sampling scheme 52- Thus, the use of 52 is not
in conflict with their results. Second, both sampling schemes 52 and S3
are valid in the unit root case provided the test statistic is of the form
T2, that is use a suitably normalized function of (p* — po).
Turning next to the explosive case \p\ > 1, Basawa et al. (1989) also
show that if the et are tin, conditional on (yi, y2, • • •, yn)

10.4 Bootstrapping unit root tests 
327
for all sample paths, where
t=i
Note that it is an that is used here but not <7*. We have not investigated
the issue of why this is valid in the explosive case except to note that
this is in conflict with the Hall and Wilson guidelines discussed in section
10.2.
Nankervis and Savin (1996) present simulation results on bootstrap
tests for unit roots in the AR(1) model with linear trend
yt = a + 6t + pyt-i + st
under different error distributions. They use sampling scheme £3 and
test statistic T2. In general the conclusions regarding sample sizes and
empirical significance levels for the unit root case are the same as for the
trend-stationary model discussed in section 10.3. The bootstrap-^ test
had, in general, the same power as the Dickey-Fuller test, although for
some nonnormal distributions, its performance was slightly better than
that of the Dickey-Fuller test.
Earlier, we stated that we did not find much difference in the form of
the bootstrap-^ tests under sampling schemes S2 and 53. This was the
case for normally distributed errors and for the three models
(i) y t = 
p y t - i + e t , P = l
(ii) y t = 
a + p y t - i + e u 
a = 0, p = l
{in) yt 
= 
a + 6t + p y t - i + e u 6 = 0, p = l .
In all cases the powers for the Dickey-Fuller test as well as two
bootstrap-^ tests using sampling schemes £2 and S3 were about the
same and in all cases the powers deteriorated going from model (i) to
(ii) and from (ii) to (iii). The models considered here (as well as the
models considered in Nankervis and Savin, 1996) are too restrictive to
be useful in practice because they are all based on the assumption that
St are iid. Extensions of the bootstrap approach to cases where there is
serial correlation in et (the ADF test) and other unit root tests are under
investigation. Schwert (1987) analyzed many US macroeconomic time
series and found that, although they appeared to be nonstationary, they
contained a significant MA(1) coefficient in their ARIMA specification
(see chapter 4). In view of this we do not want to make any definite
recommendations on the merits of sampling schemes S2 and S3.

328 
Small sample inference: bootstrap methods
One final point concerns the use of pivotal statistics. There are two
Dickey-Fuller tests: the coefficient test (10.5) and the t-test (10.6).
When it comes to the bootstrap approach there is again the question of
whether to consider the coefficient test or the t-test. We have looked into
this issue and found the t-test to be only marginally better, although this
conclusion is highly tentative. The case for considering pivotal statis-
tics may not be as strong for the unit root model (as in the stationary
models).
The paper by Harris (1992) is the earlier one on bootstrapping unit
root tests. Since the Nankervis and Savin paper is more exhaustive, we
shall not go through the paper by Harris.
10.5 The moving block bootstrap and extensions
As we discussed in the previous section, application of the residual-based
bootstrap methods is straightforward if the error distribution is speci-
fied to be an ARMA(p, q) process with known p and q. However, if
the structure of serial correlation is not tractable or is misspecified, the
residual-based methods will give inconsistent estimates (if lagged depen-
dent variables are present in the system). Other approaches which do
not require fitting the data into a parametric form have been developed
to deal with general dependent time series data. Carlstein (1986) first
discussed the idea of bootstrapping blocks of observations rather than
the individual observations. The blocks are nonoverlapping. Kiinsch
(1989) and Liu and Singh (1992) independently introduced a more gen-
eral bootstrap procedure, the moving block bootstrap (MBB) which is
applicable to stationary time series data. In this method the blocks of
observations are overlapping.
The methods of Carlstein (nonoverlapping blocks) and Kiinsch (over-
lapping blocks) both divide the data of n observations into blocks of
length I and select b of these blocks (with repeats allowed) by resam-
pling with replacement all the possible blocks. Let us for simplicity
assume n = bl. In the Carlstein procedure, there are just b blocks.
In the Kiinsch procedure there are n — I + 1 blocks. The blocks are
Lk = {xk,xk+1,'",xk+i-i} 
for k = l,2,---,(n - Z + 1). For exam-
ple with n = 6 and I = 3 suppose the data are xt = {3,6,7,2,1,5}.
The blocks according to Carlstein are {(3,6,7), (2,1,5)}. The blocks
according to Kiinsch are {(3,6,7), (6,7,2), (7,2,1), (2,1,5)}. Now draw
a sample of two blocks with replacement in each case. Suppose, the first
draw gave (3,6,7). The probability of missing all of (2,1,5) is 1/2 in

10.5 The moving block bootstrap and extensions 
329
Carlstein's scheme and 1/4 in the moving block scheme. Thus there is a
higher probability of missing entire blocks in the Carlstein scheme. For
this reason, it is not popular, and is not often used. Our own experience
with Carlstein's nonoverlapping block method is that it gave very erratic
results as the block length was varied. The MBB did better.
The literature on blocking methods is mostly on the estimation of
the sample mean and its variance, although Liu and Singh (1992) talk
about the applicability of the results to more general statistics, and
Kunsch (1989, p. 1235) discusses the AR(1) and MA(1) model. In all
these studies bootstrapping is done by sampling blocks of data. Li and
Maddala (1997), since it was a regression model, blocks of the residuals
were used. The block sampling of the data was also tried, but its perfor-
mance was slightly worse (for the reasons given earlier in section 10.2).
Many of the rules suggested for the optimal block length can possibly
be adapted for regression models with (n — p) substituted for n, where
p is the number of regressors.
10.5.1 Problems with MBB
There are some important problems worth noting about the moving
block bootstrap procedure.
(i) The pseudo-time series generated by the moving block method
is not stationary, even if the original series {xt} is stationary.
For this reason, Politis and Romano (1994) suggest the station-
ary bootstrap method. This involves sampling blocks of random
length, where the length of each block has a geometric distribu-
tion. They show that the pseudo-time series generated by the
stationary bootstrap method is indeed stationary.
The application of moving block method to 1(1) variables has
more problems. Suppose that {xt} is 1(1). Then it is not neces-
sarily true that the pseudo-data {#£} generated by the moving
block bootstrap is also 1(1).
(ii) The mean x^ of the moving block bootstrap is biased in the
sense that E(x^\xi, #2, * * * > xn) ^ xn. See the result (iii) of the-
orem 6 in Liu and Singh (1992, p. 241). Politis and Romano
(1994) show that, in contrast, for stationary bootstrap proce-
dures, E(Xn\xi,X2,-'->xn) 
=xn.
(iii) The moving block bootstrap estimator of the variance of \prix~n
is also biased. Davison and Hall (1993) argue that this creates

330 
Small sample inference: bootstrap methods
problems in using the bootstrap-^ method with the moving block
bootstrap. They suggest that the usual estimator
= n x ^2(xi - xn)
be modified to
n 
i—1 n—k
<72 = n"1 ^{(xi 
- xn)2 + ^2 ^2(xi - xn)(xi+k - xn)}
i=l 
k=l 
i=l
With this modification the bootstrap-/; can improve substantially
on the normal approximation. The reason for this bias in the es-
timator of the variance is that the block bootstrap method dam-
ages the dependence structure of the data. Unfortunately this
formula is valid only for the variance of y/nxn. For more com-
plicated problems there is no such simple correction available.
Hence, in the study of Li and Maddala (1997) no such corrections
were applied. However, the Monte Carlo studies showed that the
bootstrap-^ provided considerable improvement over asymptotic
results.
(iv) In a subsequent paper, Hall and Horowitz (1993, 1996) investigate
this problem in the context of tests based on GMM estimators.
They argue that because the blocking methods do not replicate
the dependence structure of the original data, it is necessary to
develop special bootstrap versions of the test statistics and these
must have the same distribution as the sample version of the test
statistics through Op(n~1). They derive the bootstrap versions
of the test statistics with Carlstein's blocking scheme (nonover-
lapping blocks) but argue that Kiinsch's blocking scheme is more
difficult to analyze owing to its use of overlapping blocks.
In the case of cointegration tests based on the moving block scheme,
as studied in Li and Maddala (1997), the derivation of the appropri-
ate bootstrap versions of the test statistics is still more complicated.
Although the use of the bootstrap version of the usual test statistics
cannot be theoretically justified, the Monte Carlo results unequivocally
indicate considerable improvement over the asymptotic results.

10.5 The moving block bootstrap and extensions 
331
10.5.2 Optimal length of blocks
There is some discussion on the optimal length of the blocks and the
several rules that have been suggested are based on different criteria.
However, the rules are useful as rough guides to selecting the optimal
sized blocks.
First, the number of blocks should be lower under the nonoverlapping
blocks rule of Carlstein than in the moving blocks rule of Kunsch. In
the stationary bootstrap approach, where blocks of random length are
sampled, the average length of a block is 1/p, where p is the parameter
of the geometric distribution. Thus, 1/p should play the same role as the
parameter / in the moving block bootstrap. Politis and Romano (1994)
argue that the application of a stationary bootstrap is less sensitive to
the choice of p than the application of a moving block bootstrap is to
the choice of I.
There is some discussion of optimal block lengths in the papers by
Carlstein (1986), Kunsch (1989), and the more detailed discussion in
Hall and Horowitz (1993). The rules are suggestive but putting some
numbers in them we get a rough idea of the block sizes to consider.
Carlstein is interested in minimizing the MSE of the block bootstrap
estimate of the variance of a general statistic t(xi,X2, • • • ,xn) (e.g., a
trimmed mean or a robust estimate of scale). He argues that as the
block size increases the bias goes down but the variance goes up. Also,
as the dependency among the Xi gets stronger, we need a longer block
size. Based on these considerations, he derives the optimal block size I*
for the AR(1) model yt = pyt-i + £t- His rule is
Note that as p increases I* increases. For T = 200 and p = 0.5,0.8,
and 0.9 we get I* = 7.08,15.81, and 26.18 respectively. In practice these
numbers are rounded, e.g., for p = 0.9 we might consider eight blocks of
length 25.
As for Kunsch, it is commonly believed that he suggested the optimal
number of blocks to be proportional to T1/3, i.e., I* oc T2/3. This implies
much longer blocks. But he also suggested to use subjective judgement
based on sample correlations. This is perhaps a better rule than the
other widely quoted one.
Hall and Horowitz (1993) derive rules taking into account the MSE in
the estimation of variance as in Carlstein. They argue that the rules are
similar both for the moving block scheme (they call this Kiinsch's rule)

332 
Small sample inference: bootstrap methods
and the nonoverlapping block scheme (they call this Carlstein's rule).
Further more they say the rule is the same for MSE of the estimate of
bias. Their rule is
I = (3/2)1/3r1/3^-2/3 
u n d e r Kiinsch's rule
I = Ti/3£-2/3 
u n d e r Carlstein's rule
where
3=1 
3=1
and 7(j) equals the covariance of yt at lag j . For the AR(1) process
yt = pyt-i + eu 
C = (1 - P2)/p
For the MA(1) process
yt = £t + ^ t - i , 
C = (l + 0)7#
We have computed I* for the two processes. They are for T = 200.
0
0
0
.5
.8
.9
5
11
18
AR(1)
K
.11
.40
.88
C
4.46
9.96
16.49
MA(1)
e
0.5
0.8
0.9
K
2.46
2.63
2.65
C
2.15
2.30
2.32
Note: K = Kiinsch's rule
C = Carlstein's rule.
10.6 Issues in bootstrapping cointegrating regressions
To focus on the issues concerning the bootstrapping of cointegrating
regressions consider the simple system consisting of two variables y\ and
2/2 which are both 1(1) and the cointegrating regression
where u is 1(0). As is well known the least squares estimator (3 of (3 is
superconsistent, and the asymptotic distribution of $ involves nuisance
parameters arising from endogeneity and serial correlation in u. Sev-
eral corrections for the problems of endogeneity and serial correlation
have been proposed in the literature (see chapter 5). However, all these

10.6 Issues in bootstrapping cointegrating regressions 
333
methods involve estimation of equations different from equation (10.7)
and enable the derivation of asymptotically pivotal statistics that are
needed for the proper application of bootstrap methods. This was what
was done in Li and Maddala (1997). Note that estimation of (10.7) does
not lead to asymptotically pivotal statistics, except in the special case
where the problem of endogeneity and serial correlation are absent.
10.6.1 Bootstrap data for cointegrated regressions
Suppose that there is no endogeneity problem nor the serial correlation
problem. Then the OLS estimator (3 of j3 in (10.7) has an asymptotic
distribution that is nuisance parameter free, and one can apply the boot-
strap procedure. Even then it is better to bootstrap the pivotal trather
than (3 itself to get confidence intervals for /?. The bootstrap-t con-
fidence intervals are more accurate, as discussed in section 10.2, than
those based on the bootstrap distribution of J3.
The bootstrap methods applicable to regression models as in Vinod
and McCullouch (1995) cannot be used for cointegrated systems. Nor
is bootstrapping (2/1,2/2) directly, suggested by Freedman (1981) and
Efron and Gong (1983), since this method does not use the information
that 2/2 is 7(1) and (10.7) is a cointegration relationship. Note that this
problem does not arise in the censored regression model considered by
Efron (1981b) or the examples considered in Efron and Gong (1983). A
valid procedure is the following:
(i) get ut by estimating (10.7) by OLS and get the set of residuals
vt = Ay2t,
(ii) after centering these residuals, bootstrap the pairs (ut,vt),
(iii) construct vfct using the recursive method and y*t using /3, uj, and
V2t in (10.7).
This method exploits the information that 2/2 is ^(1) and that (10.7) is
a cointegrating relationship.
What if there is no endogeneity but there is serial correlation in the
errors u in equation (10.7)? In this case as Phillips and Park (1988)
showed, the OLS and GLS estimators are asymptotically equivalent.
However, to obtain the valid t-statistic, which we need for the purpose
of bootstrap-^ confidence intervals, we need to calculate the asymptotic
variance of J3. This is described in Phillips and Park (1988) and we
need not repeat it here. As for bootstrap data generation when the
structure of autocorrelation is not known, we bootstrap blocks of (ut, Vt)

334 
Small sample inference: bootstrap methods
as discussed in Li and Maddala (1997). If the errors are assumed to be
an AR(1) process, we use recursive methods as in Li (1994). All this is
not valid in the presence of endogeneity.
It is useful to discuss the bootstrap data generation for cointegrated
systems under two headings:
(i) single equation methods and
(ii) system methods.
In both the cases an estimation method that corrects for endogeneity
and serial correlation is used, e.g., FM-OLS (or any other dynamic least
squares regression) in the single equation context, and the Johansen (or
Box-Tiao) procedure in the case of system estimation. We can then use
the estimated coefficients and residuals to generate bootstrap samples.
In the case of system methods we use the residuals from the VAR we
started out with. In the case of single equation methods, care should be
taken to bootstrap pairs of residuals as discussed earlier.
However, if the correct lag length is not used in the VAR in the case of
system methods, or the dynamic regression model in the case of single
equation methods, then the residuals will be autocorrelated and it is
better to use the moving block bootstrap (MBB) as done in Li and
Maddala (1997). These authors illustrate how the MBB and stationary
bootstrap improve asymptotic inference. The bootstrap data generation
is described in their paper. These methods are not called for in their
example where the structure of the model is known. However, they can
be used in all circumstances. Note that, as the study of Boswijk and
Pranses, referred to earlier in chapter 6, pointed out, the specification
of too few lags in the Johansen procedure results in substantial size
distortions in the tests, and overspecification of the lag length results in
a loss of power. In such circumstances it is better to specify a smaller
lag and use the MBB procedure. Van Giersbergen (1996) also used the
stationary bootstrap in bootstrapping the trace statistic in VAR models.
He says this will take care of misspecifications.
Note that Fachin (1996) does not use the MBB. He uses the estimated
coefficients from the Johansen procedure and the residuals from the VAR
(filtered out for the estimated coefficients) in resampling. He finds that
using the asymptotic distribution results in a size always higher than the
nominal and that the bootstrap-based tests correct the size distortion.
He, however, finds that the bootstrap-based tests may lack power. Li
(1997), on the other hand, does find that the bootstrap-based tests have
higher power. His investigation is not for the Johansen procedure. It

10.7 Miscellaneous other applications 
335
is for OLS and GLS testing procedures. More detailed investigation of
the power of bootstrap-based tests and the role of MBB procedures is
needed.
10.7 Miscellaneous other applications
Bootstrap methods have been widely used in the financial literature.
Maddala and Li (1996) review the applications of bootstrap methods in
finance. They classify the applications into the following categories:
(i) to obtain small sample standard errors,
(ii) to get significance levels of tests,
(iii) to get significance levels for trading rule profits,
(iv) to develop empirical approximations to population distributions,
(v) to use trading rules on bootstrapped data as a test for model
specification,
(vi) to check the validity of long-horizon predictability,
(vii) for impulse response analysis in nonlinear models.
They review several papers and point out the limitations of the way the
bootstrap procedures were implemented.
One important application listed there (pp. 476-478) is the use of
bootstrap methods for model selection using trading rules. The pro-
cedure is as follows: first, get a measure of the profits generated by a
trading rule using the actual data. Next, estimate the postulated model
and bootstrap the residuals to generate bootstrap samples. Generate
trading rule profits for each of the bootstrap samples. Compare the
bootstrap distribution of the trading rule profits with the profits from
the actual data. The basic idea is to compare the time series proper-
ties of the generated series from the given model, with the actual data.
Measures like R2 and other goodness of fit measures do not capture the
time series properties of the data, as trading rule profits do.
One other application worth mentioning is that by Oke and Lyhagen
(1996). These authors use the bootstrap methods to study the impli-
cations of pre-testing that is so common in the literature on unit roots
and cointegration. For instance, unit root tests are used as pre-tests - a
prelude to cointegration analysis. Tests for cointegration rank are used
as a prelude to tests on the coefficients of the cointegrating vectors. An
earlier discussion of the use of bootstrap in data mining (which involves
pre-testing) is in the papers by Freedman, Navidi, and Peters, and Dijk-
stra and Veldkamp in T.K. Dijkstra, ed. (1988), On Model Uncertainty

336 
Small sample inference: bootstrap methods
and Its Statistical Implications (Springer, Berlin). The use of bootstrap
methods for the pre-testing problems in unit roots and cointegration is
an important area to pursue, where the role of bootstrap methods needs
to be thoroughly investigated.
10.8 Conclusions
This chapter points out how bootstrap methods can be used in making
small sample inference on unit roots and cointegration. It has been found
that in small samples the use of asymptotic theory results in substantial
biases in the estimated coefficients and size distortions in the tests used.
Bootstrap methods help solve these problems with no loss in power.
We have discussed the appropriate procedures for bootstrap data gen-
erations, methods of constructing the test statistics, and the importance
of the use of pivotal methods in connection with the use of bootstrap. We
have also discussed the use of moving block and stationary bootstraps.
Of some concern is the applicability of bootstrap methods to the Jo-
hansen estimator - where the bootstrap does not appear to be giving
much of an improvement over asymptotic inference. More detailed anal-
ysis of this and the Box-Tiao model is needed. Another issue that needs
further investigation is the pre-testing problem.
There are many instances where defective bootstrap methods have
been used (see Maddala and Li, 1996). This raises the question: Is a de-
fective bootstrap method still better than asymptotic inference? There
are several examples in the literature where this is not so. One case
that is relevant to our discussion is the case of bootstrapping unit root
models, as shown in Basawa et al. (1991a). However, when no asymp-
totic inference is available, it is better to use a bootstrap method. Also
when the correct bootstrap method is complicated and not feasible, a
theoretically imperfect bootstrap method might still improve asymptotic
inference, as discussed in Li and Maddala (1997). Thus, unless proven
otherwise, some bootstrap may be better than no bootstrap.
References
Abramovich, L. and K. Singh (1985), "Edgeworth Corrected Pivotal
Statistics and the Bootstrap," Annals of Statistics, 13, 116-132.
Andrews, D.W.K. (1993), "Exactly Median-Unbiased Estimation of
First-Order Autoregressive/Unit-Root Models," Econometrica,
61, 139-165.

References 
337
Barnard, G.A. (1963), "Contribution to Discussion," Journal of the
Royal Statistical Society, B, 25, 294.
Basawa, I.V., A.K. Mallik, W.P. McCormick, and R.L. Taylor (1989),
"Bootstrapping Explosive Autoregressive Processes," Annals of
Statistics, 17, 1479-1486.
Basawa, I.V. and R.L. Taylor (1991a), "Bootstrapping Unstable First
Order Autoregressive Processes," Annals of Statistics, 19, 1098-
1101.
(1991b), "Bootstrapping Test of Significance and Sequential Boot-
strap Estimation for Unstable First Order Autoregressive Pro-
cesses," Communications in Statistics: Theory and Methods, 20,
1015-1026.
Beran, R. (1987), "Prepivoting to Reduce Level Error of Confidence
Sets," Biometrika, 74, 457-468.
(1988), "Prepivoting Test Statistics: A Bootstrap View of Asymp-
totic Refinements," Jouranl of American Statistical Association,
83, 687-697.
(1990), "Refining Bootstrap Simultaneous Confidence Sets," Journal
of Amererican Statistical Association, 85, 417-426.
Bhargava, A. (1986), "On the Theory of Testing for Unit Roots in
Observed Time Series," Review of Economic Studies, 53, 369-
384.
Bickel, P.J. and D.A. Freedman (1980), "On Edgeworth Expan-
sions and the Bootstrap," Manuscript, University of California,
Berkeley.
Carlstein, E. (1986), "The Use of Subseries Values for Estimating the
Variance of a General Statistic from a Stationary Sequence,"
Annals of Statistics, 14, 1171-1179.
Davison, A.C. and P. Hall (1993), "On Studentizing and Block-
ing Methods for Implementing the Bootstrap with Dependent
Data," Australian Journal of Statistics, 35, 212-224.
DiCiccio, T. and J. Romano (1988), "A Review of Bootstrap Confidence
Intervals (with discussion)," Journal of the Royal Statistical So-
ciety, B, 50, 338-354.
(1990), "nonparametric Confidence Limits by Resampling Methods
and Least Favorable Families," International Statistical Review,
58, 59-76.
Dufour, J.M. (1990), "Exact Tests and Confidence Sets in Linear Re-
gressions with Autocorrelated Errors," Econometrica, 58, 475-
494.

338 
Small sample inference: bootstrap methods
Efron, B., (1979), "Bootstrap Methods: Another Look at the Jack-
knife," The Annals of Statistics, 7, 1-26.
(1981a), "nonparametric Standard Errors and Confidence Intervals,"
Canadian Journal of Statistics, 9, 139-172.
(1981b), "Censored Data and the Bootstrap," Journal of the Amer-
ican Statistical Association, 76, 312-319.
(1987), "Better Bootstrap Confidence Intervals," Journal of the
American Statistical Association, 82, 171-200.
Efron, B. and G. Gong (1983), "A Leisurely Look at the Bootstrap,
the Jackknife, and Cross Validation," American Statistican, 37,
36-48.
Efron, B. and R. Tibshirani (1986), "Bootstrap Methods for Standard
Errors, Confidence Intervals, and Other Measures of Statistical
Accuracy," Statistical Science, 1, 54-77.
(1993), An Introduction to the Bootstrap, Chapman and Hall, New
York.
Fachin, S. (1996), "Bootstrap Tests on Cointegrating Coefficients,"
Manuscript, University of Rome, La Sapienza.
Ferretti, N. and J. Romo (1994), "Unit Root Bootstrap Tests for AR(1)
Models," Working paper, Division of Economics, Universidad
Carlos III de Madrid.
Freedman, D.A. (1981), "Bootstrapping Regression Models," The An-
nals of Statistics, 9, 1218-1228.
Freedman, D.A. and S.C. Peters (1984), "Bootstrapping a Regression
Equation: Some Empirical Results," Journal of the American
Statistical Association, 79, 97-106.
van Giersbergen, N.P.A. (1996), "Bootstrapping the Trace Statistic in
VAR Models: Monte Carlo Results and Applications," Oxford
Bulletin of Economics and Statistics, 58, 391-408.
van Giersbergen, N.P.A. and J.F. Kiviet (1993), "A Monte Carlo Com-
parison of Asymptotic and Various Bootstrap Inference Pro-
cedures in First-order Dynamic Models," Discussion paper TI
93-187, Tinbergen Institute, University of Amsterdam.
(1994), "How to Implement Bootstrap Hypothesis Testing in Regres-
sion Models," Discussion paper TI 94-130, Tinbergen Institute,
University of Amsterdam.
(1996), "Bootstrapping a Stable AD Model: Weak versus Strong
Exogeneity," Oxford Bulletin of Economics and Statistics, 58,
631-656.

References 
339
Gregory, A.W. and M.R. Veall (1985), "Formulating Wald Tests of
nonlinear Restrictions," Econometrica, 53, 1465-1468.
Hall, P. (1988a), "On Symmetric Bootstrap Confidence Intervals,"
Journal of the Royal Statistical Society, B, 50, 35-45.
(1988b), "Theoretical Comparison of Bootstrap Confidence Inter-
vals," Annals of Statistics, 16, 927-953.
(1989), "Unusual Properties of Bootstrap Confidence Intervals in Re-
gression Problems," Probability Theory and Related Fields, 81,
247-273.
(1992), The Bootstrap and Edgeworth Expansion, Springer Verlag,
New York.
Hall, P and J.L. Horowitz (1993), "Corrections and Blocking Rules for
the Block Bootstrap with Dependent Data," Working paper No.
93-11, Department of Economics, University of Iowa.
(1996), "Bootstrap Critical Values for Tests Based on Generalized
Method of Moments Estimators," Econometrica, 64, 891-916.
Hall, P. and S.R. Wilson (1991), "Two Guidelines for Bootstrap Hy-
pothesis Testing," Biometrics, 47, 757-762.
Hardle, W. and E. Mammen (1990), "Bootstrap Methods in nonpara-
metric Regression," CORE Discussion paper no. 9049.
Harris, R.I.D. (1992), "Small Sample Testing for Unit Roots," Oxford
Bulletin of Economics and Statistics, 54, 615-625.
Hartigan, J.A. (1969), "Using Subsample Values as Typical Values,"
Journal of the American Statistical Association, 64, 1303-1317.
(1986), "Comment on the Paper by Efron and Tibshirani," Statistical
Science, 1, 75-76.
Hinkley, D. V. (1988), "Bootstrap Methods (With Discussion)," Jour-
nal of the Royal Statistical Society, B, 50, 321-337.
(1989), "Bootstrap Significance Tests," Proceedings of the 47th Ses-
sion of the International Statistical Institute, 3, 65-74.
Horowitz, J.L. (1994), "Bootstrap-based Critical Values for the Infor-
mation Matrix Test," Journal of Econometrics, 61, 395-411.
(1997), "Bootstrap Methods in Econometrics: Theory and Numerical
Performance," in D.M. Kreps and K.F. Wallis (eds.), Advances
in Economics and Econometrics: Theory and Applications, vol.
Ill, Cambridge University Press, Cambridge.
Horowitz, J.L. and N.E. Savin (1992), "Non-Invariance of the Wald
Test: The Bootstrap to the Rescue," Manuscript, University of
Iowa.

340 
Small sample inference: bootstrap methods
Jeong, J. and G.S. Maddala (1993), "A Perspective on Application of
Bootstrap Methods in Econometrics," in G.S. Maddala, C.R.
Rao, and H.D. Vinod (eds.), Handbook of Statistics, vol. 11,
North-Holland, Amsterdam, 573-610.
(1996), "Testing the Rationality of Survey Data Using the Weighted
Double-Bootstrapped Method of Moments," Review of Eco-
nomics and Statistics, 78, 296-302.
Kilian, L. (1996), "Small Sample Confidence Intervals for Impulse Re-
sponse Functions," mimeo, University of Michigan.
Kiviet, J.F., (1984), "Bootstrap Inference in Lagged-Dependent Vari-
able Models," Working paper, University of Amsterdam.
Kiinsch, H.R. (1989), "The Jackknife and the Bootstrap for General
Stationary Observations," The Annals of Statistics, 17, 1217-
1241.
Li, H. (1994), "Bootstrapping Cointegrating Regressions," Economics
Letters, 32, 229-233.
(1997), "The Power of Bootstrap Based Tests for Parameters in Coin-
tegrating Regressions," Manuscript, Chinese University of Hong
Kong.
Li, H. and G.S. Maddala (1996), "Bootstrapping Time Series Models,"
with Discussion, Econometric Reviews, 15, 115-195.
(1997), "Bootstrapping Cointegrating Regressions," Journal of
Econometrics, 80, 297-318.
Liu, R.Y. and K. Singh (1992), "Moving Blocks Jackknife and Boot-
strap Capture Weak Dependence," in R. LePage and L. Billard
(eds.), Exploring the Limits of Bootstrap, Wiley, New York, 225-
248.
Maddala, G.S. and H. Li (1996), "Bootstrap Based Tests in Financial
Models," in G.S. Maddala and C.R. Rao (eds.), Handbook of
Statistics, vol. 14, Elsevier Science, Amsterdam, 463-488.
Nankervis, J.C. and N.E. Savin (1996), "The Level and Power of the
Bootstrap t-test in the AR(1) Model with Trend," Journal of
Business and Economic Statistics, 14, 161-168.
Oke, T. and J. Lyhagen (1996), "Pre-Testing for Unit Root and Boot-
strapping Results in Swedish GNP," Manuscript, Dept. of
Statistics, Uppsala University.
Phillips, P.C.B. and J.Y. Park (1988), "Asymptotic Equivalence of Or-
dinary Least Squares and Generalized Least Squares in Regres-
sions with Integrated Variables," Journal of the American Sta-
tistical Association, 83, 111-115.

References 
341
Politis, D.N. and J. Romano (1994), "The Stationary Bootstrap," Jour-
nal of American Statistical Association, 89, 1303-1313.
Rayner, R.K. (1990), "Bootstrapping p Values and Power in the First-
Order Autoregression: A Monte Carlo Investigation," Journal
of Business and Economic Statistics, 8, 251-263.
(1991), "Resampling Methods for Tests in Regression Models with
Autocorrelated Errors," Economics Letters, 36, 281-284.
Rilstone, P. (1993), "Some Improvements for Bootstrapping Regression
Estimators under First-Order Serial Correlation," Economics
Letters, 42, 335-339.
Schwert, G.W. (1987), "Effects of Model Specification on Tests for
Unit Roots in Macroeconomic Data," Journal of Monetary Eco-
nomics, 20, 73-103.
Singh, K. (1981), "On the Asymptotic Accuracy of Efron's Bootstrap,"
Annals of Statistics, 9, 1187-1195.
Tibshirani, R. (1988), "Variance Stabilization and the Bootstrap,"
Biometrika, 75, 433-444.
Veall, M.R. (1986), "Bootstrapping Regression Estimators under First-
Order Serial Correlation," Economics Letters, 21, 41-44.
(1987), "Bootstrapping the Probability Distribution of Peak Elec-
tricity Demand," International Economic Review, 28, 203-212.
(1988), "Discussion of Hall's paper," Annals of Statistics, 16, 979-
981.
Vinod, H.D. (1993), "Bootstrap Methods: Applications in Economet-
rics," in G.S. Maddala, C.R. Rao, and H.D. Vinod (eds.), Hand-
book of Statistics, vol. 11, Elsevier Science, Amsterdam, 629-
661.
Vinod, H.D. and B.D. McCullough (1995), "Estimating Cointegration
Parameters: An Application of the Double Bootstrap," Journal
of Statistical Planning and Inference, 43, 147-156.
Zivot, E. (1994), "A Bayesian Analysis of the Unit Root Hypothesis
within an Unobserved Components Model," Econometric The-
ory, 10, 552-578.

11
Cointegrated systems with 1(2) variables
In the Box-Jenkins methods one continues differencing a series until the
correlogram of the differenced series damps out. If the series has to be
differenced d times to achieve stationarity, we say the series is integrated
of order <i, or l(d). In the previous chapters we considered the case d = 1.
We shall now discuss models where d = 2. These models are referred to
as models with double unit roots. The analog of the unit root test with
unit root as the null and stationarity as the alternative is a test for 1(2)
versus 1(1). Just as we discussed (in chapter 4) tests using stationarity
as null, that is tests for 1(0) versus 1(1), we also have to discuss tests for
1(1) versus 1(2). In the following sections, we shall discuss the different
problems of testing for double unit roots and problems of estimation of
1(2) systems.
11.1 Determination of the order of differencing
Hasza and Fuller (1979) were the first to discuss the problem of double
unit roots and proposed the joint test for double unit roots. Consider
the finite-order AR model
A(L)yt = et 
(11.1)
where A(L) is a lag polynomial of order p and et ~ iid(0, a2). By defining
a new polynomial A*(L) of order p — 1, A(L) can be rearranged such
that A(L) = A(1)L + A*(L)(1 — L), and by doing the same trick on
A* (L) we get
A(L) = A(1)L + A*(l)(l - L)L + A**(L)(1 - L)2 
(11.2)
If yt has at least one unit root, then -A(l) = 0, and if two unit roots exist,
A*(l) = 0 as well. Combining (11.1) and (11.2) provides a regression of
342

11.1 Determination of the order of differencing
the augmented Dickey-Fuller form
V-2
343
(n.3)
Hasza and Fuller proposed an F-test for double unit roots testing the
hypothesis Ho : TTI = TT2 = 0 in the regression (11.3). They derived the
asymptotic distribution of the usual F-test under the maintained null
of double unit roots. The F-statistic for testing the joint hypothesis
HQ : 7Ti = 7T2 = 1 satisfies
where
and
- 
2( / WdW + \
Jo
+
D = / W / W2 - 
/ WW
" " 
2a2
Hasza and Fuller provide critical values, including situations in which
an intercept and possibly an intercept plus a trend are included in the
regression.
Dickey and Pantula (1991) investigated the effect of the standard
Dickey-Fuller test and the double unit roots test suggested by Hasza
and Fuller in the presence of additional unit roots. Their simulation
study shows that when the process had three unit roots:
(i) the 5 percent level Dickey-Fuller test rejected the null of one unit
root in favor of stationarity 9 percent of the time,
(ii) the 5 percent level double unit roots Hasza-Fuller test rejected
the null of double unit roots in favor of one unit root and two
stationary roots 9.2 percent of the time.
Intuitively, if there are more unit roots, the test for less unit roots will
strongly indicate that the process needs to be differenced, and hence the

344 
Cointegrated systems with 1(2) variables
null hypothesis will be rejected less than 5 percent of the time. However,
their simulation study does not support this intuition.
With this motivating study, Dickey and Pantula (1991) formalize the
Box-Jenkins sequential approach and investigate its validity. Suppose
one uses a sequence of unit root tests in the same order as with visual
inspection of the autocorrelation function (the Box-Jenkins sequential
approach). If the hypothesis of the presence of a unit root for the se-
ries levels were not rejected, one would then test the differences for the
presence of a second unit root, and so on. Dickey and Pantula refer to
this as the standard testing sequence. They pointed out that since the
standard DF unit root test is based on the assumption of at most one
unit root, at least the first few tests in this sequence would not be theo-
retically justified, if the series had more than one unit root. Sen (1995)
also observed empirically that, under the hypothesis of two unit roots,
the DF test rejects the null hypothesis of one unit root with probability
slightly greater than a. That is, we are more likely to conclude that the
process is stationary when there are really two unit roots present than
when there is exactly one unit root. Thus, one should avoid testing for
one unit root before testing for a higher number of unit roots. Another
problem is that the single t-statistics are not invariant tests because their
distributions under the null will depend on the actual number of unit
roots present. For instance, the ^-statistics of TTI in (11.3) will follow
the Dickey-Fuller ^-distribution if TT2 < 1, but the distribution changes
if TT2 = 1.
Dickey and Pantula (1991) show that an appropriate way to proceed
is to reverse the order of testing by starting with the highest possible
order of integration and then testing down the model. They proposed
t*- and F-tests that compare a null hypothesis of k unit roots with an
alternative of k — 1 unit roots. Consider
Vt ^
where {et} is a sequence of iid random variables with mean 0 and vari-
ance <J2 = 1 and 2/-p+i = • • • = 2/o = 0- They considered the AR model
with p = 3 and three possible unit roots. Let rai,ra2, and 777,3 denote
the roots of the characteristic equation
m3 - ftm2 - /32m - /?3 = 0

11.1 Determination of the order of differencing 
345
The AR(3) model can be written
xt = OiVt-i + 02*t-i + Oswt-i + et 
(11.5)
where zt = yt — yt-i,wt 
= %t — Zt-i, and xt = wt — wt-i. Note that
zt,wt, and xt are the first, second, and third differences of the process
yt, respectively. Then the hypotheses about the different number of unit
roots are:
(i) No unit root, Ho : |rai| < 1,
or
iiTo • 0i < 0 and some restrictions on 02 and #3.
For example, the restrictions on 02 and 03 are
-12 < 02 + 20i < 0, - 2 < 03 < 0
(ii) One unit root, Hi : mi = 1, |ra2| < 1,
or
jjj : 61 = 0,02 < 0 and some restrictions on 62 and 03.
The restrictions on 02 and 03 are 0 < 4 + 02 + 203, - 2 < 03 < 0.
(iii) Two unit roots, H2 : mi = 7712 = 1, |ra3| < 1,
or
H2: 61=62 = 0,03 < 0.
(iv) Three unit roots, H3 : mi = ra2 = m3 = 1,
or
H3 : 0i = 02 = 83 = 0.
Note that the subscript i in Hi denotes the number of unit roots of the
corresponding null hypotheses. Dickey and Pantula showed that apply-
ing regression F-statistics for testing from the higher number of unit
roots to the lower number of unit roots is valid, while the reverse order
of applying F-statistics is not valid. Based on the asymptotic distribu-
tions of F-statistics derived by Pantula (1986), they suggest testing the
hypotheses sequentially in the order H3, if2, and H1 :
(i) If H3 is rejected by F-test, then go to the next step; otherwise
conclude that H3 is true,
(ii) If H2 is rejected by F-test, then go to the next step; otherwise
conclude that H2 is true.

346 
Cointegrated systems with 1(2) variables
(iii) If Hi is rejected, then conclude that Ho is true; otherwise con-
clude that Hi is true.
The empirical percentiles of the asymptotic distributions of the F-statistics
can be found in Pantula (1986).
Dickey and Pantula also proposed an alternative testing procedure
based on t-test statistics. However, the t-statistics from the regression
(11.5) has asymptotic distributions depending on the number of unit
roots present. And thus they argue that a sequential procedure based
on these statistics is not consistent. They suggest using alternative t*-
statistics which follows the standard Dickey-Fuller ^-distribution:
(i) for iJ3 against H2, obtain the t*-statistic from the regression xt
on wt-i for testing the coefficient of wt-i is 0,
(ii) for H2 against iJi, use the t*-statistic in the regression Xt on Zt-i
and Wt-i for testing the coefficient of zt-i is 0,
(iii) for Hi against Ho, use the t*-statistic in the regression Xt on
2/t-i? zt-\-> and Wt-i for testing the coefficient of yt-i is 0.
They show that the t*-test will have higher power than the joint F-test
against the 1(1) alternative, since prior imposition of a unit root in the
first step will be correct, both under the null and the alternative. They
also argued that some power is gained since the joint test is two-sided in
nature, whereas only the one-sided alternative is addressed in their single
t-test procedure. Their results of a Monte Carlo power study supports
these findings and show that t*-procedure is more powerful than F in
most cases.
Haldrup (1994a) argued that the sequential testing procedure sug-
gested by Dickey and Pantula (1991) may result in severe problems in
the presence of an explosive root, which may be a likely alternative can-
didate to the double unit root process. He argued that prior imposition
of a single unit root under the presence of an explosive root may result
in the misleading conclusion that double unit roots are present because,
in principle, an explosive process can be differenced an infinite number
of times without being stationary. Moreover, differencing of an explosive
process will produce a noninvertible error term that may cause serious
problems with respect to estimation and inference.
Haldrup (1994a) developed a semiparametric testing procedure equiv-
alent to the augmented Hasza-Fuller procedure as a straightforward gen-
eralization of the Phillips and Perron's Z-test for a unit root in the 1(1)
model (see chapter 3 for details). He showed that the nonparametric

11.1 Determination of the order of differencing 
347
correction of the F-statistic in the equation (11.3) such that
O~
where
M = T-*J2 y2t • T-2 E *v*
t=l 
t=l
and
have the same limiting distibution given in (11.4). The limiting dis-
tribution of the Zp-statistic is invariant within a wide class of weakly
dependent and potentially heterogenously distributed errors (see chap-
ter 3 for the wide class of errors and the Newey-West estimate of the
long-run variance <r2). Since the distribution is identical to the situa-
tion in which a2 = a2, the critical values tabulated by Hasza and Fuller
(1979) remain valid.
The limiting distributions are affected by the inclusion of deterministic
components. Since an intercept, a linear trend, and possibly a quadratic
trend effectively demean and detrend the series prior to testing for unit
roots, the asymptotics will carry through in a similar and straightforward
fashion with the only modification being that the original Brownian
motions are subsituted by demeaned and detrended Brownian motions
(see chapter 3).
A Monte Carlo study by Haldrup (1994a) showed that when the DGP
is an explosive process, i.e., TTI > 1 in (11.3), the Dickey and Pantula
tests appear to have almost zero power, and thus the double unit root
is accepted in most cases. On the other hand, the joint test procedure
proposed by Hasza and Puller and the semiparameteric version by Hal-
drup which does not place a priori restriction on the data, discriminate
1(2) processes from explosive processes, although this property is less
pronounced when the sample size is small. However, the Haldrup test

348 
Cointegrated systems with 1(2) variables
suffers from serious size distortions in case of negative MA roots, like
other semiparametric tests, for example Phillips and Perron's Z-test (see
chapter 4).
In practice, it is recommended to apply a collection of several tests
and hope that some common evidence is obtained.
11.2 Cointegration analysis with 1(2) and 1(1) variables
After the determination of which variables are 1(0), 1(1), and 1(2), the
next step is the determination of any cointegrating relationships among
these variables and the estimation of equations with mixtures of 1(0),
1(1), and 1(2) variables. There are two kinds of methods that have been
proposed for estimation:
(i) Single equation methods,
(ii) System methods.
In system methods there are two subcategories:
(i) the nature of cointegration and the number of cointegrating vec-
tors are known,
(ii) these factors need to be estimated.
We will discuss these in turn.
11.2.1 Single equation methods
Haldrup (1994b) discusses, in detail, single equation models with 1(0),
1(1), and 1(2) variables. He analyzes the conditions under which stan-
dard Gaussian inference can validly be conducted and how the literature
on spurious regression for 1(1) variables can be extended to the case of
1(2) variables. He extends the residual-based Dickey-Fuller class of tests
to models involving 1(1) and 1(2) variables and provides new critical val-
ues for these tests.
When dealing with 1(2) variables, several possibilities exist. First,
linear combinations of some 1(2) variables can be 1(1) or 1(0). Second,
some linear combinations of 1(1) variables can cointegrate with differ-
ences of 1(2) variables. In this situation the cointegrating vectors will be
polynomials of the lag operator L. Hence the cointegration is referred
to as polynomial cointegration (see Yoo, 1986 and Gregoir and Laroque,
1994).

11.2 Cointegration analysis with 1(2) and 1(1) variables 
349
Consider the model where the variable yt is related to the m-dimensional
variables xt
Vt = 7oc* + Vt
xt 
— (ct5 xlti 
X2t)
xlt
X2t = 72
C* + At
where ct is a deterministic component consisting of a constant, a linear
trend, and possibly a polynomial trend, x\t and x2t are mi and vfi2
dimensional 1(1) and 1(2) processes, respectively, and mi + m2 — m.
The y° is 1(2) and is linked with x\t and x2t by the relation
If Ut is 1(2), then there is no cointegration. If ut is 1(1), then y° and
#2t are cointegrated, that is, (7/(2,1), and if the error (y$ — /?2^2t) is
cointegrated with xjt, then we have a /w/l cointegrated system and ^t is
1(0). We can write the whole model compactly as
yt = (3f
oct + P[xlt + t3'2x2t + ut
where ^ ^ ( V o
We now study the properties of the parameters of this equation. De-
pending upon the integration order of ut there will be stochastic cointe-
gration at different levels (for stochastic cointegration, see section 5.5.2
and also Ogaki and Park, 1989). Also, if some elements of /?0 turn out
to be zero, there will be deterministic co-trending among some of the
variables.
The algebra for the analysis of this model proceeds along the same
lines as that discussed in the earlier chapters for 1(1) and 1(0) variables.
The details can be found in Haldrup (1994b). However, the following
important points regarding spurious regressions are worth repeating:
(i) If ut is 1(1) or 1(2), the F-test statistic of any hypothesis diverges
to oo by the order OP(T) although the hypothesis is true.
(ii) R2 —> 1, not only when Ut is 1(0), but also when Ut is 1(1).
(iii) DW statistic —> 0 at the rapid rate OP(T) regardless of whether
ut is 1(1) or 1(2).

350 
Cointegrated systems with 1(2) variables
Another important feature of the model is that the order of the dif-
ferent least squares regressions will differ. The orders of the coefficients,
J3\ and fa corresponding to the 1(1) and 1(2) variables are Op{Td~1) and
Op(Td~2), respectively, when ut is I(d). Thus, /3i will diverge at OP(T),
if Ut is 1(2) and will be nondegenerate when Ut is 1(1). As for $2 it will
be nondegenerate when Ut is 1(2), but is consistently estimated at the
superconsistent rate OP(T~1) when ut is 1(1). When there is full cointe-
gration, that is, Ut is 1(0), $2 will tend to /?2 at the rapid rate of OP(T~2)
and thus it is super-superconsistent whereas j3i is OP(T~1) and is just
superconsistent. As for the order of /?o, it depends on the deterministic
components present in the stochastic regressors x\t and X2t (for details
see Haldrup, 1994b, pp. 162-163).
An important question is when the inference can be conducted within
the framework of the standard Gaussian distribution. In the case of 1(1)
variables we saw earlier that the distribution of the cointegrating vector
is normal in the special case of no endogeneity and no serial correlation
(see chapter 5). Haldrup shows that this result generalizes to the case
of 1(2) variables if there is full cointegration, that is, Ut is 1(0).
Another case where asymptotic normality applies is when the stochas-
tic trends for all the individual time series are dominated by determin-
istic trends and there is full cointegration as well as deterministic co-
trending among the variables. For most practical situations this means
that yt and X2t both contain quadratic trends and xu contains at least a
linear trend. By deterministic co-trending we mean that when weighted
by the cointegrating vector some or all the higher-order trends vanish.
This result is a generalization of the result by West (1988) that we dis-
cussed in chapter 4. The proof also follows similar lines.
Residual-based tests for 1(2) cointegration
Haldrup (1994b) considers residual-based tests for cointegration where
the 1(2) variables cointegrate to produce an 1(1) variable. The hypo-
theses considered are:
Ho : There is no cointegration among the 1(2) and 1(1) variables.
Hi : The 1(2) variables cointegrate into an 1(1) relation, but no further
cointegration is possible.
The residuals from the cointegrating regression under Ho and Hi are
1(2) and 1(1), respectively. The null of no cointegration means that the
underlying levels regression is spurious. The test is analogous to the
case where 7(1) variables cointegrate to produce an 1(0) variable. The

11.2 Cointegration analysis with 1(2) and 1(1) variables 
351
question is: what modifications do we need in the critical values in the
residual-based tests that we use in the case of 1(1) variables?
Haldrup shows that the distribution of the ADF test statistic in this
case is similar to that in the case of cointegration of 1(1) variables
except that it depends on mi and 777,2, the number of 1(1) and 1(2)
variables, respectively. Hence Haldrup tabulates the critical values for
different values of ra\ and 777,2. The tables are for the sample sizes
n = (25,50,100,250,500), critical values (0.01, 0.025, 0.05, 0.1), mx =
(0,1,2,3,4), and m2 = (1,2). For large sample sizes (n = 250,500) the
critical values are the same for m = mi + 777,2 and they are similar
to those tabulated in Phillips and Ouliaris (1990). However, for small
sample sizes, the critical values become lower as 777,2 increases. Critical
values where trend (linear or quadratic) is included are available from
Haldrup on request. However, he says that the numerical evidence sug-
gests that they are similar to the ones in the published tables for higher
dimensions.
Haldrup illustrates the tests with a numerical example on the demand
for money in UK. This will be discussed later along with other emipirical
illustrations of models with 1(2) variables.
11.2.2 System methods
In the preceding section, we discussed single equation methods. We
now turn to system methods suggested by Stock and Watson (1993),
Johansen (1994c, 1995), and Kitamura (1995).
Stock and Watson (1993)
Stock and Watson assume that the variables yt can be split into yu, y^t,
and yst such that yu are 1(2), Ay2t, and Ay\t are cointegrated, and yst
is cointegrated with levels of yu and y2t and the first-differences At/it.
There is, thus, a lot of prior information used about the nature of the
interrelationships. The model specifically is
A V t = ult
Ay2t = 0iAyit + u2t
Vst = #22/2t + 032/it + 04 Aylt + ust
where ut = (uit,u2t,ust) is an 1(0) process. Stock and Watson concen-
trate on the estimation of the last equation. They suggest estimation of
the equation augmented by leads and lags and show that the estimator

352 
Cointegrated systems with 1(2) variables
is consistent. Furthermore, the asymptotic distribution of the estimated
coefficients is mixed Gaussian so that usual inference can be performed.
Kitamura (1995)
Kitamura's models are extensions for the case of 1(2) variables of Phillips'
ML estimation of triangular systems for 1(1) variables. (Phillips and
Chang (1994) use FM-OLS in a single equation context.) Thus, there
is strong prior information on the nature of cointegration and the num-
ber of cointegrating vectors (as in Stock and Watson). The process yt
is an n-vector nonstationary process and Ut is an n-vector of stationary
errors. It is assumed that the n-vectors yt and Ut can be partitioned into
subvectors of dimensions: na, ni, ri2, and 713, with n = na -\-n\ -\-ri2 +^3,
yit and y^t are 1(1) processes and (yat,y3t) 
a r e 1(2) processes.
Kitamura considers several models which differ in the type of cointe-
gration relationships assumed as follows:
Model 1
2/it = Bi2V2t + ult 
(11.6)
Vat = Ba3y3t + uat 
(11.7)
Ay2t = u2t 
(11.8)
A2yst = u3t 
(11.9)
Kitamura considers two estimation methods. One is estimat-
ing (11.6) and (11.8) together and (11.7) and (11.9) together
using Phillips' procedures of estimation of triangular systems
discussed in Phillips (1991). The other method is to estimate
the four equations jointly.
Model 2
In model 1 the 1(2) variables are cointegrated to produce an 1(0)
variable. In this model, they are cointegrated to produce an 1(1)
variable.
Model 3
Model 3 is an extension of model 2 to cover cases where differ-
enced 1(2) variables are cointegrated with 1(1) variables.
In all cases Kitamura derives the triangular ECM representations and
considers both subsystem MLE and full system MLE as well as the effi-
ciency of the latter relative to that of the former.

11.2 Cointegration analysis with 1(2) and 1(1) variables 
353
Johansen
Johansen's method does not require information on the nature of coin-
tegration and the number of cointegrating vectors. These are estimated
from the model. As in the case of models with 1(1) variables, Johansen
starts with a general vector autoregressive (VAR) model in p-dimensions.
His estimation method is based on a representation of 1(2) systems de-
rived in Johansen (1992a). The full maximum likelihood estimation of
the system is complicated and discussed in Johansen (1994c). How-
ever, he describes a computationally feasible two-step ML method in
Johansen (1994a, 1995) and this method is applied in Johansen (1992b)
and Juselius (1994a, b). This is the method we will discuss here.
Consider the general VAR model with Gaussian errors in p-dimensions
It is convenient to write this in the form
A2Xt = TAXt-i + ILYt_2 + J2 Fi&2Xt-i + et 
(11-10)
The relation between the parameters (F, n, Fi,..., Tk-2) and (A±, A2...,
is obtained by identifying the coefficients of the lagged values of Xt in
the two different expressions.
Define the matrices a and /? of dimension p x r(r < p) and ax and
/3± of dimension p x (p — r) so that aa± = 0 and (3/3± = 0. Thus, a±
and /?_L are orthogonal complements of a and /?, respectively. In the
following, orthogonal complements of other matrices are denoted by a
similar notation. Similarly, define matrices <j> and 77 of full ranks and
orders (p — r) x si with s\ < (p — r). Let a\ = a±4> and /?i = /3±rj.
Supplement these by fc = (/3,/?i)± and a2 = (a,ai)j_. Thus (a, 0:1,0:2)
are mutually orthogonal and span HP. Similar is the case with (/3, /5i, /32)-
The necessary conditions for {Xt} to be 1(2) are
II = a(3' is of reduced rank r 
(11.11)
and
ctf±T/3_\_ = <j)T)' is of reduced rank s\ < (p — r) 
(11.12)
Johansen (1995) shows that the space spanned by the vector Xt can
be decomposed into r stationary direction f3 and (jp — r) nonstationary

354 
Cointegrated systems with 1(2) variables
direction /3±. The latter can be decomposed into directions
/?i = /3±rj is of dimension p x si
and
j32 = (/?, /?i)j_ is of dimension p x s2
where s\ + s2 = p — r. In the direction /?, the process /?'Xt can be made
stationary with suitable linear combinations of AXt. In the direction (3\,
the process can be made stationary by first-order differencing and in the
direction /?2, the process can only be made stationary by second-order
differencing. These properties can be described as follows
p'Xt ~ 1(1) 
ftAXt 
~ 1(0)
ftXt ~ 1(1) 
f3'2A2Xt ~ 1(0)
P'2Xt ~ 1(2) 
(3'Xt + wp2AXt - 1(0)
where w = (a/a)~1a/Tp2(f32P2)~1 and w is a p x s2 matrix of weights
designed to pick out the 1(2) components of Xt.
Johansen's two-step ML procedure is as follows: the first step is the
analysis of the 1(1) model, that is without the retrictions on T as given
in (11.12). This determines r, a, and /?. The second step is an analysis
of the 1(2) model for fixed values of r, a, and (3. The procedure, thus, is:
(i) First determine r and the parameters a and j3 by a reduced-rank
regression of A2Xt on Xt-2 corrected for lagged differences and a
constant. That is, analyze the model (11.10) with V unrestricted
(using the Johansen procedure described in chapter 5).
(ii) Note that the levels enter only through 11X^-2 = cx(5'Xt-2. Hence,
if we pre-multiply equation (11.10) by a/j_, the levels then vanish
and we are left with
p-2
a'±A2Xt = a^TAXt-i + UXt-2 + Yl Oif
ATiA2Xt^i + a'±et
(11.13)
To get to the second step in the Johansen procedure we need
some more notation. Define A = A(A/A)~1 for any matrix A
of full rank. Thus A'A = I and define the projection operation
PA = AA = A(A/A)~1Af. Using this notation we note that
Now write
t^ 
= a'±T(0

11.3 Empirical applications 
355
We have used the condition af
±Tl3± = (/V in (11.12). This
suggests the reduced-rank regression to determine s,</>, and rj
for given r, a, and /? such that the reduced-rank regression of
a'±A2Xt on /3±AXt-i 
corrected for lagged second differences
(iii) It then follows that the asymptotic inference about a, /?, 0, and rj
can be conducted by means of the y2 distribution.
This two-step estimator is not the maximum likelihood estimator and
is not in general as efficient as the latter which estimates the parameters
a, /?, (/>, and rj jointly. This is so if the information matrix for the parame-
ter sets (a, (5) and ((/>, rj) is not diagonal. Hence, the proposed estimators
of (3 and 77 are not expected to be efficient. The asymptotic properties
of the two-step estimators are derived in Johansen (1994b) where it is
shown that the two-step estimation of (a,/?) is indeed efficient. Paruolo
(1994) has shown the surprising result that the estimator of (3± = /3±r] is
also efficient. However, if the estimators of a and (3 are efficient, then it
is evident that the second step estimators are also efficient. The second
step estimators are not efficient only if any consistent estimator is used
in the first step. So this whole issue of two-step estimators needs further
investigation.
11.3 Empirical applications
There are two main areas of empirical investigation where 1(2) models
have been applied:
(i) the long-run demand for money,
(ii) the purchasing power parity (PPP).
11.3.1 Studies on the long-run demand for money
Stock and Watson (1993) apply the methods described earlier to the es-
timation of the long-run demand for money in the US based on annual
data for the period 1900-1989. As mentioned earlier, their procedure
requires at least partial knowledge of the orders of integration of the
individual series and also which variables cointegrate. Defining the vari-
ables m, y, and p, respectively as logarithms of money balances, output,
and price level, and r as interest rate (detailed definitions and data

356 
Cointegrated systems with 1(2) variables
sources are given in their paper), they conclude after some preliminary
tests on the individual series that (m—p) is 1(1) with drift, r is 1(1) with
no drift, y is 1(1) with drift, and (m — p), y, and r are cointegrated. The
tests also suggest that (r — Ap) is 1(0). Whether m and p are individu-
ally 1(1) or 1(2) is unclear, the inference depends on the subsample and
tests considered. Their overall conclusion is that the demand for money
is stable over the entire period although estimates based on the post-
war data alone are unstable with variances indicating substantial sample
variability. This conclusion of stability over the entire period is similar
to what one observes in the case of the usual regression models. If the
estimates for one of the subperiods have high sampling variances, then
one would not reject the hypothesis of stability over the entire period.
The paper by Stock and Watson also reports Monte Carlo studies on
several estimation methods for cointegrated relationship. These results
have been reviewed in chapter 5 (section 5.7).
Haldrup (1994b) illustrates his test for 1(2) cointegration using a quar-
terly data set for the UK covering the period 1963:1-1989:11 (106 obser-
vations), which is originally studied by Hendry and Ericsson (1991) and
subsequently by Johansen (1992d). Four variables are considered in the
analysis:
mt : the log of nominal Ml,
yt : the log of real income measured as total final expenditures (TFE)
in 1985 prices,
pt : the log of implicit price deflator for TFE,
Rt : an interest rate measure used by Hendry and Ericsson (1991).
By performing tests for unit roots it was found that mt and pt are
perhaps 1(2) and real money (jnt — Pt) and velocity (m* — pt — yt) are
1(1). Applying his residual-based test for 1(2) cointegration, Haldrup
found no evidence of cointegration. This is in contrast to the conclusion
of Johansen (1992d) who found one cointegrating vector by using the
same data set. The question is not one of efficiency of single equation
versus system methods. As argued by Johansen (1992c) there will be no
gain in efficiency from analyzing the full and more complicated systems
if the conditioning variables in a partial model are weakly exogenous,
and the results in Johansen (1992d) indicate that this is indeed the case
with these data. The possible explanation in this case is the absence of
short-run dynamics in the single equation analysis which is taken into
account in the Johansen VAR procedure. Haldrup concludes that this

11.3 Empirical applications 
357
example suggests that the absence of short-run dynamics in the esti-
mation of long-run cointegrating parameters could lead to the rejection
of cointegration more often compared to the evidence following from a
dynamic approach.
Juselius (1994a) investigates long-run properties of aggregate money
holdings of the private sector in Denmark. The data are quarterly cov-
ering the period 1974:11-1987:111. Five variables are considered: money
(m*), income (yt), price (pt), bond rate (i£), and deposit rate (if). Al-
lowing for two lags in the VAR model, the effective sample size is 52
which means (as Juselius admits) we have to be careful when making
inference based on asymptotic distributions for the rank test statistics.
The data are almost the same as those analyzed by Johansen and
Juselius (1990) earlier using the Johansen procedure for 1(1) systems.
The number of variables was four in Johansen and Juselius (1990) whereas
it is five in Juselius (1994a). In Johansen and Juselius (1990) the dif-
ference (if — if) was used as the cost of holding money. Johansen and
Juselius (1990) found that there was just one cointegrating vector. If
there is only one cointegrating vector it is easy to interpret it as a long-
run relationship. If there is more than one cointegrating vector, then
all linear combinations are cointegrating vectors and one has to bring
in more prior information to give economic interpretation. See Maddala
(1991).
Juselius (1994a) found in the first step, when doing a reduced-rank
analysis of the 1(2) model by Johansen's two-step method described
earlier, that r — 2, that is, there are two cointegrating vectors. Note that
the number of variables is p = 5 whereas it was p = 4 in Johansen and
Juselius (1990). Also, the starting model is different (second-differences
rather than first-differences).
In the second step of the two-step procedure, Juselius found that
si = p — r — s2 was 2. This implies s2 = p — r — s\ is 1. Thus, there
is one 1(2) trend in the data. If r = 1 is chosen, all the hypotheses
si = 0,1,2,3 are rejected and thus we would not have found the 1(2)
relation because s2 = 5 — 1 — 4 = 0. Juselius says that considering that
r is not well determined is potentially a serious problem. In any case,
Juselius argues that analysis with the 1(2) model indicates that r = 1 is
incorrect.
Since we have accepted that Xt is second-order stationary, (3fXt can
no longer be assumed stationary as in the 1(1) model. However, f3fXt
corrected for a weighted sum of the differenced process is stationary. The
(3'Xt can be decomposed into (r - s2) = 1, 1(0) vectors and s2 = 1, 1(1)

358 
Cointegrated systems with 1(2) variables
vectors. Juselius presents estimates of these vectors. The 1(0) vector
when normalized with respect to nit gives
m ~ 1.3T/ + 0.95p - 5.3i6 + 4.5id
This can be interpreted as the long-run demand for money. Johansen and
Juselius (1990) arrived at roughly the same coefficients. They first tested
for price homogeneity and since it was clearly accepted, the empirical
analysis was for real money, real income, and some proxies for the cost
of holding money. The long-run demand for money equation obtained
in Johansen and Juselius (1990) was
ra2 ~ 1.03y - 5.21ib + 4.22id + 6.06
(Note that a constant was also estimated.)
Juselius (1994a) also conducts an analysis in real terms but adds an-
other explanatory variable Ap. The empirical results are therefore not
comparable to earlier results. We will skip the details since a detailed
discussion can be found in the original paper. The interesting points to
note are the differences in the results produced from the 1(2) analysis
compared with the 1(1) analysis. One of the long-run reltionships was
the same in both the analyses.
11.3.2 Studies on PPP using 1(2) variables
Johansen (1992b) and Juselius (1994b) use 1(2) models to analyze pur-
chasing power parity (PPP) theory. There have been numerous studies
on PPP using unit roots, cointegration, fractional integration, and so
on. The main argument of the Johansen and Juselius papers is that
taking the nature of the data into consideration is important and that
there is evidence to show that some of the variables are 1(2) and this
changes the inference on PPP.
11.4 Summary and conclusions
This chapter discussed cointegrated systems with 1(2) variables. It has
been observed that some variables, money supply and prices, are better
characterized as 1(2) rather than 1(1). Thus, there are a few practical
situations where 1(2) variables do occur.
We first discuss the determination of the order of differencing: testing
1(2) versus 1(1). Next we discuss cointegration analysis with 1(2) and
1(1) variables: single equation methods and system methods. Finally

References 
359
we illustrate applications: studies on demand for money and purchasing
power parity.
Given that 1(2) variables occur only in a few situations we have kept
our discussion brief. Omission of the topic completely leaves a hole and
we wanted to avoid that. At the same time, the theoretical developments
on 1(2) variables are far ahead of their empirical usefulness.
References
Dickey, D.A. and S.G. Pantula (1991), "Determining the Order of Dif-
ferencing in Autoregressive Processes," Journal of Business and
Economic Statistics, 5, 455-461.
Gregoir, S. and G. Laroque (1994), "Polynomial Cointegration: Esti-
mation and Tests," Journal of Econometrics, 63 183-214.
Haldrup, N. (1994a), "Semiparametric Tests for Double Unit Roots,"
Journal of Business and Economic Statistics, 12, 109-122.
(1994b), "The Asymptotics of Single Equation Cointegration Regres-
sions with 1(1) and 1(2) Variables," Journal of Econometrics, 63,
153-181.
Hasza, D.P. and W.A. Fuller (1979), "Estimation of Autoregressive
Processes with Unit Roots," Annals of Statistics, 7, 1106-1120.
Hendry, D.F. and N.R. Ericsson (1991), "modeling the Demand for
Narrow Money in the United Kingdom and the United States,"
European Economic Review, 35, 833-881.
Johansen, S. (1992a), "A Representation of Vector Autoregressive Pro-
cesses Integrated of Order 2," Econometric Theory, 8, 188-202.
(1992b), "An 1(2) Cointegration Analysis of the Purchasing Power
Parity between Australia and USA," in Colin Hargreaves (ed.),
Macroeconomic modeling of the Long-run, Edward Elgar.
(1992c), "Cointegration in Partial Systems and the Efficiency of Sin-
gle Equation Analysis," Journal of Econometrics, 52, 389-402.
(1992d), "Testing Weak Exogeneity and the Order of Cointegration
in UK Money Demand Data," Journal of Policy modeling, 14,
313-335.
(1994a), "The Role of the Constant and Linear Terms in Cointegra-
tion Analysis of Non-Stationary Variables," Econometric Re-
views, 13, 205-229.
(1994b), "Estimation of Systems of Trending Variables," Economet-
ric Reviews, 13, 351-386.

360 
Cointegrated systems with 1(2) variables
(1994c), "A Likelihood Analysis of the 1(2) model," Scandinavian
Journal of Statistics.
(1995), "A Statistical Analysis of Cointegration for 1(2) Variables,"
Econometric Theory, 11, 25-59.
Johansen, S. and K. Juselius (1990), "Maximum Likelihood Estimation
and Inference on Cointegration - with Applications to the De-
mand for Money," Oxford Bulletin of Economics and Statistics,
52, 169-210.
Juselius, K. (1994a), "On the Duality between Long-Run Relations and
Common Trends in the 1(1) versus 1(2) Model. An Application
to Aggregate Money Holdings," Econometric Reviews, 13, 151-
178.
(1994b), "Do Purchasing Power Parity and Uncovered Interest Parity
Hold in the Long-Run? An Example of Likelihood Inference
in a Multivariate Time Series Model," Paper presented at the
Econometric Society Meeting, Maastricht.
Kitamura, Y. (1995), "Estimation of Cointegrated Systems with 1(2)
Processes," Econometric Theory, 11, 1-24.
Maddala, G.S. (1991), "On the Use of Cointegration Tests," Paper
presented at the Bank of Korea, Seoul in August, 1991.
Ogaki, M. and J.Y. Park (1989), "A Cointegration Approach to Esti-
mating Preference Parameters," Rochester Center for Economic
Research Working paper No. 209, University of Rochster.
Pantula, S.G. (1986), "Determining the Order of Differencing in Au-
toregressive Processes," Department of Statistics, North Car-
olina State University.
Paruolo, P. (1994), "Asymptotic Efficiency of the Two- Stage Estimator
in 1(2) Systems," University of Bologna.
Phillips, P.C.B. (1991), "Optimal Inference in Cointegrated Systems,"
Econometrica, 59, 283-306.
Phillips, P.C.B. and Y. Chang (1994), "Fully Modified Least Squares
in 1(2) Regression," Econometric Theory, 10, 967.
Phillips, P.C.B. and S. Ouliaris (1990), "Asymptotic Properties of
Residual Based Tests for Cointegration," Econometrica, 58,
165-193.
Sen, D.L. (1995), "Robustness of Single Unit Root Test Statistics in the
Presence of Multiple Unit Roots," unpublished Ph.D. Disserta-
tion, Department of Statistics, North Carolina State University.

References 
361
Stock, J.H. and M.W. Watson (1993), "A Simple Estimator of Coin-
tegrating Vectors in Higher Order Integrated Systems," Econo-
metrica, 61, 783-820.
West, K.D. (1988), "Asymptotic Normality When Regressors Have a
Unit Root," Econometrica, 56, 1397-1418.
Yoo, B.S. (1986), "Multi-Cointegrated Time Series and Generalized
Error Correction Models," Discussion paper, University of Cal-
ifornia at San Diego, CA.

12
Seasonal unit roots and seasonal
cointegration
As mentioned in chapter 2 and discussed in subsequent chapters, we
have two types of trend: deterministic trend and stochastic trend. Sim-
ilar is the case of seasonals: deterministic seasonal and stochastic sea-
sonal. Deterministic seasonals are taken care of by seasonal dummies;
stochastic seasonals are taken care of by differencing. For instance, with
quarterly data we use A ^ = yt — yt-A and with monthly data we
use Ai2?/t = Vt — yt-12 if the seasonal is a stochastic seasonal. As
with the operation of first-differences to take care of trends, the Box-
Jenkins approach uses seasonal differences (with quarterly data it is
fourth-difference, with monthly data it is twelfth-difference, and so on).
Again the tests for seasonal roots is a formal generalization of this ad
hoc procedure.
The paper by Davidson, Hendry, Srba, and Yeo (1978), to be referred
to as DHSY, which used seasonal differences also had a strong influence
on subsequent research that often used seasonal differences. The main
focus of the paper by DHSY was the development of an appropriate
econometric methodology for modeling dynamic relationships. After
examining a number of different fomulations, these authors selected a
model in which seasonal lags and seasonal differences played a prominent
role. As with the concept of cointegration in models with stochastic
trends, we have the concept of seasonal cointegration in models with
stochastic seasonals. We shall discuss these concepts in what follows.
The literature on seasonality is enormous. We shall only discuss a few
main findings concerning seasonal unit roots and seasonal cointegration.
For more comprehensive reviews see Hylleberg (1986, 1992, 1994), Ghy-
sels (1993), and Franses (1996). The book by Franses discusses a type
of seasonality where the parameters in an AR model vary with seasons.
362

Seasonal unit roots and seasonal cointegration 
363
This leads to the concepts of periodic unit roots and periodic cointegra-
tion. We shall discuss these concepts in later sections of this chapter.
There are a variety of possible models for seasonality which may differ
across series. Three classes of time series models are commonly used to
model seasonality. These are:
(i) purely deterministic seasonal processes,
(ii) stationary seasonal processes,
(iii) integrated seasonal processes.
A purely deterministic seasonal process is a process generated by sea-
sonal dummy variables. For quarterly series, a purely deterministic sea-
sonal process can be described as
xt = a0 + &iSit + ct2S2t + otsSst + St
where Sit, $2*5 and Sst are seasonal dummies. This process can be per-
fectly forcast and will never change its shape.
A stationary seasonal process can be generated by a potentially infinite
autoregression
<p(L)xt = eu 
£t ~ wd(0, a2)
with all of the roots of (p(L) = 0 lying outside the unit circle but where
some are complex pairs with seasonal periodicities. More precisely, the
spectrum of such a process is given by
which is assumed to have peaks at some of the seasonal frequencies ws.
An example for quarterly data is
Xt = pXt-4 + St
which has a peak at both the seasonal periodicities ?r/2 (one cycle per
year) and TT (two cycles per year) as well as at zero frequency (zero cycles
per year).
A series Xt is an integrated seasonal process if it has a seasonal unit
root in its autoregressive representation. The properties of seasonally
integrated series are not immediately obvious, but are quite similar to
the properties of ordinary integrated processes. They have long memory
so that shocks last forever and may in fact change permanently the
seasonal patterns. Section 12.2 will discuss the seasonally integrated
processes in detail. Before discussing seasonal integration it is worth

364 
Seasonal unit roots and seasonal cointegration
questioning what the effect of seasonal adjustment (or prefiltering data)
on econometric inference is.
12.1 Effect of seasonal adjustment
Most of the empirical studies in econometrics use seasonally adjusted
data. The question is: what is the effect of prefiltering (seasonally ad-
justing) data on econometric inference and hypothesis testing? Sims
(1974) and Wallis (1974) investigated the nature of the asymptotic bias
due to the seasonal noise in a linear regression model. They found that
linear filtering does not induce asymptotic bias as long as uniform fil-
tering is used. Using different filters for different series would produce
asymptotic biases in linear regression models estimated with OLS. How-
ever, when lagged dependent variables are used, the effect is less clear.
Ghysels (1990) showed that unit root tests applied to post-war sea-
sonality adjusted GNP series strongly support the null hypothesis of
a unit root. But the evidence with seasonally unadjusted data is far
less conclusive. However, when a unit root null is rejected, it is not
clear whether the rejection is due to the absence of a unit root or the
presence of seasonality. This problem has been pursued in Ghysels and
Perron (1993). They considered the simple dynamic regression model
with lagged dependent variables and investigated the effect of prefilter-
ing on the behavior of the OLS estimator of the sum of the autoregressive
coefficients. They show the existence of a limiting upward bias when the
process does not contain a unit root. When the autoregressive process
has a single unit root, then the OLS estimator of the first-order AR
parameter is consistent and filtering does not affect the asymptotic dis-
tribution. This result holds for two-sided symmetric filters with weights
adding up to one. For example, the widely used X-ll filter of the US
Bureau of the Census has this property.
There remains the question of whether unit root tests should be ap-
plied with seasonally adjusted or seasonally unadjusted data. When
seasonally adjusted data are used, Ghysels and Perron show that the
nonvanishing asymptotic bias can be quite substantial. This is basi-
cally due to the fact that even though seasonal adjustment eliminates
correlation in the data at seasonal frequencies (albeit imperfectly) it
induces a bias in the autocorrelation function at lags less than the sea-
sonal period which does not vanish even asymptotically. Hence, the
ADF and Phillips-Perron statistics for testing a unit root will be biased
toward nonrejection of the unit root null if filtered data are used. From

12.2 Seasonal integration 
365
an asymptotic perspective, it is, therefore, expected that the unit root
tests will have greater power if unadjusted data are used.
Ghysels and Perron (1993) also present an extensive simulation study
of finite sample behavior emphasizing both the size and power of the
usual ADF and Phillips-Perron statistics (both with and without an
estimated trend) and show that, in many cases, there is a considerable
reduction in power. Ghysels and Perron argue that their results provide
additional justification for using annual data when testing for a unit root
(see Perron, 1991 and Shiller and Perron, 1985).
Diebold (1993) in his comment on the paper by Ghysels and Perron
disagrees with some of their results, but argues that Ghysels-Perron's
paper is a natural extension of the paper by Sims, Stock, and Watson
discussed in chapter 7. Sims et al. point out that, if interest centers on
point forecasts or point estimates of impulse response functions, VARs
can be modeled in levels with trends terms included. One can then get
consistent estimates whether there is a deterministic trend, stochastic
trend, or cointegration and one does not have to impose any of them.
The Ghysels-Perron paper leads to an extension of this result. One
can specify VAR models using seasonally unadjusted data in their levels
with deterministic trend and deterministic seasonal dummies added and
get consistent parameter estimates whether the data show deterministic
trend, stochastic trend, cointegration, deterministic seasonality, stochas-
tic seasonality, seasonal integration, or seasonal cointegration. The pa-
per by Canova (1993) on forecasting makes use of their results.
12.2 Seasonal integration
The Box-Jenkins methods discussed in chapter 2 used first-difference to
eliminate trend and fourth-difference to eliminate seasonality in quar-
terly data and twelfth-difference to eliminate seasonality in yearly data.
The work on seasonal integration is an attempt to formalize and expand
on this procedure.
Let us define Ak = (1 - Lk) so that Ai = (1 - L), A4 = (1 - L4)
and A12 = (1 — L12). If yt is the observed quarterly time series, the
Box-Jenkins procedure amounts to applying the filter A1A4. But we
can write

366 
Seasonal unit roots and seasonal cointegration
where i = y/^1. Thus A1A4 has two unit roots, and the other roots are
—1, -hi, and —i. The two unit roots correspond to nonseasonal frequency,
the root —1 corresponds to 1/2 cycle per quarter or two cycles per year
and the roots +i and —i correspond to 1/4 cycle per quarter or one
cycle per year. Given that the filter A4 also has a unit root, there is the
question of the appropriateness of the double filter Ai A4. This has been
investigated by Osborn (1990) for several quarterly UK macroeconomic
time series using the methods developed in Osborn et al. (1988). The
major conclusion is that there is no evidence in favor of the double filter
A1A4 and that in practice at most the filter A4 may be useful.
The test suggested in Osborn et al. (1988), commonly known as the
Osborn test involves estimating (in the case of quarterly data) the re-
gression
AiA42/t = c0 H- ciSu + c2S2t + c3S3t +
k
where Su(i = 1,2,3) are seasonal dummies. The joint hypothesis about
the usefulness of the A1A4 operator requires the joint test of the hy-
pothesis c\ = C2 = C3 = /?i = /?2 = 0. The validity of the A4 operator
implies fa — 0 with fa < 0 and the validity of the Ai operator requires
fa = 0 and fa < 0. Critical values for these three different tests are
reported in Osborn (1990, table 1), but for a sample size of 136 only.
12.3 Tests for seasonal unit roots
There is next the question of discussing in greater detail the appropri-
ateness of the A4 filter. A time series is said to be seasonally integrated
if the A4 filter is needed to make it stationary and the time series is said
to have seasonal unit roots. We shall discuss three tests:
(i) the DHF test by Dickey, Hasza, and Fuller (1984),
(ii) the HEGY test by Hylleberg, Engle, Granger, and Yoo (1990),
(iii) the CH test by Canova and Hansen (1995).
Another alternative suggested by Harvey and Scott (1994) will be dis-
cussed in the next section.

12.3 Tests for seasonal unit roots 
367
The DHF test
DHF analyze
Vt = 042/t-4 + Ut
when ut is some stationary invertible ARMA process and consider test-
ing the null hypothesis ^4 = 1. The alternative is ^4 < 1. As discussed
earlier, the filter A4 has four roots, one of which is a unit root. The
DHF test rules out the possibility that the time series yt can be made
stationary by using the Ai filter to take care of the unit root and sea-
sonality can be taken care of by seasonal dummies. The DHF test tests
the hypothesis that all the four roots implied by the filter A4 are unity
(one nonseasonal and three seasonal unit roots) versus the alternative
of no unit roots.
The HEGY test
The HEGY test is a test for seasonal and nonseasonal unit roots in a
quarterly time series. An extension of this test to monthly series is given
in Franses (1991b) and Beaulieu and Miron (1993). The HEGY test is
a test for the appropriateness of A4 versus its nested components like
(1 — L) or (1 + L). The HEGY test is based on a result on the decom-
position of polynomials (for details, see Hylleberg et al, 1990, p. 222).
The test is based on the following auxiliary regression
7T22/2,t-l
k
-i + St 
(12.1)
where Dst are seasonal dummies, Tt is the trend, and
ylt 
= (1 + L + L2 + L3)yt
y2t = -(1-L 
+ L2-L3)yt
yst = 
-(l-L2)yt
If TTI = 0, the series contains a unit root at the zero frequency which
implies that the series contains a nonseasonal stochastic trend. If TT2 = 0,
this implies two cycles per year. If TTS = 0 and 7T4 = 0, the series contains
the roots i and —i, i.e., seasonal unit roots at the annual frequencies.
The appropriate filters to use are (1 — L) if TT\ =0, (1 -h L) if TT2 = 0
and (1 + L2) if TT3 = 0 and TT4 = 0. Simulated critical values for the
t-tests for the significance of TTI and TT2 and for an F-test for the joint

368 
Seasonal unit roots and seasonal cointegration
significance of TT^ and TT4 are tabulated in Hylleberg et al. (1990). Table
12.1 presents some of their critical values.
The results of a Monte Carlo study by Hylleberg (1995) show that the
size and the power of the HEGY test is reasonable when the DGP is an
AR process. However, the HEGY test performs poorly in the presence
of the MA errors and cannot cope with a very weakly changing seasonal
components in addition to a strong deterministic seasonal pattern.
Franses (1990) extends the HEGY (1990) procedure to monthly time
series. For monthly series the differencing operator A12 will have 12
roots on the unit circle such that
1 - L12 
= 
(1 - L)(l + L)(l - iL)(l + iL)
x 
[1 + (>/3 + i)L/2] [1 + (VS - i)L/2]
x 
[1 - (\/3 + i)L/2] [1 - (V3 - i)L/2]
x 
[l + (V3 + i)L/2] [1 - {VS - i)L/2]
x 
[1 - (V3 + i)L/2] [1 + (\/3 - i)L/2]
where all terms other than (1 — L) correspond to seasonal unit roots.
Testing for unit roots in monthly time series is equivalent to testing for
the significance of the parameters in the auxiliary equation
<p*(L)ySit 
=
(12.2)
where <p*{L) is some polynomial function of L for which the usual as-
sumption applies, \it is the deterministic component, and where
yltt 
= 
(1
l/4,t 
= 
- ( l -
y5)t 
= 
- ( l -
yBtt 
= 
(1-Lu)y

Auxiliary
Regressions
T
Table
t : 7Ti -
1%
12.1.
= 0
5%
Critical values
t : 7T2 
=
1%
for
0
5%
seasonal unit
t : 7T3 =
1%
roots
- 0
5%
in quarterly
t : 7T4
1%
data
= 0
5%
F : 7T3 = '
95%
7T4-0
99%
No intercept 
48 
-2.72 
-1.95 
-2.67 
-1.95 
-2.66 
-1.93 
-2.51 
-2.11 
3.26 
5.02
No seas. dum. 
100 
-2.60 
-1.97 
-2.61 
-1.92 
-2.55 
-1.90 
-2.43 
-2.01 
3.12 
4.89
No trend 
136 
-2.62 
-1.93 
-2.60 
-1.94 
-2.58 
-1.92 
-2.44 
-1.99 
3.14 
4.81
200 
-2.62 
-1.94 
-2.60 
-1.95 
-2.58 
-1.92 
-2.43 
-1.98 
3.16 
4.81
Intercept 
48 
-3.66 
-2.96 
-2.68 
-1.95 
-2.64 
-1.90 
-2.44 
-2.06 
3.04 
4.78
No seas. dum. 
100 
-3.47 
-2.88 
-2.61 
-1.95 
-2.61 
-1.90 
-2.38 
-1.99 
3.08 
4.77
No trend 
136 
-3.51 
-2.89 
-2.60 
-1.91 
-2.53 
-1.88 
-2.36 
-1.98 
3.00 
4.73
200 
-3.48 
-2.87 
-2.58 
-1.92 
-2.57 
-1.90 
-2.36 
-1.98 
3.12 
4.76
Intercept 
48 
-3.77 
-3.08 
-3.75 
-3.04 
-4.31 
-3.61 
-2.86 
-2.37 
6.60 
9.22
Seas. dum. 
100 
-3.55 
-2.95 
-3.60 
-2.94 
-4.06 
-3.44 
-2.78 
-2.32 
6.57 
8.74
No trend 
136 
-3.56 
-2.94 
-3.49 
-2.90 
-4.06 
-3.44 
-2.72 
-2.31 
6.63 
8.92
200 
-3.51 
-2.91 
-3.50 
-2.89 
-4.00 
-3.38 
-2.74 
-2.33 
6.61 
8.93
Intercept 
48 
-4.23 
-3.56 
-2.65 
-1.91 
-2.68 
-1.92 
-2.41 
-2.05 
2.95 
4.64
No seas. dum. 
100 
-4.07 
-3.47 
-2.58 
-1.94 
-2.56 
-1.89 
-2.38 
-1.97 
2.98 
4.70
Trend 
136 
-4.09 
-3.46 
-2.65 
-1.96 
-2.56 
-1.90 
-2.36 
-1.97 
3.04 
4.57
200 
-4.05 
-3.44 
-2.59 
-1.95 
-2.58 
-1.92 
-2.35 
-1.97 
3.07 
4.66
Intercept 
48 
-4.46 
-3.71 
-3.80 
-3.08 
-4.46 
-3.66 
-2.75 
-2.26 
6.55 
9.27
Seas. dum. 
100 
-4.09 
-3.53 
-3.60 
-2.94 
-4.12 
-3.48 
-2.76 
-2.32 
6.60 
8.79
Trend 
136 
-4.15 
-3.52 
-3.57 
-2.93 
-4.05 
-3.44 
-2.71 
-2.78 
6.62 
8.77
200 
-4.05 
-3.49 
-3.52 
-2.91 
-4.04 
-3.41 
-2.65 
-2.27 
6.57 
8.96
Note: The data generating process (DGP) is A±xt = £t ~ im(0,1).
Source: tables la and lb of Hylleberg et al. (1990, pp. 226-227).

370 
Seasonal unit roots and seasonal cointegration
Furthermore, the fit in equation (12.2) covers the deterministic part
and might consist of a constant, seasonal dummies, or a trend. This
depends on the hypothesized alternative to the null hypothesis of 12 unit
roots.
Applying OLS to equation (12.2) gives estimates of the TT^S. In case
there are (seasonal) unit roots, the corresponding 7^ are zero. Due to the
fact that pairs of complex unit roots are conjugates, it should be noted
that these roots are only present when pairs of TTS are equal to zero simul-
taneously, for example, the roots i and — i are only present when ITS and
7T4 are equal to zero (see Pranses, 1990, for detailed derivation). There
will be no seasonal unit roots if TT2 through 7ri2 are significantly different
from zero. If TTI = 0 , then the presence of root 1 cannot be rejected.
When TTI = 0 and TT2 through 7ri2 are not equal to zero, the seasonals
can be modeled with seasonal dummies and the first-differenced series.
In case all 7r^,z = 1,..., 12, are equal to zero, it is appropriate to apply
the A12 filter. Extensive tables with critical values for t-tests of the
separate TTS, and for F-tests of pairs of TTS, as well as for a joint F-test
of 7T3 = • • • = 7Ti2 can be found in Pranses (1990). Table 12.2 shows part
of critical values tabulated by Pranses (1990).
The CH test
Canova and Hansen (1995) propose a test for the null of stationarity
against seasonal unit roots which extends the KPSS test (discussed in
chapter 4) to seasonal data. The asymptotic distributions are nonstan-
dard and the critical values are provided in their paper. By a Monte
Carlo study Hylleberg (1995) found that the CH test performs poorly
under the presence of unit roots at other frequencies and moving-average
errors.
However, as argued in Hylleberg and Pagan (1996), the HEGY test
and the CH test are not directly comparable, much the same way that
a test for unit root and a test for stationarity are not comparable (the
null hypotheses are different and the alternatives are reversed). Thus,
each of the studies can start with DGPs appropriate for their test and
show that the other test does not perform well.
Hylleberg and Pagan suggest an evolving seasonal model (ESM) in
which the coefficients attached to seasonal trigonometric functions fol-
low simple autoregressive processes, and discuss its relationship to the
HEGY and CH tests. Since the ESM nests the models considered by
HEGY and CH, Hylleberg and Pagan are able to make comparisons be-
tween the two tests and discuss their limitations. They also suggest how

12.4 The unobserved component model
371
Table 12.2. Critical values for seasonal unit roots in monthly data
t-statistics
7T2
t-statistics
7T3
7T4
7T5
7T6
7T7
7T8
7T9
TTlO
TTii
7Tl2
F-statistics
7Tl,7T4
"^5,776
7ri,7T5
779,^10
7Tn,7ri2
7Ti, ...,7Ti2
0.05
-3.24
-2.65
0.0025
-2.05
-3.34
-3.29
-3.38
-0.18
-3.40
-2.86
-3.36
-1.08
-3.42
0.90
4.81
4.86
4.94
4.76
4.92
4.00
with trend
0.05
-1.71
-3.12
-2.99
-3.12
0.12
-3.15
-2.54
-3.07
-0.73
-3.16
0.95
5.63
5.84
5.90
5.71
5.84
4.45
0.10
-2.92
-2.39
0.95
1.72
-0.45
-0.06
-0.44
2.98
-0.43
0.81
-0.40
2.55
-0.44
Regression
0.975
2.10
-0.15
0.24
-0.11
3.28
-0.17
1.12
-0.09
2.80
-0.17
without trend
0.05
-2.63
-2.65
0.0025
-2.11
-3.34
-3.29
-3.39
-0.27
-3.39
-2.87
-3.37
-1.11
-3.43
0.90
4.83
4.89
4.94
4.79
4.94
4.00
0.05
-1.76
-3.12
-3.00
-3.12
0.05
-3.14
-2.54
-3.07
-0.78
-3.16
0.95
5.62
5.86
5.86
5.75
5.89
4.46
0.10
-2.35
-2.40
0.95
1.74
-0.44
-0.05
-0.42
3.00
-0.42
0.82
-0.39
2.56
-0.42
0.975
2.11
-0.14
0.25
-0.09
3.31
-0.18
1.13
-0.07
2.83
-0.14
Notes: The DGP is yt = t/12 + et, et ~ iin(0,1), T = 120.
The test for TTI and TT2 are one-sided tests,
while the other t-tests are two-sided.
Source: Pranses (1990, pp. 12-18)
all the unit root tests discussed in chapter 4, such as alternative tests
for stationarity and efficient tests for unit roots by Elliott et al. can be
extended to tests of seasonal unit roots.
12.4 The unobserved component model
There are two principal ways to model time series with trend, seasonal,
and irregular components. The first is to fit a seasonal ARIMA model,
such as airline model in Box and Jenkins (1970), and then decompose
it by maximizing the variance of the irregular components. The second

372 
Seasonal unit roots and seasonal cointegration
is to specify explicitly models for the unobserved trend, seasonal, and
irregular components. This is called the unobserved component model
or structural time series model. The unobserved components model is
widely used as a workhorse in the statistical time series literature on
seasonality. The history of the unobserved components model can be
found in Nerlove, Grether, and Carvalho (1979), Bell and Hillmer (1984),
and Hylleberg (1986).
Consider
Vt = Vt + It + et
where fit is the trend, 7t the seasonal, and Et the irregular component.
Et is white noise, and thus stationary. The trend fit is only stationary in
first- or second-differences and 7t is stationary when multiplied by the
seasonal summation operator
O \ ± J ) = 1 -f- L/ •+- Li 
f ' - ' + L
where s is the number of seasons and S(L) contains both real and com-
plex unit roots. The seasonal 7* is said to be seasonally integrated (see
section 12.2). It ensures that the seasonal component, which is nonsta-
tionary, is not confounded with the trend and that the seasonal pattern
projected into the future sums to zero over s consecutive time periods.
In the context of structural time series modeling, the trend component
is formulated as
fit = fit-i + Pt-i + Vu 
Vt ~ md(0, o-v)
This collapses to a random walk plus drift if o~^ = 0 and to a determinis-
tic linear trend if a^ = 0 as well. Setting o~^ to zero when a^ is positive
tends to give a trend which changes relatively smoothly. As regards
seasonality, a number of different specifications exist, the simplest one
is
s-l
5(L)7t = ^2 7t-j =wu 
wt~ im(0, crw) 
(12.3)
3=0
This is known as the dummy variable form of stochastic seasonality. It
reduces to a standard deterministic component if d2^ is zero. Another
formulation, trigonometric seasonality, can be written in the following

12.4 The unobserved component model 
373
way when s is even
8/2
7t = 5>;,t 
(12.4)
where
7j,t = 7i,t-i c o s *j + 7j,t-i s i n A? + wjt
7j,t = 7i,t-i s i n AJ + 7j,t-i c o s XJ + ™*t
for j = 1,..., s/2 — 1 and Xj = 2TTJ/S, and
7j,t = -7j,t-i + ^i,t» i = 5/ 2
where Wjt and ^ t are normally distributed, zero mean, white noise pro-
cesses. The advantage of (12.4) over (12.3) is that it allows for smoother
changes in the seasonals, while the advantage of (12.3) is analytical sim-
plicity and is often used in the context of developing theoretical points.
All the disturbances are assumed mutually uncorrelated. The extent
to which trend and seasonal components evolve over time depends on
the parameters <7^, cr^, cr^, and o\ which can be estimated by maximum
likelihood in the time or frequency domain (see Harvey, 1989, chapter
4).
Harvey and Scott (1994) (hereafter denoted by HS) show how sea-
sonality can be incorporated into ECMs using an unobserved compo-
nents model and compared this with the earlier model studied by DHSY
(1978). HS include the stochastic seasonal j t into the ECM such that
Ayt = 0 + 7t + (3Axt + A(yt-i ~ <*xt-i) + et, 
t = 2,..., T
s-l
3=0
Setting a%j to zero generates a deterministic seasonal pattern, while a
nonzero value of a^ allows the seasonal pattern to evolve over time.
The higher is a^ relative to a\ the greater is the importance of seasonal
influences in accounting for variability in Ayt. Applying S(L) yields
Asyt = sO + f3Asxt + A(y£_i - aXt-i) + uu 
t = s + l,..., T
where As = A • S(L) and
ut = S(L)st + wt
Yt = S(L)yt

374 
Seasonal unit roots and seasonal cointegration
Xt = S(L)xt
Thus Yt_i is the sum of the observations over the year ending in period
t — 1 and similarly for Xt-\. If the disturbance ut is assumed to be
white noise, the model becomes very similar to the DHSY specification.
DHSY tried to capture seasonality by means of a general lag structure
and selected the following ECM
Asyt = 0 + PAsxt + Kvt-s - OLXts) + vt
where Vt is assumed to be white noise. The difference between the two
models lies in the error correction term; the DHSY specification uses
the quarterly term (if s = 4) yt-s — axt-s, while the HS specification
uses the annual aggregate Yt-i — aXt-\. Because these terms differ by
an order of magnitude s we would expect the estimate of A arising from
the DHSY model to be approximately s times that from the HS model.
Note that the DHSY model is a special case of the HS model in which
there is no irregular component, i.e., o~\ = 0 and Ut simply equals the
seasonal white noise disturbance term Wt>
HS argue that stochastic seasonality (for example, in ECM) has im-
portant implications for autoregressive distributed lag modeling. HS
criticize seasonal unit root tests developed by HEGY because of (i) poor
size and power properties and (ii) wrong hypothesis. Especially about
(ii) HS argue that the issue is not whether seasonal movements are sta-
tionary or nonstationary, but whether they are stochastic (nonstation-
ary) or deterministic. Thus, HS suggest that it is safer to proceed with
the more general assumption that the seasonal is stochastic rather than
imposing the constraint that it be deterministic. They show that letting
a seasonal component in a regression model be stochastic has little im-
pact on efficiency. On the other hand, the use of deterministic seasonals
when seasonality is stochastic is likely to lead to spurious dynamics.
HS estimate the above unobserved components model with the trig-
onometric specification of seasonals for quarterly UK nondurable con-
sumption and disposable income from 1957:111 to 1992:11. For nondurable
consumption they found that the fitted model may be written as
ct = /J<t+ It
where [it is a random walk plus drift. The drift parameter was estimated
as 0.0069 with a standard error of 0.0008 and G\ — 0. The nonzero
estimate of a^ implies that the seasonal component j t changes over

12.5 Seasonal cointegration 
375
time. Taking seasonal differences yields
Asct = S(L)r)t + sp
where As = 1 - Ls = A • 5(L). When S(L)7* is at most an MA(s - 2)
process, A SQ is at most an MA(s — 1) process and so
EtAsct+s = s(3
indicating that the expected value of consumption this time next year is
what it is now plus trend growth. Hence this model is a generalization
of the random walk theory of consumption, which takes account of the
fact that consumption will be different in different seasons and which
implies a degree of predictability when forecasting less than one year
ahead. The authors use a computer package called STAMP to perform
the ML estimation. (Details of this package can be obtained from the
authors.)
HS also applied the ECM with the stochastic seasonals for UK con-
sumption and income. They found that the DHSY model provided a
good fit over the sample period used, 1958:II-1970:IV, but it breaks
down when the sample is extended to 1958:11-1992:11. The general HS
model which encompassed the DHSY model as a special case shows that
it is possible to arrive at a stable ECM between consumption and income
over the extended period 1958:11-1992:11.
12.5 Seasonal cointegration
The HEGY procedure allows one to separately test for the four roots: 1,
— 1, ±i implied by the A4 filter. Corresponding to this Engle, Granger,
Hylleberg, and Lee (1993) (to be referred to as EGHL) suggest different
levels of seasonal cointegration. Suppose that two series yt and zt are
seasonally cointegrated so that A±yt and A±zt are stationary. When
there is cointegration at the zero frequency, i.e., at the root 1, we have
ut = (1 + L + L2 + L3)yt - ai(l + L + L2 + L3)zt 
(12.5)
is stationary. When there is cointegration at frequency 1/2, i.e., at the
root (—1), we have
vt = (1 - L + L2 - L3)yt - a2(l - L + L2 - Ls)zt 
(12.6)
is stationary. Finally when there is cointegration at frequency 1/4, i.e.,
at the root ±i or corresponding to the filter (1 — L2), we have
Wt = (l-L2)yt-a3(l-L2)zt-a4(l-L2)yt-1-as(l-L2)zt-i 
(12.7)

376 
Seasonal unit roots and seasonal cointegration
is stationary. We have to consider cointegration with one period lagged
because the filter (1 — L2) covers only the half-year period.
In the case where all the three variables ut,vt, and Wt are stationary
a simple version of the seasonal cointegration model is
-i + #2lVt-l + /?31™i-2 + PllWt-3 + eit
+ e2t (12.8)
where the /?s are the error correction terms. One can include a constant,
seasonal dummies, and trend in these equations.
EGHL propose tests for seasonal cointegration similar to the Engle-
Granger two-step test for cointegration, i.e., estimate equations (12.5)-
(12.7) by OLS and apply unit root tests (ADF tests) to iit,vt, and wt.
The critical values for the tests applied to ut and vt are the same as
those tabulated in Engle and Granger (1987), but for the test applied to
wt they are different. EGHL derive the asymptotic distributions in this
case and tabulate the critical values.
Of course, in principle, any of the cointegration tests discussed in
chapter 6 can be applied here, but the appropriate asymptotic distri-
butions have not been studied yet. Also, ECM tests applied to (12.8)
need further study. However, some more fundamental problems with
the seasonal cointegration model analyzed by EGHL need to be ad-
dressed before all these extensions can be carried out. In her comment on
the EGHL paper, Osborn argues that the seasonal cointegration model
(12.8) implies that the equilibrium relations between yt and zt vary with
the lag, e.g., the long-run relation at time (t — 1) is different from that
at time (t — 2). She argues that a more reasonable model is one where
coefficients vary with seasons. This leads to the periodic cointegration
model, which we shall discuss in section 12.9 later.
12.6 Estimation of seasonally cointegrated systems
In the previous section we discussed the EGHL extension of the En-
gle-Granger two-step method to the case of seasonal cointegration. Lee
(1992) extends the Johansen procedure to the case of seasonal cointe-
gration. The procedure consists of the same type of decomposition as
adopted earlier with the HEGY test as in equation (12.1).
Starting with a vector Yt of 1(1) variables and a VAR, the equation
we end up with corresponding to (12.1) is (for quarterly data)
A4yt 
= ADt + ni Yi,^

12.6 Estimation of seasonally cointegrated systems 
377
p-4
(12.9)
i=\
where Dt denotes the matrix of the deterministic variables (constant,
trend and seasonal dummies) and
Ylt = (1 + L + L2 + L3)Yt
Y2t = -(l-L 
+ L2- L3)Yt
Y3t = -(1 - L2)Yt
The ranks of the matrices III, II2, and II3 determine the number of co-
integrating vectors at zero, 1/2, and 1/4 frequencies. Lee derives trace
statistics as done in the Johansen procedure and derives the asymptotic
distributions. The details are too lengthy to be reproduced here. They
can be found in Lee's paper. Lee points out that the distribution of the
ML cointegration test statistics may be quite different from their asymp-
totic distributions. In fact, we tend to reject the true null hypothesis
more often than what is suggested by the asymptotic distribution. This
bias, however, decreases in very large samples. Hence Lee and Siklos
(1995) present finite sample critical values for the Johansen seasonal
cointegration test statistics. In particular, they consider a DGP which
contains unit roots at the seasonal and zero frequencies, as well as a time
trend. They present these for sample sizes 50, 100, 150, 200, and 00 and
also present a comparison of finite versus asymptotic critical values (see
tables 1, 2, 3 of their papers. We shall not reproduce those tables here.)
Kunst (1992) applies Lee's procedure to analyze seasonal cointegration
in the Austrian economy using quarterly data for 1964-1990 on five
variables. Although he finds evidence of seasonal cointegration, this
does not help in obtaining better forecasts. Kunst (1993) contains more
applications of Lee's procedure.
The regularity conditions for using the Johansen procedure require
that the DGP does not have unit roots other than the zero frequency.
However, Ghysels et al. (1994) show that seasonal unit roots do not
pose a problem in the use of the Johansen procedure, which can still be
used to test the hypothesis of cointegration at the zero frequency.
There are other extensions of cointegration methods (besides the
Johansen procedure) to seasonal cointegration. For instance, Joyeux
(1992) suggests an extension of the frequency domain cointegration tests
of Phillips and Ouliaris (see chapter 6) to the case of seasonal cointegra-

378 
Seasonal unit roots and seasonal cointegration
tion. In principle all the methods discussed in chapters 5 and 6 can be
extended to seasonal cointegration
12.7 Empirical evidence
EGHL (1993) applied the testing procedure discussed in the section
12.3 (except the test of seasonal cointegration at frequency 1/4 with
estimated cointegration vector) to consumption (c =log of consumption
expenditures on durables) and income (y = log of personal disposable
income) in UK from 1955:1 to 1984:IV. They investigate the permanent
income hypothesis where the log of income and the log of consumption
may be thought to be cointegrated with the cointegrating vector (1,-1).
Their results show strong evidence for the presence of a unit root at zero
frequency in all c, y, and c — y which implies that there is no cointegra-
tion between c and y at the long-run frequency, at least not for the
cointegrating vector (1,-1). The hypotheses that c, y, and c — y are
A/2(l) cannot be rejected and thus c and y are not cointegrated at the
bi-annual cycle either. The hypothesis that y is i~i/4(0) is rejected, but
that for c is not, and thus c and y cannot possibly be cointegrated at
this frequency. They conclude that the unit-elasticity error-correction
model is not valid at any frequency for the UK consumption and income
data.
EGHL (1993) applied the above testing procedure (including the case
with estimated cointegration vectors) to total consumption and dispos-
able income in Japan from 1961:1 to 1987:IV. Their results indicate
that the log of the income series is integrated of order 1 at both the
long-run frequency (6 = 0) and at the seasonal frequencies. This im-
plies that income is nonstationary and that the seasonal pattern has
significant variation over the period. A similar result is obtained for
consumption, although the seasonal pattern is more regular and it is
in fact a question whether a deterministic annual seasonal pattern is
to be preferred. The results of the cointegration analysis show that
the log of the two series is not cointegrated at any frequency with the
cointegrating vector (1,-1). It is also shown that the seasonally ad-
justed data, which are adjusted for seasonal unit roots by summing
over four consecutive quarters, are not cointegrated at the long-run
frequency, neither with cointegrating vector (1,-1) nor with an es-
timated cointegrating vector. The test for seasonal cointegration in-
dicates no cointegration at the bi-annual frequency, while there may
be signs of cointegration at the annual frequency. This indicates that

12.8 Periodic autoregression and periodic integration 
379
the annual seasonal components of consumption and income are simi-
lar.
In her comment on EGHL, Osborn (1993) argues that as long as
there is no good economic rationalization for seasonal unit roots, we
should not worry about them (see the criticism of EGHL in section
12.5).
Sarantis and Stewart (1993) apply the HEGY test to nine exchange
rate series and relative prices and conclude that none of the exchange
rates and relative prices contain seasonal unit roots, but all have an
autoregressive unit root. On the other hand, Ghysels, Lee, and Sik-
los (1993) and Lee and Siklos (1991) examine respectively, for the US
and Canada, several macroeconomic series and find unit roots at some
seasonal (but not all) frequencies. Further evidence on seasonal unit
roots is also presented in Pranses (1996) for 11 quarterly series from
the US, Canada, Germany, and the UK, and Granger and Siklos (1995)
for international data consisting of 16 quarterly and eight monthly time
series. The purpose of this latter study is to investigate the effect of
skip sampling and time aggregation. Overall these latter studies show
more evidence of seasonal unit roots at least at some seasonal frequen-
cies.
To investigate the robustness of the results from the seasonal unit root
tests, Pranses (1996) also investigates recursive unit root tests. The re-
sults suggest that the observed significance of the seasonal unit root test
statistics, might be due to seasonal mean shifts. The evidence in favor of
seasonal unit roots may become less pronounced, or may even disappear,
when one allows for seasonal mean shifts. This observation is similar to
the one about the evidence in favor of unit roots when one allows for
broken trends, as noted by Perron (1989).
Also, as discussed in section 12.4, Harvey and Scott point out the lim-
itations of the seasonal unit root tests suggested by Hylleberg if there
are moving-average errors. Thus, there are several parallels in the lim-
itations between the unit root tests discussed in chapters 3 and 4 and
the seasonal unit root tests discussed here.
12.8 Periodic autoregression and periodic integration
Here we shall discuss briefly the concepts of periodic autoregression and
periodic integration. More detailed discussion can be found in Osborn
(1990, 1991), Liitkepohl (1991), and the comprehensive book by Pranses
(1996).

380 
Seasonal unit roots and seasonal cointegration
Periodic autoregression
A periodic autoregression (PAR) model is one where the autoregressive
parameters vary with the seasons, e.g., if we consider an AR(1) model,
the PAR(l) model is (for quarterly data):
yt = (j)syt-i + uu 
s = 1,2,3,4
When t falls in season s, yt is seasonal because of seasonal variation in
the autoregressive parameter 0.
One important point to note about periodic models is that although
using annual data are thought to remove seasonality, the effects of pe-
riodic autoregressive coefficients are not necessarily removed by taking
annual sums. One common complaint against PAR models is that they
increase the number of parameters estimated, but this is the case only
if one uses a long autoregression and say monthly data. However, it is
shown in Osborn (1991) that the orders of time invariant models can
be higher than that of the PAR model. Neglecting periodicity may ne-
cessitate the inclusion of several (seasonal) lags in a nonperiodic model.
Periodic integration
Define the operator
dsVt = Vt -(f>syt-i, 
5 = 1 , 2 , 3 , 4
Then we say that yt is periodically integrated of order 1 or PI(1) if yt
is nonstationary and 8syt is stationary. This concept is related to time
varying parameter integration TVP(l) discussed by Granger (1986, p.
224).
It is shown in Osborn (1991) as well as Franses (1996, p. 99) that
yt is stationary if 0i(/>2</>3</>4 < 1- If </>i</>2</>3</>4 = 1 the PAR(l) process
has a unit root. A special case of interest is fa = <f> for all i. In this
case the condition </>i</>203</>4 = 1 can hold if (j) = 1 and also if 4> = — 1.
The first case corresponds to the case where yt has a nonseasonal unit
root and the second case corresponds to a seasonal unit root (half-year
cycle). Thus, both seasonal and nonseasonal unit roots are nested in
the PAR(l) model. Franses suggests first testing whether </>i02<fe</>4 = 1
and second testing whether fa — 1 or fa = — 1 for all i.
As for estimation of PAR processes, one can use OLS methods which
give maximum likelihood estimators under the assumption of normality.
The next step is to test the hypothesis of the constancy of the autore-
gressive parameters over the different seasons. For a PAR(p) process,

12.9 Periodic cointegration and seasonal cointegration 
381
one tests the hypothesis (for quarterly data)
#o • 4>is = <i>% for 5 = 1,2,3,4 and i = 1,2, ...,p
It has been shown in Boswijk and Franses (1996) that the likelihood
ratio test for this null hypothesis has an asymptotic X2(3p) distribution
irrespective of whether the yt series has nonseasonal or seasonal unit
roots.
The concept of periodic autoregression can be extended to VAR mod-
els as well (although this results in too many parameters to be esti-
mated). This extension is discussed in Liitkepohl (1991) and Franses
(1996, p. 124).
Tests for periodic unit roots
As discussed earlier, the PAR(l) process has a unit root if 01020304 = 1-
Tests for periodic unit roots are slightly more complicated than tests
for seasonal unit roots. Since a discussion of these tests involves more
algebra, we shall skip the details and give the appropriate references:
Boswijk and Franses (1995a, 1996) and Franses (1996, pp. 127-149).
12.9 Periodic cointegration and seasonal cointegration
Periodic cointegration refers to the case of seasonally varying long-run
relationships. A detailed discussion of this concept involves repeating
a lot of algebra from Franses (1996). Hence we refer to the sources:
Boswijk and Franses (1995b) and Franses (1996, chapter 9).
Periodic models involve many more parameters than seasonal models.
If one has a lot of data, they are worth considering because they make
more sense. But with few observations, of cource, one cannot afford the
luxury of estimating so many parameters.
It has been well documented that seasonal patterns are not constant
over time. One alternative to the periodic model is the evolving seasonal
model discussed in Hylleberg and Pagan (1996) This involves estimation
of fewer parameters than the periodic model.
12.10 Time aggregation and systematic sampling
Most macroeconomic data are obtained from time aggregation or sys-
tematic sampling. Granger and Siklos (1995) investigate the effects of
these procedures on tests for seasonal and nonseasonal unit roots and on
tests for cointegration. Systematic sampling occurs when observations

382 
Seasonal unit roots and seasonal cointegration
are at fixed intervals, e.g., end of March, June, September, and Decem-
ber for quarterly data. This is the case for variables like interest rates
and prices. Temporal aggregation occurs when flows are aggregated over
some fixed periods of time. This is the case with GNP, consumption,
money supply, and so on.
Granger and Siklos also investigate the effects of official seasonal ad-
justment procedures on tests for seasonal unit roots (see section 12.1 for
earlier discussion of effects on tests for unit roots). They show that:
(i) A time series which is integrated of order 1 at the zero frequency,
denoted by Io(l) can arise because of systematic sampling and/or
seasonal adjustment. This suggests that practiceners should re-
sort to unadjusted data.
(ii) Two time series can appear cointegrated at the zero frequency
but this can be due to a unit root at zero frequency in one series
and a seasonal unit root in another.
(iii) We can miss seasonal unit roots when using skip sampling. For
instance the existence of seasonal unit roots in, say, monthly data
can lead to missing seasonal unit roots with quarterly data. This
problem stems from skip sampling while temporal aggregation
produces no such results. They find that Ml has a unit root at
the bi-annual frequency when monthly data are averaged, but not
when the third monthly observation is used to generate quarterly
data.
(iv) Cointegration relationships are affected by the type of seasonal
adjustments used. It is important to consider cointegration by
seasonal frequency.
We shall not go into the detailed derivations here. Granger and Siklos
present empirical evidence on these propositions using an international
data set on US, UK, Canada, France, Japan, Germany, and Italy with
monthly and quarterly observations.
12.11 Conclusion
The present chapter discusses the statistical issues relating to seasonality
in time series. The problems discussed are the effects of seasonal adjust-
ment procedures on tests for unit roots, sesonal integration and seasonal
cointegration, tests for sesonal unit roots, periodic integration and coin-
tegration, and effects of time aggregation and skip sampling. There is
a large amount of literature on the economics of seasonality, which we

References 
383
have not discussed here. See the references in Hylleberg (1994) and
the papers in Hylleberg (1992). As mentioned in Hylleberg (1994, pp.
171-172), some things about seasonality are widely accepted. These are:
(i) It is the best to use seasonally unadjusted data, rather than offi-
cially seasonally adjusted data.
(ii) Seasonal variation often accounts for a major part of variation in
time series.
(iii) The seasonal and nonseasonal components are dependent and eco-
nomic considerations are important in explaining both the com-
ponents.
There is some controversy over the relative merits of modeling season-
ality through periodic models.
References
Beaulieu, J.J. and J.A. Miron (1993), "Seasonal Unit Roots in Aggre-
gate US Data", Journal of Econometrics, 55, 305-328.
Bell, W.R. and S.C. Hillmer (1984), "Issues Involved with the Seasonal
Adjustment of Economic Time Series," Journal of Business and
Economic Statistics, 2, 526-534.
Boswijk, H.P. and P.H. Pranses (1995a), "Testing for Periodic Integra-
tion," Economic Letters, 48, 241-248.
(1995b), "Periodic Cointegration : Representation and Inference,"
Review of Economics and Statistics, 77, 436-454.
(1996), "Unit Roots in Periodic Autoregression," Journl of Time
Series Analysis, 17, 221-245.
Box, G.E.P. and G.M. Jenkins (1970), Time Series Analysis Forecasting
and Control, Holden-Day, San Francisco.
Canova, F. (1993),"Forecasting Time Series with Common Seasonal
Patterns," Journal of Econometrics, 55, 173-200.
Canova, F. and B.E. Hansen (1995), "Are Seasonal Patterns Constant
over Time: A Test for Seasonal Stability," Journal of Business
and Economic Statistics, 13, 237-252.
Davidson, J.E.H., D.F. Hendry, F. Srba, and S. Yeo (1978), "Econo-
metric Modeling of the Aggregate Time Series Relationship be-
tween Consumers' Expenditure and Income in the UK," The
Economic Journal, 88, 661-692.

384 
Seasonal unit roots and seasonal cointegration
Dickey, D.A., H.R Hasza, and W.A. Fuller (1984), "Testing for Unit
Roots in Seasonal Time Series," Journal of the American Sta-
tistical Association, 79, 355-367.
Diebold, F.X. (1993), "Discussion : The Effect of Seasonal Adjustment
Filters on Tests for a Unit Root," Journal of Econometrics, 55,
99-103.
Engle, R.F. and C.W.J. Granger (1987), "Co-integration and Error
Correction: Representations, Estimation and Testing," Econo-
metrica, 55, 252-276.
Engle, R.F., C.W.J. Granger, S. Hylleberg, and H.S. Lee (1993), "Sea-
sonal Cointegration: The Japanese Consumption Function,"
Journal of Econometrics, 55, 275-298.
Franses, P.H. (1990), "Testing for Seasonal Unit Roots in Monthly
Data," Econometric Institute Report No. 9032A, Erasmus uni-
versity, Rotterdam.
(1991a), Model Selection and Seasonality in Time Series, Timbergen
Institute: Thesis Publisher.
(1991b), "Seasonality, Nonstationarity and the Forecasting of
Monthly Time Series," International Journal of Forecasting, 7,
199-208.
(1992), "Testing for Seasonality," Economics Letters, 38, 259-262.
(1996), Periodicity and Stochastic Trends in Economic Time Series,
Oxford university press, Oxford.
Franses, P.H. and T.J. Vogelsang (1995), "Testing for Seasonal Unit
Roots in the Presence of Changing Seasonal Means," Report no.
9532A, Econometric Institute, Erasmas University, Rolterdam.
Ghysels, E. (1990), "Unit Root Tests and the Statistics Pitfalls of Sea-
sonal Adjustment: The Case of US Postwar Real Gross National
Product," Journal of Business and Economic Statistics, 8, 145-
152.
(1993), "On the Economics and Econometrics of Seasonality," in C.
Sims (ed.), Advanced in Econometrics, Cambridge University
Press, Cambridge.
(1994), " On the Periodic Structure of the Business Cycle", Journal
of Business and Economic Statistics, 12, 289-298.
Ghysels, E. and P. Perron (1993), "Effect of Seasonal Adjustment Fil-
ters on Tests for a Unit Root," Journal of Econometrics, 55,
57-98.

References 
385
Ghysels, E. , H.S. Lee, and P.L. Siklos (1993), "On the Misspecifica-
tion of Seasonality: An Empirical Investigation with US Data,"
Empirical Economics, 18, 747-760.
Ghysels, E., H.S. Lee, and J. Noh (1994), "Testing for Unit Roots
in Seasonal Time Series: Some Theoretical Extensions and a
Monte Carlo Investigation," Journal of Econometrics, 62, 415-
442.
Granger, C.W.J. (1986), "Developments in the Study of Cointegrated
Economic Variables," Oxford Bulletin of Economics and Statis-
tics, 48, 213-228.
Granger, C.W.J. and P.L. Siklos (1995), "Systematic Sampling, Tempo-
ral Aggregation, Seasonal Adjustment and Cointegration: The-
ory and Evidence," Journal of Econometrics, 66, 357-369.
Harvey, A.C. (1989), Forecasting Structural Time Series Models and
the Kalman Filter, Cambridge University Press, Cambridge.
Harvey, A.C. and A. Scott (1994), "Seasonality in Dynamic Regression
Models," Economic Journal, 104, 1324-1345.
Hylleberg, S. (1986), Seasonality in Regression, Academic Press, New
York.
(1994), "Modeling Seasonal Variation," in C.P. Hargreaves (ed.),
Nonstationary Time Series Analysis and Cointegration, Oxford
University Press, Oxford.
(1995), "Tests for Seasonal Unit Roots: General to Specific or Spe-
cific to General?" Journal of Econometrics, 69, 5-25.
Hylleberg, S. (ed.), (1992), Modeling Seasonality, Oxford University
Press, Oxford.
Hylleberg, S. and A.R. Pagan (1996), "Seasonal Integration and the
Evolving Seasonal Model," manuscript, Australian National
University.
Hylleberg, S., R.F. Engle, C.W.J. Granger, and B.S. Yoo (1990), "Sea-
sonal Integration and Cointegration," Journal of Econometrics,
44, 215-238.
Joyeux, R. (1992), "Testing for Seasonal Cointegration Using Principal
Components," Journal of Time Series Analysis, 13, 109-118.
Kunst, R.M. (1992), "Seasonality in the Austrian Economy : Common
Seasonality and Forecasting," Research memorandom, No. 305,
Institute for Advance Studies, Vienna, Austria.
(1993), "Seasonal Cointegration in Macroeconomic Systems: Case
Studies for Small and Large European Countries," Review of
Economics and Statistics, 75, 325-330.

386 
Seasonal unit roots and seasonal cointegration
Lee, H.S. (1992), "Maximum Likelihood Inference and Cointegration,"
Journal of Econometrics, 54, 1-47.
Lee, H.S. and P.L. Siklos (1991), "Unit Roots and Seasonal Unit Roots
in Macroeconomic Time Series: Canadian Evidence," Economics
Letters, 35, 273-277.
(1995), "A Note on the Critical Values for the Maximum Liklihood
(Seasonal) Cointegration Tests," Economics Letters, 49, 137-
145.
Liitkepohl, H. (1991), Introduction to Multiple Time Series Analysis,
Springer Verlag, Berlin.
Nerlove, M., D.D. Grether, and J.L. Carvalho (1979), Analysis of Eco-
nomic Time Series - A Synthesis, Academic Press, New York.
Osborn, D.R. (1990), "A Survey of Seasonality in UK Macroeconomic
Variables," International Journal of Forecasting, 6, 327-336.
(1991), "The Implication of Periodically Varying Coefficients for Sea-
sonal Time Series Processes," Journal of Econometrics, 48, 373-
84.
(1993), "Discussion: Seasonal Cointegration: The Japanese Con-
sumption Function," Journal of Econometrics, 55, 275-298.
Osborn, D.R. and J.P. Smith (1989), "The Performance of Periodic Au-
toregressive Models in Forecasting Seasonal UK Consumption,"
Journal of Business and Economic Statistics, 7, 117-27.
Osborn, D.R., A.P.L. Chiu, J.P. Smith, and C.R. Birchenhall (1988),
"Seasonality and the Order of Integration for Consumption,"
Oxford Bulletin of Economics and Statistics, 50, 361-377.
Perron, P. (1989), "The Great Crash, the Oil Price Shock and the Unit
Root Hypothesis," Econometrica, 57, 1361-1401.
(1991), "Test Consistency with Varying Sampling Frequency,"
Econometric Theory, 7, 341-368.
Sarantis, N. and C. Stewart (1993), "Seasonality, Cointegration, and
the Long-Run Purchasing Power Parity: Evidence for Sterling
Exchange Rates," Applied Economics, 25, 243-250.
Sims, C.A. (1974), "Seasonality in Regression," Journal of American
Statistical Association, 69, 618-626.
Shiller, R. and P. Perron (1985), "Testing the Random Walk Hypoth-
esis: Power versus Frequency of Observation," Economics Let-
ters, 18, 381-386.
Wallis, K.F. (1974), "Seasonal Adjustment and Relations Between Vari-
ables," Journal of American Statistical Association, 69, 18-31.

Part IV
Structural change
Structural change is an important problem in time series and affects
all the inferential procedures discussed in the previous chapters. The
area is so vast that it warrants a separate book. However, omitting it
completely would leave an important gap in this book. We have therefore
decided to cover it briefly.
This part has 3 chapters.
Chapter 13 on structural change, unit roots, and cointegration starts
with a discussion of tests for structural change with known and un-
known break points. We next discuss tests for unit roots under struc-
tural change. Both the classical and Bayesian approaches are covered.
Next we discuss tests for structural change in cointegrated relationships.
Chapter 14 is on outliers and unit roots. We first discuss the different
kinds of outliers and their effects on unit root tests. Given that outliers
have serious effects on unit root tests, some outlier-robust unit root
testing procedures as well as outlier-robust estimation procedures for CI
systems, are discussed.
Chapter 15 is on regime switching models. These have been popular-
ized by Hamilton who suggested using the Markov switching regression
(MSR) models. This chapter discusses Hamilton's basic MSR model
and several extensions of this basic model. The models in this chapter
complement the outlier models discussed in the previous chapter.
One problem with the MSR model is that it models only sudden
changes. In practice structural change is gradual. We, therefore, discuss
model with gradual change. An important class of models with gradual
change is the structural time series model introduced by Harvey. This
model is estimated by using the Kalman filter. We discuss this model
briefly and give the relevant references because this is a vast area to
cover here at length.
387


13
Structural change, unit roots, and
cointegration
One major drawback of unit root tests is that, in all of them, the im-
plicit assumption is that the deterministic trend is correctly specified.
Perron (1989) argued that if there is a break in the deterministic trend,
then unit root tests will lead to a misleading conclusion that there is a
unit root, when in fact there is not. Perron's paper started a contro-
versy about the effect of trend breaks on unit root tests, and a criticism
that Perron assumed the break point to be known. It was argued that
if the break point is treated as endogenous, then Perron's conclusions
are reversed. Subsequent literature partially reversed this conclusion
and also extended the problem of trend breaks to other areas such as
cointegration and seasonality.
The purpose of this chapter is to present a selective survey of the liter-
ature on trend breaks in unit roots, cointegration, and seasonal integra-
tion and comment on their implications for further research. Because of
space limitations technical details will be omitted.1 Since the literature
is vast and it is easy to get lost in the details, the present chapter should
help readers to get a perspective on this area. In this chapter we shall
first review the literature on tests for structural change, and tests for
unit roots under structural changes with a known break point and an
unknown break point. We then move on to the Bayesian approach and
multiple structural changes. Next we discuss the problems of the effect
of structural change on cointegration tests.
1 A technical account of this area from the classical point of view is given in a
lengthy paper by Stock(1994). This chapter covers the Bayesian aspects as well,
and also discusses cointegration tests and problems of seasonality.
389

390 
Structural change, unit roots, and cointegration
13.1 Tests for structural change
Because of events like the great depression, oil price shocks, abrupt pol-
icy changes, and so on, models with constant coefficients have been found
to perform poorly, either for forecasting purposes or for the purpose of
analyzing the effect of policy changes. The solutions to this problem
have been
(i) Models with continuous parameter changes: these are estimated
using some recursive algorithms like the Kalman filter. The prob-
lem with these models is that they do not capture sudden shifts.
(ii) Outlier models: these models argue that sudden shocks produce
outliers (with temporary or permanent level shifts). These are
discussed in the next chapter.
(iii) Switching regression models, with abrupt switches and gradual
switches: one popular model during recent years in this category
has been the Markov switching regression (MSR) model. These
are discussed in chapter 15.
Before adopting any of these approaches, it is customary to test for
structural changes. There is now an enormous number of statistical tests
for testing structural change. The tests can conveniently be classified
under the categories:
(i) known break points versus unknown break points.
(ii) single break versus multiple breaks,
(iii) univariate versus multivariate relationships,
(iv) stationary versus nonstationary variables.
We shall discusses the different tests within this classification scheme.
13.2 Tests with known break points
The earliest tests for structural breaks in the economic literature are
the tests in Chow (1960) which are for stationary variables and a single
break. The model is a linear regression model with k variables and
two regimes with observations n\ and n^ respectively. Chow derives
two tests: (a) for rt\ > A;, n<z > k and (b) for n\ > fc, ri2 < k. The
first is known as an analysis of variance test, and has a long history. It
was discussed in Rao (1952) and Kullback and Rosenblatt (1957) among
others. The second test is known as a predictive test because it does
not test for stability of the coefficients but in fact tests for unbiasedness

13.3 Tests with unknown break points 
391
of predictions for the n<i observations, from a regression estimated with
n\ observations. This test is properly called the Chow test. Rao (1965)
also considers this deficient rank case under the heading: "The Third
Fundamental Theorem on Least Squares."
Dufour (1982) extends these tests to the case of multiple regimes and
covers the cases where subsamples size are less than k. He also provides a
single derivation for both the cases of full rank and deficient rank (tests
(a) and (b)). Consider the case of J groups with rij observations in the
jth group (j = 1,2,..., J) (some but not all of rij can be less than k). Let
RSSo be the residual sum of squares from a regression with the pooled
data, and RSST be the total residual sum of squares from the sets for
which rij > k. Let there be r such sets (r < J). Let n = J2nj a nd
ni — S r J2nj 
o v e r r s e t s f°r which nj > k. Then RSSo/cr2 ~ %2 with
d.f. (n - k) and RSST/(J2 ~ X2 with d.f. (m - rk) and
(RSS0 ~ RSST)/(n 
-k-ni+rk)
RSST/(ni - rk)
has an F-distribution with degree of freedom (n — k — ni +rfc), {n\ — rk).
Further discussion of Dufour's test can be found in Cantrell et al. (1991).
There are several extensions of the Chow tests (though one has to bear
in mind which of these are extensions of the analysis of the variance test,
and which are extensions of the predictive test). Lo and Newey (1985)
and Park (1991) extend these tests to simultaneous equations. Andrews
and Fair (1988) provide extensions of the analysis of the variance test
to general nonlinear models. Dufour, Glyssels, and Hall (1994) provide
extensions of the predictive test to general nonlinear models.
13.3 Tests with unknown break points
The traditional Chow (1960) test is developed to test the null hypoth-
esis of parameter constancy against the alternative of a known break
point a priori under the assumption of constant variances. The paper
by Quandt (1960) discusses testing the null hypothesis of constant co-
efficients against a more general alternative, where a structural change
has occurred at some unknown time and the error variance is also al-
lowed to change. Quandt considered a switching regression such that
the observations are thought, for theoretical reasons, to have been gen-
erated by two distinct regression regimes. Thus, for some subset of the

392 
Structural change, unit roots, and cointegration
observations
and for the complementary subset
yj=Xj(3j+u2j, 
(jeJ)
The essence of this simple formulation is that all the observations up
to the unknown time m come from one regime and all the observations
after that point come from the other. If we wish to test the hypothesis
that there is no switch in regimes against the alternative of one switch,
the appropriate likelihood ratio is
A = -ralog<72 + -(T - m) logaf - -Tlog<72
The estimate of the point at which the switch from one relationship to
another has occurred is then the value of m at which A attains its mini-
mum. However, implementation of this procedure has been hindered by
the lack of a distribution theory. Quandt noted on the basis of a Monte
Carlo experiment that a proposed chi-squared approximation to the sig-
nificance level of the likelihood ratio test is very poor. It was shown
empirically in Quandt that the x2 distribution is a poor approximation
to that of —2 log A.
Kim and Siegmund (1989) examined likelihood ratio tests to detect
a structural change in a simple linear regression when the alternative,
Hi, specifies that only the intercept changes and when the alternative,
#2, permits the intercept and the slope to change. They show the
asymptotic distributions of the maximum value of likelihood ratio, i.e.,
Quandt's LR statistic for the null of no structural change against two
different alternatives, which are functions of Wiener processes. They
tabulated the critical values by Monte Carlo methods.
Brown, Durbin, and Evans (1975) suggest the CUSUM test based on
recursive residuals. This test is for the general alternatives, including
the case of a single break. The CUSUM test involves considering the
plot of the quantity
t=k+i
where Wt is the recursive residual. Under Ho, probabilistic bounds for
the path of Wm can be determined and HQ is rejected if Wm crosses
the boundary (associated with the level of the test) for some m. This

13.3 Tests with unknown break points 
393
test is aimed mainly at detecting systematic movements of coefficients.
Against haphazard rather than systematic types of movements, Brown
et al. proposed the CUSUM of squares test, which uses the squared
recursive residuals and is based on a plot of the quantities
Sm = ^ ^ , 
S 2 = J2 < 
m = k + l,...,T
t=fc+l
The HQ is rejected if the path of Sm crossed a boundary determined by
the level of the test. These tests are of the goodness-of-fit type in the
sense that they seem applicable against a wide variety of alternatives.
Extensions of this test have been made by Ploberger, Kramer, and Alt
(1989) and Kramer, Ploberger, and Alt (1988) for models with lagged
dependent variables, and Kao and Ross (1992) for models with seri-
ally correlated disturbances. Ploberger and Kramer (1990) extend the
CUSUM test to OLS residuals and argue that it could as well be ap-
plied with OLS residuals and not just recursive residuals. A drawback
of the CUSUM tests is that they have asymptotically low power against
instability in the intercept but not the entire coefficient vector. Kramer,
Ploberger, and Alt (1988) show this using asymptotic local power and
Garbade (1977) shows this using simulations.
For the power problem of the CUSUM test, Ploberger, Kramer, and
Kontrus (1989) proposed the fluctuation test based on successive param-
eter estimates rather than on recursive residuals. A similar procedure
for single regression model has been suggested by Sen (1980) and the
fluctuation test was first suggested by Ploberger (1983).
Ploberger, Kramer, and Kontrus (1989) considered the varying pa-
rameter models and proposed the fluctuation test which is based on
rejecting the null hypothesis of parameter constancy whenever these es-
timates fluctuate too much. Their test statistic is
where
and || • ||oo denotes the maximum norm. They derived the limiting dis-
tribution of the test statistic and tabulated the critical values by Monte
Carlo methods. They also show that the fluctuation test has nontriv-
ial local power irrespective of the particular type of structural change.
Kramer, Ploberger, and Kontrus (1989) compare the fluctuation test

394 
Structural change, unit roots, and cointegration
with the CUSUM test and find that the former does better for many
alternatives. However, Kontrus and Ploberger's (1984) Monte Carlo
results show that neither the CUSUM test nor the fluctuation test dom-
inates the other in small samples.
Another class of tests is for continuous parameter variation as the
alternative. The tests in this category are those by Leybourne and
McCabe (1989), Nabeya and Tanaka (1988), and Nyblom (1989) who
consider tests for parameter constancy against nonstationary alterna-
tives (e.g., that the parameters follow a random walk or a martingale).
Nyblom (1989) developed the locally most powerful test against a param-
eter variation in the form of a martingale. The martingale specification
has an advantage of covering several types of departure from constancy:
for example, a single jump at an unknown time point (change point
model) or slow random variation (typically random walk specification of
parameters).
The problem of deriving the appropriate asymptotic distribution of
the test statistic in these problems involves hypothesis testing when the
nuisance parameter is present only under the alternative. This problem
was dealt with in Davies (1977, 1987) and more recently in Hansen
(1996).
Quandt's LR test and Kim and Siegmund's (1989) procedures can
be considered in the line of Max Chow test, since their procedure is
to find out the significance of the maximum value of the likelihood ra-
tio statistic from recursive switching models. Andrews (1993) derives
the asymptotic distribution of the LR-like test for one-time structural
change with an unknown change point (the Quandt test) as well as
analogous Wald (W) and Lagrange Multiplier (LM) tests. They are
for models with no deterministic or stochastic trends. Whereas, the
CUSUM test and the fluctuation test apply only to linear models, the
Andrews' tests apply to nonlinear models estimated by ML or GMM
methods. Andrews shows that his Sup F test (or Max Chow) has better
power properties than the CUSUM test and the fluctuation test (in the
context of a linear model). He provides asymptotic critical values for
1 percent, 2.5 percent, 5 percent, and 10 percent significance levels, for
his tests. (These are the critical values for the Sup F or Max Chow
tests.)
In the case of a simple AR(1) model Andrews' three tests are defined
as follows: consider

13.3 Tests with unknown break points
395
Under the null of no structural change we estimate this equation by OLS
and let it be the residuals.
The model with a one time break at m is
subsample 1: yt = piyt-i + £it, 
£ = 1,2,..., m
subsample 2: yt = p2yt-i + £2t, * = m + 1,..., T
Let in and e^t be the residuals from the OLS estimation of these equa-
tions. Define 5 = i!e,S\ = i'lii, and 52 = £2^2• Then "Sup" statistics
suggested by Andrews are:
Sup LM = max T
7T
Sup LR = max T
S — S\ — S2
Si + S2
where TT = m/T. It is customary to take ?r G [0.15,0.85], so that breaks
toward the ends are ruled out.
Diebold and Chen (1996) argue that the use of the asymptotic critical
values suggested by Andrews leads to size distortions and suggest a
better procedure generating small sample critical values using bootstrap
methods. This is similar to the conclusions in several other models (see
chapter 10) where critical values generated by bootstrap methods were
found to be better than those derived from asymptotic approximations.
Andrews and Ploberger (1994) develop tests with stronger optimality
properties than those in Andrews (1993). We shall define these test
statistics for the Wald test. The definitions for the LM and LR statistics
are similar and the asymptotic distributions are the same for the W, LM,
and LR statistics.
Let T\ < m < T2 be the range within which the structural break
occurs. Let W* be the Wald statistic for a break at t = m. Then
Sup W = max W*
T1<m<T2
The test statistics suggested by Andrews and Ploberger are
T2
Exp W = \n
Ave W =
T2-'
1
m=T1
w*
m=Ti

396 
Structural change, unit roots, and cointegration
Under a wide set of regularity conditions, Andrews (1993) and Andrews
and Ploberger (1994) show that the asymptotic distributions of the test
statistics are given by functions of the Wiener processes (these are the
same for the LM and LR test statistics)
Sup W => max Q(r)
7Ti<T<7T2
Exp W =» In 
/ 
exp(Q(r)/2)dr
Ave W = 
]_ f 2 Q(r)dr
where TTI = Ti/T,7r2 = T2/T, and
Q{T) = 
mr)-rW(lMW(r)-rW(l))
where W is a vector of k independent Wiener process (see section 3.2).
When r is known, this is simply X2(&)«
Hansen (1995) provides numerical approximations to these asymptotic
distributions and provides detailed tables whereby one can compute up-
values, rather than use significance tests. (Note that many regression
programs now present p-values.)
Andrews, Lee, and Ploberger (1996) present a detailed Monte Carlo
study comparing all these tests: the three Sup tests, three Exp tests,
and the three Ave tests.
Instead of the Max Chow test of Andrews, Hansen (1990) suggests the
Mean Chow test. Note that this is the Ave test discussed by Andrews
and Ploberger. The Mean Chow statistic is the average of the likelihood
ratios from the separate regressions over the subsamples 1,... ,ra and
m + 1,..., T. As mentioned above, test statistics of Quandt (1960) and
Kim and Siegmund (1989) are the Max Chow type test - the maximum of
the Chow sequence that was designed to detect a single abrupt structural
change over sample periods. Hansen's Mean Chow statistic is testing
for the null hypothesis of parameter constancy against more general
alternatives such as those in the fluctuation test and the locally most
powerful test. He suggested two statistics, one for stationary regressors
and the other for nonstationary regressors.
On the basis of a limited Monte Carlo experiment, Hansen claims
that the Mean Chow test performs better in regression models with both
trending and nontrending regressors in terms of size accuracy and power.

13.3 Tests with unknown break points 
397
Note that the Monte Carlo experiment reported in Andrews et at (1996)
is more extensive. Since this diagnostic test is based on the average value
rather than maximum value, it does not provide any guidance for the
identification of breakpoints. Hansen (1992) suggests the Max F, Mean
F, and Lc statistics. The last one is a test statistic for testing constancy
of the parameter against the random walk alternative. The Mean F
and Max F statistics are for the alternative of sudden breaks. The Lc
statistic is for the alternative of a gradual change. Hansen (1992) gives
tables for critical values of the Max F, Mean F, and Lc statistics.
Other papers that consider unknown breaks with nonstationary re-
gressors are: Banerjee, Lumsdaine, and Stock (1992), Zivot and Andrews
(1992), and Chu and White (1992). Banerjee et at consider recursive,
rolling, and sequential tests for unit roots and/or changing coefficients.
The recursive and rolling tests are based on changing subsamples of the
data. The sequential statistics are computed using the full sample par-
titioned into two groups sequentially. Chu and White consider a test
for a broken trend at an unknown point. One interesting result in their
paper is that the critical values for the asymptotic distribution for test-
ing structural change in nonstationary time series differ from those in a
stationary time series by a factor of y/3. (Note that the test statistics are
different.) Of all these tests, the tests by Hansen (1992) are the easiest
to apply.
In the Bayesian approach, the case of a single unknown break has
been considered by Holbert (1982) and Salazar (1982). The case of mul-
tiple unknown breaks has been discussed by Kim and Maddala (1991).
All these procedures are applicable for stationary regressors and also
nonstationary regressors.
A commonly used approach for multiple breaks is the Markov switch-
ing regression (MSR) model - also sometimes called the Hamilton model
(though the Hamiton model is a particular MSR model. This is discussed
in chapter 15.) There is the question of how many regimes to consider,
though most empirical work is on the two-state model. For example
the date of the second switch in the paper by Garcia and Perron (1995)
depended on whether the MSR model was considered as a two-state
model or a three-state model. The tests for three-state versus two-state,
two-state versus one-state (no structural break) are nonstandard tests.
Their distribution is discussed in Garcia and Perron, as well as Garcia
(1997).

398 
Structural change, unit roots, and cointegration
13.4 A summary assessment
As the previous survey indicates there is a large number of tests: tests
of no break versus single break, tests of no break versus multiple breaks.
One can raise the question of testing for one break versus two breaks
and so on. But the problem is not one of testing for one versus two
breaks, but one of estimation of the number of breaks. This is a model
selection problem as noted in Kim and Maddala (1991) for which they
use the BIC criterion. This problem has been analyzed further by Peter
Phillips using the PIC criterion (see chapter 8). The analysis of this
problem of multiple breaks from the classical point of view and the
asymptotic distributions are discussed in Bai and Perron (1995). In
tests for no break versus single break, note that rejecting the null does
not necessarily imply one-time structural change.
We not only need to know that breaks exist, but also the location of the
breaks. Several tests for breakpoints merely test whether a break exists
or not, but do not identify the location of the breaks. The CUSUM
tests, the Bayesian procedure used by Kim and Maddala (1991), the
MSR models, all identify the location of the break points. In the MSR
models, as found by Garcia and Perron (1996), the location of the switch
points depends on the number of states assumed.
There is a lot of work on testing with unknown switch points. In
practice, there is a lot of prior information and there is no reason why
we should not use it. For instance, suppose there is a drastic policy
change or some major event (for example, oil price shock) that occurred
at time to. It does make sense to ask the question of whether there was
a structural change around that period. It is not very meaningful to
search for a break over the entire period ignoring this prior information.
Thus criticism of Perron (1989) and the subsequent proliferation of the
literature on endogenizing the breakpoint are not totally jusitified. If a
search is conducted, it should be around the events.
Finally, there is the question of the relative merits of tests based on
residuals versus tests based on scores. Tests based on residuals like
simple CUSUM type tests have been found to be not appropriate for
tests concerning the stability of the entire coefficient vector. Hansen
(1992) shows that tests using the full score XiUi rather than just the
residual Ui generate test statistics with better power.

IS.5 Tests for unit roots under structural change 
399
13.5 Tests for unit roots under structural change
The exact definition of structural changes has not been given in the
literature. Usually it is interpreted as changes of regression parameters.
We shall use this conventional definition. Thus our focus is on finding
the effect of changes in regression parameters on the tests for unit roots.
Recent developments in unit root tests are concerned with the effect of
changes in the coefficients of the deterministic trends.
13.5.1 Single known break
Given a known structural break which is assumed to be given exoge-
nously, Perron (1989) has proposed a modified DF test for a unit root
in the noise function with three different types of deterministic trend
function. The time of a structural change is referred to as TB. First, the
crash model, model (A), allows for a one-time change in the intercept of
the trend function
DTt = n0 + inDUt + 6t
where
t>TB
r i if
\ 0 
ot]
DUt =
1 A otherwise
The changing growth model, model (B), allows for a change in the slope
of the trend function without any sudden change in the level at the time
of the break
DTt = /i + 60t + 6iDTt
t-TB 
if t > TB
DT =
0 
otherwise
And both effects are allowed in the model (C)
DTt = /xo + ViDUt + Sot -
t 
if t > TB
0 otherwise
The null hypothesis of a unit root is different, since the deterministic
trend function includes dummy variables {DUt,DTt,DTt). 
The alter-
native hypothesis is a broken-trend stationary system which also incor-
porates the same dummy variables.

400 
Structural change, unit roots, and cointegration
To assess the effects of the presence of a shift in the level or a shift in
the slope (at a single point of time) on tests for the presence of a unit
root, Perron (1989) first performed a Monte Carlo experiment. The
Monte Carlo results show that if the magnitude of the shift is signifi-
cant, one could hardly reject the unit root hypothesis, even if the series
is stationary with a broken trend and iid disturbances (thus, there is no
unit root in the noise term). Perron extended the Dickey-Puller test-
ing strategy to ensure a consistent testing procedure against shifting
trend functions, which is first to detrend the series and then analyze the
behavior of the estimated residuals. Let {y\} be the detrended series
according to either model (A), (B), or (C), where i = model (A), (B),
(C). Then {$} are the residuals from a regression of yt corresponding
to the models (A), (B), and (C).
Perron derived the limiting distributions of the normalized least
squares estimators p and their t-statistics from the following regression
y\ = p{yi-i + et
Those distributions are functions of functionals of Wiener processes and
the parameter A = TB/T, the ratio of the pre-break sample size to to-
tal sample size. For the exact forms of the distributions, see theorem
2 of Perron (1989). He tabulated the percentage points of the limit-
ing distributions for given values of A. When A is either 0 or 1, the
limiting distributions are identical over all models and the critical val-
ues are identical to those of Dickey and Puller. When A is not equal
to either 0 or 1 (thus, there is a structral break over the sample pe-
riod), the critical values under the various models are noticeably smaller
(greater in absolute value) than the standard Dickey-Fuller critical val-
ues. Perron applied the modified Dickey-Fuller test for the same US
macroeconomic series used by Nelson and Plosser (1982) and found the
quite strikingly different result that the unit root hypothesis can be re-
jected for all but three series: consumer price, velocity, and interest
rate.
Perron's procedure is a conditional test given a known break point.
This assumption of a known break date (treated as an exogenous event)
raised the problem of pre-testing and data-mining regarding the choice
of the break date. After Perron (1989), several methods have been devel-
oped for endogenizing the choice of a break point into testing procedures.
These procedures incorporate the estimation of a break point and use
recursive methods (using subsamples) or sequential methods (using full
sample with dummies).

13.5 Tests for unit roots under structural change 
401
13.5.2 Single unknown break
Perron's study was criticized on the grounds that he treated the date
of the break as known. Subsequent work used a variety of recursive
and sequential tests endogenizing the break point. The recursive test
statistics are computed over subsamples t = l,...,ra for m = rao,...,^
where mo is the start-up value and T is the size of the full sample. The
sequential test statistics are computed using the full sample, sequentially
incrementing the date of the hypothetical break (thus using different
dummy variables).
Zivot and Andrews (1992) use a sequential test, derive the asymptotic
distribution of the test statistic, mm;\£(A), and tabulate the critical
values. They fail to reject the unit root hypothesis for four of ten series
for which Perron rejected the unit root null (real per capita GNP, GNP
deflator, money stock, and real wages). With finite sample critical values
obtained by bootstrap methods (see chapter 10 for bootstrap methods)
they fail to reject the unit root null for three more series (employment,
nominal wages, and common stock prices).
Banerjee, Lumsdaine, and Stock (1992), to be referred to as BLS,
apply a variety of recursive and sequential tests endogenizing the break
point to international data. For instance, in the recursive tests they
consider the maximum DF statistic, the minimum DF statistic, and the
difference between the two. They derive the asymptotic distributions
of the recursive and sequential test statistics and tabulate the critical
values. Further results endogenizing the break points are also presented
in Perron (1994).
The summary picture one gets from these studies is that endogeniz-
ing the break point reverses the conclusions arrived at by Perron (1989).
However, these early results have been partially reversed by recent em-
pirical work. One is the study by Ben-David and Papell (1995) covering
a long time series which we shall discuss later (see section 13.8). The
other is the argument by Lumsdaine and Papell (1997) (to be referred to
as LP) and Ben-David, Lumsdaine, and Papell (1997) (to be referred to
as BLP) that the previous literature considered endogenizing only one
break point but do not consider the possibility of two break points.
LP follows the earlier study by BLS (1992) and consider the sequential
tests for two breaks with unknown break points. Following the methods
in BLS, they derive the asymptotic distributions of the test statistics.
Like Zivot and Andrews, they also use bootstrap methods to get finite
sample critical values. (The tables of critical values are available from

402 
Structural change, unit roots, and cointegration
the authors on request.) Applying these tests to the Nelson-Plosser data
LP find more evidence against the unit root hypothesis than Zivot and
Andrews but less than Perron. There is of course the question: what
about three breaks instead of two? Ultimately, determination of the
number of breaks is best viewed as a model selection problem.
BLP apply the procedure developed in LP to international data for
16 countries. They find evidence of two breaks in three quarters of the
countries and reject the unit root hypothesis in 50 percent more cases
than in models that consider only one break.
13.6 The Bayesian approach
Turning to the Bayesian work on structural breaks, there are two broad
avenues to view. One starts with Perron's work along classical lines, the
other can be viewed along the lines of Bayesian analysis of switching
regressions with unknown break points. We shall discuss these in turn.
13.6.1 Bayesian analysis of classical models
Zivot and Phillips (1994) start with Perron's model. They allow for a
one-time change in the level (constant) and a one-time change in the
slope of time. These occur at an unknown point t0 and they assume a
uniform prior for t0. However, for the autoregressive root p, they assume
Jeffreys' prior. To counteract the objection to this prior, that it places
increasing weight on extreme and unlikely values of p as far as economic
time series are concerned, they limit the effective range of p and hence
the degree of nonstationarity allowed a priori Finally, to evaluate the
evidence for the unit root model, when allowance is made for structural
change, they let q G [0,1] denote the prior probability for the model
with no structural change (NSC). Zivot and Phillips find that the trend
structural change model with to = 1929 is quite likely for nominal GNP
and nominal wages if the prior probability of NSC is not too large. Thus,
the Bayesian analysis allowing for an endogenous break supports some
of Perron's conclusions. As shown by Banerjee et al. (1992) and Zivot
and Andrews (1992) the asymptotic theory for testing for unit roots with
endogenous breaks is quite complicated and also as shown by Zivot and
Andrews (1992) and Perron (1990), the finite sample distributions of
the unit root test statistics can be very different from their asymptotic
counter parts. This factor might also account for the reversal under

13.6 The Bayesian approach 
403
Bayesian methods of the conclusions arrived at earlier on the effect of
endogenous breaks on unit root tests.
DeJong (1996) also starts with Perron's model. However, he considers
a third-order autoregression
yt = a0 + art + 0iDlt + f32D2t + ^ PiVt-i + et
2 = 1
where D\t and D2t are dummies for the breaks in the intercept and slope
coefficients. He assumes flat priors for the location of these breaks as
well as for pi,p2,p3. The parameter of interest is the dominant root
(note that a flat prior on pi does not imply a flat prior on the dominant
root). The marginal distribution of this dominant root \<max is obtained
through integration by Monte Carlo. DeJong calculates the odds in favor
a unit root: Xmax > 0.98 versus Amax < 0.98 . This avoids the problem
of testing a point null hypothesis which is tricky under Bayesian inference
(see chapter 8). DeJong concludes that, allowing for uncertainty about
the location of the break point, the data support trend stationarity.
13.6.2 Bayesian analysis of switching regression models
The switching regression approach for detecting a structural break is
as follows: consider the two-phase regression model with an unknown
break point ra,
y = X262+e2, 
t = m + l,...,T
We can rewrite the model in the standard form
y = W6 + e
where
ynl 
I" a 1 
r e
1 
0 = 
M 
P = 
*
0 
X 2 J ' 
L 02 J ' 
[ £2
£i and e2 are assumed to be independently and normally distributed
with a zero mean vector and unknown variance a2. Then the likelihood
function for ra, 6, and a2 is given by
L(61,62,a^m\y,W)<x(<T2rT/2 
expiry-We)'(y-W9)\

404 
Structural change, unit roots, and cointegration
We assume that the break point m is independent of 9 and a2, and m
is equally likely at any observation point between TQ and T — TQ where
To > k is the end point. These assumptions result in a uniform prior
over ra. We further assume a flat prior on 9 and cr2, and thus the joint
prior is given by
K{m,6,G2) oc 1/<T2
Combining the prior with the likelihood function, we have the joint
posterior density function
p(0, a2, m\y, W) oc (<72)-(T/2)+1 exp L - L { S + (0 - 0)'W'W(0 - 0)}1
Integrating out 9 and a2 yields the marginal posterior distribution func-
tion of a break point
p(m\y,W) oc
where \W'W\ = |X{Xi||X^X2| and S = Si + S2 with
Si = (yi - XiOiYfa - XA) and k = (X^)'1 
Xfa
The marginal posterior density function of the regression parameters is
obtained by
T-k
p(0\y,W) oc
where p(9\m, y, W) is the conditional marginal posterior density function
of 9 given m and it is given by a matrix t density
p(0\m, y, W)<x[I+(0- 
9)'H{9 - 0)]-("+2*-i)/2
where v = T - 2k + 1 and H = W'W/S.
The procedure outlined above has been used by Holbert (1982),
Salazar (1982), and Kim (1991). The procedure has also been checked
by performing Monte Carlo experiments. The Monte Carlo results re-
ported in chapter 3 of Kim (1991) showed that a break point can be
identified as the peak of the marginal posterior distribution of within
the sample period, if 0.1T < m < 0.9T, regardless of the stationarity of
regressors. The problem arises only if the break is at the beginning or
the end.

13.6 The Bayesian approach 
405
13.6.3 Multiple unknown breaks
We shall now discuss the case of multiple structural breaks. Suppose
that there are q multiple breaks, then
y =
Define the matrix W as
W =
Xi 0
0 X2
0 
...
... 
0
... 
0
0 Xq+1 _
and assume that e^i = l,...,g + 1 are independently and normally
distributed with a zero mean vector and unknown variance a2. Then
the expressions of posterior distributions derived in the previous section
remain the same. The only difference is that the posterior distribution
of m should be the joint discrete distribution of mi,..., mq which will
be given by
where \W'W\ =
a n d S = H%i Si w i t h
Si = {yi-XA)'{yi-XA) 
and §> = (XiXi^Xfc
To obtain the marginal posterior distribution of m^, we need to in-
tegrate out mi,..., mj_i, mj+i,..., mq. Analytical derivation is not
possible, so we have to use numerical methods. One possible way of
numerical calculation is to compute the probabilities in each grid of
mi,..., mq and sum it corresponding to rrij. This requires (T — 2TQ)9
iterations. Thus when T and q are large, it is impossible to use this
method for integration. Kim and Maddala (1991) use Gibbs sampling
to get the marginal distributions out of the joint distribution of break
points. Gibbs sampling is a Markovian updating scheme based on the
cycle of conditional distributions. It has been found to be very useful in
computing marginal densities in Bayesian inference. For an exposition
of Gibbs sampling, see Gelfand and Smith (1990).

406 
Structural change, unit roots, and cointegration
The problem of choosing between difference-stationary and trend-
stationary models has been often approached as a hypothesis testing
problem (that of testing for unit roots), but it should properly be ap-
proached as a model selection problem. This is done in McCulloch and
Tsay (1994) and Phillips (1995) as discussed earlier in chapter 8.
The model selection approach can also be used to select the number of
breaks. Kim and Maddala (1991) use the BIC criterion for selecting the
best model among the models with different numbers of breaks. Kashi-
wagi (1991) analyzed the same problem of determining the number of
breaks by the predictive likelihood approach. The method used by Kim
and Maddala (1991) is described in Maddala and Kim (1996a, 1996b)
and consists of the following steps:
(i) Given the maximum number of breaks, calculate the marginal
posterior distributions of 777-1, • • • > ™>q-
(ii) Find the peaks of the marginal posterior distributions of mi,...,
mqi say mi,...,mg.
(iii) Calculate the BIC corresponding to mi,..., mq
BIC = -21og(ML) + (log T) (number of parameters)
where the number of parameters is that (q + 1) x k (number of
regressors) since the q breaks correspond to the (#+1) subsamples.
Compute these values in the case of no break (q = 0), BIC0, one
break (q = 1), BIC1, ..., and so on, up to q,BICq.
(iv) Find the min{£JC\ BIC2,..., 
BICq}. 
The number of breaks
can be estimated as
q = SLVgmin{BIC\BIC2,..., 
BICq}
This procedure estimates the true number of breaks and location
of break points (mi,..., mq).
Kashiwagi (1991) proposes a method to evaluate the posterior dis-
tribution of the number of breaks and the posterior probability of each
break point by using the predictive log likelihood. Kashiwagi's approach
is not completely Bayesian. In his approach the probabilities of break
points and the number of breaks are computed using Bayes theorem, but
the likelihood function is evaluated using predictive likelihood, which can
be described as follows. Consider the predictive density of a future ob-
servation yf given data yd and parameter 6. In practice 6 is not known.
There are two solutions to this problem. One is to specify a prior density
for 9 and integrate it out to get the posterior density of yf. The other

13.7 A summary assessment of the empirical work 
407
is to replace 6 by a sufficient statistic. This is the predictive likelihood
approach.
For choosing the best model among alternatives under the different
assumptions of the number of breaks, the procedures of Kashiwagi (1991)
and Kim and Maddala (1991) are the same in that they choose the
model which has the highest penalized likelihood. Also both procedures
adopt numerical methods for computing the posterior distributions of
the break points - Kashiwagi uses an approximation method and Kim
and Maddala use Gibbs sampling. The main difference between the two
approaches is the way they penalize the likelihood function, Kashiwagi
penalized the likelihood function with the expected bias
- 2
where mi is the number of outliers and m<i = T—mi and ki is the number
of regression parameters. On the other hand, Kim and Maddala (1991)
penalized the likelihood function with
(log T) x (number of parameters)
The two methods can be viewed in the context of comparing the (classi-
cal) predictive likelihood approach and the Bayesian approach for select-
ing the best model among alternatives with different number of breaks.
Bianchi (1995) extends Kashiwagi's approach to the case of autore-
gressive errors. See Bianchi (1995, p. 45). This extension is useful
because the assumption of independent errors is not likely to be valid in
practice. Bianchi shows that the results are sensitive to this assumption.
13.7 A summary assessment of the empirical work
The empirical results in the preceding sections suggest that in many
areas allowing for structural change has changed conclusions about the
inference on unit roots. Here we shall review some more empirical re-
sults. Some Monte Carlo evidence on this issue is provided in Hendry
and Neale (1991) who argue that (i) regime shifts can mimic unit roots
in stationary autoregressive time series and (ii) such shifts may be very
hard to detect using conventional parameter constancy tests.
As mentioned earlier, Perron (1989) argued that, allowing for struc-
tural breaks, most macroeconomic variables are trend-stationary. Rap-
poport and Reichlin (1989) present similar evidence and Reichlin (1989)

408 
Structural change, unit roots, and cointegration
analyzes the systematic bias in tests for l(d) models with constant pa-
rameters against l(d — 1) models with time-varying parameters, and
shows that if the true process has deterministic structural change, we
will accept the unit root hypothesis against a model corresponding to
the true one but without the structural change; moreover, if the true
process is a random walk, a goodness of fit criterion will lead one to
accept the hypothesis of structural change against that of no change.
Perron's method of testing for unit roots allowing for trend breaks at
known break points has also been used by Corbae and Ouliaris (1991)
to test for long-run PPP for the Australian dollar. The conclusions are
similar, that allowing for structural breaks reverses conclusions about
unit roots. Perron (1990), with correction in Perron and Vogelsang
(1992), considers testing for a unit root in a time series characterized by
a structural change in the mean level (rather than in the trend). Again
the analysis is for a known break point, but it is shown that allowing
for a break reverses previous conclusions that the real interest rate for
the US is characterized by a unit root. The data Perron considers are
the US ex-post real interest rate over the period 1961:1-1986:111. Rose
(1988) analyzed related series and concluded that the real interest rate
was characterized by a unit root. Perron (1990) observes a marked
discontinuity around 1980. Before 1980:111 the average was close to zero
and it was close to 6 percent during 1980:IV-1986:III.
Garcia and Perron (1995) analyze this same problem of structural
breaks in real interest rates using the Markov switching model (discussed
in chapter 15). The break points are thus estimated, and not known
a priori. The sample used is monthly 1961:1-1986:12, though they put
more emphasis on using the quarterly version of this data set. The
results suggest that the ex-post real interest rate is essentially random
around a mean that is different for the periods 1961-1973, 1973-1980,
and 1980-1986. The variance is also different, being higher in both the
1973-1980 and 1980-1986 subperiods. Though the first break can be
identified as due to the oil price shock, the dating of the second break
is ambiguous. The two-regime model locates it at the end of 1979,
suggesting that the change in monetary policy is its origin, and the
three-regime model places it in the middle of 1981, a date more in line
with the federal budget deficit explanation. In any case, the unit root
hypothesis is rejected when allowance is made for structural breaks.
As mentioned earlier, the effect of endogenizing the break point in
Perron's (1989) study has been shown by Banerjee, Lumsdaine, and
Stock (1992), who use a variety of recursive and sequential tests, and

13.7 A summary assessment of the empirical work 
409
Zivot and Andrews (1992), who use sequential Dickey-Fuller tests to
reverse Perron's conclusions. However, Ben-David and Papell (1995)
replicate the sequential Dickey-Fuller tests used by Zivot and Andrews
(1992) and Banerjee et al. (1992) using a larger time span (130 and
120 years) and including more countries (sixteen) than is common in
unit root studies. The model used is an endogenous trend break model.
After determination of each country's break date, they find that they
can reject the unit root null in 20 of the 32 cases at a 10 percent sig-
nificance level (16 of these at the 1 percent level) compared with only
two rejections at the 5 percent level in the absence of a break (these
are for the US aggregate and per capita GDP). They also find that on
average post-break growth of aggregate GDP for the countries with sig-
nificant trend breaks was twice the pre-break growth. They conclude
that with longer time spans, incorporating trend breaks provides sig-
nificant evidence for stationarity. Two issues, emphasized by Campbell
and Perron (1991) guided the choice of their data: first, the power of
unit root tests is better with long spans of data. Second, lengthen-
ing the span of the data increases the possibility of major structural
changes. The papers by Lumsdaine and Papell (1997) and Ben-David,
Lumsdaine, and Papell (1997), as discussed earlier, argue that allowing
for the possibility of two endogenous break points, shows more evidence
against the unit root hypothesis than Zivot and Andrews (although less
than Perron).
In summary, the work on endogenizing the break point in Perron's
study (1989) seems to suggest that his conclusions are only partially
reversed, if at all. In the area of exchange rates, we have referred
to the study by Corbae and Ouliaris earlier. Perron and Vogelsang
(1992) show how allowing for level shifts affects conclusions about unit
roots in real exchange rates and thus conclusions about the purchas-
ing power parity. Dropsy (1996) performs several structural stability
tests to five foreign exchange rates relative to the dollar and five for-
eign exchange rates relative to the Deutsche Mark using quarterly data
over 20 years (starting in the first quarter of 1974). He thus misses
the important structural break of the switch to flexible exchange rates
in March 1973. He identifies several series with structural breaks thus
changing conclusions about unit roots and PPP. However, the dates
for structural breaks in his study vary a lot and need further study,
whether they can be identified with major events or policy changes.
In summary, structural breaks in foreign exchange rates need further
study. Several structural breaks also were identified earlier in the study

410 
Structural change, unit roots, and cointegration
by Kim and Maddala (1991) but they have not been investigated fur-
ther.
In conclusion, the evidence on unit roots in real exchange rates and
real interest rates has been found to be weaker than thought earlier,
once allowance is made for structural breaks.
13.8 Effect of structural change on cointegration tests
As noted earlier, the studies by Rappoport and Reichelin (1989), Hendry
and Neale (1991), and Perron (1989) among others, showed that infer-
ence on unit roots is affected by structural change (the unit root tests
tend to underreject the null of a unit root). The same is the case with
tests for cointegration. However, when considering cointegrated rela-
tionships one has to distinguish between breaks in the relationships
and breaks in the individual variables. In the latter case there is the
problem that the dates of the breaks in the different variables may not
coincide (see Hendry, 1996). We shall not, however, go into the lat-
ter issue because it is much more complicated than we wish to discuss
here. We shall review two studies investigating the effect of structural
breaks on tests for cointegration relationships in the single equation
context. In the next section we shall consider the case of cointegrated
systems.
The paper by Gregory, Nason, and Watt (1996) studies the sensitiv-
ity of the ADF test for cointegration in the presence of a single break.
Their Monte Carlo results show that the rejection frequency of the ADF
test decreases substantially. That is, in the presence of a break, you
tend to underreject the null of no cointegration. (The underrejection
is similar to the underrejection of the unit root null in the case of unit
root tests.) However, in this case, the underrejection of the null indi-
cate correctly that the constant parameter cointegration relation is not
appropriate.
Campos, Ericsson, and Hendry (1996) investigate the properties of
several cointegration tests when the marginal process of one of the vari-
ables is stationary with a structural break. They find that the break has
little effect on the test size (compared with the underrejection noted by
Gregory et al). However, tests based on ECM (see chapter 6, section 6.3)
are more powerful than the Engle-Granger two-step procedure employ-
ing the DF unit root test. These authors again emphasize the importance
of common factor restrictions discussed in section 6.3 of chapter 6.

13.9 Tests for structural change in cointegrated relationships 411
13.9 Tests for structural change in cointegrated relationships
There have been several studies deriving tests for structural change in
cointegration relationships. These fall into two categories. Those in sin-
gle equations and those in cointegrated systems (following the Johansen
procedure). Before we discuss these we shall consider a simple diagnostic
test for structural change.
13.9.1 A simple diagnostic test
Wright (1993) extends the CUSUM test (see section 13.3) to nonstation-
ary trended variables and to integrated variables. Hoa and Inder (1996)
extend the OLS-based CUSUM test discussed by Ploberger and Kramer
(1992) for nonstationary regressors and since the test does not explicitly
specify the nature of the alternative, they suggest its use as a diag-
nostic test for structural change. Instead of considering OLS residuals,
they consider the FM-OLS residuals and the estimated error variance
is replaced by the long-run variance estimate. Hoa and Inder derive
the asymptotic distribution of the FM-OLS-based CUSUM test statis-
tic, tabulate the critical values, and show that the test has nontrial local
power irrespective of the particular type of structural change. They
tabulate the asymptotic critical values for two models
Model 1 (Mi) 
yt = at + (3txt + ut
xt =xt-i +vt, 
t = 1,2,...,T
Model 2 (M2) 
yt = at + /3txt + ut
xt = // + xt-i +vt, 
t = 1,2,..., T
In Mi, Xt is 1(1) without drift. In M2, xt is 1(1) with drift. The asymp-
totic critical values are shown in table 13.1. The bootstrap-based small-
sample critical values can be computed and compared with these asymp-
totic critical values.
13.9.2 Single equation based tests
We shall discuss three papers related to structural change in cointe-
grated regressions. Quintos and Phillips (1993) develop tests for param-
eter constancy in cointegrated regressions. The alternative hypothesis
is that the coefficients follow a random walk. These types of tests were
considered earlier by Nabeya and Tanaka (1988), Leybourne and Mc-
Cabe (1989), and extended to tests for cointegration by Hansen (1992).

412
Structural change, unit roots, and cointegration
Table 13.1. Asymptotic critical values for the diagnostic test
No. of
Regressors
Mi
10%
M2
Significance Levels
5%
Mi 
M2
Mi
1%
M2
1.048
0.934
0.838
0.769
0.712
0.768
0.721
0.680
0.642
0.617
1.168
1.041
0.934
0.847
0.789
0.833
0.786
0.739
0.695
0.668
1.425
1.256
1.178
1.038
0.968
0.969
0.916
0.876
0.827
0.779
Source: Hoa and Inder (1996, tables 1 and 2).
Quintos and Phillips generalize these to tests for breaks in subvectors of
the cointegrated regression. They derive an LM test for the hypothesis
of cointegration versus the alternative of no cointegration (unlike the
usual tests for which the null hypothesis is of no cointegration). They
derive the (nonstandard) asymptotic distributions of the test statistics,
tabulate the critical values, and present an empirical illustration.
The paper by Gregory and Hansen (1996) can be viewed as com-
plementary to that of Hansen (1992) and Quintos and Phillips (1993).
They propose ADF, Za, and Z^-type tests designed to test the null of
no cointegration against the alternative of cointegration in the pres-
ence of a possible regime shift. The shift considered is a single break
in the intercept and/or slope coefficients in the cointegration relation-
ship at an unknown break point. They derive the asymptotic dis-
tributions and tabulate the critical values using simulation methods.
They also provide an empirical example of the money demand func-
tion in the US using annual data for the period 1901-1985 and quar-
terly data covering the period 1960:I-1990:IV. Apart from some detail,
for the annual data, on balance both the tests, ignoring the breaks
and allowing for the breaks, reject the null of no cointegration. For
the quarterly data, however, the conventional ADF test fails to reject
the null of no cointegration but allowing for shifts in both the inter-
cept and slope coefficients results in rejecting the null of no cointegra-
tion.
Gregory, Nason, and Watt (1996) investigate in detail tests for cointe-
gration following the testing procedures in Hansen (1992), in the context
of a quadratic adjustment cost model. The detailed conclusions are too
long to be discussed here.

13.9 Tests for structural change in cointegrated relationships 413
13.9.3 System 
tests
When it cames to analysis of structural change in CI systems, there are
two types of structural change to consider:
(i) A change in the number of CI vectors or the rank of CI space,
(ii) A change in the coefficients of a CI vector. In this case, since only
the CI space is determined by the Johansen procedure, one needs
to impose some normalization condition before one can devise
tests for structural change.
Tests for a change in the rank of the CI space
These tests have been considered by Hansen and Johansen (1993) and
Quintos (1995). Both start from the equation
Rot = a/3'Rlt + ut
defined in section 5.5.1 of chapter 5. This can be written as
Rot = ILRit + ut
Quintos separates the sample into different periods assuming the break
dates known. For instance, let there be one known break date and let
II and (IIi, II2) t>e the parameters for the whole sample and the split
samples. Her hypothesis is
Ho : Rank(IIi) = Rank(n2) = Rank(II)
She uses the trace test statistic and derives the LR test for Ho and its
distribution. (Details of the test statistic and its distribution can be
found in her papers.) Applying this to quarterly data on US govern-
ment revenues and expenditures for the period 1947-1992 and using the
fourth quarter of 1980 as the break date, she found the variables to be
cointegrated before the break but not after the break.
Hansen and Johansen (1993) do not assume the break point to be
known. They, instead, use recursive methods (which will enable the
break point to be located). It would be difficult to plot the recursive
coefficients. They, therefore, suggest looking at the larger roots Aj (in
the trace statistic) and derive tests for the significance of the differences
in the successive estimates of the A;.
When the break points are known, as in the case that Quintos con-
siders, one can always estimate the model in the separate regimes and
find the number of CI vectors in the different regimes. What she has is
a test for the significance of the observed differences.

414 
Structural change, unit roots, and cointegration
When the break point is not known, the recursive estimation method
in Hansen and Johansen would be useful in locating the break point.
This is in the spirit of the CUSUM tests.
Tests for changes in the coefficients of a given CI vector
The tests we considered in the previous section by Hansen (1992), Quin-
tos and Phillips (1993), etc. are for changes in the coefficients of a CI
vector. However, these are based on the FM-OLS (or a single equation)
estimation method. Tests for changes in the coefficient vector estimated
by the Johansen procedure (or a system method) have been discussed
by Seo (1995). He derives the tests and shows that the distributions are
nonstandard, but different from those found by Andrews and Ploberger
(1994). He also illustrates the methods with some empirical examples
on money demand functions and term structure of interest rates.
13.10 Miscellaneous other issues
13.10.1 Seasonal models
Smith and Otero (1995) examine the effects of structural breaks on the
HEGY test for seasonal integration (see chapter 12, section 12.3 for the
HEGY test). They consider an exogenous change in the seasonal pattern
and find that the distribution of the test statistic associated with the
HEGY test is more skewed in the presence of the structural break. In
their application to Columbian money supply and GDP as well as UK
transportation, the results show that inference about a unit and seasonal
roots is overturned. This is similar to the finding of Perron (1989) about
the effects of structural breaks on unit root tests.
Ghysels and Perron (1996) show, both theoretically and via Monte
Carlo studies that for linear models with a one-time structural break,
seasonal adjustment using the X-ll seasonal adjustment procedure re-
sults in smoothed data and disguises structural instability and decreases
the probability of detecting such a break. This result reinforces the con-
clusions of several studies (discussed in chapter 12) that analysis (in-
cluding tests for structural breaks) should be conducted with seasonally
unadjusted data.
13.10.2 Exact tests
Dufour and Kiviet (1996) discuss several finite sample tests of param-
eter constancy against the presence of structural change in first-order

13.10 Miscellaneous other issues 
415
dynamic models. They suggest a method for the derivation of an exact
confidence set for the autoregressive parameter using an expanded re-
gression and derive CUSUM, CUSUM of squares, and predictive tests
(see section 13.3). The details are too long to be presented here. The
methods depend on Monte Carlo simulations and their relationships with
the bootstrap methods discussed in chapter 10 needs to be explored.
They also use a union-intersection principle for combining several tests.
13.10.3 Spurious breaks
As we have seen in section 13.7, for the detection of multiple struc-
tural breaks, Maddala and Kim (1996a, 1996b) suggested Schwartz's
Bayesian information criterion (BIC) to estimate the number of breaks
based on the posterior distributions of break points. On the other hand,
Nunes, Kuan, and Newbold (1995) and Nunes, Newbold, and Kuan
(1996) (hereafter NNK) showed that the BIC failed to detect the true
number of breaks, but estimated the maximum permitted number of
breaks when the data generating process (DGP) is a random walk.
Maddala and Kim (1996a, 1996b) and NNK suggested using the BIC
fordetecting the number of structural breaks as follows (see section 3.7.1)
q = argming<qma
BIC* = log(^2) + [k + q(k + 1)]
where k is the number of regressors and qmax is a given maximum num-
ber of breaks. For cr2, Maddala and Kim estimated them by using the
posterior distributions of parameters, while NNK used the maximum
likelihood estimators. Yao (1988) and Maddala and Kim (1996a) show
that q is a consistent estimator of the true number of breaks, </o? provided
Qo < Qmax and errors are normally distributed.
To evaluate the performance of the BIC for detecting the number of
breaks, NNK considered two cases for the DGP: (i) Ayt = et and (ii)
yt = et where et ~ 7/iV(0,1). For the estimating models they used only
deterministic regressors as follows: (i) Xt = {1} (only constant) and
(ii) xt = {1,£} (constant and trend). NNK's Monte Carlo results show
that the BIC estimates q0 consistently when the DGP is a white noise.
When the DGP is a random walk, however, the BIC selects qmax in an
overwhelming majority of occasions.
The estimating models of NNK, where only constant and/or trend

416 
Structural change, unit roots, and cointegration
were used as regressors, belong to the trend-stationary processes (TSP)
class, while one of the true DGPs was a random walk which belongs to
the difference-stationary processes (DSP) class. Since it is well known
that the inference based on the model of the TSP class leads to spu-
rious conclusions when the true DGP belongs to the DSP class, Kim
(1997) argues that the correct estimating model should include yt-i as
a regressor.
In contrast to the results reported by NNK (1996), Kim (1997) shows
that the BIC for detecting the number of structural breaks estimates the
true number of breaks accurately provided that appropriate regressors
are used. When the DGP is a random walk with no break, however,
additional deterministic regressors in the estimating model significantly
lowers the probability of detecting the true number of breaks by the
BIC. These results are similar to the fact that the power of a test of the
unit root hypothesis against stationary alternatives decreases as addi-
tional deterministic regressors are included (Campbell and Perron, 1991,
Rule 5). On the other hand, when the DGP is a white noise with no
break, the BIC estimates the true number of breaks on most occasions
regardless of the maximum number of breaks and additional determin-
istic regressors. These results highlight the well-known fact that when
a true DGP belongs to the DSP class, inference based on the models of
the TSP class misleads to spurious conclusions.
13.11 Practical conclusions
We have presented an overview of:
(i) tests for structural change,
(ii) effects of structural change on tests for unit roots and on cointe-
gration tests,
(iii) tests for unit roots under structural change,
(iv) tests for structural change in cointegration relationships, and
(v) structural change in seasonal models.
As far as possible, both the classical and Bayesian viewpoints have been
presented.
The predominant conclusion that emerges from this voluminous work
is that structural change does affect inference on unit roots, on coin-
tegration, and seasonal integration, and it is important to allow for its
possibility at the estimation stage itself. Some pieces of advice can be
gleaned from all this work. These are:

13.11 Practical conclusions 
417
(i) Tests for structural change that merely test whether or not there
is structural change are not very useful. What is of importance
is the determination of the number and location of break points.
One can argue that the detection of the number and location of
break points can proceed after testing whether a break exists or
not. However, this line of investigation results in an unknown
pre-testing bias.
(ii) There is a problem with the consistent estimation of the break
point (which we have not discussed). The max statistics we dis-
cussed do not necessarily give a consistent estimate of the break
point. They merely test whether or not there is a break. The
problem of consistent estimation of the break point is dealt with
in the paper by Bai and Perron (1995).
(iii) There is no point in searching for a break over the entire time pe-
riod of observations. Some prior information always exists about
the dates of major shocks (real or financial) and this suggests the
approximate location of the breaks. Searching for breaks around
these dates is a valid procedure. The Bayesian procedure is more
flexible in taking account of this limited prior information rather
than the classical procedure.
(iv) The Bayesian approach to unit roots and structural breaks is
simpler and more straightforward than the classical approach.
But the choice of the prior is an important issue and is more
intricate in time series models (especially nonstationary models)
than in the usual regression models. Much work has been done in
this area but there is no concensus yet. Hence, some sensitivity
analysis with alternative priors should be presented. Given the
progress made in numerical methods, many more priors can now
be handled than was the case earlier.
(v) Since the switch from one regime to another is never sudden,
models involving gradual structural change should receive more
attention.
(vi) Given the importance of structural breaks on inference about
unit roots, cointegration, and seasonal integration, it is best to
start with models allowing for structural breaks and then sim-
plify them rather than test for structural breaks first and then
complicate them. This is consistent with the recent philosophy of
dynamic model building of going from general to specific models
rather than the reverse. The model selection approach seems to
be especially promising in this repect.

418 
Structural change, unit roots, and cointegration
(vii) More work needs to be done for a comparison of the different
methods of determining the number of structural breaks.
References
Andrews, D.W.K. (1993), "Tests for Parameter Instability and Struc-
tural Change with Unknown Change Point," Econometrica, 61,
821-856.
Andrews, D.W.K. and Fair, R.C. (1988), "Inference in Nonlinear
Econometric Models with Structural Change," Review of Eco-
nomic Studies, 55, 615-640.
Andrews, D.W.K. and W. Ploberger (1994), "Optimal Tests When a
Nuisance Parameter is Present Only under the Alternative,"
Econometrica, 62, 1383-1414.
Andrews, D.W.K., I. Lee, and W. Ploberger (1996), "Optimal Change-
point Tests for Normal Linear Regression," Journal of Econo-
metrics, 70, 9-38.
Bai, J. and P. Perron (1995), "Estimating and Testing Linear Mod-
els with Multiple Structural Changes," Manuscript, Dept. of
Economics, MIT.
Banerjee, A., R.L. Lumsdaine, and J.H. Stock (1992), "Recursive and
Sequential Tests of the Unit Root and Trend Break Hypotheses:
Theory and International Evidence," Journal of Business and
Economic Statistics, 10, 271-287.
Ben-David, D. and D.H. Papell (1995), "The Great Wars, the Great
Crash, and the Unit Root Hypothesis," Journal of Monetary
Economics, 36, 453-475.
Ben-David, D., R.L. Lumsdaine, and D.H. Papell (1997), "The Unit
Root Hypothesis in Long-Term Output: Evidence from Two
Structural Breaks in 16 Countries," Manuscript, University of
Houston.
Bianchi, M. (1995), "Time Series modeling in the Presence of Structure
Change," Ph.D. dissertation, Universite Catholique de Louvain.
Brown, R.L., J. Durbin, and J.M. Evans (1975), "Techniques for
Testing the Constancy of Regression Relationships over Time,"
Journal of the Royal Statistical Society, Series B, 37, 149-192.
Campbell, J.Y. and P. Perron (1991), "Pitfalls and Opportunities:
What Macroeconomists Should Know about Unit Roots," in
O.J. Blanchard and S. Fisher (eds.), NBER Macroeconomics
Annual, The MIT Press, Cambridge, MA, 141-201.

References 
419
Campos, J., N.R. Ericsson, and D.F. Hendry (1996), "Cointegration
Tests in the Presence of Structural Breaks," Journal of Econo-
metrics, 70, 187-220.
Cantrell, R.S., P.M. Burrows, and Q.H. Vuong (1991), "Interpretation
and Use of Generalized Chow Tests," International Economic
Review, 32, 725-741.
Chow, G.C. (1960), "Tests of Equality between Sets of Coefficients in
Two Linear Regressions," Econometrica, 28, 591-605.
Christiano, L.J. (1992), "Searching for a Break in GNP," Journal of
Business and Economic Statistics, 10, 237-250.
Chu, C.S. and H. White (1992),"A Direct Test for a Changing Trend,"
Journal of Business and Economic Statistics, 10, 289-299.
Corbae, D. and S. Ouliaris (1991), "A Test of Long-run Purchasing
Power Parity Allowing for Structural Breaks," The Economic
Record, 67, 26-33.
Davies, R.B. (1977), "Hypothesis Testing When a Nuisance Parameter
is Present Only Under the Alternative," Biometrika, 64, 247-
254.
(1987), "Hypothesis Testing When a Nuisance Parameter is Present
Only Under the Alternative," Biometrika, 74, 33-43.
DeJong, D. N. (1996), "A Bayesian Search for Structural Breaks in US
GNP," in T. Fomby (ed.), Advances in Econometrics: Bayesian
Methods Applied to Time Series Analysis, vol. 11, Part B, JAI
Press, Greenwich, Conn.
Diebold, F.X. and C. Chen (1996), "Testing Structural Stability with
Endogenous Breakpoint: A Size Comparison of Analytic and
Bootstrap Procedures," Journal of Econometrics, 70, 221-241.
Dropsy, V. (1996), "Real Exchange Rates and Structural Breaks," Ap-
plied Economics, 28, 209-219.
Dufour, J.M. (1982), "Recursive Stability Analysis of Linear Regression
Relationships," Journal of Econometrics, 19, 31-76.
Dufour, J.M. and J.F. Kiviet (1996), "Exact Tests for Structural
Change in First Order Dynamic Models," Journal of Econo-
metrics, 70, 39-68.
Dufour, J.M., E. Ghysels, and A. Hall (1994), "Generalized Predic-
tive Tests and Structured Change Analysis in Econometrics,"
International Economic Review, 35, 199-229.
Garbade, K. (1977), "Two Methods for Examining the Stability of Re-
gression Coefficients," Journal of the American Statistical As-
sociation, 72, 54-63.

420 
Structural change, unit roots, and cointegration
Garcia, R. (1997), "Asymptotic Null Distribution of the Likelihood Ra-
tio Test in Markov Switching Models," International Economic
Review, Forthcoming.
Garcia, R. and P. Perron (1995), "An Analysis of Real Interest Rate
under Regime Shifts," Review of Economics and Statistics, 78,
111-25.
Gelfand, A.E. and A.F.M. Smith (1990), "Sampling-based Approaches
to Calculating Marginal Densities," Journal of the American
Statistical Association, 85, 398-409.
Ghysels, E. and P. Perron (1996), "The Effects of Linear Filters on Dy-
namic Time Series with Structural Change," Journal of Econo-
metrics, 70, 69-98.
Gregory, A.W. and B.E. Hansen (1996), "Residual Based Tests for
Cointegration in Models with Regime Shifts," Journal of Econo-
metrics, 70, 99-126.
Gregory, A.W., J.M. Nason, and D. Watt (1996), "Testing for Struc-
tural Breaks in Cointegrated Relationships," Journal of Econo-
metrics, 71, 321-341.
Hansen, B.E. (1990), "Lagrange Multiplier Tests for Parameter Insta-
bility in nonlinear Models," Paper presented at the Sixth World
Congress of the Econometric Society, Barcelona, August 1990.
(1992), "Tests for Parameter Instability in Regressions with 1(1) Pro-
cesses," Journal of Business and Economic Statistics, 10, 321-
335.
(1995), "Approximate Asymptotic p-values for Structural Change
Tests," Manuscript, Boston College.
(1996), "Inference When a Nuisance Parameter is Not Identified Un-
der the Null Hypothesis," Econometrica, 64, 413-430.
Hansen, H. and S. Johansen (1993), "Recursive Estimation in Coin-
tegrated VAR Models," Institute of Mathematical Statistics,
Working paper No. 1, Universdity of Copenhagen.
Hendry, D.F. and A.J. Neale (1991), "A Monte Carlo Study of the Ef-
fects of Structural Breaks on Tests for Unit Roots," in P. Hackl
and A. Westlung (eds.), Economic Structural Change, Springer
Verlag, New York, 95-119.
Hoa and Inder (1996) "Diagnostic Test for Structural Change in Coin-
tegrated Regression Models," Economics Letters, 50, 179-187.
Holbert, D. (1982), "A Bayesian Analysis of a Switching Linear Model,"
Journal of Econometrics, 19, 77-87.

References 
421
Kao, C. and S.L. Ross (1992), "A CUSUM Test in the Linear Regression
Model with Serially Correlated Disturbances," Syracuse Univer-
sity, May 1992.
Kashiwagi, N. (1991), "Bayesian Detection of Structural Changes," An-
nals of the Institute of Statistical Mathematics, 43, 77-93.
Kim, H.J. and D. Siegmund (1989), "The Likelihood Ratio Test for
a Changepoint in Simple Linear Regression," Biometrika, 76,
409-423.
Kim, I.M. (1991), "Structural Change and Unit Roots," unpublished
Ph. D. dissertation, University of Florida.
(1997), "Detecting the Number of Structural Breaks," Economics
Letters, 57, 145-148.
Kim, I.M. and G.S. Maddala (1991), "Multiple Structural Breaks and
Unit Roots in Exchange Rates," Paper presented at the Econo-
metric Society Meeting at New Orleans, Dec. 1991.
Kontrus, K. and W. Ploberger (1984), "A Simulation Study of Some
Tests for Parameter Constancy in Linear Models," Paper given
at the Econometric Society European meeting, Madrid.
Kramer, W., W. Ploberger, and R. Alt (1988), "Testing for Structural
Change in Dynamic Models," Econometrica, 56, 1355-1369.
Kramer, W., W. Ploberger, and K. Kontrus (1989), "A Comparison
of Two Significance Tests for Structural Stability in the Linear
Regression Model," in B. Raj (ed.), Advances in Econometrics
and Modeling.
Kullback, S. and H.M. Rosenblatt (1957), "On the Analysis of Multiple
Regression in k Categories," Biometrika, 67-83.
Leybourne, S.J. and B.P.M. McCabe (1989), "On the Distribution of
Some Test Statistics for Coefficient Constancy," Biometrika, 76,
169-177.
Lo, A.W. and W.K. Newey (1985), "A Large Sample Chow Test for the
Linear Simultaneous Equation," Economics Letters, 18, 351-
353.
Lumsdaine, R.L. and D.H. Papell (1997), "Multiple Trend Breaks
and the Unit Root Hypothesis," The Review of Economics and
Statistics, 79, 212-218.
Maddala, G.S. and I.M. Kim (1996a), "Structural Change and Unit
Roots," Journal of Statistical Planning and Inference, 49, 73-
103.
(1996b), "Bayesian Detection of Structural Changes," in D.A.
Berry, K.M. Chaloner, and J.K. Geweke, Maddala, G.S. and

422 
Structural change, unit roots, and cointegration
LM. Kim (eds.), Bayesian Analysis in Statistics and Economet-
rics, Wiley, New York.
McCulloch, R.E. and R.S. Tsay (1994), "Bayesian Inference of Trend
and Difference Stationary," Econometric Theory, 10, 596-608.
Mizrach, B. (1992), "Mean Reversion in EMS Exchange Rates,"
Manuscript, University of Pennsylvania, paper presented at the
Econometric Society meetings, Seattle, June 1992.
Nabeya, S. and K. Tanaka (1988), "Asymptotic Theory of a Test for
the Constancy of Regression Coefficients against the Random
Walk Alternative," The Annals of Statistics, 16, 218-235.
Nelson, C.R. and C.I. Plosser (1982), "Trends and Random Walks in
Macroeconomic Time Series," Journal of Monetary Economics,
10, 139-162.
Nunes, L.C., CM. Kuan, and P. Newbold (1995), "Spurious break,"
Econometric Theory, 11, 736-749.
Nunes, L.C., P. Newbold, and CM. Kuan (1996), "Spurious Number
of Breaks," Economics Letters, 50, 175-178.
Nyblom, J. (1989), "Testing for the Constancy of Parameters over
Time," Journal of the American Statistical Association, 84, 223-
230.
Park, S.B. (1991), "The Wald and LM Tests for Structural Change
in a Linear Simultaneous Equations Model," Working paper,
Carleton University.
Perron, P. (1989), "The Great Crash, the Oil Price Shock and the Unit
Root Hypothesis," Econometrica, 57, 1361-1401.
(1990), "Testing for a Unit Root in a Time Series with a Changing
Mean," Journal of Business and Economic Statistics, 8, 153-
162.
(1994), "Further Evidence on Breaking Trend Functions in Macroe-
conomic Variables," Manuscript, Universite de Montreal.
Perron, P. and T.J. Vogelsang (1992), "Testing for a Unit Root in a
Time Series with a Changing Mean: Corrections and Exten-
sions," Journal of Business and Economic Statistics, 10, 467-
470.
Phillips, P.CB. (1995), "Bayesian Model Selection and Prediction with
Empirical Applications," with comments by F.C Palm and J.F.
Richard and Reply by P.CB. Phillips, Journal of Econometrics,
69, 289-365.
Ploberger, W. (1983), "Testing the Constancy of Parameters in Linear

References 
423
Models," Paper given at the Econometric European meeting,
Pisa.
Ploberger, W. and W. Kramer (1990), "The Local Power of the
CUSUM and CUSUM of Squares Tests," Econometric Theory,
6, 335-347.
(1992), "The CUSUM Test with OLS residuals," Econometrica, 60,
271-285.
Ploberger, W., W. Kramer, and R. Alt (1989), "A Modification of
the CUSUM test in the Linear Regression Model with Lagged
Dependent Variables," Empirical Economics, 14, 65-75.
Ploberger, W., W. Kramer, and K. Kontrus (1989), "A New Test for
Structural Stability in the Linear Regression Model," Journal
of Econometrics, 40, 307-318.
Quandt, R. (1960), "Tests of the Hypothesis that a Linear Regression
System Obeys Two Separate Regimes," Journal of the American
Statistical Association, 55, 324-330.
Quintos, C.E. (1995), "Sustainability of the Deficit Process with Struc-
tural Shifts," Journal of Business and Economic Statistics, 13,
409-417.
Quintos, C.E. and P.C.B. Phillips (1993), "Parameter Constancy in
Cointegrating Regressions," Empirical Economics, 18, 675-706.
Rao, C.R. (1952), Advanced Statistical Methods in Biometric Research,
John Wiley, New York.
(1965), Linear Statistical Inference and Its Applications, 2nd edn,
1973, John Wiley, New York.
Rappoport, P. and L. Reichlin (1989), "Segmented Trends and Nonsta-
tionary Time Series," Economic Journal, 99 (supplement), 168-
177.
Reichlin, L. (1989), "Structural Change and Unit Root Econometrics,"
Economics Letters, 31, 231-233.
Rose, A.K. (1988), "Is the Real Interest Rate Stable?" Journal of
Finance, 43, 1095-1112.
Salazar, D. (1982), "Structural Changes in Time Series Models," Jour-
nal of Econometrics, 19, 147-163.
Sen, P.K. (1980), "Asymptotic Theory of Some Tests for a Pos-
sible Change in the Regression Slope Occurring at an Un-
known Time Point," Zeitschrift fur Wahrscheinlichkeitstheorie
und Verwandte Gebiete, 52, 203-218.
Seo, B.S. (1995), "Tests for Structural Change in Cointegrated Sys-
tems," Manuscript, University of Rochester.

424 
Structural change, unit roots, and cointegration
Smith, J. and J. Otero (1995), "Structural Breaks and Seasonal Integra-
tion," Warwick economic research papers No. 435, University
of Warwick.
Stock, J.H. (1994), "Unit Roots, Structural Breaks and Trends," in
R.F. Engle and D.L. McFadden (eds.), Handbook of Economet-
rics, vol. IV, Elsevier, Amsterdam, 2740-2841.
Wright, J.H. (1993), "The CUSUM Test Based on Least Squares Resid-
uals in Regressions with Integrated Variables," Economics Let-
ters, 41, 353-8.
Yao, Y.C. (1988), "Estimating the Number of Change-Points via
Schwartz's Criterion," Statistics and Probability Letters, 6, 181-
189.
Zivot, E. and D.W.K. Andrews (1992), "Further Evidence on the Great
Crash, the Oil-Price Shock and the Unit Root Hypothesis,"
Journal of Business and Economic Statistics, 10, 251-287.
Zivot, E. and P.C.B. Phillips (1994), "A Bayesian Analysis of Trend
Determination in Economic Time Series," Econometric Reviews,
13, 291-336.

14
Outliers and unit roots
14.1 Introduction
In the preceding chapter we discussed the effects of structural breaks
on unit root tests and cointegration tests. The breaks considered were
breaks in trends, mostly one break and in a few cases multiple breaks.
In this chapter we shall discuss another related problem, that of outliers.
The structural breaks considered in the previous chapter correspond to
one type of outliers.
There is a large amount of literature on outliers in time series, but
this has had an impact on unit roots only in recent years. As Balke and
Fomby (1991a, 1994) noted, there is considerable evidence of different
types of outliers in macroeconomic time series. In this chapter we shall
consider the different types of outliers and also outlier-robust unit root
tests. Outlier-robust estimation methods for nonstationary time series
will also be discussed briefly. Much of this material is covered in Maddala
and Yin (1996) and Yin and Maddala (1996, 1997).
14.2 Different types of outliers in time series models
Outliers are aberrant observations that are away from the rest of the
data. They can be caused by recording errors or unusual events such as
changes in economic policies, wars, disasters, and so on, as for instance
those considered by Perron (1989). They are also likely to occur if the
errors have fat-tailed distributions (as is the case with financial time
series). Outliers can also be caused by misspecification of the estimated
relationships, e.g., linear instead of nonlinear relationships, omitted vari-
ables, and so on.
There is no question that outliers can cause problems with inference.
425

426 
Outliers and unit roots
The only question is what we should do with them. Legendre, in the first
paper on least squares in 1805, suggested throwing these observations
out. So did Edgeworth in 1887. However, if outliers are caused by
misspecification of the relationships estimated, then a proper course is
to change the specification and if they are caused by fat-tailed error
distributions, a proper course is to use robust methods. Thus, there are
three courses of action one can take (see Donald and Maddala, 1993,
p. 680):
(i) Throw the rascals out. (It will be argued in a later section that
although often suggested, this is not always a desirable course of
action in time series models.)
(ii) Leave them in, but under control (robust methods).
(iii) Change the model.
For (i) we need to identify the outliers. For the linear regression model,
there are many diagnostics to identify outliers. Most of these have been
built into the several standard computer programs. However, many of
them are not applicable for time series data.
In time series problems, because successive observations are corre-
lated, outliers can cause more problems for detection. Fox (1972) first
addressed outlier problems in time series by classifying outliers as ad-
ditive outliers (AO) and innovation outliers (10). An additive outlier
occurs when a particular observation is bad but subsequent observa-
tions in time series are not affected. An innovation outlier occurs when
the effect of a large innovation is perpetrated through the dynamics of
the model. Tsay (1988) extended this classification to allow for struc-
tural changes as well and considers transient changes (TC), level changes
(LC), and variance changes (VC). This classification will be followed
here.
Although the discussion of these outliers can be conducted in terms
of more general time series models, we shall define them in terms of
first-order autoregressive models. The definition of the different types
of outliers according to Tsay's classification is as follows: let yt be the
time series without any disturbances and f(t) denote the disturbance so
that the observed time series is zt = yt + fit). We shall consider three
models:
(Model A) No drift case: yt = Xt
(Model B) Drift case: 
yt = a + Xt 
(14.1)
(Model C) Trend case: 
yt = a + 8t + xt

14-2 Different types of outliers in time series models 
427
where
xt = pxt-i + et, 
et rsj 7iV(0, a2)
These are standard models for studying the properties of unit root tests.
The different outlier models in this simplified setup are defined as follows:
AO model: f(t) = wAO Ctro
10 model: 
±
LC model:
VC model: /(<) = 
^
where L is the lag operator, Wi,i = A0,I0,LC,VC, 
denote the mag-
nitudes of the respective disturbances, m denotes the location of the
outlier, and
if t = ra
* 
\ 0 otherwise
0 
t<m
\ et 
t>m
The following important points are worth noting regarding these
models:
(i) The behavior of the series zt in the 10 and VC models depends
on the AR structure of yt which, for simplicity, we have assumed
to be an AR(1) model. On the other hand, the behavior of zt
under the AO and LC models does not depend on the structure
of yt-
(ii) 10 affects the mth observation by wio and affects the subsequent
observations by pt~mwiO' Thus, its impact on the subsequent
observations decays at the rate of p. In the case of p = 1, the
impact is constant and permanent over time and thus the pattern
of the impact will coincide with that of LC. For the VC model
we have
Zt = pZt-i -T Wyc^t 
' et — Pzt-1 + et
where
et 
t <m
(1 + wyc)et 
t > m
In this case, the variance of the innovation driving up the series Zt
changes at time m from a2 to (l + wyc)2o~2. Thus the subsequent
ZfS are also affected.

428 
Outliers and unit roots
(iii) The sources of these disturbances can be different. AO can come
from recording errors, natural disasters, a bizzarre day on the
stock market, and so on. 10 normally represents the onset of an
external cause. Several factors can cause LC which represents a
changing mean of the series. This falls into the category of struc-
tural change discussed in the previous chapter. VC is another
example of structural change, but this time it is the variance of
innovations that changes. One example of this is the exchange
rate when it switches from the fixed exchange rate regime to the
flexible exchange rate regime.
(iv) The implications of these models on unit root testing are differ-
ent. Note that in the presence of these disturbances, both the
null and the alternative hypotheses should encompass these dis-
turbances which makes them different from the ordinary unit root
hypotheses. For example, to test the unit root hypotheses in the
model with drift against the alternative of trend stationarity in
the presence of LC, the null hypotheses now become a one-time
change in the drift of the random walk model while the alternative
becomes a changing mean in the trend-stationary model.
(v) Finally in following the literature on outliers and structural
breaks, it is important to bear in mind the distinction between
outliers in levels and in first-differences of the time series. In Per-
ron (1989) the terms AO and 10 refers to breaks in the trend
function (in his models B and C) because he considers outliers
(AO and 10) in first-differenced series. In Perron (1990) the
outliers refer to breaks in means (levels). In Balke and Fomby
(1991a) also, outliers corresponds to changes in trend whereas
in Hendry and Neale (1990), Franses and Haldrup (1994), Lucas
(1995a,b), and Yin and Maddala (1997) outliers affect the levels
of the series. When discussing the properties of unit root tests,
this distinction is important.
14.3 Effects of outliers on unit root tests
It has been noted in several papers (which will be reviewed in the fol-
lowing sections) that outliers lead to size distortions (overrejection or
underrejection of the null) in unit root tests. The type of distortion
depends on the nature of the outlier (AO, 10, etc.). Also, as mentioned
earlier, the studies fall into two categories: outliers in first-differences

14-3 Effects of outliers on unit root tests 
429
and outliers in levels. Outliers in first-differences refer to breaks in trends
as in Perron (1989, models B and C in his paper) whereas outliers in
levels refer to breaks in the mean as in Perron (1989, his model A) and
Perron (1990).
14'3.1 Outliers in first- differences
These are discussed in Perron (1989) and Balke and Fomby (1991a). Per-
ron (1989) first pointed out that standard unit root tests of the unit root
hypothesis against trend-stationary alternatives are biased toward the
nonrejection of the unit root hypothesis if the data generation process
is that of stationary fluctuations around a trend with a one-time break.
He suggested a two-step procedure. First, he detrended the series. Then
if yt are the detrended series, he considered the limiting distributions of
the normalized p and the associated ^-statistics for the three models he
considered, based on the regression
yt = pyt-i + et
(see the discussion of Perron's paper in chapter 13).
Balke and Fomby (1991a) is another example of outliers in first-
differences. They proposed a random-level, random-slope model which
encompasses deterministic trends, shifting and segmented trends, and
stochastic trends. While the primary purpose of this model is to em-
body both the hypotheses of small, frequent permanent shocks and large,
infrequent permanent shocks, this model also links the outliers in the
first-difference of the series to the different types of trend models. In
their model, the trend component Xt is given by
and
Gt = 0 + Gt-i
where Gt is the stochastic growth component, Sit and S2t are indepen-
dent with Su ~ N(fjLt, erf), i = 1,2. lit and ht are independent random
variables taking on values of 0 or 1 and are assumed to have an in-
dependent Bernoulli distribution with Pr(/^ — 1) — Pi, i — 1,2 and
0 < pi < 1. In their model the outliers or disturbances are modeled as
random because the original purpose is to model random shocks.
There are several special cases of this model that are of interest.
Assume, for simplicity that 6 — 0 and G_i = 0.

430 
Outliers and unit roots
Case 1 pi — 0 and p2 = 0. In this case we have Xt = Xo + f3t which is
a deterministic trend.
Case 2 pi = 1 and p2 = 0. In this case, we have Xt = Xo + /?£ +
]Ci=i -S'li- Thus Xt is a random walk with drift.
Case 3 pi = 0 and ^2 = 1. In this case, we get Xt = (3 + J2l=i $2,1-1
which means AXt is a random walk. Note that we get this only
by assuming 8 = 0 and G-\ = 0. If either one is nonzero, AXt
will be a random walk with a drift.
Case 4 pi > 0 and p2 > 0. In general, this will make Xt similar to
a shifting or segmented trend where the breaks in trend are
determined by Bernoulli draws.
Balke and Fomby (1991b) argue that the usual unit root tests will
break down in this framework. They showed that the Dickey-Fuller test
statistics will converge to one in probability and their asymptotic distri-
butions depend on piaf and thus the Dickey-Fuller tests are incapable of
distinguishing between small frequent and large infrequent shocks. This
finding is similar to those in Perron (1989, 1990), Rappoport and Reich-
lin (1989), and Hendry and Neale (1990) who also showed that standard
Dickey-Fuller tests have problems in distinguishing between a shifting
or segmenting deterministic trend and a stochastic trend.
U.S.2 Outliers in levels
One of the models considered in Perron (1989) is that of a break in the
mean. This model has been analyzed in Perron (1990). These results
by Perron for a break in mean have been extended in several directions
and detailed critical values have been tabulated for the different cases.
(We shall not go into the details of these tables here.) The extensions
are as follows
(i) Perron and Vogelsang (1992b) make some corrections to the dis-
tributions of unit root tests for the AO model in Perron (1990).
Note that this model referred to nontrending data.
(ii) Perron and Vogelsang (1992a) extend the paper of Perron (1990)
to the case of the unknown break point, and present detailed
critical values.
(iii) Perron and Vogelsang (1993) make corrections to the distribu-
tions for the AO model in Perron (1989) with trending data.
They also present extensions to the case of the unknown break

14-3 Effects of outliers on unit root tests 
431
point and provide critical values for several models that could be
useful in applications.
(iv) Vogelsang and Perron (1994) provide further extensions to the
unknown break point case. Unlike the previous studies, where
the break is considered only under the alternative of trend sta-
tionarity, they derive the limiting distributions of statistics in
both the AO and 10 frameworks where a shift is permitted under
the null hypothesis of a unit root. They show that the limiting
distributions are invariant to a mean shift but not to a change in
slope. The paper provides several tables of critical values for the
different tests.
(v) Finally, Vogelsang (1994) examines different procedures for deal-
ing with additive outliers in unit root tests.
One important issue in all this work is how to actually apply unit root
tests in the presence of outliers. There are two possible approaches in
the case of additive outliers. The first is to formally test for the presence
and location of outliers and then to perform a modified DF unit root
test. There are two ways of modifying the DF regression. One is to use
dummy variables to the DF regression to account for outliers. This is the
procedure followed by Franses and Haldrup (1994). Vogelsang (1994) ar-
gues that this is the preferred approach. The other method involves first
removing the outliers from the data, treating the corresponding obser-
vations as missing, and then estimating the DF regressions. Vogelsang
(1994) shows that this leads to misleading inferences and this procedure
should not be used.
The second approach to dealing with additive outliers is to use unit
root tests that are robust to MA errors with a root close to unity (which
is a consequence of additive outliers). These modified tests suggested by
Perron and Ng (1996) have been discussed in chapter 4 (section 4.3.4).
Franses and Haldrup (1994) addressed the problem of unit root tests
in the presence of AO by using a probabilistic setup. They showed that
there is overrejection of the null hypothesis using standard DF tests in
the presence of additive outliers. Yin and Maddala (1997) adopted a
similar approach and studied the effect of different types of disturbances
on unit root tests. They considered not only additive outliers but also
innovation outliers, level changes, and variance changes. Lucas (1995a)
also looked at the problems of standard DF tests from the point of view
of robust estimation and also showed that standard DF tests in the

432 
Outliers and unit roots
presence of AO and 10 suffer from size distortions. In all these papers,
outliers are defined in levels of the series.
14*3.3 The impact of disturbances
To focus on the impact of the presence of disturbances, we assume that
the error term is iid. Consider the DF regression
zt = Dt + pzt-i+ut 
(14.3)
where Dt is the deterministic trend and we shall consider three different
cases:
(i) Model A: Dt = 0,
(ii) Model B: Dt = a,
(iii) Model C: Dt = a + St.
AO model
Because the impact of finite AOs will go to zero as sample size goes
to infinity, it is necessary to use a probabilistic model to study the AO
model. Franses and Haldrup (1994) used the following probabilistic
setup to study the large-sample theory for model A
zt = Vt + 6dt
where dt is a Bernoulli variable taking the value of 1 or —1 with a
probability TT/2 and 6 is the magnitude of AO. This is similar to the
measurement error models except that a Bernoulli variable instead of a
normal variable controls the contamination process.
Using this setup, Franses and Haldrup showed that the asymptotic dis-
tributions of both the Dickey-Fuller K-test statistics and t-test statistics
in model A shift to the left in the presence of AO. Thus by using the
usual critical values, one tends to overreject the null hypothesis of unit
root in the presence of AO. Their results can be readily extended for
models B and C as well. Yin and Maddala (1997) adopt their setup but
use a different method to derive the asymptotic distirbutions. We shall
present their results here (they will be referred to as YM).
Let yt be generated by model A to model C as defined in (14.1) and
let the observed zt be generated by

1J/..3 Effects of outliers on unit root tests 
433
where dt is iid Bernoulli with
{
1 
with probability TT/2
0 
with probability 1 — TT
—1 with probability n/2
Note that in this case the us in the DF regression (14.3) are no longer
iid. They follow an MA(1) process
=vt + 0vt-\
with 70 = var(ut) = o~2 + 2-KW2
AO and A2 = a2(l + 8)2 = a2. It is well
known that the asymptotic distribution of the DF-test statistics has an
extra term in the numerator
5 ('"?) — (=f)'
Thus the asymptotic distributions of the DF-test statistics are
JW*(r)dW(r)-ir(wAO/a)2
JW*(r)2dr
fW*(r)dW(r)-ir(wAO/<T)2
where W(r) is the standard Wiener process and
W(r) 
for model A
Wjr) 
for model B 
(14.4)
W(r) 
for model C
W{r) = W(r)- 
fw{r)dr
W{r) = 
W(r) - 4 | / W(r)dr - - f rW(r)d
+6r [ / W(r)dr -2 f rW(r)d
The extra term in the numerator of the asymptotic distributions are
due to the MA structure of the error terms, and they are the same for
all three models A, B, and C. These terms are positive and cause the
asymptotic distributions to shift to the left, and this results in overrejec-
tion if the standard critical values are used. The extra terms depend on
the probability of occurance of AO, TT, the magnitude of AO, w2
AO, and

434 
Outliers and unit roots
the variance of the innovation et, cr2. One thing to note is that only the
product of 7T and (WAO/&)2 matters. In other words, large shocks with
small chance of occurence have the same effect as that of small shocks
with high probability of occurence. Balke and Fomby (1991b) pointed
out this fact and argue that the DF tests cannot distinguish infrequent
large shocks from frequent small shocks. Note that a positive AO and
a negative AO with the same magnitute have the same effect on the
limiting distributions which depend on (WAO/&)2, not {WAO/&)-
From the derivation we know that the problem is caused by the MA
structure of the error terms in the DF regression, a certain departure
from the iid case where the DF tests are appropriate. This might sug-
gest the use of more general tests like the ADF test and/or PP tests to
handle the serial correlation in errors. However, the tests discussed in
Perron and Ng (1996) would be more appropriate, as mentioned earlier.
The IO and LC models
Under the null hypothesis, the impact of IO will be the same as that
of LC. Thus, in testing for a unit root, the test statistics derived from
these two models have the same limiting distributions under the null
hypothesis. This is why they are discussed together in the same place.
However, the test statistics have different power functions. We shall
discuss models B and C only, since model A is meaningless under this
circumstance.
Perron (1989, 1990) and Perron and Vogelsang (1992a) have studied
these cases in detail. But there are some differences in their models
and those considered by YM. Perron and Vogelsang consider outliers
in the first-difference while YM, following the literature, consider the
traditional definitions of IO and LC. Although the results are similar,
one should notice that the models are different.
First, YM get the limiting distribution of the DF tests. Let yt be
generated by equation (14.1) and (14.2) for models B and C, respectively,
and let the observed zt be generated by
yt 
for t < m
yt + w 
for t > m
where m is the date of break. It is relatively easy to show that both the
limiting distributions of the DF, K-, and t-statistics will not change in
this case under the null hypothesis. The key feature is that the partial
sum process has a higher convergence rate than the extra constant term

11^.3 Effects of outliers on unit root tests 
435
w after time m. Because the proof is quite lengthy and tedious, we leave
it out here.
A question arises as to why one should worry about the existence of
10 and LC in the model if they do not cause any trouble in the sense
that the limiting distributions do not change even if they exist in the
model. There are two reasons. The first reason is the problem of small
sample size distortions. Since the DF tests suffer from small sample size
distortions, in empirical work researchers normally use those critical
values adjusted for small sample biases. But those critical values are
normally derived via simulations based on the uncontaminated models.
Apparently in small samples the test statistics are definitely different
from the usual ones. So, even if researchers use those adjusted critical
values, small sample size distortions are still expected to exist in the
presence of 10 and LC.
Secondly, Perron (1989) has shown that under the alternative hypoth-
esis he specified, p would be inflated toward 1. This is also true for the
LC model. It can be shown that for the LC model we have
w2\(l 
- A)
P 
* 7o + w2A(l - A)
for both models B and C under the alternative, where 70 is the variance
of zt, w is the magnitude of LC, and A is a fixed constant defined as
m = AT. YM use a fixed A to make sure that both before-break sample
and after-break sample grow at the same rate as the full sample. Thus
under the alternative p is not a consistent estimator and it will over-
estimate the true parameter. This will make the low power problem of
the unit root tests worse.
Surprisingly, this result does not hold for the 10 model. Using the
same procedure, it can be shown that p is still consistent under the
alternative for the 10 model. So there will be no corresponding power
loss for the 10 model.
These results on the behavior of the DF test statistics in the presence
of 10 and LC can be summarized as follows: both statistics converge
to the usual DF distributions under the null, while they normally have
extra small sample biases caused by the contamination. These biases
persist even if one uses the small sample critical values generated via
simulations of the uncontaminated models. However, under the alterna-
tive, the test statistics in 10 and LC models have different properties.
In the LC model the estimate is bised toward 1 and goes to 1 when the
magnitude of the level change becomes extremely large relative to the

436 
Outliers and unit roots
variance of the innovations. Thus there is a tendency to underreject the
unit root hypothesis for the LC model. On the other hand, for the 10
model, the estimate of the autoregressive parameter converges to its true
value asymptotically. Nevertheless in small samples, it may behave quite
differently from that in the uncontaminated model. YM confirm these
results using some simulations to show the small sample size distortions
and power functions.
The VC model
In the presence of VC, the model reduces to
Zt=Vt + T^—f wvceT 
(14.5)
1 — pL
with e™ defined after equation (14.2). YM investigate the behavior of
the DF-test statistics in the presence of VC. Let yt be generated by
model A to model C in (14.1) and let the observed zt be generated by
(14.5). They then derived the following
T(ii-r\ =*
JW*(r)dW*(r)
where
(
W*{r) 
i = model A
W* (r) - JW* (r)dr 
i = model B
W* (r) - 4 [/ W* (r)dr - § / rW* (r)dr]
+6r [J W*{r)dr - 2 / rW*(r)dr] 
i = model C
(14.6)
and
W*(r) = {(7W{r) 
T<X
K } 
\ axW(r) 
r>\
a\ = var((l + wVc)et) = (1 + wVc)2o-2
Note that the difference between the above distributions and the stan-
dard DF distributions is that instead of W(r) we have W*(r) defined
in (14.6). This is simply because the variance of the innovation driving
up the series changes at time m. By modifying those derivations using
the functional central limit theorem, these will give an integral from 0
to A with the initial variance a2 and an integral from A to 1 with the

14-4 Outlier detection 
437
changed variance a2. Thus by defining W*(r) one can easily derive the
above limiting distributions.
The implications of these limiting distributions are not obvious to
observe. One cannot tell immediately whether the limiting distributions
shift to the left or to the right. One obvious thing is that these limiting
distributions are not the same as the standard DF distributions unless
a2 = a2, i.e., there is no variance change. Another observation is that the
date when the variance changes matters here. Different values of A result
in different W*(r) and thus different limiting distributions. Although
the shift of the distributions are not clear, the shift depends on A and
fc = 
a2/a2.
One can tackle this problem by computing the distributions using
some numerical methods for given A. Yin and Maddala (1997) investigate
this through a Monte Carlo simulation.
The presence of the VC results in the most complicated situation. In
the presence of the AO, there is a tendency for overrejection of the unit
root null, while there is a tendency of underrejection in the presence of
the LC. But for the VC case, whether there is a tendency for underre-
jection or overrejection of unit root hypothesis depends on the values of
A and k.
14.4 Outlier detection
In the preceding section we noted that outliers produce misleading in-
ferences in the application of unit root tests and that the nature of the
distributions depends on the type of the outlier.
There are two solutions often suggested for handling outliers. The first
is to detect the outliers and discard them (throw the rascals out). The
other is to use robust methods. We shall discuss these two solutions in
turn. For the use of the robust methods, detection of the outliers is not
necessary although, as will be noted later, robust estimation methods
can also help the detection of outliers.
In the regression context, the procedures used for the detection of
outliers fall into the categories of detection methods and likelihood dis-
placement methods. See Cook (1987) and, for a critique, Donald and
Maddala (1993).
Fox (1972) first proposed distinguishing between two types of outliers
in time series: AO and 10. He also proposed two parametric models for
these categories and suggested likelihood ratio tests for outliers when
the location and type of the outlier is known. Abraham and Yatawara

438 
Outliers and unit roots
(1988) propose a score test for identifying the location of an outlier and a
procedure for distinguishing between AO and 10, assuming that the lo-
cation of the outlier is known. On the other hand, Abraham and Chung
(1989) and Bruce and Martin (1989) discuss the use of deletion diagnos-
tics to detect outliers in time series. The deletion diagnotics can refer
to the effect on the estimates of the regression coefficients or the error
variance. Bruce and Martin found that in the case of time series mod-
els, deletion diagnostics based on innovation variance a2 give a clearer
indication of an outlier than diagnostics based on estimates of the regres-
sion parameters. Ledolter (1990) discusses outlier diagnostics based on
the likelihood displacement method in time series models. Ljung (1993)
contains a detailed discussion of a comparison between likelihood-based
methods and deletion methods, although the discussion is in the context
of only additive outliers in ARM A models.
Atkinson, Koopman, and Shephard (1997) also use deletion methods
to detect outliers and breaks in time series. Their model is based on mea-
suring the influence of deletion measures. In the usual regression model,
an outlier can be detected by the effect of its deletion on the residual
sum of squares. An equivalent procedure is to add an extra variable
to the mean of the deleted observation (dummy variable method), also
known as intervention analysis. Atkinson et al. argue that such inter-
vention analysis can also be conducted in unobserved components and
structural time series models. However, these models contain more than
one variance, so the effect of intervention has to be measured by the
change in the individual variances. The procedures for doing this are
rather complicated to be reviewed here, but Atkinson et al demonstrate
that this can be done by the use of score statistics and recent develop-
ments in smoothing snd filtering. They also provide four examples that
illustrate the fragility of inferences about outliers and level breaks.
As for methods suggested for distinguishing between different types
of outliers, there are many studies. But many of these are valid under
very restrictive assumptions. For instance the procedure in Abraham
and Yatawara (1988) is valid for only one outlier with known location.
Fiebig and Maasoumi (1990) suggest nesting both AO and 10 in a com-
prehensive model
yt = ayt-i + /3fxt + faz^ + <M
where z^ 
= 1 if £ = ra and z[ 
= 0 otherwise (ra denotes the location
of the outlier). For the AO model fa = — ce^i, but the 10 model requires
02=0.

144 Outlier detection 
439
The main drawback of the Fox method is that it is designed for the
case of a single outlier. This procedure was extended for several outliers
and studied later in full length by several researchers. Tsay (1988) com-
bined some previous work on this issue and proposed a unified model to
deal with different types of outliers and structural breaks. He considered
not only AO and 10 but also TC, LC, and VC, and put them into a uni-
fied model. This procedure was later modified by Chen and Liu(1993)
to take care of bogus detected outliers, and by Balke (1993) to better
distinguish IOs and LCs. The procedure described by Tsay (1988) con-
sists of the following steps: specifying and estimating an initial ARM A
model, detecting outliers based on some prespecified criteria, discarding
the outliers, and then re-specifying and re-estimating the ARMA model.
Balke (1993) argues that Tsay's method does not always perform satis-
factorily if level shifts are present. He suggests a simple modification
of Tsay's procedure which consists of two steps: (i) estimating an ini-
tial ARMA (0,0) or white noise model in addition to the ARMA model
as Tsay suggests and (ii) combining the results from the two outlier
searches to form a single intervention model.
There is an important problem with the procedure by Tsay noted by
Balke and Fomby (1991a, p. 73) and Balke (1993). The estimation of
the parameter from models based on the data with outliers left in, may
produce a bias in the parameter estimates and thus affect the efficiency
of outlier detection. The paper by Chen and Liu (1993) addresses this
problem.
In addition to these classical approaches there are Bayesian approaches
as well. Some references in this area are West (1986), West and Harrison
(1986), and Durbin and Cordero (1994).
Another avenue is to use a robust estimation procedure at the first
stage in outlier detection. The idea is, as suggested in Franses and Lucas
(1995), that a useful by-product of robust estimators is that weights are
obtained for the individual observations and these weights can be used
to detect outliers and structural breaks. A low weight indicates that the
observation does not correspond to the rest of the data. However, the
main idea of outlier detection is to throw them out. The main idea of
robust estimation is to leave them in. But the procedures of starting
outlier detection on the basis of models estimated with outliers left in as
in Tsay and Balke produces biased estimates and can lead to spurious
outliers. This is precisely the reason for all the deletion diagnostics
and methods suggested in regression models (Cook and Weisberg, 1982,
Donald and Maddala, 1993).

440 
Outliers and unit roots
14.5 Robust unit root tests
In the previous section we presented a brief review of outlier detection
methods. The purpose of these methods is to throw the outliers out.
Another alternative is to use robust methods. In the area of robust
estimation, Huber (1964, 1973) introduced the class of M estimators.
For the linear regression model
yt = x'tf3 + ut
An M estimator is the minimand of
p £ 
in.
where p(-) is a function denned on 5R and a2 is the variance of Uf. For
p(u) = u2 one obtains the OLS estimator. The first-order condition for
the minimization problem is
where I/J(U) = dp(u)/du. Two commonly used functions for I/J(') are the
Huber tj) function
ip(u) = min(c, max(—c, u))
and the bi-square ip function
where c is a tuning constant and 1 A is an indicator function of the set
A.
One would expect that the M estimators would be applicable in the
case of time series to take care of outliers as well. But Martin and Yohai
(1986) pointed out that the performance of M estimators in the presence
of outliers, especially AO, is not satisfactory. This is because in ARM A
models, contaminated yts are also included in explanatory variables.
Therefore Martin and Yohai (1986) proposed to use generalized M (GM)
estimators. In an AR(1) model
yt = pyt-i +ut
a GM estimator is a solution of
E
fyt-pyt-i 
\ 
n
v I 
,yt-i 1 yt-i = o

14-5 Robust unit root tests 
441
where rj(-, •) is some robustifying function. A typical example of 7/(-, •)
as suggested by Martin and Yohai is
with ip being the Huber -0 function or the bisquare xp function. Martin
and Yohai showed that the GM estimator can handle both AO and 10
quite successfully in the AR(1) model. The problem with the GM esti-
mator is its performance when the order of the AR structure increases.
In the area of robust estimations, high breakdown point (HBP) estima-
tors have been widely suggested over the last decade. These estimators
are concerned with the concept of the breakdown point of an estima-
tor. The breakdown point of an estimator measures the largest possible
proportion of outliers in the data set an estimator can tolerate before
it collapses to some nonsensical value. Because the performance of the
M estimator is not always satisfactory, HBP estimators have been intro-
duced into time series anlysis as well. For a brief discussion see Lucas
(1995a), who presents the class of S estimators introduced by Rousseeuw
and Yohai (1984) and the MM estimator introduced by Yohai (1987).
The S estimator can be calculated as follows: suppose that for a given
/?, a scalar estimate crs(/3) > 0 is the solution of
where #i(-) is some bounded function. The S estimator of (3 is the value
which minimizes as {(3). It has been shown that the S estimator is a HBP
estimator compared with Huber's M estimator. The MM estimator is
defined as follows: let a MM be as ((3) evaluated at the S estimator of (3.
The MM estimator of (3 is the one which minimizes
Vt - x'tf3
&MM
with respect to (3. The function #i(-) and ^(*) have to satisfy certain
conditions stated in Yohai (1987). In Lucas (1995a), for his purpose of
estimating the AR(1) model, he used the following functions
')/6 
for lul < a 
. 
_
with c\ = 1.547 and c<i = 4.685. Note that the derivative of 8i(u) gives
the bisquare X/J function with different tuning constant.
The preceding discussion is concerned with estimation problems. The

442 
Outliers and unit roots
effects of outliers on forecasts in time series models have been discussed
in Ledolter (1991). Turning to the robust unit root tests, the tests that
have been proposed are:
(i) Lucas (1995a): based on the MM estimator proposed by Yohai
(1987),
(ii) Lucas (1995b): based on M estimators,
(iii) Herce (1994, 1996): based on LAD estimators,
(iv) Rothenberg and Stock (1997): based on nonnormal likelihoods,
(v) Hoek et al. (1995): based on student t-likelihood rather than
Gaussian likelihood (MLT estimation),
(vi) Hasan and Koenker (1997): based on ranks.
We shall discuss these in turn.
Lucas (1995a)
In this paper Lucas develops outlier-robust unit root tests, using the MM
estimator proposed by Yohai (1987). Lucas first established the asymp-
totic distribution for the t-test statistic, tp based on the M estimator of
p in the AR(1) model
Vt = PVt-i + et
In the case of the AR(p) model
yt = piyt-i + p2&Vt-2 + • • • + ppAyt-p+i + et
an M estimator p of p is the solution of
where p = (pu...,pp)f 
and xt = (yt-i, Ayt-u - , Aj/t-p+i)'. The covari-
ance matrix of (p — p) can be estimated by a2A~1CA~1 with
and in this case the DF t-test statistic based on M estimator is
t

14-5 Robust unit root tests 
443
where Sp is the square root of the (1,1) element of a2A~1CA~1. Lucas
proved that both tp and tPl have the same limiting distribution given by
(14.7)
where B and S are generated by the following partial sums
[sT]
t=l
He showed that tp and tPl based on the MM estimator instead of the
M estimator have the limiting distribution (14.7). The critical values
of (14.7) are computed via simulations. In his table 1, Lucas provides
critical values for t-tests based on MM estimates for samples of 50, 100,
and 200. He compares the performance of the DF t-test based on the
OLS estimator with the one based on the MM estimator and finds that
the DF t-tests based on the MM estimator are much more robust to the
outliers he considered.
Lucas (1995b)
Lucas (1995b) considers unit root tests based on M estimators and de-
velops the asymptotic theory for these estimators. The behavior of the
M estimator in nearly stationary models is treated in Cox and Liatas
(1991). Lucas uses these results to derive the asymptotic distributions
for his M estimators. Lucas (1995a) develops unit root tests based on
the high breakpoint estimators suitable for a large number of outliers.
The M estimators, by contrast have a low break down point, but Lucas
(1995b) considers them because they are much easier to compute and
are useful for fat-tail error distributions and provide some protection
against outliers.
To apply the unit root tests based on the M estimators, we need the
variance matrix of the M estimators. Lucas uses the standard one in
Hampel et al (1986, p. 316) which is a heteroskedasticity-consistent
covariance matrix. For comparison he uses this type of standard error
for both the M estimator and the OLS estimator. Thus the unit root
tests based on the OLS estimator are different from the PP tests. But

444 
Outliers and unit roots
Lucas shows that the difference between the two vanishes in the limit,
although it is important in finite samples.
Prom simulation experiments Lucas provides some evidence that unit
root tests based on M estimators are more powerful than those based
on OLS estimators, if the errors come from fat-tailed distributions, al-
though they are less powerful if the errors are normally distributed.
Herce (1994, 1996)
Herce (1996) derives the asymptotic distributions of the least absolute de-
viation (LAD) estimator of the autoregressive parameter under the unit
root null, when errors have finite variance, and suggests LAD-based unit
root tests. He shows that the asymptotic distributions of the LAD esti-
mators depend on nuisance parameters and suggests corrections to the
test statistics similar to those used in the PP tests for OLS-based unit
root tests. He also provides extensive simulation evidence to show that
the LAD-based tests are more powerful than the PP tests in the case
of heavy tailed distributions of errors (which is the case, for instance, of
many financial time series) but their performance is poor if the errors
are normally distributed.
The results of Herce (1996) are for the case of finite error variances.
Herce (1994) suggests a unified unit root testing framework, allowing for
both finite and infinite variance errors.
Rothenberg and Stock (1997)
This is an extension of the likelihood-based tests in Elliott et al. (1996)
to the case of nonnormal errors. They assume that the log density pos-
sesses the first- and second-derivatives. Rothenberg and Stock develop
the asymptotic theory of the test statistics following the lines of Cox
and Liatas (1991) and Lucas (1995a, 1995b). Like the tests by Lucas
(1995b) based on M estimators and by Herce (1996) based on LAD es-
timators, these tests are designed for fat-tailed distributions commonly
encountered in financial time series. A comparison of these three tests
would, therefore, be useful.
Hoek et al. (1995)
Hoek et al. show that in both the classical and Bayesian frameworks,
the presence of additive outliers leads to a biased inference toward sta-
tionarity (the overrejection of the unit root null). They suggest us-
ing an independent student ^-distribution instead of a Gaussian dis-
tribution for an inference on unit roots. This yields maximum like-

14-6 Robust estimation of cointegrating regressions 
445
lihood and posterior results that are less sensitive to the presence of
outliers. The use of the t-distribution for robust inference has been
suggested earlier by Lange et al. (1989). The t-distribution is also
amenable to Bayesian analysis. They argue that the MM estimator
can cope with a large number of outliers, but the MLT (ML estimator
based on the ^-distribution) is robust only to a few outlying observa-
tions.
Hasan and Koenker (1997)
Rank-based methods play an important role in nonparametric inference
but they have not been used much in econometrics. Hasan and Koenker
(1997) apply rank-based methods to the problem of unit root testing.
The paper also gives extensive references to the literature on rank-based
methods in statistics. Hasan and Koenker develop rank-based unit root
tests exploiting the formal duality between rank statistics and quantile
regression. We shall omit the details, which are complicated and can
be found in their paper, but note that they found that the rank tests
they develop have lower than nominal sizes. They, therefore, exam-
ine size-corrected power, which is comparable to that of the ADF test
in the case of normal innovations, and substantially better than that
of the ADF test for t- and Cauchy errors. The rank tests have been
suggested to tackle nonnormal distributions of the errors and thus are
comparable to the other tests discussed here. There is as yet no com-
parative study of the different tests here to give advise to practitioners.
14.6 Robust estimation of cointegrating regressions
In the preceding section we discussed some robust unit root tests. We
shall discuss briefly robust estimation methods for single cointegrating
regression. We shall discuss the FM-LAD and FM-M estimation meth-
ods due to Phillips (1996) and the extention by Hoek et al. (1995),
using ^-distributed errors, and the cointegrating regressions by Franses
and Lucas (1995).
In the case of nonstationary time series, Phillips (1996) suggests two
robust methods of estimation: the fully modified least absolute deviation
(FM-LAD) estimation and fully modified M (FM-M) estimation. These
are extensions of the FM-OLS estimator of Phillips and Hansen suitable
for heavy tailed error distributions.

446 
Outliers and unit roots
Consider the conintegrated system
yt = (3'xt + ult
Axt = u2t
where ut = {u\t,U2t) is a vector of stationary errors. Xt and yt are
1(1) and cointegrated since u\t is 1(0). The least squares estimator of (3
is superconsistent but its asymptotic distribution depends on nuisance
parameters arsing from the endogeneity and serial correlation in the
errors Ut. The FM-OLS estimator starts with POLS and applies semi-
parametric corrections to it, to take care of the two problems of endo-
geneity and serial correlation.
The FM-LAD estimator starts with (3LAD and then applies semi-
parameteric corrections to it in the same fashion as with FM-OLS es-
timator. Similarly, the FM-M estimator starts with (3 M- The semi-
parameteric corrections are for the problems of endogeneity and serial
correlation mentioned above. Thus, the FM-LAD and FM-M estimators
are designed to combine the features of estimators for nonstationary re-
gression like FM-OLS with the characteristics of LAD and M estimators
that are robust to the presence of outliers.
Phillips develops the asymptotic theory necessary for these estimators
which we shall not discuss here. He also presents some simulation re-
sults and outlines the possible extensions to multivariate regressions or
subsystem conintegration. Finally, an empirical illustration of the prac-
tical usefulness of the FM-LAD procedure is presented. The example
refers to the efficiency of the Australian foreign exchange market (Aus-
tralian dollar in terms of the US dollar), using the daily exchange rate
data over the period January 1984-April 1991 and a forward contract of
three months (a total of 1,830 observations). The equation estimated is
St+k = ® + fift,k + Ut+k
where St+k is the log of the spot exchange rate and ft,k is the log of
the forward exchange rate for a A:-period ahead contract delivery. The
forward rate unbiasedness hypothesis states that (3 = 1. The FM-OLS
and FM-LAD estimators of (3 (with standard errors in parentheses) are
FM-LAD : 0.700(0.040)
FM-OLS : 0.883(0.092)
The nonrobust estimates are biased in favor of the unbiasedness hypoth-
esis, whereas the robust estimates do not support it.

I4..6 Robust estimation of cointegrating regressions 
447
Phillips, McFarland, and McMahon (1996) also investigate the unbi-
asedness hypothesis based on daily data on spot and one-month forward
exchange rates over the period May 1922-May 1925. The currencies
considered are the Belgian Franc, French Franc, Italian Lira, and the
US dollar, all measured in terms of pounds sterling. The estimates of (3
(with standard errors in parentheses) are:
Belgium FM-LAD : 
0.880(0.040)
FM-OLS : 
0.952(0.03)
France 
FM-LAD : 
0.863(0.046)
FM-OLS : 
0.942(0.032)
Italy 
FM-LAD : 
0.863(0.055)
FM-OLS : 
0.950(0.043)
The results for US did not change the inference and hence are not re-
ported here. But for the other currencies, the robust method does not
support the unbiasedness hypothesis, whereas the OLS method is biased
in favor of the hypothesis. More details and tests of market efficiency can
be found in Phillips et al. The major conclusion of these studies is that
the use of robust methods makes an important difference to the conclu-
sions. The papers also provide evidence to show that the distributions
are fat tailed.
Franses and Lucas (1995) discuss another method for robust estima-
tion of cointegrating regressions. Their method which follows the sug-
gestion of Lange et al. (1989) consists of using ^-distribution errors
rather than Gaussian errors and using the Johansen maximum likeli-
hood procedure. They argue, based on the results presented in Hoek
et al. (1995), that maximum likelihood estimation of the model with
student ^-distributed errors (MLT) guards against the presence of out-
liers, if there are not too many of them. They present an empirical
example involving interest rate spreads in the Netherlands where the
maximum likelihood method based on Gaussian likelihood and student
t-likelihood led to different conclusions. Franses and Lucas also argue
in favor of using the weights generated by the use of robust methods to
identify outliers and structural breaks.
In the preceding discussion, it was assumed that nonstationarity was
caused by an autoregressive unit root in the process. Nonstationar-
ity can also arise from fractional roots (see chapter 9). Beran (1994,
chapter 7) discusses robust estimation methods. Following Martin and
Yohai (1986), he develops bounded influence estimation for long-memory
models.

448 
Outliers and unit roots
14.7 Outliers and seasonal unit roots
Franses and Vogelsang (1995) extend the HEGY test for the seasonal
unit root test (see chapter 12 for the HEGY test) to the case of AO
and 10 models. They consider the problems of testing for seasonal unit
roots in the presence of a single break in each season in one particular
year. They consider both the cases where the date of break is known and
unknown. They consider the additive outlier (AO) model, which treats
the shifts as immediate and the innovative outlier (10) model which
treats the shifts as gradual. They derive the limiting distributions of
test statistics for seasonal unit roots, present asymptotic critical values,
and also investigate how well the asymptotic distributions approximate
finite sample distributions for a sample size of 20 quarterly observations.
The proofs of asymptotic distributions are presented in appendix A and
the table of critical values in appendix B of their paper.
Franses and Vogelsang also analyze quarterly industrial production for
the US based on data for 1960:I-1991:IV (128 quarterly observations).
When they do not incorporate mean shifts, they find evidence of a sea-
sonal unit root at the bi-annual frequency. However, when they test for
seasonal unit roots in the AO or 10 model, the evidence for a seasonal
unit root disappears.
14.8 Conclusions
The chapter considers several problems relating to outliers in time series
models, and robust estimation methods in time series models.
First the different types of outliers are defined: additive outliers (AO),
innovation outliers (10), level changes (LC), and variance changes (VC).
Next the effects of these different types of outliers on unit root tests
have been discussed. For this an important distinction has to be made
between outliers in levels versus outliers in differences.
Given that the unit root tests are sensitive to outliers, different outlier-
robust unit root tests have been discussed. Some of these robust tests
have been suggested as robust to distributional assumptions. There is
as yet no comparative study of these different robust unit root tests.
Finally, two robust cointegration methods: FM-LAD and FM-M have
been discussed. The area of robust unit root tests and robust cointegra-
tion methods is new and will no doubt be an active area of research in
the future.
What are the new insights obtained by these outlier models? The

References 
449
answer depends on whether the outliers detected agree or disagree with
the breakpoints identified by extraneous information and other models
incorporating structural changes, and, if they disagree, what new infor-
mation is provided by the outliers. There is some evidence on this in the
paper by Balke and Fomby (1991a) but the evidence is not conclusive. A
major problem with outlier analysis that has yet not been satisfactorily
solved is the masking problem, that the initial model estimated with the
outliers might mask the true outliers. In the usual regression models this
is analyzed by the deletion methods. An extension of these methods to
time series models within the context of a structural time series model
can be found in Atkinson et al. (1997) who apply it to the analysis of
gas sales data with quarterly observation for 1960-1986.
Assuming that the masking problem is not serious or is approximately
solved, the outlier analysis can give us answers to questions whether
some shocks have temporary or permanent effects (innovative outliers
and level changes). The outlier analysis can be a useful complement to
the regime switching models discussed in the next chapter.
We have discussed removing outliers as a solution to the outlier prob-
lem. This has some consequence on the power of unit root tests. Large
shocks (which may or may not be outliers) contain a great deal of in-
formation as to whether or not there is a unit root. If the series mean
reverts quickly, a conclusion of stationarity is likely. If not, then a unit
root will not be rejected. Removing this observation as an outlier thus
has important power implications.
How does one use the outlier models for prediction purposes. There is
not much discussion or empirical evidence on this. McCulloch and Tsay
(1993) suggest tagging on a probit model to the outlier model. Though
their analysis is in the Bayesian framework using Gibbs sampling, the
idea can be equally implemented (if the necessary data are available) in
the usual framework. Suppose that you identify a number of outliers. If
we have a set of explanatory variables relating to their occurence, then
we can use this set of variables to predict when some future outliers
would occur. In principle, this can work (and McCulloch and Tsay
show through a Monte Carlo study that it does) but the only question
is how to implement it in practice.
References
Abraham, B. and A. Chung (1989), "Outlier Detection and Time Series
Modeling," Technometrics, 31, 241-248.

450 
Outliers and unit roots
Abraham, B. and N. Yatawara (1988), "A Score Test for Detection
of Time Series Outliers," Journal of Time Series Analysis, 9,
109-119
Atkinson, A.C., S.J. Koopman and N. Shephard (1997), "Detecting
Shocks : Outliers and Breaks in Time Series," Journal of Econo-
metrics, 80, 387-422.
Balke, N.S.(1993), "Detecting Level Shifts in Time Series," Journal of
Business and Economic Statistics, 11, 81-92.
Balke, N.S. and T.B. Fomby (1991a), "Shifting Trends, Segmented
Trends, and Infrequent Permanent Shocks," Journal of Mon-
etary Economics, 28, 61-85.
(1991b), "Infrequent Permanent Shocks and the Finite-Sample Per-
formance of Unit Root Tests," Economics Letters, 36, 269-273.
(1994), "Large Shocks, Small Shocks, and Economic Fluctuations:
Outliers in Macroeconomic Time Series," Journal of Applied
Econometrics, 9, 181-200.
Beran J. (1994), Statistics for Long Memory Processes, Chapman and
Hall, New York.
Bruce, A.G. and R.D. Martin (1989), "Leave K-Out Diagnostics for
Time Series (with Discussion)," Journal of Royal Statistical So-
ciety, Series B, 51, 363-424.
Chen, C. and L.M. Liu(1993), "Joint Estimation of Model Parameters
and Outlier Effects in Time Series," Journal of the American
Statistical Association, 88, 284-297.
Cook, R.D. (1987), "Influence Assessment," Journal of Applied Statis-
tics, 14, 117-131.
Cook, R.D. and S. Weisberg (1982), Residuals and Influence in Regres-
sion, Chapman and Hall, New york.
Cox, D.D. and I. Liatas (1991), "Maximum Liklihood Type Estimation
for Nearly Nonstationary Autoregressive Time Series," Annals
of Statistics, 19, 1109-1128.
Donald, S.G. and G.S. Maddala (1993), "Identifying Outliers and Influ-
ential Observations in Econometric Models," in G. S. Maddala,
C. R. Rao, and H. D. Vinod (eds.), Handbook of Statistics, vol.
11, North-Holland, Amsterdam, 663-701.
Durbin, J. and M. Cordero (1994), "Handling Structural Shifts, Out-
liers and Heavy-Tailed Distributions in State Space Time Series
Models," Manuscript, London School of Economics.
Elliott, G., T.J. Rothenberg, and J.H. Stock (1996), "Efficient Tests
for an Autoregressive Unit Root," Econometrica, 64, 813-836.

References 
451
Fiebig, D.G. and E. Maasoumi (1990), "Specification Analysis in Dy-
namic Models," Journal of Business and Economic Statistics,
8, 443-451.
Fox, A.J. (1972), "Outliers in Time Series," Journal of the Royal Sta-
tistical Society, Series B, 34, 350-363.
Franses, RH. and N. Haldrup (1994), "The Effects of Additive Outliers
on Tests for Unit Roots and Cointegration," Journal of Business
and Economic Statistics, 12, 471-478.
Franses, RH. and A. Lucas (1995), "Outliers Robust Cointegration
Analysis," Report 9529A, Erasmus University, Rotterdam.
Franses, RH. and T.J. Vogelsang (1995), "Testing for Seasonal Unit
Roots in the Presence of Changing Seasonal Means," Report no.
9532A, Econometric Institute, Erasmas University, Rolterdam.
Hampel, H.R., E.M. Ronchetti, RJ. Rousseeuw, and W.A. Stahel
(1986), Robust Statistics: The Approach Based on Influence
Functions, Wiley, New York.
Hasan, M.N. and R.W. Koenker (1997), "Robust Rank Tests of the
Unit Root Hypothesis," Econometrica, 65, 133-161.
Hendry, D. F. and A.J. Neale (1990), "The Impact of Structural Breaks
on Unit Root Tests," in P. Hackl and A. Westlung (ed.), Eco-
nomic Structural Change: Analysis and Forecasting, Springer
Verlag, Berlin.
Herce, M.A. (1994), "A Unified Approach to Unit Root Testing Based
on LAD Estimation," Manuscript, University of North Carolina
at Chapel Hill.
(1996), "Asymptotic Theory of LAD Estimation in a Unit Root Pro-
cess with Finite Variance Errors," Econometric Theory, 12, 129-
153.
Hoek, H., A. Lucas and H.K. van Dijk (1995), "Classical and Bayesian
Aspects of Robust Unit Root Inference," Journal of Economet-
rics, 69, 27-59.
Huber, P.J. (1964), "Robust Estimation of a Local Parameter," Annals
of Mathematical Statistics, 35, 73-101.
(1973), "Robust Regression: Asymptotics, Conjectures, and Monte
Carlo," Annals of Statistics, 14, 781-818.
Lange, K.L., R.J.A. Little, and J.M.G. Taylor (1989), "Robust Statis-
tical Modeling Using the t-Distribution," Journal of American
Statistical Association, 84, 881-896.
Ledolter, J. (1990), "Outlier Diagnostics in Time Series Analysis,"
Journal of Time Series Analysis, 11, 317-324.

452 
Outliers and unit roots
(1991), "Outliers in Time-Series Analysis: Some Comments on Their
Impact and Their Detection," in W. Stahel and S. Weisberg
(eds.), Directions in Robust Statistics and Diagnostics, vol. I,
Springer Verlag, New York, 159-165.
Ljung, G.M. (1993), " On Outlier Detection in Time Series," Journal
of Royal Statistical Society, Series B, 55, 559-567.
Lucas, A. (1995a), "An Outlier Robust Unit Root Test with An Appli-
cation to the Extended Nelson-Plosser Data," Journal of Econo-
metrics, 66, 153-173.
(1995b), "Unit Root Tests Based on M-Estimators," Econometric
Theory, 11, 331-346.
Maddala, G.S. and Y. Yin (1996), "Outliers, Unit Roots and Robust
Estimation of Non-Stationary Time Series," in G. S. Maddala
and C. R. Rao (eds.), Handbook of Statistics, vol. 15, Elsevier,
Amsterdam, 237-266.
Martin, R. D. and V. J. Yohai (1986), "Influence Functional for Time
Series," Annals of Statistics, 14, 781-818.
McCulloch, R.E. and R.S. Tsay (1993), "Bayesian Inference and Pre-
diction for Mean and Variance Shifts in Autoregressive Time Se-
ries," Journal of the American Statistical Association, 88, 968-
978.
Perron, P. (1989), "The Great Crash, the Oil Price Shock and the Unit
Root Hypothesis," Econometrica, 57, 1361-1401.
(1990), "Testing for a Unit Root in a Time Series with a Changing
Mean," Journal of Business and Economic Statistics, 8, 153-
162.
Perron, P. and S. Ng (1996), "Useful Modifications to Some Unit Root
Tests with Dependent Errors and Their Local Asymptotic Prop-
erties," Review of Economic Studies, 63, 435-465.
Perron, P. and T.J. Vogelsang (1992a), "Nonstationarity and Level
Shifts with an Application to Purchasing Power Parity," Journal
of Business and Economic Statistics, 10, 301-320.
(1992b), "Testing for a Unit Root in a Time Series with a Changing
Mean: Corrections and Extensions," Journal of Business and
Economic Statistics, 10, 467-470.
(1993), "A Note on the Asymptotic Distributions of Unit Root Tests
in the Additive Outlier Model with Breaks," Revista De Econo-
metric, 8, 181-202.
Phillips, P.C.B. (1996), "Robust Non-Stationary Regression," Econo-
metric Theory, 11, 912-951.

References 
453
Phillips, P.C.B., J.W. McFarland, and P.C. McMahon (1996), "Robust
Tests of Forward Market Efficiency with Empirical Evidence
from the 1920s," Journal of Applied Econometrics, 11, 1-22.
Rappoport, P. and L. Reichlin (1989), " Segmented Trends and Non-
Stationary Time Series," Economic Journal, 99, 168,177.
Rothenberg, T.J. and J.H. Stock (1997), "Inference in a Nearly In-
tegrated Autoregressive Model with nonnormal Innovations,"
Journal of Econometrics, 80, 269-286.
Rousseeuw, P.J. and V.J. Yohai (1984), "Robust Regression by means
of S-Estimators," in J. Franke, W. Hardle, and R. D. Martin
(eds.), Robust and Nonlinear Time Series, Springer Verlag, New
York, 256-272.
Tsay, R.S. (1988), "Outliers, Level Shifts, and Variance Changes in
Time Series," Journal of Forecasting, 7, 1-20.
Vogelsang, T.J. (1994), "On Testing for a Unit Root in the Presence
of Additive Outliers," CAE Working paper No. 94-30, Cornell
University.
Vogelsang, T.J. and P. Perron (1994), "Additional Tests for a Unit
Root Allowing for a Break in the Trend Function at an Unknown
Time," CAE Working paper No. 94-13, Cornell University.
West, M. (1986), "Bayesian Model Monitoring," Journal of the Royal
Statistical Society, Series B, 48, 70-78.
West, M. and P. J. Harrison (1986), "Monitoring and Adaptation in
Bayesian Forecasting Models," Journal of the American Statis-
tical Association, 81, 741-750.
Yin, Y. and G.S. Maddala (1996), "Effects of Variance Change on Unit
Root Tests," Working Paper, Department of Economics, The
Ohio State University.
(1997), "The Effects of Different Types of Outliers on Unit Root
Tests," in T. Fomby and R. C. Hill (eds.), Advance in Econo-
metrics, vol. 13, JAI Press, Greenwich, Conn.
Yohai, V.J. (1987), "High Breakdown-Point and High Efficiency Robust
Estimates for Regression," Annals of Statistics, 15, 642-656.

15
Regime switching models and structural
time series models
In the previous two chapters we considered some methods of an analysis
of structural change. In this chapter we shall discuss regime switching
models of which the widely used Markov switching model is a special
case. We also discuss several extensions of the Markov switching model.
This model implies sudden switches. More reasonable models are grad-
ual switching models - Harvey's structural time series model being a
special case. These models are also known as state-space models.
We shall discuss the main ideas and the problems these models try
to solve. We omit the detailed mathematics, which is quite involved,
because we do not see any point in reproducing it from the papers cited.
Those interested in the details can refer to the particular paper relevant
for their work, on which they can decide from our review. We feel it
is important to understand the merits and limitations of the different
procedures before getting bogged down in the mathematical detail.
The estimation problems with the Markov switching models are not
as complex as those of testing. An illustration of the problems of testing
is the paper by Garcia (1997) which has been around for many years in
its several unpublished versions.
15.1 The switching regression model
A commonly used model for the analysis of structural change is the
switching regression model. This model was first introduced into the
econometric literature by Quandt (1958), though it has a long history
(see Lindgren, 1978). The simplest two-regime model is given by
Yt 
= XuPi + ult 
in regime 1
Yt 
= 
X2t/3i + u2t 
in regime 2
454

15.2 The Markov switching regression model 
455
Xu and X<it are explanatory variables. The variable Yt is generated from
regime 1 or regime 2, but not both. Suppose we define the indicator
variable It by
It 
= 
1 
if It comes from regime 1
It 
= 
0 
if Yt comes from regime 2
and It is observed. In this case, equations (15.1) can be estimated by
OLS using the observations in the respective regimes. If the indicator
It is not observed but its determinates Xst are, we can define another
equation relating a latent variable Zt to
Zt = X3t03 + U3t 
(15.2)
and define
1 
if Zt > 0
0 
ifZt<0
In this case we have probabilistic sample separation. Let the covariance
matrix ^1,^2, and u3 be denoted by
Jl2 
0"22 
&23
O~13 
CT23 
1
Since Zt is observed only as a dichotomous variable, we assume
var(Zt) = 1. If &13 = cr23 = 0, we have a switching regression model
with exogenous switching. Otherwise we have endogenous switching (see
Maddala, 1983, p. 284). With endogenous switching, even if the sample
separation is known, equation (15.1) cannot be estimated by OLS.
The model given by equations (15.1) and (15.2) results in a mixture
distribution. For these problems, the maximum likelihood method fails
because the likelihood function explodes for some values of the error vari-
ances. This problem has been noted by Quandt (see Maddala, 1983, p.
299). For this reason, Goldfeld and Quandt (1973) suggested a Markov
switching regression (MSR) model.
15.2 The Markov switching regression model
In the Markov switching regression (MSR) model, the probabilities of
switching from one regime to the other in the next period are assumed
to be constant. These are given by the following transition matrix

456 
Regime switching models and structural time series models
Regime at time t
It = 1 
It = 0
Regime at time t — 1 
It~\ = 1 
pn 
pio
h-i = 0 
poi 
Poo
Another modification of the switching regression model is that by Lee
and Porter (1984) who consider the availability of a dichotomous indica-
tor Wt that provides sample separation but this indicator is imperfect.
They postulate a transition probability matrix
t = l 
Wt =
it = 1 
Pn 
Pio
it = 0 
Poi 
Poo
If Pn = Poi? the indicator does not provide any information on sample
separation. If pu = 1 and poi = 0, the indicator Wt provides perfect
sample separation. In the case pu ^ 1 and poi ¥" Pu(> Poi) w e have
imperfect sample separation. Lee and Porter examine the transportation
prices charged by the Joint Executive Committee railroad cartel from
1880-1886 using the price equation
log Pt = /?o + Xtfc + fait + ut 
(15.3)
where It = 0 if there is a price war and It = 1 otherwise. Thus, there is
a regime switch. They used the indicator Wt obtained from the Railroad
Review which reported in each period whether a price war was going
or not. One reason Lee and Porter suspected measurement error in Wt
is that it conflicted with an index of cartel adherence constructed by
MacAvoy.
Cosslett and Lee (1985) extend this model by Lee and Porter to the
case where the error term ut in (15.3) is serially correlated. In addition,
they assume that the true indicator variable It follows a Markov chain
with transition probability matrix
A = [Xij] 
a n d A^- = Prob(It 
= j | / £ _ i = i), 
i,j = 0 , 1
Cosslett and Lee derive an algorithm for computing the likelihood and
its derivatives for this model by means of a recurrence relation. In
the appendix we review how this algorithm can also be used for the
ML estimation of the MSR model. They also argue that the original
method proposed by Goldfeld and Quandt (1973) yields consistent but
inefficient estimates. This is because it does not take into account an

15.3 The Hamilton model 
457
essential feature of a Markov chain, namely that the state probabilities
at time t depend on the realized state at time (t — 1). Cosslett and Lee
present the correct log likelihood function for this model.
15.3 The Hamilton model
The MSR model considered by Hamilton (1989, 1990, 1992) is an ex-
tension of the idea of Quandt (1958), Goldfeld and Quandt (1973), and
Cosslett and Lee (1985) to an autoregression. Related models have been
discussed by Sclove (1983), though he did not discuss maximum likeli-
hood (ML) estimation of the parameters.
Suppose that the time series yt is composed of two parts
yt = nt + zt 
(15.4)
where
nt = nt-i + ao + aist
and
zt -zt-i 
= <t>(zt-i - zt-2) + et
with et ~ im(0,cr2) and \(f>\ < 1. Suppose that there exists a finite set
of K states (regimes) and that each yt is associated with an unobserved
state St. We also assume that the process {st} is a Markov chain with
the stationary transition probability matrix P = (pij) where
pij=Prob(st=j\st-i=i), 
i = 0,l, .7 = 0,1
with pio = 1 — pn and poo = 1 — Poi- This model is referred to as the
MSR model or Hamilton's model.
The MSR model implies that the level of the series, yt, is an ARIMA
(1,1,1) and its representation conditional on st is an ARIMA(l,l,0) with
shifts in the mean. Define yt = Ayt and zt = Aif Differencing (15.4)
gives
yt 
= ao + aist + zt
where zt follows a Gaussian AR(p) process. Suppose that zt is AR(1),
then the MSR can be written as
yt-Vst 
= zt = (j)zt-i + et
= 
<t>(yt-i -A*8t-i) + et

458 
Regime switching models and structural time series models
The dynamic structure of MSR in the framework of AR models implies
that the effect of a change in regime is immediate, and is not tied down
by the dynamic consequences of St-
In the two-state MSR, there are 2n state configurations for a sample
of size n. To overcome this problem, Cosslett and Lee (1985) suggest
a recursive algorithm for the ML estimation of the model. Hamilton
(1989) uses an adaptation of probability smoothers and the EM algo-
rithm. Earlier, Baum et al. (1970) derived the EM algorithm for ML
estimation of probabilistic functions of Markov chains. Suppose that to
each A G A, we have a smooth assignment A —• [p(A),/(A)]. For each
fixed 2/t, L(X) is a smooth function of A. The likelihood for the MSR
model can be rewritten as
L(\) 
=
ses
ses 
t=i
The EM algorithm is based on the fact that the derivatives of the log-
likelihood have a representation as the difference of an unconditional and
a conditional expectation of the sufficient statistics. The expectation
step is to estimate the complete-data sufficient statistics (in our case
observed y and unobserved s) by computing the auxiliary function
Q(A,A') 
=
= 
p(s,\)\ogp{s,\f)d/jJ(s)
J s
The maximization step is to find A' which maximizes Q(A, A'). With
a given normal density function fi(mi,Gi),L{\) 
can be maximized by
maximizing
Hamilton(1989) shows how to derive j t = p(st\yt, 0) in the MSR model
and named it as smooth inferences or smooth probabilities. The expecta-
tion step of the EM algorithm requires the estimation of the components
of 7t = p(st\yt,0) given the observed yt and the current fitted parame-
ters 8. The maximization step is then equivalent to the complete-data

15.3 The Hamilton model 
459
maximization with weights given by the If-estimated components of S*.
For the MSR model
y t = X't/3i + et, 
i = l , . . . , f c
Hamilton shows that the M-step is an OLS regression of ytypTt ° n Xty/it
with smoothed probabilities. To obtain the ML estimates, with an initial
guess on parameters, the E- and M-step needs to be iterated until the
change in parameter values between iterations is less than some target
convergence criterion, say 10~8.
Kim (1993) suggests an alternative two-stage estimation method based
on the Maximum A Posteriori (MAP) decision rule. The MAP decision
rule originated from the decision-directed learning procedure which is,
with a quasi-Bayes learning procedure (Makov and Smith, 1977), one
of the approximations to the formal Bayes learning procedures (Titter-
ington, Smith, and Makov, 1985). He shows that the sequence of {st}
can be obtained by imposing the MAP decision rule and it is a solution
path of Bellman's dynamic programming algorithm. Then the sequence
of {st} can be used in place of the state (dummy) variables st in MSR
models. A Monte Carlo experiment performed by Kim shows that the
MAP procedure outperforms Hamilton's method in terms of the prob-
abilities of error events (misclassification of states). And the bound of
the probabilities of error events is tighter than in Hamilton's method
regardless of sample size. When he applied the MAP procedure for the
same data set used by Hamilton (1989), Kim found that the dating of
the US business cycles by the MAP procedure is closer to the dating by
NBER than that given by Hamilton's method.
The Markov switching autoregressive model has been applied with
success to a variety of economic and financial data. Hamilton (1988,
1989) uses the model to study the term structure of interest rates and
to date the timing of recessions and booms with the US GNP data. Engel
and Hamilton (1990) use it to explain the long swings in exchange rates.
Pagan and Schwert (1990) use it to model conditional stock volatility
in stock returns. Turner, Startz, and Nelson (1989) also use it to study
risk-return relationships in the stock market. Cecchetti, Lam, and Mark
(1990) use it to study regime switches in century-long data on consump-
tion, output, and dividends. McCulloch and Tsay (1994b) even use it
to make inference about trend and difference stationarity. They treat
trend stationarity and difference stationarity as two competing models
and allow each observation to switch from one model to the other, with
the transition being governed by a Markov process. This method, they

460 
Regime switching models and structural time series models
argue, is superior to unit root tests based on the entire data, because it
can identify periods during which trend stationarity is more appropriate
than difference stationarity.
15.4 On the usefulness of the MSR model
The MSR model is a model that allows us to take into account multiple
structural breaks in a given time series. It is also a model that allows us
to explain nonlinearities in the data. One major limitation of the model
is that it restricts us to the two-regime case. However, there have been
extensions of the model to the case of several regimes, which we shall
discuss in the next section.
The MSR model has been used for a variety of purposes and it has
yielded some new insights in some cases, and not in some others. For in-
stance, the model has been used by Hamilton (1989) to date the turning
points in the US business cycle using the quarterly data for 1951:11-
1984:IV. Hamiton shows that the turning points identified by his model
agree closely with those identified by the NBER. No new insights are ob-
tained except to confirm the NBER method of timing business cycles or
to demonstrate the usefulness of the MSR model. Rabault (1992) stud-
ies the ability of the Hamilton model to reproduce the business cycles
of six industrialized countires (UK, Prance, Germany, US, Canada, and
Japan) and the OECD as a whole. He argues that only three countries
(US, Germany, and Japan) show a satisfactory coincidence between the
Markovian transition and cycle chronology. He also complains about the
fragility of the ML estimation method because multiple local maxima
were often found. Kim (1993) shows that the dates of the US business
cycle identified using the same MSR model used by Hamilton can be
estimated even more closely with those identified by the NBER, if the
MAP estimation procedure is applied.
Engel and Hamilton (1990) found that the simple two-state MSR
model in which the log of the exchange rate is a random walk with
drift in each of the two states provided a good description of exchange
rate behavior. However, Engel (1994) investigated the forecasting be-
havior of the MSR model. He fits the model for 18 bilateral exchange
rates at quarterly and monthly frequencies. The 18 exchange rates
were: US/Canada, US/Prance, US/Italy, US/Japan, US/Switzerland,
US/UK, US/Germany, Japan/Canada, Japan/Prance, Japan/Italy,
Japan/Switzerland, 
Japan/UK, 
Japan/Germany, 
UK/Canada,
UK/Prance, UK/Italy, UK/Switzerland, and UK/Germany. The esti-
mation period was 1973:3-1986:1 and the post-sample forecast period

15.4 On the usefulness of the MSR model 
461
was 1986:2-1991:1. The model was also estimated for the period 1973-
1988 and forecasts were constructed for the remaining period. Engel's
general conclusion was that the MSR model fits well with the sample,
but it does not generate superior forecasts in terms of mean square er-
rors (MSE) to a random walk or the forward rate. Engel, however, finds
some evidence that the MSR model is superior to the others in fore-
casting the direction of change of the exchange rate. He speculates that
with the Louvre accord of March 1987, perhaps the foreign exchange
rates entered a third state, and the MSR model might perform better in
a three-state model.
Lam (1990) extends the Hamilton model of GNP growth to a model
with a general autoregressive component, in which the Markov com-
ponent, nt, is integrated but the Gaussian component, £*, is not. He
specified that zt follows the Gaussian AR(p) process, while zt = Azt
is the Gaussian AR(p) process in Hamilton's model. Lam's framework,
thus, could be viewed as the generalization of Perron's (1989) specifica-
tion of a stationary process around an occasionally shifting linear trend.
Define st to represent the cumulative number of times that the event
sT = 1 has occurred during dates r = 1,2,..., t; thus st takes on an in-
teger value in (0,1,..., t). Then, in the framework of an AR(1) model,
Lam's model can be written as
yt-wt 
= (j){yt-i ~ Wt-i) + et
where wt = n0 + fiiSt + (t — s*)/^. Unlike the Hamilton model, Lam's
model does not restrict one of the roots of the AR process to unity.
His algorithm is consequently more complicated than that of Hamilton.
For the same data used by Hamilton, Lam compares the forecasting
performance of his model with that of the Hamilton model and the
traditional ARIMA model by considering within-sample forecasts over
different horizons of 1, 4, 5, 20, and 40 quarters. None of the models
dominated the others over all horizons. Out-of-sample forecasting was
not considered. Also, unlike the conclusion of Hamilton (1989), the
dichotomous shift in states identified by Lam do not resemble the NBER
dating of US business cycles very closely. Both Hamilton's and Lam's
papers, however, argue that there were shifts in the trend of US GNP
other than that caused by the major oil price shock of 1974-1975.
Albert and Chib (1993) and McCulloch and Tsay (1994b) present a
Bayesian analysis of the MSR model using the Gibbs sampling approach.
The gibbs sampling method enables us to obtain marginal distribu-
tions of random variables numerically from the full cycle of conditional

462 
Regime switching models and structural time series models
distributions (see Gelfand and Smith, 1990 and Casella and George,
1992, and chapter 16 for its adaptation to the analysis of multiple struc-
tral changes by Kim and Maddala, 1992). The method is useful when
the joint distribution is complicated but the conditional distributions
are well defined, which is the case with the MSR model.
Albert and Chib (1993) claim that Hamilton's smooth inferences 7$ =
p(st\yt, 0) are based on the observed yt and the current fitted parameters
6 and thus, the uncertainty about 6 is not incorporated in the inferences
about the states. In the Bayesian approach this uncertainty is taken into
account because it is the marginal distribution p(st) that is considered.
Albert and Chib present a Bayesian analysis of the Hamilton model and
implement it using Gibbs sampling. However, when applied to the US
GNP data, they found that the timing of booms and recessions is not
different from those obtained by Hamilton.
McCulloch and Tsay (1994a) also use a Bayesian analysis with the
Gibbs sampler, but they allow the dynamic pattern of US GNP growth
to be different during expansions and contractions. They first estimate
the Hamilton-type MSR model, where the dynamic pattern of US GNP
growth is the same in both states, and get estimated probabilities close to
those of Hamilton, but the posterior probabilities are in general smaller
and smoother than those obtained by the Hamilton method. This is
because the uncertainty in 6 is taken into account. They then expand
the model to allow for different dynamics in the two states
Vt = aoiyt-i + a02yt-2 + «O32/t-3 + a04yt-4 + Co 4- eOt, if st = 0
t-i + ai2yt-2 + aisVt-3 + auVt-4 + Ci + eit, if st = 1
They find that the classification of expansion and contraction becomes
somewhat less clear when the dynamic pattern of GNP growth is made
to be state dependent. There is, however, no evidence in either of these
two Bayesian papers on the post-sample predictive ability of the MSR
model relative to say the ARIMA model. Albert and Chib (1993) also
apply the Gibbs sampler to the Markov switching model. However,
they consider a model where only the level parameter and innovational
variances differ from state to state. The autoregressive coefficients are
the same in both the states. Thus, it is a special case of the model
considered by McCulloch and Tsay (1994a).
McCulloch and Tsay (1994b) treat trend stationarity and difference
stationarity as two competing models and cast them into the frame-
work of a Markov switching model. Considering the monthly series of

15.5 Extensions of the MSR model 
463
industrial production index for the US from 1947:1 to 1992:1 (541 obser-
vations), they conclude that there might be a structural change around
t = 300 (December 1971) and that the series was difference-stationary
before that, but moved closer to trend stationarity after December 1971.
It would be interesting to examine this same time series using the grad-
ual switching models and see the switch date and the length of the
adjustment period.
Garcia and Perron (1996) consider the time series behavior of US real
interest rates from 1961 to 1986. They consider a three-state MSR model
in levels rather than in first-differences as in the Hamilton model. They
find that the real interest rate is essentially random around a mean that
is different for the periods 1961-1973, 1973-1980, and 1980-1986. The
variance of the random process is also different in the three periods, being
higher in both the last two subperiods. Garcia and Perron compared
the within-sample forecasting ability of the three-state MSR model to a
simple random walk model and find the former better (though no out-of-
sample forecasts were made). They argue that the earlier investigations
that found the real interest rate to be a random walk arrived at the
(wrong) conclusion because of ignoring structural breaks. Garcia and
Perron also discuss three tests: Davies-test, Gallant-test, and the J-
test to determine the number of states (1, 2, or 3) and find that the
tests generally favored a three-state model. These tests are discussed
further in Garcia (1997). One important difference in the economic
interpretation of the structural break point is that the two-state model
picks the second break at the end of 1979, suggesting that a change in
monetary policy is its origin, whereas, the three-state model puts the
second break in the middle of 1981, a date more in line with the federal
deficit explanation.
Overall, apart from the paper by Engel (1994) there is not much evi-
dence on out-of-sample forecasting ability of the MSR model. There is
evidence on the ability of the MSR model in forecasting turning points
and identifying trend breaks, through there is some skepticism on this
issue in the papers by McCulloch and Tsay (1994a) and Rabault (1992).
The paper by Garcia and Perron (1996) demonstrates the importance
of tests for the number of states.
15.5 Extensions of the MSR model
In the preceding section we considered an example of a three-state MSR
model by Garcia and Perron (1996), who concluded that the appar-

464 
Regime switching models and structural time series models
ent unit root in the real interest rate is an artifact due to structural
changes. Another example of multiple states is the paper by Raymond
and Rich (1992) who consider a four-state model and analyze the quar-
terly growth rates of the GNP deflator over the period 1948:I-1987:IV.
They find that the time series is characterized by a low mean high vari-
ance regime over 1949-1954, a low mean, low variance regime over 1955-
1967, and recurrent shifts between medium mean and high mean regimes
over 1968-1983, before returning to the low mean, low variance regime
in 1984. They also found that the apparent evidence of a unit root in
the conditional mean and ARCH structure in the conditional variance
are artifacts due to shifts in the mean and shifts in variance.
One major drawback of the models considered by Hamilton (1989) and
Lam (1990) is that they are pure broken trend models with no explana-
tory variables. There are now several papers that include explanatory
variables in the MSR model. Rich and Raymond (1992) generalize the
Hamilton model of real GNP growth to include lagged growth rates of
the real price of oil and analyze the importance of real oil prices on the
GNP growth rate within the context of a two-state MSR model. They
find that real GNP growth rates are significantly negatively correlated
with real oil price increases but have no significant correlation with real
oil price decreases. But the fluctuations in GNP cannot be fully ac-
counted for by fluctuations in real oil prices. Thus, other variables need
to be included in the model.
One other extension of the Hamilton model is by Durland and Mc-
Curdy (1992). They argue that extending the MSR model to multiple
states increases the computations polynomially and that unrestricted
higher-order Markov processes result in substantial loss in degrees of
freedom. They, therefore, suggest a semi-Markov process (which could
also be interpreted as a restricted higher-order Markov process). In a
semi-Markov process the successive states are determined not only by
the transition probabilities but also by the holding and waiting periods
in each state. Durland and McCurdy consider a restricted version of the
semi-Markov process, suggest an algorithm to estimate the model, and
present evidence with the US GNP data. They find evidence of signifi-
cant asymmetry between recessions and expansions and some duration
dependence for the former but not the latter. They also suggest that
Hamilton's first-order Markov models can be rejected when compared
with the AR(4) model, whereas the semi-Markov model cannot be re-
jected against the AR(4) model. These preliminary results suggest that
duration dependence is an important factor in MSR models.

15.5 Extensions of the MSR model 
465
Bekaert and Hodrick (1993) consider endogenous regime shifts in the
MSR model. In their model the conditional means in each state are
allowed to depend autoregressively on lagged values of the rate of depre-
ciation and the forward premium. Kaminsky (1993) considers another
generalization of the MSR model by Hamilton. She assumes that ratio-
nal investors not only recognize the possibility of changes in the regime
but also incorporate the information provided by the Fed announcements
into their forecasts for the future. However, the Fed announcements are
a noisy indicator. Thus the model is in the spirit of the papers by Lee and
Porter (1984) and Cosslett and Lee (1985). Kaminsky also extends this
model by allowing serial correlation in the measurement errors. During
the period from March 1976 to December 1987 there were 274 announce-
ments by the Fed classified as easy money policy (indicator =1) and
tight money policy (indicator = 0). Kaminsky reports results from out of
sample forecasts with the MSR models compared with the random walk
model and finds the latter superior. However, she argues that even if the
MSR models does not give better out of sample forecasts, it still explains
most of the variation in the exchange rate and gives new insights into
the behavior of rational investor's response to changing fundamentals.
Lahiri and Wang (1994) applied the two-state MSR model to evalu-
ate the Commerce Department's Composite Index of Leading Indicators
(CLI) as a predictor of business cycle turning points. They found that
the predictive performance of CLI is quite good. However, they found
that imposing any degree of autoregression in the errors on the simple
regime-shift model caused the MSR model to signal turning points in-
appropriately. Evans (1993) extended Hamilton's estimation methods
to the MSR model with serial correlation in errors. He applied the two-
state MSR model for quarterly US inflation and could not reject the
presence of serial correlation of errors. Diebold et al. (1994) proposed
a class of MSR models in which the transition probabilities vary with
underlying (economic) fundamentals. They develop an EM algorithm
for estimation of the model and illustrate it with a simulation example.
Lee (1991) and Filado (1994) include explanatory variables with tran-
sition probability depending on economic fundamentals. Lee (1991)
uses real interest rate difference as the fundamental factor. Kim and
Lee (1996) employ the magnitude of the deviation of the exchange rate
from a monetary equilibrium value as the economic fundamental with
which the transition probabilities vary. They find that the predictions
from the TVTP (time-varying transition probability) Markov switching
model, using this fundamental variable as an explanatory variable, fore-

466 
Regime switching models and structural time series models
casts exchange rates much better than FTP (fixed transition probability)
models as used in Engel (1994). The TVTP Markov model can identify
both the appreciation state and depreciation state better than the FTP
Markov model. Finally, the TVTP model is also better at predicting
the direction of the exchange rate than the FTP model.
Kim and Lee (1996) note that in the Markov switching model, the
lack of observations on states can be regarded as an incomplete data
problem. Hence they suggest and use the EM algorithm to estimate the
TVTP model. Rabault (1992) also uses the EM algorithm. Kim and
Lee, also derive the exchange rate from the monetary equilibrium value,
the latter being determined by a time-varying coefficient cointegration
regression as described in Kim and Yu (1996).
Finally Hamilton and Susmel (1994) extend the Markov switching
model to ARCH models. The resulting SWARCH models have been used
by Kim (1994) and found to be better than ARCH models in explaining
risk premia.
Franses and Rapp (1996) investigate a different issue: do the usual
seasonal adjustment procedures (like the census X-ll) affect inference
about the probabilities in the MSR model? They answer this question
in the affirmative. The commonly used seasonal adjustment procedures
(since they smooth the data) result in increasing the probabilities in
the MSR model of staying in the same regime. Hence they suggest es-
timating MSR models using seasonally unadjusted data. (See chapter
12 for arguments in several other situations, where it is suggested that
using seasonally adjusted data should be avoided.) They illustrate this
problem through Monte Carlo analysis as well as two empirical examples
concerning German unemployment and US industrial production. Sev-
eral studies (e.g., Golodwin, 1993) study business cycles with an MSR
model using seasonally adjusted data. Franses and Rapp argue that this
can affect business cycle turning points.
15.6 Gradual regime switching models
In the previous sections we discussed regime switching models where
the switch takes place at a certain point of time. Suppose there is a
policy change at time t. In actual practice, the switch to the new regime
need not take place at time t, nor need the switch be sudden. There
are several reasons for this. Some economic agents may anticipate the
policy change and adjust before it takes place. Some economic agents
may not consider the policy change as credible and may take time to

15.6 Gradual regime switching models 
467
learn. Also, there will be some costs of adjustment that accounts for the
delays. Because of these factors, some gradual regime switching models
have been suggested.
The earliest example is that of Bacon and Watts (1971), followed by
Tsurumi (1983), Ohtani and Katayama (1985), and Ohtani et al. (1990).
In Bacon and Watts and Tsurumi, the transition path from one regime to
the other is specified by a hyperbolic tangent of time. In this method the
start point of the transition from one regime to the other is estimated
but the end point is obtained only indirectly through the adjustment
coefficient. Tsurumi (1988) used this method to examine the stability
of the US demand for money function using quarterly data from 1959:11
to 1979:IV. As an estimate of the switch point he obtains 1973:1 for the
abrupt switching model and 1972:IV for the gradual switching model.
However, the estimate of the speed of adjustment he gets implies that
it takes roughly 26 quarters or 6.5 years to reach the second regime,
that is 1972:IV to almost the end of the sample period 1979:IV as an
adjustment period.
Ohtani and Katayama (1985) suggested a gradual switching regression
model where both the start point and end point are estimated but the
transition path is assumed to be a linear function of time. The transition
path is thus very restrictive as compared to the one used by Bacon and
Watts, and Tsurumi. However, in Ohtani et al. (1990) this procedure
is extended by assuming the transition path to be a polynomial of time,
the degree of the polynomial being decided by a model selection criterion
(such as Akaike's AIC or Schwarz's BIC).
The model considered by Ohtani et al. is
yt = x't(0 + \t6) + et, 
t = l , 2 , . . . , T
where
At 
= 
0, 
t<ti
= 
a0 + ait + a2t2 H 
\-antn, 
t1<t<t2
= 
1, 
t > t2
Since \t = 0 for t = t\ and A^ = 1 for t = t2, any two of the as can be
eliminated. (This is similar to the end-point restrictions in the Almon
lag.) We can eliminate a$ and a\. The unknown parameters are thus
/?, 6,a2,ti,t2,n 
and a2, as,..., an with two of the as eliminated, and
the number of unknown parameters is thus 2k + n + 3.
Ohtani et al. illustrate this with an estimation of structural change

468 
Regime switching models and structural time series models
in the import demand function for Japan following the second oil crisis.
The data used were quarterly 1975:I-1986:IV. They estimated the start
date of the structural change as 1978:111 (slightly earlier than the second
oil crisis) and that the adjustment to the new regime took about five
years. We have given only two empirical examples of gradual switching
models but both of them have yielded long lags in adjustment. Fur-
ther experimentation with alternative functional forms for adjustment
is needed to see whether these long lags are a statistical artifact. The
Bayesian analysis of the model in Ohtani et al. is, however, rather
complicated. The Bayesian analysis of the gradual switching model of
Tsurumi is discussed in detail in Broemling and Tsurumi (1987).
In addition to these methods there is the D method suggested by
Goldfeld and Quandt (1972) who consider the two-regime switching re-
gression model
Vt = oti + fi\xt + uu, 
1 < t < n0
Vt = ot2 + fcxt + u2u 
n0 
<t<n
where ui ~ im(0,of) for i = 1,2. Define an indicator variable Dt such
that if Dt = 0, then observations come from regime 1, and if Dt = 1, the
observations come from regime 2. The Quandt method assumes that the
indicator function Dt is a step function at t = n0. Goldfeld and Quandt
assume the switch to be gradual and that this is given by the indicator
Dt to be cumulative normal
where 3>(c) = Prob(x < c) with x ~ JV(O, l),/i gives the mean point
of the switch, and the lower a is, the more sudden the switch is. The
advantage of this method is that the distribution theory for /x is simpler
because one can use standard asymptotic results. We combine the two
equations as
Vt = ai(l - Dt) + a2Dt + [/?i(l " A) + ft%t + tiit(l - A) + u2tDt
and maximize the likelihood function for this model after substituting
the expression for Dt. A recent application of this method is in Varo-
ufakis and Sapsford (1991), where the methods of sudden switching pro-
duced anomalous results but the use of the D-method suggested that the
problem was the assumption of a sudden switch, rather than a smooth
transition to a second regime. The analysis with sudden shifts identified

15.7 A model with parameters following a random walk 
469
two switch points, 1967 and 1973, but the D-method suggested that
there was one switch which was gradual during 1967-1973.
For a Bayesian analysis of the gradual switching regression models,
the .D-method appears to be a very fruitful model to consider. There
is, as yet, no Bayesian analysis of this method. The other models that
incorporate gradual switching are the models with continuous parameter
variation. A Bayesian analysis of these models is discussed extensively
in West and Harrison (1989) and hence this area will not be pursued
here.
15.7 A model with parameters following a random walk
Another class of gradual switching models is the one where the parame-
ters follow a random walk. These models are also known as state-space
models.
Consider the model
yt 
= XtPt + ut
Pt = Pt-i+vt, 
t = l,...,n 
(15.5)
where
Substituting the second equation into the first yields
Vt — Xt(Pt-i + vt) +ut 
= XtPt-i + Xt(Pt-i — Pt-i) -
where Pt is the estimator of pt based on observations up to t and wt is the
one-step ahead prediction error. If var(Pt-i) = At_i, then var(wt) = ft
where ft = X^Xt-i + X?o~% + o\. Then maximizing the likelihood is
equivalent to maximizing
Note that this model can be written as an expanded regression as

470 
Regime switching models and structural time series models
follows
0 
Xi 
0 
••• 
0
0 
0 
X2 
-" 
0
2/1
V2
Vn
0
0
0
0 
0 
0
1 
- 1 
0
0 
1 
- 1
0 
0 
•••
xn
1 
- 1
V2
We can first estimate this equation by OLS, get a\ and o\ and then apply
GLS. This expanded regression approach can be cumbersome. However,
some have argued (see Snyder, 1990) that this does not pose a problem.
Snyder argues that although the expanded regression is large in size,
Page and Saunders (1977) have shown that the least squares problem
can be solved efficiently with orthogonalization methods, such as Givens
transformation (see Gentleman, 1973), which exploit the sparsity of the
associated design matrix.
The ML estimates of a\ and o^ should be identical to those obtained
from a representation of (15.5) in terms of the expanded regression model
(written in the dummy variable form). Also, the likelihood function is
conditional on (30 but it should be possible to estimate (30 as well, as can
be seen from the expanded regression model. The estimate of (3t based
on data up to t are obtained by estimating this expanded regression
using data up to and including t only. This can be done for all t. On the
other hand, if we use a regression with all the n observations, we get the
smoothed estimates (see section 15.9).
15.8 A general state-space model
We shall now consider a general state-space model, from which we can
derive many other models as special cases. The general model is
Measurement equation:
Transition equation:
yt = XtPt +
fit = Tt/3t-i
where yt is n x 1 vector, Xt is n x m matrix,
coefficients, and
Rt 
0
0 
Qt
£ = 1,..., n
is m x 1 vector of

15.8 A general state-space model 
471
In the usual applications of these models, we assume that Tt,Qt, and Rt
are known.
Define bt-\ as the GLS estimator of fa-\ using data at time t — 1
and bt\t-i as the predictor of fa using information up to t — 1. Let Vt—i
denote the variance of &t-i- Note the following
6t|t_i = Ttbt-i
is the prediction of fa based on the information up to time t — 1. Since
where wt = Tt{f3t-\ — &t-i) + Vt, we have the variance of the prediction
error
var(wt) = TtVt-iT[ + Qt
Denote this by I^|t-i- At time t, using the new observations, we have
two estimators for fa:
(i) (XiR^Xt^X^R^yt 
with variance {X[R^1Xt)-1
(ii) bt\t-i with variance Vt\t-\
We get the efficient GLS estimator as the matrix weighted average of
these estimators. The estimator bt and its variance are given by
h = VtiV^bt^! + X'tR^yt) 
(15.6)
Vt = [VtJt
1_1+X^1Xt]~1 
(15.7)
Suppose we follow a different route and use the recursive prediction
error method. Then we get a different set of equations. The one-step
ahead prediction error in yt is
Since we can write
yt = Xtbt\t-i + Xt(fa - bt\t-i) + ut
the variance of the prediction error is
Dt=XtVt\t-1X't 
+ Ri
Kalman's recurrence relations are
bt = bt\t-i + Vtit-iXlD^toH - *tfyt-i) 
(15.8)

472 
Regime switching models and structural time series models
This can also be written as
bt = &t|t_i + Gtut
where Gt — Vt\t-iXr
tD^1 is called the Kalman gain. It is the correction
term applied to the one-step ahead prediction error ut. Also
Vt = Vt\t-i ~ V^X'tD^XtV^ 
(15.9)
Note that the GLS eqations (15.6) and (15.7) are different from the
Kalman recursive equations in (15.8) and (15.9). However, we can show
their equivalence using the following two lemmas.
Lemma 15.1 IfV = (M^+X'R^X)-1, 
then V = M-MX'D~lXM
where D = R + XMX'.
The result can be checked by showing that V~1V = I. This shows that
the equations (15.7) and (15.9) are equivalent.
Lemma 15.2 Ifh = V[M~lbQ+X'R-ly\, 
thenbx =
Xbo).
Note that bx = [M - MX'D^XM^M^bo 
+ X'R^y] 
using lemma
15.1. This shows that the equations (15.6) and (15.8) are equivalent.
This proves the equivalence of Kalman's recurrence relation and GLS.
This elegant proof is due to Duncan and Horn (1972).
15.9 Derivation of the Kalman filter
To derive the Kalman filter given by equation (15.8) and (15.9) we shall
use the following fact based on normal regression theory. Suppose
V2 ) 
[A M2 ) ' V E21 £22
Then the conditional distribution of y\ given y2 is normal with mean
Mi — ^12^22(1^2 — M2) and variance En — E ^ E ^ ^ i .
We shall use this fact to derive the distribution of /3t conditional on
yt. First note that
Pt = bt\t-i + Pt 
-h\t-i
yt = Xtbyt-x + Xt(0t - fyt-i) + ut

15.9 Derivation of the Kalman 
filter 
473
Hence we have a multivariate normal distribution as follow
Vt J 
L V Xtbt\t-i 
) ' V ^tVt\t-i 
Dt
Taking an estimator of bt as the conditional mean E(fa\yt) and using
the above result we get the Kalman filter equation (15.8) and (15.9).
The above analysis is based on the assumption that E(fa\yt) is the
best estimator of f3 given the observations up to and including time
t. Also, the Kalman filter estimator of fa depends on the normality
assumption. If the disturbances in the state-space model are not nor-
mally distributed, it is no longer true, in general, that the Kalman filter
yields the conditional mean of the state vector. The computation of the
conditional mean when the disturbances are nonnormal is described in
section 3.7.3 of Harvey (1989). However, even if the disturbances are
not normal the estimator bt given by the Kalman filter is the minimum
mean-squared-error linear estimator of fa based on the observations up
to and including time t. Proof of this result can be found in Duncan and
Horn (1972) or Anderson and Moore (1979).
Smoothing
Consider the estimation of fa based on data up to and including ys. The
terminology used in state-space models is as follows:
s <t: prediction
s = t: filtering
s > t: smoothing
Another term used is signal extraction. This refers to estimating the
signal Xtfa. In many models the dimension of yt is less than that of fa.
In this case there is a more efficient way of estimating the signal. In
the case of prediction several steps ahead, the prediction equations are
used repeatedly but the updating equations (15.8) and (15.9) are not
used. As for smoothing one particular case of interest is s — t. In this
case we use all the sample data to derive estimates of fa. The expanded
regression model discussed in section 15.7 produces estimates of all the
fa using the entire set of n observations. When rational expectation
models are estimated in the state-space framework, the estimates of fa
have to be obtained by the filtering method because the assumption
underlying these models is that economic agents use all the information
currently available. In these models, getting the smoothed estimates of
fa does not make sense.

474 
Regime switching models and structural time series models
To get the smoothed estimates of (3t we use the backward recursions
starting with the final period estimates bn and Vn, which are obtained
by using the forward recursions given by equations (15.8) and (15.9).
There are two backward recursion methods for smoothing. The first,
given in Harvey (1989, section 3.6) and derived in Anderson and Moore
(1979, chapter 7) is as follows
h\n = h + V
Vt\n = Vt + VWt+Un ~ Vt+lit)Vt*
where
with bn\n = bn and Vn\n = Vn. The problem with this smoother is that
it involves inversion of (n — 1) matrices T^+i|t for £ = 1,2,..., n — 1. An
alternative smoother given by DeJong (1989) involves less computation.
It involves no other inversion than that of Dt, which anyhow is inverted
in the forward recusion of the Kalman filter.
Let us define
where Gt is the Kalman Gain defined earlier as Gt =
Define
— KtXt
Then the DeJong smoothing equations are
h\n = bt\t-i + Vt|t—i^*t—i
where rt satisfies the backward recursion
rt-i = X'tD^iit -f l!tru t = n,n-l,..., 1
with rn = 0. The Ut is the one-step ahead forecast error in ut. This
smoother is more elegant that the earlier one.
Note that in the expanded regression approach discussed in section
15.7 we estimate the /?$ from all the observations 1, ...,n. Thus, the es-
timates we have are the smoothed estimates. We can get the filtered
estimate of /3S by just estimating the expanded regression model us-
ing data t = 1,..., 5. Thus, to get the filtered estimates we have to run
many regressions. For the smoothed estimates, we have only one regres-
sion equation to estimate. Thus, in the expanded regression approach
smoothing is easier than filtering.

15.10 Harvey's structural time series model (1989) 
475
Why use the Kalman filter?
There have been some papers that trace the Kalman filter to much ear-
lier work. Sorenson (1970) suggests that the Kalman filter is not entirely
due to Kalman. Lauritzen (1981) traces the recursive least squares esti-
mation and prediction method to papers by a Danish statistician T.N.
Thiele in 1880 (long before Kalman was born). Snyder (1990) ques-
tions the usefulness of the Kalman filter. He argues that when properly
implemented, the regression approach not only involves lower computa-
tional loads than numerically stable versions of the Kalman filter, but
it is inherently simpler and more transparent. Steyn (1989) argues that
for some simple commonly used state-space models the asymptotic least
squares (ALS) method can be recommended in preference to the Kalman
filter.
15.10 Harvey's structural time series model (1989)
Consider
yt = z[3t + d + uu 
t = 1,2,..., n
where z is 1 x m vector of coefficients assumed to be time-invariant, fit
i s m x l vector, d is a time invariant scalar. The evolution of f3t is given
by
Pt=T/3t-i+vu 
* = l,2,...,n
where E(vt) = 0,E(vtv't) = Q,E{utvt) = 0,E(ut) = 0, and E(u2) = (T2
u.
Let bt-i be the estimator of fa-i obtained from observations up to
time (t — 1) and let its variance be denoted by Vt-\. Let us denote by
bt\t_i the predictor of fit obtained from bt-\. Then we have
6t|t-i = Tbt-i
Also if we denote the variance of the prediction error T(f3t-i — 6t—l) + vt
by Vt|t—i, then we have
Now consider the one-step ahead prediction error in yt. If we denote this
by Uf> we have
ut = yt- zbt\t-i ~ dt

476 
Regime switching models and structural time series models
and its variance, which we shall denote by Dt is given by
Dt=al + zVt\t-iz'
Note that Dt is a scalar. The ML estimates of the parameters in the
model are obtained by maximizing
To carry on the iterations, we also need the recursive relations for the
estimator bt of (3t and its variance Vt. These are given by
h = ot\t-i H 
-jj
and
Vt\txz'zVt\tx
Vt = Vt\t-ii
To simplify matters, consider the model
yt = nt + ut
l^t = Ht-i + It-i + m
It = 7t-i + Ct
This is a slowly evolving trend model.
Case I cr^ = a^ = 0,70 = 0. Then we get
2/t = Mo + ut
Case II tf = cr%= 0,70 ^ 0. Then we get
Vt = Mo + Jot + ut
which is a deterministic trend model.
Case III a^ ^ 0 or a^ ^ 0. Then the time series is difference-stationary.
The case a^ ^ 0 allows for a parallel shift in the linear time
trend, and the case a^ ^ 0 allows for a change in the slope of
the time trend.
The correspodence between this model and the previous one is given
by the following equations
& = [/** it]'

15.11 Further comments on structural time series models 
477
d = 0, 
z = [1 0]
o 
i j '
 
Q - [ o
15.11 Further comments on structural time series models
The estimation of structural time series models is discussed in Harvey
and Peters (1990). The method is maximum likelihood (ML) using the
Kalman filter. A computer program to do this (that does not need any
knowledge of the Kalman filter) is the STAMP program of Koopman et
al (1995).
There is one problem with the ML estimation of structural time series
models discussed in Shephard (1993). Consider the model
yt = f3'xt + ut
\H = IH-i + r}U 
rjt ~ /N(0, qa2)
Interest centers on q. If q = 0 we have a deterministic trend. If q > 0,
then the trend is a random walk.
Shephard (1993) argues that it is often observed that the ML esti-
mate of q is zero even when the true q > 0. This probability of getting
an estimate of q = 0 (implying a deterministic trend) is very sensitive
to the type of the likelihood function used as the basis of inference. He
considers two types of likelihood: profile likelihood (which, in economet-
ric terminology is concentrated likelihood) and marginal likelihood. For
profile likelihood, we maximize the likelihood function with respect to
the other parameters, and substitute the ML estimates to get the con-
centrated likelihood function L(q). In the marginal likelihood method,
we integrate out the other parameters and get the marginal likelihood
L*(q). In both cases we get an estimate of q by maximizing L(q) or
L*(q). Shephard argues that we get an estimate q = 0 more often with
profile likelihood than with marginal likelihood. Thus, the estimation
of the structural time series models should be done with marginal like-
lihood.
We shall not go into the details of the different problems with Kalman
filtering. These are dealt with in the book by Harvey and several papers
by him. He has presented several arguments for the use of the struc-
tural time series model. But the most scathing attack on traditional

478 
Regime switching models and structural time series models
time series modeling (with its emphasis on unit roots, VAR models, and
cointegration) is in Harvey (1997). We feel that this has a sobering effect
on current time series modeling (the unit root revolution, cointegration
revolution, and so on) discussed in the previous chapters. Here are some
quotes from Harvey (1997):
Since a deterministic trend is too restrictive, the obvious thing to do is to
make it more flexible by letting the level and slope parameters change over
time. (p. 192)
The problem with ARIMA class is that there are many models and parameter
values which have no sensible interpretation and give forecast functions which
may have undesirable properties, (p. 194)
Testing for unit roots has become almost mandatory in applied economics.
This is despite the fact that, much of the time, it is either unnecessary or
misleading or both. (p. 196)
Why worry about testing for unit roots in the first place? ... within the
structural framework deciding on the degree of integration is not crucial ...
very little is lost by starting off with a stochastic trend model which has the
deterministic slope and level as a special case. If there really is a need to test
whether a component can be treated as deterministic, it is best to use a test
that is not based on an autoregressive approximation, (pp. 196-197)
Trying to draw conclusion about whether the world is Keynesian or Neoclassi-
cal on the basis of the presence or otherwise of unit root is complete nonsense,
(p. 197)
To many econometricains, VAR stands for Very Awful Regression. (I am
indebted to Arnold Zellner for introducing me to this delightful terminology.)
(p. 199)
There are a number of reasons why my enthusiasm for VAR based cointegra-
tion methods is somewhat muted. The fundamental objection is that autore-
gressive approximation can be very poor, even if a large number of parameters
is useful, (p. 199)
... what have economists learnt from fitting such models? The answer is very
little. I cannot think of one article which has come up with a cointegrat-
ing relationship which we did not know about already from economic theory,
(p. 199)
The solution is to combine the flexibility of a time series model with the
interpretation of regression ... The recent emphasis on unit roots, vector
autoregression and cointegration has focused too much attention on tackling
uninteresting problems by flawed methods, (p. 200)

15.12 Summary and conclusions 
479
15.12 Summary and conclusions
The present chapter reviews the literature on regime switching models
that have been used for identifying break points in time series. It also
reviews the work on another alternative: outlier analysis. A major con-
cern of these studies has been the consequence of structural breaks on
inference about unit roots in macroeconomic time series. Another objec-
tive has been the testing of some economic theories within the context
of a flexible nonlinear framework. As far as prediction goes, most of the
papers have concentrated on within-sample predictions and compared
them with simple random walk models (or in some cases ARIMA mod-
els). The out-of-sample predictions have not been investigated in detail,
but the meager evidence available suggests that it is not encouraging.
A major defect of the regime switching models in use, particularly the
Markov switching regression (MSR) models (often termed the Hamilton
model, though the terminology is unfortunate) is that they are simple
trend change models with no explanatory variables. There have been
some recent attempts to include explanatory variables in the MSR mod-
els, but more work needs to be done. The outlier models, on the other
hand, start with an ARIMA model and then identify the outliers through
residual analysis. These models again do not have any explanatory vari-
ables. Also the outlier models are less flexible than the regime switching
models in this respect.
The switching regression models are based on sudden switches. In
practice, the switch is gradual rather than sudden. Hence some gradual
switching models are also reviewed in this chapter.
Another class of models that have been found useful is the set of mod-
els with continuous parameter variation rather than sudden or gradual
switches. These models are also called state-space models. An exam-
ple of this is Harvey's structural time series model which is estimated
using the Kalman filter. In this chapter we present a brief review of
this model and explain the Kalman filter and its limitations. We also
review Harvey's criticism of the current emphasis on unit roots, vector
autoregressions, and cointegration in time series modeling.
References
Albert, J. and S. Chib (1993), "Bayesian Inference via Gibbs Sam-
pling of Autoregressive Time Series Subject to Markov Mean

480 
Regime switching models and structural time series models
and Variance Shifts," Journal of Business and Economic Statis-
tics, 11, 1-15.
Anderson, B.D.O. and J.B. Moore (1979), Optimal Filtering, Prentice
Hall, New York.
Bacon, D.W. and D.G. Watts (1971), "Estimating the Transition be-
tween Two Intersecting Lines," Biometrika, 58, 525-534.
Baum, L. E., T. Petrie, G. Soules, and N. Weiss (1970), "A Maximiza-
tion Technique Occurring in the Statistical Analysis of Proba-
bilistic Functions of Markov Chains," Annals of Mathematical
Statistics, 41, 164-171.
Bekaert, G. and R.J. Hodrick (1993), "On Biases in the Measurement
of Foreign Exchange Risk Premiums," Journal of International
Money and Finance, 12, 115-138.
Broemling, L.D. and H. Tsurumi (1987), Econometrics and Structural
Change, Marcel Dekker, New York.
Casella, G. and E. I. George (1992), "Explaining the Gibbs Sampler,"
The American Statistician, 46, 167-174.
Cecchetti, S.G., P.S. Lam, and N.C. Mark (1990), "Mean Reversion
in Equilibrium Asset Prices," American Economic Review, 80,
398-418.
Cosslett, S. R. and L. Lee (1985), "Serial Correlation in Discrete Vari-
able Models," Journal of Econometrics, 27, 79-97.
DeJong, P. (1989), "Smoothing and Interpolation with the State-Space
Model," Journal of the American Statistical Association, 84,
1085-1088.
Diebold, F.X., J.H. Lee, and G.C. Weinbach (1994), "Regime Switching
with Time-Varying Transition Probabilities," in C. Hargreaves
(ed.), Non-Stationary Time Series Analysis and Cointegration,
Oxford University Press, Oxford.
Duncan, D.B. and S.D. Horn (1972), "Linear Dynamic Recursive Esti-
mation from the Viewpoint of Regression Analysis," Journal of
the American Statistical Association, 67, 815-821.
Durland, J.M. and T.H. McCurdy (1992), "modeling Duration Depen-
dence in Cyclical Data Using a Restricted Semi-Markov Pro-
cess," Manuscript, Queens University.
Engel, C. (1994), "Can the Markov Switching Model Forecast Exchange
Rates?" Journal of International Economics, 36, 151-165.
Engel, C. and J.D. Hamilton (1990), "Long Swings in the Dollar: Are
They in the Data and Do Market Know It?" American Eco-
nomic Review, 80, 689-713.

References 
481
Evans, M.D.D. (1993), "Estimating General Markov Switching Mod-
els," Manuscript, Stern School of Business, New York Univer-
sity.
Filado, A.J. (1994), "Business Cycle Phases and Their Traditional Dy-
namics," Journal of Business and Economic Statistics, 12, 299-
308.
Pranses H. and R. Rapp (1996), "Does Seasonal Adjustment Change
Inference from Markov Switching Models?" Report 9615/A,
Econometric Institute, Erasmus University, Rotterdam.
Garcia, R. (1997), "Asymptotic Null Distribution of the Likelihood Ra-
tio Test in Markov Switching Models," International Economic
Review, Forthcoming.
Garcia, R. and P. Perron (1996), "An Analysis of Real Interest Rate
under Regime Shifts," Review of Economics and Statistics, 78,
111-125.
Gelfand, A.E. and A.F.M. Smith (1990), "Sampling-Based Approaches
to Calculating Marginal Densities," Journal of the American
Statistical Association, 85, 398-409.
Gentleman, W.M. (1973), "Least Squares Computations by Givens
Transformations with Square Roots," Institute of Mathemati-
cal Applications, 12, 329-336.
Goldfeld, S.M. and R. Quandt (1972), nonlinear Methods in Economet-
rics, North-Holland Publishing Co., Amsterdam.
(1973), "A Markov Model for Switching Regressions," Journal of
Econometrics, 1, 3-16.
Golodwin, T.H. (1993), "Business Cycle Analysis with a Markov
Switching Model," Journal of Business and Economic Statis-
tics, 11, 331-339.
Gurland, J.M. and T.H. McCurdy (1992), "modeling Duration De-
pendence in Cyclical Data Using a Restricted Semi-Markov
Process," Manuscript, Queen's University, Kingston, Ontario,
Canada.
Hamilton, J.D. (1988), "Rational Expectations Econometric Analysis
of Changes in Regime," Journal of Economic Dynamics and
Control, 12, 385-423.
(1989), "A New Approach to the Economic Analysis of Nonstationary
Time Series and the Business Cycle," Econometrica, 57, 357-
384.
(1990), "Analysis of Time-Series Subject to Changes in Regime,"
Journal of Econometrics, 45, 39-70.

482 
Regime switching models and structural time series models
(1992), "Estimation, Inference, and Forecasting of Time Series Sub-
ject to Changes in Regime," in G.S. Maddala, C.R. Rao, and
H.D. Vinod (eds.), Handbook of Statistics, vol. 11, North-
Holland, Amsterdam, chapter 9.
Hamilton, J.D. and R. Susmel (1994), "Autoregressive Conditional Het-
eroskedasticity and Changes in Regime," Journal of Economet-
rics, 64, 307-333.
Harvey, A.C. (1989), Forecasting, Structural Time Series Models and
the Kalman Filter, Cambridge University Press.
(1997), "Trends, Cycles, and Autoregressions," The Economic Jour-
nal, 107, 192-201.
Harvey, A.C. and S.C. Peters (1990), "Estimation Procedures for Struc-
tural Time Series Models," Journal of Forecasting, 9, 89-108.
Kaminsky, G. (1993), "Is there a Peso Problem? Evidence from Dol-
lar/Pound Exchange Rate," American Economic Review, 83,
450-472.
Kim, B.H. (1994), "A Study of Risk Premiums in the Foreign Exchange
Market," unpublished Ph.D. dissertation, The Ohio State uni-
versity.
Kim, B.H. and J.H. Lee (1996), "Can the Markov Switching Model
with Time-Varying Probabilities Forecast Exchange Rates?"
Manuscript, Inje University, Korea.
Kim, B.H. and B.C. Yu (1996), "On Time-Varying Coefficients Im-
prove the Empirical Performance of Monetary Exchange Rate
Determination Models?" Manuscript, Inje university, Korea.
Kim, I.M. (1993), "A Dynamic Programming Approach to the Esti-
mation of Markov Switching Regression Models," Journal of
Statistical Computation and Simulation, 45, 61-76.
Kim, I.M. and G.S. Maddala (1992), "Multiple Structural Breaks and
Unit Roots in Exchange Rates," Paper presented at the Econo-
metric Society Meeting at New Orleans.
Koopman, S.J., A.C. Harvey, J.D. Doornik, and N. Shephard (1995),
STAMP 5.0 Structural Time Series Analysis, modeler and Pre-
dictor, Chapman and Hall, London.
Lahiri, K. and J.G. Wang (1994), "Predicting Cyclical Turning Points
with Leading Index in a Markov Switching Model," Journal of
Forecasting, 13, 243-263.
Lam, P.S. (1990), "The Hamilton Model with a General Autoregressive
Component: Estimation and Comparison with Other Models of

References 
483
Economic Time Series," Journal of Monetary Economics, 26,
409-432.
Lauritzen, S.L. (1981), "Time Series Analysis in 1880: A Discussion of
Contributions Made by T.N. Thiele," International Statistical
Review, 49, 319-331.
Lee, J.H. (1991), "Non-Stationary Markov Switching Models of Ex-
change Rates: The Pound-Dollar Exchange Rate," Ph.D. dis-
sertation, University of Pennsylvania.
Lee, L.F. and R.H. Porter (1984), "Switching Regression Models with
Imperfect Sample Separation Information," Econometrica, 52,
391-418.
Lindgren, G. (1978), "Markov Regime Models for Mixed Distributions
and Switching Regressions," Scandinavian Journal of Statistics,
5, 81-91.
Maddala, G.S. (1983), Limited-Dependent and Qualitative Variables in
Econometrics, Cambridge University Press.
Makov, U.E. and A.F.M. Smith (1977), "A Quasi-Bayes Unsupervised
Learning Procedure for Priors," IEEE Transactions on Infor-
mation Theory, IT-23, 761-764.
McCulloch, R.E. and R.S. Tsay (1994a), "Statistical Analysis of Eco-
nomic Time Series via Markov Switching Models," Journal of
Time Series Analysis, 15, 523-539.
(1994b), "Bayesian Inference of Trend- and Difference-Stationarity,"
Econometric Theory, 10, 596-608.
Ohtani, K. and S. Katayama (1985), "An Alterntive Gradual Switch-
ing Regression Model and Its Applications," Economic Studies
Quarterly, 36, 148-153.
Ohtani, K., S. Kakimoto, and K. Abe (1990), "A Gradual Switching
Model with a Flexible Transition Path," Economics Letters, 32,
43-48.
Pagan A.R. and G.W. Schwert (1990), "Alternative Models for Condi-
tional Stock Volatility," Journal of Econometrics, 45, 267-290.
Page, C.C. and M.A. Saunders (1977), "Linear Least Squares Esti-
mation of Discrete Linear Dynamical Systems Using Orthogo-
nal Transformations," SIAM Journal of Numerical Analysis, 15,
180-193.
Perron, P. (1989) "The Great Crash, the Oil Price Shock and the Unit
Root Hypothesis," Econometrica, 57, 1361-1401.
Quandt, R. (1958), "The Estimation of the Parameters of a Linear

484 
Regime switching models and structural time series models
Regression System Obeying Two Separate Regimes," Journal
of American Statistical Association, 53, 873-880.
Rabault, G. (1992), "Une Application du Modele de Hamilton a
L'Estimation des Cycles Economiques" (in French), Discussion
paper No. 9210, INSEE, Paris, France.
Raymond, J.E. and R.W. Rich (1992), "Changes in Regime and the
Behavior of Inflation," Manuscript, Vanderbilt University.
Rich, R.W. and J.E. Raymond (1992), "Oil and the Macro-economy:
A Markov State Switching Approach," Manuscript, Vanderbilt
University.
Sclove, S. L. (1983), "Time-Series Segmentation: a Model and a
Method," Information Sciences, 29, 7-25.
Shephard, N. (1993), "Maximum Likelihood Estimation of Regression
Models with Stochastic Trend Components," Journal of Amer-
ican Statistical Association, 88, 590-601.
Snyder, R.D. (1990), "Why Kalman Filter?" Working paper No.11/90,
Monash University.
Sorenson, H.W. (1970), "Least Squares Estimation from Gauss to
Kalman," IEEE Spectrum, 7, 63-68.
Steyn, I.J. (1989), "ALS Estimation of Parameters in a State Space
Model," Institute of Actuarial Science and Econometrics Report
AE 2/89, University of Amsterdam.
Titterington, D.M., A.F.M. Smith, and U.E. Makov (1985), Statistical
Analysis of Finite Mixture Distributions, John Wiley and Sons,
New York.
Tsurumi, H. (1983), "A Bayesian and Maximum Likelihood Analysis
of a Gradual Switching Regression Model with Sampling Exper-
iments," Economic Studies Quarterly, 34, 237-248.
(1988), "A Survey of Bayesian and Non-Bayesian Testing of Model
Stability in Econometrics," in J.C. Spall (ed.), Bayesian Anal-
ysis of Time Series and Dynamic Models, Marcel Dekker, New
York.
Turner, CM., R. Startz, and C.R. Nelson (1989), "A Markov Model
of Heteroskedasticity, Risk and Learning in the Stock Market,"
Journal of Financial Economics, 25, 3-22.
Tyssedal, J.S. and D. Tj0stheim (1988), "An Autoregressive Model
with Suddenly Changing Parameters and an Application to
Stock Market Prices," Applied Statistics, 37, 353-369.
Van Norden, S. (1994), "Regime Switching as a Test for Exchange Rate
Bubbles," Manuscript, Bank of Canada.

References 
485
Varoufakis, Y. and D. Sapsford (1991), "Discrete and Smooth Switch-
ing Regressions for Australian Labor Productivity Growth," Ap-
plied Economics, 23, 1299-1304.
West, M. and J. Harrison (1989), Bayesian Forecasting and Dynamic
Models, Springer Verlag, New York.

16
Future directions
A chapter on future directions will have several subjective elements.
Here we list topics that should be pursued in the future and also men-
tion those topics that should not be pursued in the future.
Nonlinear models
Throughout the book we considered linear models only. Obviously all
the topics discussed earlier need to be extended to nonlinear models.
Thus, nonlinear error correction, nonlinear cointegration, and nonlinear
structural change are possible topics. Some references here are: Granger
(1995), Granger and Swanson (1996, 1997), Granger and Terasvirta
(1993), and Granger, Inoue, and Norin (1997).
Bootstrap methods
There is currently too much asymptotic theory. Significance levels ob-
tained from asymptotic distributions have been found to be misleading
unless samples are very large. As reviewed in chapter 10, bootstrap
methods are promising in making inferences with moderate-sized sam-
ples. More work needs to be done in this direction.
Robust methods
In chapter 14, we discussed robust unit root tests (section 14.5) and
robust estimation methods for cointegrating regressions (section 14.6).
Given that many error distributions are nonnormal, more developments
of robust methods will be practically useful. Lucas (1995) develops tests
for cointegration using pseudo-likelihood methods.
Pre-testing problems
The problems with unit roots and cointegration analysis within the con-
486

Future directions 
487
text of linear models are far from solved. Thus, future developments
in nonlinear models (as suggested by Granger and Swanson 1996, 1997)
maybe premature. One important question that has not even been asked
is the appropriate significance levels at which the unit root tests should
be applied, when they are pre-tests - prelude to cointegration analysis.
A similar pre-testing problem occurs when a test is used to determine
the number of cointegrating vectors in the Johansen procedure and then
subsequently the coefficients of the estimated cointegrating vectors are
tested. More work needs to be done in this important area.
Bayesian methods
The Bayesian methods (reviewed in chapter 8) argue that the commonly
used procedure of applying a unit root test and behaving as if the se-
ries is a unit root process leads to misleading inferences. There is a
nonzero probability that the process is a stationary process. In this re-
spect the Bayesian methods are on a sounder footing than the classical
methods because the posterior probabilities of the process being a unit
root process and a stationary process are taken into account. There
are still many open issues particularly regarding the appropriate priors
for VAR models and cointegration analysis. The priors used for mul-
tivariate regression models are not appropriate here because the VAR
model is different from a multivariate regression model with exogenous
explanatory variables. Also, the Bayesian approach to the pre-testing
problem in cointegration analysis needs to be investigated.
Structural time series models
Harvey (1997) has vehemently attacked the whole approach to unit root
testing and offers structural time series models as an alternative. Harvey
and Koopman (1996) discuss multivariate structural time series models.
Structural time series models offer a promising alternative to the unit
root route. One often gets the feeling that the unit root mania has gone
too far. The concepts of unit roots and cointegration have acquired a
life of their own because the mathematics is fascinating and many just
get carried away without any thinking about why we are doing what we
are doing. It is time to ask the questions: why use unit root tests and
why do cointegration analysis?
An important problem that has received attention recently and on
which more work needs to be done is that of analyzing structural breaks
within the framework of state-space models. Some references here are:
McCulloch (1997) and Iyer and Andrews (1997).

488 
Future directions
Identification problems
Without feeding in some economic information, the cointegrating vectors
do not have any economic interpretation. The starting point is a VAR
(which is a-theoretical) and cointegration is a purely statistical concept.
As we discussed in chapter 4, there is the issue of at what stage we feed
in economic information, at the beginning by starting with a structural
system rather than by VAR, or after determining the cointegration space
as done by Johansen. The issues in identification need more detailed
study.
The frequent procedure of doing cointegration analysis without dis-
cussing the identification problems has prompted Wickens (1995, p.
1645) to say that
Far from uncovering long-run relations, it can be shown that cointegration
analysis is more likely to obscure them.
As we discussed earlier in chapter 6 not all cointegrating relationships
can be interpreted as long-run relationships and any linear combinations
of cointegrating vectors is also a cointegrating relationship.
Miscellaneous other problems
There have been other avenues in which unit roots and cointegration
have been extended, e.g., stochastic unit roots by Granger and Swanson
(1997) and multicointegration by Granger and Lee (1990) which has been
applied to study present value relationships by Engsted et al. (1995).
More such extensions will be forthcoming in the future. It remains to
be seen what more comes out of all these extensions.
What is not needed
What we do not need is more unit root tests (each of which uses the
Nelson-Plosser data as a guinea pig).
References
Engsted, T., J. Gonzalo, and N. Haldrup (1995), "Multicointegration
and Present Value Models," Manuscript, Aarhus University,
Denmark.
Granger, C.W.J. (1995), "Modeling nonlinear relationships Between
Extended Memory Variables," Econometrica, 63, 265-279.
Granger, C.W.J. and T.H. Lee (1990), "Multicointegration," in G.F.

References 
489
Rhodes and T.B. Fomby (eds.), Advances in Econometrics, 8,
JAI Press, Greenwich, Conn., 71-84.
Granger, C.W.J. and N.R. Swanson (1996), "Future Dvelopments in
the Study of Cointegrated Variables," Oxford Bulletin of Eco-
nomics and Statistics, 58, 537-553.
(1997), "An Introduction to Stochastic Unit Root Processes," Jour-
nal of Econometrics, 80, 35-62.
Granger, C.W.J. and T. Terasvirta (1993), Modeling nonlinear Eco-
nomic Time Series, Oxford University Press, Oxford.
Granger, C.W.J., T. Inoue, and N. Norin (1997), "Nonlinear Stochastic
Trends," Journal of Econometrics, Forthcoming.
Harvey, A.C. (1997), "Trends, Cycles, and Autoregressions," The Eco-
nomic Journal, 107, 192-201.
Harvey, A.C. and S.J. Koopman (1996), "Multivariate Structural Time
Series Models," in C. Heij et al. (eds.), Systematic Dynamics in
Economic and Financial Models, John Wiley, New York.
Iyer, S. and R. L. Andrews (1997), "Forecasting with Multi-Regime
Structural Time Series Models: An Application to Nominal In-
terest Rates," Working paper 97-07, Department of Economics,
University of Delaware.
Lucas, A.,(1995), "Cointegration Testing Using Pseudo-Likelihood Ra-
tio Tests," Discussion paper TI 95-202, Tinbergen Institute,
Rotterdam.
McCulloch (1997), "Structural Shifts within the Context of State-
Space Models," Manuscript, Department of Economics, The
Ohio State University.
Wickens, M.R. (1995), "Real Business Cycle Analysis: A Needed Rev-
olution in Macroeconometrics," The Economic Journal, 105,
1637-1648.

Appendix 1
A brief guide to asymptotic theory
As we mentioned in the introduction we omitted completely asymptotic
theory in our discussion saying that it can be found in the respective
papers cited. However, the following references give the basics of asymp-
totic theory needed in most cases. For more specialized results, the
respective papers have to be consulted.
The papers by Chan and Wei (1987), Phillips (1987a), and Chan
(1988) are on local to unity asymptotics.
Andrews, D.W.K. (1993), "An Introduction to Econometric Applica-
tions of Empirical Process Theory for Dependent Random Vari-
ables," Econometric Reviews, 12, 183-216.
Billingsley, P. (1968), Convergence of Probability Measures, Wiley and
Sons, New York.
Chan, N.H. (1988), "On the Parameter Inference for Nearly Nonsta-
tionary Time Series," Journal of the American Statistical Asso-
ciation, 83, 857-862.
(1989), "Asymptotic Inference for Unstable Autoregressive Time Se-
ries with Drifts," Journal of Statistical Planning and Inference,
23, 301-312.
Chan, N.H. and C.Z. Wei (1987), "Asymptotic Inference for Nearly
Nonstationary AR(1) Processes," The Annals of Statistics, 15,
1050-1063.
(1988), "Limiting Distributions of Least Squares Estimates of Un-
stable Autoregressive Processes," The Annals of Statistics, 16,
367-401.
Johansen, S. (1995), Likelihood Based Inference in Cointegrated Vector
Autoregressive Models, Oxford University Press, Oxford.
490

A brief guide to asymptotic theory 
491
Park, J.Y. and P.C.B. Phillips (1988), "Statistical Inference in Re-
gressions with Integrated Processes I," Econometric Theory, 4,
468-497.
(1989), "Statistical Inference in Regressions with Integrated Processes
II," Econometric Theory, 5, 95-131.
Phillips, P.C.B. (1986), "Understanding Spurious Regressions in Econo-
metrics," Journal of Econometrics, 33, 311-340.
(1987a), "toward a Unified Asymptotic Theory for Autoregression,"
Biometrika, 74, 535-547.
(1987b), "Time Series Regression with a Unit Root," Econometrica,
55, 277-301.
Phillips, P.C.B. and V. Solo (1992), "Asymptotics for Linear Pro-
cesses," The Annals of Statistics, 20, 971-1001.
Tanaka, K. (1996), Time Series Analysis: Non-Stationary and Non-
Invertible Distribution Theory, John Wiley, New York.

Author index
Abadir, K. M., 140, 144-146
Abe, K., 483
Abraham, B., 437, 438, 449, 450
Abramovich, L., 311, 336
Abuaf, N., 241, 242
Agiakoglou, C, 77, 93, 101, 146
Ahn, S. K., 168, 187, 191, 192, 196, 225
Albert, J., 461, 462, 480
Algoskoufis, G., 35, 42, 192
Alt, R., 393, 421, 423
Amano, R. A., 125, 126, 147
Anderson, B. D. O., 473, 474, 480
Anderson, R. L., 61, 93
Anderson, T. W., 3, 6, 87, 93, 116, 147,
168, 192
Andrews, D. W. K., 64, 97, 141-144,
147, 277, 292, 322, 336, 391,
394-397, 401, 402, 409, 414,
418, 424, 490
Andrews, R. L., 487, 489
Ansley, C. F., 19, 42, 93
Arellano, C, 120, 125, 147
Atkinson, A. C, 438, 449, 450
Backus, D. K., 303, 306
Bacon, D. W., 467, 480
Bai, J., 398, 417, 418
Baillie, R. T., 234, 235, 242, 300,
302-306
Balke, N. S., 190, 192, 425, 428-430,
434, 439, 449, 450
Banerjee, A., 37, 42, 157, 162, 176-178,
192, 203-205, 242, 251, 252,
258, 397, 401, 402, 408, 409,
418
Bardsen, G., 36, 42
Barnard, G. A., 309, 337
Basawa, I. V., 325, 326, 336, 337
Baum, L. E., 458, 480
Bauwens, L., 288-290, 292
Beaulieu, J. J., 367, 383
Bekaert, G., 465, 480
Bell, W. R., 91, 94, 372, 383
Beltratti, A., 237, 244
Ben-David, D., 401, 409, 418
Beran, J., 300, 306, 447, 450
Beran, R., 312, 313, 315, 337
Berger, J. O., 277, 278, 284, 292
Bernardo, J. M., 284, 292, 293
Beveridge, S., 32, 42, 101, 147
Bewley, R., 36, 37, 42, 164, 169-171,
181, 182, 192, 211, 222-225,
242, 243, 248, 254
Bhargava, A., 38-40, 42, 70, 82-86, 88,
92, 93, 97, 105, 115, 116, 118,
152, 159, 196, 325, 337
Bianchi, M., 407, 418
Bickel, P. J., 311, 337
Bierens, H. J., 125, 140, 147
Billingsley, P., 54, 93, 490
Birchenhall, C. R., 386
Blanchard, O. J., 93, 193
Blough, S. R., 102, 103, 147
Bollerslev, T., 234, 235, 242, 302, 304,
306
Bossaerts, P., 169, 181, 192, 222, 223,
243
Boswijk, H. P., 175, 192, 205, 215, 219,
243, 334, 381, 383
Box, G. E. P., 3, 4, 6, 8, 17-19, 42, 43,
47, 93, 140, 165, 169, 173,
181, 182, 184, 191, 192, 211,
222, 230, 334, 336, 342, 344,
362, 365, 371, 383
Breitung, J., 119, 120, 134, 147
Breusch, T. S., 37, 44, 197, 254
Broemling, L. D., 468, 480
Brown, R. L., 392, 393, 418
Bruce, A. G., 438, 450
Burke, S. P., 127, 128, 147, 206
492

Author index
493
Burridge, P., 140, 147
Burrows, P. M., 419
Campbell, J. Y., 73, 90, 91, 93, 168,
169, 175, 192, 193, 196, 202,
212, 217, 233, 243, 247, 409,
416, 418
Campos, J., 193, 410, 419
Canova, F., 365, 366, 370, 383
Cantrell, R. S., 391, 419
Cappuccio, N., 182, 193
Carlstein, E., 328-332, 337
Carvalho, J. L., 372, 386
Casella, G., 277, 292, 462, 480
Cavanagh, C. L., 255-258
Cecchetti, S. G., 87, 93, 459, 480
Chan, N. H., 66, 93, 490
Chang, Y., 352, 360
Chen, C, 395, 419, 439, 450
Chen, D., 142, 144, 147
Cheung, Y., 134, 147, 216, 225, 243,
302, 303, 306
Chib, S., 461, 462, 480
Chiu, A. P. L., 386
Choi, I., 106, 107, 109, 110, 117, 118,
120, 125-127, 129-131, 148,
206, 207, 247
Chow, G. C, 390, 391, 394, 396, 419
Christiano, L. J., 419
Chu, C. S., 397, 419
Chung, A., 438, 449
Chung, B., 130, 148
Clements, M. P., 186, 193
Cochrane, J. H., 73, 86-88, 91, 93, 101,
102, 127, 139, 146, 148, 298,
299, 306
Cook, R. D., 437, 439, 450
Copeland, L. S., 241, 246
Corbae, D., 408, 409, 419
Cordero, M., 439, 450
Cosslett, S. R., 456-458, 465, 480
Cox, D. D., 443, 444, 450
Cramer, H., 131, 148
Crato, N., 300, 303, 306
Cuthbertson, K., 237, 243
Davidson, J. E. H., 35, 43, 94, 362, 383
Davies, R. B., 394, 419
Davison, A. C, 329, 337
Dawid, A. P., 292
de Lima, P. J. F., 300, 306
DeJong, D. N., 79, 82, 94, 100, 148,
268-270, 278, 292, 294, 403,
419
DeJong, P., 474, 480
Delampady, M., 278, 292
DiCiccio, T., 312, 313, 337
Dickey, D. A., 39, 43, 47, 61-63, 65-67,
71-77, 79, 82, 90-92, 94, 97,
100, 101, 104-107, 109, 112,
113, 119, 127, 131, 133, 148,
150, 152, 203, 223, 231, 298,
306, 325-328, 343-348, 359,
366, 384, 400, 409, 430, 432
Diebold, F. X., 129, 148, 298, 302-304,
306, 365, 384, 395, 419, 465,
480
Dolado, J. J., 42, 176, 192, 246, 258
Domowitz, L, 80, 97
Donald, J. J., 242
Donald, S. G., 426, 437, 439, 450
Donsker, M. D., 54, 55, 61, 94
Doornik, J. D., 482
Dropsy, V., 409, 419
Dueker, M., 302, 303, 305, 307
Dufour, J. M., 322, 337, 391, 414, 419
Duncan, D. B., 472, 473, 480
Durbin, J., 392, 418, 439, 450
Durland, J. M., 464, 480
Eddy, W. F., 278, 292
Edison, H. J., 228, 243
Efron, B., 309-314, 318, 333, 338, 339,
341
Eitrheim, O., 182, 193, 215, 243
Elliott, G., 99, 113, 134, 148, 193, 219,
221, 222, 230, 231, 233, 243,
244, 254, 255, 258, 259, 371,
444, 450
Engel, C, 459-461, 463, 466, 481
Engle, R. F., 40, 41, 43, 97, 153, 157,
162, 169, 176, 178, 181, 185,
186, 190, 193, 197-199, 202,
205, 215, 220, 221, 224, 234,
235, 241, 244, 246, 366, 375,
376, 384, 385, 410
Engsted, T., 488
Entorf, H., 29, 33, 43, 180, 181, 193
Ericsson, N. R., 163, 193, 246, 356, 359,
410, 419
Escribano, A., 193
Evans, G. B. A., 61, 64, 94
Evans, J. M., 392, 418
Evans, M. D. D., 465, 481
Fachin, S., 334, 338
Fair, R. C, 391, 418
Ferretti, N., 325, 326, 338
Fiebig, D. G., 438, 451
Filado, A. J., 465, 481
Fisher, L. A., 192
Fisher, R. A., 134, 137, 149, 230
Fisher, S., 93, 193
Fomby, T. B., 152, 190, 192, 194, 196,

494
Author index
247, 292, 419, 425, 428-430,
434, 439, 449, 450, 453
Fox, A. J., 426, 437, 439, 451
Fox, R., 300-302, 307
Franses, P. H., 205, 215, 219, 241, 243,
244, 334, 362, 367, 368, 370,
371, 379-381, 383, 384, 428,
431, 432, 439, 445, 447, 448,
451, 466, 481
Freedman, D. A., 311, 318-320, 333,
335, 337, 338
Friedman, M., 63, 94
Fuller, W. A., 3, 7, 39, 43, 47, 61-63,
65-67, 71-77, 79, 81, 82, 92,
94, 99-101, 104-107, 109,
112, 113, 127, 148-152, 203,
223, 231, 244, 298, 306,
325-328, 342-344, 346-348,
359, 366, 384, 400, 409, 430,
432
Gagnon, J. E., 243
Galbraith, J. W., 42, 192, 258
Garbade, K., 393, 419
Garcia, R., 397, 398, 408, 420, 454, 463,
481
Gardeazabal, J., 304, 306
Geisser, S., 278, 292
Gelfand, A. E., 405, 420, 462, 481
Gentleman, W. M., 470, 481
George, E. L, 462, 480
Geweke, J., 91, 95, 267, 270, 288, 289,
293, 300, 305, 307, 421
Ghysels, E., 362, 364, 365, 377, 379,
384, 385, 414, 419, 420
Gil-Alana, L. A., 300, 305, 307
Goldfeld, S. M., 455-457, 468, 481
Gong, G., 333, 338
Gonzalez-Farias, G., 151
Gonzalo, J., 101, 149, 170, 171, 173,
181, 182, 191, 193, 216, 220,
221, 244, 488
Goodwin, T. H., 466, 481
Gould, J. P., 63, 94
Granger, C. W. J., 3, 7, 26, 28, 29, 35,
40, 41, 43, 157, 162, 169, 176,
178, 181, 185, 186, 188-190,
193, 194, 198, 202, 203, 205,
215, 220, 221, 224, 234, 235,
241, 244, 246, 296, 307, 366,
375, 376, 379-382, 384, 385,
410, 486-489
Gregoir, S., 348, 359
Gregory, A. W., 216, 218, 219, 227, 244,
314, 339, 410, 412, 420
Grether, D. D., 372, 386
Grilli, V., 237, 244
Guerre, E., 140, 147
Guo, S., 125, 147
Gurland, J. M., 481
Hakkio, C. S., 229, 234, 236, 244
Haldrup, N., 252, 259, 346-351, 356,
359, 428, 431, 432, 451, 488
Hall, A., 78, 79, 94, 104-106, 149, 151,
164, 391, 419
Hall, P., 54, 94, 309, 311-313, 315, 316,
323, 327, 329-331, 337, 339
Hamilton, J. D., 48, 94, 397, 457-466,
479, 481-484
Hampel, H. R., 443, 451
Hansen, B. E., 161, 162, 177-179, 184,
194, 196, 202, 203, 231, 232,
241, 244, 246, 256, 366, 370,
383, 394, 396-398, 411, 412,
414, 420, 445
Hansen, H., 413, 414, 420
Hardle, W., 318, 339
Hargreaves, C., 96, 183, 194, 244, 385
Harris, D., 171, 172, 182, 194, 206, 209,
210, 226, 244
Harris, R., 194
Harris, R. I. D., 328, 339
Harrison, J., 439, 453, 469, 485
Hartigan, J. A., 309, 310, 312, 339
Harvey, A. C., 117, 122, 149, 366, 373,
379, 385, 454, 473, 474,
477-479, 482, 487, 489
Hasan, M. N., 442, 445, 451
Hassett, K., 106, 149
Hassler, U., 298, 307
Hasza, D. P., 112, 148, 342, 343, 346,
347, 359, 366, 384
Haug, A. A., 218, 219, 244
Hendry, D. F., 3, 7, 35, 42-44, 157, 162,
163, 165, 176-180, 186,
192-194, 258, 356, 359, 362,
383, 407, 410, 419, 420, 428,
430, 451
Henricsson, R., 126, 149
Herce, M. A., 442, 444, 451
Herrndorf, N., 54, 95
Heyde, C. C., 54, 94
Hillmer, S. C., 372, 383
Hinkley, D. V., 315, 339
Ho, M. S., 220, 245
Hoa, K., 411, 412, 420
Hodges, J., 277, 293
Hodrick, R. J., 465, 480
Hoek, H., 442, 444, 445, 447, 451
Holbert, D., 397, 404, 420
Hooker, M. A., 228, 229, 245
Horn, S. D., 472, 473, 480

Author index
495
Horowitz, J. L., 309, 314, 315, 330, 331,
339
Horvath, M. T. K., 227, 228, 245
Hosking, J. R. M., 296, 297, 307
Hsiao, C, 174, 194
Hu, W., 138, 150, 229, 245
Huber, P. J., 440, 441, 451
Hurwicz, L., 141, 149
Hwang, J., 114-116, 149
Hylleberg, S., 4, 7, 362, 366-368, 370,
372, 375, 379, 381, 383-385
Im, K. S., 136, 137, 149
Imhof, J. P., 64, 82, 95, 142, 149
Inder, B., 162, 164, 179, 180, 191, 194,
206, 209, 210, 244, 411, 412,
420
Inoue, T., 486, 489
Iyer, S., 487, 489
Jeffreys, H., 263, 265, 266, 271-276,
284, 286, 289, 291, 293, 295
Jenkins, G. M., 3, 4, 6, 8, 17, 18, 42, 43,
47, 93, 140, 173, 342, 344,
362, 365, 371, 383
Jeong, J., 309, 310, 314, 318, 319, 340
Johansen, S., 165, 166, 168-175,
181-184, 187, 188, 191, 192,
194-196, 202, 203, 211, 212,
214-221, 223-227, 229, 230,
234, 235, 237-242, 245-247,
249, 312, 334, 336, 351,
353-360, 376, 377, 411, 413,
414, 420, 447, 490
Jorion, P., 241, 242
Joutz, F., 187, 195
Joyeux, R., 296, 307, 377, 385
Juselius, K., 173, 174, 195, 211, 218,
237, 240, 245, 353, 357, 358,
360
Kiinsch, H. R., 328-332, 340
Kakimoto, S., 483
Kaminsky, G., 465, 482
Kang, H., 34, 44
Kang, K. M., 116, 149
Kao, C, 393, 421
Kashiwagi, N., 406, 407, 421
Kass, R. E., 284, 293
Katanaka, M., 259
Katayama, S., 467, 483
Kendall, M. G., 141, 149
Kilian, L., 313, 314, 340
Kim, B. H., 465, 466, 482
Kim, H. J., 392, 394, 396, 421
Kim, I. M., 230, 245, 271, 293, 397, 398,
404-407, 409, 415, 416, 421,
459, 460, 462, 482
King, M. L., 113, 149
Kitamura, Y., 256, 257, 259, 351, 352,
360
Kiviet, J. F., 316, 317, 319-321,
323-325, 338, 340, 414, 419
Kleibergen, F., 188, 195, 227, 228, 245,
288, 289, 293
Kloek, T., 270, 293
Koedijk, K., 241, 246
Koenker, R. W., 442, 445, 451
Kofman, P., 244
Kolmogorov, A., 8, 43
Kontrus, K., 393, 394, 421, 423
Koop, G., 283, 287, 288, 293, 305, 307
Koop, K., 291, 293
Koopman, S. J., 438, 450, 477, 482
Koopmans, T. C, 96
Koopmen, S. J., 487, 489
Kramer, W., 393, 411, 421, 423
Kremers, J. J., 203-205, 232, 246
Kuan, C. M., 415, 422
Kullback, S., 390, 421
Kunst, R. M., 377, 385
Kwiatkowski, D., 120, 149
Liitkepohl, H., 379, 381, 386
Lahiri, K., 139, 150, 229, 246, 465, 483
Lai, K., 134, 147, 216, 225, 243, 302, 306
Lam, P. S., 87, 93, 459, 461, 464, 480,
483
Lange, K. L., 445, 447, 451
Laroque, G., 348, 359
Larson, H. K., 235, 247
Lauritzen, S. L., 475, 483
Layton, A. P., 235, 246
Le Breton, A., 141, 150
Learner, E., 239, 246, 271, 277, 280, 281,
293
Ledolter, J., 438, 442, 451, 452
Lee, D., 299, 305, 307
Lee, H. S., 375-377, 379, 384-386
Lee, L, 396, 418
Lee, J., 105, 150
Lee, J. H., 465, 466, 480, 482, 483
Lee, L. F., 456-458, 465, 480, 483
Lee, T. H., 101, 149, 173, 220, 221, 244,
488
LeSage, J. P., 187, 195
Levin, A., 135, 136, 138, 145, 150
Leybourne, S. J., 99, 109-112, 117, 120,
122-124, 126, 127, 131, 150,
205-207, 246, 394, 411, 421
Li, H., 105, 150, 184, 195, 309, 312, 317,
319, 322, 329, 330, 333-336,
340

496
Author index
Li, W. K., 300, 301, 307
Liatas, I., 443, 444, 450
Lin, C-F., 135, 136, 138, 145, 150
Lindgren, G., 454, 483
Lindley, D. V., 266, 293
Little, R. J. A., 451
Liu, L. M., 439, 450
Liu, P. C, 101, 137, 150, 240, 246
Liu, R. Y., 328, 329
Ljung, G. M., 438, 452
Lo, A. W., 87, 95, 298, 299, 305, 307,
391, 421
Loretan, M., 161-165, 177, 179-181, 196
Lovell, M. C, 4, 7
Lubian, D., 182, 193
Lubrano, M., 275, 276, 288-290, 292,
294
Lucas, A., 428, 431, 439, 441-445, 447,
451, 452, 486, 489
Lumsdaine, R. L., 397, 401, 408, 409,
418, 421
Lundback, E., 126, 149
Luukkonen, R., 152
Lyhagen, J., 335, 340
Maasoumi, E., 438, 451
MacDonald, R., 234, 235, 246
MacKinlay, A. C, 87, 95, 298, 299, 307
MacKinnon, J. G., 199, 201, 202, 216,
246
Maddala, G. S., 20, 43, 95, 137, 138,
150, 151, 184, 187, 195, 215,
233, 246, 253, 259, 271, 293,
309, 310, 312, 314, 317-319,
322, 329, 330, 333-336, 340,
357, 360, 397, 398, 405-407,
409, 415, 421, 425, 426, 428,
431, 432, 437, 439, 450, 452,
453, 455, 462, 482, 483
Maekawa, K., 253, 254, 259
Makov, U. E., 459, 483, 484
Mallik, A. K., 337
Mamingi, N., 229, 246
Mammen, E., 318, 339
Mann, H. B., 61, 95
Mark, N. C., 459, 480
Marriott, F. H. C., 141, 151
Martin, R. D., 438, 440, 441, 447, 450,
452
McCabe, B. P. M., 117, 120, 122-124,
126, 127, 131, 150, 151,
205-207, 246, 394, 411, 421
McCallum, B. T\, 34, 44, 90, 95
McCormick, W. P., 337
McCulloch, J. H., 487, 489
McCulloch, R. E., 406, 422, 449, 452,
459, 461-463, 483
McCullough, B. D., 341
McCurdy, T. H., 464, 480, 481
McDonald, R., 138, 151
McFarland, J. W., 446, 453
McLeish, D. L., 54, 95
McLeod, A. L, 300, 301, 307
McMahon, P. C., 446, 453
Meese, R., 91, 95
Melick, W. R., 243
Mestre, R., 242
Meyer, W., 134, 147
Miller, R. B., 91, 94
Min, C., 285, 295
Miron, J. A., 367, 383
Mizrach, B., 422
Moore, J. B., 473, 474, 480
Moore, M. J., 241, 246
Morgan, M. S., 3, 7
Moser, J., 244
Nabeya, S., 65, 95, 120, 121, 151, 394,
411, 422
Nankervis, J. C., 64, 94, 95, 148, 317,
324, 325, 327, 328, 340
Nason, J. M., 410, 412, 420
Neale, A. J., 407, 410, 420, 428, 430, 451
Nelson, C. R., 3, 7, 32, 34, 42, 44, 47,
48, 63, 73, 87-89, 95, 101,
111, 127, 143, 144, 146, 147,
151, 232, 268-270, 276, 294,
400, 402, 422, 459, 484
Nerlove, M., 246, 372, 386
Newbold, P., 3, 7, 28, 29, 43, 77, 93,
101, 146, 415, 422
Newey, W. K., 80, 83, 95, 125, 162, 207,
347, 391, 421
Ng, S., 77, 78, 82, 83, 95, 96, 99, 108,
109, 130, 151, 152, 157, 158,
162, 165, 195, 431, 434, 452
Nicholls, D. F., 131, 151
Noh, J., 385
Norin, N., 486, 489
Nunes, L. C., 415, 422
Nyblom, J., 394, 422
Ogaki, M., 168, 196, 349, 360
Oh, K. Y., 134, 136, 138, 151
Ohanian, L. E., 187, 196
Ohtani, K., 467, 468, 483
Oke, T., 335, 340
Orcutt, G. H., 141, 151
Orden, D., 181, 192, 211, 222, 223, 242
Osborn, D. R., 366, 376, 379, 380, 386
Oseiwalski, J., 293, 307
Osterwald-Lenum, M., 212-214, 225,
247
Otero, J., 414, 424

Author index
497
Ouliaris, S., 199, 200, 203, 226, 247,
351, 360, 377, 408, 409, 419
Pagan, A. R., 35, 43, 189, 196, 251, 259,
370, 381, 385, 459, 483
Page, C. C, 470, 483
Pan, J. J., 64, 95
Pantula, S. G., 104-106, 113, 119, 120,
125, 131, 133, 147, 148, 151,
343-347, 359, 360
Papell, D. H., 130, 136, 151, 401, 409,
418, 421
Park, H. J., 112, 152
Park, J. Y., 120, 124, 125, 127, 152,
161, 168, 196, 206-209, 247,
249, 253, 259, 333, 340, 349,
360, 491
Park, S. B., 391, 422
Paruolo, P., 355, 360
Paul, M., 139, 150
Pedroni, P., 241, 247
Pena, D., 193
Perron, P., 47, 48, 58, 65, 73, 75, 77-80,
82, 83, 90, 91, 93, 95, 96, 99,
100, 108, 109, 118, 121, 122,
129, 130, 145, 152, 153, 157,
158, 162, 165, 168, 169, 192,
195, 196, 199, 202, 207, 212,
217, 218, 233, 243, 247, 298,
346, 348, 364, 365, 379, 384,
386, 389, 397-403, 407-410,
414, 416-418, 420, 422, 425,
428-431, 434, 435, 452, 453,
461, 463, 481, 484
Pesaran, M. H., 136, 137, 145, 149, 194
Peters, S. C, 319, 320, 335, 338, 477,
482
Petrie, T., 480
Pham, D. T., 141, 150
Phillips, P. C. B., 28, 29, 39, 44, 47, 48,
58, 59, 61, 62, 64, 75, 78-84,
86, 90, 91, 96, 97, 99, 100,
105, 115, 118, 120-122, 130,
145, 149, 152, 153, 156,
161-165, 173, 177-184, 194,
196, 197, 199, 200, 203, 207,
209, 218-220, 226, 241,
246-249, 253, 255-257, 259,
263, 266, 268, 270-273,
276-278, 281-283, 286, 288,
291, 294, 295, 298, 325, 333,
340, 346, 348, 351, 352, 360,
364, 365, 377, 398, 402, 406,
411, 412, 414, 422-424,
445-447, 452, 453, 490, 491
Pierce, D. A., 19, 43, 93
Ploberger, W., 255, 278, 281-283, 291,
294, 393-396, 411, 414, 418,
421, 423
Plosser, C. I., 3, 7, 34, 44, 47, 48, 73,
87-89, 95, 111, 116, 127, 143,
144, 146, 151, 152, 232,
268-270, 276, 294, 400, 402,
422
Podivinsky, J. M., 215, 219, 239, 247
Politis, D. N., 329, 331, 341
Pope, J. A., 141, 151
Porter-Hudak, S., 300, 303, 307, 308
Praschnik, J., 101, 150
Quah, D., 135, 152
Quandt, R., 391, 392, 394, 396, 423,
454-457, 468, 481, 484
Quenouille, M., 18, 44, 96
Quinn, B. G., 131, 151
Quintos, C. E., 411-414, 423
Rabault, G., 460, 463, 466, 484
Raiffa, H., 265, 267, 294
Rao, A. S., 253, 259
Rao, C. R., 107, 152, 314, 390, 391, 423
Rao, M. M., 3, 7, 61, 96
Rapp, R., 466, 481
Rappoport, P., 407, 410, 423, 430, 453
Raymond, J. E., 464, 484
Rayner, R. K., 315, 323-325, 341
Reichelin, L., 410
Reichlin, L., 407, 423, 430, 453
Reimers, H. E., 216, 225, 247
Reinsel, G. C., 103, 104, 154, 168, 187,
191, 192, 196, 225
Rich, R. W., 464, 484
Rilstone, P., 313, 315, 341
Robinson, P. M., 300, 305, 307, 308
Romano, J., 312, 313, 329, 331, 337, 341
Romo, J., 325, 326, 338
Ronchetti, E. M., 451
Rose, A. K., 408, 423
Rosenblatt, H. M., 390, 421
Ross, S. L., 393, 421
Rothenberg, T. J., 99, 113, 148, 219,
442, 444, 450, 453
Rothman, P., 303, 306
Rousseeuw, P. J., 441, 451, 453
Rubin, H., 61, 96
Rudebusch, G., 128, 141, 148, 152, 298,
302, 303, 306
Rush, M., 229, 234, 236, 244
S0rensen, B. E., 220, 245
Said, S. E., 75, 76, 82, 90, 97, 100, 152
Saikkonen, P., 120, 152, 161-163, 165,
196, 227, 247
Salazar, D., 397, 404, 423

498
Author index
Sampson, M., 91, 92, 97 
Srba, F., 43
Sapsford, D., 468, 485 
Stahel, W. A., 451
Sarantis, N., 379, 386 
Startz, R., 302, 303, 305, 307, 459, 484
Sargan, J. D., 35, 43, 44, 82, 83, 92, 97, 
Steel, M., 293, 307
116, 118, 152, 159, 196 
Stewart, C, 379, 386
Satchell, S. E., 64, 97 
Steyn, I. J., 475, 484
Saunders, M. A., 470, 483 
Stock, J. H., 79, 83, 97, 99, 108, 109,
Savin, N. E., 61, 64, 94, 95, 148, 314, 
113, 117, 139, 145, 148, 153,
317, 324, 325, 327, 328, 339, 
157, 161-163, 171, 172, 193,
340 
196, 197, 203, 218, 219, 221,
Schlaifer, R., 265, 267, 294 
222, 226, 244, 248, 254, 255,
Schmidt, P., 39, 44, 84, 86, 97, 105, 
258, 259, 351, 352, 355, 356,
114-116, 120, 149, 150, 153, 
361, 365, 389, 397, 401, 408,
299, 307 
418, 424, 442, 444, 450, 453
Schotman, P., 143, 144, 153, 241, 246 
Susmel, R., 466, 482
Schotman, P. C, 267, 272, 275, 276, 
Swanson, N. R., 486-489
278, 294
Schwartz, A. J., 63, 94 
Takanehi, Y., 259
Schwert, G. W., 77, 78, 81, 82, 97, 100, 
Takemura, A., 116, 147
103, 116, 123, 124, 127, 152, 
Tanaka, K., 65, 66, 95, 97, 119-121,
153, 327, 341, 459, 483 
151, 153, 394, 411, 422, 491
Schwert, W. G., 34, 44 
Taqqu, M. S., 300-302, 307
Sclove, S. L., 457, 484 
Taylor, J. M. G., 451
Scott, A., 366, 373, 379, 385 
Taylor, M. P., 234, 235, 237, 243, 246
Sellke, T., 277, 292 
Taylor, R. L., 337
Sen, D. L., 344, 360 
Terasvirta, T., 486, 489
Sen, P. K., 393, 423 
Thornber, H., 267, 295
Seo, B. S., 414, 424 
Tiao, G. C., 165, 169, 181, 182, 184,
Sephton, P. S., 235, 247 
191, 192, 211, 222, 230, 334,
Shea, G. S., 303, 308 
336
Shephard, N., 117, 153, 438, 450, 477, 
Tibshirani, R., 310, 312, 313, 338, 339,
482, 484 
341
Shiller, R., 129, 130, 153, 175, 193, 365, 
Titterington, D. M., 459, 484
386 
Tj0stheim, D., 485
Shin, Y., 120, 136, 137, 145, 149, 206, 
Toda, H. Y., 217, 220, 248, 258, 259
208, 209, 248 
Tran, H. A., 193
Siegmund, D., 392, 394, 396, 421 
Tremayne, A. R., 131, 150, 151
Siklos, P. L., 377, 379, 381, 382, 384-386 
Trost, R. P., 187, 195
Sims, C. A., 34, 44, 249, 259, 263, 
Tsay, R. S., 406, 422, 426, 439, 449, 452,
266-268, 271, 277, 278, 280, 
453, 459, 461-463, 483
286, 291, 294, 364, 365, 384, 
Tsurumi, H., 467, 468, 480, 484
386 
Turner, C. M., 459, 484
Singh, K., 311, 328, 329, 336, 340, 341 
Tyssedal, J. S., 485
Slutsky, E., 13, 44
Smith, A. A., 303, 308 
Uhlig, H., 267, 268, 275, 285, 286,
Smith, A. F. M., 292, 405, 420, 459, 
289-291, 294, 295
462, 481, 483, 484 
Urbain, J. D., 188, 197, 245
Smith, G. W., 176, 192
Smith, J., 298, 299, 308, 386, 414, 424 
van Dijk H. K., 294
Smith, R., 35, 42, 192 
van Dijk, D., 143, 144, 153, 227, 245,
Snyder, R. D., 470, 475, 484 
451
Solo, V., 491 
van Dijk, H. K., 267, 270, 272, 275, 276,
Sorenson, H. W., 475, 484 
278, 288, 289, 293
Soules, G., 480 
van Giersbergen, N. P. A., 316, 317,
Sowell, F., 270, 294, 298, 300-303, 305, 
321, 323-325, 338
308 
van Norden, S., 125, 126, 147, 485
Spanos, A., 10, 44 
Varoufakis, Y., 468, 485

Author index
499
Veall, M. R., 310, 314, 315, 319, 339,
341
Vinod, H. D., 309, 333, 341
Vogelsang, T. J., 384, 408, 409, 422,
430, 431, 434, 448, 451-453
Vuong, Q. H., 419
Wald, A., 61, 95
Wallis, K. F., 44, 364, 386
Wang, J. G., 465, 483
Wasserman, L., 284, 293
Watson, M. W., 139, 153, 161-163, 171,
172, 196, 197, 203, 218,
226-228, 245, 248, 259, 351,
352, 355, 356, 361, 365
Watt, D., 410, 412, 420
Watts, D. G., 467, 480
Wei, C. Z., 66, 93, 280, 282, 295, 490
Weinbach, G. C, 480
Weisberg, S., 439, 450
Weiss, A. A., 35, 43
Weiss, N., 480
West, K. D., 67, 80, 83, 95, 97, 125, 162,
207, 347, 350, 361
West, M., 439, 453, 469, 485
White, H., 80, 97, 318, 321, 397, 419
White, J. S., 3, 7, 61, 97
Whiteman, C. H., 94, 148, 268-270,
278, 292, 294
Wickens, M. R., 37, 44, 175, 197, 251,
254, 259, 488, 489
Wilson, S. R., 315, 316, 323, 327, 339
Winokur, H. S., Jr., 141, 151
Wolter, J., 298, 307
Wooldridge, J. M., 251, 260
Wright, J. H., 411, 424
Wu, S., 137, 151
Wu, Y., 134, 136, 138, 153
Xiao, Z., 197, 219, 220, 248
Yajima, Y., 302, 308
Yamamoto, T., 258, 259
Yang, C. W., 173, 194
Yang, M., 169-171, 192, 223-225, 243,
248
Yang, R., 284, 292
Yao, Y. C, 415, 424
Yap, S. F., 103, 104, 154
Yatawara, N., 437, 438, 450
Yeo, S., 43
Yilmaz, K., 304, 306
Yin, Y., 425, 428, 431, 432, 437, 452,
453
Yohai, V. J., 440-442, 447, 452, 453
Yoo, B. S., 185, 186, 193, 199, 244, 348,
361, 366, 385
Yu, B. C, 117, 118, 125, 127, 131, 148,
466, 482
Yule, G. U., 3, 7, 14, 15, 28, 44
Zellner, A., 266, 267, 275, 285, 286, 288,
295
Zin, S. E., 303, 306, 308
Zivot, E., 64, 97, 203, 205, 248, 275,
276, 295, 325, 341, 397, 401,
402, 409, 424

Subject index
Abadir test, 141
aggregation problem
cointegration, 182, 190-191, 229
unit root test, 129, 130, 145
Akaike Information Criterion (AIC), 19,
78, 127, 191, 217, 467
ARCH disturbanes, 324
asymptotic convergence, 21
asymptotic independence, 9
augmented Dickey-Fuller (ADF)
choice of lag-length, 77-79
comparison with the PP test,
81-82
augmented Dickey-Fuller (ADF)
distribution, 75
augmented Dickey-Fuller (ADF)
regression, 75
augmented Dickey—Fuller (ADF) test,
45-47, 98-100, 103, 122, 124,
127, 129, 130, 133, 135, 137,
140, 144, 145, 161, 162, 164,
165, 199, 202, 205, 215, 218,
228, 229, 234, 252, 276, 298,
299, 327, 351, 364, 365, 376,
410, 412, 434, 445
size distortions and low power,
100-103
autocorrelation function (acf), 11
autocovariance function (acvf), 11
autoregressive (AR) processes, 13-15
autoregressive distributed lag (ADL)
model, 35-37
autoregressive fractionally integrated
moving average (ARFIMA)
model, 296-298
frequency-domain method,
301-302
Geweke and Porter-Hudak
procedure, 300-301
ML estimation, 301
unit root tests, see unit root tests
against ARFIMA alternatives
autoregressive moving-average (ARMA)
processes, 15-17
autoregressive unit root, 49
Bardsen transformation, 36
Bayesian inference
introduction, 264-266
Markov switching regression,
461-463
on Nelson-Plosser data, 268-271
structural change, 402-407
Bayesian Information Criterion (BIC),
19, 78, 79, 191, 217, 280, 282,
283, 398, 406, 415, 416, 467
Bayesian likelihood ratio (BLR), 283,
284, 291
Bayesian methods
future direction, 487
Bayesian vector autoregressive (BVAR)
model, 185, 187
Beran's pivotal method, 313
Berger-Bernardo method, 284
Beveridge—Nelson decomposition, 101
Beveridge-Nelson decomposition, 31
Bhagava test, 82-84
Bhargava-Schmidt-Phillips (BSP) test,
86
Bieren—Guo test, 125
Bierens test, 140
bootstrap
application in finance, 335
AR(1) model, 322-325
cointegrating regressions, 332-335
confidence intervals, 311-314
future direction, 486
hypothesis testing, 315—317
moving block, see moving block
bootstrap (MBB)
500

Subject index
501
percentile-t method, see
bootstrap-* method
pre-testing problem, 335-336
review, 309-311
samples, 318-322
unit root tests, 325-328
bootstrap-1 method, 311-313
Box-Jenkins methods, 17-20
Box-Tiao method, 46, 169-171, 334
Breitung-Meyer test, 134-135
Brownian motion, see Wiener processes
geometric, 51
Burridge-Guerre test, 140
Canova-Hansen (CH) test, 370-371
central limit theorem (CLT), 52
Chow test, 390-391
cointegrated system
general, 155
triangular system, 160
two variable model, 156
cointegrating regression Durbin-Watson
(CRDW) test, 202
cointegration, 26—28
cointegration and Granger causality,
188-189
cointegration test
cointeration as null, 205-210
effect of time disaggregation,
228-229
pre-testing problem, 229-231
cointegration with exogenous variables,
187-188
cointegration with 1(2) and 1(1)
variables
Johansen method, 353-355
Kitamura model, 352
residual-based tests, 350-351
single equation methods, 348-350
Stock and Watson method,
351-352
confirmatory analysis, 126-128
continuous mapping theorem (CMT), 54
CUSUM test, 392-393
OLS residual, see OLS-based
CUSUM test
power, 393
structural change in cointegration,
see OLS-based CUSUM test
deterministic trend, 29-33
detrendning methods, 33
Dickey-Fuller (DF) distribution, 62-66,
84, 104, 113, 232, 435-437
AO model, 433
IO and LC models, 434
Dickey—Fuller (DF) forward regression,
109
Dickey-Fuller (DF) regression, 107,
431-434
Dickey—Fuller (DF) reverse regression,
110
Dickey-Fuller (DF) test, 45, 47, 63, 82,
85, 86, 107, 113, 115, 126,
200, 254, 255, 300, 343, 344,
399-401, 409, 410, 431, 434,
435, 442, 443
additive outliers, 431
AO model, 432, 434
innovation outliers, 431
MM estimator, 443
outliers in first-differences, 430
VC model, 436
Dickey-Fuller GLS (DF-GLS) test,
113-115
Dickey-Pantula test, 343-346
Dickey-Hasza-Fuller (DHF) test, 367
difference-stationary process (DSP), 25,
26, 33, 34, 37, 42, 73, 74, 88,
89, 143, 144, 416
double unit root, 131, see Hasza—Fuller
test, Dickey-Pantula test,
Halrup test
Dufour test, 391
Durbin-Hausman test, 106-107
Durbin-Watson (DW) statistic, 28
Durbin-Watson statistic, 82, 202, 215,
252
dynamic generalized least squares
(DGLS) method, 163-164
dynamic linear regression (DLR) model,
36
dynamic OLS (DOLS) method, 163
ECM tests, 203-205
economic identification, 174
Elliot-Rothenberg-Stock procedure, see
Dickey-Fuller GLS
(DF-GLS) test
EM algorithm, 458-459
empirical identification, 174
endogeneity of regressors, 158
Engle-Granger two-step method,
156-159, 410
Engle-Granger-Hylleberg-Lee (EGHL)
tests, 375-376
error correction model (ECM), 35-37,
40-42, 46, 157, 162, 163,
177-180, 182, 185-187, 189,
195, 202-205, 212-215, 218,
220, 233, 242, 352, 373-376,
410
exchange rates dynamics

502
Subject index
using ARFIMA model, 303-304
using Markov switching model,
460-461
Fisher test, 137
fluctuation test, 393-394, 396
forecast
ARIMA model, 20
cointegrated system, 184-187
interval forecasts, 91—92
TS versus DS, 90-91
fractional unit roots, see ARFIMA
model
fractionally cointegrated models,
302-303
Franses test, 368-370
frequency of observations, 129—131
fully modified least absolute deviation
(FM-LAD) estimation, 445
fully modified M (FM-M) estimation,
445
fully modified OLS (FM-OLS)
estimation, 46, 161-162, 165,
172, 178-180, 182, 183, 256,
334, 352, 411, 414, 445, 446
fully parametric least squares (FPLS)
method, 164
functional central limit theorem
(FCLT), 52
Gaussian process, 9
general to specific rule, 78
generic identification, 174
Geweke and Porter-Hudak procedure,
300-301
gradual regime switching models,
467-469
Bayesian analysis, 469
Granger causality, 188-189
Granger representation theorem, 41
Gregory-Hansen tests, 412
Haldrup test, 346-348
Hamiton model, 457
Hansen-Johansen tests, 413-414
Harris-Inder test, 209-210
Hasza-Fuller test, 342-343
Horvath-Watson test, 228
Hylleberg-Engle-Granger-Yoo (HEGY)
test, 367-368
identification problem, 173—175
future direction, 488
Im-Pesaran-Shin test, 136-137
integrated variables, I(k), 25
invertibility condition, 12
Ito process, 51
IV test, 104-106
Johansen procedure, 165—168
limitation, 173
with trends, 168-169
Johansen tests, 211-222
critical values, 212-214
finite sample evidence, 214-220
maximum eigenvalue test, 212
nearly unit root variables, 221-222
spurious cointegration, 220-221
trace test, 211-212
Kalman filter, 472-473
Kleibergen-van Dijk tests, 227-228
KPSS test, 120-122
LAD-based tests, 444
lag length, 77—79, see selection of lag
length
lag operator, 12
lagged dependent variables, 252-254
Lam model, 461
levels canonical correlation analysis
(LCAA) tests, 222-226
Levin-Lin test, 135-136
Leybourne-McCabe test, 122-124,
206-207
local level model, 117
locally most powerful test, see tests for
varying parameter, 396
long memory in stock price, 305
long-run demand for money, 236—239
cointegration with 1(2) and 1(1)
variables, 355-358
LR test in ARMA models, 103-104
MA unit roots, 49, 116
Choi-Yu test, 117-119
ML estimator, 116-117
spectral density test, 119
Tanaka test, 119-120
VD test, 119
MacKinnon critical values, 199
MAP procedure, 459
market efficiency hypothesis (MEH),
234-235
Markov switching regression model,
455-457
ARCH models, 466
Bayesian analysis, 461-463
effects of X-ll, 466
endogenous regime shifts, 464
MAP procedure, 459
ML estimation, 458—459
multiple state models, 463, 464
three-state model, 463

Subject index
503
time-varying transition
probability, 466
with explanatory variables, 464
martingale, 304, 394
martingale difference sequences, 74, 75
max Chow type tests, 394—396
mean Chow test, 396-397
mean-reverting, 23, 190, 298
median-unbiased estimation, 141—144
minimum MSE estimator, 144—145
moving block bootstrap (MBB),
328-329
optimal length of blocks, 331-332
problems, 329-330
moving-average (MA) processes, 12-13
moving-average (MA) unit root
Arellano-Pantula test, 125
Choi test, 125
multiple structural breaks, 405—407
Nabeya-Tanaka LM test, 121
near unit root, 66, 298
nearly stationary, 157
Newey-West estimator, 83, 162, 207,
347
Newey-West estimator of <J2 , 80
nonlinear least squares (NLS) method,
163
nonlinear models
future direction, 486
nonstationary, 20
normalization, 46, 60, 107, 156, 157,
162, 165, 168, 189, 211, 226,
227, 250, 289, 413
OLS-based CUSUM test, 393
nonstationary variables, 411
Ornstein-Uhlenbeck process, 65
outliers
additive outliers (AO), 426-428,
430-434, 437-441, 448
detection, 437-439
different types, 425-428
effects on unit root tests, 428—437
innovation outliers (AO), 426-428,
431, 432, 434-439, 441, 448
level changes (LC), 426-428,
434-437, 439, 448
transient changes (TC), 426, 439
variance changes (VC), 426-428,
436, 437, 439, 448
overdifferencing, 49, 89
overdiffrencing, 116
panel data unit root, 133—134
power of tests, 137
the hypothesis of interest, 138-139
Park J test, 124-125, 207-208
periodic
autoregression, 379—380
cointegration, 381
integration, 380-381
models, 381
unit roots tests, 381
Phillips-Perron (PP) test, 45, 46,
79-81, 83, 86, 92, 98-100,
103, 107-109, 122, 124, 126,
129, 130, 140, 145, 161, 162,
364, 434, 443, 444
comparison with the ADF test,
81-82
modifications, 107-109
size distortions and low power,
100-103
Phillips-Perron test, 83
PICF, 283, 291
posterior
distribution, 264-270, 279-281,
283, 291
odds, 276-281, 283, 284, 291
posterior informtion criterion (PIC),
282, 283, 291
pre-testing problem, 46, 100, 139-140,
229-231, 242, 255, 286, 290,
335, 336, 417, 487
future direction, 486
predictive
approach, 278
density, distribution, 280-283, 287
odds, 281, 283, 291
sample reuse (PSR), 281
principal components and cointegration,
171-172
principal components method, 226
prior
Berger-Bernardo, 284
conjugate, 264, 265
diffuse, 264, 266, 270, 282, 289, 292
distribution, 264, 265, 279
flat, 263, 265-269, 271-273, 278,
288, 291
ignorance, 263, 266, 271, 272
improper, 265
Jeffreys', 263, 266, 271-276, 284,
286, 289, 291, 295
maximal data information
priors(MDIP), 285
Normal-Wishart, 286, 289
Wishart, 289
purchasing power parity (PPP), 126,
134, 138, 156, 232, 233, 236,
237, 239-241, 355, 358, 408
purely random process, 11

504
Subject index
Q test, 19-20
Quandt test, 391-392
Quintos tests, 413-414
Quintos-Phillips tests, 411-412
random walk, 20, 24, 27, 28, 33, 48, 50,
52, 54, 57, 67, 82, 83, 87,
102, 123, 160, 190, 214, 303,
408, 415, 416, 430, 461, 463,
465, 479
continuous, 51
parameters, 120, 394, 397, 411, 469
weak, 157
with a change in the drift, 428
with drift, 29, 31, 33, 38, 67, 70,
101, 372, 374, 430
with two states (Markov
switching), 460
reduced rank regression, 168
regression with 1(1) and 1(0) variables
1(1) regressors cointegrated,
250-251
1(1) regressors not cointegrated,
249-250
residual-based tests for cointegration,
198-203
Cochrane-Orcutt version, 202-203
robust estimation of cointegrating
regressions, 445-447
robust methods
future direction, 486
robust unit root tests, 440-445
Saikkonen tests, 227
Sargan—Bhargava test, 82
Sargan-Bhargava test, 82
scaling factors, 59-60
Schmidt-Phillips LM test, 84-86
seasonal adjustment, 364—365
seasonal cointegration, 375-376
estimation, 376-378
seasonal integration, 365-366
seasonal processes
integrated, 363
purely deterministic, 363
stationary, 363
seasonal unit roots, 366—371, 448
selection of lag length, 164-165
Seo tests, 414
Shin test, 208-209
signal-noise ratio, 178, 184
single equation ECM (SEECM)
method, 162-163
skip sampling problem
cointegration, 182, 229
seasonal unit root, 382
unit root, 129-131
Slutsky effect, 13
smooth estimates, 470
smooth inferences, 458
smooth probabilities, 458
smoothing, 473-474
spurious breaks, 415-416
spurious periodicity, 34
spurious regression, 28-29
STAMP program, 477
state-space models, 469
general form, 470-471
GLS and Kalman's recurrence
relation, 471-472
stationarity, 10-11
covariance, 24
stochastic process, 8
stochastic trend, 29-33
Stock and Watson method, 172
structural change
Bayesian approach, 402-407
effects on cointegration tests, 410
empirical works, 407-410
exact tests, 414-415
in cointegrated relationships,
411-414
seasonal models, 414
tests with known break, 390-391
tests with unknown break, 391-397
unit root tests, see unit root tests
under structural change
structural time series model, 475—477
future direction, 487
ML estimation, 477
program for estimation, see
STAMP program
superconsistency, 157
switching regression model, 454-455
systematic sampling, 381-382
tests for TSP versus DSP, 87-90
tests for varying parameter, 394
threshold cointegration, 190
time aggregation, 381-382
trend-stationary process (TSP), 25, 26,
33, 34, 37, 42, 73, 74, 88, 89,
143, 144, 416
unbalanced equations, 251—252
uncertain unit roots, 139-140, 254-256
uncertain unit roots and cointegration,
256-258
uncovered interest parity (UIP), 240
underdifferencing, 33, 89
unit root tests
against ARFIMA alternatives,
298-300

Subject index
505
bootstrapping, see bootstrap,
unit root tests
LAD estimator, 444
low power, 100-103
M estimator, 444
rank-based method, 445
size distortion, 100-103
stationary as null, 120-126
under structural change, 399—402
using cointegration, 231-233
unobserved components (UC) model,
117, 371-375
variance ratio (VR) test, 86-87
vector autoregressive (VAR) model, 34,
35, 42, 155, 156, 164, 165,
172, 174, 175, 182, 184-187,
195, 212, 215-217, 220, 227,
232, 233, 238, 248, 257, 258,
285, 287, 288, 290, 292, 304,
334, 338, 353, 356, 357, 365,
376, 381, 478, 487, 488
weighted symmetric (WS) estimator,
111-113
white noise, 12
wide class of errors, 74
Wiener process, 50-51
generalized, 51
relationships with normal
distributions, 56-59
some basic results, 54-56
X-ll filter, see seasonal adjustment
Yule—Walker equations, 14, 15


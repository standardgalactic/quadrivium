cover
cover
next page >
  
title :
Modern Heuristic Search Methods
author :
Rayward-Smith, V. J.
publisher :
John Wiley & Sons, Ltd. (UK)
isbn10 | asin :
0471962805
print isbn13 :
9780471962809
ebook isbn13 :
9780585356808
language :
English
subject  
Heuristic programming, Combinatorial optimization.
publication date :
1996
lcc :
T57.84.M63 1996eb
ddc :
658.4
subject :
Heuristic programming, Combinatorial optimization.
cover
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/cover.html2009-7-7 16:55:51

page_1
< previous page
page_1
next page >
Page 1
1 
Modern Heuristic Techniques
Colin R Reeves
Abstract
This book is concerned with applications and developments of heuristic techniques for solving complex optimization 
problems. The three most popular methods which go beyond simple local search are simulated annealing, tabu search, 
and genetic algorithms, and each is used in several papers in this volume. Because most of these assume a fairly high 
level of knowledge of the terminology and practice, this introductory chapter has been written with the aim of 
explaining the basics of the methods and of setting each of them in context. Experts in the field will find little new here, 
but this chapter should provide a useful summary of some of the underlying concepts.
1.1 
Introduction
Many 'hard' OR problems involving areas such as resource allocation, packing and scheduling have traditionally used 
methods based on linear and/or integer programming. Such approaches inevitably place restrictions on the form of the 
objective function and/or the constraintsusually linear, although they have been adapted to deal with slightly more 
general situations. More complex non-linear problems can be solved by calculus-based methods, but these rely on 
differentiability. Furthermore, such traditional methods all assume deterministic quantities, and have problems dealing 
with any stochastic effects.
There is no denying that these methods have produced excellent results over the past 40 years, but precisely because of 
their success, there may be a tendency to model problem situations using a framework which permits a solution by such 
means. Yet the assumptions inherent in such models are often only approximately true at best, and in recent years 
several very good heuristic methods have been developed which have no need of such assumptions. This opens up the 
possibility of building more realistic models with a real chance of obtaining 'good' solutions.
Three methods which have become particularly popular recently have all arisen at least in part from a study of natural 
processes which perform an analogy of
Modern HeuristicSearch Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_1
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_1.html2009-7-7 16:55:52

page_2
< previous page
page_2
next page >
Page 2
optimizationsimulated annealing, tabu search, and genetic algorithms. While many other heuristic methods have been 
proposed in the last 30 years, they have often been tailored to a particular problem. In contrast, these more modern 
approaches are capable of very wide application.
1.2 
Combinatorial Optimization
Many problems can be structured as a function of some decision variables, perhaps in the presence of some constraints. 
Such problems can be formulated generally as follows:
Here, x is a vector of decision variables, and f(·) and gi(·) are general functions. This formulation has assumed that the 
problem is one of minimization, but the modification necessary for a maximization problem is obvious.
There are many specific classes of such problems, obtained by placing restrictions on the type of functions under 
consideration, and on the values that the decision variables can take. Perhaps the most well known of these classes is 
that obtained by restricting f(·) and gi(·) to be linear functions of decision variables which are allowed to take fractional 
(continuous) variables, which leads to problems of linear programming.
The term combinatorial is usually reserved for problems in which the decision variables are discretei.e. where the 
solution is a set, or a sequence, of integers or other discrete objects. The problem of finding optimal solutions to such 
problems is therefore known as combinatorial optimization. Some examples of this kind of problem are as follows:
Example 1 (The Assignment Problem)
A set of n resources is available to carry out n tasks. If resource i is assigned to task j, it costs cij units. Find an 
assignment {π1, . . ., πn} which minimizes
Here the solution is represented by the permutation {π1, . . ., πn} of the numbers {1, . . ., n}.
Example 2 (The 01 Knapsack Problem)
A set of n items is available to be packed into a knapsack with capacity C units. Item i has value vi and uses up ci units 
of capacity. Determine the subset I of items which should be packed in order to maximize
  
< previous page
page_2
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_2.html2009-7-7 16:55:52

page_3
< previous page
page_3
next page >
Page 3
such that
Here the solution is represented by the subset I ⊆ {1, . . ., n}.
Example 3 (The Set Covering Problem)
A family of n subsets collectively contains n items such that subset Si contains ni (≤ n) items. Select k(≤ m) subsets 
such that 
so as to minimize
where ci is the cost of selecting subset Si.
Here the solution is represented by the family of subsets 
.
Combinatorial problems, such as those described above, have close links with linear programming (LP), and most of the 
early attempts to solve them used developments of LP methods, generally by introducing integer variables taking the 
values 0 or 1, in order to produce an integer programming (IP) formulation. For example, in the case of the 01 knapsack 
problem, we define
The problem then reduces to the following integer program:
Not all combinatorial problems are so easily formulated in IP terms, and even when they can be, solution by this means 
may not be computationally feasible.
1.3 
The Case for Heuristics
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_3.html（第 1／2 页）2009-7-7 16:55:53

page_3
A naive approach to solving an instance of a combinatorial problem is simply to list all the feasible solutions of a given 
problem, evaluate their objective functions, and pick the best. However, it is immediately obvious that this approach of 
complete enumeration is likely to be grossly inefficient; further, although it is possible in principle to solve any problem 
in this way, in practice it is not, because of the vast
  
< previous page
page_3
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_3.html（第 2／2 页）2009-7-7 16:55:53

page_4
< previous page
page_4
next page >
Page 4
number of possible solutions to any problem of a reasonable size. To illustrate this point, consider the famous travelling 
salesman problem (TSP).
This problem has exercised a particular fascination to researchers in combinatorial optimization, probably because it is 
so easily stated, yet so hard to solve (the book by Lawler et al. [LLKS85] and the more recent monograph by Reinelt
[Rei94] contain a wealth of detail about the various methods used to find solutions to this problem). For the record, the 
problem is as follows: a salesman has to find a route which visits each of N cities once and only once, and which 
minimizes the total distance travelled. As the starting point is arbitrary, there are clearly (N 1)! possible solutions (or (N 
1)!/2 if the distance between every pair of cities is the same regardless of the direction of travel). Suppose we have a 
computer that can list all possible solutions of a 20 city problem in 1 hour. Then, using the above formula, it would 
clearly take 20 hours to solve a 21-city problem, and 17.5 days to solve a 22-city problem; a 25-city problem would take 
nearly 6 centuries. Because of this exponential growth in computing time with the size of the problem, complete 
enumeration is clearly a non-starter.
In the early days of operational research, the emphasis was mostly on finding the optimal solution to a problemor rather, 
to a model of a problem which occurred in the real world. To this end, various exact algorithms were devised which 
would find the optimal solution to a problem much more efficiently than complete enumeration. The most famous 
example is the Simplex algorithm for linear programming problems1. At first, while such algorithms were capable of 
solving small instances of a problem, they were not able to find optimal solutions to larger instances in a reasonable 
amount of computing time. As computing power increased, it became possible to solve larger problems, and researchers 
became interested in how the solution times varied with the size of a problem. In some cases, such as the 'Hungarian' 
method [Kuh55] for solving the assignment problem, or Johnson's method [Joh54] for 2-machine sequencing, it can be 
shown that the computing effort grows as a low-order polynomial in the size of the problem. However, for many others, 
such as the travelling salesman problem, the computational effort required was an exponential function of the problem 
size.
In a sense, therefore, these exact methods (such as branch-and-bound or dynamic programming) perform no better than 
complete enumeration. The question that began to exercise the minds of researchers in the late 1960s was the following: 
is there a 'polynomial' optimizing algorithm for a problem such as the TSP? Nobody has yet been able to answer this 
question one way or the other, but in 1972, Karp[Kar72] was able to show that if the answer to this question is 'yes' for 
the TSP, then there is also a polynomial algorithm for a number of other 'difficult' problems. As no such algorithm has 
yet been found for any of these problems, it strongly suggests that the answer to the original question is 'no'. However, 
the actual answer is still unknown, some 20 years after Karp's original work.
It is for this reason that attention has recently focussed more and more on the use of heuristics. Given the diversity of 
methods to which the label 'heuristic' has been applied, an exact definition of the term on which everyone can agree is 
perhaps not possible; the following is an attempt to provide one that at least captures the salient features.
1Actually, Simplex is now known to show exponential behaviour in the worst case, although on average its 
performance is excellent.
  
< previous page
page_4
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_4.html2009-7-7 16:55:53

page_5
< previous page
page_5
next page >
Page 5
Definition 1
A heuristic technique (or simply, a heuristic) is a method which seeks good (i.e. near-optimal) solutions at a reasonable 
computational cost without being able to guarantee optimality, and possibly not feasibility. Unfortunately, it may not 
even be possible to state how close to optimality a particular heuristic solution is.
Although in the light of this definition it might appear that there are severe problems inherent in the use of heuristics, it 
should be emphasized that many modern heuristic techniques do give high-quality solutions in practice.
1.4 
Neighbourhood Search
A fundamental idea of heuristic methodology is that of neighbourhood search (NS). This concept has been around for at 
least 40 years; one of the earliest references to it is by Croes[Cro58] who used the idea to get good solutions to the TSP. 
Neighbourhood search (NS) is a widely used method in solving combinatorial optimization problems. A recent 
introductory treatment can be found in Reeves [Ree93c].
Table 1.1 Neighbourhood Search Method
1
(Initialization)
1.1 Select a starting solution xnow∈ X.
1.2 Record the current best-known solution by setting xbest = xnow and define best_cost = c
(xbest).
2
(Choice and termination)
2.1 Choose a solution xnext∈ N(xnow). If the choice criteria used cannot be satisfied by any 
member of N(xnow) (hence no solution qualifies to be xnext), or if other termination criteria 
apply (such as a limit on the total number of iterations), then the method stops.
3
(Update)
3.1 Re-set xnow = xnext, and if c(xnow) < best_cost, perform Step 1.2. Then return to Step 2.
 
Here we assume a slightly different statement of the problem of combinatorial optimization than that given in Section 
1.2. In the context of neighbourhood search, we shall assume that a solution is specified by a vector x, where the set of 
all (feasible) solutions is denoted by X, and the cost of solution x is denoted by c(x)often called the objective function. 
Each solution x ∈ X has an associated set of neighbours, N(x) ⊂ X, called the neighbourhood of x. Each solution x′∈ N
(x) can be reached directly from x by an operation called a move, and x is said to move to x′ when such an operation is 
performed. A description of the method is given in Table 1.1 (adapted from Glover and Laguna [GL93]), where it is 
assumed that we are trying to minimize the cost. Here we assume the choice criteria for selecting moves, and 
termination criteria for ending the search, are given by some external set of prescriptions. By specifying
  
< previous page
page_5
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_5.html2009-7-7 16:55:54

page_6
< previous page
page_6
next page >
Page 6
these prescriptions in different ways, the method can easily be altered to yield a variety of procedures. Descent methods, 
for example, which only permit moves to neighbours that improve the current c(xnow) value, and which end when no 
improving solutions can be found, can be expressed by the provision in Step 2 shown in Table 1.2.
Table 1.2 Descent Method
2
(Choice and termination)
2.1 Choose xnext∈ N(xnow) such that c(xnext) < c(xnow) and terminate if no such xnext can be 
found.
 
The failing of NS is well known: its propensity to deliver solutions which are only local optima. Randomized 
procedures, including the important technique of simulated annealing, offer one way of circumventing the local-
optimum problem, and they can similarly be represented by adding a simple provision to Step 2 (Table 1.3). The next 
section will explore this idea in more detail.
Table 1.3 Randomization Method
2
(Choice and termination)
2.1 Randomly select xnext from N(xnow).
2.2 If c(xnext) ≤ c(xnow) accept xnext (and proceed to Step 3).
2.3 If c(xnext) > c(xnow) accept xnext with a probability that decreases with increases in the 
difference ∆c = c(xnext) c(xnow). If xnext is not accepted on the current trial by this 
criterion, return to Step 2.1.
2.4 Terminate by a chosen cutoff rule.
 
1.5 
Simulated Annealing
Simulated annealing (SA) is a technique which first became popular about a decade ago, and has since proved itself as 
an effective approach to a large number of problems. It works by searching the set of all possible solutions, reducing the 
chance of getting stuck in a poor local optimum by allowing moves to inferior solutions under the control of a 
randomized scheme. Specifically, if a move from one solution x to another neighbouring but inferior solution x′ results 
in a change in value ∆c, the move to x′ is still accepted if
  
< previous page
page_6
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_6.html2009-7-7 16:55:54

page_7
< previous page
page_7
next page >
Page 7
where T is a control parameter, and R ∈ [0, 1] is a uniform random number. The parameter T is initially high, allowing 
many inferior moves to be accepted, and is slowly reduced to a value where inferior moves are nearly always rejected. 
There is a close analogy between this approach and the thermodynamic process of annealing in physics; it was this 
analogy which originally motivated the development of the method.
The ideas that form the basis of simulated annealing were first published by Metropolis et al. [MRTT53] in 1953 in an 
algorithm to simulate the cooling of material in a heat batha process known as annealing. If solid material is heated past 
its melting point and then cooled back into a solid state, the structural properties of the cooled solid depend on the rate 
of cooling. For example, large crystals can be grown by very slow cooling, but if fast cooling or quenching is employed 
the crystal will contain a number of imperfections. The annealing process can be simulated by regarding the material as 
a system of particles. Essentially, Metropolis's algorithm simulates the change in energy of the system when subjected 
to a cooling process, until it converges to a steady 'frozen' state. Thirty years later, Kirkpatrick et al.[KGV83] suggested 
that this type of simulation could be used to search the feasible solutions of an optimization problem, with the objective 
of converging to an optimal solution.
In Kirkpatrick's proposal, the control parameter T takes the rôle of the temperature, and it becomes important to devise a 
'cooling schedule' whereby this parameter is decreased. In order to complete the specification of the procedure, we must 
also specify initial and final values of T ('temperatures'). These are what Dowsland[Dow93] calls generic parameter 
choices, while there will also usually be specific decisions depending on the context of the given problem, such as the 
choice of the space of feasible solutions, the form of the cost function and the neighbourhood structure employed.
Both types of decision need some care, as they may affect both the effectiveness and the efficiency of the algorithm. 
Since Kirkpatrick's original paper considerable strides have been made in research into the theory of simulated 
annealing. This uses results based on modelling SA as a Markov chain which show that to guarantee convergence to a 
global optimum will, in general, need more iterations than exhaustive search.
This may not sound too helpful, but these theoretical studies also give some guidance in how to make SA function better 
simply as a heuristic. Dowsland [Dow93] gives a brief review and discussion of the theory, while a deeper treatment of 
theoretical questions can be found in Van Laarhoven and Aarts[LA88].
1.5.1 
A Simple Example
Before considering these decisions in more detail, it might be helpful to consider a short example to clarify the SA 
methodology.
We consider the problem:
where x is coded as a 5-bit binary integer in the range [0,31]. This has a maximum at (0 1 0 1 0), i.e. x = 10 (where f = 
4100). In digitized form, it can be shown that there are also 3 local maxima using a 'greedy' 1-bit neighbourhood search.
To illustrate SA here (Table 1.4), we start from the string (1 0 0 1 1), binary code for x = 19, f = 2399, which under a 
greedy 1-bit NS would lead to the local optimum (1 0 0 0 0), i.e. x = 16, f = 3236. We choose neighbours (i.e. the bits to 
change)
  
< previous page
page_7
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_7.html2009-7-7 16:55:55

page_8
< previous page
page_8
next page >
Page 8
randomly.
Table 1.4 First Attempt: T=100
T
bit
string
f
∆f
move?
new string
100
1
0 0 0 1 1
2287
112
N
1 0 0 1 1
90
3
1 0 1 1 1
1227
1172
N
1 0 0 1 1
81
5
1 0 0 1 0
2692
< 0
Y
1 0 0 1 0
72.9
2
1 1 0 1 0
516
2176
N
1 0 0 1 0
65.6
4
1 0 0 0 0
3236
< 0
Y
1 0 0 0 0
59.0
3
1 0 1 0 0
2100
1136
N
1 0 0 0 0
 
Unfortunately, the temperature is unlikely to be high enough now to move out of this local optimum: at any rate in a 
further 50 trials, nothing would move it!
Table 1.5 Second Attempt: T=500
T
bit
string
f
∆f
move?
new string
500
1
0 0 0 1 1
2287
112
Y
0 0 0 1 1
450
3
0 0 1 1 1
3803
< 0
Y
0 0 1 1 1
405
5
0 0 1 1 0
3556
247
Y
0 0 1 1 0
364.5
2
0 1 1 1 0
3684
< 0
Y
0 1 1 1 0
328.0
4
0 1 1 0 0
3988
< 0
Y
0 1 1 0 0
295.2
3
0 1 0 0 0
3972
16
Y
0 1 0 0 0
265.7
4
0 1 0 1 0
4100
< 0
Y
0 1 0 1 0**
239.1
5
0 1 0 1 1
4071
29
Y
0 1 0 1 1
215.2
1
1 1 0 1 1
343
3728
N
0 1 0 1 1
 
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_8.html（第 1／2 页）2009-7-7 16:55:56

page_8
In the second attempt (Table 1.5) the initial temperature was high enough to get out of the attraction region of 3236, and 
the optimum was found. Note however that the search did move away again, but in a further 150 trials most of the states 
visited were either the optimum or one of its immediate neighbours.
1.5.2 
Generic Decisions
The choice of cooling schedule and initial temperature in the above example might seem (and in fact were!) rather 
arbitrary. We now consider in more detail the generic decisions that need to be taken in implementing SA in practice.
Initial Temperature
The process must start in such a way that most if not all moves can be acceptedi.e. the initial temperature must be 'high'. 
In practice
  
< previous page
page_8
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_8.html（第 2／2 页）2009-7-7 16:55:56

page_9
< previous page
page_9
next page >
Page 9
this may require some knowledge of the magnitude of neighbouring solutions; in the absence of such knowledge, one 
may choose what appears to be a large value, and run the algorithm for a short time and observe the acceptance rate. If 
the rate is 'suitably high', this value of T may be used to start the process. What is meant by a 'suitably high' acceptance 
rate will vary from one situation to another, but in many cases an acceptance rate of between 40% and 60% seems to 
give good results. More sophisticated methods are possible, but not often necessary.
Cooling Schedule
Perhaps the most important factor in practical application is the cooling schedule. Here we should note that there are 
basically two types of schedule, having analogies to homogeneous and inhomogeneous Markov chains respectively. In 
the homogeneous case, annealing is carried out at a fixed temperature until 'equilibrium' is reached. (This involves a 
further problem in that one has to decide when this has occurred.) Once this state is judged to have been reached, the 
temperature is reduced, and the procedure repeated. The number of attempted moves at each temperature may be quite 
large, although the temperature steps can be relatively large also. In the inhomogeneous case, the temperature is reduced 
(but by a very small amount) after every move. This is less complicated than the homogeneous case, and is the one more 
commonly used in practice.
In either case, one has to decide on the 'shape' of the cooling curve. Two methods are popularthe simplest is a geometric 
schedule:
where α is a constant close to 1 (typically in the range 0.9 to 0.99). The other method is due to Lundy and Mees[LM86]:
where β is a constant near to zero.
Final Temperature
In theory the procedure should be continued until the final temperature Tf is zero, but in practice it is sufficient to stop 
when the chance of accepting an uphill move has become negligible. To some extent this is problem-dependent, and as 
in the case of selecting an initial temperature, may involve some monitoring of the ratio of acceptances. Lundy and 
Mees proposed stopping when
where S is the solution space. This is designed to produce a solution which is within ε of the optimum with probability θ.
Number Of Iterations
It should be noted that the number of iterations Nit is effectively fixed by the above three choices. In the homogeneous 
case, it also depends on how equilibrium is detected at each stage, but in the inhomogeneous case α, β and Nit are 
related by
  
< previous page
page_9
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_9.html2009-7-7 16:55:57

page_10
< previous page
page_10
next page >
Page 10
and
respectively.
1.5.3 
Problem Specific Decisions
The most important decision in any problem is specifying the neighbourhood structure. Theoretical results require that 
every solution be reachable from every other, but this is a condition that is usually easy to ensure. However, in the case 
of constraints on feasibility, one may need to use penalty terms which, while they ensure reachability, may create an 
undesirably rough solution 'landscape'. Constraints and penalty factors may thus require careful attention and some fine-
tuning.
It is often an advantage to define neighbourhoods in such a way that the calculation of the effect of a move can be 
carried out in an economical manner. For example, in evaluating the effect of a 'λ-exchange' for the TSP (where λ links 
of the tour are broken and reconnected), it is not necessary to compute the whole tour-length each time, but simply to 
evaluate the effect of the λ links that have been exchanged.
1.5.4 
Other Factors
The acceptance function is usually assumed to be exponential:
It has been pointed out that calculating this function (or equivalently, evaluating the logarithm of R) is comparatively 
expensive on a digital computer. Approximating by 1 ∆c/T often saves computer time at little cost in effectiveness. 
Other possibilities are discussed by Dowsland[Dow93].
It is worth mentioning here also the idea of 'threshold acceptance' which simply accepts any uphill move below a certain 
levela threshold which is slowly reduced as the search proceeds in a similar manner to the SA cooling schedule.
Reannealing is an idea which has also been used successfully. Connolly[Con90] carries out an initial SA pass, during 
which he records the temperature at which the best solution was found. He then 're-heats' the process and carries out a 
more lengthy search at this temperature until some stopping condition is satisfied.
Several other authors have suggested carrying out a sequence of heatings and coolingsin fact this was done by 
Kirkpatrick et al. in the very first trials of the SA method. Dowsland[Dow93] also proposes an adaptive version of the 
procedure: on accepting a move, the temperature is reduced according to
whereas when a move is rejected, the temperature is increased according to
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_10.html（第 1／2 页）2009-7-7 16:55:57

page_10
< previous page
page_10
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_10.html（第 2／2 页）2009-7-7 16:55:57

page_11
< previous page
page_11
next page >
Page 11
Restricted neighbourhoods have been found beneficial in several applications. Problem-specific information may 
suggest that some moves are so unlikely to be accepted that they can be ignored, and the potential disadvantages of 
destroying reachability may be outweighed by the advantages of spending less computer time in searching unproductive 
areas of the solution space. As an example, candidate moves in solving the TSP may usefully be restricted to those 
which relate to 'close' points.
The order of searching a neighbourhood may sometimes be varied to advantage. Most descriptions of SA assume 
neighbourhoods are sampled randomly, but there are several reported instances where sampling in a systematic or 
adaptive way has produced better results.
1.5.5 
Summary
Simulated annealing (SA) is a simple procedure to apply, yet there are several decisions to be made in applying it. This 
section has described some of these in outline, and given some idea of possible modifications and enhancements. All 
these are covered in much greater detail by Dowsland[Dow93], who also describes an extensive list of applications.
It is usually true that implementing SA with a simple neighbourhood, a starting temperature suggested by some small-
scale initial experiments, and a geometric or Lundy and Mees cooling schedule will produce results which are better 
than a simple neighbourhood search. Further improvements can often be made by more detailed analysis of problem 
characteristics, and/or by combining SA with other techniques.
1.6 
Tabu Search
Tabu search, like simulated annealing, is based on neighbourhood search with local-optima avoidance, but in a 
deterministic way which tries to model human memory processes. Memory is implemented by the implicit recording of 
previously-seen solutions using simple but effective data structures. These centre on the creation of a 'tabu list' of moves 
which have been made in the recent past of the search, and which are 'tabu' or forbidden for a certain number of 
iterations. This helps to avoid cycling, and serves also to promote a diversified search of the solutions.
The modern form of tabu search was originally developed by Glover[Glo86], and a comprehensive account of the basic 
concepts, and of recent developments, is given by Glover and Laguna[GL93]. What follows here should be adequate as 
a basic introduction, but the reader is referred to Glover and Laguna[GL93] for a more detailed account.
1.6.1 
Basic Concepts
If we recall the prototype neighbourhood search description in Section 1.4, TS can be simply described in terms of the 
way it modifies the neighbourhood. In the TS method, a history record H is kept of the states previously encountered 
during the
  
< previous page
page_11
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_11.html2009-7-7 16:55:58

page_12
< previous page
page_12
next page >
Page 12
search, so that the neighbourhood N(xnow) is modified to N(H, xnow). The basic NS method is thus modified as follows:
Table 1.6 Tabu search method
1
(Initialization)
1.1 Begin with the same initialization used by Neighbourhood Search, and with the history 
record H empty.
2
(Choice and termination)
2.1 Determine Candidate_N(xnow) as a subset of N(H, xnow). Select xnext from Candidate_N
(xnow) to minimize c(H, x) over this set. (xnext is called a highest evaluation element of 
Candidate_N(xnow).)
2.2 Terminate by a chosen iteration cut-off rule.
3
(Update)
3.1 Perform the update for the Neighbourhood Search Method, and additionally update the 
history record H.
 
The simplest way in which H can be defined is in terms of a prohibition on re-visiting certain states in N(xnow). Such 
states are termed tabu, and H thus has the effect of restricting the search. In enhanced versions of TS, H may also 
include states which are not members of N(xnow) itself, thus allowing both diversification and intensification of the 
search. Furthermore, H may contain information which serves to modify the evaluation of the objective function, as is 
indicated in the above description by the notation c(H, x).
These various prescriptions for identifying history-based characteristics are often described in terms of four 'dimensions' 
in tabu searchrecency, frequency, quality and influence. Of these, recency and frequency are the most important, and we 
now examine them in greater detail.
1.6.2 
Recency
One objective in TS is to encourage exploration of parts of the solution space that have not been visited previously. This 
can be achieved in practice by prohibiting the reversal of previous movesthat is, these (reverse) moves become 'tabu'. 
However, to make this prohibition absolute would confine the search to a straitjacket, so we prohibit the reversal of the 
most recent moves only. Recency may be construed as a fixed parameter (the tabu tenure of a move), or it may be 
allowed to vary dynamically during the search. It is also often beneficial to focus on some component or attribute of a 
move rather than on the complete move itself. Finally, in many implementations, tabu moves are allowed to be 
overridden under certain conditions specified by an aspiration criterion.
In order to clarify the recency-based TS method, we next present a simple worked example.
  
< previous page
page_12
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_12.html2009-7-7 16:55:58

page_13
< previous page
page_13
next page >
Page 13
A Simple Example
Maximize f(x) = x3 60x2 + 900x + 100.
Table 1.7 Maximizing f (x)
Iteration
bit
string
f
new string
1
1
0 0 0 1 1
2287
2
1 1 0 1 1
343
3
1 0 1 1 1
1227
4
1 0 0 0 1
2692
**
(bit 4 tabu until iteration 5)
1 0 0 0 1
2
5
1 0 0 0 0
3236
**
1
0 0 0 0 1
941
2
1 1 0 0 1
725
3
1 0 1 0 1
1801
(bit 5 tabu until iteration 6)
1 0 0 0 0
3
4
tabu
5
tabu
1
0 0 0 0 0
100
2
1 1 0 0 0
964
**
(bit 2 tabu until iteration 7)
1 1 0 0 0
4
3
1 1 1 0 0
212
4
tabu
5
tabu
1
0 1 0 0 0
3972
**
(bit 1 tabu until iteration 8)
0 1 0 0 0
5
2
tabu
3
0 1 1 0 0
3988
4
0 1 0 1 0
4100
**
5
tabu
(bit 4 tabu until iteration 9)
0 1 0 1 0
6
1
tabu
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_13.html（第 1／2 页）2009-7-7 16:55:59

page_13
2
tabu
3
0 1 1 1 0
3684
**
4
tabu
(bit 3 tabu until iteration 10)
0 1 1 1 0
 
Again we start from (1 0 0 1 1), using a neighbourhood of size 4, sampled in cyclic order, and with a tabu tenure of 3 
iterations (Table 1.7). Here the use of tabu restrictions has effectively moved the search out of the attraction region of 
3236, and enabled it to find the optimum. However, as in the case of SA, the heuristic doesn't know the optimum has 
been found and carries on exploring. Unlike SA, however, the search may not remain close to the optimum.
  
< previous page
page_13
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_13.html（第 2／2 页）2009-7-7 16:55:59

page_14
< previous page
page_14
next page >
Page 14
Practical Considerations
In practice, the 'short-term' memory used in the concept of recency can be implemented in several ways. Perhaps the 
most obvious is to maintain a list of all solutions previously seen, and make any move tabu which would lead to a 
solution on this list. This would entail keeping an ever-increasing store of information, and searching it could be very 
expensive in computer time. This can be a problem even when only the most 'recent' solutions are stored. (More 
efficient methods based on hashing have been used with some success.)
However, it is usually convenient, and often more desirable, to focus on certain attributes of a solution. An attribute of a 
move from xnow to a tentative solution xtrial may mean any aspect that changes as a result of the move. Some natural 
types of attributes are shown in Table 1.8.
Table 1.8 Illustrative Move Attributes for a Move xnowto xtrial
(A1) Change of a selected variable xj from 0 to 1.
(A2) Change of a selected variable xk from 1 to 0.
(A3) The combined change of (A1) and (A2) taken together.
(A4) Change of c(xnow) to c(xtrial).
(A5) Change of a function g(xnow) to g(xtrial) (where g may represent a function that occurs 
naturally in the problem formulation or that is created strategically).
(A6) Change represented by the difference value g(xtrial) g(xnow).
(A7) The combined changes of (A5) or (A6) for more than one function g considered 
simultaneously.
 
What is meant by assigning values to a selected variable xj can be understood in a more general sense than the literal 
setting of its value to 0 or 1, although this may often be the context. However, it may also be used to refer to any 
operation that is used to define a neighbourhood, such as adding or deleting edges from a graph, assigning or removing 
a facility from a particular location, or changing the processing position of a job on a machine.
We note that a single move can give rise to multiple attributes. For example, a move that changes the values of two 
variables simultaneously may generate each of the three attributes (A1), (A2), and (A3). Attributes that represent 
combinations of other attributes may sometimes provide more information. Attributes (A5) to (A7) are based on a 
function g that may be strategically chosen to be completely independent from c. For example, g may be a measure of 
distance (or dissimilarity) between any given solution and a reference solution, such as the last local optimum visited or 
the best solution found so far, in which case attribute (A6) would indicate whether a trial solution leads the search 
further from or closer to the reference point.
Move attributes are often used in tabu search to impose restrictions that prevent moves from being chosen that would 
reverse the changes represented by these attributes. More precisely, when a move from xnow to xnext is performed that 
contains an attribute e, a record is maintained for the reverse attribute which we denote by 
,
  
< previous page
page_14
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_14.html2009-7-7 16:56:00

page_15
< previous page
page_15
next page >
Page 15
in order to prevent a move from occurring that contains some subset of such reverse attributes. Examples of some 
common kinds of tabu restriction are shown in Table 1.9.
Table 1.9 Illustrative Tabu Restrictions
A move is tabu if:
(R1) xj changes from 1 to 0 (where xj previously changed from 0 to 1).
(R2) xk changes from 0 to 1 (where xk previously changed from 1 to 0).
(R3) at least one of (R1) and (R2) occur. (This condition is more restrictive than either (R1) or 
(R2) separatelyi.e. it makes more moves tabu.)
(R4) both (R1) and (R2) occur. (This condition is less restrictive than either (R1) or (R2) 
separatelyi.e. it makes fewer moves tabu.)
(R5) both (R1) and (R2) occur, and in addition the reverse of these moves occurred 
simultaneously on the same iteration in the past. (This condition is less restrictive than 
(R4).)
(R6) g(x) receives a value v′ that it received on a previous iteration (i.e. v′ = g(x′) for some 
previously visited solution x′).
(R7) g(x) changes from v'' to v′, where g(x) changed from v′ to v" on a previous iteration (i.e. v′ 
= g(x′) and v" = g(x") for some pair of solutions x′ and x" previously visited in sequence.)
 
Again, combinations of attributes may be used, and an auxiliary function g may be defined independently from c.
A simple way of using such attributes in practice is to define arrays tabu_start(e) and tabu_end(e) which define the 
iterations on which attribute e begins and ceases to be active. This length of time is often called the tabu tenure, denoted 
by t. The choice of t may influence the quality of the solution obtained: having it too small increases the risk of 'cycling', 
while having it too large may restrict the search unduly. A value t = 7 has often been found sufficient to prevent cycling; 
other values commonly used are t = √n where n is some natural measure of problem size. Dynamic rules may be useful 
toousually this means choosing lower and upper limits tmin and tmax on the tabu tenure, and allowing t to vary in some 
way between them.
Finally, we should also mention the use of aspiration criteria. It may happen that a trial move that is tabu would lead to 
a new xbest. In such circumstances it would seem odd to reject this move, so an aspiration criterion may be invoked to 
deal with such cases. This is the simplest and most obvious case where an aspiration criterion may be useful, but Glover 
and Laguna[GL93] suggest several others.
1.6.3 
Frequency
Recency simulates short-term memory; Glover and Laguna[GL93] suggest that a form of long-term memory can be 
implemented by the use of a variety of frequency measures. Two types of frequency measure are identified: residence 
measures and transition
  
< previous page
page_15
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_15.html2009-7-7 16:56:00

page_16
< previous page
page_16
next page >
Page 16
measures. The former relate to the number of times a particular attribute is observed, the latter to the number of times an 
attribute changes from one value to another.
Frequency measures are normally ratios, where the numerators represent counts of the number of occurrences of a 
particular event (e.g. the number of times a particular attribute belongs to a solution or move), as in Table 1.10.
Table 1.10 Example Frequency Measures (Numerators)
(F1)
|S(xj = p)|
(F2)
|S(xj = p for some xj)|
(F3)
|S(to xj = p)|
(F4)
|S(xj changes)|, i.e. |S(from-or-to xj = p for some p)|
(F5)
(F6)
Replace S(xj = p) in (F5) with S(xj≠ to xj = p)
(F7)
Replace c(x) in (F6) with a measure of the influence of the solution attribute xj = p
 
Here S is a set of solutions seen during the search so far (not necessarily all the solutions seen), and symbols such as S
(xj = p) denote the subset of S for which it is true that xj = p.
Measures (F1) to (F4) are fairly obvious; the measures (F5) to (F7) are weighted measures, created by reference to 
solution quality in (F5) and (F6), and by reference to move influence in (F7). Measure (F5) may be interpreted as the 
average c(x) value over S when xj = p. This quantity can be directly compared to other such averages or can be 
translated into a frequency measure using appropriate denominators.
Table 1.11 Denominators
(D1) The total number of occurrences of all events represented by the numerators (such as the 
total number of associated iterations).
(D2) The sum of the numerators.
(D3) The maximum numerator value.
(D4) The average numerator value.
 
The denominators represent quantities such as those in Table 1.11. Denominators (D3) and (D4) give rise to what may 
be called relative frequencies. In cases where the numerators represent weighted counts, some of which may be 
negative, (D3) and (D4) are expressed as absolute values and (D2) is expressed as a sum of absolute values (possibly 
shifted by a small constant to avoid a zero denominator).
  
< previous page
page_16
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_16.html2009-7-7 16:56:01

page_17
< previous page
page_17
next page >
Page 17
In either case, the frequency measures are usually used to generate penalties which modify the objective function c(x) to 
c(H, x) as discussed above. Diversification is thereby encouraged by the generation of solutions that embody 
combinations of attributes significantly different from those previously encountered. Conversely, intensification is 
promoted by incorporating attributes of solutions from selected élite subsets (implicitly focussing the search in 
subregions defined relative to these subsets).
1.6.4 
Summary
We have not formally discussed the concepts of quality and influence, although they have already been seen implicitly 
in some of the discussion above. It may thus be helpful to provide a brief reiteration of the basic notions.
Quality in TS usually refers simply to those solutions with good objective function values; a collection of such solutions 
may be used to stimulate a more intensive search in the general area of these élite solutions, as already mentioned above.
Influence is, roughly speaking, a measure of the degree of change induced in solution structurecommonly expressed in 
terms of the distance of a move from one solution to the next. It is an important aspect of the use of aspiration criteria, 
and is also relevant to the development of candidate list strategies. A candidate list is simply a list of, for example, the 
most promising moves to be explored at a given iteration, which are usually identified with respect to moves either used 
or ignored at previous iterations. Reeves[Ree95a] found such an approach highly beneficial in the context of a machine 
sequencing problem.
These ideas are treated comprehensively in Glover and Laguna [GL93], along with a comprehensive review of 
applications which suggests that tabu search is a very effective way of navigating through large and complex solution 
spaces.
From a practical point of view, a recency-based approach with a simple neighbourhood structure which is searched 
using a restricted candidate list strategy will often provide very good results.
1.7 
Genetic Algorithms
Genetic algorithms (GAs) have become perhaps the most well-known of all these modern heuristics, having had many 
column-inches of newsprint and several TV programmes devoted to them. They can also be viewed as a form of 
neighbourhood search (Reeves[Ree94]; Rayward-Smith[RS95b]), although their original inspiration comes from 
population genetics. Unlike SA and TS, GAs use a collection (or population) of solutions, from which, using selective 
breeding and recombination strategies, better and better solutions can be produced. Simple genetic 'operators' such as 
crossover and mutation are used to construct new solutions from pieces of old ones, in such a way that for many 
problems the population steadily improves.
  
< previous page
page_17
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_17.html2009-7-7 16:56:02

page_18
< previous page
page_18
next page >
Page 18
1.7.1 
Crossover
Crossover is a simple idea: suppose we have 2 strings a and b, each consisting of 6 variables, i.e.
representing two solutions to a problem. A crossover point is chosen at random from the numbers 1, . . ., 5, and a new 
solution produced by combining the pieces of the the original 'parents'. For instance, if the crossover point was 2, then 
the 'offspring' solutions would be
In many applications, the component vector, or chromosome, is simply a string of 0s and 1s. Goldberg[Go189] suggests 
that there are significant advantages if the chromosome can be so structured, although other arguments [Ant89] have 
cast some doubt on this. Nevertheless, much of the theoretical development is easier to understand if it is thought of in 
this way.
Continuing the genetic analogy, variables are often called genes, the possible values of a variable alleles, and the 
position of a variable in a string is called its locus. In simple cases, the locus of a variable/gene is usually irrelevant, but 
in more complex problems it becomes important. A further distinction is drawn, in genetics, between the chromosome 
(or a collection of chromosomes) as genotype, meaning the actual structure (or structures), and the phenotypethe 
physical expression of the structure (or structures). In terms of a GA, we may interpret the genotype as the coded string 
which is processed by the algorithm, while the decoded set of parameters represents the phenotype.
1.7.2 
Mutation
The other most commonly used operator is mutation, which provides the opportunity to reach parts of the search space 
which perhaps cannot be reached by crossover alone. Each gene of a string is examined in turn, and with a small 
probability its current allele is changed. For example, a string
if the 3rd and 5th alleles are mutated.
1.7.3 
Reproduction
Crossover and mutation are the basic tools for creating new solutions (reproduction). However, which chromosomes are 
chosen as a basis for the reproductive step (or selected for reproduction) is clearly critical in what happens to a 
population as a whole. Holland[Hol75], who was responsible for the initial development of GA concepts, suggested that 
at least one 'parent' should always be chosen on the basis of its 'fitness'in terms of combinatorial problems, this implies 
some monotonic function of the objective function value. With this he was able to prove the important Schema
  
< previous page
page_18
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_18.html2009-7-7 16:56:05

page_19
< previous page
page_19
next page >
Page 19
Theorem which provides one way of understanding how a GA works. Details of this can be found in Goldberg's book
[Gol89], and also in Reeves[Ree93a], but the basic idea is fundamental to traditional GA theory, so we describe it here.
If we take two strings such as
we notice something they have in common: they both have a 0 for the first gene, and a 1 for the second. In GA jargon, 
they are both instances of the schema (01∗∗∗∗), where the ∗ symbol is essentially used as a 'don't care' character. 
Schemata thus represent similarity subsets of the hypercube over which the search is taking place. In this case the subset 
consists of all 16 (= 24) strings generated by substituting particular values for the ∗ symbols. The order of a schema is 
the number of defined (0 or 1) genes, while its length is the distance between the outermost defined genes. (In the above 
example, the order is 2, the length is 1.)
Using this way of grouping strings together, Holland showed that, given a fixed number of instances of a schema S in 
the population at generation t, the expected number of instances of the schema at generation t + 1 would increase 
provided that its fitness ratio f(S) (its fitness divided by the average population fitness) exceeded
where l(S) is the length and k(S) the order of S, and Pm the probability of mutation. This is often interpreted as saying 
that low-order, short and above-average schemata will propagate at an exponentially increasing rate, but this is 
incorrectall it says is what is expected to happen in the next generation. The actual number of instances of S may be 
quite different from the expected number. Nevertheless, it provides a basis for an explanation of how GAs work. 
Holland also put forward other arguments, but they are not really germane to this discussion.
1.7.4 
A Simple Example
To make the action of a GA more transparent, we again use the problem of maximizing the function
Suppose now we generate five random strings with the results as shown in Table 1.12.
  
< previous page
page_19
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_19.html2009-7-7 16:56:06

page_20
< previous page
page_20
next page >
Page 20
Table 1.12 Five random strings
No.
String
x
f(x)
P[select]
1
10011
19
2399
0.206
2
00101
5
3225
0.277
3
11010
26
516
0.044
4
10101
21
1801
0.155
5
01110
14
3684
0.317
Average fitness
2325
 
Treating the f(x) values simply as fitness values, the probability of selecting each string as Parent 1 is in direct 
proportion to its value, and is given in the P[select] column. By drawing uniform random numbers between 0 and 1, we 
select Parent 1 using this distribution, and Parent 2 using a discrete uniform distribution. Again, by drawing uniform 
random numbers, we proceed to carry out crossover (with Pc = 1) at a randomly determined site, followed by mutation 
(with Pm = 0.02 at each locus). The results of a typical experiment were as shown in Table 1.13.
Table 1.13 A typical experiment.
Step
Parent 
1
Parent 
2
Crossover point
Mutation?
Offspring
String
f(x)
1
1
2
4
NNNYN
10001
2973
2
5
3
2
NNNNN
01010
4100
3
5
2
3
NNNNN
01101
3857
4
4
2
1
NYNNN
11101
129
5
2
5
4
NNNNN
00100
2804
Average fitness
2773
 
The search would continue by recalculating the selection probabilities and carrying out a new cycle of selection, 
crossover and mutation. This is a very simple GA, but it has been effective in raising the population average quite 
substantially, and it has actually found the optimum at step 2.
  
< previous page
page_20
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_20.html2009-7-7 16:56:08

page_21
< previous page
page_21
next page >
Page 21
1.7.5 
Modifications and Enhancements
The basic GA has been developed and improved to an enormous extent over the past 20 years. Some of the most 
important of these are described in Reeves[Ree93a]; here we give a brief description, without giving a detailed 
justification for the various proposals.
Selection in proportion to fitness is common to nearly everything that can be called a GA. However, the exact 
mechanism for accomplishing it can vary significantly. The simplest is roulette wheel selection, where chromosomes 
are sampled using a cumulative probability distribution which reflects their relative fitnesses (as in the example above). 
In practice this introduces too much noise into the selection process, and normally it is better to use something like 
stochastic universal selection, which can be thought of as a roulette wheel which is sampled with M equally spaced 
pointers. Hancock[Han94] gives a useful summary of the effects of many different selection methods.
However, the fitness values usually have to be repeatedly re-scaled as the search progresses, and in many 
implementations, it is better to carry out selection using the ranks rather than the actual fitnesses. This can be carried out 
very simply using the probability distribution
where [k] is the kth chromosome when they are ranked in ascending order. This distribution seems to give a sensible 
selective pressure, in that the best (chromosome [M]) will have a chance of 2/(M + 1) of being selected, roughly twice 
that of the median, whose chance of selection is 1/M. However, there are many other ways of using ranks.
Another approach is tournament selection where groups of T chromosomes are selected from the population and 
compared, the best being selected. When T = 2, this is very similar in its effect to the ranking approach suggested above.
Crossover has been developed from the simple 1-point crossover described above, to 2-point, 3-point and indeed multi-
point crossover. There are certainly some situations where 1-point crossover is less effective, but it is hard to give a 
definitive statement as to which is best in general. Uniform crossover, where each bit is equally likely to come from 
either parent, is often useful, as it makes no assumptions about the compactness of the coding of the strings. (A biased 
version where selection is preferentially from one parent is also possible.)
Many other special-purpose recombination operators have been devised; perhaps the most important non-binary family 
comprises those developed for sequence-based coding. Many problems of interest in OR can be most naturally 
represented as a permutation, the obvious one being the TSP. Unfortunately, the ordinary crossover operator is no use in 
such situations, as can easily be seen from the following example.
  
< previous page
page_21
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_21.html2009-7-7 16:56:08

page_22
< previous page
page_22
next page >
Page 22
Goldberg and Lingle[GL85] defined an operator called PMX (partially mapped crossover), which used two crossover 
points. The section between these points defines an interchange mapping. Thus, in the example above, PMX might 
proceed as follows:
Here the crossover points X and Y define an interchange mapping {3 ↔ 6,4 ↔ 2,5 ↔ 7}.
Goldberg[Gol89] describes two other types of crossover for sequence representations, while Reeves[Ree95] used yet 
another which performed well in the context of a flowshop sequencing problem. This operator works by choosing a 
crossover point X randomly, taking the pre-X section of the first parent, and filling up the chromosome by taking in 
order each 'legitimate' element from the second parent. For the example above it might generate the following offspring:
The rationale for this operator is that it preserves the absolute positions of the jobs taken from P1, and the relative 
positions of those from P2, which seems to provide enough scope for modification of the chromosome without 
excessively disrupting it. It should perhaps be noted here that neither of these operators is actually very suitable for the 
TSP; some reasons for this, and the consequent development of criteria for better operators, are contained in the work of 
Radcliffe (in, for example, Radcliffe[Rad91] and Radcliffe and Surry[RS95a]).
Mutation is usually employed with a very low probability per bit. There is some evidence however that a somewhat 
higher rate of around 1/n (where n is the string length) should be used. Some experimental studies have also found that 
it may be beneficial to change the rate of mutation as the search progresses, and also that it may be better to have 
different rates for different bits.
Replacement is often carried out en bloc, and this was the underlying assumption in the description of the basic GA 
given above: M new strings are generated from the current population and become the next generation. This 
generational approach is popular, but a significant fraction of the GA community prefer to use incremental 
replacement. In this case a few strings are generated (often just one), and then used to replace some of the existing 
population. This approach (sometimes called a steady-state GA) is almost always quicker, but may lead to premature 
convergence with small populations. For this reason, it is important to consider carefully the deletion strategy: one 
approach is always to delete the worst, but this may often converge far too quickly, and something less severe may be 
better overall unless the population is really large. The previously-cited paper by Hancock [Han94] is a useful source of 
information on the possibilities.
  
< previous page
page_22
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_22.html2009-7-7 16:56:09

page_23
< previous page
page_23
next page >
Page 23
1.7.6 
Summary
Like SA and TS, GAs provide the user with several parameters to tune. But also like the other heuristics, GAs are often 
fairly robust, and simple one-point or uniform crossover, a 1/n mutation rate, and an incremental replacement policy 
with a mild deletion strategy will often give good results.
1.8 
Conclusion
These notes have given a brief introduction to three important modern heuristic methodssimulated annealing, tabu 
search and genetic algorithms. The focus of the first two has traditionally been that of combinatorial optimization, while 
the last has more commonly dealt with problems where real-valued parameters are discretized. However, this distinction 
is really artificial  by discretizing a set of continuous parameters, the problem does become combinatorial. GAs can be 
(and have been) used successfully for combinatorial problems, while reports are starting to appear where SA and TS 
have been applied to discretized versions of continuous problems.
This points up a remarkable fact about all of these methods: we do not need to know anything about the problem we are 
trying to solve, for the method will work so long as we have a 'black box' which will evaluate the solution proposed. 
Nor do we have to assume all quantities are exact or deterministicthere are several reported applications in the case of 
GAs (in, for example, Grefenstette and Fitzpatrick[GF85]; Reeves[Ree92]) which use approximate and/or noisy 
evaluation functions, and still get solutions of high quality. There is also considerable scope for parallelization of the 
methods, and in the case of GAs in particular such concepts have led to interesting new ideas of 'spatial' as well as 
'temporal' population interaction.
Having said all that, it is nonetheless true that in many cases the performance of the techniques can be considerably 
enhanced by building in some problem-specific knowledge. The use of heuristic methods should not be a substitute for 
thought! For instance, it should be realized that constrained problems may give rise to some difficulties for these 
methods. The conventional practice is to incorporate constraints into the objective by using penalty factors. However it 
can be difficult to find appropriate penalty terms, and alternatives such as a modified definition of a neighbourhood (for 
SA and TS), or modified recombination operators (for GAs) should be explored.
Finally, it is becoming clear that there is plenty of scope for traditional OR methods to be used in conjunction with their 
more modern counterparts, and much research is going into building more effective hybrids. There are already several 
examples in the specialist literature of cases where combining elements of GAs with SA, or of SA with TS, has been 
found helpful, although an introductory chapter is not the place to describe these developments. However, it is hoped 
that sufficient has been said here for interested but previously unaware readers to be able to understand the remaining 
chapters of this volume, and also to encourage them to explore the wider literature for ideas that will help them to find 
better solutions to messy real-world optimization problems.
  
< previous page
page_23
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_23.html2009-7-7 16:56:10

page_24
< previous page
page_24
next page >
Page 24
References
[Ant89] Antonisse J. (1989) A new introduction of schema notation that overturns the binary encoding constraint. In 
Schaffer J. D. (ed) Proceedings of 3rd International Conference on Genetic Algorithms. Morgan Kaufmann, Los Altos, 
CA.
[Con90] Connolly D. T. (1990) An improved annealing scheme for the qap. Euro.J.OR 46: 93100.
[Cro58] Croes G. A. (1958) A method for solving travelling salesman problems. Opns.Res. 6: 791.
[Dow93] Dowsland K. (1993) Simulated annealing. In Reeves [Ree93c], chapter 2.
[GF85] Grefenstette J. J. and Fitzpatrick J. M. (1985) Genetic search with approximate function evaluations. In 
Grefenstette J. J. (ed) Proceedings of an International Conference on Genetic Algorithms and their applications. 
Lawrence Eribaum Associates, Hillsdale, NJ.
[GL85] Goldberg D. E. and Lingle R. (1985) Loci and the travelling salesman problem. In Grefenstette J. J. (ed) 
Proceedings of an International Conference on Genetic Algorithms and their applications. Lawrence Erlbaum 
Associates, Hillsdale, NJ.
[GL93] Glover F. and Laguna M. (1993) Tabu search. In Reeves [Ree93c], chapter 3.
[Glo86] Glover F. (1986) Future paths for integer programming and links to artificial intelligence. Computers & Ops.
Res. 5: 533549.
[Gol89] Goldberg D. E. (1989) Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley, 
Reading, Mass.
[Han94] Hancock P. J. (1994) An empirical comparison of selection methods in evolutionary algorithms. In Fogarty T. 
C. (ed) Evolutionary Computing: AISB Workshop, Leeds, UK, April 1994: Selected Papers, number 865 in Lecture 
Notes in Computer Science. Springer-Verlag, Berlin.
[Hol75] Holland J. H. (1975) Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann Arbor.
[Joh54] Johnson S. M. (1954) Optimal two- and three-stage production schedules with setup times included. NRLQ 1: 
6168.
[Kar72] Karp R. M. (1972) Reducibility among combinatorial problems. In Miller R. E. and Thatcher J. W. (eds) 
Complexity of Computer Communications. Plenum Press, New York.
[KGV83] Kirkpatrick S., Gellat C. D., and Vecchi M. P. (1983) Optimization by simulated annealing. Science 220: 
671680.
[Kuh55] Kuhn H. W. (1955) The Hungarian method for the assignment problem. NRLQ 2: 8397.
[LA88] Laarhoven P. V. and Aarts E. H. L. (1988) Simulated Annealing: Theory and Applications. Kluwer, Dordrecht.
[LLKS85] Lawler E., Lenstra J., Kan A. R., and Shmoys D. (eds) (1985) The Travelling Salesman Problem. John Wiley 
& Sons, Chichester.
[LM86] Lundy M. and Mees A. (1986) Convergence of an annealing algorithm. Math.Prog. 34: 111124.
[MRTT53] Metropolis N., Rosenbluth A. W., Teller A. H., and Teller E. (1953) Equation of state calculation by fast 
computing machines. J. of Chem. Phys. 21: 10871091.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_24.html（第 1／2 页）2009-7-7 16:56:10

page_24
[Rad91] Radcliffe N. J. (1991) Equivalence class analysis of genetic algorithms. Complex Systems 5: 183205.
[Ree92] Reeves C. R. (1992) A genetic algorithm approach to stochastic flowshop sequencing. In Proceedings of IEE 
Colloquium on Genetic Algorithms for Control and Systems Engineering. IEE, London.
[Ree93a] Reeves C. R. (1993) Genetic algorithms. [Ree93c], chapter 4.
[Ree93b] Reeves C. R. (1993) Improving the efficiency of tabu search in machine
  
< previous page
page_24
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_24.html（第 2／2 页）2009-7-7 16:56:10

page_25
< previous page
page_25
next page >
Page 25
sequencing problems. J. Opl. Res. Soc 44: 375382.
[Ree93c] Reeves C. R. (ed) (1993) Modern Heuristic Techniques for Combinatorial Problems. Blackwell Scientific 
Publications, Oxford.
[Ree94] Reeves C. R. (1994) Genetic algorithms and neighbourhood search. In Evolutionary Computing: AISB 
Workshop, Leeds, UK, April 1994: Selected Papers, number 865 in Lecture Notes in Computer Science. Springer-
Verlag, Berlin.
[Ree95] Reeves C. R. (1995) A genetic algorithm for flowshop sequencing. Computers & Ops. Res. 22: 513.
[Rei94] Reinelt G. (1994) The Travelling Salesman. Springer-Verlag, Berlin.
[RS95a] Radcliffe N. J. and Surry P. (1995) Formae and the variance of fitness. In Whitley D. and Vose M. (eds) 
Foundations of Genetic Algorithms 3. Morgan Kaufmann, San Mateo, CA.
[RS95b] Rayward-Smith V. J. (1995) A unified approach to tabu search, simulated annealing and genetic algorithms. In 
Rayward-Smith V. J. (ed) Applications of Modern Heuristic Techniques. Alfred Waller Ltd., Henley-on-Thames, UK.
  
< previous page
page_25
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_25.html2009-7-7 16:56:11

page_28
< previous page
page_28
next page >
Page 28
with an associated domain D1, . . ., Dn. A complete label for a CSOP is an assignment of a value to each variable from 
its associated domain. A consistent label is one that satisfies all the constraints. A solution is a complete and consistent 
label. An optimal label is a solution that optimizes the objective function. This is achieved by using an objective 
constraint which is associated with the objective function; this constraint is soft in the sense that its admissible values 
are not fixed but are to be optimized.
Many constraint satisfaction and constraint optimization techniques have been developed over the years. The focus in 
this paper is on a stochastic technique which has proved to be effective for large-scale applications. Simulated annealing 
(SA) [KGV83] is an iterative stochastic search method derived from the physical annealing process whereby material is 
gradually cooled to achieve a minimum energy state. It involves a process of moving from one solution to a 
neighbouring solution under the control of an annealing schedule. The process starts with a high temperature and 
gradually drops the temperature after a certain number of iterations at each temperature.
The success of SA on a given problem typically depends on devising an appropriate annealing schedule. In general, 
approximating a stationary distribution arbitrarily closely requires an exponential-time execution of the simulated 
annealing algorithm [AvL85]. Since annealing schedules must be short for practical reasons, they may not produce a 
good approximation to the stationary distributions, which guide the search procedure to the global minima.
By the nature of a practical annealing schedule, the cooling rate will slow down with the decrease in temperatures; 
consequently the majority of search time will be spent at low temperatures. Spending a lot of time on greedy local 
search at low temperatures will overlook many good solutions if the approximate distributions are not close to the 
stationary distributions at these temperatures. It will be worse if the search is trapped by local optima.
In summary, given a limited search time there are two issues in devising a proper annealing schedule:
1. Closely approximating stationary distributions at high temperatures could occupy all the search time, and leave no 
time for finding good or optimal solutions.
2. Greedily focusing on the better solutions in current local spaces will sacrifice the approximation of stationary 
distributions, and this will degrade the probability of finding globally good solutions.
There is a fundamental need for 'compromise': the tasks of approximating stationary distributions and approaching 
globally good solutions are in conflict due to the practical limitation on search time. An annealing schedule represents a 
compromise in this conflict; but there are difficulties in finding a good one.
1. Devising a suitable annealing schedule is highly problem-dependent. Good annealing schedules vary from problem to 
problem.
2. The criteria for judging an annealing schedule are empirical. It is typical that a good annealing schedule is confirmed 
only after testing a set of parameters.
Nevertheless, there is a tendency to use certain general principles in devising an annealing schedule for practical 
applications [CEG87], in particular, the following.
  
< previous page
page_28
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_28.html2009-7-7 16:56:11

page_27
< previous page
page_27
next page >
Page 27
2 
Localized Simulated Annealing in Constraint Satisfaction and Optimization
Y. H. Li, E. B. Richards, Y. J. Jiang and N. A. S. Azarmi
Abstract
Simulated annealing (SA) is a stochastic technique for solving constraint satisfaction and optimisation problems. 
Research on SA usually focuses on neighbourhood operators and annealing schedules. While SA can theoretically 
converge to a global optimum with stationary distributions, this requires an exponential-time execution of the annealing 
algorithm; in practice only a short annealing schedule can be used. To devise an appropriate annealing schedule one 
must consider how much time to spend at high temperatures for approximating stationary distributions and how much 
time to spend at low temperatures for greedily approaching better solutions. Unfortunately these two things are always 
in conflict. Annealing schedules are problem-dependent, and deciding a good one for a specific problem relies on 
empirical studies. This makes it difficult to balance the conflict. We argue that the process of balancing the conflict 
should be a dynamic procedure rather than a static one. We present a refined SA strategy that can adjust annealing 
schedules according to the solution distribution formed in past search. The basic idea is to divide the search space into 
disjoint subspaces which are then processed by a localized SA strategy. Different temperatures and annealing speeds 
can be maintained in the different subspaces depending upon certain evaluation criteria. To assess the performance of 
the proposed strategy, we apply it to two real-world applications: flight scheduling problems and freight train 
regrouping problems. Experimental results demonstrate that the proposed SA strategy is consistently better than 
standard SA.
2.1 
Introduction
A constraint satisfaction and optimisation problem (CSOP) [Tsa93] consists of an objective function and a number of 
constraints on a set of variables X1, . . ., Xn each
Modern HeuristicSearch Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_27
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_27.html2009-7-7 16:56:12

page_29
< previous page
page_29
next page >
Page 29
A very high initial acceptance probability or a very high starting temperature
Cooling rates usually from 0.8 to 0.99
Iterations for each temperature which typically equal the number of neighbours
A terminating criterion which could be transferred to a very low temperature
For a given problem, different annealing schedules will produce different qualities of final solutions. This suggests that 
an annealing schedule interacts with certain problem features during the problem-solving procedure. It is our 
assumption that the quality of the interaction will be enhanced by applying different annealing schedules to different 
parts of the search space.
We propose a new version of simulated annealing called 'localized simulated annealing (LSA)'. The basic idea is to 
divide the search space of a problem into disjoint subspaces. These subspaces provide local information to influence the 
annealing characteristics in the subspaces. During the search process, each subspace is evaluated according to criteria 
that relate to the features of the subspace, and the annealing procedure in this subspace is adjusted accordingly. This 
facilitates different annealing speeds in different subspaces.
Unlike standard simulated annealing, the proposed method keeps repairing the probability distributions by maintaining 
high temperatures in no-good subspaces. It also allows the move from a low-temperature subspace to a high-
temperature subspace and vice versa. When a global annealing schedule efficiently guides the search process, most 
subspaces will have similar temperatures. In this case the behaviour of the annealing schedule will be similar to that of 
standard SA. However, when the global annealing schedule does not efficiently guide the search process, LSA can still 
escape from local minima that would normally trap standard simulated annealing. In this way it increases the search 
efficiency by gradually improving the convergence direction.
We shall offer empirical justification for the effectiveness of LSA. We shall apply the method to two real-world 
applications: the flight scheduling problem and the freight train regrouping problem. Empirical results on these 
problems have shown that the proposed SA consistently reaches a higher-quality solution than standard SA.
This paper is organized as follows. Section 2.2 introduces the formulation of subspaces and their evaluations. Section 
2.3 describes the localized SA strategy. Section 2.4 provides a theoretical analysis of the proposed SA and justifications. 
Section 2.5 presents the empirical results of the proposed strategy on some real-world applications.
2.2 
Subspaces and Their Evaluation
The search space of a combinatorial optimization problem consists of the possible combinations of values from the 
domains of all variables of the search problem. A subspace of the problem is characterized by certain sub-domains of a 
subset of the variables of the search problem. Ideally, the division of a search space should reflect the problem structure 
of an application. This may not always be possible to determine
  
< previous page
page_29
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_29.html2009-7-7 16:56:12

page_30
< previous page
page_30
next page >
Page 30
in practice. For the purpose of this paper, we choose a division of the domain of a particular variable into a manageable 
number of disjoint subdomains.
In simulated annealing, once the temperature is fixed, the only thing that determines the acceptance probability is the 
distance between the cost of the current solution and the cost of its neighbouring solution. Any neighbouring solution 
that has a similar cost to the current best cost will obviously have a high probability of being accepted. So the current 
best solution can be regarded as the representative of these solutions and can be treated as a quasi-equilibrium point; all 
solutions around this point have a higher probability of forming the quasi equilibrium. Similarly, the local best can 
represent the quasi equilibrium point of a subspace and the global best represents the quasi equilibrium point of the 
entire search space. The best solutions in individual subspaces can be used to assess these subspaces and the gap 
between a local best and the global best will reflect the goodness of a subspace for the global search. Here we introduce 
a new term 'approachability' to characterize a subspace.
Definition 1
We define the approachability of a subspace as a measure of the goodness of the subspace with respect to the global 
best.
The evaluation procedure for the approachability of a subspace is roughly as follows. In carrying out a search of the 
whole search space, randomly jumping among all subspaces according to the neighbourhood generator, the procedure 
records the local best S* for all subspaces and the current global best solution Sb. At the end of each temperature stage, 
the procedure measures the distances between Sb and each S*. If |cost(S*) cost(Sb)| > εT (where εT is a control 
parameter for the evaluation of approachability), in other words the local best solution is not reasonably near to the 
global best solution, we say the subspace represented by S* is no-good. Otherwise we say this subspace is good.
The control parameter εT determines what LSA 'looks like'. For example, if the values of εT are large, LSA will behave 
like standard simulated annealing, because the approachabilities of all subspaces are similar at any temperature. On the 
other hand, if the values are very small, the behaviour will more likely be random search. Since LSA is essentially SA, 
small values should initially be avoided. Ideally, εT should be set to maximize the goodness of the approachability of 
each subspace at each temperature.
2.3 
Localized Simulated Annealing (LSA)
The motivation behind the approachability measure of subspaces is to reduce the conflict between approximating 
stationary distributions and approaching globally good solutions.
The most important aspect of the LSA algorithm is its flexible annealing schedule. Standard SA uses the same annealing 
schedule for the whole search space. This kind of annealing procedure may 'overlook' some good parts of the search 
space if the annealing schedule is not designed carefully. In most real-world applications, it is impractical to admit a 
large number of iterations at each temperature or a sufficiently slow annealing speed. This means that the annealing 
procedure inevitably 'cuts of' some good search areas. The different distributions of feasible solutions among different 
subspaces call for different annealing schedules in the local spaces. The proposed SA
  
< previous page
page_30
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_30.html2009-7-7 16:56:13

page_31
< previous page
page_31
next page >
Page 31
thus employs a localized annealing strategy that allows different annealing speeds in different subspaces.
The LSA annealing strategy contains two parts.
1. A global annealing schedule.
This schedule is exactly the same as that of standard SA. It involves an initial temperature, a rate of temperature 
decrease, a number of iterations at each temperature and a terminating criterion.
(a) The initial temperature is the starting temperature for all subspaces.
(b) The temperature in the subspace that contains the current best solution is always equal to the globally current 
temperature.
(c) After completing the fixed number of iterations at a given temperature, the algorithm evaluates the subspaces 
and at least the subspace that contains the current best solution will decrease its temperature to the new globally 
current temperature.
(d) When the terminating criterion is satisfied, the algorithm will stop.
2. A localized annealing procedure among the subspaces.
The localized annealing procedure applies different annealing speeds to different subspaces. It involves the following.
(a) A procedure for evaluating the approachability of subspaces. Note that the evaluation parameter εT of the 
approachability of a subspace is dependent on the temperature, and its value decreases with the reduction of the 
temperature.
(b) A procedure for decreasing the temperatures of subspaces. The approachability of a subspace is a criterion for 
decreasing the temperature of this subspace. The size of the temperature decrease is decided by εT. For example, 
suppose a subspace S1 has the temperature Ti while the subspace that contains the current global best solution has 
temperature Tj and Ti > Tj. Given that |cost(S1best) cost(globalbest)| 
 where Ti > Tk≥ Tj (and there is no 
temperature lower than Tk which satisfies the given conditions), the algorithm decreases the temperature of the 
subspace from Ti to Tk. This procedure is applied to all subspaces.
(c) The use of the temperatures of subspaces. Suppose a new solution j, generated from a solution i by a neighbour 
generator move, is in a different subspace than the one containing i; the algorithm uses the temperature of the 
subspace that contains i to calculate the acceptance probability. If the current subspace containing i has a higher 
temperature than the new subspace containing j, the search procedure has a relatively high probability of going to 
the new subspace. Conversely, the search procedure has a relatively low probability of going to the new subspace 
if the temperature is lower.
It is important to note that LSA employs the same neighbourhood operator that is employed by a standard SA 
procedure. The difference is that the neighorhood operator can move from one subspace to another subspace.
  
< previous page
page_31
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_31.html2009-7-7 16:56:13

page_32
< previous page
page_32
next page >
Page 32
The detailed procedure of LSA is described in Figure 2.1. In this procedure, K denotes the Kth temperature stage, TK 
the temperature of the Kth temperature stage, LK the iterations at the temperature TK, f the cost function over solutions, 
lbest( subspace i) the local best solution of the subspace in which solution i occurs, gbest the current global best solution 
for all subspaces, Tsubspace i the temperature of the subspace of the solution i, and Tglobal(k) the global temperature at 
the Kth temperature stage of the LSA schedule.
The procedure starts from an initial random solution istart. The initial temperature of all subspaces is T0. Suppose j is 
the neighbour solution of solution i; if j is better than lbest(subspace j), j will become lbest(subspace j). Furthermore, if j 
is better than the current gbest, j will become the gbest and the temperature will be immediately dropped to the current 
global temperature that is the temperature of the subspace containing the current best solution. After each LK iteration 
the global temperature is decreased, which is done by Calculate TEMPERATURE(Tglobal(K)). The Calculate 
TEMPERATURES(subspaces) calculates what temperature each subspace should be dropped to based on the evaluation 
of the subspace by Evaluate(subspaces).
2.4 
Analysis and Justification
Simulated annealing uses the same annealing speed for the whole search space. From a theoretical point of view the 
global optimal convergence of simulated annealing requires a sufficient amount of search time which is mainly related 
to two factors: (1) speed of decreasing temperature and (2) number of iterations at each temperature stage [JMS89]. In 
practice, we often cannot achieve a sufficiently slow rate of decreasing temperatures or sufficiently many iterations at 
each temperature. This is especially so for large-scale problems in reactive environments where one can allow only a 
very limited amount of search time at each temperature Tk with a limited number of temperature-decreasing stages. This 
produces two problems. Let qj(Tk) denote the stationary distribution at temperature Tk and aj(Tk) denote the quasi-
equilibrium at temperature Tk [AK89].
1. At each temperature Tk, the quasi equilibrium aj(Tk) is not sufficiently close to the stationary distribution qj(Tk). At 
the next temperature Tk+1, aj(Tk) cannot be used to generate an aj(Tk+1) that is close to qj(Tk+1). In other words aj(Tk) 
cannot be relied upon to produce a good aj(Tk+1).
2. Because of the nature of the practical annealing schedule, the cooling speed at low temperatures is much slower than 
at high temperatures. Thus a majority of search time will be spent at low temperatures. The lower a temperature is, the 
fewer will be the solutions the SA algorithm will visit at that temperature. Moreover at low temperatures good quasi 
equilibriums will require longer Markov chains than at high temperatures. So finding better solutions than the local 
optima supported by good quasi equilibriums at low temperatures is not likely. As a result, the SA algorithm will more 
likely stay in the local spaces around the local optima. Because of the low quality of the quasi
  
< previous page
page_32
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_32.html2009-7-7 16:56:14

page_33
< previous page
page_33
next page >
Page 33
begin 
     INITIALIZE (istart,T0,L0); 
     K := 0; 
     i := istart; 
     repeat 
          for l := 1 to LK do 
          begin 
                j := Neighbour(i); 
                    if f(j) < f(lbest(subspace j)); 
                    then lbest(subspace j) := j 
              if f(j) ≤ f(i) 
              then i := j 
                    if f(j) < f(gbest) 
                    then gbest := j 
                    endif; 
                    if f(j) ≤ f(gbest) 
                    then Tsubspace j := Tglobal(K) 
                    endif 
              else 
                    if e ( f (i)  f  ( j )) / Tsubspace i >random(0,1) 
                   then i := j 
                   endif 
             endif 
         end; 
         K := K + 1; 
         Calculate LENGTH(LK); 
         Calculate TEMPERATURE(Tglobal (K )); 
         Evaluate(subspaces); 
         Calculate TEMPERATURES(subspaces); 
    until stopcriterion 
end
Figure 2.1 
Localized simulated annealing.
  
< previous page
page_33
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_33.html2009-7-7 16:56:15

page_34
< previous page
page_34
next page >
Page 34
equilibriums, many good solutions may not be found and the local optima found may not be good enough.
Unlike SA, LSA keeps repairing the quasi equilibria of previous temperatures while it runs the global annealing 
schedule. If the solution distribution in a subspace is not good, LSA will leave the temperature at a higher level than the 
temperatures in those subspaces that have better solution distributions. Once the search is back to this subspace, LSA 
will visit more solutions than SA in this subspace and these solutions can improve quasi equilibrium for the temperature 
of the subspace. If the currently global temperature is low, LSA can use the available search time more effectively than 
would be the case if search were to continue in the local spaces that are around current local optima.
In conclusion there are at least two beneficial results arising from LSA.
LSA uses search time more effectively by identifying and avoiding no-good local spaces.
LSA revises and improves approximate stationary distributions.
In effect LSA can dynamically enhance the 'balance' of approximating stationary distributions and approaching good or 
optimal solutions. To support these claims let us analyse how LSA moves from one subspace to another. Suppose that 
LSA generates a neighbour j of i and j is accepted. Let T(i) and T(j) denote respectively the temperatures of the 
subspace containing i and the subspace containing j. Then there are several cases.
1. T(i) 
T(j). In this case the search procedure is not different from the standard SA procedure.
2. T(j) > T(i) but f(i) 
f(j). This means that j is probably a good solution for subspace j and better than the local best of 
subspace j, because the lbest(subspace i) is better than lbest(subspace j) and solution i has a higher probability of being 
near lbest(subspace i) than being distant from it. The temperature of subspace j will be reduced in the next global 
temperature stage.
3. T(j) > T(i) and f(j) > f(i). In this case there are two possibilities.
(a) If the search procedure has been trapped in a local optimum, using the higher temperature T(j) will provide a 
higher probability of escaping from this optimum.
(b) If the search procedure is not trapped, because decreasing temperature has guided the search procedure 
correctly to globally good solutions or optimal solutions, the solution distributions at high temperatures are good 
and the approximate probability distributions for finding good solutions are close to the stationary distributions. 
This implies that many subspaces should have the same or similar low temperatures. So it has a very high 
probability of going to these subspaces.
Two key points should be noted.
1. If global annealing schedule leads to optimal solutions, LSA does.
  
< previous page
page_34
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_34.html2009-7-7 16:56:16

page_35
< previous page
page_35
next page >
Page 35
2. LSA has a better chance than SA of escaping local optima.
What is it that allows LSA to work using the information arising from previous search? The answer is the predefined 
subspaces.
In SA the cost of solutions in a trial cannot be used individually to judge a proper annealing schedule that should be 
carried out thereafter because the solutions do not individually provide significant information for assessing a part of the 
search space. Moreover a local space is a set of the neighbours of a current solution and in most cases each solution has 
its own distinguishable neighbours. This means that the number of local spaces is equal to the number of solutions in the 
search space. Obviously using cost to assess a local space is not different from assessing the solution itself.
In LSA the task of capturing information from previous search can be distributed among these subspaces. From the 
discussion of cost in Section 2.2, we know that the local best can represent the quasi equilibrium point of a subspace and 
the global best represents the quasi equilibrium point of the entire search space. So the cost of the best solutions in the 
individual subspaces can be used to assess these subspaces and the approachability of a subspace can be used to 
measure its goodness. This allows LSA to adjust the annealing procedure in each subspace.
In effect LSA utilizes more valuable information than SA.
2.5 
Empirical Results
To evaluate the effectiveness of LSA against SA, we first used a set of travelling salesman problems (TSP). We chose 
TSPs because of their importance in real-world applications and because they are a good measure of the effectiveness of 
a search algorithm. For simplicity, we only tested LSA on TSPs of size 64 cities (the data is randomly generated on the 
board (0, 400) × (0, 300)). A TSP is represented as a sequence of 64 different variables, with the domain of all variables 
being the set of 64 cities. A label is an assignment to the variables such that no two variables have the same value. The 
neighbourhood operator chooses randomly two positions from the sequence and reverses the subsequence delimited by 
the two positions.
For the given problem structure, dividing the domain of one variable cannot form disjoint subspaces. As a result we use 
two variables to build the subspaces. The value of the first variable is fixed as city 1. The domain of the second variable 
X is divided into subdomains which characterize the subspaces. For example, if X's value in a label is in the range of 
[2,8], then the label is in one subspace. If X's value in a label is in the range of [9,16], then the label is in a different 
subspace.
Our initial results are summarized in Table 2.1. It contains the average costs and the best costs of TSP tours produced by 
SA and LSA respectively over 35 runs for 7 instances. It also lists the percentage improvement of LSA over SA.
The initial results of LSA compared to SA are very encouraging; they are even more encouraging for two real-world 
problems: the flight scheduling problem and the train grouping problem. The specifications and empirical results of 
these problems are described in turn as follows.
  
< previous page
page_35
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_35.html2009-7-7 16:56:17

page_36
< previous page
page_36
next page >
Page 36
Table 2.1 Computation results for TSP problems
SA Av.
LSA Av.
Improvement
SA Best
LSA Best
Improvement
2411
2324
3.61
2275
2197
3.43
 
2.5.1 
The Flight Scheduling Problem
The flight scheduling problem consists of a set of flight requests and a set of planes. Each flight request (e.g. from 
London to Paris) demands a plane to satisfy the request. This plane can either be at the airport (London) in the 
beginning (in this case, it is a new plane) or it is brought by some other flight request (e.g. another flight from 
Amsterdam to London). If a flight request is assigned a new plane, it incurs a cost of 30 units. If a flight request is 
assigned a plane that is not brought by a request in the same tour, it incurs a cost of 10 units. There is a preference that 
requests in the same tour share the same plane. An example is given below.
Request number 3: London to Paris Flight from 10 am to 11:25am
Tour number of this request: 16
Possible planes for this request: (plane1, newplane), (plane2, newplane), (plane3, from request 1 or request 
2), (plane4, from request 1), (plane5, from request 2), (plane6, from request 1 or request 2).
Some planes have defects which matter to some flight requests thereby generating a cost. A hard constraint is that no 
two flight requests can take the same aircraft unless the plane assigned to one request arrives, prior to the departure time 
of the other request, at the airport of the other request. The objective of the flight scheduling problem is to allocate each 
flight request with a plane in such a way that the overall allocation minimizes cost.
Curiously, there is no need to specify the hard constraint which states that no one flight request can take more than one 
plane. This is because we are minimizing the overall costs. So if a flight request is assigned two planes that do not 
violate any other hard constraint, then choosing one of the two planes will never increase the cost and will probably 
reduce it.
The label of a flight scheduling problem is simply the assignment of planes to flight requests from their domains. Here 
each flight request is represented by a variable and the domain of the variable is the set of aircraft that can be assigned 
to the flight request. The neighbourhood operator involves first repairing those flight requests that violate a hard 
constraint and then any flight request that violates a soft constraint. Repairing a flight request simply assigns an 
alternative aircraft from the domain of the flight request. Clearly, a label may not be a solution label. By selecting flight 
requests violating hard constraints to repair first, the neighbourhood operator is geared towards feasible solutions while 
moving towards more optimal solutions. To allow comparisons
  
< previous page
page_36
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_36.html2009-7-7 16:56:17

page_37
< previous page
page_37
next page >
Page 37
Table 2.2 Computation results for sample BA scheduling problems Class B3
Problem
SA Best
Success Rate
LSA Best
Success Rate
b3a
475
90
475
99
b3b
515
89
515
99
b3c
525
87
525
98
 
Table 2.3 Computation results for sample BA scheduling problems Class B5
Problem
SA Best
Success Rate
LSA Best
Success Rate
b5a
690
82
690
95
b5b
765
78
765
93
b5c
755
75
755
90
 
between labels with different sets of hard constraint violations, each hard constraint violation incurs a cost that is equal 
to the upper bound of the total costs of all soft constraints in the flight scheduling problems.
We divide the search space of the problem into 6 disjoint subspaces based on the value of the flight request variable 
(call it X) that has the largest domain size. Instances of the flight scheduling problem are of two types. The first type B3 
consists of 45 requests and 16 aircraft. The second type B5 consists of 100 requests and 26 aircraft. The problem 
instances of each class only differ on their soft constraints. The same annealing schedules are used for both LSA and SA 
on the same machine implemented in the same language. In this schedule, the initial acceptance probability X0 is 0.95, 
the cooling rate is 0.9 and the number of iterations at each temperature is 600 for B3 instances and 2000 for B5 
instances.
The empirical results are summarized in Table 2.2 and Table 2.3. Each table lists the best costs found by LSA and SA 
for 100 runs. It also gives the average success rate and the number of runs within each 100 runs that achieves the best 
optima.
Like TSPs, LSA produces better-quality solutions than SA within the time limit of the annealing schedule. The time 
limit is determined by the number of iterations at each temperature and the rate of temperature decrease. Although the 
best optima found by LSA and SA are the same in 100 runs each time, the success rate of LSA in finding the optimum 
solution is about 10% better than SA. This is because SA is more easily trapped at local optima. For example, in b3a 
both SA and LSA reach cost value 495 at all runs, but SA sometimes is trapped there while LSA is seldom trapped. This 
difference in success rate is vitally important when one considers the scalability of the LSA algorithm. LSA is likely to 
produce better optima than SA for larger size problems within a given time limit. We are currently investigating this 
claim.
  
< previous page
page_37
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_37.html2009-7-7 16:56:18

page_38
< previous page
page_38
next page >
Page 38
2.5.2 
The Freight Train Regrouping Problem
The freight train regrouping problem(FTRP) is a scheduling problem for cargo transportation. It involves regrouping 
cargo carriages from different arrival trains in the marshalling yard of a train station in such a way that carriages from 
the same direction of destination are put on the same marshalling track. If the capacity of a marshalling track is to be 
exceeded by the carriages of an arrival train with the same direction as that of the track, then an unused marshalling 
track (if there is one) will be used. Usually, the capacity of the marshalling yard is much less than the total number of 
carriages of the arrival trains to be regrouped. Clearly in this case, different regrouping sequences of arrival trains in the 
marshalling yard will affect the total number of carriages marshalled given the limited capacity of the yard. The 
objective of FTRP is to find a label that has maximum marshalling of cargo carriages for the marshalling yard.
The total number of carriages in the arrival yard is defined as follows:
This expression represents the distribution of trains that will be regrouped. Here i is the ith incoming train in an arrival 
track and j is the jth direction of a destination of a carriage. The term aij represents the number of carriages on the ith 
track with jth direction. K is the number of trains in the arrival yard and M is the total number of directions. Let di 
denote the number of carriages on the ith marshalling track. Let L be the total number of tracks in the marshalling yard 
and N be the maximum number of carriages that a track can take. Then we have the following constraints.
1. di≤ N (i = 1, . . ., L)
2. L < K
3. Each track in the marshalling yard can only take carriages that go in the same direction.
4. aij is the basic regrouping unit. This means that in the regrouping procedure each element of aij cannot be separated.
The objective function is
The resource constraints in FTRP include the limited number of tracks in the arrival yard, the number of carriages that 
can be carried by a train, the number of available marshalling tracks and the capacity of each marshalling track.
In FTRP the key thing that decides the number of carriages accommodated in a marshalling yard is the sequence of 
regrouping trains. We use a label to represent such a sequence and each label is a solution. In this context the variables 
represent the positions of the label. The domain values are the regrouping trains. The hard constraint for a feasible 
solution is that no train appears in a label twice.
In FTRP the search space is composed of all possible regrouping sequences of trains in the arrival yard. We choose an 
arbitrary position of the label to divide the subspaces and select a configuration that distinguishes what set of trains 
represents a subspace.
  
< previous page
page_38
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_38.html2009-7-7 16:56:18

page_39
< previous page
page_39
next page >
Page 39
Table 2.4 Computation results for sample FTRP problems
Data Size
SA Av.
LSA Av.
Improvement
SA Best
LSA Best
Improvement
12×20
387
392
1.51
398
405
1.75
25×40
906
925
2.08
942
961
2.01
45×60
1606
1657
3.17
1681
1724
2.55
 
The FTRP consists of three types. In the first type, the number of marshalling tracks is 12 and the capacity of each track 
is 20 trains. In the second type, the number of marshalling tracks is 25 and the capacity of each track is 40 trains. In the 
third type, the number of marshalling tracks is 45 and the capacity of each track is 60 trains.
The empirical results are summarized in Table 2.4. This table lists the average and best results found by LSA and SA 
for 100 runs. It also lists the improvement rate of LSA over SA in each case. From the table, it can be seen that LSA 
always finds better solutions than SA although the improvement rate varies between 1.51 and 3.17.
2.6 
Conclusion
In this paper we have presented a flexible simulated annealing strategy LSA that is based on the division of a search 
space. We have shown that by exploiting the information of each subspace, the refined strategy provides better control 
over temperature and annealing speed in different subspaces. We have demonstrated the effectiveness of LSA over SA 
by considering two real-world applications: flight scheduling and freight train regrouping problems.
References
[AK89] Aarts E. and Korst J. (1989) Simulated annealing and Boltzman Machines. John Wiley and Sons, Chichester.
[AvL85] Aarts E. and van Laarhoven P. (1985) Statistical cooling: a general approach to combinatorial optimization 
problems. Philips Journal of Research 40: 193226.
[CEG87] Collins N., Eglese R., and Golden B. (1987) Simulated annealing an annotated bibliography. Computer 21.
[JMS89] Johnson D. C. A., McGeoch L., and Schevon C. (1989) Optimisation by simulated annealing: an experimental 
evaluation; part i, graph partitioning. Operation Research 37: 865892.
[KGV83] Kirkpatrick S., Gelatt Jr. C., and Vecchi M. (1983) Optimization by simulated annealing. Science 220: 
671680.
[Tsa93] Tsang E. (1993) Foundations of Constraint Satisfaction. Academic Press, New York.
  
< previous page
page_39
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_39.html2009-7-7 16:56:19

page_41
< previous page
page_41
next page >
Page 41
3 
Observing Logical Interdependencies in Tabu Search Methods and Results
Stefan Voß
Abstract
We survey some tabu search approaches for combinatorial optimization problems with special emphasis on dynamic 
tabu list management. In the latter approach the most important aspect is to observe logical interdependencies between 
the solutions encountered throughout the search.
Various successful applications from the area of location including new results are reviewed.
3.1 
Introduction
Due to the complexity of a great variety of combinatorial optimization problems, heuristic algorithms are especially 
relevant for dealing with these problems. As an extension of simple heuristics, a large number of local search 
approaches, such as deterministic exchange methods, have become increasingly important to improve given feasible 
solutions. The main drawback of these local search approaches is their inability to continue the search upon becoming 
trapped in a local optimum. This leads to consideration of modern heuristic search methods for guiding known 
heuristics to overcome local optimality.
Following this theme, we investigate the tabu search metastrategy for solving combinatorial optimization problems. In 
the following sections we first give a brief introduction to the necessary basic ingredients of tabu search as well as some 
hints in use from the relevant literature. Most of all, however, we focus on a survey of extensions for observing logical 
interdependencies and corresponding applications in the area of location problems as they may be derived from a 
general problem in the field of network design.
Modern Heuristic Search Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_41
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_41.html2009-7-7 16:56:20

page_42
< previous page
page_42
next page >
Page 42
3.2 
Tabu Search
Tabu search is a metastrategy for guiding local search approaches to overcome local optimality. This metaheuristic has 
been reported in the literature during the last few years as providing successful solution approaches for a great variety of 
problem areas. In the sequel we investigate the basic ingredients of tabu search as well as some issues that have proved 
successful for various applications. These advanced issues include specialized methods that incorporate different means 
of intelligence in providing history based search trajectories and allowing the inclusion of learning capabilities. Before 
going into detail, however, we provide a rough historical development including some of the most important references 
on tabu search.
3.2.1 
Historical Development
Following Glover and Laguna [GL93], 'tabu search has its antecedents in methods designed to cross boundaries of 
feasibility or local optimality standardly treated as barriers, and to systematically impose and release constraints to 
permit exploration of otherwise forbidden regions'. That is, the real foundation for tabu search may be sought in 
concepts that systematically violate feasibility conditions as in heuristic procedures based on surrogate constraints 
(Glover[Glo77]) or even in cutting plane algorithms. Note, however, that tabu search nowadays has become a separate 
powerful tool where the differences, for instance, to cutting plane algorithms are much more important than the 
resemblance. Most early references to tabu search in its present form are Glover[Glo86] and Glover and McMillan
[GM86] where some foundations and a first application are provided, respectively. In addition, there are a number of 
contributions that have influenced the exploration of tabu search to the tool it is today: Glover[Glo89, Glo90a], Glover 
and Greenberg [GG89], de Werra and Hertz[dWH89]. But even older references both of conceptual (Glover[Glo77]) 
and of application oriented nature (Lin[Lin65]) may be useful to explain the inspirations for tabu search.
Most influential is the basic two-part work of Glover[Glo89, Glo90a] and, in particular, the first part. (It seems that up 
to now, mainly aspects of the first part have been investigated in the literature with only a few exceptions referring to 
the second part.) Early applications that have greatly influenced the subsequent literature are Skorin-Kapov[SK90] with 
a tabu navigation approach for the quadratic assignment problem and Widmer and Hertz[WH89] providing an example 
with respect to flow shop scheduling. For a survey on successful applications of the tabu search metastrategy and a 
comparison to other search techniques the reader is referred to Glover and Laguna[GL93] as well as Voß[Voß96] and 
the references given therein. References that clarify concepts by means of extensive but easy examples in the area of 
network design and location are, especially, Glover[Glo90b] and Domschke et al.[DFV92].
Moreover, tabu search has received considerable interest in the operations research community, e.g. through special 
issues on applied local search or tabu search of various journals (Annals of Operations Research, Computers & 
Operations Research, OR Spektrum among others, see Glover et al.[GLTdW93], Laporte and Osman[LO96], Erenguc 
and Pirkul[EP94] and Pesch and Voß[PV95] or even through promoting a specialized metaheuristics conference (see 
Osman and Kelly[OK96]).
  
< previous page
page_42
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_42.html2009-7-7 16:56:20

page_43
< previous page
page_43
next page >
Page 43
3.2.2 
Guiding Process versus Application Process
The basic principle of any iteration guided by tabu search for solving a problem like
may be stated as follows: Transform the actual solution to a neighbour solution and try to guide the search into a 
meaningful direction by 'intelligently' utilizing the knowledge or memory about the previous iterations. That is, the 
search history is used to incorporate adaptive memory and responsive exploration into problem solving. In the easiest 
way the guidance is accomplished by the idea to avoid visiting solutions yet explored. Following a steepest descent / 
mildest ascent approach an admissible transformation may, in a simplified way, result in either a best possible 
improvement or a least possible deterioration of the objective function value.
The transition to a neighbour solution is called a move and may be described by one or more attributes or elements. The 
attributes of the moves (when properly chosen) can become the foundation for creating a so-called attribute based 
memory. Depending on the application the abstract term move has to be specified. For example, in a zero-one integer 
programming context the attributes may be the set of all possible value assignments (or changes in such assignments) 
for the binary variables. With respect to location problems this may refer to opening or closing a facility, respectively. 
Two attributes e and 
, which denote that a certain binary variable is set to 1 or 0, are called complementary to each 
other. That is, the complement of a move cancels the effect of the considered move. If a move and its complement are 
performed, the same solution is reached as without having performed both moves. This is independent from the number 
of moves that have been explored between the two complementary moves.
Moves or attributes eventually leading to a previously visited solution may be stored in a so-called tabu list and are 
hence forbidden or tabu for a certain number of transformations or iterations (called tabu tenure). The tabu list is 
derived from the running list (RL), which is an ordered list of all moves (or their attributes, respectively) performed 
throughout the search. That is, the RL represents the trajectory of solutions encountered. Whenever the length of RL is 
limited the attribute based memory of tabu search based on exploring the RL is structured to provide a recency based or 
short-term memory function. Note that other kinds of memory are important features, too. For instance, recording 
complete solutions may be referred to as explicit memory. If we not only keep track of attributes but also of the number 
of transformations they are involved with, we enter considerations of longer-term aspects by means of a frequency 
based memory. Corresponding long-term memory functions become especially relevant in intensification and 
diversification strategies.
Now, each iteration consists of two parts: The guiding or tabu process and the application process. The tabu process 
updates the tabu list hereby requiring the actual RL; the application process chooses among the moves that are not tabu 
and updates the RL. (For faster computation or storage reduction both processes are often combined.) The application 
process is in principle the same for all tabu search methods except for some additional information provided to the tabu 
process, whereas the latter process varies. The application process is a specification on, for example, the neighbourhood 
definition and has to be provided by the user. Furthermore,
  
< previous page
page_43
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_43.html2009-7-7 16:56:21

page_44
< previous page
page_44
next page >
Page 44
making judicious choices of moves is often influenced by candidate list strategies. Those strategies may be used to 
restrict the number of moves to be evaluated, e.g., to reduce the number of computations or to identify subsets of 
promising influential moves seeming to be more attractive than others.
Subsequently we will refer to three tabu search methods: static tabu search (STS), the reverse elimination method 
(REM) and the cancellation sequence method (CSM). For extensive background on these methods see Glover[Glo89, 
Glo90a]. Specifications used throughout the sequel are partly given in Dammeyer et al. [DFV91] and Voß[Voß96] 
whereas a comparison of all three methods with a specific simulated annealing implementation for a quadratic 
optimization problem can be found in Domschke et al.[DFV92].
With the considerations and concepts given in more detail below we can summarize the basic ideas of tabu list 
management, the very heart of tabu search, by the visualization given in Figure 3.1. This distinguishes the main ideas 
used throughout the following discussion. A different scheme might discard 'static' and 'dynamic' and replace them with 
'ad hoc' and 'exact'. (Then the dashed line to random variation of s could be dropped.) Distinguishing between short-
term and long-term memory may be based on the question of whether the RL observes the whole search history or 
whether, for example, a frequency based measure is incorporated (cf. Glover and Laguna[GL93]).1
Figure 3.1 
Tabu list management scheme (simplified).
1 Basically the different views of the ingredients and most influencing aspects of tabu search may result in the 
following inequality (which of course may be adapted to other search methods as well): TABU ≠ TABU.
  
< previous page
page_44
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_44.html2009-7-7 16:56:21

page_45
< previous page
page_45
next page >
Page 45
3.2.3 
Tabu Navigation Method Static Tabu Search
The STS, i.e. the tabu navigation method, is a rather simple approach requiring one parameter s called tabu list size. 
STS disallows the complements of the s most recent moves of the running list. That is, the tabu list (or tabu buffer) 
consists of a (complementary) copy of the last part of the RL. Older moves are disregarded.
The tabu status derived from the s most recent moves forces the algorithm to go s moves away from any explored 
solution before the first step backwards is allowed. Obviously, this approach may disallow more moves than necessary 
to avoid returning to a yet visited solution. This encourages the intention to keep s as small as possible without 
disregarding the principal aim of guiding into meaningful directions without endless repetition: If s is too small the 
algorithm probably will return to a local optimum just left. This process may result in a so-called cycling process: If a 
solution is re-visited the same sequence of moves may be repeated consecutively until the algorithm eventually stops. 
Here, we should realize that tabu search (in its pure form) is a deterministic method. However, probabilistic versions of 
tabu search also exist and have been shown to be a versatile approach within the tabu search concept (see, e.g., Glover
[Glo89], Faigle and Kern[FK92], Lokketangen and Glover[LG95]. The danger of cycling favours large values for s. An 
adequate value for s has to be adopted to problem structure, the characteristics of the considered problem instances 
(especially problem size) and the values of cost or other coefficients. The parameter s is usually fixed but could also be 
randomly or systematically varied after a certain number of iterations. Finally, we should note that instead of using a 
tabu list size one could think of a so-called prohibition period (which indeed is more independent from specific 
implementations, see Battiti and Tecchiolli [BT95]).
The fact that STS disallows moves which are not necessarily tabu led to the development of a so-called aspiration level 
criterion (ALC) which may override the tabu status of a move. The basic form of the ALC is to choose a move in spite 
of its tabu status (or the tabu status of the encompassed attributes) if it leads to an objective function value better than 
the best obtained in all preceding iterations. The reason is obvious: such a move cannot lead to a known solution 
because otherwise the new value would already have been known.
3.2.4 
Reverse Elimination Method and Cancellation Sequence Method
The REM and the CSM are based on cancellation sequences which utilize some sort of memory structure imposed on 
the sequence of moves or attributes yet performed throughout the search. In general, any connected part of the running 
list is a cancellation sequence (CS): just the sequence of moves between two solutions. It may contain pairs of 
complementary moves. A residual cancellation sequence (RCS) results from a CS by cancelling (i.e. eliminating) some 
of its moves. REM and CSM differ in their way to carry out these cancellations but they match in their fundamental 
principle: Any move that would cancel an RCS is set tabu.
REM and CSM compute and maintain RCSs in different ways: REM in each iteration recomputes all RCSs between the 
current and any previous solution. CSM in one iteration computes and stores the RCSs that are created by the actual 
move. Older RCSs are updated which may cause tabu settings.
  
< previous page
page_45
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_45.html2009-7-7 16:56:22

page_46
< previous page
page_46
next page >
Page 46
The REM is based on the fact that each iteration starts at the current solution and then step by step traces in reverse direction through the RL. 
At each step an RCS between the current solution and the actually considered older solution is computed. Each iteration starts with the tabu 
list and the RCS being empty. For ease of exposition let us temporarily assume that a move consists of exactly one attribute. First, in each step 
this sequence is expanded by one move. Then, the algorithm checks whether the respective complementary move is part of the RCS, too. If 
this is true the complementary pair is eliminated and the RCS is reduced. If the complement is not included, the expanded RCS remains 
unchanged (Figure 3.2 gives the corresponding pseudo code to visualize the concept of REM for iteration iter).
Each time the RCS reduces to one move the respective complementary move is set tabu with a tenure of 1. That is, it stays tabu only for that 
iteration immediately following. If the RL is considered in its full length each move that should be tabu is found. Also each move that is set 
tabu in fact would lead to a yet explored solution. Therefore, no better (or even optimal) solution can be 'forgotten' because of being tabu 
without need. Thus, an ALC is not necessary. Finally, we should note that the REM provides a necessary and sufficient condition to prevent 
from revisiting yet explored solutions, as long as there is no maximal length of the RL limiting the backtracing.
for all elements k do status(k)=.free. 
                           (* comment: designate all elements 'not tabu' *)
                                                 (* initialize RCS *)
for i = iter, 1,  step 1 do       (* backtracing of the running list RL *)
     if 
 then
           
       (* RL(i) cancels its complement on RCS *)
     else
          RCS:=RCS ∪ {RL(i)}                    (* RL(i) is added to RCS *)
     endif
     if | RCS| =1 then status(k):=.tabu. for k ∈ RCS
                                                (* designate tabu status *)
 
Figure 3.2 
The concept of the REM
Up to now, with respect to REM we have left the case of multi-attribute moves purposefully unspecified. Consider paired-attribute moves 
where we have a clear distinction, e.g., between add- and drop-attributes (i.e. defining a binary variable as 1 - add - or 0 - drop). In this case 
there is a straightforward adaptation of the REM and the consideration of necessity and sufficiency conditions in avoiding revisiting 
previously encountered solutions is the same as in the simpler case (cf. Glover[Glo90a], Voß[Voß96]). More generally, we may define 
arbitrary multi-attribute moves by means of classes of attributes (the above add- and drop-attributes provide examples of such classes). 
Whenever the sequence of the attributes is not critical the same observations are made as in the simpler case. Designating a tabu status (cf. the 
last line of Figure 3.2) is performed whenever RCS constitutes an admissible move. Only in cases where different sequences of the same set 
of attributes define different moves adaptations to
  
< previous page
page_46
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_46.html2009-7-7 16:56:23

page_47
< previous page
page_47
next page >
Page 47
a certain block structure have to be performed where necessity need not be guaranteed in all cases.
The CSM embodies the concepts of the static method and the REM. Moves are set tabu in two ways: The simple tabu 
process is identical with the STS. The main tabu process, similar to the REM, disallows any move that would cancel an 
RCS. The following description of one iteration of CSM will show the handling of the RCSs.
We run the CSM with three parameters c1, c2 and c3:
c1 is the maximal length of the running list.
c2 is the tabu tenure based on the main CSM tabu process.
c3 is the tabu tenure corresponding to the tabu list length s of STS. The value c3=1 prevents the immediate return to the 
just visited solution. Higher values are also reasonable because for limitations of RL imposed by c1 the main CSM tabu 
process might not find every move that should be tabu. Usually, c3 < s because of this additional way of setting tabu.
Assume that a move mj has just been performed (with j < c1), then one iteration of the CSM is as follows:
1. Store mj in the RL which becomes (m1, . . ., mi, . . ., mj) with the trajectory (S0, S1, . . ., Si, . . ., Sj) of solutions with 
the starting solution S0 and the current solution being Sj.
2. Set 
 tabu for the next c3 iterations.
3. If there exists an mi in the RL that is cancelled by mj, i.e. mj = 
, then: Create a new RCS between Si and Sj as (mi
+1, . . ., mj1) and cancel mi from RL. (Note that RL may be modified in the CSM, i.e., it may be viewed as a so-called 
active tabu list ATL.) This shortens existing RCSs containing mi. If an RCS now consists of only one move mt, set 
 
tabu for the next c2 iterations and cancel the respective RCS. Note that mt as well as mj remain in RL.
With respect to multi-attribute moves eventually the attributes of the moves have to be treated within blocks to get 
reasonable adaptations (cf. Glover[Glo90a], Voß[Voß96]).
Let us again compare REM and CSM: the definition of an RCS, the duration of the tabu status, and the treatment of 
RCSs at the end of each iteration are different. REM in each iteration computes the tabu moves for only the next 
iteration. According to the implementation imposed on Figure 3.2 only one RCS is known at a time it is changed within 
the tabu process. (Note that different implementations are possible where corresponding to each iteration an actual RCS 
is maintained and modified, thus utilizing far more storage space.) When the RL has been traced back the last RCS 
becomes useless and is disregarded. The next iteration again starts with an empty RCS no information despite the RL is 
kept from the previous iteration. Contrary to this the CSM keeps each RCS until it reduces to exactly one move. A 
newly created RCS corresponds to the current difference of two 'boundary' solutions. When cancellations occur later on 
this is no longer true. As long as an RCS is not completely cancelled it is prevented from returning to any solution 
attained before the solution from the original start of the RCS. Even more than c3 iterations after the last move of an 
RCS has been cancelled the search trajectory need not return to a previous solution (depending on the whole sequence 
of moves performed before and after the RCS). It is reasonable to forbid the complementary move of the last move of 
an RCS for the
  
< previous page
page_47
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_47.html2009-7-7 16:56:23

page_48
< previous page
page_48
next page >
Page 48
'next' sufficiently large number of iterations c2 and to disregard it later on. No matter how c2 is chosen: CSM in general 
may forbid more moves than necessary without guaranteeing prevention of returning to a known solution.
3.2.5 
Logical Interdependencies
Most successful applications of tabu search reported in the literature so far apply the static tabu list management 
concept. Nevertheless, with this method cycling may still occur. Note that strategic considerations may enforce the 
search to revisit previously explored solutions as may be the case when so-called elite solutions are reconsidered for a 
restart of the search. Various considerations in this respect investigate the interplay of intensification and diversification 
(see Battiti[Bat95] for the feedback mechanism using hashing in reactive search, Sondergeld and Voß[SV95] and Voß
[Voß95] for the REM where a diversification measure is introduced to assure the solutions recorded differ from each 
other by a desired degree, and Rochat and Taillard[RT95] for probabilistic tabu search).
To overcome the deficiency that cycling may still occur, different concepts may be used (assuming for the time being 
that cycling is not desirable). One idea for so-called cycle detection is to incorporate some hashing function which can 
be implemented even in cases where moves are not represented in a proper way, i.e., where they do not satisfy a 
sufficiency property (see, e.g., the reactive tabu search of Battiti and Tecchiolli[BT95] and Woodruff and Zemel[WZ93] 
for some references). As the STS as described above usually assumes fixed values of s, more flexibility is obtained by 
varying the tabu list size. If this variation is predetermined in a certain way such as in random modification of s, the 
attribute 'dynamic', however, may be doubtful. This becomes especially relevant when the treatment of REM and CSM 
is considered, where logical relationships are exploited as underlying the RL, i.e. the sequence of attributes performed 
throughout the search. In that sense logical interdependencies may be viewed as relationships between the 
transformations observed. In REM and CSM these interdependencies are even more fully exploited as the immediate 
interplay of the attributes enforces tabu restrictions. Both methods have been shown to be purely dynamic approaches in 
the sense that tabu settings (especially the number of attributes or moves designated tabu) are only developed in the 
course of the search.
Although successful in most applications, one drawback of tabu search sometimes mentioned is the lack of theoretical 
foundations. Of course, there are a few theoretical results that have to be taken into account like the necessity and 
sufficiency of the REM. (That is, based on the above-mentioned interdependencies, no such drawback is appearing for 
the basic idea of REM.) Furthermore, when probabilistic ideas are incorporated even into the static tabu search concept 
sound theoretical results may be obtained, too (see, e.g., Faigle and Kern[FK92]).
3.2.6 
Moving Gap
In this survey, dynamic tabu list approaches are emphasized more and other approaches less because most attention in 
the literature is focused on the latter methods. Seeking to fill a gap by disclosing more fully the relevance of the 
dynamic methods is motivated by different aspects. Nevertheless, there are variations of the
  
< previous page
page_48
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_48.html2009-7-7 16:56:24

page_49
< previous page
page_49
next page >
Page 49
navigation approach that might deserve the name dynamic. We explain this by the moving gap idea, leading to a tabu 
list of changing size and composition (cf. Hübscher and Glover[HG94] as well as an application to the quadratic 
assignment problem provided by Skorin-Kapov[SK94]).
Figure 3.3 
A moving gap tabu list.
A moving gap tabu list consists of a static part, i.e. a partial list considering a fixed number of the most recent moves 
whose complements obtain a tabu status. Furthermore, a second part observes the next recent moves and defines a tabu 
status for all or for a subset of them. This subset changes in a time-dependent way in that a fixed part (gap) is omitted 
for a certain number of iterations. Figure 3.3 shows the procedure in more detail. There the static part consists of a list 
with length s = 4 whereas the moving gap part is partitioned into four equal sublists (of length 1 in our example), where 
the choice of a subset of these lists varies over time. Like any parameter, these can be modified with respect to specific 
applications, too. Finally, however, one has to admit that the moving gap tabu list again does not consider any logical 
interdependencies.
  
< previous page
page_49
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_49.html2009-7-7 16:56:25

page_50
< previous page
page_50
next page >
Page 50
3.3 
Applications in Location Planning
In this part we discuss several applications of the above tabu search methods as they have been applied mainly based on 
our experience in the dynamic tabu list management surrounding. The area under consideration is that of network 
design and location. Respective problems are formulated by different means and motivation concerning the design and 
analysis of networks (cf. Magnanti and Wong[MW84]). The outcome of a design process may be a set of locations in 
connection with a production and distribution problem, a telecommunications or traffic network, just to mention some 
of the areas where network design either plays or should play an important role. Some of the questions concerned with 
the design and analysis of networks involve the following issues: How many sites or locations and how many 
connections should be chosen and where should they be located etc.? In the sequel we investigate Steiner's problem in 
graphs and focus on some new results for a set of benchmark problems. In addition, we consider a class of warehouse 
location problems and provide a short review of further location problems.
3.3.1 
Network Design and Steiner Trees
In this section we first introduce Steiner's problem in undirected and directed graphs and present some move definitions 
from the literature. Then some new best results for some benchmark problems with respect to the rectilinear Steiner 
problem are reported.
Given a graph with weights on its edges, the Steiner tree problem in graphs (SP) is to determine a minimum cost 
subgraph spanning a set of specified vertices. Correspondingly, the Steiner tree problem in directed graphs (SPD) is to 
find a minimum cost directed subgraph of a given graph that contains a directed path between a root node and every 
vertex of a set of specified vertices. Real-world problems arising in the layout of connection structures in networks as 
in, for example, VLSI-design may often be decomposed into a number of well-known combinatorial optimization 
problems. SP and SPD are included within this context.
More formally with respect to the SPD, let G = (V, A) be a directed graph with a vertex set V and a set A of arcs with 
nonnegative weights. V consists of a root w and two disjoint sets Q of q basic vertices and S of possible Steiner vertices 
(i.e., V = {w} ∪ Q ∪ S; to simplify matters we may refer to w as a basic vertex with w ∈ Q and V = Q ∪ S). SPD is to 
find a minimum cost subgraph of G including a directed path from w to every basic vertex. In order to achieve this 
minimum subgraph additional vertices from the set S may be used. Since all arc weights are assumed to be nonnegative 
there is an optimal solution which is a directed tree (a Steiner tree) or arborescence with root w (provided that a feasible 
solution exists). Note that any SP instance may easily be transformed into an SPD instance by replacing each undirected 
edge with two arcs between the same nodes directed opposite each other both having the same weight as the original 
edge.
Recently, it turned out that a specific set of 78 problem instances for SP available through e-mail (see Beasley[Bea90]) 
has been considered by several authors (data set B with 18 and sets C to E with 20 instances each). These instances 
comprise data with the number of vertices ranging from 50 to 2500 and the number of edges ranging from 63 up to 65 
000. With respect to our idea of using binary variables for
  
< previous page
page_50
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_50.html2009-7-7 16:56:25

page_51
< previous page
page_51
next page >
Page 51
problem representation we have a good testbed for comparing the different approaches. With respect to a proper 
neighbourhood definition for SP two main ideas may be distinguished (see, e.g., Duin and Voß[DV94]):
Edge-oriented Transformation
1. Given a feasible solution T = (VT, ET) randomly select an edge (i, j) ∈ ET and delete it from E.
2. While there exists a leaf of T being a Steiner vertex do delete that leaf and its incident edge.
3. Let Ti and Tj be the two disconnected components of T. Find nearest vertices i* and j* belonging to Ti and Tj, 
respectively, and add the vertices and edges of the shortest path between i* and j* to T (eventually disallowing to 
use (i, j), i.e. defining a tabu status for (i, j)).
Node-oriented Transformation
1. Given a feasible solution T = (VT, ET). Let (x1, . . ., X|S|) be a binary vector with xi denoting that i is included (xi 
= 1) or excluded (xi = 0) from the solution.
2. Choose a vertex i ∈ S and define xi := 1 xi. 
Recompute a minimum spanning tree of the graph induced by Q ∪ {i ∈ S | xi = 1}.
Within the local search literature usually an edge-oriented transformation is described (see e.g. Verhoeven et al.
[VSA95]). For numerical results comparing different tabu search concepts see, e.g., Duin and Voß[DV94] as well as 
Voß[Voß96]. Results are reported on all data sets B to E. For instance, considering the REM in its pure form as 
described above for at most 200 iterations following the node-oriented transformation idea the following results are 
obtained for data sets B (number of nodes: from 50 to 100) and C (number of nodes: 500). Starting from initial feasible 
solutions obtained by a cheapest insertion heuristic an average deviation from optimality of 0.19 % and 1.75 % is gained 
for data sets B and C, respectively.
An additional set of benchmark instances for Steiner's problem that has received considerable attention in the literature 
throughout the last two decades is provided by Soukup and Chow[SC73]. They put together 46 test instances by giving 
the coordinates of a number of basic vertices within the unit square. The instances arising are Steiner problems in the 
plane with the underlying metric being the Euclidean or the rectilinear metric. For a survey on heuristics for the 
rectilinear Steiner problem the reader is referred to, e.g., Servit[Ser81] and Hwang et al.[HRW92].
Following a result of Hanan[Han66] the rectilinear Steiner problem may be formulated as a special case of SP. For a 
given set Q of basic vertices there exists an optimal solution which uses only vertices and edges of the grid graph 
induced by the set of basic vertices by drawing a horizontal and a vertical line through each basic vertex and retaining 
only the finite segments of these lines. The vertices of the graph will be the set of all interconnections of any two lines. 
Applying this result we are able to solve even rectilinear problems with the above-mentioned tabu search heuristics by 
means of node-oriented transformations as well as edge-oriented transformations.
  
< previous page
page_51
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_51.html2009-7-7 16:56:26

page_52
< previous page
page_52
next page >
Page 52
Table 3.1 Numerical results for the Soukup and Chow benchmark 
problems
problem
| Q |
previously best known solution
(new) solution
15
14
1.48b
1.48
18
62
4.06b
4.04
19
14
1.90b
1.88
31
14
2.60b
2.59
32
19
3.23b
3.13
33
18
2.69b
2.68
34
19
2.54b
2.42
35
18
1.54b
1.52
38
14
1.66b
1.66
39
14
1.66b
1.66
41
20
2.24b
2.24
43
16
2.66b
2.42
44
17
2.56k
2.56
45
19
2.24k
2.20
 
Table 3.1 shows results for a subset of 14 out of the 46 test instances with the rectilinear metric where optimal solutions 
are not known or not proved within the literature (to the best of our knowledge). All of these test problems have been 
converted into equivalent instances of SP and tried to be solve by one of the best-known exact algorithms for SP of 
Beasley[Bea89]. Nevertheless, following Beasley[Bea92] and Khoury and Pardalos[KP93] the optimal solutions to 
these problems are still not known. The first two columns of Table 3.1 indicate the problem number and the number of 
basic vertices (indicating that the problems should be easily solvable the interested reader might check this with respect 
to the problem instances number 18 and number 32 which are provided in Figures 3.4 and 3.5 together with the best 
solutions we have found by applying tabu search; basic vertices are given as black circles). Then the previously best 
known solutions found in the literature are given as they are provided by Beasley[Bea92] and by Khoury and Pardalos 
[KP93] (indicated by the lettersb and k, respectively). Furthermore, we provide our own results indicating
  
< previous page
page_52
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_52.html2009-7-7 16:56:26

page_53
< previous page
page_53
next page >
Page 53
Figure 3.4 
Problem instance 18 with 62 basic vertices.
Figure 3.5 
Problem instance 32 with 19 basic vertices.
that our approach (again the REM with at most 500 iterations starting with a cheapest insertion solution after having 
performed a simple reduction technique) finds the best-known solutions in all cases within a few seconds and is able to 
improve them for most of the problems (cf. the above neighbourhood definitions). The overall best-
  
< previous page
page_53
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_53.html2009-7-7 16:56:27

page_54
< previous page
page_54
next page >
Page 54
known values are shown in italics. Similar results are obtained for CSM. For STS with different fixed values of s the 
results are slightly worse.
Further extensive computations have been performed with respect to the rectilinear Steiner arborescence problem, i.e. 
the directed counterpart of the rectilinear Steiner problem. Although a complexity result for this problem still needs to 
be discovered for the case that all arcs are directed away from the origin with no cycles arising we investigated tabu 
search ideas for this problem superimposed on the solutions found by any of the algorithms described by Voß and Duin
[VD93]. The case of additional obstacles has been treated, too. For these problems the algorithms even starting with 
STS tend to behave very efficiently.
3.3.2 
Warehouse Location Problems
As may be deduced from the above considerations it is obvious that the warehouse location problem (WLP) is a special 
case of Steiner's problem in directed graphs. In that sense the algorithms for the SPD may be applied to the WLP as 
well. The only difference worth noting is the fact that the node-oriented transformation outperforms the edge-oriented 
transformation due to its simplicity while exploiting the underlying problem structure. Subsequently, we provide some 
information on corresponding implementations in a more general problem setting, i.e. the capacitated warehouse 
location problem with concave production costs.
The problem is to satisfy the demands bj of n customers j = 1, . . ., n. Decisions are performed on the production 
quantities as well as on the transportation quantities. For each of m possible supplier locations i there is some fixed cost 
Fi and a maximum quantity of ai units that may be produced (whenever a supplier is located at i). Furthermore, we have 
to observe concave cost functions fi for the production at location i as well as linear transportation costs cij (for the 
transport of one unit supplied from i to customer j). Considering binary variables xi indicating whether a supplier is 
located at i (xi = 1) or not (xi = 0) and continuous variables (yij) for the amount of goods supplied from i to customer j, a 
mathematical model for such a problem is as follows (cf. Francis et al.[FJW92]):
subject to
Due to (3.4) we have nonnegativity for the amount of production to be transported.
  
< previous page
page_54
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_54.html2009-7-7 16:56:27

page_55
< previous page
page_55
next page >
Page 55
Then for each supplier located at a location i the amount of goods produced at i may be calculated as Σjyij. With this the 
objective function (3.1) minimizes the sum of the fixed costs plus the transportation costs plus the production costs. The 
latter are described by a function fi(Σjyij) a monotonously increasing continuous concave function for the production 
costs at location i (with fi(0) := 0). Constraints (3.2) ensure that all demands are satisfied and capacity restrictions with 
respect to the quantities produced are imposed by restrictions (3.3).
Whenever the binary variables are fixed the resulting problem becomes a transportation problem. Therefore, 
considering the binary variables as the only decision variables directly handled by tabu search all our programs 
available for the SP/SPD based on the node-oriented transformation (or even the multiconstraint zero-one knapsack 
problem, MCKP, cf. Dammeyer and Voß[DV93] and Thiel and Voß[TV94]) may be applied to this problem, too. Most 
important, following Schildt[Sch94], who used our implementations in her study with respect to the above problem 
(3.1) to (3.5), the same ranking between the different procedures may be concluded as, e.g., provided by the above 
works for the MCKP, even when comparing with genetic algorithms or simulated annealing as well as hybrids. On a 
basis of 20 representative hard problem instances with n ≤ 30 and m ≤ 30 for the street network of Germany, Austria 
and Switzerland she finds average relative deviation values for STS (with s = 5), CSM (with c2 = 2 and c3 = 3), and 
REM of 0.145 %, 0.169 % and 0.141 %, respectively. At most 50 iterations had been performed for all algorithms. For 
REM a tabu tenure of five had been designated instead of one to have some diversification for the price of eventually 
losing the necessity property. Due to the latter modification for REM an ALC as described above had been applied for 
all three tabu search approaches. For simulated annealing and genetic algorithms much worse results were obtained (cf. 
Schildt[Sch94]).
Nevertheless, the dominance of REM and/or CSM over STS need not be mandatory. For instance, Captivo[Cap94] 
reports that for some benchmark instances of the ordinary capacitated WLP the picture may change (see Beasley
[Bea90] on how to obtain the data). The reason for this may be sought in the strictness of the capacity constraints, i.e., in 
(3.1) to (3.5) the capacity constraints in principle do not restrict the number of neighbours which might happen in 
different implementations tailored for the ordinary capacitated WLP.
3.3.3 
Further Location Problems
It should be noted that we have applied the different versions of tabu search to various network design and location 
problems such as, for example, the capacitated minimum spanning tree problem (CMST), the quadratic (semi-) 
assignment problem (QAP and QSAP) and the p-median problem. Again the problem with capacity constraints becomes 
obvious when considering the CMST. Furthermore, certain ideas of diversification become necessary, e.g., when 
considering the QAP and the p-median problem. For these two problems we have proposed different diversification 
strategies based on the REM. For the p-median problem a learning approach based on the idea of modifying the 
condition when to designate a tabu status (cf. Figure 3.2) proved to be successful (Voß[Voß94]). For the QAP a strategy 
building on elite solutions as well as a parallel development of solution trajectories (cf. Sondergeld and Voß[SV95]) 
may be
  
< previous page
page_55
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_55.html2009-7-7 16:56:28

page_56
< previous page
page_56
next page >
Page 56
Table 3.2 Literature survey
Problem
Authors
Approach / comments
CMST
Amberg et al.[ADV95]
STS / CSM / REM, 
comparison to simulated annealing
QAP
Skorin-Kapov[SK90] 
Battiti and Tecchiolli[BT94] 
 
Kelly et al.[KLG94] 
Skorin-Kapov[SK94] 
Voß[Voß95]
STS 
reactive tabu search comparison to simulated 
annealing diversification strategies moving 
gap 
REM / elite solution approach
QSAP
Domschke et al.[DFV92]
STS / CSM / REM, 
comparison to simulated annealing
SP/SPD
Duin and Voß [DV94] 
Voß [Voß96]
REM 
STS / CSM / REM
WLP
Captivo[Cap94] 
Schildt[Sch94]
STS / CSM / REM 
STS / CSM / REM, 
comparison to genetic algorithms and 
simulated annealing
p-median Voß [Voß96] 
Rolland et al.[RSC95] 
Voß [Voß94]
STS / CSM / REM 
STS, incl. random variation of s REM
 
considered. The minimum weighted p-cardinality tree problem asks for a minimum cost connected acyclic subgraph 
with p nodes of a given graph. An application appears in location and oil lease management in the Norwegian part of 
the North Sea (cf. Jörnsten and Lokketangen[JL94]). The authors describe a moving gap approach. Kincaid[Kin95] 
compares STS with a simulated annealing approach for the damper placement problem for large flexible space truss 
structures which is to determine those p truss members of a structure to replace with dampers so that the modal damping 
ratio is as large as possible for all significant modes of vibration. Murray and Church[MC95] compare STS with a 
simulated annealing approach for an operational forest planning problem. For a survey on relevant references from the 
literature see Table 3.2.
3.4 
Conclusions
The basic foundation of incorporating some sort of intelligence into search methods is to emulate human behaviour. 
Human beings are creatures of memory, using a variety of memory functions to help guide their way through a maze of 
problem solving considerations. Therefore, it seems to be natural to try to endow solution methods with similar 
capabilities including a transparent and natural rationale. Its goal may be to emulate intelligent uses of memory, 
particularly for exploiting structure. This led to the development especially of tabu search, a metastrategy or general 
purpose approach
  
< previous page
page_56
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_56.html2009-7-7 16:56:29

page_57
< previous page
page_57
next page >
Page 57
superimposed on local search approaches. One of the goals of tabu search is to permit good moves in each iteration, 
while including some sort of intelligence into the search, e.g. by means of adaptive memory and responsive exploration. 
Preventing the search from revisiting previous solutions may be based on some sort of memory function. The basic idea 
of tabu search is to store previous solutions (the search trajectory) or, more efficiently, the corresponding 
neighbourhood changes which led to these solutions and then to forbid the reversal of these exchanges for a certain 
amount of time.
In this paper special attention is given to a class of tabu strategies that take account of dynamic interdependencies 
among the memory elements. These interdependencies rely on logical structure as well as on heuristic elements. With 
these methods, i.e. REM and CSM, we were able to improve the best-known solutions to some benchmark problems 
that had previously been investigated by a large variety of researchers.
From a theoretical point of view REM discloses the gap between a short-term memory and a long-term memory 
framework in that it allows for sufficiency and necessity to prevent revisiting previously encountered solutions. Due to 
this fact it becomes obvious that REM has an intensificational behaviour and different aspects may be taken into 
account to allow for diversification. Corresponding methods have been developed and investigated with respect to some 
examples.
To judge the merit of this work our main goal may be re-read in the following sentence: We have emphasized the 
dynamic tabu search approaches more and other approaches less because most attention in the literature is on the 
other methods, and we were seeking to fill a gap by disclosing more fully the relevance of the dynamic methods.
Future directions shall be based on ideas presented, among others, in Glover and Laguna[GL93] and some of the 
contributions in Glover et al.[GLTdW93] as well as other edited volumes as indicated above. Furthermore, we strongly 
emphasize hybrids with ideas of Glover[Glo90a] and Voß[Voß96].
References
[ADV95] Amberg A., Domschke W., and Voß S. (1995) Capacitated minimum spanning trees: algorithms using 
intelligent search. TH Darmstadt and TU Braunschweig.
[Bat95] Battiti R. (1995) Reactive search: Toward self-tuning heuristics. University of Trento.
[Bea89] Beasley J. (1989) An SST-based algorithm for the Steiner problem in graphs. Networks 19: 116.
[Bea90] Beasley J. (1990) OR-library: distributing test problems by electronic mail. Journal of the Operational 
Research Society 41: 10691072.
[Bea92] Beasley J. (1992) A heuristic for euclidean and rectilinear Steiner problems. European Journal of Operational 
Research 58: 284292.
[BT94] Battiti R. and Tecchiolli G. (1994) Simulated annealing and tabu search in the long run: a comparison on QAP 
tasks. Computers & Mathematics with Applications 28: 18.
[BT95] Battiti R. and Tecchiolli G. (1995) Local search with memory: Benchmarking RTS. OR Spektrum 17: 6786.
[Cap94] Captivo E. (1994) Private communication.
[DFV91] Dammeyer F., Forst P., and Voß S. (1991) On the cancellation sequence method of tabu search. ORSA 
Journal on Computing 3: 262265.
  
< previous page
page_57
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_57.html2009-7-7 16:56:29

page_58
< previous page
page_58
next page >
Page 58
[DFV92] Domschke W., Forst P., and Voß S. (1992) Tabu search techniques for the quadratic semi-assignment 
problem. In Fandel G., Gulledge T., and Jones A. (eds) New Directions for Operations Research in Manufacturing, 
pages 389405. Springer, Berlin.
[DV93] Dammeyer F. and Voß S. (1993) Dynamic tabu list management using the reverse elimination method. Annals 
of Operations Research 41: 3146.
[DV94] Duin C. and Voß S. (1994) Steiner tree heuristics a survey. In Dyckhoff H., Derigs U., Salomon M. and Tijms 
H. (eds) Operations Research Proceedings 1993, pages 485496. Springer, Berlin.
[dWH89] de Werra D. and Hertz A. (1989) Tabu search techniques: a tutorial and an application to neural networks. OR 
Spektrum 11: 131141.
[EP94] Erenguc S. and Pirkul H. (eds) (1994) Heuristics, genetic and tabu search. Computers & Operations Research 21
(8).
[FJW92] Francis R., McGinnis Jr. L., and White J. (1992) Facility Layout and Location: an Analytical Approach. 
Prentice Hall, Englewood Cliffs, NJ, 2nd edition.
[FK92] Faigle U. and Kern W. (1992) Some convergence results for probabilistic tabu search. ORSA Journal on 
Computing 4: 3237.
[GG89] Glover F. and Greenberg H. (1989) New approaches for heuristic search: a bilateral linkage with artificial 
intelligence. European Journal of Operational Research 39: 119130.
[GL93] Glover F. and Laguna M. (1993) Tabu search. In Reeves C. (ed) Modern Heuristic Techniques for 
Combinatorial Problems, pages 70150. Blackwell, Oxford.
[Glo77] Glover F. (1977) Heuristics for integer programming using surrogate constraints. Decision Sciences 8: 156166.
[Glo86] Glover F. (1986) Future paths for integer programming and links to artificial intelligence. Computers & 
Operations Research 13: 533549.
[Glo89] Glover F. (1989) Tabu search - part i. ORSA Journal on Computing 1: 190206.
[Glo90a] Glover F. (1990) Tabu search - part ii. ORSA Journal on Computing 1: 432.
[Glo90b] Glover F. (1990) Tabu search: a tutorial. Interfaces 20(4): 7494.
[GLTdW93] Glover F., Laguna M., Taillard E., and de Werra D. (eds) (1993) Tabu search. Annals of Operations 
Research 41.
[GM86] Glover F. and McMillan C. (1986) The general employee scheduling problem: an integration of MS and AI. 
Computers & Operations Research 13: 563573.
[Han66] Hanan M. (1966) On Steiner's problem with rectilinear distance. SIAM Journal on Applied Mathematics 14: 
255265.
[HG94] Hübscher R. and Glover F. (1994) Applying tabu search with influential diversification to multiprocessor 
scheduling. Computers & Operations Research 21: 877884.
[HRW92] Hwang F., Richards D., and Winter P. (1992) The Steiner Tree Problem. North-Holland, Amsterdam.
[JL94] Jörnsten K. and Lokketangen A. (1994) Tabu search for weighted k-cardinality trees. Molde College. To appear.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_58.html（第 1／2 页）2009-7-7 16:56:30

page_58
[Kin95] Kincaid R. (1995) Solving the damper placement problem via local search heuristics. OR Spektrum 17: 149158.
[KLG94] Kelly J., Laguna M., and Glover F. (1994) A study of diversification strategies for the quadratic assignment 
problem. Computers & Operations Research 21: 885893.
[KP93] Khoury B. and Pardalos P. (1993) A heuristic for the Steiner problem in graphs. University of Florida, 
Gainesville.
[LG95] Lokketangen A. and Glover F. (1995) Probabilistic move selection in tabu search for zero-one mixed integer 
programming problems. Molde College. To
  
< previous page
page_58
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_58.html（第 2／2 页）2009-7-7 16:56:30

page_59
< previous page
page_59
next page >
Page 59
appear.
[Lin65] Lin S. (1965) Computer solutions of the traveling salesman problem. Bell Syst. Tech. J. 44: 22452269.
[LO96] Laporte G. and Osman I. (eds) (1996) Metaheuristics in combinatorial optimization. Annals of Operations 
Research, to appear.
[MC95] Murray A. and Church R. (1995) Heuristic solution approaches to operational forest planning problems. OR 
Spektrum 17: 193203.
[MW84] Magnanti T. and Wong R. (1984) Network design and transportation planning: models and algorithms. 
Transportation Science 18: 155.
[OK96] Osman I. and Kelly J. (eds) (1996) Proceedings of the Metaheuristics International Conference. Kluwer, 
Dordrecht, to appear.
[PV95] Pesch E. and Voß S. (eds) (1995) Applied local search. OR Spektrum 17.
[RSC95] Rolland E., Schilling D., and Current J. (1995) An efficient tabu search procedure for the p-median problem. 
University of California at Riverside.
[RT95] Rochat Y. and Taillard E. (1995) Probabilistic diversification and intensification in local search for vehicle 
routing. Universite de Montreal. To appear.
[SC73] Soukup J. and Chow W. (1973) Set of test problems for the minimum length connection networks. ACM / 
SIGMAP Newsletter 15: 4851.
[Sch94] Schildt B. (1994) Betriebliche Standortoptimierung in der strategischen Produktions- und 
Distributionsplanung bei degressiv verlaufenden Produktions-kosten. PhD thesis, TH Darmstadt.
[Ser81] Servit M. (1981) Heuristic algorithms for rectilinear Steiner trees. Digital Processes 7: 2132.
[SK90] Skorin-Kapov J. (1990) Tabu search applied to the quadratic assignment problem. ORSA Journal on Computing 
2: 3345.
[SK94] Skorin-Kapov J. (1994) Extensions of a tabu search adaptation to the quadratic assignment problem. Computers 
& Operations Research 21: 855866.
[SV95] Sondergeld L. and Voß S. (1995) A star-shaped diversification approach in tabu search. TU Braunschweig. To 
appear.
[TV94] Thiel J. and Voß S. (1994) Some experiences on solving multiconstraint zero-one knapsack problems with 
genetic algorithms. INFOR 32: 226242.
[VD93] Voß S. and Duin C. (1993) Heuristic methods for the rectilinear Steiner arborescence problem. Engineering 
Optimization 21: 121145.
[Voß94] Voß S. (1994) A reverse elimination approach for the p-median problem. TH Darmstadt.
[Voß95] Voß S. (1995) Solving quadratic assignment problems using the reverse elimination method. In Nash S. and 
Sofer A. (eds) The Impact of Emerging Technologies on Computer Science and Operations Research, pages 281296. 
Kluwer, Dordrecht.
[Voß96] Voß S. (1996) Intelligent search. Springer, Berlin, to appear.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_59.html（第 1／2 页）2009-7-7 16:56:31

page_59
[VSA95] Verhoeven M., Severens M., and Aarts E. (1995) Local search for Steiner trees in graphs. Eindhoven 
University of Technology.
[WH89] Widmer M. and Hertz A. (1989) A new heuristic method for the flow shop sequencing problem. European 
Journal of Operational Research 41: 186193.
[WZ93] Woodruff D. and Zemel E. (1993) Hashing vectors for tabu search. Annals of Operations Research 41: 123137.
  
< previous page
page_59
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_59.html（第 2／2 页）2009-7-7 16:56:31

page_61
< previous page
page_61
next page >
Page 61
4 
Reactive Search: 
Toward Self-tuning Heuristics
Roberto Battiti
Abstract
Local (neighborhood) search can be guided to go beyond local minima through meta-heuristic methods that use the 
information obtained in the previous part of the run. The reactive search (RS) method proposes the integration of simple 
history-based feedback schemes in local search for the on-line determination of free parameters, therefore endowing the 
algorithm with the flexibility needed to cover a wide spectrum of problems but avoiding a human trial-and-error 
adjustment. In particular, in the reactive tabu search (RTS) algorithm a simple feedback scheme determines the value of 
the prohibition parameter in tabu search, so that a balance of exploration versus exploitation is obtained that is 
appropriate for the local characteristics of the task. This paper summarizes the reactive search framework, reviews RTS 
and its applications, and presents novel results about the behavior of different tabu search schemes when escaping from 
a local attractor.
4.1 
Reactive Search: 
Feedback Applied to Heuristics
The reactive search (RS) framework proposes the introduction of feedback schemes in heuristics for discrete 
optimization problems. RS belongs to the meta-heuristic class of algorithms, where a basic scheme (local neighborhood 
search) is complemented to avoid its known defects. A simple form of reinforcement learning [Lin92] is applied: the 
machine-learning agent is the discrete dynamical system that generates the search trajectory, whose internal parameters 
are changed through simple sub-symbolic mechanisms. Reactive search is intensively history-based, in fact learning can 
be defined as the influence on the behavior of a system of the past experienced events. Let us now summarize the 
context and the main motivations of the approach.
The local search heuristic for minimizing a cost function f starts from an admissible solution and tries to obtain a better 
one in an iterative way by 'looking in the neighborhood'. A search trajectory is generated in a greedy way if the 
algorithm
Modern Heuristic Search Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_61
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_61.html2009-7-7 16:56:31

page_62
< previous page
page_62
next page >
Page 62
chooses at each step the best neighbor of the current tentative solution. Local search can be effective if the 
neighborhood structure matches the characteristics of the problem, but it stops when the current point is a local 
minimizer, i.e., when all neighbors have higher f values. Some possible actions to go beyond local minima while aiming 
at better suboptimal points are: i) the repetition of local search after restarting from different (possibly random) points, 
ii) the acceptance of upward-moving steps to exit local minima (e.g., in simulated annealing [KJV83], tabu search 
[Glo89a]), iii) the consideration of more complex schemes based on the combination and selection of a set of 
suboptimal solutions (genetic algorithms [Hol75] are based on this principle). There are at least a couple of potential 
drawbacks of simple implementations of the above schemes:
parameter tuning: some schemes require a careful selection of parameters by the user before competitive results are 
obtained for specific problems. This requires either a deep knowledge of the problem structure or simply a lengthy 'trial 
and error' tinkering process, that is not always fully reproducible (see for example the discussion in [BGK+ 95]).
search confinement: after a local minimizer is encountered, all points in its attraction basin lose any interest for 
optimization. The search should avoid wasting excessive computing time in a single basin and diversification should be 
activated. On the other hand, in the assumptions that neighbors have correlated cost function values, some effort should 
be spent in searching for better points located close to the most recently found local minimum point (intensification). 
The two requirements are conflicting and finding a proper balance of diversification and intensification is a crucial issue 
in heuristics.
Can one obtain wide spectrum algorithms covering many applications without parameter tuning and, at the same time, 
avoiding excessive wastes of computing resources? We claim that this can be done in some cases by using reactive 
schemes, where the past history of the search is used for:
feedback-based parameter tuning: the algorithm maintains the internal flexibility needed to cover many problems, but 
the tuning is automated, and executed while the algorithm runs and 'monitors' its past behavior.
automated balance of diversification and intensification: the 'exploration versus exploitation' dilemma is present in 
many heuristics: is it better to intensify the search in the promising regions, or to diversify it to uncharted territories? An 
automated heuristic balance can be obtained through feedback mechanisms, for example by starting with intensification, 
and by progressively increasing the amount of diversification only when there is evidence that diversification is needed.
The focus of the RS method is on wide spectrum heuristic algorithms for discrete optimization, in which local search is 
complemented by feedback (reactive) schemes that use the past history of the search to increase its efficiency and 
efficacy. Related approaches in a different context are, for example, the self-adaptation of the generating
  
< previous page
page_62
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_62.html2009-7-7 16:56:32

page_63
< previous page
page_63
next page >
Page 63
distributions of Evolutionsstrategie algorithms in [Sch], or the use of adaptive annealing schedules proposed by 
[HRSV86] and by other researchers.
The paper is organized as follows. Some underlying principles of Tabu Search (TS) are summarized in Section 4.2 and a 
classification of some TS-based algorithms is illustrated in Section 4.3. The application of reactive schemes in TS is 
summarized in Section 4.4. The results obtained by different versions of TS on a paradigmatic problem defined on 
binary strings are discussed in Section 4.5. The paper is concluded by a review of RS applications and future projects 
(Sections 4.6 to 4.7).
4.2 
Tabu Search: 
Beyond Local Search
The tabu search meta-heuristic [Glo89a] is based on the use of prohibition-based techniques and 'intelligent' schemes as 
a complement to basic heuristic algorithms like local search, with the purpose of guiding the basic heuristic beyond 
local optimality. It is difficult to assign a precise date of birth to these principles. For example, ideas similar to those 
proposed in TS can be found in the denial strategy of [SW68] (once common features are detected in many suboptimal 
solutions, they are forbidden) or in the opposite reduction strategy of [Lin65] (in an application to the travelling 
salesman problem, all edges that are common to a set of local optima are fixed). In very different contexts, prohibition-
like strategies can be found in cutting planes algorithms for solving integer problems through their linear programming 
relaxation (inequalities that cut off previously obtained fractional solutions are generated) and in branch and bound 
algorithms (subtrees are not considered if the leaves cannot correspond to better solutions), see the textbook [PS82].
The renaissance and full blossoming of 'intelligent prohibition-based heuristics' starting from the late eighties is greatly 
due to the role of F. Glover in the proposal and diffusion of a rich variety of meta-heuristic tools [Glo89a,Glo89b], but 
see also [HJ90] for an independent seminal paper. A growing number of TS-based algorithms has been developed in the 
last years and applied with success to a wide selection of problems [Glo94, Voß93]. It is therefore difficult, if not 
impossible, to characterize a 'canonical form' of TS, and classifications tend to be short-lived. Nonetheless, at least two 
aspects characterize many versions of TS: the fact that TS is used to complement local (neighborhood) search, and the 
fact that the main modifications to local search are obtained through the prohibition of selected moves available at the 
current point. Local search is effective if the neighborhood is appropriate to the problem structure but it clearly stops as 
soon as the first local minimizer is encountered, when no improving moves are available. TS acts to continue the search 
beyond the first local minimizer without wasting the work already executed, as is the case if a new run of local search is 
started from a new random initial point, and to enforce appropriate amounts of diversification to avoid the search 
trajectory remaining confined near a given local minimizer.
In our opinion, the main competitive advantage of TS with respect to alternative heuristics based on local search like 
simulated annealing (SA) [KJV83] lies in the intelligent use of the past history of the search to influence its future steps. 
On the contrary, SA generates a Markov chain: the successor of the current point is chosen stochastically, with a 
probability that depends only on the current point and not on the
  
< previous page
page_63
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_63.html2009-7-7 16:56:33

page_64
< previous page
page_64
next page >
Page 64
previous history. The non-Markovian property is a mixed blessing: it permits heuristic results that are much better in 
many cases, but makes the theoretical analysis of the algorithm difficult. In fact, the practical success of TS techniques 
should motivate new search streams in mathematics and computer science for its theoretical foundation. Incidentally, 
the often cited asymptotic convergence results of SA are unfortunately irrelevant for the application of SA to 
optimization. In fact, repeated local search [FZ93], and even random search [CC88] has better asymptotic results. 
According to [AKZ95] 'approximating the asymptotic behavior of SA arbitrarily closely requires a number of 
transitions that for most problems is typically larger than the size of the solution space . . . Thus, the SA algorithm is 
clearly unsuited for solving combinatorial optimization problems to optimality.' Of course, SA can be used in practice 
with fast cooling schedules, but then the asymptotic results are not directly applicable. The optimal finite-length 
annealing schedules obtained on specific simple problems do not always correspond to those intuitively expected from 
the limiting theorems [SK91].
Let us define the notation. 
 is the search space (as an example, the set of binary strings with a given length 
), X(t) is the current solution along the trajectory at iteration ('time') t. N(X(t)) is the neighborhood of 
point X(t), obtained by applying a set of basic moves µ0, µ1, . . ., µM to the current configuration:
In the case of binary strings, the moves can be those changing (complementing) the individual bits, and therefore M is 
equal to the string length L. Some of the neighbors are prohibited, a subset NA(X(t)) ⊂ N(X(t)) contains the allowed 
ones. The general way of generating the search trajectory that we consider is given by:
The set-valued function Allow selects a subset of N(X(t+1)) in a manner that depends on the entire search trajectory X
(0), . . ., X(t+1).
4.3 
Some Forms of Tabu Search
There are several vantage points from which to view heuristic algorithms. By analogy with the concept of abstract data 
type in computer science [AHU83], and with the related object-oriented software engineering techniques [Cox90], it is 
useful to separate the abstract concepts and operations of TS from the detailed implementation, i.e., realization with 
specific data structures. In other words, policies (that determine which trajectory is generated in the search space, what 
the balance of intensification and diversification is, etc.) should be separated from mechanisms that determine how a 
specific policy is realized. An essential abstract concept in TS is given by the discrete dynamical system of equations 
(4.1) and (4.2) obtained by modifying local search. A taxonomy of tabu search is now presented where the abstract 
point of view of the dynamical systems is separated from the different implementation possibilities. Only a limited 
subset of the existing TS algorithms is cited in the presented classification.
  
< previous page
page_64
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_64.html2009-7-7 16:56:33

page_65
< previous page
page_65
next page >
Page 65
4.3.1 
Dynamical Systems
A classification of some TS-related algorithms that is based on the underlying dynamical system is illustrated in Figure 
4.1.
Figure 4.1 
A classification based on discrete dynamical systems.
A first subdivision is given by the deterministic versus stochastic nature of the system. Let us first consider the 
deterministic versions. Possibly the simplest form of TS is what is called strict-TS: a neighbor is prohibited if and only 
if it has already been visited during the previous part of the search [Glo89a] (the term 'strict' is chosen to underline the 
rigid enforcement of its simple prohibition rule). Therefore equation (4.2) becomes:
Let us note that strict-TS is parameter-free.
Two additional algorithms can be obtained by introducing a prohibition parameter1 T that determines how long a move 
will remain prohibited after the execution of its inverse. The fixed-TS algorithm is obtained by fixing T throughout the 
search [Glo89a]. A neighbor is allowed if and only if it is obtained from the current point by applying a move such that 
its inverse has not been used during the last T iterations. In detail, if LastUsed(µ) is the last usage time of move µ 
(LastUsed(µ) = ∞ at the beginning):
If T changes with the iteration counter depending on the search status (in this case the notation will be T(t)), the general 
dynamical system that generates the
1 The term prohibition parameter is chosen instead of the more traditional list size because list size refers to a 
specific implementation: prohibitions can be obtained without using any list.
  
< previous page
page_65
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_65.html2009-7-7 16:56:34

page_66
< previous page
page_66
next page >
Page 66
search trajectory comprises an additional evolution equation for T(t), so that the three defining equations are now:
Let us note that µ = µ1 for the basic moves acting on binary strings. Now, possible rules to determine the prohibition 
parameter by reacting to the repetition of previously-visited configurations have been proposed in [BT94a] (reactive-TS, 
RTS for short). In addition, there are situations where the single reactive mechanism on T is not sufficient to avoid long 
cycles in the search trajectory and therefore a second reaction is needed [BT94a]. The main principles of RTS are 
briefly reviewed in Section 4.4.
Stochastic algorithms related to the previously described deterministic versions can be obtained in many ways. For 
example, prohibition rules can be substituted with probabilistic generation-acceptance rules with large probability for 
allowed moves, small for prohibited ones, see for example the probabilistic-TS [Glo89a]. Stochasticity can increase the 
robustness of the different algorithms, in addition [Glo89a] 'randomization is a means for achieving diversity without 
reliance on memory,' although it could 'entail a loss in efficiency by allowing duplications and potentially unproductive 
wandering that a more systematic approach would seek to eliminate.' Incidentally, asymptotic results for TS can be 
obtained in probabilistic TS [FK92]. In a different proposal (robust-TS) the prohibition parameter is randomly changed 
between an upper and a lower bound during the search [Tai91]. Stochasticity in fixed-TS and in reactive-TS can be 
added through a random breaking of ties, in the event that the same cost function decrease is obtained by more than one 
winner in the Best-Neighbor computation. At least this simple form of stochasticity should always be used to avoid 
external search biases, possibly caused by the ordering of the loop indices.
If the neighborhood evaluation is expensive, the exhaustive evaluation can be substituted with a partial stochastic 
sampling: only a partial list of candidates is examined before choosing the best allowed neighbor.
4.3.2 
Implementations
While the above classification deals with dynamical systems, a different classification is based on the detailed data 
structures used in the algorithms and on the consequent realization of the needed operations. Different data structures 
can possess widely different computational complexities so that attention should be spent on this subject before 
choosing a version of TS that is efficient on a particular problem.
  
< previous page
page_66
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_66.html2009-7-7 16:56:35

page_67
< previous page
page_67
next page >
Page 67
Figure 4.2 
The same search trajectory can be obtained with different data structures.
Some examples of different implementations of the same TS dynamics are illustrated in Figure 4.2. Strict-TS can be 
implemented through the reverse elimination method (REM) [Glo89b,DV93], a term that refers to a technique for the 
storage and analysis of the ordered list of all moves performed throughout the search (called 'running list'). The same 
dynamics can be obtained in all cases through standard hashing methods and storage of the configurations [WZ93, 
BT94a], or, for the case of a search space consisting of binary strings, through the radix tree (or 'digital tree') technique 
[BT94a]. Hashing is an old tool in computer science: different hashing functions possibly with incremental calculation 
are available for different domains [CLR90]. REM is not applicable to all problems (the 'sufficiency property' must be 
satisfied [Glo89b]), in addition its computational complexity per iteration is proportional to the number of iterations 
executed, while the average complexity obtained through incremental hashing is O(1), a small and constant number of 
operations. The worst-case complexity per iteration obtained with the radix tree technique is proportional to the number 
of bits in the binary strings, and constant with respect to the iteration. If the memory usage is considered, both REM and 
approximated hashing use O(1) memory per iteration, while the actual number of bytes stored can be less for REM, 
because only changes (moves) and not configurations are stored.
Trivially, fixed-TS (alternative terms [Glo89a,Glo89b] are: simple TS, static tabu search STS or tabu navigation 
method) can be realized with a first-in first-out list where the prohibited moves are located (the 'tabu list'), or by storing 
in an array the last usage time of each move and by using equation (4.4).
Reactive-TS can be implemented through a simple list of visited configurations, or with more efficient hashing or radix 
tree techniques. At a finer level of detail, hashing
  
< previous page
page_67
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_67.html2009-7-7 16:56:35

page_68
< previous page
page_68
next page >
Page 68
can be realized in different ways. If the entire configuration is stored (see also Figure 4.4) an exact answer is obtained 
from the memory lookup operation (a repetition is reported if and only if the configuration has been visited before). On 
the other hand, if a 'compressed' item is stored, like a hashed value of a limited length derived from the configuration, 
the answer will have a limited probability of error (a repetition can be reported even if the configuration is new, because 
the compressed items are equal by chance an event called 'collision'). Experimentally, small collision probabilities do 
not have statistically significant effects on the use of reactive-TS as a heuristic tool, and hashing versions that need only 
a few bytes per iteration can be used. The effect of collision probabilities when hashing is used in other schemes is a 
subject worth investigating.
4.4 
Reactive Tabu Search (RTS)
The RTS algorithm [BT94a] represents the first realization of the principles introduced in Section 4.1 in the framework 
of F. Glover's tabu search (TS). Let us first note that the prohibition parameter T used in equation (4.4) is related to the 
amount of diversification: the larger T, the longer the distance that the search trajectory must go before it is allowed to 
come back to a previously visited point. But T cannot be too large, otherwise no move will be allowed after an initial 
phase. In detail, if the search space is given by binary strings of length L, an upper bound T ≤ (L 2) guarantees that at 
least two moves are allowed at each iteration, so that the search does not get stuck and the move choice is influenced by 
the cost function value (it is not if only one move is allowed!). Now, if only allowed moves are executed, and T satisfies 
T ≤ (L 2), one obtains:
The Hamming distance H between a starting point and successive points along the trajectory is strictly increasing for T 
+ 1 steps.
The minimum repetition interval R along the trajectory is 2(T + 1).
Some problems arising in TS that are worth investigating are:
1. the determination of an appropriate prohibition T for the different tasks,
2. the robustness of the technique for a wide range of different problems,
3. the adoption of minimal computational complexity algorithms for using the search history.
The three issues are briefly discussed in the following subsections, together with the RTS methods proposed to deal 
with them.
4.4.1 
Self-Adjusted Prohibition Period
In RTS the prohibition T is determined through feedback (reactive) mechanisms during the search. T is equal to one at 
the beginning (the inverse of a given move
  
< previous page
page_68
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_68.html2009-7-7 16:56:36

page_69
< previous page
page_69
next page >
Page 69
is prohibited only at the next step), it increases only when there is evidence that diversification is needed, it decreases 
when this evidence disappears. In detail: the evidence that diversification is needed is signaled by the repetition of 
previously visited configurations. All configurations found during the search are stored in memory. After a move is 
executed the algorithm checks whether the current configuration has already been found and it reacts accordingly (T 
increases if a configuration is repeated, T decreases if no repetitions occurred during a sufficiently long period).
Figure 4.3 
Dynamics of the the prohibition period T on a QAP task.
Let us note that T is not fixed during the search, but is determined in a dynamic way depending on the local structure of 
the search space. This is particularly relevant for 'inhomogeneous' tasks, where the statistical properties of the search 
space vary widely in the different regions (in these cases a fixed T would be inappropriate).
An example of the behavior of T during the search is illustrated in Figure 4.3, for a quadratic assignment problem task 
[BT94a]. T increases in an exponential way when repetitions are encountered, it decreases in a gradual manner when 
repetitions disappear.
4.4.2 
The Escape Mechanism
The basic tabu mechanism based on prohibitions is not sufficient to avoid long cycles (e.g., for binary strings of length 
L, T must be less than the length of the string, otherwise all moves are eventually prohibited, and therefore cycles longer 
than 2 × L are still possible). In addition, even if 'limit cycles' (endless cyclic repetitions of a given set of configurations) 
are avoided, the first reactive mechanism is not sufficient to guarantee that the search trajectory is not confined in a 
limited region of the search
  
< previous page
page_69
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_69.html2009-7-7 16:56:36

page_70
< previous page
page_70
next page >
Page 70
space. A 'chaotic trapping' of the trajectory in a limited portion of the search space is still possible (the analogy is with 
chaotic attractors of dynamical systems, where the trajectory is confined in a limited portion of the space, although a 
limit cycle is not present).
For both reasons, to increase the robustness of the algorithm a second more radical diversification step (escape) is 
needed. The escape phase is triggered when too many configurations are repeated too often [BT94a]. A simple escape 
consists of a number of random steps executed starting from the current configuration (possibly with a bias toward steps 
that bring the trajectory away from the current search region).
With a stochastic escape, one can easily obtain the asymptotic convergence of RTS (in a finite-cardinality search space, 
escape is activated infinitely often: if the probability for a point to be reached after escaping is different from zero for all 
points, eventually all points will be visited clearly including the globally optimal points). The detailed investigation of 
the asymptotic properties and finite-time effects of different escape routines to enforce long-term diversification is an 
open research area.
4.4.3 
Fast Algorithms for Using the Search History
The storage and access of the past events is executed through the well-known hashing or radix-tree techniques in a CPU 
time that is approximately constant with respect to the number of iterations. Therefore the overhead caused by the use of 
the history is negligible for tasks requiring a non-trivial number of operations to evaluate the cost function in the 
neighborhood.
Figure 4.4 
Open hashing scheme: items 
(configuration, or compressed hashed value, etc.) 
are stored in 'buckets'. The index of the bucket array is calculated 
from the configuration.
An example of a memory configuration for the hashing scheme is shown in Figure 4.4. From the current configuration 
phi one obtains an index into a 'bucket array.' The items (configuration or hashed value or derived quantity, last time of 
visit, total
  
< previous page
page_70
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_70.html2009-7-7 16:56:37

page_71
< previous page
page_71
next page >
Page 71
number of repetitions) are then stored in linked lists starting from the indexed array entry. Both storage and retrieval 
require an approximately constant amount of time if: i) the number of stored items is not much larger than the size of 
the bucket array, and ii) the hashing function scatters the items with a uniform probability over the different array 
indices. More precisely, given a hash table with m slots that stores n elements, a load factor α = n/m is defined. If 
collisions are resolved by chaining, searches take O(1 + α) time on average.
4.5 
Different Ways of Escaping from a Local Attractor
Local minima points are attractors of the search trajectory generated by deterministic local search. If the cost function is 
integer-valued and lower bounded it can be easily shown that a trajectory starting from an arbitrary point will terminate 
at a local minimizer. All points such that a deterministic local search trajectory starting from them terminates at a 
specific local minimizer make up its attraction basin. Now, as soon as a local minimizer is encountered, its entire 
attraction basin is not of interest for the optimization procedure, in fact its points do not have smaller cost values. It is 
nonetheless true that better points could be close to the current basin, whose boundaries are not known. One of the 
problems that must be solved in heuristic techniques based on local search is how to continue the search beyond the 
local minimizer and how to avoid the confinement of the search trajectory. Confinements can happen because the 
trajectory tends to be biased toward points with low cost function values, and therefore also toward the just abandoned 
local minimizer. The fact that the search trajectory remains close to the minimizer for some iterations is clearly a 
desired effect in the hypothesis that better points are preferentially located in the neighborhood of good sub-optimal 
points rather than among randomly extracted points.
Simple confinements can be cycles (endless repetition of a sequence of configurations during the search) or more 
complex trajectories with no clear periodicity but nonetheless such that only a limited portion of the search space is 
visited (they are analogous to chaotic attractors in dynamical systems).
An heuristic prescription is that the search point is kept close to a discovered local minimizer at the beginning, snooping 
about better attraction basins. If these are not discovered, the search should gradually progress to larger distances 
(therefore progressively enforcing longer-term diversification strategies).
Some very different ways of realizing this general prescription are illustrated here for a 'laboratory' test problem defined 
as follows. The search space is the set of all binary strings of length L. Let us assume that the search has just reached a 
(strict) local minimizer and that the cost f in the neighborhood is strictly increasing as a function of the number of 
different bits with respect to the given local minimizer (i.e., as a function of the Hamming distance). Without loss of 
generality, let us assume that the local minimizer is the zero string ( [00 . . . 0] ) and that the cost is precisely the 
Hamming distance. Although artificial, the assumption is not unrealistic in many cases. An analogy in continuous space 
is the usual positive-definite quadratic approximation of the cost in the neighborhood of a strict local minimizer of a 
differentiable function. In the following parts the discussion is mostly limited to deterministic versions.
  
< previous page
page_71
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_71.html2009-7-7 16:56:38

page_72
< previous page
page_72
next page >
Page 72
4.5.1 
Strict-TS
In the deterministic version of strict-TS, if more than one basic move produces the same cost decrease at a given 
iteration, the move that acts on the right-most (least significant) bit of the string is selected.
Figure 4.5 
Search trajectory for deterministic strict-TS: iteration 
t, Hamming distance H and binary string.
The set of obtained configurations for L = 4 is illustrated in Figure 4.5. Let us now consider how the Hamming distance 
evolves in time, in the optimistic assumption that the search always finds an allowed move until all points of the search 
space are visited. If H(t) is the Hamming distance at iteration t, the following holds true:
This can be demonstrated after observing that a complete trajectory for an (L 1)-bit search space becomes a legal initial 
part of the trajectory for L-bit strings after appending zero as the most significant bit (see the trajectory for L = 3 in 
Figure 4.5). Now, a certain Hamming distance H can be reached only as soon as or after the H-th bit is set (e.g., H=4 
can be reached only at or after t = 8 because the fourth bit is set at this iteration), equation (4.8) trivially follows.
In practice the above optimistic assumption is not true: strict-TS can be stuck (trapped) at a configuration such that all 
neighbors have already been visited. In fact, the smallest L such that this event happens is L = 4 and the search is stuck 
at t = 14, so that the string [1101] is not visited. The problem worsens for higher-dimensional
  
< previous page
page_72
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_72.html2009-7-7 16:56:38

page_73
< previous page
page_73
next page >
Page 73
strings. For L = 10 the search is stuck after visiting 47 % of the entire search space, for L = 20 it is stuck after visiting 
only 11 % of the search space.
Figure 4.6 
Evolution of the Hamming distance for deterministic strict-TS ( L = 32).
If the trajectory must reach Hamming distance H with respect to the local minimum point before escaping (i.e., before 
encountering a better attraction basin) the necessary number of iterations is at least exponential in H. Figure 4.6 shows 
the actual evolution of the Hamming distance for the case of L = 32. The detailed dynamics is complex, as 'iron curtains' 
of visited points (that cannot be visited again) are created in the configuration space and the trajectory must obey the 
corresponding constraints. The slow growth of the Hamming distance is related to the 'basin filling' effect [BT94a] of 
strict-TS: all points at smaller distances tend to be visited before points at larger distances (unless the iron curtains 
prohibit the immediate visit of some points). On the other hand, let us note that the exploration is not as 'intensifying' as 
an intuitive picture of strict-TS could lead one to believe: some configurations at small Hamming distance are visited 
only in the last part of the search. As an example let us note that the point at H = 4 is visited at t = 8 while one point at H 
= 1 is visited at t = 11 (t is the iteration). This is caused by the fact that, as soon as a new bit is set for the first time, all 
bits to the right are progressively cleared (because new configurations with lower cost are obtained). In particular, the 
second configuration at H = 1 is encountered at t > 2, the third at t > 4, . . . the n-th at t > 2(n1). Therefore, at least a 
configuration at H = 1 will be encountered only after 2(L1) have been visited.
Let us note that the relation of equation (4.8) is valid only in the assumption that strict-TS is deterministic and that it is 
not stuck for any configuration. Let us now assume that one manages to obtain a more 'intensifying' version of strict-TS, 
i.e.,
  
< previous page
page_73
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_73.html2009-7-7 16:56:39

page_74
< previous page
page_74
next page >
Page 74
that all configurations at Hamming distance less than or equal to H are visited before configurations at distance greater 
than H. The initial growth of the Hamming distance is in this case much slower. In fact, the number of configurations 
CH to be visited is:
It can be easily derived that CH > > 2H, if H < < L. As an example2, for L = 32 one obtains from equation (4.9) a value 
C5 = 242 825, and therefore this number of configurations have to be visited before finding a configuration at Hamming 
distance greater than 5, while 25 = 32. An explosion in the number of iterations spent near a local optimum occurs 
unless the nearest attraction basin is very close. The situation worsens in higher-dimensional search spaces: for L = 64, 
C5 = 8 303 633, C4 = 679 121. This effect can be seen as a manifestation of the 'curse of dimensionality': a technique 
that works in very low-dimensional search space can encounter dramatic performance reductions as the dimension 
increases. In particular, there is the danger that the entire search span will be spent while visiting points at small 
Hamming distances, unless additional diversifying tools are introduced.
4.5.2 
Fixed-TS
Figure 4.7 
Evolution of the Hamming distance for both 
deterministic and stochastic fixed-TS (L = 32, T = 10).
2 Computations have been executed by Mathematica
.
  
< previous page
page_74
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_74.html2009-7-7 16:56:40

page_75
< previous page
page_75
next page >
Page 75
The analysis of fixed-TS is simple: as soon as a bit is changed it will remain prohibited ('frozen') for additional T steps. 
Therefore (see Figure 4.7), the Hamming distance with respect to the starting configuration will cycle in a regular 
manner between zero and a maximal value H = T + 1 (only at this iteration the ice around the first frozen bit melts down 
and allows changes that are immediately executed because H decreases). All configurations in a cycle are different 
(apart from the initial configuration). The cycling T behavior is the same for both the deterministic and the stochastic 
version, the property of the stochastic version is that different configurations have the possibility of being visited in 
different cycles. In fact all configurations at a given Hamming distance H have the same probability of being visited if 
H ≤ T + 1, zero probability otherwise.
The effectiveness of fixed-TS in escaping from the local attractor depends on the size of the T value with respect to the 
minimal distance such that a new attraction basin is encountered. In particular, if T is too small the trajectory will never 
escape, but if T is too big an 'over-constrained' trajectory will be generated.
4.5.3 
Reactive-TS
The behavior of reactive-TS depends on the specific reactive scheme used. Given the previously illustrated relation 
between the prohibition T and the diversification, a possible prescription is that of gradually increasing T if there is 
evidence that the system is confined near a local attractor, until a new attractor is encountered. In particular, the 
evidence for a confinement can be obtained from the repetition of a previously visited configuration, while the fact that 
a new attraction basin has been found can be postulated if repetitions disappear for a suitably long period. In this last 
case, T is gradually decreased. This general procedure was used in the design of the RTS algorithm in [BT94a], where 
specific rules are given for the entire feedback process.
In the present discussion we consider only the initial phase of the escape from the attractor, when increases of T are 
dominant over decreases. In fact, to simplify the discussion, let us assume that T = 1 at the beginning and that the 
reaction acts to increase T when a local minimum point is repeated, in the following manner:
The initial value (and lower bound) of one implies that the system does not come back immediately to a just left 
configuration. The upper bound is used to guarantee that at least two moves are allowed at each iteration. Non-integral 
values of T are cast to integers before using them (the largest integer less than or equal to T).
The evolution of T for the deterministic version is shown in Figure 4.8, repetitions of the local minimum point cause a 
rapid increase up to its maximal value. As soon as the value is reached the system enters a cycle. This limit cycle is 
caused by the fact that no additional attraction basins exist in the test case considered, while in real-world fitness 
surfaces the prohibition T tends to be small with respect to its upper bound, both because of the limited size of the 
attraction basins and because of the complementary reaction that decreases T if repetitions disappear.
  
< previous page
page_75
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_75.html2009-7-7 16:56:40

page_76
< previous page
page_76
next page >
Page 76
Figure 4.8 
Evolution of the prohibition parameter T for deterministic 
reactive-TS with reaction at local minimizers (L = 32).
Figure 4.9 
Evolution of the Hamming distance for simplified 
reactive-TS (L = 32).
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_76.html（第 1／2 页）2009-7-7 16:56:41

page_76
< previous page
page_76
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_76.html（第 2／2 页）2009-7-7 16:56:41

page_77
< previous page
page_77
next page >
Page 77
The behavior of the Hamming distance is illustrated in Figure 4.9. The maximal Hamming distance reached increases in 
a much faster way compared to the strict-TS case.
Now, for a given T(t) maximum Hamming distance that is reached during a cycle is Hmax = T(t) + 1 and the cycle 
length is 2(T(t) + 1). After the cycle is completed the local minimizer is repeated and the reaction occurs. The result is 
that T(t) increases monotonically, and therefore the cycle length does also, as illustrated in Figure 4.10 which expands 
the initial part of the graph.
Figure 4.10 
Evolution of the Hamming distance for reactive-TS, first 100 iterations 
(L = 32)
Let us now consider a generic iteration t at which a reaction occurs (like t = 4, 10, 18, . . . in Figure 4.10). At the 
beginning equation (4.10) will increase T by one unit at each step. If the prohibition is T just before the reaction, the 
total number t of iterations executed is:
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_77.html（第 1／2 页）2009-7-7 16:56:41

page_77
where the relation T = Hmax 1 has been used. Therefore the increase of the maximum reachable Hamming distance is 
approximately O(√t) during the initial steps. The
  
< previous page
page_77
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_77.html（第 2／2 页）2009-7-7 16:56:41

page_78
< previous page
page_78
next page >
Page 78
increase is clearly faster in later steps, when the reaction is multiplicative instead of additive (when T × 1.1 T + 1 in 
equation (4.10)), and therefore the above estimate of Hmax becomes a lower bound in the following phase.
Let us note that the difference with respect to strict-TS is a crucial one: one obtains an (optimistic) logarithmic increase 
in the strict algorithm, and a (pessimistic) increase that behaves like the square root of the number of iterations in the 
reactive case. In this last case bold tours at increasing distances are executed until the prohibition T is sufficient to 
escape from the attractor. In addition, if the properties of the fitness surface change slowly in the different regions, and 
RTS reaches a given local minimizer with a T value obtained during its previous history, the chances are large that it 
will escape even faster.
4.6 
Applications of Reactive Search: 
A Review
The reactive search framework and, in particular, the RTS algorithm have been and are being applied to a growing 
number of discrete optimization problems (Section 4.6.1), continuous optimization tasks (Section 4.6.2), and different 
problems arising in the context of sub-symbolic machine learning (Section 4.6.3). Special purpose VLSI systems have 
been designed and realized for real-time pattern recognition applications with machine learning techniques (Section 
4.6.4).
4.6.1 
Combinatorial Tasks
Reactive search has been applied to the following list of problems.
quadratic assignment problem (QAP) [BT92, BT94a, BT94b]
Nk model, a model of biological inspiration [BT92, BT95b]
01 knapsack [BT94a]
multi-knapsack (with multiple constraints) [BT95b]
max-clique [BP95]
biquadratic assignment problem (Bi-QAP) [BC95]
In many cases the results obtained with alternative competitive heuristics have been duplicated with low computational 
complexity, and without intensive parameter and algorithm tuning. In some cases (e.g., in the max-clique and bi-QAP 
problems) significantly better results have been obtained. A comparison of RTS with alternative heuristics (repeated 
local minima search, simulated annealing, genetic algorithms and mean field neural networks) is presented in [BT95b]. 
A comparison with simulated annealing on QAP tasks is contained in [BT94b]. A simple parallel implementation is 
presented in [BT92]. It has to be noted that RS is not a rigid algorithm but a general framework: specific algorithms 
have been introduced for unconstrained and constrained tasks, with different stochastic mechanisms and prohibition 
rules. As usually happens in heuristics, the more specific knowledge about a specific problem is used, the better the 
results. Nonetheless, it was often the case that simple RS versions realized with very limited effort could duplicate the 
performance of more complex schemes because of their simple embedded feedback (reinforcement learning) loop. A
  
< previous page
page_78
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_78.html2009-7-7 16:56:42

page_79
< previous page
page_79
next page >
Page 79
long-term goal of RS could be stated as the progressive shift of the learning capabilities from the algorithm user to the 
algorithm itself, through machine learning techniques.
4.6.2 
Continuous Optimization
A simple benchmark on a function with many suboptimal local minima is considered in [BT94a], where a 
straightforward discretization of the domain is used.
A novel algorithm for the global optimization of functions (C-RTS) is presented in [BT95a], in which a combinatorial 
optimization method cooperates with a stochastic local minimizer. The combinatorial optimization component, based on 
RTS, locates the most promising boxes, where starting points for the local minimizer are generated. In order to cover a 
wide spectrum of possible applications with no user intervention, the method is designed with adaptive mechanisms: in 
addition to the reactive adaptation of the prohibition period, the box size is adapted to the local structure of the function 
to be optimized (boxes are larger in 'flat' regions, smaller in regions with a 'rough' structure).
4.6.3 
Sub-Symbolic Machine Learning (Neural Networks)
While derivative-based methods for training from examples have been used with success in many contexts (error 
backpropagation [RHW86] is an example in the field of neural networks), they are applicable only to differentiable 
performance functions and are not always appropriate in the presence of local minima. In addition, the calculation of 
derivatives is expensive and error-prone, especially if special-purpose VLSI hardware is used. We use a radically 
different approach: the task is transformed into a combinatorial optimization problem (the points of the search space are 
binary strings), and solved with the RTS algorithm [BT95c]. To speed up the neighborhood evaluation phase a 
stochastic sampling of the neighborhood is adopted and a 'smart' iterative scheme is used to compute the changes in the 
performance function caused by changing a single weight.
RTS escapes rapidly from local minima, it is applicable to non-differentiable and even discontinuous functions and it is 
very robust with respect to the choice of the initial configuration. In addition, by fine-tuning the number of bits for each 
parameter one can decrease the size of the search space, increase the expected generalization and realize cost-effective 
VLSI, as it is briefly described in the next section. Representative papers are [Bat93] [BT95c] [BTT95] [BST+95].
4.6.4 
VLSI Systems with Learning Capabilities
In contrast to the exhaustive design of systems for pattern recognition, control, and vector quantization, an appealing 
possibility consists of specifying a general architecture, whose parameters are then tuned through machine learning 
(ML). ML becomes a combinatorial task if the parameters assume a discrete set of values: the reactive tabu search 
(RTS) algorithm permits the training of these systems with low number of bits per weight, low computational accuracy, 
no local minima 'trapping', and limited sensitivity to the initial conditions [BLST94a, BLST94b].
  
< previous page
page_79
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_79.html2009-7-7 16:56:42

page_80
< previous page
page_80
next page >
Page 80
Figure 4.11 
Layout of TOTEM chip: top level view of 32 processors, 
each containing the RAM cells (left), decoders (center), the MAC 
unit (bottom-right) and the output register (top-right). The control 
block is at the left.
Our project aims at developing special-purpose VLSI modules to be used as components of fully autonomous massively 
parallel systems for real-time 'adaptive' applications. Because of the intense use of parallelism at the chip and system 
level and the limited precision used, the obtained performance is competitive with that of state-of-the-art 
supercomputers (at a much lower cost), while a high degree of flexibility is maintained through the use of combinatorial 
algorithms.
In particular, neural nets can be realized. In contrast to many 'emulation' approaches, the developed VLSI completely 
reflects the combinatorial structure used in the learning algorithms. The first chip of the project (TOTEM, funded by 
INFN and designed at IRST [BLST94b, BLST95]) achieves a performance of more than one billion multiply-
accumulate operations. The chip layout is shown in Figure 4.11. Applications considered are in the area of pattern 
recognition (optical character recognition), events 'triggering' in high energy physics [ABL+95], control of non-linear 
systems [BT95c], compression of EEG signals [BST+95].
4.7 
Software
A two-year project 'Algorithms and software for optimization, and models of complex systems' started in 1995. The aim 
of the project is to apply optimization and machine learning algorithms in different domains and therefore particular 
attention is devoted to modern software engineering methods [Car95]. A code fragment is illustrated in Figure 4.12, that 
shows the basic RTS loop in the C programming language. On-line
  
< previous page
page_80
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_80.html2009-7-7 16:56:43

page_81
< previous page
page_81
next page >
Page 81
information about the project as well as additional information about reactive search are available through an Internet 
WWW archive (http://rtm.science.unitn.it/).
1   void  RTSLoop(long   max_time, 
2                 long   report_every) 
3    { 
4      while (Task.current.time < max_time) 
5         { 
6           if (check_for_repetitions(&Task.current) == DO_NOT_ESCAPE) 
7             { 
8                evaluate_neighborhood(); 
9                choose_best_move(); 
10               make_tabu(Task.chosen_bit); 
11               update_current_and_best(); 
12            } 
13         else 
14            { 
15               escape(); 
16            } 
17          if ((Task.current.time % report_every) == 0) 
18            PrintStateCompressed(); 
19        } 
20   }
Figure 4.12 
RTS basic loop, in the C programming language.
4.8 
Acknowledgements
This research was funded in part by the Special Project 199596 of the University of Trento (Math. Dept.), the initiative 
RTS of INFN (Istituto Nazionale di Fisica Nucleare), and MURST 'Progretto 40%-Efficienzo di algoritmi e progretto di 
structure informative'. Dr. G. Di Caro provided assistance in the computational tests.
References
[ABL+95] Anzellotti G., Battiti R., Lazzizzera I., Lee P., Sartori A., Soncini G., Tecchiolli G., and Zorat A. (1995) 
Totem: a highly parallel chip for triggering applications with inductive learning based on the reactive tabu search. 
International Journal of Modern Physics C 6(4): 555560.
[AHU83] Aho A. V., Hopcroft J. E., and Ullman J. D. (1983) Data Structures and Algorithms. Addison-Wesley, 
Reading, MA.
[AKZ95] Aarts E., Korst J., and Zwietering P. (1995) Deterministic and randomized local search. In P. Smolenskij M. 
M. and Rumelhart D. (eds) Mathematical Perspectives on Neural Networks. Lawrence Erlbaum Publishers, Hillsdale, 
NJ. to appear.
[Bat93] Battiti R. (1993) The reactive tabu search for machine learning. In Mauri G. (ed) Atti del Quarto Workshop del 
Gruppo AI*IA di Interesse Speciale su Apprendimento Automatico, pages 4155. Milano.
[BC95] Battiti R. and Çela E. (1995) Reactive tabu search for the biquadratic
  
< previous page
page_81
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_81.html2009-7-7 16:56:43

page_82
< previous page
page_82
next page >
Page 82
assignment problem. University of Trento. Working Paper.
[BGK+95] Barr R. S., Golden B. L., Kelly J. P., Resende M. G. C., and Stewart W. (1995) Designing and reporting on 
computational experiments with heuristic methods. In Proceedings of MIC-95. Breckenridge, CO. in press.
[BLST94a] Battiti R., Lee P., Sartori A., and Tecchiolli G. (June 1994) Combinatorial optimization for neural nets: Rts 
algorithm and silicon. Technical Report UTM 435, University of Trento.
[BLST94b] Battiti R., Lee P., Sartori A., and Tecchiolli G. (1994) Totem: A digital processor for neural networks and 
reactive tabu search. In Proceedings of the Fourth International Conference on Microelectronics for Neural Networks 
and Fuzzy Systems MICRONEURO'94, pages 1725. IEEE Computer Society Press, Torino, Italy.
[BLST95] Battiti R., Lee P., Sartori A., and Tecchiolli G. (1995) Special-purpose parallel architectures for high-
performance machine learning. In Proc. High Performance Computing and Networking, Milano Italy, page 944. 
Springer-Verlag, Berlin.
[BP95] Battiti R. and Protasi M. (1995) Reactive local search for the maximum clique problem. Technical Report TR-
95052, ICSI, Berkeley, CA.
[BST+95] Battiti R., Sartori A., Tecchiolli G., Tonella P., and Zorat A.(1995) Neural compression: an integrated 
approach to eeg signals. In Proc. of the International Workshop on Applications of Neural Networks to 
Telecommunications (IWANNT*95), pages 210217. Stockholm, Sweden.
[BT92] Battiti R. and Tecchiolli G. (1992) Parallel biased search for combinatorial optimization: Genetic algorithms 
and tabu. Microprocessor and Microsystems 16: 351367.
[BT94a] Battiti R. and Tecchiolli G. (1994) The reactive tabu search. ORSA Journal on Computing 6(2): 12640.
[BT94b] Battiti R. and Tecchiolli G. (1994) Simulated annealing and tabu search in the long run: a comparison on qap 
tasks. Computer and Mathematics with Applications 28(6): 18.
[BT95a] Battiti R. and Tecchiolli G. (1995) The continuous reactive tabu search: blending combinatorial optimization 
and stochastic search for global optimization. Annals of Operations Research in press.
[BT95b] Battiti R. and Tecchiolli G. (1995) Local search with memory: Benchmarking rts. Operations Research 
Spektrum 17(2/3): 6786.
[BT95c] Battiti R. and Tecchiolli G. (1995) Training nets with the reactive tabu search. IEEE Transactions on Neural 
Networks 6(5): 11851200.
[BTT95] Battiti R., Tecchiolli G., and Tonella P. (1995) Vector quantization with the reactive tabu search. In 
Proceedings of MIC-95. Breckenridge, CO. in press.
[Car95] Caro G. D. (1995) Regole di stile e di organizzazione per lo sviluppo di programmi in c. Technical Report 
UTM 462, University of Trento.
[CC88] Chiang T.-S. and Chow Y. (1988) On the convergence rate of annealing
processes. SIAM Journal on Control and Optimization 26(6): 14551470.
[CLR90] Cormen T. H., Leiserson C. E., and Rivest R. L. (1990) Introduction to Algorithms. McGraw-Hill, New York.
[Cox90] Cox B. J. (1990) Object Oriented Programming, an Evolutionary
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_82.html（第 1／2 页）2009-7-7 16:56:44

page_82
Approach. Addison-Wesley, Milano.
[DV93] Dammeyer F. and Vofi S. (1993) Dynamic tabu list management using the
reverse elimination method. Annals of Operations Research 41: 3146.
[FK92] Faigle U. and Kern W. (1992) Some convergence results for probabilistic tabu search. ORSA Journal on 
Computing 4(1): 3237.
[FZ93] Ferreira A. G. and Zerovnik J. (1993) Bounding the probability of success of stochastic methods for global 
optimization. Computer Math. Applic. 25: 18.
[Glo89a] Glover F. (1989) Tabu search part i. ORSA Journal on Computing 1(3): 190260.
[Glo89b] Glover F. (1989) Tabu search part ii. ORSA Journal on Computing
  
< previous page
page_82
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_82.html（第 2／2 页）2009-7-7 16:56:44

page_83
< previous page
page_83
next page >
Page 83
2(1): 432.
[Glo94] Glover F. (1994) Tabu search: Improved solution alternatives. In Birge J. R. and Murty K. G. (eds) 
Mathematical Programming, State of the Art 1994, pages 6492. The Univ. of Michigan.
[HJ90] Hansen and Jaumard B. (1990) Algorithms for the maximum satisfiability problem. Computing 44: 279303.
[Hol75] Holland J. (1975) Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann Arbor, MI.
[HRSV86] Huang M., Romeo F., and Sangiovanni-Vincentelli A. (1986) An efficient general cooling schedule for 
simulated annealing. In Proceedings of IICAD 86, pages 381384.
[KJV83] Kirkpatrick S., Jr. C. D. G., and Vecchi M. P. (1983) Optimization by simulated annealing. Science 220: 
671680.
[Lin65] Lin S. (1965) Computer solutions of the travelling salesman problems. BSTJ 44(10); 224569.
[Lin92] Lin L.-J. (1992) Self-improving reactive agents based on reinforcement learning, planning and teaching. 
Machine Learning Journal, Special Issue on Reinforcement Learning 8: 293322.
[PS82] Papadimitriou C. H. and Steiglitz K. (1982) Combinatorial Optimization, Algorithms and Complexity. Prentice-
Hall, Englewood Cliffs, NJ.
[RHW86] Rumelhart D., Hinton G., and Williams R. (1986) Learning internal representations by error propagation. In 
Parallel Distributed Processing Vol 1, pages 318362. MIT Press, Cambridge, MA.
[Sch] Schwefel H.-P. Numerische optimierung von computer-modellen mittels der evolutionsstrategie. Interdisciplinary 
systems research 26.
[SK91] Strenski P. N. and Kirkpatrick S. (1991) Analysis of finite length annealing schedules. Algorithmica 6: 346366.
[SW68] Steiglitz K. and Weiner P. (1968) Some improved algorithms for computer solution of the traveling salesman 
problem. In Proceedings of the Sixth Allerton Conf. on Circuit and System Theory, pages 81421. Urbana, Illinois.
[Tai91] Taillard E. (1991) Robust taboo search for the quadratic assignment problem. Parallel Computing 17: 443455.
[Voß93] Voß S. (1993) Intelligent search. Manuscript, TH Darmstadt.
[WZ93] Woodruff D. L. and Zemel E. (1993) Hashing vectors for tabu search. Annals of Operations Research 41: 
123138.
  
< previous page
page_83
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_83.html2009-7-7 16:56:44

page_85
< previous page
page_85
next page >
Page 85
5 
Combinatorial Optimization by Genetic Algorithms: 
The Value of the Genotype/Phenotype Distinction
J. David Schaffer and Larry J. Eshelman
Abstract
To date, genetic algorithms (GAs) have been generally more successful at solving parameter optimization problems 
than combinatorial problems. The reason seems to be that a string of numeric parameters, represented as bit strings or 
numbers, do naturally induce schemata that can be readily discovered and exploited for many problems. Combinatorial 
problems, on the other hand, call for discrete patterns or arrangements of objects which often exhibit highly 
discontinuous performance changes with small changes in the pattern. These problems seems to lack the schemata 
needed for GA success. We advocate an approach in which combinatorial problems are cast into parameter optimization 
problems by developing domain-specific algorithms that produce solutions (phenotypes) from strings of parameters 
(genotypes). After considerable experience with this approach, we recommend it over alternative approaches like 
developing specialized representations of operators for specific combinatorial problems. We illustrate the advantages 
with a successful industrial application: balancing an assembly line of robots placing chips on circuit boards.
5.1 
Background
It is generally recognized that combinatorial problems are difficult to solve with genetic algorithms (GAs). Why? 
Because representations that induce good schemata are hard to find [RS95]. The usual approaches are to either devise a 
special representation, hoping it will induce good schemata (most do not), or devise special operators designed to 
preserve and recombine schemata that the user 'knows' are important
Modern HeuristicSearch Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_85
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_85.html2009-7-7 16:56:44

page_86
< previous page
page_86
next page >
Page 86
for the problem at hand (e.g. subtours for the travelling salesman problem) (or both). Unfortunately, success has been 
limited to date, largely because there is yet no clear theory of schemata (or formae [Rad90] ) for combinatorial problems 
to guide these efforts.
There is an additional difficulty. Even if those working on finding ways to represent classical combinatorial problems 
for GAs were to succeed, the results would still be difficult to apply. The reason is that real world combinatorial 
problems are usually plagued by engineering details that are abstracted away in the well-studied versions of these 
problems (e.g. traveling salesman or bin packing).
Fortunately, there is an alternative approach that depends only on the existing body of theory and practical experience 
with GAs on parameter optimization problems and allows one to deal with all the necessary engineering messiness. 
That is, cast the problem as a parameter optimization one. The key to this approach is to devise an algorithm that 
produces solutions to the combinatorial problem (e.g. an arrangement of objects) that vary considerably when some 
parameters are varied. This step may be considered as a development process that produces a phenotype from a 
genotype. It seems to be easier to devise such an algorithm that induces good schemata in the genotype space than it is 
to find a more direct representation of the phenotypes that do so. One element of success in this approach seems to be to 
avoid the temptation to include in this algorithm very strong (optimum-seeking) heuristics. After all, if we possessed 
optimum-seeking heuristics that were strong enough, then the development algorithm would be a sufficient optimizer by 
itself.
We are not the only ones to employ this approach [KN90, Sys91], but we have come to believe it is an approach with 
wide practical applicability, given the large number of combinatorial problems of economic significance. We will 
illustrate the approach with a class of industrial problems: balancing an assembly line of robots that place surface mount 
devices (SMDs) on printed circuit boards (PCBs).
5.2 
The Line Balancing Problem
There is a well-studied problem called the 'line balancing' problem [FD92]. For SMD assembly, the problem can be 
stated as follows:
Given: A set P {pi, i = 1, 2, . . . n} of parts to be placed each with a time ti needed to pick and place it on a PCB, and a 
set M {mj, j = 1,2, . . ., m} of machines for picking and placing the parts,
The X matrix contains the assignments of parts to machines and the first constraint specifies that each part must be 
assigned to one and only one machine. The maximum machine time is often called the cycletime of the production line.
  
< previous page
page_86
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_86.html2009-7-7 16:56:45

page_87
< previous page
page_87
next page >
Page 87
The classical 'line balancing' problem usually also includes precedence constraints which specify that some parts must 
be placed before others, but this is usually not the case for SMD assembly problems. It can be shown that the line 
balancing problem is NP-complete since it contains the bin packing problem [GJ79] as a special case.
Figure 5.1 
A sketch of the FCM SMD assembly machine.
This problem has been tackled with GAs [AF90, FD92], but unfortunately, this formulation of the problem is not 
sufficiently close to the real problems one finds in the circuit assembly factories. For instance, a Philips SMD assembly 
machine, the fast component mounter (FCM), has 16 independent robot heads as illustrated in Figure 5.1. It looks like a 
line balancing problem until you realize that the PCBs are advanced on a transport system called a walking beam which 
must advance all PCBs in lock step. This forces synchronization of all robots at each index step of the transport system. 
Furthermore, the working area of each robot is smaller than most PCBs which means that only a portion of the PCB real 
estate is accessible to each robot at any one time. Obviously, only parts in the reachable area can be placed. There is 
also a limited amount of space for the component feeders inducing a feeder packing problem on each robot. Add to this 
the need to allow the assembly line managers to impose constraints on the solutions, because of limited supplied of the 
needed components, and the problem takes on additional complexity. A further requirement is that optimizers solve 
what is called the family of boards problem. In this problem one is given the
  
< previous page
page_87
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_87.html2009-7-7 16:56:45

page_88
< previous page
page_88
next page >
Page 88
specifications for a group of printed circuit boards (PCBs) and the goal is to find a solution that minimizes the group's 
assembly time while requiring no alterations in the robots' setups (no changeovers).
So the real optimization task is as follows:
Given: a PCB (or family of PCBs) and a production line specification
Subject to: user-specified constraints
Find: a complete setup and action specification
To Minimize: the assembly line cycle time.
A complete setup means an assignment of part feeders to the robots' feeder slots, and part gripping and alignment 
tooling to each robot head. A feasible setup must provide a combination of tooling that covers the required parts (a set 
covering problem, also NP-hard). This problem arises because of the wide variety of tooling available, each covering a 
different subset of part types. But the covering constraint is only a feasibility constraint. The numbers of robots assigned 
to each tooling type in order to give a well-balanced solution is not known.
The assignment of part feeders to feeder slots involves another constraint satisfaction problem because feeders vary in 
size as do the parts they feed and feeder slots are often at prespecified locations. Again, the minimum number of feeders 
required for a feasible solution is known (at least one feeder must be provided for each part type), but the optimum 
number for a well-balanced solution is not known.
An action specification is the detailed specification for every robot action needed to pick and place all the parts assigned 
to it. This may be as simple as specifying the sequence of pick and place locations, but may also involve specifying a 
route (a TSP problem).
5.3 
An Inadequate Approach
One approach that has been applied to this problem is the well-established divide-and-conquer approach. Since the 
problem consists of a small number of identifiable subtasks, solve them one by one. To do this, an algorithm is 
developed to solve the tooling assignment task to optimality, then the feeder assignments are solved and finally the 
action specification is produced. The difficulty that must be overcome (besides the NP-hard nature of these tasks) to 
make this approach work is to define performance measures for the subtasks that can be computed in isolation. This is 
where this approach fails; it is usually not possible to evaluate, say a feeder assignment, without knowing which parts 
are picked and placed from each one. The true cost of any complete setup can easily be computed, given an accurate 
timing model of each robot which is easy to build, but the worth of any part of a setup cannot be accurately computed in 
isolation. Nevertheless, algorithms have been developed using
  
< previous page
page_88
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_88.html2009-7-7 16:56:46

page_89
< previous page
page_89
next page >
Page 89
this approach and proxy measures, but in our experience, they cannot compete with the approach we describe next.
5.4 
A GA Approach
Figure 5.2 
A schematic of the solution strategy.
As described above, this problem can be cast as a parameter optimization problem by developing a domain-specific 
solution-generator to be run with chromosome-supplied parameters. For this domain we call this algorithm a heuristic 
layout
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_89.html（第 1／2 页）2009-7-7 16:56:46

page_89
  
< previous page
page_89
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_89.html（第 2／2 页）2009-7-7 16:56:46

page_90
< previous page
page_90
next page >
Page 90
generator (HLG). The concept is shown schematically in Figure 5.2.
The HLG consists of a collection of decision-making routines, one for each of the main decisions needed and each using 
simple greedy heuristics with a proxy measure of desirability. The key point is that these greedy heuristics contain 
embedded parameters, the values for which are supplied by a GA. The concept is that by altering these parameters, the 
GA can induce the greedy heuristics to yield a wide variety of layouts even with the same proxy measures of 
desirability. As stated above, this proxy desirability cannot be depended upon to give optimum decisions, but this is not 
needed. At the end of each call to the HLG, a timing model is run giving an accurate cycle time that reflects the 
ensemble of all layout decisions. By allowing the GA to call the HLG with thousands of different parameter sets, it can 
find sets that give overall good performance. A schematic of what occurs inside the HLG is shown in Figure 5.3.
To illustrate, we will give some details for one version of an HLG for the FCM machine. The rest of this section 
necessarily contains much detail which pertains only to the specifics of optimizing FCM layouts and therefore may be 
of little interest to most readers. It is presented here by way of illustration of the general concept. The FCM HLG 
contains the following steps:
Step 1: assign grippers 
Step 2: assign feeder types 
Step 3: assign parts 
Step 4: timing model
Each of these steps makes irrevocable decisions about part of a production line layout for a given PCB.
The goal of this step is to make an assignment of machine tooling (a combination of a suction nozzle for picking up the 
parts and an alignment mechanism (PPU) for accurately centering the part on the nozzle) to each of the robot heads 
available. Among the task descriptors computed when the PCB is defined is an accommodation matrix as illustrated in 
Table 5.1.
Table 5.1 An example accommodation matrix
PPU: PIP
LPIP
MPIP
PIP
MPIP
LPIP
noz: F3
F3
F3
F2
F2
F2
GT_max: 16
16
8
16
8
16
pkt
p_types
n_parts
1210R
3
3 1
1
1
0
0
0
2010R
4
6 0
0
0
1
1
0
SOT23
2
3 1
1
1
0
0
0
1206R
39
68 1
1
1
0
0
0
1206C
9
53 1
1
1
0
0
0
0805R
8
16 0
0
0
1
1
1
0805C
4
6 0
0
0
1
1
1
 
SMD parts come in different package types which define the external dimensions of the plastic envelope that contains 
the circuit elements. The part handling equipment
  
< previous page
page_90
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_90.html2009-7-7 16:56:47

page_91
< previous page
page_91
next page >
Page 91
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_91.html（第 1／2 页）2009-7-7 16:56:47

page_91
Figure 5.3 
A schematic of the Heuristic Layout Generator.
  
< previous page
page_91
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_91.html（第 2／2 页）2009-7-7 16:56:47

page_92
< previous page
page_92
next page >
Page 92
is only concerned about the package type since this is what must be picked up by the nozzle and aligned by the PPU and 
then accurately placed. In the example, there are six different package types (1210R, 2010R, . . .) each with the 
specified number of different part types (specific circuit elements, say a resistor of a given magnitude) and the specified 
number of individual parts of that type. Table 5.1 lists the 6 gripper types (combination of PPU and nozzle) that might 
be used for this task. The body of the accommodation matrix shows which package type may be handled by each 
gripper type by the presence of a ''1".
There is an algorithm that examines the accommodation matrix and identifies partitions which allow the breakdown of 
the entire task into subtasks. A partition, called a phantom gripper, is a set of grippers which handles a separable group 
of package types. Table 5.1 shows two phantoms, one comprising package types 2010R, 0805R and 0805C covered by 
grippers PIP-F2, MPIP-f2 and LPIP-F2, and the other comprising the remaining package types and grippers. Some 
problems cannot be partitioned and so are 1-phantom problems. Notice that within a phantom, some grippers may be 
completely interchangeable (they cover the same package types) while others are not. For the problem illustrated in 
Table 5.1, there are 16 robot heads available and most of the grippers are available in unlimited numbers. However, 
only 8 of the PPU type 'MPIP' are available so the GT_max for grippers using this type are similarly limited.
The desirability calculus used in this step depends on two numbers computed from the accommodation matrix for each 
phantom. An estimate of the minimum number of robot heads (hmin) is needed to accommodate at least one feeder for 
each of the part types it covers. This figure is necessarily an estimate since to know the exact number would require the 
solution of the problem of packing the feeders on the feeder bars of each robot head. The method used is to compute a 
lower bound for min-heads and then to increment it if experience shows it is too low. The other number precomputed is 
hideal. This too is an estimate and represents the number of heads that would result in a perfect balance of the pick-and-
place workload between the phantoms, if it could be achieved. Note that the min-heads is not necessarily less than the 
target-heads. It sometimes happens that a particular phantom has only a few parts to place (a small workload), but these 
parts are of many different types (each requiring a separate feeder) and they are large package types (requiring wide 
feeders that take up a lot of the available feeder bar space).
Armed with the minimum (hmin) and ideal (hideal) number of heads, the assign-grippers algorithm is as follows:
assign_grippers 
    while (there are heads left to assign) 
        compute desirability of each gripper 
        for most desirable gripper, 
            choose most desirable of the remaining heads 
        assign best gripper to best head 
    if a feasible assignment was found 
        return zero 
    else 
        return a measure of how close feasibility was approached
  
< previous page
page_92
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_92.html2009-7-7 16:56:48

page_93
< previous page
page_93
next page >
Page 93
The desirability of each gripper is a straightforward (if tedious) calculation based on head-shortfall. A shortfall is the 
difference between hmin (or hideal) and the number currently assigned. But since many grippers can cover the same 
package types, shortfalls must be computed for phantoms and package types and the worst value used for each gripper. 
The algorithm begins using shortfalls from the hminS, and only when these are all zero (indicating a feasible assignment 
has been achieved) does it begin using shortfalls from hidealS. The key aspect of this algorithm is that the desirability 
actually used is the product of the computed shortfalls and a chromosome-supplied gripper weight (a binary coded value 
between zero and one). Thus, some chromosomes may have such a poor set of weights that they cannot induce this 
algorithm to find a feasible gripper assignment. Others may find feasible assignments, but do so so inefficiently that 
only poorly balanced solutions result. However, a main point of our recommendation for this approach is that a GA is 
needed for this problem because a good gripper assignment cannot be assessed based only on the numbers of heads 
assigned. All the remaining steps of the HLG must be executed before the cycle time of the production line can be 
accurately computed.
Once heads have grippers assigned, feeders must be assigned to the feeder slots provided. There is an independent 
subproblem of this type for each phantom. The packing of feeders of different geometries into a given amount of space 
can be a difficult problem in general, but again we resort to a simple greedy algorithm, outlined below:
assign_feeders 
    for each phantom 
        while there are feeders with non-zero desirability 
            compute desirability of each feeder 
            for most desirable feeder, choose most desirable slot 
            assign best feeder to best remaining slot 
    if a feasible assignment was found 
        return zero 
    else 
        return a measure of how close feasibility was approached
Again, desirability is computed based on the precomputed minimum and maximum number of slots for each feeder, and 
again, these numbers are multiplied by chromosome-supplied feeder weights which allows the behavior of this simple 
algorithm to be varied producing a potentially wide variety of different feeder assignments for any given problem.
Perhaps the core of the HLG is this step. It works with the assignments made in the previous steps and assigns the 
individual parts to specific robot heads, either assigning a new feeder (to the part type needed) or reusing an existing 
feeder (if one is already available supplying the needed part types). There are chromosome-supplied parameters which 
dictate the order in which parts are sorted for placement. The desirability in this step is based on each head's slack time, 
i.e. the difference between its current pp_workload (the sum of the pp-time for the parts currently assigned to it) and the 
line's heartbeat time (i.e. the pp-time of the slowest head). There are also other chromosome-supplied parameters called 
bucket_choice_bits, one for each part. These dictate whether slack time alone should determine desirability or whether 
the choice should be restricted to those heads which would not require giving away a free
  
< previous page
page_93
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_93.html2009-7-7 16:56:48

page_94
< previous page
page_94
next page >
Page 94
feeder. Thus, the algorithm can be made to behave as a complex mixture of assign-to-minimize-slack and assign-to-
conserve-feeders under chromosome control.
This is a straightforward calculation of the time it takes and FCM to execute the task as described in the layout resulting 
from the previous steps. The output from this routine is treated as the fitness of the chromosome.
5.5 
How Well Does it Work?
This domain is one where small improvements in the cycle time of a production line can be very valuable and also 
where fairly long computation times (say overnight) are tolerable. Fortunately, the algorithm described has the property 
of being quite strongly guided towards feasible solutions, so it usually reports a feasible solution quickly and, when 
given more time, reports steadily improving solutions. The user need only wait until a solution of acceptable quality 
appears. By way of estimating the value of the solutions provided by this algorithm we point out that a 40 hour week 
contains 144 000 seconds. If a manufacturer plans a production run of 144 000 PCBs (not an overly large number for 
major electronics or automotive producers), then a one second smaller cycle time layout can save the cost of a 40 hour 
shift.
With these figures in mind we illustrate the performance of this algorithm in Figures 5.4 and 5.5 for two different PCBs. 
Each figure shows several replications of the GA/HLG each initiated with different random seeds. The performance 
measure is called the optimality gap and is the difference between the current best cycle time and the estimated lower 
bound cycle time (i.e. perfect balance). The GA used is Eshelman's CHC [Esh91], Note that there is some variation in 
the best solution achievable from PCB to PCB. For instance, there is quite a small gap for PCB 1, while PCB 2 has a 
large one. PCB 2 is a particularly challenging test case because there is very limited scope available for balancing. 
There are so many different part types that only one feeder slot can be allocated to each one and the challenge is mainly 
to find any feasible packing of the feeders. Once this is achieved, the line is limited to the slowest robot which in turn is 
the one with the most instances of its most populous part type. However, there are still many permutations among the 
one-slot-per-part-type solutions.
  
< previous page
page_94
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_94.html2009-7-7 16:56:48

page_95
< previous page
page_95
next page >
Page 95
Figure 5.4 
Six replicated learning curves for PCB 1.
Figure 5.5 
Six replicated learning curves for PCB 2.
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_95.html（第 1／2 页）2009-7-7 16:56:49

page_95
< previous page
page_95
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_95.html（第 2／2 页）2009-7-7 16:56:49

page_96
< previous page
page_96
next page >
Page 96
A version of this software has been supplied with Philips FCM machines for about two years and reports from the field 
have been positive. Efforts to improve the solutions provided by this algorithm by both human experts and other 
computer programs have consistently failed. Note that no tuning of the GA is needed to handle different PCBs, or even 
when going from single PCBs to families.
We should point out that the approach advocated is not limited to a genetic algorithm. In principle, any powerful search 
method could be coupled to the HLG with good results. However, our own efforts in this direction have convinced us 
that simulated annealing is unacceptably slow and a variety of hillclimbers are either hopelessly slow or they become 
trapped by local optima quite far from the best solutions.
5.6 
Conclusions
The balancing of production lines of real robots is known to be a difficult problem. This is so for many reasons 
including the significant differences between the real-world situations and the idealized versions studied academically. 
Their search spaces are very large (10100 is not uncommon) and complex (including several NP-hard subproblems). 
They usually do not readily admit to a divide-and-conquer strategy. The success we have described of a GA solving 
problems of this class might be taken as an illustration of the general power of GAs for such problems, but there is a 
more important lesson we wish to stress. The approach that allowed a combinatorial problem to be solved by the GA 
was the development of the HLG; a procedure analogous to the development of a phenotype from the genotype. This 
procedure, comprising fairly simple greedy solution-generating algorithms with embedded behavior-modifying genes, is 
not a optimization algorithm by itself; it needs the GA to tune it for each problem it tries to solve. What it does do is 
allow the established power of GAs for solving fixed parameter optimization problems to be applied to combinatorial 
problems. We are optimistic that this approach can be replicated on other combinatorial problems which have heretofore 
resisted the GA.
References
[AF90] Anderson E. and Ferris M. (1990) A genetic algorithm for the assembly line balancing problem. In Proceedings 
of the Integer Programming/Combinatorial Optimization Conference. University of Waterloo Press, Waterloo, Ontario, 
Canada.
[Esh91] Eshelman L. (1991) The chc adaptive search algorithm: How to have safe search when engaging in 
nontraditional genetic recombination. In Rawlins G. (ed) Foundations of Genetic Algorithms, pages 265283. Morgan 
Kaufmann, San Mateo, CA.
[FD92] Falkenauer E. and Delchambre A. (1992) A genetic algorithm for bin packing and line balancing. In Proc. IEEE 
1992 Int'l Conference on Robotics and Automation (RA92). Nice, France.
[GJ79] Garey M. and Johnson D. (1979) Computers and Intractability: A Guide to the Theory of NP-Completeness. 
Freeman, New York.
[KN90] Kadaba N. and Nygard K. (1990) Improving the performance of genetic
  
< previous page
page_96
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_96.html2009-7-7 16:56:49

page_97
< previous page
page_97
next page >
Page 97
algorithms in automated discovery of parameters. Technical report, Dept. of SC and OR, North Dakota State 
University.
[Rad90] Radcliffe N. (1990) Genetic Neural Networks on MIMD Computers. PhD thesis, Dept. of Theoretical Physics, 
University of Edinburgh, Edinburgh, Scotland.
[RS95] Radcliffe N. and Surry P. (1995) Fitness variance of formae and performance prediction. In Whitley D. and 
Vose M. (eds) Foundations of Genetic Algorithms 3, pages 5172. San Mateo, CA.
[Sys91] Syswerda G. (1991) Schedule optimization using genetic algorithms. In Davis L. (ed) Handbook of Genetic 
Algorithms, pages 332349. Van Nostrand Reinhold, New York.
  
< previous page
page_97
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_97.html2009-7-7 16:56:49

page_99
< previous page
page_99
next page >
Page 99
6 
Integrating Local Search into Genetic Algorithms
Colin R Reeves and Christian Höhn
Abstract
Genetic algorithms (GAs) have proved to be a versatile and effective approach for solving combinatorial optimization 
problems. Nevertheless, there are many situations in which the simple GA does not perform particularly well, and 
various methods of hybridization have been proposed. These often involve incorporating other methods such as 
simulated annealing or local optimization as a post-processor to the basic GA strategy of selection and reproduction.
In this paper, rather than keeping the local optimization separate from recombination, we intend to explore the 
possibilities of integrating it more directly into the standard genetic algorithm. The graph partitioning problem and the 
no-wait flowshop sequencing problem will be used to illustrate the approach.
6.1 
Introduction
Genetic algorithms (GAs) as developed by Holland[Ho175] have frequently been applied to combinatorial optimization 
problems, such as bin-packing (Falkenauer and Delchambre[FD92], Reeves[Ree93], machine-sequencing (Reeves
[Ree95b]), vehicle routing (Blanton and Wainwright[BW93]), and of course the travelling salesman problem (Jog et al.
[JSG91], Whitley et al.[WSS91], Homaifar et al.[HGL93]). All of these problems have serious practical applications, 
but they typically require some fairly major adjustments to the 'classical' GA paradigm in order to find high-quality 
solutions. In what follows, we assume the reader has a basic understanding of the way in which GAs operateif not, we 
refer them to Goldberg[Go189] or Reeves[Ree93a] for an introduction. A discussion of GAs in the particular context of 
combinatorial optimization can be found in Reeves[Ree95b].
The successful application of GAs requires a good adaptation to the problem to be solved. A promising approach is 
when existing methods such as constructive heuristics or local search methods can be incorporated into the algorithm. 
This idea has been
Modern Heuristic Search Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_99
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_99.html2009-7-7 16:56:50

page_100
< previous page
page_100
next page >
Page 100
used by Jog et al.[JSG91] (amongst others) in the context of the TSP. Other examples of this type of approach are in 
Mühlenbein[Müh91], Ulder et al.[UAB+91] and Prinetto et al.[PRR93]. An alternative strategy is to run the GA for a 
fixed number of cycles and then apply neighbourhood search to all the final population. However, in all these cases, the 
application of local optimization is kept distinct from the recombination operation.
However, recent work (Reeves[Ree94]; Culberson[Cul93]) which has started to explore GAs from a more traditional 
neighbourhood search perspective has suggested that local search methods can be integrated in a more fundamental 
way. It is assumed that readers will have at least a basic understanding of both GAs and neighbourhood search (NS). 
Further details can be found in Reeves[Ree94], but in the next section we briefly discuss the relation between the two 
approaches.
6.2 
GAs and NS
Traditional genetic algorithms have three main characteristics by which they are commonly distinguished from classical 
and modern local search techniques.
Population-based selection: In point-based NS methods selection is restricted to a choice between the parent and one or 
more offspring, but with GAs the presence of a population gives a spatial dimension to selection.
Recombination: By recombining two parent solutions, offspring usually inherit those variables for which both parents 
share the same value. Thus recombination essentially results in a reduction of the size of the search space.
Crossover: Traditionally crossover involves two (often binary) strings. Having defined a set of crossover points, the 
intermediate bits are swapped.
While selection and recombination represent general search techniques, crossover essentially stands for a special class 
of operators and thus could be replaced by an abstract type of operator. In the context of local search an operator w 
defines a neighbourhood N(x,w) ⊆ S for all points x in the search space S. This general definition holds for a 1-bit 
Hamming hill climber as well as for many more sophisticated heuristics. However, crossover as introduced by genetic 
algorithms seems to be an exception in so far as two points, say x and y, are involved in the operation such that N = N(x,
y,w), x, y ∈ S. Some implications of this are explored in Reeves[Ree94], but it is obvious that defining a neighbourhood 
by more than one point complicates the analysis. Furthermore, in this way the crossover operation is inevitably linked to 
recombination leading to a superposition of both effects. As a consequence the terms are often used synonymously.
However, having two points effectively reduces the neighbourhood size by the total of all elements the two points have 
in common. This is an effect of recombination which occurs before any operators are applied. Following the ideas of 
Culberson[Cul93] the crossover effect can then be implemented by a unary operator which always pairs a point x ∈ S 
with its complementary point 
 in the reduced neighbourhood.
  
< previous page
page_100
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_100.html2009-7-7 16:56:50

page_101
< previous page
page_101
next page >
Page 101
This operator, which we term complementary crossover, thus fits into the general definition above. As a consequence, 
the traditional crossover may be decomposed into neighbourhood reduction and complementary crossover.
In traditional GAs the search space defined by both parents is usually explored by evaluating only one or two randomly 
selected offspring. However, this is not a necessary part of a GAsome have suggested, for example, evaluating several 
randomly selected offspring and picking the best. In fact, one could take this to its logical conclusion and carry out a 
local optimization within the reduced neighbourhood. The types of problem where this approach is likely to be most 
effective are those where the objective function of the neighbouring points can be evaluated quickly and cheaply: if 
every point has to be evaluated from scratch, this is not likely to be a computationally efficient scheme.
6.3 
No-Wait Flowshop Problem
We have tried this out on some combinatorial problems. Initially, the problem studied was the n/m/P, nowait/Cmax 
problem (i.e. there are n jobs, m machines and the objective is to minimize the makespan), which can be modelled as an 
asymmetric TSP. This has the advantage that the cost of a small change (here an exchange of two job positions) is easily 
calculated, so it meets the criterion described above. Nine sets of 10 problem instances were investigated, with either 
20, 50 or 100 jobs, and 5, 10 or 20 machines. The neighbourhood search was carried out by a simple exchange operator 
as indicated above, and this was compared with the C1 and PMX crossovers'traditional' GA crossover operators for 
sequence-based problems. The total amount of computation was fixed at that used in Reeves[Ree95b]. The results were 
as shown in Table 6.1 (the results of G1 and PMX were so similar that only C1 is shown). The figures shown are the 
mean and standard deviation of the difference in makespan values; a positive difference means C1 gave better results, a 
negative one that the embedded NS operator was superior.
Table 6.1 Mean and standard deviation of 
differences in makespan (C1-NS), averaged over 
10 instances in each group
Problem group
mean diff
stdev diff
n/m
20/5
8.50
19.40
20/10
5.60
32.07
20/20
79.90
85.00
50/5
9.0
44.0
50/10
63.7
59.3
50/20
94.7
132.8
100/5
45.9
69.1
100/10
107.5
145.5
100/20
284.6
154.5
 
  
< previous page
page_101
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_101.html2009-7-7 16:56:51

page_102
< previous page
page_102
next page >
Page 102
It can be seen that for the smallest problem instances (20/5, 20/10 and 50/5) there was little difference between the 
methods. However, for the larger cases, the average makespan obtained from the embedded NS was (statistically) 
significantly less than those obtained by C1 or PMX.
What is not shown here is the relative amounts of computer time consumed; each technique was allotted what was 
nominally the same fixed amount of timea function of n and m (for further details see Reeves[Ree95b]. However, 
because the test for exceeding the stopping time was only carried out after each new string was generated, which is 
more frequent for the conventional GA with C1 or PMX than for the embedded NS approach, the results recorded for 
embedded NS were actually those on the iteration before the stopping criterion was reached. Thus, on average the time 
taken to achieve the results of Table 6.1 were 5 to 10% less for the embedded NS approach, so that by integrating NS 
into the GA we have not only found higher-quality solutions, but also achieved them in less computer time.
6.4 
Graph Partitioning
What the above results do not show is how much of the performance is due to the operators used, and how much to the 
effect of recombination. This is not easy to disentangle in the context of the flowshop problem, and for a more 
fundamental study, we looked at the rather simpler problem of bipartitioning a graph which, as it has a natural binary 
encoding, is more amenable to analysis. A fuller discussion of this work is contained in Höhn and Reeves[HR94].
The uniform bipartitioning of a graph is a special case (i.e. k = 2) of the k-way graph partitioning problem which is as 
follows: given a graph G = (V, E) with a cost function 
 and a size S(u) attached to each node, 
partition the nodes of G into k subsets of equal size such that the total cost of the edges cut is minimized. The uniform k-
way partitioning problem is a constrained optimization problem, i.e. the search space usually contains a non-empty set 
of illegal solutions. This can pose severe problems for GAs (see Reeves[Ree95b], for a dicussion of these), but happily 
in this case, the problem can be quite simply transformed into a free optimization problem as in Lee et al.[LPK89] and 
Tao and Zhao[TZ93].
For the purposes of this investigation we define the following abstract GA:
1. Initialize a population of size N.
2. Apply the local hill climber to each individual until a local optimum is reached or another stopping criterion is met.
3. Choose 2N individuals from the population according to fitness and pair them (randomly) so that each pair defines a 
reduced neighbourhood. This is passed to the next generation together with the complete bitstring and objective value 
from one parent.
4. All the offspring define the population for the next generation.
5. If not TERMINATED go to 2, otherwise stop.
The scheme is not restricted to a particular hill-climbing technique and thus provides a high degree of flexibility. 
Furthermore, for a given application several local hill climbing techniques may be evaluated individually before being 
extended by recombination
  
< previous page
page_102
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_102.html2009-7-7 16:56:51

page_103
< previous page
page_103
next page >
Page 103
and selection. In the rest of the paper such an approach is illustrated for the graph bipartitioning problem. First we 
describe the operators.
6.4.1 
The LPK Heuristic
One of the traditional operators for solving the k-way graph partitioning problem is the heuristic by Lee et al. [LPK89] 
which is based on the heuristic of Kernighan and Lin[KL70]. Given an initial partition, π = (A, B), consisting of the 
blocks A and B, a pass starts with moving the vertex having the highest gain to its complementary block. Once moved, 
the vertex becomes locked for the remainder of the pass and the gains of the remaining vertices are updated. The pass 
proceeds with the next best move and terminates after all vertices have been moved, i.e. when the initial partition is 
reached again. Then the best partition encountered during the pass is taken as initial partition for the next pass. The 
algorithm stops when no further improvement can be achieved, i.e at a local optimum.
The LPK-heuristic uses a steepest ascent strategy where the neighbourhood searched forms a closed path. For problems 
involving l vertices the path is of length l starting and ending at the initial point and always follows the maximum gain. 
Initializing and maintaining the gains require O(l2) time per pass.
6.4.5 
Complementary Crossover
Using group-number encoding a solution for the bipartitioning problem of l vertices requires a binary string of length l. 
Usually when crossover is applied to partitioning problems two common pitfalls arise:
The operator is not closed with respect to the search space. Thus illegal solutions may result.
The coding is redundant, multiplying the number of local and global optima.
Because of the graph transformation the solution space is extended to the whole search space, so that no illegal solutions 
occur. Furthermore, since in this study the investigations are restricted to the bipartitioning problem any influence of 
redundancy can be eliminated fairly easily.
1-Point Complementary Crossover (1CX)
The implementation of the 1CX is based on the following definition:
Unlike Culberson[Cul93], we allow a crosspoint to occur in front of the first bit resulting in an inversion of the complete 
bit string. 1CX produces a neighbourhood structure similar to the LPK-heuristic. It forms a closed path of length l 
starting and ending with the initial partition. Unlike the LPK-heuristic the path is determined by the
  
< previous page
page_103
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_103.html2009-7-7 16:56:52

page_104
< previous page
page_104
next page >
Page 104
order of the vertices. A steepest ascent strategy will follow all the path where each step requires the update of the 
associated gains. Thus the whole pass requires O(l2) time. Obviously, although the crossover does not process 
additional information the time required is the same magnitude as that of LPK heuristic, clearly an advantage for the 
more sophisticated heuristic.
2-Point Complementary Crossover (2CX)
The 2CX is defined as follows:
Defined in this fashion 2CX resembles the 1-bit Hamming hill climber (if j = i + 1) as well as 1CX (if j = l + 1). Since 
there are l + 1 feasible sides for a cross point the neighbourhood contains 
 points. Thus the size of the 
neighbourhood is l(l+1)/2, i.e bounded by O(l2). Consequently, for larger problems an exhaustive search is out of the 
question. Instead a next ascent search is performed with an upper limit of tmax = l trials per pass. Since the complete 
evaluation of the objective value involves l(l1)/2 operation the time bound is given by O(l2) for a single operation and O
(l3) for a complete pass.
6.4.3 
1-Bit Hamming Hill Climber (HC)
The 1-bit Hamming hill climber can be defined as follows:
Obviously, applying a 1-bit Hamming hill climber combined with a steepest ascent search strategy to the graph 
bipartitioning problem corresponds to a strategy that always moves the vertex of the highest gain. Since only a single 
vertex is moved it is convenient to maintain the gains and update the solution after each move which requires O(l) time. 
Since spotting the best vertex can be done in linear time too the computational complexity of a steepest ascent 
Hamming hill climber is bound by O(l). The search terminates after a local optimum has been reached.
6.4.4 
Mutation (MUT)
Mutation traditionally flips a single bit with a given probability p. Thus
The effect of mutation corresponds to moving several vertices at a time. Thus the computational complexity for a single 
operation will range between O(l) for a low
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_104.html（第 1／2 页）2009-7-7 16:56:52

page_104
< previous page
page_104
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_104.html（第 2／2 页）2009-7-7 16:56:52

page_105
< previous page
page_105
next page >
Page 105
mutation rate and O(l2) for a high mutation rate. If only a single vertex is moved the move can be suspended if the 
associated gain is nonpositive. In the following experiments the mutation rate has been set to p = 1/l. The search 
terminates after the maximum number of evaluations per pass, tmax = l, has been exceeded. Thus the time required for 
one pass is bounded by O(l3).
Table 6.2 Certain characteristic of the operators considered. SA and 
NA stand for steepest and next ascent.
Op
N'hood
Time
Strategy
Termination
LPK
l
O(l2)
SA
local optimum
1CX
l
O(l2)
SA
local optimum
2CX
l2
O(13)
NA
< l trials/pass
HC
l
O(l)
SA
local optimum
MUT
2l
O(13)
NA
< l trials/pass
 
6.5 
Evaluation of the Operators
In the first step 1-point and 2-point complementary crossover, the 1-bit Hamming hill climber and mutation are 
compared against the LPK-heuristic. Therefore, they are implemented in a simple local hill climbing template that uses 
both next ascent and steepest ascent respectively and terminates when a local optimum has been reached or the 
maximum number of trials per pass has been exceeded.
For evaluation a set of random graphs has been generated. All graphs have uniform weights at the edges and at the 
vertices. The parameters are summarized in Table 6.3.
Table 6.3 Parameter of the graphs
Acronym
R.50.5
R.50.10
R.100.5
R.100.10
Vertices
50
50
100
100
Mean degree
5
10
5
10
Edges
124
252
244
472
Max degree
10
18
11
16
Min degree
2
5
0
3
 
The results of fifty runs are displayed in Tables 6.4 to 6.7. Besides the standard measures (best result, worst result, 
average and standard deviation) the average number of moves per pass has been measured as well as the time required. 
While the first gives a rough idea of the distribution of local optima the latter highlights the computational complexity. 
The results can be summarized as follows:
The LPK-heuristic performs best in terms of accuracy and requires a
  
< previous page
page_105
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_105.html2009-7-7 16:56:53

page_106
< previous page
page_106
next page >
Page 106
Table 6.4 R.50.5
Operator
max
min
average
deviation
min cut
average moves
time in s
LPK
572
562
569
3
26
2.280
2.0
1CX
532
488
511
8
46
1.720
1.0
2CX
530
498
515
8
47
6.340
8.0
MUT
552
498
531
13
36
13.060
7.0
HC
544
482
515
13
40
2.920
0.0
 
Table 6.5 R.100.5
Operator
max
min
average
deviation
min cut
average moves
time in s
LPK
1142
1124
1137
5
51
2.500
7.0
1CX
1058
972
1019
14
93
1.820
2.0
2CX
1060
1002
1029
12
92
7.540
55.0
MUT
1116
1040
1081
16
64
32.460
46.0
HC
1082
972
1030
26
81
4.500
1.0
 
Table 6.6 R.50.10
Operator
max
min
average
deviation
min cut
average moves
time in s
LPK
600
572
597
5
76
2.220
1.0
1CX
540
478
512
13
106
1.980
0.0
1CX
542
498
518
11
105
6.020
7.0
MUT
580
490
539
18
86
12.360
6.0
HC
558
478
519
17
97
2.920
0.0
 
Table 6.7 R.100.10
Operator
max
min
average
deviation
min cut
average moves
time in s
LPK
1208
1176
1200
8
132
2.820
6.0
1CX
1074
972
1025
20
199
2.060
4.0
2CX
1080
1012
1048
17
196
7.220
57.0
MUT
1164
1064
1114
23
154
32.260
58.0
HC
1128
966
1038
39
172
4.500
1.0
 
  
< previous page
page_106
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_106.html2009-7-7 16:56:54

page_107
< previous page
page_107
next page >
Page 107
reasonable amount of time.
1-bit Hamming hill climbing is the fastest operator and achieves a similar performance to 1CX and 2CX.
Mutation shows the second best performance after the LPK. However, it suffers from high computational costs mainly 
caused by the evaluation procedure.
2CX requires the most computational time which, however, is not justified by the performance achieved.
To sum up, operators have to be judged according to the search space they induced (number of local optima, speed of 
convergence) and the degree up to which they support an efficient evaluation of the objective function. Concerning 
graph bipartitioning the LPK-heuristic exhibits an efficient tradeoff between both measures.
6.6 
Recombination
Recombining two binary strings of length l results in a search space Sd = {0,1}d, 0 ≤ d ≤ l where d is the Hamming 
distance between both strings. Since the time required for performing an operation on a bit string is usually a 
(polynomial) function of l a reduced search space will speed up the operator and thus save computational time. This 
effect has been described elsewhere (Reeves[Ree94]; Höhn[Höh94]).
However, for some operators recombination exhibits an even more interesting phenomenon which is now considered in 
more detail. Consider two bit strings, say x, y, both of length l. Applying a 1-bit Hamming hill climber the 
neighbourhood of both points contains l neighbours. Assuming that the Hamming distance between x and y is 0 < d < l, 
recombination of x and y generates a neighbourhood N(x, y, HC) containing d points. Obviously, N(x,y,HC) ⊆ N(x,HC) 
and N(x,y,HC) ⊆ N(y,HC). In contrast, as an easy example reveals (Figure 6.1), the last relation does not necessarily 
apply for 1-point complementary crossover. Here recombination is able to introduce new points into the reduced 
neighbourhood and thus it may help to escape out of a local optimum. The same observation may be made for the 2-
point complementary crossover or the LPK-heuristic.
To investigate both effects of recombination a population of 50 hill climbers was implemented in the scheme of the 
abstract GA. Tables 6.8 to 6.11 display the results for a typical run of five generations. Convergence and diversity of the 
population can be measured by the size of the search space averaged over the population and the deviation in the cost 
values. Furthermore, the average number of moves is given which is cumulative over the generation. Thus it can be 
considered as a measure for how efficient the local optima encountered at the end of one generation are circumvented in 
the next generation.
  
< previous page
page_107
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_107.html2009-7-7 16:56:55

page_108
< previous page
page_108
next page >
Page 108
Figure 6.1 
Full and reduced neighbourhood of some bit 
string when 1CX is applied. In the lower picture the 
second bit from the right is fixed due to 
recombination. Obviously, the reduced neighbourhood 
contains two points which are not included in the full 
neighbourhood.
  
< previous page
page_108
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_108.html2009-7-7 16:56:55

page_109
< previous page
page_109
next page >
Page 109
Table 6.8 Typical run of the GA when applied to R.50.5
Operator
max
min
average
deviation
min cut
average moves
average size
time in s
LPK
572
562
569
3
26
2.280
50.00
1.0
572
566
571
1
26
2.660
12.96
2.0
572
572
572
0
26
2.860
5.72
2.0
572
572
572
0
26
2.860
1.92
3.0
572
572
572
0
26
2.860
2.08
3.0
1CX
532
488
511
8
46
1.720
50.00
0.0
536
510
520
6
44
2.460
21.60
0.0
538
520
529
5
43
3.160
21.44
1.0
544
526
535
4
40
3.620
15.36
1.0
550
536
539
3
37
4.100
11.12
1.0
2CX
530
498
515
8
47
6.340
50.00
9.0
538
512
524
6
43
7.540
22.12
12.0
544
518
530
6
40
8.680
19.84
14.0
548
526
535
5
38
9.500
20.44
16.0
548
530
538
5
38
10.040
18.12
18.0
MUT
552
498
531
13
36
13.060
50.00
6.0
552
514
539
10
36
14.380
20.04
7.0
556
532
547
6
34
15.340
20.40
8.0
558
534
551
5
33
15.860
18.72
8.0
558
544
553
3
33
16.540
17.64
9.0
HC
544
482
515
13
40
2.920
50.00
1.0
544
494
522
12
40
2.920
21.32
1.0
544
512
527
9
40
2.920
21.28
1.0
544
518
533
8
40
2.920
19.92
1.0
544
522
537
7
40
2.920
18.28
2.0
 
  
< previous page
page_109
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_109.html2009-7-7 16:56:56

page_110
< previous page
page_110
next page >
Page 110
Table 6.9 Typical run of the GA when applied to R.100.5
Operator
max
min
average
deviation
min cut
average moves
average size
time in s
LPK
1142
1124
1137
5
51
2.500
100.00
7.0
1142
1132
1140
2
51
2.920
34.96
8.0
1142
1140
1141
1
51
3.120
29.56
8.0
1142
1142
1142
0
51
3.120
31.16
9.0
1142
1142
1142
0
51
3.120
26.48
10.0
1CX
1058
972
1019
14
93
1.820
100.00
3.0
1064
1014
1035
14
90
2.700
46.00
3.0
1064
1030
1052
9
90
3.300
45.64
4.0
1070
1048
1059
5
87
3.900
30.92
7.0
1070
1056
1064
5
87
4.240
26.44
8.0
2CX
1060
1002
1029
12
92
7.540
100.00
59.0
1064
1020
1042
11
90
9.120
46.32
72.0
1074
1030
1054
10
85
10.680
43.16
86.0
1078
1040
1061
9
83
11.320
43.12
98.0
1092
1050
1067
7
76
12.320
35.60
112.0
MUT
1116
1040
1081
16
64
32.460
100.00
38.0
1116
1068
1092
11
64
34.700
44.52
41.0
1118
1082
1100
8
63
35.980
41.84
43.0
1120
1090
1106
8
62
37.200
38.40
45.0
1122
1094
1110
7
61
38.380
31.96
47.0
HC
1082
972
1030
26
81
4.500
100.00
1.0
1082
994
1041
24
81
4.500
45.40
1.0
1082
998
1053
20
81
4.500
44.60
2.0
1082
1024
1058
18
81
4.500
39.36
3.0
1082
1036
1066
15
81
4.500
41.08
3.0
 
For the given set of test problems the following observations may be made.
Owing to neighbourhood size reduction all the operators are speeded up. Thus five generations require less than double 
the time of the first generation.
As predicted, recombination does not have any effect on the 1-bit Hamming hill climber. The increasing average 
performance is merely due to selection.
Clearly, for 1CX and LPK, recombination leads the search away from the local optima found in the first generation. The 
increasing average number of moves indicates the progress.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_110.html（第 1／2 页）2009-7-7 16:56:58

page_110
For mutation and 2CX the local search stops after l trials have been failed. However, in the next generation the selected 
points will be evaluated again which essentially exceeds the stopping criterion and accounts for the progress of one move 
per generation. Obviously, this effect is an artefact of the selection scheme and cannot be attributed to
  
< previous page
page_110
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_110.html（第 2／2 页）2009-7-7 16:56:58

page_111
< previous page
page_111
next page >
Page 111
recombination. However, both operators benefit from the speed up.
Table 6.10 Typical run of the GA when applied to R.50.10
Operator
max
min
average
deviation
min cut
average moves
average size
time in s
LPK
600
572
597
5
76
2.220
50.00
1.0
600
598
600
0
76
2.460
8.64
1.0
600
600
600
0
76
2.460
3.20
1.0
600
600
600
0
76
2.460
2.72
2.0
600
600
600
0
76
2.460
3.32
2.0
ICX
540
478
512
13
106
1.980
50.00
0.0
546
508
526
9
103
2.980
21.28
0.0
556
522
538
7
98
3.640
18.44
1.0
556
534
546
6
98
4.300
15.40
1.0
566
542
554
5
93
5.040
14.32
1.0
2CX
542
498
518
11
105
6.020
50.00
5.0
548
516
532
9
102
7.340
22.08
7.0
572
520
540
10
90
8.120
20.28
9.0
588
528
553
12
82
9.220
18.24
10.0
586
540
562
9
83
10.020
16.76
11.0
MUT
580
490
539
18
86
12.360
50.00
5.0
580
518
550
15
86
13.300
20.68
6.0
592
540
561
13
80
14.180
20.04
6.0
592
548
569
10
80
14.900
17.12
7.0
594
552
576
10
79
15.300
16.12
7.0
HC
558
478
519
17
97
2.920
50.00
0.0
558
494
528
15
97
2.920
21.80
0.0
558
510
536
13
97
2.920
20.40
1.0
558
520
544
10
97
2.920
17.12
1.0
558
522
548
9
97
2.920
18.64
1.0
 
  
< previous page
page_111
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_111.html2009-7-7 16:56:59

page_112
< previous page
page_112
next page >
Page 112
Table 6.11 Typical run of the GA when applied to R.100.10
Operator
max
min
average
deviation
min cut
average moves
average size
time in s
LPK
1208
1176
1200
8
132
2.820
100.00
5.0
1208
1196
1206
3
132
3.340
28.52
5.0
1208
1204
1208
1
132
3.700
12.04
6.0
1208
1208
1208
0
132
3.780
7.96
6.0
1208
1208
1208
0
132
3.780
5.12
7.0
1CX
1074
972
1025
20
199
2.060
100.00
2.0
1074
1014
1048
17
199
2.920
46.28
3.0
1086
1044
1066
9
193
3.640
43.96
4.0
1096
1064
1075
6
188
4.320
41.52
4.0
1096
1074
1083
7
188
4.680
30.68
5.0
2CX
1080
1012
1048
17
196
7.220
100.00
43.0
1098
1026
1066
15
187
8.440
45.16
55.0
1114
1052
1080
14
179
9.520
41.96
66.0
1120
1072
1092
12
176
10.460
36.24
75.0
1120
1078
1101
12
176
11.200
38.28
84.0
MUT
1164
1064
1114
23
154
32.260
100.00
35.0
1164
1082
1128
17
154
34.200
41.56
38.0
1172
1106
1136
16
150
35.640
41.04
40.0
1174
1116
1146
14
149
36.920
39.60
41.0
1184
1126
1156
13
144
37.980
34.80
43.0
HC
1128
966
1038
39
172
4.500
100.00
1.0
1128
976
1062
34
172
4.500
45.28
2.0
1128
1010
1079
30
172
4.500
44.60
2.0
1128
1024
1092
23
172
4.500
41.80
3.0
1128
1044
1095
21
172
4.500
38.52
3.0
 
However, since the LPK-heuristic found the best values always within the first generation, it is not clear whether 
recombination is more effective than simply trying different initial points. To obtain a preliminary answer on this question 
a hill climber was compared to the GA when applied to a more ambitious random graph with 500 vertices and 1224 edges 
corresponding to a mean degree of 5. In 30 runs the CPU time has been fixed for both algorithms and the best values 
found per run are compared. The results are shown in Table 6.12. It appears that LPK coupled with recombination 
performs better than LPK alone. In particular, the worst value found in 30 runs with recombination exceeds the average 
performance of LPK on its own. The last column displays the effective population size, i.e. 80 restarts require the same 
amount of time as 50 individuals evolved for 6 generations.
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_112.html（第 1／2 页）2009-7-7 16:57:00

page_112
< previous page
page_112
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_112.html（第 2／2 页）2009-7-7 16:57:00

page_113
< previous page
page_113
next page >
Page 113
Table 6.12 The LPK heuristic applied alone and coupled with recombination
Operator
max
min
average deviation popsize
LPK
5778 5754
5762
6
80
GA + LPK
5788 5764
5774
5
50
 
6.7 
Conclusion
We have shown elsewhere (Reeves[Ree94]) that GAs are closer to NS than is sometimes believed, in that a GA works 
by first finding a reduced neighbourhood and then searching it in a random fashion. Adding local optimization as an 
extra 'operator' (as suggested by previous work) has been found to improve the GA's performance, albeit at a cost in 
terms of the computational requirements. However, integrating local optimization more firmly into the GA can help to 
maintain the benefits of recombination, and if implemented efficiently can speed up the time to reach a good solution.
In this paper an abstract genetic algorithn has been presented and tested on a special case of graph partitioning. The 
proposed GA couples a local hill climbing technique with recombination and selection in a general way, so that any 
local search technique may be accommodated within the proposed scheme. In this paper the investigation has focused 
on several operators and the effects of recombination. Initially the operators have been studied individually when 
implemented in the framework of a simple local hill climber. Such preliminary investigations may form a first step in 
the design of more sophisticated algorithms.
Secondly, we have separated the effect of the operator used from that of the recombinative effect of a GA, and shown 
that, at least for larger problem instances, recombination has been proved to save computational time as well as offering 
a mechanism to circumvent local optima for certain operators. Both effects provide a reasonable explanation why GAs 
can improve traditional hill climbing techniques. Furthermore, both characteristics may be used to guide the design of 
application-specific genetic algorithms.
However, the efficiency of recombination certainly depends on the population size as well as the selection scheme 
applied. Both problems deserve further investigation. In future work the performance of the GA will be investigated 
when more sophisticated hill climbers are incorporated.
References
[ARS93] Albrecht R. F., Reeves C. R. and Steele N. C. (eds) (1993) Proceedings of the International Conference on 
Artificial Neural Networks and Genetic Algorithms. Springer-Verlag, Vienna.
[BW93] Blanton Jr. J. and R.L. Wainwright (1993) Multiple vehicle routing with
  
< previous page
page_113
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_113.html2009-7-7 16:57:01

page_114
< previous page
page_114
next page >
Page 114
time and capacity constraints using genetic algorithms. In S. Forrest [For93].
[Cul93] Culberson J. C. (1993) Mutation-crossover isomorphisms and the construction of discriminating functions. 
Evolutionary Computation to appear.
[Dav91] Davis L. (ed) (1991) Handbook of Genetic Algorithms. Van Nostrand Reinhold, New York.
[For93] Forrest S. (ed) (1993) Proceedings of 5th International Conference on Genetic Algorithms. Morgan Kaufmann, 
San Mateo, CA.
[FD92] Falkenauer E. and Delchambre A. (1992) A genetic algorithm for bin packing and line balancing. In Proc. IEEE 
1992 Int'l Conference on Robotics and Automation (RA92). Nice, France.
[Go189] Goldberg D. E. (1989) Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley, 
Reading, Mass.
[Hol75] Holland J. (1975) Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann Arbor, MI.
[Höh94] Höhn C. (1994) Heuristic neighbourhood search operators for graph partitioning tasks. In Proc. 10th Int. Conf. 
on Systems Engineering, pages 469476. Coventry, UK.
[HGL93] Homaifar A., Guan S., and Liepins G.E. (1993) A new approach on the travelling salesman problem by 
genetic algorithms. In Forrest [For93].
[HR94] Höhn C. and Reeves C. R. (1994) Embedding local search operators in a genetic algorithm for graph 
partitioning. Technical report, School of Mathematics and Information Sciences, Coventry University, UK.
[JSG91] Jog P., Suh J. Y., and Gucht D. V. (1991) The effects of population size, heuristic crossover and local 
improvement on a genetic algorithm for the travelling salesman problem. In J. D. Schaffer [Sch89]
[KL70] Kernighan B. W. and Lin S. (1970) An efficient heuristic procedure for partitioning graphs. Bell System 
Technical Journal pages 291307.
[LPK89] Lee C-H., Park C-I., and Kim M. (1989) Efficient algorithm for graph partitioning problem using a problem 
transformation method. Computer Aided Design 21: 611618.
[Müh91] Mühlenbein H. (1991) Evolution in time and spacethe parallel genetic algorithm. In G.J.E. Rawlins [Raw91].
[MM92] Männer R. and Manderick B. (eds) (1992) Parallel Problem-Solving from Nature, 2. Elsevier Science 
Publishers, Amsterdam.
[PRR93] Prinetto P., Rebaudengo M., and Reorda M. (1993) Hybrid genetic algorithms for the travelling salesman 
problem. In R. F. Albrecht and N. C. Steele [ARS93].
[Raw91] G.J.E. Rawlins (ed) (1991) Foundations of Genetic Algorithms. Morgan Kaufmann, San Mateo, CA.
[Ree93] Reeves C. R. (1993) Hybrid genetic algorithms for bin-packing and related problems. Annals of OR to appear.
[Ree93a] Reeves C. R. (ed) (1993) Modern Heuristic Techniques for Combinatorial Problems. Blackwell Scientific 
Publications, Oxford.
[Ree94] Reeves C. R. (1994) Genetic algorithms and neighbourhood search. In Fogarty T. C. (ed) Evolutionary 
Computing: AISB Workshop, Leeds, UK, April 1994: Selec ted Papers, number 865 in Lecture Notes in Computer 
Science. Springer-Verlag, Berlin.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_114.html（第 1／2 页）2009-7-7 16:57:02

page_114
[Ree95a] Reeves C. R. (1995) A genetic algorithm for flowshop sequencing. Computers & Ops. Res. 22: 513.
[Ree95b] Reeves C. R. (1995) Genetic algorithms and combinatorial optimisation. In Rayward-Smith V. J. (ed) 
Applications of Modern Heuristic Methods, pages 111126. Alfred Waller, Henley-on-Thames, UK.
[Sch89] Schaffer J. D. (ed) (1989) Proceedings of 3rd International Conference on Genetic Algorithms. Morgan 
Kaufmann, Los Altos, CA.
[TZ93] Tao L. and Zhao Y. (1993) Multi-way graph partitioning by stochastic probe. Computers & Ops. Res. 20: 
321347.
  
< previous page
page_114
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_114.html（第 2／2 页）2009-7-7 16:57:02

page_115
< previous page
page_115
next page >
Page 115
[UAB+91] Ulder N. L. J., Aarts E. H. L., Bandelt H.-J., Laarhoven P. J. M., and Pesch E. (1991) Genetic local search 
algorithms for the travelling salesman problem. In H.-P.Schwefel and R.Männer (eds) Parallel Problem-Solving from 
Nature. Springer-Verlag, Berlin.
[WSS91] Whitley D., Starkweather T., and Shaner D. (1991) The traveling salesman and sequence scheduling: quality 
solutions using genetic edge recombination. In L.Davis [Dav91].
  
< previous page
page_115
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_115.html2009-7-7 16:57:02

page_117
< previous page
page_117
next page >
Page 117
7 
Local Search for Steiner Trees in Graphs
M.G.A. Verhoeven, M. E. M. Severens and E. H. L. Aarts
Abstract
We present a local search algorithm for the Steiner tree problem in graphs, which uses a neighbourhood in which paths 
in a Steiner tree are exchanged. The exchange function of this neighbourhood is based on a multiple-source shortest 
path algorithm. We present computational results for a known benchmark set of generated Steiner tree problem 
instances and for a set of instances derived from real-world travelling salesman problem instances, containing up to 
18512 vertices. The results show that good quality solutions can be obtained in moderate running times.
7.1 
Introduction
In the Steiner tree problem in graphs a minimum weight tree has to be found that includes a prespecified subset of 
vertices of a graph. The Steiner tree problem occurs in several practical applications, such as the design of telephone, 
pipeline, and transportation networks, and the design of integrated circuits. Although the Steiner tree problem is NP-
hard [HRW92], several sophisticated optimization algorithms for it exist, which are able to solve instances with up to 2 
500 vertices and 62 500 edges at the cost of substantial amounts of computation time, viz., several hours on a powerful 
workstation or supercomputer. In addition many heuristics have been proposed, most of which have running times that 
are polynomially bounded at the risk of finding sub-optimal solutions. Hwang, Richards and Winter[HRW92] present 
an overview of both optimization algorithms and heuristics.
In this paper we study local search approaches to the Steiner tree problem. Local search is a generally applicable 
approximation technique for hard combinatorial optimization problems, and the literature reveals that this approach can 
find good quality results for a wide variety of problems. For an overview of local search we refer to [Ree93, AL96]. For 
the Steiner tree problem local search has so far only been applied to relatively small instances containing up to 100 
vertices, requiring several minutes
Modern Heuristic Search Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_117
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_117.html2009-7-7 16:57:02

page_118
< previous page
page_118
next page >
Page 118
of running time. Such instances are well within the range of current optimization algorithms, using a smaller or equal 
amount of running time. So, up to now local search is not competitive with efficient optimization algorithms. In this 
paper we present a local search algorithm that is able to find solutions with a relative error of a few percent requiring 
moderate running times. Moreover, it is able to handle very large problem instances in an acceptable amount of time.
7.2 
Local Search
The use of local search presupposes the specification of an instance of a combinatorial optimization problem and a 
neighbourhood structure.
Definition 1
An instance of a combinatorial optimization problem is denoted as a pair (S, f), where the solution space S is the set of 
feasible solutions, and the cost function 
gives the cost of a solution. The problem, in the case of 
minimization, is to find a solution with minimal cost. A neighbourhood structure 
assigns to each 
solution a set of solutions. A solution s∈S is a local minimum of 
 if f(s′)≥f(s) for all 
.
A neighbourhood is weakly connected if it is possible to reach an optimal solution from any solution, and it is strongly 
connected if it is possible to reach each solution from any other solution.
The neighbours of a solution are not given explicitly, but can be constructed by a polynomially computable function. 
Most neighbourhoods are based on the replacement of a few elements that constitute a solution. For this we need the 
following definition in which we assume that solutions are represented by sets.
Definition 2
Let the set of elements constituting solutions in S be given by ε = 
, and let the exchange 
function 
be a partial function with 
. Then, an l-exchange neighbourhood structure 
 is 
defined by 
 for each s ∈ S.
Local search algorithms constitute a class of approximation algorithms that are based on continually improving a 
solution by searching its neighbourhood for a solution with lower cost. The simplest form of local search is the iterative 
improvement algorithm presented in Figure 7.1. There are many variants on this basic local search algorithm known in 
the literature; examples are simulated annealing, tabu search, and genetic local search. Simulated annealing and some 
forms of probabilistic tabu search converge to a global minimum provided that the neighbourhood is at least weakly 
connected.
7.2.1 
The Steiner Tree Problem in Graphs (SP)
Formally the Steiner tree problem in graphs (SP) is defined as follows.
Definition 3
Let G = (VG, EG) be an undirected graph, 
 a function that assigns a weight to each edge, and let X⊆ 
VG be a set of terminals. The problem is to find a subtree T = (VT,ET) of G with X⊆ VT for which the sum of the edge 
weights 
is minimal.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_118.html（第 1／2 页）2009-7-7 16:57:03

page_118
  
< previous page
page_118
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_118.html（第 2／2 页）2009-7-7 16:57:03

page_119
< previous page
page_119
next page >
Page 119
proc Iterative_Improvement (s : S)
var w : P(S);
begin
while 
 do
if f(s′) ≥ f(s) then w := w ∪ {s′}
else s := s′; 
 fi
od {s is a local minimum of 
}
end
 
Figure 7.1 
An iterative improvement algorithm.
Vertices in VG \ X are called non-terminals. Since no non-terminals with degree one are included in the optimal 
solution, the solution space S consists of all subtrees T of G with X ⊆ VT that contain no non-terminals with degree one. 
Such a tree is called a Steiner tree. Non-terminals in a Steiner tree T are called the Steiner vertices of T. Steiner vertices 
with a degree at least three are called key vertices. A key path is a path in a Steiner tree T of which all intermediate 
vertices are Steiner vertices with degree two in T and whose end vertices are terminals or key vertices. It follows 
directly that a minimal Steiner tree consists of key paths that are shortest paths between key vertices or terminals. A 
basic property of the SP is that Steiner trees contain at most |X| 2 key vertices, and consequently they consist of at most 
2|X| 3 key paths.
7.3 
Neighborhoods for the SP
The question of finding appropriate neighbourhoods for the Steiner tree problem in graphs has been addressed by 
several authors. Duin and Voss[DV93] distinguish between vertex-oriented neighbourhoods based on exchanging 
Steiner vertices and edge-oriented neighbourhoods in which edges are exchanged.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_119.html（第 1／2 页）2009-7-7 16:57:04

page_119
Osborne and Gillett[OG91] propose a neighbourhood based on the observation that a Steiner tree T can be represented 
by its vertices VT, since its edges ET can be determined by computing a minimum spanning tree for the subgraph of G 
induced by VT. Neighbors are then constructed by adding and removing elements to and from VT. A similar 
neighbourhood is used in the genetic algorithm of [KRSS93]. Although the neighbourhood size is 
, 
verification of local optimality for these neighbourhoods is computationally expensive, as construction of neighbours 
requires a minimum spanning tree computation with 
 time complexity. All the above papers present 
only computational results for small instances with up to 100 vertices. Esbensen[Esb95] presents a genetic algorithm 
based on a vertex-oriented neighbourhood and a heuristic to construct Steiner trees from sets of vertices. This
  
< previous page
page_119
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_119.html（第 2／2 页）2009-7-7 16:57:04

page_120
< previous page
page_120
next page >
Page 120
genetic algorithm gives good-quality solutions, albeit at the expense of substantial running times, viz., several hours for 
instances with 2500 vertices. Moreover, memory requirements are 
, which makes it difficult to handle larger 
instances.
Voss[Vos92] and Dowsland[Dow91] propose edge-oriented neighbourhoods based on exchanging key paths. Neighbors 
are constructed by removing one key path from a Steiner tree T and connecting the remaining two components by a 
shortest path between two arbitrarily chosen vertices, one in each component, such that a Steiner tree is obtained. A 
disadvantage of this neighbourhood is that the complexity of verification of local optimality, and thus the complexity of 
a single local search step, is 
, since a Steiner tree T contains at most 2|X| 3 key paths. Moreover, a time 
 pre-processing step is needed to compute shortest paths between all pairs of vertices, and storing all paths 
requires a 
 space complexity. [Dow91] furthermore presents a second neighbourhood that consists of the 
above neighbourhood extended with Steiner trees obtained by connecting the two components by shortest paths from 
any vertex not in the tree to any two vertices, one in each component. This extension is needed to prove strong 
connectivity of this neighbourhood, i.e., the possibility to transform each feasible solution to any other feasible solution. 
The time complexity to evaluate all neighbours in this neighbourhood is 
, which makes this 
neighbourhood not very suited for local search algorithms in which neighbourhoods have to be enumerated, e.g. 
iterative improvement or tabu search. In this paper, we present neighbourhood structures and exchange functions that 
have better time and space complexities and are therefore more adequate for larger problem instances.
Next, we present a neighbourhood which is comparable to the 1exchange neighbourhood of [Dow91], but it uses a 
different exchange function that has a smaller computational complexity, requires no pre-processing, and has smaller 
memory storage demands. First, we note that removal of a key path splits a Steiner tree into two components. 
Reconnection of these components can be considered as a new STPG instance in which components are treated as 
terminals. A SP instance with two components can be solved optimally by computing a shortest path between these 
components with a straightforward variant of the single-source shortest path algorithm of [Dij59]. This observation 
gives rise to the following neighbourhood.
Definition 4
Let T be a Steiner tree which consists of K key paths, l1, . . . ,lK. Let Si, 
be the two components that remain after 
removal of key path li from T. Let the function sp : P(VG) × P(VG)→ P(EG) give a shortest path from a subset of 
vertices to another subset of vertices. Then, the neighbourhood 
 is defined by
Neighborhood sizes can be different for different Steiner trees. Removal of a key path which ends in a key vertex with 
degree three turns that key vertex into a non key vertexSteiner vertex with degree twoand the two remaining key paths 
are joined to a single key path. Addition of a key path that ends in non key vertex converts that vertex into a key vertex, 
and the key path that passes through it is split into two key paths. So, |T| 2 ≤ |T′| ≤ |T| + 2 for 
, where |T| 
denotes the number of key paths in T.
The number of key paths whose removal has to be considered in a given neighbourhood 
 (T) is at least |X| 1 and at 
most 2|X| 3, since a Steiner tree
  
< previous page
page_120
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_120.html2009-7-7 16:57:05

page_121
< previous page
page_121
next page >
Page 121
proc sp (W, W′ : P(VG)) 
  begin 
    
 Y1 :=W; 
    for v ∈VG \ W do m(v) := ∞ rof; 
    for v ∈ W do m(v) := 0 rof; 
    while 
do 
      
 
      if v ∉ W′ then 
         for v′ ∈ {w ∈ VG | (v, w) ∈ EG} do 
           if v′ ∉ Y0 then 
              m(v′) := min{m(v′), m(v) + d(v, v′)}; 
              Y1 := Y1 ∪ {v′}; 
           fi 
         rof 
      fi; 
      Y0 := Y0 ∪ {v}; Y1 := Y1 \ {v}; 
  od {m(v) gives the length of the shortest path from W to W′} 
end
 
Figure 7.2 
A multiple source shortest path algorithm.
T contains at least |X| 1 and at most 2|X| 3 key paths. Hence, a neighbourhood contains at most 2|X| 3 neighbours. An interesting 
observation is that solutions only have neighbours with lower or equal cost, because only paths with at most the length of the 
removed path are inserted since this length is an upper bound on the length of the shortest path between components. So, an 
attempted replacement of a key path in a Steiner tree can lead to the same Steiner tree if no shorter path exists. In particular this 
can imply that local minima have no neighbours.
The function sp is computed with the algorithm presented in Figure 7.2. The complexity of this algorithm is 
 
if the set Y1 is represented by the classical heap data structure. Identification of the two components that arise when a key path 
is removed from a Steiner tree can be done in 
 time.
An important observation is that the algorithm in Figure 7.3 computes the shortest paths to vertices in ascending order. In this 
algorithm m(v) gives the length of a shortest path to a vertex v ∈ Y0. The algorithm can be terminated as soon as m(v) ≥ d(li) for 
a vertex v ∈ Y0 and a removed key path li, because then the shortest path from Si to 
 is at least d(li) long. Consequently, 
replacement of li cannot lead to a lower-cost tree. This leads to the following upper bound for the time complexity of the 
function sp. Let T be a Steiner tree from which a path l with length d(l) is removed, which results in two components S, S′. Let 
W be the largest set of vertices that are within distance d(l) from S for some l in T. Then, the complexity of the function sp is 
.
As a consequence of the insertion of shortest paths between components, the neighbourhood 
 is not connected, i.e., it is not 
always possible to reach a globally
  
< previous page
page_121
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_121.html2009-7-7 16:57:06

page_122
< previous page
page_122
next page >
Page 122
minimal Steiner tree by a sequence of exchanges, as can be seen by a simple example. Therefore, we present a 
neighbourhood 
 that is a small augmentation of 
 such that it is weakly connected, i.e., it is possible to reach an 
optimal solution from any solution by a sequence of exchanges. Moreover, 
(T) also contains neighbours with higher 
cost than a Steiner tree T, so here neighbourhoods of local minima are definitely not empty. Such neighbourhoods are 
needed in tabu search algorithms to escape from local minima.
In 
 also a single key path is replaced with another key path, as is the case in 
. The supplement of 
 to 
 
consists of neighbours constructed by adding shortest paths from one component to the other component via vertices 
that lie at a distance of one edge from the other component.
Definition 5
Let T be a Steiner tree which consists of K key paths, l1, . . ., lK. Then, the neighbourhood 
(T) is equal to
The complexity of evaluating the cost of all neighbours in a neighbourhood 
(T) of a Steiner tree T is 
, as all neighbours originating from removal of the same key path, can be evaluated in a single 
execution of the multiple-source shortest path algorithm. Note that the neighbourhood 
(T) is a strict subset of the 
second neighbourhood of [Dow91] and that all excluded neighbours have higher cost than those that are included in 
(T) The average cost of neighbours in 
(T) is therefore lower than in Dowsland's second neighbourhood, which also 
has a much larger computational complexity than 
, viz. 
. 
 is not strongly connected since it is not 
possible to transform a Steiner tree into another Steiner tree that differs only in a single key path that is not a shortest 
path between two components. However, we have the following result.
Theorem 1
The neighbourhood structure 
 is weakly connected, i.e., it is possible to reach an optimal solution from any solution 
by a valid sequence of exchanges.
Proof. Let T* be an optimal Steiner tree, let T be a Steiner tree, and let sp(A, B) denote a shortest path from component 
A to B. Partition T* into subtrees such that all leaves are terminals and all other vertices are Steiner vertices. Let S ⊆ T* 
be such a subtree that is not duplicated in T. We consecutively add adjacent key paths in S to T, starting with the key 
paths rooted from the leaves in S. We distinguish between two cases based on whether vertex i with which a vertex a ∈ 
T has to be connected, is included in T.
Let i ∈ T, and let sp(a, i) be a key path in T* not in T that is to be added. Remove a key path l from T not in T* such that 
addition of sp(a, i) would result in a Steiner tree. This is always possible since addition of sp(a, i) to T gives a cycle in T 
of which at least one key path is not included in T*. Removal of l splits T in components A and B with a ∈ A and i ∈ B. 
Let (j, i) ∈ sp(a, i) and construct 
. If sp(a, j) ≠ sp(A, j) = sp(a′, j) 
then addition of sp(a, j) to T′ results in a cycle. So, there exists a key path in T′ not in T* whose replacement with sp(a, 
j) would result in a Steiner tree. We can repeat the
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_122.html（第 1／2 页）2009-7-7 16:57:06

page_122
< previous page
page_122
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_122.html（第 2／2 页）2009-7-7 16:57:06

page_123
< previous page
page_123
next page >
Page 123
above procedure, adding one edge of sp(a, i) at a time, until the entire path sp(a, i) has been added.
Let i ∉ T, then i is a key vertex connected with at least three vertices a, b, c ∈ S that also exist in T. Let sp(a,i) be a key 
path in T* not in T that is to be added. Remove a key path l from T not in T* such that addition of sp(b, a) would result 
in a Steiner tree. This is always possible since addition of sp(b, a) to T gives a cycle in T of which at least one key path 
is not included in T*. Removal of l splits T in components A and B with a ∈ A and b ∈ B. Let (j,a) ∈ sp(a,i) and 
construct 
. Next, remove a key path l′ from T′ not in T*, which results 
in components A′ and C, such that addition of sp(c,j) would give a Steiner tree. Let (j′, j) ∈ sp(a, i) and construct 
. Repeating these steps of adding edges of sp(a, i) one after the 
other, starting alternately from b and c, gives a Steiner tree that includes i, at which point the first case applies.
Once a key path of T* has been added to a subtree S, other key paths from T* are consecutively added to S in order of 
ascending number of key paths to the leaves of S. This process gives a valid sequence of exchanges that transforms a 
Steiner tree T to an optimal Steiner tree T*.
The neighbourhoods 
 and 
 are based on the insertion of a single shortest path between two components. They 
are inspired by the polynomially solvable Steiner tree problem with two terminals. Chen[Che83] presents a polynomial 
algorithm for the SP with three terminals. This motivates the following neighbourhood in which two paths are removed 
from a Steiner tree. Reconnection of the remaining three components can be done optimally in polynomial time with the 
following algorithm which improves upon the time complexity of the algorithm of [Che83]. The essential observation in 
this algorithm is the following one.
Lemma 1
Consider an SP instance with three terminals s,s′,s′′, and let T be a minimal Steiner tree with key vertex w. Then, the 
length of the path in T from w to s and the length of the path from w to s′ is at most d(sp(s,s′)).
Note that it is possible that w coincides with a terminal. If we replace terminals s,s′,s′′ with three components S,S′,S′′ we 
see that the key vertex w in a minimal Steiner tree that contains the components S, S′, S′′, is included in the set W of 
vertices for which the shortest path to S and the shortest path to S′ is at most d(sp(S,S′)). This optimal key vertex w can 
be found as follows.
1. Construct the set of vertices for which the shortest path to S is at most d(sp(S,S′)). This set is given by the final value 
of the set Y1 in the algorithm in Figure 7.3 for the computation of sp(S,S′). Similarly, the set of vertices for which the 
shortest path to S′ is at most d(sp(S, S′)) is given by the final value of Y1 in the computation of sp(S′, S). The set W of 
candidate key vertices is the intersection of these sets.
2. The algorithm in Figure 7.2 is used to determine the shortest path from W to S′′. In this algorithm m(v) for v ∈ W has 
to be initialized as the distance from S to v plus the distance from S′ to v. The starting vertex w of this path is the key 
vertex in a minimal Steiner tree T for the components S, S′, S′′.
  
< previous page
page_123
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_123.html2009-7-7 16:57:07

page_124
< previous page
page_124
next page >
Page 124
The time complexity for finding w 
. The above algorithm adds at most three additional key paths to 
S,S′,S′′ to construct a minimal Steiner tree that contains S,S′,S′′. Let stp3 (S,S′,S′′) denote this set of key paths. Using the 
function stp3, we can define the following 2-exchange neighbourhood 
.
Definition 6
Let T be a Steiner tree which consists of K key paths, l1, . . ., lk. The function stp3 gives for three components the 
additional key paths in a minimal Steiner tree that contains these components. Then, the neighbourhood 
 is defined 
by
The size of this neighbourhood is 
 as the number of key paths K in a Steiner tree is at most 2|X| 3. The 
complexity of identifying S,S′,S′′ is 
 and the complexity of the algorithm to implement the function stp3 is 
. Hence, the complexity of verifying local optimality w.r.t. 
. The time 
complexity of the function stp3 can be reduced by terminating its execution as soon as m(v) > li + lj, where m(v) is the 
summed length of the shortest paths from S, S′ and W to v ∈ VG, because then replacing key paths li and lj cannot lead 
to a Steiner tree with lower cost.
For all neighbours 
 holds that f(T′) ≤ f(T) since neighbours are constructed by computing a minimal 
Steiner tree to connect the remaining components after removal of two key paths. Note that removal of two key paths 
can lead to a Steiner tree in which Steiner vertices with degree one exist. This occurs if the two from T removed key 
paths lj and lj have a key vertex with degree three in T in common. The remaining key path from this vertex can also be 
removed from T leading to an additional cost decrease. So, in some cases three key paths are removed from a Steiner 
tree T for the construction of its neighbours in 
 (T).
The neighbourhood 
 (T) is not a subset of 
 (T) for some Steiner trees T, and in some cases the intersection can 
even be empty. However, the following result holds.
Theorem 2
Let a Steiner tree T ∈ S be given. If T is a local minimum of 
, then T is also a local minimum of 
. Moreover, let 
 and 
 with l,l′∈ T and l,l′∈ T and l,l′∉ T′′, then f(T′) ≤ f(T′′).
Proof. Consider a local minimum T of 
. Let 
, where key paths 
l and +l′ are removed from T, which results in three components S,S′,S′′. T′ is a minimal Steiner tree that connects the 
components S,S′,S′′. Let 
T′′ is also a Steiner tree that 
connects the components S,S′,S′′, so f(T′′) ≥ f(T′) = f(T).
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_124.html（第 1／2 页）2009-7-7 16:57:08

page_124
A disadvantage of the neighbourhood 
 is its considerable time complexity for a single local search step since the 
number of neighbours is 
 and the time complexity to evaluate the cost of a neighbour is 
. 
Therefore, we present the following neighbourhood 
 which is a restriction of 
.
Definition 7
Let T be a Steiner tree which consists of K key paths, l1, . . ., lK. Let the function stp3 return, on input of three 
components, the additional key paths in a
  
< previous page
page_124
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_124.html（第 2／2 页）2009-7-7 16:57:08

page_125
< previous page
page_125
next page >
Page 125
minimal Steiner tree that contains these components. Then, the neighbourhood 
 (T) is equal to
In 
 only those two key paths are removed that have a vertex in common, which can be a terminal or a key vertex. 
Typically, if this key vertex has degree three in the Steiner tree, the remaining key path is also removed. The three 
remaining components are connected using the algorithm stp3 to compute a minimal Steiner tree for three components. 
The size of the neighbourhood 
 (T) for a Steiner tree T is 
, where K, is the maximum degree of a key 
vertex in T. The complexity of verifying local optimality of T for 
 is then 
, which is 
substantially less than the complexity of verifying local optimality for 
. Moreover, it still holds that a local 
minimum of 
 is a local minimum of 
 since 
 for a Steiner tree 
T ∈ S.
Implementation Issues
The time complexity of a single step of an iterative improvement algorithm depends also on the data structures used to 
represent solutions. A Steiner tree is a set of key paths. This set is given by a list of records that represent key paths. 
Each record also contains cross references to key paths that share end vertices with this key path. Furthermore, a 
function is maintained that gives, for each vertex included in the Steiner tree, a key path that contains this vertex. Given 
the end vertices of a key path it is possible to find the corresponding key path by following the cross references. In this 
way removal and addition of key paths can be done in a time linearly bounded by the number of vertices in a key path 
and the degrees of the end vertices. It should be noted that removal or addition of key paths can require that other key 
paths are joined to one key path or split into two key paths.
Another important aspect in the implementation of an iterative improvement algorithm is how neighbourhoods are 
enumerated. Although this aspect is rarely discussed in the literature it can make a substantial difference in running time 
how the neighbourhood is enumerated. Essential for enumerating neighbourhoods is that exchanges which do not lead 
to lower cost solutions do not have to be examined. Of course it is generally not possible to determine beforehand which 
exchanges do not lead to a lower cost solution, but often exchanges which do not yield any gain when applied to a 
Steiner tree T also do not lead to a lower cost Steiner tree when applied to a neighbour 
. This implies that 
it is profitable to enumerate neighbourhoods in such a way that exchanges which have not yet been examined are 
explored first. To this end, a Boolean function b is introduced that assigns a Boolean to arguments of the exchange 
function that indicates whether the corresponding exchange has been evaluated and has led to cost increase. While 
enumerating the neighbourhood of the current solution in a local search algorithm, first those exchanges are examined 
for which this Boolean does not hold. If no lower cost neighbour has been found, the remaining exchanges are evaluated.
In the neighbourhood 
 only a single path is removed in an exchange, so Booleans can be associated with key paths, 
and the space-complexity for storing the Boolean function b is 
. In the neighbourhood 
 two key paths are 
removed in an exchange, so Booleans have to be associated with pairs of key paths, and here the
  
< previous page
page_125
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_125.html2009-7-7 16:57:08

page_126
< previous page
page_126
next page >
Page 126
space complexity of b is 
. To reduce the space complexity of b, it is possible to associate a Boolean only with 
a single key path and to evaluate those exchanges first for which at least one Boolean associated with one of the two 
removed key paths is false. A key path's Boolean is set to true if all exchanges which remove this key path have been 
examined and do not result in cost decrease. In the neighbourhood 
 most exchanges can be identified by marking 
key vertices, because when a key vertex in a Steiner tree has degree three, neighbours are obtained by removing the 
three key paths which end in this key vertex. Considering that most key vertices in a Steiner tree have degree three, we 
can associate Booleans with key vertices to indicate whether they have been exchanged. The space complexity of b can 
then be reduced to 
7.4 
Computational Results
We have implemented an iterative improvement algorithm that uses the neighbourhood 
. Computational results are 
presented for 40, randomly generated, instances from [Bea90]. Furthermore, a number of real-world Euclidean 
travelling salesman problem instances of the TSPLIB from [Rei91] are transformed to Steiner tree problems, since for 
many combinatorial optimization problems it is observed that real-world instances typically are much harder to solve 
than randomly generated instances. This transformation is done as follows. Edges in an SP instance are connections 
between adjacent Voronoi regions in a Euclidean travelling salesman instance. The resulting graph G is a planar graph 
and contains at most 3n edges where n is the number of vertices. To construct SP instances with more edges, we add 
edges between vertices that are reachable from a vertex in 1,2,3 or 4 steps along the edges in G. From each graph 
constructed in this way three SP instances are obtained by randomly denoting 15%, 25%, or 35% of the vertices as 
terminals. No reduction techniques to reduce Steiner tree problem instances have been used.
Initial solutions are constructed with a shortest path heuristic [TM80]. Tables 7.1 and 7.2 present the results obtained 
with our iterative improvement algorithm. In these tables |V|, |E|, and |X| give the number of vertices, edges, and 
terminals, respectively. For all problems but one (e18), in Beasley's series D and E, the optimal solution values are 
known [Bea89, CGR92] and given by opt. Note that for the Euclidean SP instances optimal solutions are only known 
for the instances f1 f24 with up to 1000 vertices. These solutions are computed by the exact algorithm of [Dui94], which 
is among the fastest optimization algorithms available for the SP. It can handle instances with at most 1000 vertices. 
The best solution found in ten runs of the iterative improvement algorithm is given by best, the average cost of these ten 
local minima by avg, and their average deviation in percentages from the optimal solution is given by ∆avg. The 
average running time in seconds on a Sun Classic workstation is given by t(s).
We observe that our iterative improvement algorithm finds solutions whose relative excess is within a few percent over 
the optimal solution for the instances of [Bea90] in a small amount of running time (at most a few minutes, whereas 
exact algorithms require several hours on supercomputers or powerful workstations for instances with 2 500 vertices). 
For the Euclidean instances with up to 1000 vertices we are able to find solutions with a relative deviation of 0.5% to 
2.3% from optimality in a few seconds.
  
< previous page
page_126
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_126.html2009-7-7 16:57:09

page_127
< previous page
page_127
next page >
Page 127
Table 7.1 Results for Beasley's series D and E.
name
|V|
|E|
|X|
opt
best
∆aug
t(s)
d1
1000
1250
5
106
106
2.64
0.7
d2
10
220
220
0.73
0.3
d3
167
1565
1567
0.22
9.4
d4
250
1935
1939
0.34
11.9
d5
500
3250
3254
0.18
24.7
d6
2000
5
67
70
5.67
0.8
d7
10
103
103
0.00
0.9
d8
167
1072
1082
1.64
8.2
d9
250
1448
1454
0.70
14.5
d10
500
2110
2119
0.70
29.5
d11
5000
5
29
29
4.14
0.8
d12
10
42
42
3.81
0.1
d13
167
500
509
2.46
9.2
d14
250
667
674
1.27
15.0
d15
500
1116
1123
0.79
32.6
d16
25000
5
13
13
3.08
0.1
d17
10
23
23
4.35
0.3
d18
167
223
236
7.40
12.3
d19
250
310
336
9.32
22.8
d20
500
537
558
4.49
53.4
e1
2500
3125
5
111
111
0.00
0.2
e2
10
214
214
2.62
0.5
e3
417
4013
4035
0.64
57.1
e4
625
5101
5118
0.35
85.2
e5
1250
8128
8130
0.11
171.4
e6
5000
5
73
73
2.04
0.2
e7
10
145
145
3.70
0.7
e8
417
2640
2661
0.98
76.2
e9
625
3604
3633
1.03
120.1
e10
1250
5600
5621
0.47
242.9
e11
12500
5
34
34
4.41
0.2
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_127.html（第 1／2 页）2009-7-7 16:57:10

page_127
e12
10
67
67
1.49
0.6
e13
417
1280
1312
2.82
77.5
e14
625
1732
1756
1.59
139.4
e15
1250
2784
2794
0.69
265.2
e16
62500
5
15
15
6.67
0.3
e17
10
25
25
4.80
0.8
e18
417
568
613
8.87
94.1
e19
625
758
809
7.68
156.3
e20
1250
1342
1398
4.71
384.5
 
  
< previous page
page_127
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_127.html（第 2／2 页）2009-7-7 16:57:10

page_128
< previous page
page_128
next page >
Page 128
Table 7.2 Results for Euclidean instances.
name
|V|
|E|
|X|
opt
∆avg
t(s)
f1
783
2322
117
2992
1.5
4.4
f2
196
3826
0.7
7.5
f3
274
4401
1.2
8.9
f4
7532
117
2899
1.9
5.1
f5
196
3744
1.8
8.8
f6
274
4368
1.5
10.1
f7
28365
117
2892
1.0
28.9
f8
196
3742
1.4
18.2
f9
274
4368
1.0
19.5
f10
109895
117
2892
1.1
24.9
f11
196
3742
1.5
51.5
f12
274
4368
1.0
53.9
f13
1000
2981
150
6509412
2.3
11.2
f14
250
8123458
1.3
16.8
f15
350
9804964
1.2
24.0
f16
9699
150
6316653
2.3
11.3
f17
250
7946357
0.7
19.5
f18
350
9675663
1.1
26.1
f19
38080
150
6297956
2.0
20.2
f20
250
7931250
0.6
41.8
f21
350
9673224
0.9
98.7
f22
184597
150
6297956
1.6
66.4
f23
250
7931250
0.7
99.1
f24
350
9673224
0.5
151.7
name
|V|
|E|
|X|
opt
∆avg
t(s)
g10
3795
11326
569
13266
13288.0
153.2
g11
949
14745
14772.5
276.4
g12
1328
15901
15960.3
367.2
g13
52684
569
13105
13117.8
237.6
g14
949
14628
14634.3
394.4
g15
1328
15716
15775.3
547.9
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_128.html（第 1／2 页）2009-7-7 16:57:12

page_128
g16
325093
569
13063
13076.2
850.3
g17
949
14612
14629.7
1171.2
g18
1328
15685
15716.9
1770.7
g22
11849
35532
1777
383245
383694.4
2475.5
g23
2962
480145
480879.8
4296.3
g24
4147
561050
561668.0
5740.1
g25
14051
42128
2108
154479
154575.1
2698.5
g26
3513
198609
198667.3
4565.9
g27
4918
235609
235697.7
6177.6
g28
18512
55510
2777
205697
205775.5
4761.3
g29
4628
273631
273712.7
7369.0
g30
6479
324805
324875.1
10338.7
 
  
< previous page
page_128
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_128.html（第 2／2 页）2009-7-7 16:57:12

page_129
< previous page
page_129
next page >
Page 129
We can also handle even larger instances in an acceptable amount of time.
Furthermore, we are able to deal with much larger instances than those studied in other local search approaches as 
presented in [Dow91], [KRSS93], and [Esb95], who all consider instances with at most 100 vertices. Moreover, our 
approach has limited memory requirements since only a compact representation of an instance is stored. Most other 
heuristic and exact approaches require 
 space for storing all-pairs shortest paths, which makes it very hard to 
adapt them for handling large instances efficiently.
References
[AL96] Aarts E. and Lenstra J. (eds) (1996) Local Search in Combinatorial Optimization. Wiley, New York.
[Bea89] Beasley J. (1989) An SST-based algorithm for the Steiner problem in graphs. Networks 19: 116.
[Bea90] Beasley J. (1990) OR-library: distributing test problems by electronic mail. Journal of the Operational 
Research Society 41: 10691072.
[CGR92] Chopra S., Gorres E., and Rao M. (1992) Solving the Steiner tree problem using branch and cut. ORSA 
Journal on Computing 12: 320335.
[Che83] Chen N. (1983) New algorithms for Steiner tree on graphs. In Proc. of the IEEE Int. Symposium on Circuits 
and Systems, pages 12171219.
[Dij59] Dijkstra E. (1959) A note on two problems in connection with graphs. Numer. Math. 1: 269271.
[Dow91] Dowsland K. (1991) Hill-climbing, simulated annealing and the Steiner problem in graphs. Engineering 
Optimization 17: 91107.
[Dui94] Duin C. (1994) Steiner's Problem in Graphs. PhD thesis, Amsterdam University.
[DV93] Duin C. and Voß S. (1993) Steiner tree heuristics a survey. In Dyckhoff H., Derigs U., Salomon M., and Tijms 
H. (eds) Proc. 22nd annual meeting DGOR / NSOR. Springer, Berlin.
[Esb95] Esbensen H. (1995) Computing near-optimal solutions to the Steiner problem in a graph using a genetic 
algorithm. Networks 26: 173185.
[HRW92] Hwang F., Richards D., and Winter P. (1992) The Steiner Tree Problem, volume 53 of Annals of Discrete 
Mathematics.
[KRSS93] Kapsalis A., Rayward-Smith V., and Smith G. (1993) Solving the graphical Steiner tree problem using 
genetic algorithms. Journal of the Operational Research Society 44(4): 397406.
[OG91] Osborne L. and Gillett B. (1991) A comparison of two simulated annealing algorithms applied to the directed 
Steiner problem on networks. ORSA Journal on Computing 3: 213225.
[Ree93] Reeves C. (ed) (1993) Modern Heuristic Techniques for Combinatorial Problems. Blackwell, London.
[Rei91] Reinelt G. (1991) TSPLIB: A traveling salesman problem library. ORSA Journal on Computing 3(4): 376384.
[TM80] Takahashi H. and Matsuyama A. (1980) An approximate solution for the Steiner tree problem in graphs. Math. 
Japonica 24(6): 573577.
[Vos92] Voß S. (1992) Steiner's problem in graphs: heuristic methods. Discrete Applied Mathematics 40: 4572.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_129.html（第 1／2 页）2009-7-7 16:57:13

page_129
  
< previous page
page_129
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_129.html（第 2／2 页）2009-7-7 16:57:13

page_131
< previous page
page_131
next page >
Page 131
8 
Local Search Strategies for the Vehicle Fleet Mix Problem
Ibrahim H. Osman and Said Salhi
Abstract
The vehicle fleet mix (VFM) problem is the vehicle routing problem with additional practical extensions. The VFM 
problem involves a heterogeneous fleet of vehicles with their associated variable running and fixed purchasing/renting 
costs. The objective is to find the optimal fleet composition of vehicles and a set of feasible routes that minimize the 
total costs. In this paper, two techniques are proposed: a constructive heuristic and a tabu search metaheuristic. The 
constructive heuristic is an enhanced modification of the Salhi and Rand route perturbation procedure. The tabu 
metaheuristic is new and it uses a compound-moves neighbourhood with a special data structure. Computational results 
are reported on a set of 20 test problems from the literature. The proposed methods obtain new results that improve 
upon the best published results.
8.1 
Introduction
The vehicle fleet mix (VFM) problem is the vehicle routing problem with a heterogeneous fleet of vehicles. Each 
vehicle is characterized by the carrying capacity, maximum travel time, variable running cost and fixed purchasing/
renting cost. Each vehicle route originates and terminates at a central depot in order to service a set of customers with 
known demands. The total cost involved in the VFM problem consists of the fixed vehicle utilization costs as well as 
the variable routing and scheduling costs. The later costs include the distance and travel time costs in addition to the 
service (loading/unloading) time costs. The constraints of the VFM problem are: (i) each customer must be supplied by 
exactly one vehicle route; (ii) the total load carried by any vehicle must not exceed its maximum capacity; (iii) the total 
length of any route, which includes the inter-customer travel times and service times, must not exceed a pre-specified 
limit. The objective is to find the optimal mix of heterogeneous vehicles and the associated set of feasible routes that 
minimizes the total variable and fixed
Modern Heuristic Search Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_131
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_131.html2009-7-7 16:57:14

page_132
< previous page
page_132
next page >
Page 132
costs.
The VFM problem is practically more important than its counterpart, the vehicle routing problem (VRP). The VRP, 
however, has attracted much more research attention. The lack of VFM research is mainly due to the less obvious way 
of how to determine the best mix of vehicles which is a medium-term decision problem. The decision problem involves 
an important cost investment factor. If an unwise decision was made to purchase a fleet of vehicles, it would be difficult 
to change such a decision. In general, researchers have considered a homogeneous fleet of vehicles. Yet in real life, the 
appropriate fleet is, by no means, homogeneous and a heterogeneous vehicle fleet is likely to yield better results. 
Furthermore, the VFM problem still has the difficulty of the operational routing decision which is encountered in the 
daily operations of vehicles in the classical VRP. Hence, the VFM problem is computationally more complex than the 
VRP which is known to be NP-hard (Lenstra and Rinnooy Kan[LR81]). Consequently, an exhaustive search of the 
solution space is impossible for VFM problems of large size and there is no known algorithm that can solve the VFM 
problem to optimality in a polynomial time. The complexity of the VFM problem therefore necessitates the 
development of effective heuristics that are capable of providing high-quality approximate solutions within a reasonable 
amount of computational effort.
Local search methods form a class of heuristics that proceed by examining some neighbourhoods of the current 
solution. The simplest type of local search method is the descent algorithm. It starts with any solution, neighbours are 
searched until an improved solution is found. A further search is then initiated from the improved solution and the 
algorithm continues until no further improvement is possible. The solution obtained may deviate significantly from the 
optimum. Tabu search (TS) is a metaheuristic which is superimposed on the descent algorithm to guide the search 
process to avoid getting trapped in bad local optima. TS allows the search to proceed to a neighbour even if it causes a 
deterioration in the objective function value. TS has been proven to be a very effective and successful metaheuristic for 
solving many combinatorial optimization problems. Particularly impressive results have been obtained for many vehicle 
routing problems: Eglese and Li[EL96], Gendreau, Hertz and Laporte[GHL94], Potvin, Kervahut, Garcia and Rousseau
[PKGR96], Osman [Osm93], Rego and Roucairo [RR96], Salhi and Sari[SS95], Taillard[Tai93], Thangiah, Osman and 
Sun[TOS94] and Thangiah, Osman, Vinayagamoorthy and Sun[TOVS93]. For recent bibliographies, we refer to Osman 
and Laporte[OL96] on metaheuristics, to Laporte and Osman[LO95] on routing problems. For an overview and an 
introduction on metaheuristics, we refer to Osman[Osm95b] and Osman and Kelly[OK96b]. For metaheuristic books, 
we refer to: Aarts and Lenstra[AL96], Laporte and Osman[LO96], Osman and Kelly[OK96a], Rayward-Smith[RS95] 
and Reeves[Ree93]. Motivated by tabu search successes, in this paper, we present a tabu search algorithm for solving 
the mixed fleet vehicle routing problem. We design a special data structure to evaluate efficiently trial solutions 
generated by the 1-interchange (compound-moves) neighbourhood mechanism[Osm95a, Osm93, OC94]. Finally, we 
introduce some modifications and refinements to enhance the performance of the RPERT procedure proposed by Salhi 
and Rand [SR93] for the VFM problem.
The paper is organized as follows. The vehicle fleet mix and its relevant literature are reviewed in Section 8.2. The 
modified RPERT construction heuristic is described in Section 8.3. The important features of the tabu search algorithm 
with its embedded
  
< previous page
page_132
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_132.html2009-7-7 16:57:14

page_133
< previous page
page_133
next page >
Page 133
data structures are discussed in Section 8.4. Computational comparisons on a set of 20 test problems with the best 
published methods in the literature are provided in Section 8.5. Finally, our conclusion and perspectives on future 
research are presented in Section 8.6.
8.2 
The Vehicle Fleet Mix (VFM) Problem
8.2.1 
Representation of the VFM
Let '0' denote the depot with no service time and zero demand; the remaining notations are defined as follows:
K = The set of different vehicle types, K = {1, . . ., kmax}.
Fk = The vehicle fixed cost of type k ∈ K.
Qk = The vehicle capacity of type k ∈ K.
Tk = The maximum travel time for the vehicle capacity of type k ∈ K.
v = The decision variable indicating the total number of mixed vehicles used in the final solution of the VFM problem.
V = The set of desired vehicles of different types, V = {1, . . .,v}, V ⊆ K.
N = The set of customers, N = {1, . . .,n} where n is the total number of customers.
qi = The demand of customer for i ∈ N,
δi = The service time of customer for i ∈ N.
dij = The distance between customers i and j, dij = dji∋∋i, j ∈ N ∪ {0}.
αk = The variable cost per unit of distance for a vehicle of type k.
βk = The time factor per unit of distance for a vehicle of type k.
Rp = The set of customers serviced by a vehicle p ∈ V.
σ = The function σ : V → K, where σ(p) indicates the smallest type of vehicles that can serve the customers in the set 
Rp.
πp = The travelling salesman route which serves the set Rp∪ {0}, where πp(i) indicates the position of customer i in the 
route, πp.
D(πp) = The total distance of the route, πp.
  
< previous page
page_133
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_133.html2009-7-7 16:57:15

page_134
< previous page
page_134
next page >
Page 134
T(πp) = The total travel time of the route, πp.
C(πp) = The total variable and fixed cost of the route, πp.
S = The feasible solution which is defined as S = {R1 . . .,Rv}
Π = The set of all travelling salesman routes in S, Π = {π1, . . ., πv}.
Our goal is to solve the following optimization problem:
such that
Constraint (8.2) ensures that each customer is supplied in one route. The set of constraints in (8.3) guarantees that the 
vehicle capacity is not exceeded. Equations in (8.4) represent the total sum of distance of each route, πp. Since the TSP 
is a hard problem by itself, we have used approximate procedures to estimate each route, πp. Equations (8.5) guarantee 
that the maximum travel time is not exceeded. Equations (8.6) represent the total cost per route including variable and 
fixed cost while Equation (8.1) is the total sum of costs in the solution to be minimized over all routes.
8.2.2 
Literature Review of the VFM
There are few published works on the VFM problems. In the early 1980s, Golden, Assad, Levy and Gheysens
[GALG84] were among the first to address this problem. They presented a mixed integer formulation, generated lower 
bounds on the optimal solution and described various heuristics based on the savings method of Clarke and Wright
[CW64] and the route-first cluster-second algorithm. The later heuristic starts with a giant route that visits all customers 
but not the depot. This giant route is further improved by the Or-OPT exchange procedure before it is then partitioned 
into feasible routes. This heuristic is then repeated using a sample of five initial routes, hence leading to its name (MGT
+OrOPT)5. Excellent results were reported using the (MGT+OrOPT)5 heuristic on problems ranging from 12 to 100 
customers. Gheysens, Golden and Assad[GGA84] developed a cluster-first route-second heuristic. In the
  
< previous page
page_134
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_134.html（第 1／2 页）2009-7-7 16:57:16

page_134
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_134.html（第 2／2 页）2009-7-7 16:57:16

page_135
< previous page
page_135
next page >
Page 135
first stage, a heterogeneous fleet mix was obtained using a lower bound procedure developed in Golden, Assad, Levy 
and Gheysens[GALG84]. In the second stage, they adopted a generalized assignment based heuristic to solve the VRP 
problem using the vehicle fleet mix obtained in the first stage. This heuristic was also run for five different times using 
the fleet compositions associated with each of the five best lower bounds and it was denoted by LB5+VRP. Gheysens, 
Golden and Assad [GGA84] presented another mixed integer formulation for the VFM problem with time windows and 
performed a computational comparison of heuristics developed in Gheysens, Golden and Assad[GGA86]. Ferland and 
Michelon[FM88] showed that an exact method for the VRP with time windows and a homogeneous fleet size can be 
extended to the VRP with the heterogeneous fleet size. The proposed exact method was based on a column generation 
approach with relaxed time windows but no computational results were reported.
In the 1990s, Desrochers and Verhoog[DV91] presented a new savings heuristic called MBSA which was based on 
successive route fusions. At each iteration, the best fusion was selected by solving a weighted matching problem. The 
MBSA heuristic was implemented considering several weights in the matching problem such as the total savings in 
routing costs, savings in fixed costs, or opportunity savings associated with each feasible combination. Computational 
results were provided for a number of benchmark problems in order to compare the algorithm's performance to that of 
other methods. Ronen[Ron92] presented a mixed integer formulation to assign trips to mix fleet vehicles with the 
minimal cost. A two-step heuristic was proposed based on the assignment of trips to vehicles first, then slide-and-switch 
of trips between vehicles second. The heuristic provided results within 1% of the linear relaxation bound.
Salhi, Sari, Saidi and Touati[SSST92] presented a mixed integer formulation for the VFM problem with fixed and 
variable running costs. They assessed the effect of neglecting the variable running cost on the solution of different 
procedures. They modified the route-first cluster-second and the savings algorithms in order to take into account both 
variable running and fixed costs. This mixed integer formulation is similar to that of Gheysens, Golden and Assad
[GGA84] which assumes the same value of the unit running cost across the different vehicle types. Salhi and Rand
[SR93] presented a review on the VFM and also developed an interactive route perturbation procedure (RPERT) 
consisting of a series of refinement modules. New best-known solutions were reported by the RPERT procedure for 
some standard test problems. Finally, the importance of the strategic decisions on the fleet make-up and the vehicle 
number of each types was discussed in the context of distribution system design by Bookbinder and Reece[BR88] and 
Beaujon and Turnquist[BT91].
8.3 
Modified RPERT Procedure (MRPERT)
Salhi and Rand[SR93] proposed a successful interactive procedure denoted by RPERT. The RPERT procedure 
constructs a VFM solution using a series of seven phases. Each phase uses one perturbation module in order to improve 
upon the solution of its predecessor. If the new solution is better and feasible in terms of vehicle capacities and 
maximum travel times, it is then accepted as the current solution and passed into the next phase for further 
improvements. Otherwise the previous solution is retained and
  
< previous page
page_135
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_135.html2009-7-7 16:57:16

page_136
< previous page
page_136
next page >
Page 136
the search continues until the RPERT procedure is terminated at the end of the last phase. It was noticed that in some 
modules, the routing cost was increased but with a greater decrease in the fixed cost. The aim was to explore the 
compromise between having a higher total operating cost and a lower total fixed cost, while achieving a maximum 
utilization of the whole fleet.
We shall briefly describe these modules in the same order as they were implemented in the RPERT procedure. For more 
details, the reader should refer to Salhi and Rand[SR93]. RPERT begins with the savings module developed for the 
vehicle routing problem in Salhi and Rand[SR87]. The savings module constructs an initial set of routes using a given 
vehicle capacity. This savings module uses a 'shape' parameter to modify the savings function used in [Daw86] to 
compute the savings in distance if pairs of customers were served in a single route. The shape parameter value was 
varied between 0 and 2 to generate a set of different initial routes. A matching module was then used to determine for 
each route the smallest type of vehicles that can serve the customers on it. This module was followed by a reduction 
module which was attempted, whenever possible, to eliminate a given route by relocating its customers and inserting 
them in other routes or merging the route with another to build a larger single one. A sharing module was also 
implemented. It attempted to split a given route into few smaller routes to achieve a cost reduction, if possible. Finally, 
the RPERT procedure improved the cost of routes by a swapping module which exchanged customers between routes. 
Let us define a cycle of search to be a single execution of the above sequence of the RPERT modules. The RPERT 
procedure was only performed using one cycle and was terminated at the end of its last module. The best stored solution 
at the end of this cycle was called the RPERT final solution.
In RPERT, a rigid restriction was imposed on the allowed type of moves in order to speed up the procedure. A move is a 
transition from one solution to another neighbouring solution. More precise, a move was not allowed to be performed if 
it resulted in a utilization of larger-sized vehicles than the currently used ones when implementing the reallocation and 
swapping modules, i.e, only feasible moves in terms of capacity and maximum travel were allowed and infeasible 
moves were prohibited. To alleviate these restrictions in this paper, we introduce the following two modifications. First, 
to enlarge the neighbourhood size (set of available moves), the feasibility restriction is relaxed, i.e., a move which leads 
to a utilization of a larger-sized vehicle was allowed. Second, we also allow the RPERT procedure to restart for another 
cycle starting from the best solution obtained at the end of previous cycle. The search continues for a number of cycles 
and it is terminated when a cycle is performed without finding any improvements. The reason is that RPERT performed 
only one cycle before terminating its search. However, if an improvement happened in any (say sharing) modules it may 
be possible to improve this solution by using an earlier (say reallocate or reduction) module. These further 
improvements would not be detected unless the RPERT procedure is restarted. The restart process and the allowance of 
infeasible moves were not implemented in RPERT. These modifications form the basis of the new procedure which is 
denoted by MRPERT.
  
< previous page
page_136
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_136.html2009-7-7 16:57:17

page_137
< previous page
page_137
next page >
Page 137
8.4 
Tabu Search
Tabu search (TS) is a relatively novel technique for solving hard combinatorial optimization problems. Tabu search 
ideas were proposed by Glover[Glo86]. It is based on selected concepts from artificial intelligence. Its goal is to emulate 
intelligent uses of memory for exploiting historical information. Tabu search uses memory structures to record in a tabu 
list, TABL, attributes of the recently accepted solutions, i.e. the changes occurred when performing a move from S to 
the best solution S′ in its neighbourhood N(S). Moreover, attributes can consist of customers removed, added or 
repositioned by the moves executed. Sometime, attributes can be strategically combined to create other attributes. 
Selected attributes, that are stored in TABL, are designated tabu-active. Solutions, that contain tabu-active elements, are 
designated tabu. The duration that an attribute remains on the tabu list and hence remains tabu-active, is called the tabu-
list size which is denoted by t.
The approach of using attributes to identify the tabu status of future moves is very easy to implement and requires less 
storage than the approach of using the actual solutions which is very expensive to store and difficult to check and 
maintain. The attributes-based approach, however, may wrongly forbid moves leading to unvisited solutions that may 
be attractive. It is therefore necessary to override the tabu status of such moves, if an aspiration level criterion is 
satisfied, in order to correct this wrong diagnosis. A move is considered admissible if it is not a tabu move or its tabu 
status is overridden by the aspiration criterion.
In general, TS uses an aggressive guiding strategy to direct any local search procedure to carry out exploration of the 
solution space to avoid getting trapped in local optima. When a local optimum is encountered, the aggressive strategy 
moves to the best solution S′ in the whole neighbourhood N(S) even if it may cause a deterioration in the objective 
function value. For situations, where the neighbourhood is large or its elements are expensive to evaluate, candidate list 
strategies are used to help restrict the number of solutions examined on a given iteration. A candidate list of solutions N′
(S) is generated either randomly or strategically using memory structures[Osm93] in order to identify the best move 
exactly, or heuristically. Finally, the rule for execution is generally expressed by a stopping criterion. Several stopping 
criteria can be used to terminate the search: either a pre-specified limit on the number of iterations or on the number of 
iterations since the last improvement was found. Since TS embeds heuristic rules and different strategies to guide the 
search, it becomes a metastrategy algorithm or simply a metaheuristic. The basic TS procedure is sketched in Figure 8.1.
  
< previous page
page_137
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_137.html2009-7-7 16:57:17

page_138
< previous page
page_138
next page >
Page 138
Tabu Search Procedure (TSVFM).
Step 1: initialization:
Generate an initial solution, S, for the VFM problem.
Set the best current solution Sbest = S.
Evaluate all the moves in the neighbourhood N(S).
Set values for: t, the tabu list size; TABL, the tabu list; DS, the special Data Structure for attributes;
MAXBEST, the maximum number of iterations after Sbest.
Set iteration counters: nbiter=0 (current iteration number) and bestiter=0 (iteration number of the best solution).
Step 2: Candidate list of solutions:
Determine strategically using DS, the exact set of the candidate list of best moves in the neighbourhood, i.e., N′(S) 
⊆ N(S).
Update DS after each iteration.
Step 3: Selection Strategy:
Choose the best admissible solution S′∈ N′(S).
Set S = S′ and nbiter= nbiter +1.
If C(S′) < C(Sbest), then set Sbest = S′ and bestiter = nbiter.
Record in TABL the changed attributes.
Step 4: Stopping criterion:
If { (nbiter bestiter) > MAXBEST }, then Stop,
Else go to Step 1.
Figure 8.1 
A basic TS procedure.
In the following, we shall give a description of the basic components of the above TS procedure and practical 
implementation details for solving the vehicle fleet mix problem. For recent details on tabu search, we refer to by Glover
[Glo95a].
Initial Solution
The initial solution, S, can be generated using any VFM or VRP heuristic. In this study, we have used the VRP heuristic 
of Salhi and Rand[SR87] to generate the initial set of routes, II. The simple matching module of Salhi and Rand[SR93] 
is then used to determine for each route, πp, the type of the vehicle, σ(p), that can serve the set of customers Rp. The 
total cost C(S) of this VFM solution is computed using equations (8.1) and (8.6). An empty route with no customer 
assigned to it is added to the set of routes with zero costs. The reason for this empty route is to allow infeasible moves to 
be considered.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_138.html（第 1／2 页）2009-7-7 16:57:18

page_138
  
< previous page
page_138
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_138.html（第 2／2 页）2009-7-7 16:57:18

page_139
< previous page
page_139
next page >
Page 139
Neighbourhood Generation Mechanism
The neighbourhood generation mechanism describes how a given VFM solution S = {R1, . . ., Rp, . . ., Rv} can be 
altered to generate another solution S′ in N(S), the neighbourhood of S. Here, we adapt the λ-interchange mechanism 
which was introduced in Osman[Osm93] and successfully used for many problems, e.g., Chiang and Russell[CR96], 
Hasan and Osman[HO95], Osman[Osm95a], Thangiah, Osman and Sun[TOVS93, TOS94]. It has the property that λ-
optimal solutions are µ-optimal solutions for any integer µ < λ, Osman and Christofides[OC94].
Given a pair of route sets Rp and Rq in S, a 1-interchange generation mechanism invokes two processes to generate 
neighbouring solutions: a shift process which is represented by the (0,1), (1,0) operators and an interchange process 
which is represented by the (1,1) operator. Note that the reallocation and swapping procedures in Salhi and Rand[SR93] 
have some similarities with the shift and the interchange processes. The (1,0) shift process denotes the reallocation of 
one customer (say i) from the set of customers, Rp, to another set of customers, Rq. This shift process, if performed, 
would result in a new pairs of route sets: 
 and 
. The (0,1) shift process 
denotes a shift in the opposite direction, i.e., a shift of j from Rq to Rp and the new route sets become 
 and 
. The (1,1) interchange process combines the two shift processes 
simultaneously to generate compound moves to reach solutions that can not be generated by consecutively applying any 
of the shift process alone due to capacity/time restrictions. Each customer i ∈ Rp is systematically attempted for 
interchange with every other customer j ∈ Rq to get two new route sets 
 and 
.
The advantage of the (1,0) and (0,1) shift processes is that an empty vehicle may be produced or a single customer route 
may be formed. In both cases, savings in fixed costs may be obtained. To allow a single route to be formed using the 
shift operators, at least one empty set of customers, 
, at any iteration needs to be available for usage among the other 
non-empty route sets. If a decrease in the vehicle number has occurred, we may have two empty route sets, one of 
which is redundant and can be deleted. Another advantage of the empty set is that it allows infeasible moves to be 
considered when searching the neighbours. Here, we have a non-monotonic increase/decrease in the number of vehicles 
as well as a non-monotonic search of feasible/infeasible regions. For more details on non-monotonic search strategies, 
we refer to Glover [Glo95b].
Finally, the neighbourhood, N(S), is defined to be the set of all solutions that can be generated by considering a total 
number of v(v + 1)/2 pairs which is equal to v(v 1)/2 + v different pairs of route sets. The first term, v(v + 1)/2, 
determines the number of pairs of route sets (Rp, Rq) where 1 ≤ p < q ≤ v, involving v non-empty route sets, while the 
second term, v, is the number of pairs of route sets, 
, considered by the (1,0) process operating on 
the pair of non-empty and empty route sets. The customers in a given pairs of route sets are searched sequentially and 
systematically for improved solutions by the shift and interchange processes of the 1-interchange mechanism.
Evaluation of the Cost of a Move
Let 1IM denote a 1-interchange move from a current solution S to one of its neighbours S′∈ N(S) by either the shift or 
the interchange processes, i.e., 1IM (S) = S′. Given a pair of route sets Rp and Rq, the
  
< previous page
page_139
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_139.html2009-7-7 16:57:18

page_140
< previous page
page_140
next page >
Page 140
cost of the move, 1IM depends on the type of operators involved. There are three type of operators (1,0), (0,1) and (1,1).
(I) The cost of (1,0)-based move.
The (1,0)-based move involves shifting a customer i from the route πp of Rp and inserting it in the best 
possible position on arcs (least cost insertion) of the approximate TSP route πq of Rq. The (1,0) move would 
result in two route sets 
 and 
. Let a and b be the predecessor and the 
successor of i in the old TSP route πp of Rp. Let us assume c and e to be the predecessor and the successor in 
the new TSP route 
 of 
 where i is to be best inserted. The costs of the new TSP routes are then 
computed as follows:
where p′ and q′ are the vehicles that serve the new route sets 
 and 
, respectively.
(II) The cost of (0,1)-based move.
The (0,1)-based move involves shifting a customer j ∈ Rq to another route set Rp. This move is similar to the 
one in (I) except that Rp and Rq are in reversed order. Hence the cost of the (0,1) move is simply obtained by 
applying the (1,0) process on the route sets Rq, Rp in a similar way. Let (f,g) and (l,m) be the (predecessor, 
successor) of j in the routes πp and 
 respectively. Equations (8.7) to (8.11) can then be similarly rewritten 
as follows:
where p′ and q′ are the vehicles that serve the new route sets 
 and 
, respectively.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_140.html（第 1／2 页）2009-7-7 16:57:19

page_140
(III) The cost of (1,1)-based move.
The (1,1)-based move involves an exchange of i ∈ Rp with j ∈
  
< previous page
page_140
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_140.html（第 2／2 页）2009-7-7 16:57:19

page_141
< previous page
page_141
next page >
Page 141
Rq. This move can be seen as a combination of two simultaneous shifts: a shift of i into the route set 
 by the (1,0) operator and another shift of j into the route set 
 
by the (0,1) operator. Note that, the change in the objective function value ∆(1,1) due to the (1,1) operator can 
be easily evaluated from the stored data used for computing equations (8.7) to (8.11) and (8.12) to (8.16) 
bearing in mind some special cases that need careful examinations. For example, let us assume that the best 
insertion of customer i inside the route 
 is recorded to be between two customers (c and e) in the old route 
πq, where j was included. If c and e are different from j, then the new insertion cost of i into 
 is only the 
comparison between the old insertion cost of i in πq with that along the newly created arc joining f to g 
(predecessor and successor of j) due to the removal of j. However, if either c or e is identical to j, then the 
insertion cost of i into 
 needs to be evaluated in the same way as indicated in (I). Similar analysis needs to 
be conducted to find the insertion cost of j into 
. Then the computed costs will be used to evaluate the cost 
of the (1,1) move, ∆(1,1).
The best 1-interchange move involving either i or j or both is determined from the values of the 1IM costs obtained by 
using the (1,0), (0,1) or (1,1) operators. Its value, δij, is the most negative or the least positive change in costs and it is 
computed as:
The best 1-interchange move, 1IMpq, involving the pair of routes (Rp, Rq) is the 1IM move which gives the best change 
in costs over all δij obtained by searching systematically all possible shifts or exchanges of customers between Rp and 
Rq. Its value, ∆pq, is computed as:
The change in the objective function values, ∆ = C(S′) C(S), is then set equal to ∆pq, if the best move, 1IMpq, is 
performed using the set of routes (Rp,Rq). The 1IMpq move can be identified by its attributes: the move value, ∆pq; the 
customer indices, i and j; the operator type; the route indices and the new/old predecessors and new/old successors. 
These attributes can be stored in a special data structure, DS, to be retrieved fast and save computational effort.
Data Structures for the Candidate List of Moves
Data structures (DS) for strategically defining the set of best moves were introduced for the VRP in Osman[Osm93] and 
resulted in big savings of more than a half of the required computation time. Osman's DS stored only the set of best 
moves between all pairs of routes. However, it did not make use of any computed information generated in early 
evaluations in relation to the best insertion/removal positions and their associated
  
< previous page
page_141
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_141.html2009-7-7 16:57:20

page_142
< previous page
page_142
next page >
Page 142
costs. These attributes could be shared among many moves. Hence, storing them would generate further savings in 
computational effort. In this paper, we extend the previous data structure to include the above mentioned information. 
TS selects the best move, 1IMbest, from S to its best neighbours S′ requiring the whole neighbourhood N(S) to be re-
evaluated after each iteration. The neighbourhood N(S) consists of v(v + 1)/2 pairs of routes which is very expensive to 
compute for large-sized problems. The set of best candidate moves N′(S) in N(S), which consists of the best 1IMpq 
moves for all 1 ≤ p < q ≤ v, can strategically be determined and updated using two levels of memory structures. The aim 
is to identify the changes and intelligently update the candidate list of 1IMpq moves with the minimal effort without 
scarifying the quality of the 1IMbest move. At each iteration, we are able to find exactly the best move rather than an 
inferior one which can be obtained by random sampling of the neighbourhood. Hence, the importance of the DS 
structure can not be over-emphasized to achieve efficient TS implementations.
There are two levels of memory structures. The first level stores the costs of all 1-interchange moves, 1IM, in the 
neighbourhood of S. For instance, the cost 
 of serving a route, πp, without a customer, i, as evaluated in (8.7), 
can be stored in a cell COST-WITHOUT (p, i) of a v × n matrix. This value will remain unchanged once computed, 
during all the (1,0) and (0,1) processes of evaluating the cost of shifting customer i into all other different routes, for at 
least the current iteration. The COST-WITHOUT matrix can also help to evaluate quickly the cost of (1,1) move with a 
little extra effort as explained earlier. The memory concept is not only used for the move costs but can be extended to 
store other useful information on 
, such as its current load, its type, and its status in terms of capacity and time 
restrictions.
The second level of memory structure is mainly used to identify the set of all the best v(v + 1)/2 moves, 1IMpq, 
obtained when all v(v+1)/2 pairs of route sets, (Rp,Rq) for all p < q ∈ V, are considered by the 1-interchange mechanism 
to generate the neighbourhood N(S) after each iteration. This set of best 1IMpq moves defines the list of elite candidate 
solutions, N′(S), in the neighbourhood. In Step 2 of the TS procedure, we select the best move, 1IMbest, from the set of 
best moves and apply it to the current solution S to obtain the next solution S′∈ N′(S). At this stage, S′ is exactly the best 
solution in N′(S) and in N(S). Once, the best move, 1IMbest, is performed S′ becomes the current solution for the next 
iteration. It can be seen after performing the best move that the change between S and S′ is only in one pair of routes, 
say (Rp, Rq), and the other routes remain intact. Consequently, 2 × (v 1) pairs of route sets, (Rp, Rm) and (Rm, Rq) (for 
all m ∈ V, m ≠ p, m ≠ q), are necessary to be re-evaluated in order to update the candidate list of best neighbouring 
solutions.
Having defined the set of elite moves and solutions, we shall describe our data structure (DS) to update the candidate 
list of moves and N′(S) with a small number of re-evaluations. The data structure DS consists of two matrices. 
DSTABLE takes the form of a v × v matrix, whereas DSINF is a {v(v + 1)/2} × A matrix where 'A' is the number of 
attributes required to identify a move in DS. Each entry of the top triangular part of DSTABLE is used to store the best 
change in the objective value associated with one of the v(v+1)/2 pairs of routes (Rp, Rq). For example, DSTABLE(p, q) 
stores ∆pq associated with the best 1IMpq between the pair (Rp, Rq), or a large positive value if no such move exists. 
The lower triangular part DSTABLE(q, p) is used to
  
< previous page
page_142
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_142.html2009-7-7 16:57:20

page_143
< previous page
page_143
next page >
Page 143
store a positional index l associated with the pair (Rp, Rq) in the set of possible pair combinations {1,  . . ., v(v + 1)/2}. 
Note that the determination of the index needs special care as the number of vehicles may increase during the 
computation. Increasing the vehicle number option was not considered in Osman's data structure whereas it is of a 
primary importance in the VFM.
In this study, we have used a value of six for 'A' indicating that six attributes are to be saved in DSINF. For instance, the 
best move, 1IMpq, between Rp and Rq is identified by its ∆pq value in DSTABLE, the index l associated with the (Rp, 
Rq) pair that is used to identify other attributes in DSINF as follows: DSINF(l, 1) = l, DSINF(l, 2) = p, DSINF(l, 3) = q, 
DSINF(l, 4) = i, DSINF(l, 5) = j, DSINF(l, 6) = ∆pq. Note that both DSINF(l, 1) and DSINF(l, 6) could be ignored as 
they are already recorded in DSTABLE, but are introduced for convenience. If ∆pq, is associated with (1,0) or (0,1) 
operators, then we replace the values of i or j by 
 in DSINF so that we can identify easily the type of the operators 
applied to this move. The columns in DSINF can be increased easily to store more information, if necessary.
The management of DS occurs in Step 1 of the TS procedure. At the first iteration, all the moves in the neighbourhood N
(S) are evaluated and the best moves are stored in the appropriate matrices. When the TS procedure returns to this step, 
the DSTABLE matrix is updated considering only the 2 × (v 1) pairs of routes that may contain improved moves due to 
performing the previous move. After this quick update, DSTABLE is scanned to identify the best move, 1IMbest, in N′
(S) for the next iteration. 1IMbest is identified by comparing the ∆pq stored values of the 1IMpq moves. The attributes 
of the identified move are then retrieved from DSINF. The corresponding new routes, 
 and 
, are further checked 
for possible improvements applying the 2-OPT or 3-OPT post optimization procedures of Lin[Lin65]. After the post 
optimization procedure is applied, this best move, 1IMbest, is performed to update the current solution. The tabu list is 
also updated before the search continues for another iteration. The selection of the best moves based on the above data 
structure has an advantage over the random sampling approach as it is performed with the minimal computational effort. 
Random sampling is normally used to reduce the computational effort as the systematic search is expensive to use 
without a data structure. Consequently, good moves may be missed and can not be detected unless a systematic search is 
used.
Tabu List
TABL takes the form of a (v + 1) × n matrix (v rows: one for each route set Rp, one for the empty route; n columns: one 
per customer). In the tabu list, we store some attributes of the best 1-interchange move, 1IMbest which was accepted to 
transform S to S′∈ N (S). To prevent reversing the accepted move from returning back to S, its attributes are stored in 
TABL for t iterations, in the hope that we would be in a different region. The reverse move can be regenerated by 
returning immediately i to Rp and j to Rq. Let us assume that the current iteration number is nbiter and the best 1-
interchange move involves the pair of customers (i, j) in (Rp, Rq). The return to previous solutions during the next t 
iterations can be prevented by storing in TABL the future value (nbiter + t) in both TABL(i, p) and TABL(j, q) entries. 
After nbiter+t iterations, customers i and j are allowed to return to Rp and Rq, respectively.
  
< previous page
page_143
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_143.html2009-7-7 16:57:21

page_144
< previous page
page_144
next page >
Page 144
Tabu Condition
One way to prevent cycling is to store exactly all previously accepted solutions, but this approach can be 
computationally very expensive for checking and maintaining solutions. Another approach is to use hashing functions 
(Hasan and Osman[HO95]) to represent these solutions. We found in many implementations that tabu conditions based 
on move attributes provided good results. Therefore, we did not try other types of tabu conditions and the reader may 
refer to Osman[Osm95a] for a comprehensive computational study on the effect of different types of tabu conditions on 
the quality of solution.
In this paper, we implemented one type of tabu condition. A 1-interchange move, 1IM, is considered tabu if i is returned 
to Rp and j is returned to Rq. At iteration nbiter, we can check the status of a move 1IM involving the pairs (i, j) and 
(Rp, Rq) using the simple test in equation (8.19) and the information in TABL as follows. A move is considered tabu if
TABL is initialized with zeros so that all moves are not considered tabu by the test.
Tabu List Size
Tabu list size seems to be related to tabu conditions, selection strategy of neighbours and problem characteristics, such 
as the number of customers, the number of vehicles and the ratio of the total required demands to the available 
capacities. Osman[Osm93] established such linkages and used a statistically derived formula to find an estimate of the 
tabu list size, t. Then, in a systematic way, the tabu list sizes were varied every 2 × t iterations over three different 
values (±10% centred at the estimate). In this paper, a simpler way is used with a static value of t such as 
 for p 
varying between 2 and 7 to watch the best range between these extremes. The aim is to identify the value of t for other 
problems. A small value may lead to cycling with poor results while a large value may lead to deterioration in solution 
quality caused by forbidding too many moves.
Aspiration Criterion
Tabu conditions are based on stored attributes of moves; it may be possible to forbid wrongly unvisited solutions which 
share these attributes. Aspiration criteria are tests to correct such prevention. We use the following aspiration criterion. 
If 1IM is a tabu move but gives a new solution which is better than the best found so far, then we drop its tabu status. 
Therefore, a move from S to S′ is considered admissible, if it is a non-tabu move, or a tabu move which passed an 
aspiration level criterion, C(S′) < C(Sbest).
Stopping Criterion
Our TS procedure is terminated after a pre-specified number of iterations MAXBEST= 5 × n is performed after the best 
iteration number, bestiter, at which the best solution was found without finding any improvement.
8.5 
Computational Experience
It is a common practice to compare the performance of a heuristic with that of existing techniques on a set of 
benchmarks (test problems) in terms of solution quality and
  
< previous page
page_144
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_144.html2009-7-7 16:57:21

page_145
< previous page
page_145
next page >
Page 145
computer requirements. Other measures such as simplicity, flexibility, ease of control, interaction and friendliness are 
also of interest but are not considered here.
Test Problems
A set of 20 test problems are used. These problems were proposed by Golden, Assad, Levy and Gheysens[HN93]. They 
vary in size from 12 to 100 customers, there is no restrictions on maximum travel time (Tk = ∞, for all k ∈ K) nor on the 
number of vehicles. There are at least three types of vehicles and can be up to six for some problem instances, i.e., k 
varies between 3 and 6. The unit running cost, αk, and βk are set to a constant equal to one (αk = 1, βk = 1, for all k ∈ 
K) to be able to compare with published results which do not consider these two factors. However, the fixed cost, Fk, 
increases with the vehicle capacities.
Quality of Solutions
A common measure to decide on the quality of a heuristic solution is the relative percentage deviation from the optimal 
solution. However, if an optimal solution can always be easily obtained, we do not need to use a heuristic. Due to the 
difficulty of finding an optimal value, either a good lower bound from a linear relaxation of a mixed integer formulation 
or the best known solution can be used to replace the optimal value in the analysis. For each test problem, the relative 
percentage deviation over the best known solution is computed as follows:
In this study, we also report the actual solution, the average relative percentage and standard deviations, and the number 
of times a particular heuristic found the best solution. In Table 8.1, the actual solutions for our heuristics are given. The 
first and second columns give the problem number and the number of customers, respectively. The results of RPERT 
are in Salhi and Rand[SR93], our results of the MRPERT (modified RPERT) and the TSVFM (tabu search) with their 
respective CPU computation time in seconds on VAX 4500 are next in the table. The 'Old Best' are the cost of the best 
known solutions in the literature. These results can be found in either Golden, Assad, Levy and Gheysens[GALG84] or 
Gheysens, Golden and Assad[GGA84], Desrochers and Verhoog[DV91], and Salhi and Rand[SR93]. The final column 
contains the 'New Best' known solutions adjusted to reflect our findings. The new best solutions are indicated in bold 
while the references of the old best solutions are put between brackets [.]. It can be seen from Table 8.1 that we have 
found seventeen new or equal best known solutions for the twenty test problems using either the TSVFM or MRPERT 
heuristics. In fact, twelve out of sixteen are new best known solutions and the other five problems are equal to the 
published ones. Looking at the performance of each individual heuristic, we notice that the tabu search heuristic, 
TSVFM, has obtained fifteen best known values out of which ten are new best results. The modified heuristic, 
MRPERT, has obtained six best known solutions out of which two are new best results. MRPERT provides an 
improvement over RPERT in thirteen problems and is equal in the remaining instances.
  
< previous page
page_145
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_145.html2009-7-7 16:57:22

page_146
< previous page
page_146
next page >
Page 146
Table 8.1 Comparison of the best known solutions
No.
Size
RPERT
MRPERT
CPU
TSVFM
CPU
Old Best
New Best
1
12
614
606
0.3
602
3
602[TL92]
602 [*]
2
12
722
722
0.2
722
2
722[GALG84]
722 [*]
3
20
1003
971.95
0.7
971.24
5
965[HO95]
965
4
20
6447
6447.80
0.2
6445.10
6
6446[GALG84]
6445 [*]
5
20
1015
1015.13
0.2
1009.15
5
1013[GALG84]
1009 [*]
6
20
6516
6516.56
0.2
6516.56
4
6516[SR93]
6516 [*]
7
30
7402
7377
4.6
7310
15
7298[GALG84]
7298
8
30
2367
2352
3.4
2348
17
2349[GALG84]
2348 [*]
9
30
2209
2209
0.8
2209
14
2209[SR93]
2209 [*]
10
30
2377
2377
0.6
2363
14
2368[GALG84]
2363 [*]
11
30
4819
4787
0.4
4755
19
4763[GALG84]
4755 [*]
12
30
4092
4092
0.3
4092
10
4092[SR93]
4092 [*]
13
50
2493
2462.01
7.8
2471.07
62
2437[GALG84]
2437
14
50
9153
9141.69
9.3
9125.65
71
9132[GALG84]
9125 [*]
15
50
2623
2600.31
2.8
2606.72
46
2621[TL92]
2600 [*]
16
50
2765
2745.04
1.2
2745.01
35
2765[SR93]
2745 [*]
17
75
1767
1766.81
6.3
1762.05
85
1767[SR93]
1762 [*]
18
75
2439
2439.40
4.5
2412.56
116
2432[GALG84]
2412 [*]
19
100
8751
8704.20
8.1
8685.71
289
8700[TL92]
8685 [*]
20
100
4187
4166.03
61.1
4188.73
306
4187[SR93]
4166 [*]
[X]:
x indicates the reference in which the best known solution is reported and, * refers to this paper.
RPERT:
Interactive procedure of Salhi and Rand[SR93].
MRPERT:
The modified RPERT in this paper.
TSVFM:
The Tabu search procedure.
CPU:
The CPU time in seconds on VAX 4500.
 
  
< previous page
page_146
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_146.html2009-7-7 16:57:23

page_147
< previous page
page_147
next page >
Page 147
Table 8.2 The relative percentage deviation above the best known solution.
No
Size
(MGT- OrOPT)5
LB5+ VRP
MBSA RPERT MRPERT TSVFM OLBBEST
1
12
3.32
2.65
0.00
1.99
0.66
0.00
0.00
2
12
0.00
5
0.00
0.00
0.00
0.00
0.00
3
20
0.10
0.31
0.10
3.93
0.72
0.62
0.00
4
20
7.52
0.09
1.70
0.03
0.03
0.00
0.01
5
20
0.39
2.08
2.27
0.59
0.59
0.00
0.39
6
20
7.02
0.03
0.01
0.00
0.00
0.00
0.00
7
30
1.24
0.76
1.69
1.42
1.08
0.16
0.00
8
30
0.80
0.59
0.21
0.80
0.17
0.00
0.04
9
30
0.49
2.39
1.26
0.00
0.00
0.00
0.00
10
30
0.29
1.05
2.28
0.59
0.59
0.00
0.21
11
30
0.16
0.69
2.56
1.34
0.67
0.00
0.16
12
30
1.07
0.97
3.39
0.00
0.00
0.00
0.00
13
50
0.04
-
0.28
2.29
1.02
1.39
0.00
14
50
0.06
0.32
0.06
0.29
0.17
0.00
0.06
15
50
1.53
0.80
0.80
0.88
0.00
0.23
0.08
16
50
2.80
-
2.33
0.72
0.14
0.00
0.72
17
75
1.19
-
6.52
0.28
0.28
0.00
0.28
18
75
0.82
-
3.10
1.11
1.11
0.00
0.82
19
100
0.41
-
0.17
0.75
0.21
0.00
0.17
20
100
0.69
-
1.99
0.50
0.00
0.55
0.50
Average deviation
1.50
0.91
1.54
0.88
0.36
0.14
0.21
Standard deviation 2.10
0.83
1.59
0.94
0.39
0.35
0.28
# best solutions
1
1
2
4
7
14
8
(MGT-OrOPT)5:
Route first-cluster second algorithm of Golden et al. [GALG84].
LB5+VRP:
Cluster First-route Second algorithm of Gheysens et al. [GGA84].
MBSA:
Matching based savings algorithms of Desrochers and Verhoog[DV91].
Bold:
shows the worst instance performance of each algorithm and the best average over all problems.
Others:
As explained in Table 8.1.
 
  
< previous page
page_147
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_147.html2009-7-7 16:57:24

page_148
< previous page
page_148
next page >
Page 148
In Table 8.2, we have reported the relative percentage deviation over the new best known solutions for most of the state-
of-the-art algorithms in the literature. We have included those heuristics which have produced at least one of the best 
known solutions. The first row contains the headers of the considered heuristics: (MGT-OrOPT)5 is the route first-
cluster second heuristic based on route partitioning approach by Golden, Assad, Levy and Gheysens[GALG84]; LB5
+VRP is the cluster first-route second heuristic based on the generalized assignment approach by Gheysens, Golden and 
Assad[GGA84]; MBSA is the matching based savings heuristic by Desrochers and Verhoog[DV91] and those 
remaining are our own algorithms. It should be noted that the relative percentage deviations for the MBSA column are 
the best relative percentage deviations from several variants (CM, OOM, ROM, ROM-γ, ROM-ρ) of the matching 
based savings heuristic. Desrochers and Verhoog[TL92] did not report the objective solution values, we had to estimate 
these results from their best percentage deviations in order to compute the data in the MBSA column.
It can be seen from Table 8.2 that TSVFM has the best performance as it produces the largest number of best known 
solutions and the smallest average relative percentage deviation of 0.14%. The MRPERT, however, produces the second 
largest number of best solutions, has the second average percentage deviation and the least worst percentage deviation 
value. These best average and worst relative percentage deviations for each heuristic are shown in bold in Table 8.2. 
The statistical analysis demonstrates the robustness of our heuristics when compared to the best available algorithms in 
the literature.
Computational Effort
The computer programs are coded in Fortran 77 and run on a VAX 4500 computer. In the previous section, we have 
shown that TSVFM is the best heuristic; however, there is a cost for a such good performance. In Table 8.1, the CPU 
time for both TSVFM and MRPERT heuristics are reported and the average CPU time over all the twenty instances for 
TSVFM and MRPERT are 56.2 and 5.7 CPU seconds respectively.
Comments
In this section, we discuss observations and issues related to our experimental results. In Table 8.1, we observe that 
MRPERT has produced better solutions than TSVFM in three problems, namely: 13, 15 and 20. We attempted to see if 
any link may exist between this failure and the existence of many vehicle types since problem 13 has six different 
vehicle types. We are not yet able to justify the link since TSVFM has produced results better than or equal to MRPERT 
for other problems of similar types. Further, both methods have failed to find the best known solutions for three 
problems. The failure may be attributed to the combinatorial structure of these problems. In our view this observation 
may need a closer look. A further study is needed to find a concrete answer for this failure.
The second observation is that there is a good range for t values which vary with the size of problems. Figure 8.2 shows 
the average relative and standard percentage deviations of the best solutions for different values for the tabu list sizes, 
 for p = 2 to 7, from the best known solutions for all the twenty test problems. From the figure, it can be seen 
that the performance of tabu search depends on the values of t. A good range for t values seem to lie between 
 and 
. Outside this range t is
  
< previous page
page_148
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_148.html2009-7-7 16:57:24

page_149
< previous page
page_149
next page >
Page 149
either too small or too big. In the former case, cycling would occur and relatively bad solutions would be generated. In 
the later case, many moves are prevented leading to unvisited solutions that may be attractive and longer computation 
time is needed to find good solutions. It was found that when using the right interval of the tabu list size value, the total 
CPU time required by the TS algorithm was reduced by a half of that needed for other values of p. To derive a better 
range for tabu list size, we could generate a random permutation of 
 values with p = 3, 4 and 5 and let t takes each 
value in this permutation. Then t can be kept at each value, say T, for 2 × T iterations before it can be changed to take 
the other values. Varying tabu list size values has been shown to be effective in Taillard[Tai93] and Osman[Osm93]. 
Other recent approaches based on a reverse elimination method or the reactive tabu method can be used and merit 
further investigations. These approaches are reviewed in Battiti[Bat96] and Voß[Vos96].
Figure 8.2 
Effect of tabu list size on the quality of solution.
The initial starting solution was generated by solving a VRP problem using one vehicle type at a time by Salhi and Rand 
VRP's heuristic. The best VRP solution is then used to start both MRPERT and TSVFM for further improvements. No 
other starting solutions were considered for convenience, simplicity and consistency. It may be possible to use other 
VFM heuristics and lower bounds on the fleet composition
  
< previous page
page_149
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_149.html2009-7-7 16:57:25

page_150
< previous page
page_150
next page >
Page 150
to generate good initial VFM solution, and then feed them into TSVFM or RPERT heuristics to obtain a more refined solution 
with probably less computational effort. The fleet compositions, which are used by the Salhi and Rand's VRP heuristic together 
with its saving multipliers to generate initial solutions, are given in Table 8.3. The best fleet compositions for each problem 
which are obtained either by MRPERT or TSVFM are also shown in Table 8.3. It should be mentioned that since MRPERT is 
an iterative procedure and depends on the starting solution, changing the savings multipliers and the vehicle types may lead to 
different initial solutions.
Table 8.3 Details of initial and final results for both TSVM and MRPERT heuristics.
No Size
VRP (s, V C)
MRPERTf
VRP (s, V C)
TSVFMf Bestiter
1
12
1:C
B2C2
1:C
B2C2
44
2
12
1:C
A3C
1:C
A3C
10
3
20
1.1:E
ABDE2
1.0:E
ABDE2
67
4
20
0.8:A
A6
0.8:A
A6
67
5
20
0.6:E
E3
0.6:E
E3
73
6
20
0.7:A
A6
0.7:A
A6
0
7
30
0.1:B
AB7CD
0.1:B
AB4C3D
155
8
30
1:D
C2D2
1:D
C2D2
122
9
30
1.2:E
DE3
1.2:E
DE3
1
10
30
1.8:D
C2D4
1.8:D
C2D4
10
11
30
0.1:C
BC5
0.1:C
BC5
144
12
30
0.9:E
E6
0.9:E
E6
0
13
50
0.2:F
A4B4F4
0.2:F
A2BEF4
165
14
50
1:A
A7B
1.4:A
A7B
276
15
50
1:B
A8B4
0.5:B
A4B6
154
16
50
1.2:B
B10
1.3:B
B10
55
17
75
0.6:C
C7
0.6:C
C7
2
18
75
0.5:C
B2C13
0.5:C
B4C12
102
19
100
1:A
A15
1:A
A15
419
20
100
1:A
A4B9
1.9:B
A25
491
VRP: 
(s, VC) denotes that the initial solution is obtained by using a vehicle capacity of VC, a savings 
multiplier s at the routing stage of Salhi and Rand VRP heuristic.
f:
A,. . . , F.denote the proposed fleet composition. For instance in problem 1, B2C2 means that two 
vehicles of type B and two vehicles of type C are proposed to be the final solution suggested by the 
modified, MRPERT, and the tabu search, TSVFM, heuristics
Bestiter: 
the iteration number at which the best solution is found using TSVFM.
 
  
< previous page
page_150
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_150.html2009-7-7 16:57:26

page_151
< previous page
page_151
next page >
Page 151
Finally, for each test problem, the iteration number at which the best TSVFM solution is found, is listed in Table 8.3. 
The solution quality based on the MAXBEST stopping criterion would require more computation time, since it 
performs 5 × n to confirm that the solution can not be improved further. However, this criterion has the advantage of 
allowing time requirements to be varied according to problems structure and avoiding underestimated total number of 
iterations. Moreover, allowing a larger value for MAXBEST would give even better results. Other stopping criteria 
based on a total number of iterations, or a cut-off time can also be used to terminate the search.
8.6 
Conclusion and Future Directions
The vehicle routing problem with different vehicle capacities is investigated. A method based on tabu search is 
developed using an appropriate data structure. Tabu search has obtained interesting results and new best known 
solutions were found. The modified route perturbation procedure (MRPERT) enlarged the neighbourhood of RPERT by 
performing moves that were considered infeasible by the old RPERT, in addition to allowing MRPERT to run for a few 
entire cycles until no further improvement is found at the last cycle. With these modifications, MRPERT has produced 
significant improvements over RPERT in all the cases, but required extra computing time. It also found two new best 
known solutions. This makes the total number of new best known solutions obtained by both procedures reach twelve 
out of twenty problems. Tabu search and the modified route perturbation algorithm are also compared to the best 
methods in the literature. It was found that the tabu search procedure produces the smallest average relative percentage 
deviations and the largest number of best known solutions. Our aim of this study was not to show how much we can 
stretch the limit of tabu search, but to present it as an effective tool which still has a great potential to be discovered.
There is a number of research avenues that deserve further investigations. It seems that TS has failed in problems with a 
large number of vehicle types. A further study is needed to establish the reasons for such a failure. One way would be to 
extend the neighbourhood by increasing the value of λ from one to two in the λ-interchange mechanism, or considering 
more than two routes simultaneously. Other tabu search features such as intensification, diversification, and a dynamic 
tabu list size can be explored. Combining in a hybrid way more than one metaheuristic as in [Osm95b, Osm93, OC94, 
TOVS93, TOS94], investigating the effect of good initial solutions on the quality of solution, introducing limitations on 
the maximum available number of vehicles in each type, implementing the data structure within MRPERT, testing other 
metaheuristics are parts of an on-going research on this problem. Further investigations on the above issues will be 
reported.
8.7 
Acknowledgement
The authors would like express their gratitudes to the referees who provided a number of constructive comments that 
helped to improve the presentation of the paper.
  
< previous page
page_151
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_151.html2009-7-7 16:57:27

page_152
< previous page
page_152
next page >
Page 152
References
[AL96] Aarts E. H. L. and Lenstra J. K. (1996) Local Search in Combinatorial Optimization. John Wiley, Chichester.
[Bat96] Battiti R. (1996) Towards self-tuning heuristics. In Rayward-Smith V. J., Osman I. H., Reeves C. R., and Smith 
G. D. (eds) Modern Heuristic Search Methods. John Wiley, Chichester.
[BR88] Bookbinder J. H. and Reece K. E. (1988) Vehicle routing considerations in distribution and system design. 
European Journal of Operational Research 37: 204213.
[BT91] Beaujon G. L. and Turnquist M. A. (1991) A model for the fleet sizing and vehicle allocation. Transportation 
Science 25: 1945.
[CR96] Chiang W.-C. and Russell R. (1996) Simulated annealing metaheuristics for the vehicle routing problem with 
time windows. Annals of Operations Research 63. Baltzer, Basel.
[CW64] Clarke G. and Wright J. W. (1964) Scheduling of vehicles from a central depot to a number of delivery points. 
Operations Research 12: 568581.
[DV91] Deroschers M. and Verhoog T. W. (1991) A new heuristic for the fleet size and mix vehicle routing problem. 
Computers and Operations Research 18: 263274.
[EL96] Eglese R. W. and Li L. (1996) Heuristics for arc routing with a capacity constraint and time deadline. In Osman 
I. H. and Kelly J. P. (eds) Metaheuristics, Theory and Applications. Kluwer, Boston.
[FM88] Ferland J. A. and Michelon P. (1988) The vehicle routing with multiple types. Journal of Operational Research 
Society 39: 577583.
[GALG84] Golden B. L., Assad A., Levy L., and Gheysens F. G. (1984) The fleet size and mix vehicle routing 
problem. Computers and Operational Research 11: 4966.
[GGA84] Gheysens F., Golden B. L., and Assad A. (1984) A comparison of techniques for solving the fleet size and 
mix vehicle routing problem. OR Spektrum 6: 207216.
[GGA86] Gheysens F. G., Golden B. L., and Assad A. (1986) A new heuristic for determining fleet size and 
composition. Mathematical Programming Studies 26: 233236.
[GHL94] Gendreau M., Hertz A. and Laporte G. (1994) A tabu search heuristic for the vehicle routing problem. 
Management Science 40: 12761290.
[Glo86] Glover F. (1986) Future paths for integer programming and links to artificial intelligence. Computers and 
Operations Research 13: 533549.
[Glo95a] Glover F. (1995) Tabu search fundamentals and uses. Technical report, University of Colorado Boulder. 
Working Paper.
[Glo95b] Glover F. (1995) Tabu thresholding: Improved search by non-monotonic trajectories. ORSA Journal on 
Computing 7: 426442.
[HO95] Hasan M. and Osman I. H. (1995) Local search strategies for the maximal planar graph. International 
Transactions in Operational Research 2: 89106.
[Lin65] Lin S. (1965) Computer solutions to the travelling salesman problem. Bell System Technology Journal 44: 
22452269.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_152.html（第 1／2 页）2009-7-7 16:57:28

page_152
[LO95] Laporte G. and Osman I. H. (1995) Routing problems: A bibliography. Annals of Operations Research 61: 
227262.
[LO96] Laporte G. and Osman I. H. (1996) Metaheuristics in combinatorial optimization. Annals of Operations 
Research 63. Baltzer, Basel.
[LR81] Lenstra J. K. and Rinnooy Kan A. H. G. (1981) Complexity of vehicle routing and scheduling problem. 
Networks 11: 221227.
[OC94] Osman I. H. and Christofides N. (1994) Capacitated clustering problems by hybrid simulated annealing and 
tabu search. International Transactions in Operational Research 1: 317336.
[OK96a] Osman I. H. and Kelly J. P. (1996) Metaheuristics. Theory and
  
< previous page
page_152
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_152.html（第 2／2 页）2009-7-7 16:57:28

page_153
< previous page
page_153
next page >
Page 153
Applications. Kluwer, Boston.
[OK96b] Osman I. and Kelly J. (1996) Metaheuristics. an overview. In Osman I. H. and Kelly J. P. (eds) 
Metaheuristics, Theory and Applications. Kluwer, Boston.
[OL96] Osman I. H. and Laporte G. (1996) Metaheuristics: A bibliography. Annals of Operations Research on 
Metaheuristics 63. Baltzer, Basel.
[Osm93] Osman I. H. (1993) Metastrategy simulated annealing and tabu search algorithms for the vehicle routing 
problem. Annals of Operational Research 41: 421451.
[Osm95a] Osman I. H. (1995) Heuristics for the generalised assignment problem. simulated annealing and tabu search 
approaches. OR Spektrum 17: 211225.
[Osm95b] Osman I. H. (1995) An introduction to metaheuristics. In Lawrence M. and Wilsdon C. (eds) Operational 
Research Tutorial Papers. Operational Research Society, Birmingham.
[PKGR96] Potvin J.-Y., Kervahut T., Garcia B., and Rosseau J. (1996) A tabu search heuristic for the vehicle routing 
problem with time windows. ORSA Journal on Computing, Forthcoming.
[Ree93] Reeves C. R. (1993) Modern Heuristic Techniques for Combinatorial Problems. Blackwell, Oxford.
[Ron92] Ronen D. (1992) Allocation of trips to trucks operating from a single terminal. Computers and Operations 
Research 19: 451451.
[RR96] Rego C. and Roucairol C. (1996) A parallel tabu search algorithm using ejection chains for the vehicle routing 
problem. In Osman I. H. and Kelly J. P. (eds) Metaheuristics, Theory and Applications. Kluwer, Boston.
[RS95] Rayward-Smith V. J. (1995) Applications of Modern Heuristic Methods. Alfred Waller, Oxford.
[SR87] Salhi S. and Rand G. K. (1987) Improvements to vehicle routing heuristics. Journal of Operational Research 
Society 38: 293295.
[SR93] Salhi S. and Rand G. K. (1993) Incorporating vehicle routing into the vehicle fleet composition problem. 
European Journal of Operational Research 66: 313330.
[SS95] Salhi S. and Sari M. (1995) A heuristic approach for the multi-depot vehicle fleet mix problem. Technical 
report, School of Mathematics and Statistics, University of Birmingham, U.K.
[SSST92] Salhi S., Sari M., Saidi D., and Touati N. A. C. (1992) Adaptation of some vehicle fleet mix heuristics. 
OMEGA 20: 653660.
[Tai93] Taillard E. D. (1993) Parallel iterative search methods for vehicle routing problems. Networks 23: 661673.
[TOS94] Thangiah S., Osman I., and Sun T. (1994) Metaheuristics for the vehicle routing problems with time windows. 
Technical Report UKC/IMS/OR94/4, Institute of Mathematics and Statistics, University of Kent, Canterbury.
[TOVS93] Thangiah S., Osman I., Vinayagamoorthy R., and Sun T. (1993) Algorithms for vehicle routing problems 
with time deadlines. American Journal of Mathematical and Management Sciences 13: 323357.
[Vos96] Voss S. (1996) Observing logical interdependencies in tabu search. methods and results. In Rayward-Smith V. 
J., Osman I. H., Reeves C. R., and Smith G. D. (eds) Modern Heuristic Search Methods. John Wiley, Chichester.
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_153.html（第 1／2 页）2009-7-7 16:57:29

page_153
< previous page
page_153
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_153.html（第 2／2 页）2009-7-7 16:57:29

page_155
< previous page
page_155
next page >
Page 155
9 
Simulated Annealing Solutions for Multi-objective Scheduling and Timetabling
Kathryn A. Dowsland
Abstract
Simulated annealing has been successfully applied to a variety of scheduling and timetabling problems. In cases where 
multiple objectives are encountered, the traditional approach is to incorporate them all in a single cost function with 
appropriate weights. If the problem is so highly constrained that feasible solutions are difficult to obtain then the 
solution space is frequently relaxed and violation of constraints is penalised by further terms in the cost function. This it 
may well be difficult to find appropriate values for all the weights. This is particularly true if there is a distinct hierarchy 
of importance. This paper suggests an alternative approach which deals with groups of objectives in distinct phases. The 
factors which should be taken into account when deciding how to allocate the objectives to phases are considered, and 
the approach is illustrated using two successful applications.
9.1 
Introduction
Schedules and timetables cover a broad spectrum of human activity, governing our school days, leisure activities and 
travel, and affecting our working lives through shift patterns, production plans and delivery schedules. In an industrial 
setting the main objective when solving such problems is usually some form of cost minimisation. However, where the 
task is essentially one of scheduling people, personal preferences or concepts of fairness may become major factors in 
judging the quality or feasibility of a particular schedule. The result is frequently a formulation involving hierarchies of 
objectives and a mixture of hard and soft constraints, of a size and complexity which renders exact optimisation 
procedures ineffective. Furthermore the precise nature of the objectives and constraints may vary over time, 
necessitating a flexible approach. They may also include subjective criteria, making it desirable to produce a number
Modern Heuristic Search Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_155
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_155.html2009-7-7 16:57:29

page_156
< previous page
page_156
next page >
Page 156
of different schedules so that the final choice can be left to the decision maker. It is, therefore, not surprising that 
metaheuristics such as simulated annealing and tabu search have proved to be popular and successful solution 
techniques for problems of this type.
Ideally, the search should be conducted over the space of feasible solutions, but for many timetabling and scheduling 
problems simply finding a feasible solution is often a difficult task and it is usual practice to relax some of the 
constraints and to use a cost function which is a weighted linear combination of constraint violations and original 
objectives. As the number of different cost function elements grows it becomes increasingly difficult to balance the 
weights appropriately and this becomes critical in situations where some terms relate to hard constraints, which must be 
satisfied, while others are concerned with soft constraints and secondary objectives. In view of these difficulties this 
paper offers an alternative strategy in which different groups of cost function terms are considered in different phases, 
so that costs minimised early on are not allowed to increase in subsequent phases. This approach is illustrated with 
reference to two case studies based on successful simulated annealing solutions to two scheduling problems which arise 
annually at Swansea University.
9.2 
Local Search, Simulated Annealing and Scheduling Problems
Simulated annealing (SA) is a local search algorithm which avoids becoming trapped in local optima by accepting some 
uphill moves in a controlled manner. A full description of the method is given in Figure 9.1. In order to implement SA 
in the solution of a particular problem it is necessary to make a number of decisions concerning the way in which the 
problem is to be defined within the local search framework and the details of the parameters which control the algorithm 
itself. It is well known that both types of decision can have a considerable effect on the efficiency of the search and the 
quality of solutions produced. The problem specific decisions involve defining a solution space and neighbourhood 
structure, and associating an appropriate cost with each feasible solution. The generic decisions are mainly concerned 
with the cooling parameter, t, and define its starting and finishing values and the way it is to be reduced.
Scheduling problems are particularly amenable to solution by local search as they frequently give rise to very natural 
neighbourhoods. These are either based on changing the time, or other attribute, of a single event, or on swapping the 
attributes of two different events. For example Abramson [Abr91], [AD93] uses the first type of neighbourhood 
structure in his SA solution to the school timetabling problem. He defines his feasible solutions as the set of all 
allocations of classes to time-periods, teachers and rooms, and defines a neighbourhood move as the result of changing 
one of the attributes of a single class. Conversely Wright [Wri91] uses a swap neighbourhood in his application of local 
search to an umpire scheduling problem, completely swapping the events allocated to any two umpires at each iteration. 
Once such an algorithm has been implemented with a suitable solution space and neighbourhood structure it is relatively 
easy to modify the cost function or some of the constraints to deal with any changes or crises which may arise, and this 
flexibility is frequently cited as a distinct advantage by those electing to solve their problems in this way. It is also 
worth noting that many problems of this type occur at most once or twice a year and do not require
  
< previous page
page_156
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_156.html2009-7-7 16:57:30

page_157
< previous page
page_157
next page >
Page 157
Select an initial solution s0; 
Select an initial temperature t0 > 0; 
Select a temperature reduction function α; 
Repeat 
    Repeat 
        Randomly select s ∈ N(s0); 
        δ = f(s)  f(s0); 
        if δ < 0 then 
              s0 = s 
        else 
              generate random x ∈ exp(δ/t) then 
                   s0 = s; 
              endif 
        endif 
    until iteration-count = nrep; 
    t = α(t); 
Until stopping condition = true. 
s0 is the approximation to the optimal solution.
 
Figure 9.1 
Simulated annealing for minimisation
immediate solution. Thus the major criticism against local search, that it requires a considerable amount of time to 
produce high quality solutions, is not an important factor. Many applications allow an overnight or longer run in order 
to get the best possible results from the search.
Many successes have been reported using a cost function which combines all the objectives and constraints into a single 
weighted sum. For example Abramson [Abr91], [AD93] takes this approach with the school timetabling problem and 
Forbes et al.[FHKW94] use it successfully to schedule the buses run by Brisbane City Council. However, other 
researchers suggest that it can be difficult to set the weights appropriately, especially when they involve a mixture of 
hard and soft constraints and it is unlikely that all terms can simultaneously be reduced to zero. For example, Dige et al.
[DLR93] report serious problems in obtaining a feasible solution to their version of the school timetabling problem. 
Wright[Wri91, Wri92, Wri94] has had more success with a number of different multi-objective problems associated 
with the English cricket season but concludes that when selecting weights many different factors such as the relative 
importance of the objectives, their relative difficulty and the way in which they interact with each other and with the 
solution space should all be considered. In the context of tabu search, he also suggests that if binding constraints are 
enforced using large weights then some form of intelligent diversification may be necessary at appropriate points, in 
order to allow the search to accept large increases in the cost
  
< previous page
page_157
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_157.html2009-7-7 16:57:30

page_158
< previous page
page_158
next page >
Page 158
function.
In practice, such an allocation of very high weights to those terms relating to the binding constraints may be the only 
way of guaranteeing feasibility. In a simulated annealing implementation this will mean that at moderate to low 
temperatures this part of the cost function will not be increased. Such an approach effectively deals with the heavily 
weighted terms first and then concentrates on the secondary terms, while ensuring that they are not optimised at the 
expense of primary terms. At first glance it seems that this is almost equivalent to tackling the problem in distinct 
phases, considering only the binding constraints in the first phase, and then reducing the solution space so that only 
feasible solutions are considered while the secondary objectives are optimised. However, in many cases a phased 
approach can be very much more efficient. The remainder of this paper concentrates on two case studies which serve to 
illustrate why this is so, and to highlight some of the problems which may be encountered when implementing a phased 
approach in the solution of a particular problem.
9.3 
Examination Scheduling
We first consider the problem of producing the annual or biannual examination timetable in higher education 
establishments. Although the precise rules governing the examination schedule differ from institution to institution the 
problem is usually one of multiple objectives and a variety of constraints. The most common variant of the problem 
involves fitting a given set of exams into a fixed number of time-slots, in such a way that no student is required to take 
more than one exam at a time, and so that there are sufficient desks or rooms to accommodate all the exams scheduled 
for any one period. In addition, some papers may be subject to time-window constraints, groups of papers may have to 
be scheduled in a pre-specified order and some pairs of exams may have to be scheduled at the same time or at different 
times. Other, secondary, requirements which are considered by many institutions involve spreading the exams evenly 
over the exam period for each individual, placing papers taken by large number of students early in the examination 
period and incorporating further time-window requests which may be preferred but are not regarded as binding 
constraints.
The problem of scheduling the degree exams at Swansea University is typical and involves around 3000 students taking 
600 exams which are to be scheduled within 24 time-slots. The binding constraints are as follows:
there are only 24 time-slots available
no student clashes to be allowed
certain pairs of exams to be scheduled at the same time
certain pairs of exams to be scheduled at different times
certain groups of exams to be scheduled in order
certain exams to be scheduled within time-windows
no more than 1200 students to be involved in any one session
in addition there are two secondary objectives:
minimise the number of exams with over 100 students scheduled after period 10
  
< previous page
page_158
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_158.html2009-7-7 16:57:31

page_159
< previous page
page_159
next page >
Page 159
minimise the number of occurrences of students having exams in consecutive periods.
Finding a feasible solution without considering the secondary objectives is itself a difficult problem. Therefore, the first step in implementing a 
simulated annealing solution method was to decide which constraints should be incorporated into the definition of the solution space and which 
should be penalised in the cost function. These decisions were motivated by the underlying graph colouring model for the problem. This 
involves mapping each exam onto a vertex in a graph and defining an edge between each pair of vertices representing exams which cannot take 
place in the same time period. A feasible colouring, in which vertices are allocated colours so that no two adjacent vertices have the same 
colour, then defines a clash-free timetable with the colours representing the different time-slots. Balakrishnan [Bal91] extended this model to 
include time-windows by adding a set of dummy vertices representing the different time-slots. Using this model the problem at Swansea can be 
regarded as that of finding a colouring of the underlying graph in 24 colours so that orderings and capacity constraints are obeyed and the 
secondary objectives are optimised. Chams et al. [CHdW87] report considerable success using simulated annealing to solve the problem of 
colouring a graph in k colours, and our implementation is based on theirs, with additional restrictions on the solution space and extra terms in 
the cost function. They used the following problem specific decisions:
Solution space:
partitions of vertices into k colour classes
Neighbourhood move:
change of colour of a single vertex
Cost:
number of edges between pairs of vertices in the same colour class
 
The objective is to reduce the cost function to its optimal value of zero.
In our case there are four different types of edges in the underlying model: those representing student clashes, those representing exams which 
cannot go on at the same time for other reasons, those relating to time-window constraints, and those relating to large exams which are 
effectively time-windowed. As some of these represent binding constraints while the latter represents a secondary objective it is necessary to 
consider weighting each type differently, giving four separate terms. Of the remaining features, second order conflict is an objective and is 
therefore included as the fifth term. This leaves the decision as to whether to include the remaining constraints, i.e. desk capacities and 
orderings, in the definition of the solution space, or to include further terms in the cost function.
In any modified local search, such as SA, which aims to be independent of the starting solution, it is important that the solution space remains 
connected under the chosen neighbourhood structure, in order to ensure that the optimal solutions are reachable from any starting point. This 
observation led to a decision to include the ordering constraint in the definition of solution space and to include the desk capacity constraint in 
the cost function, as removing solutions which violate the latter could potentially result in a disconnection.
This results in the following definitions.
Solution space:
allocations of exams to 24 time-slots subject 
to ordering constraints
Neighbourhood move:
change of time-slot for a single exam
Cost:
w1c1 + w2c2 + w3c3 + w4c4 + w5c5 + w6c6
 
  
< previous page
page_159
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_159.html2009-7-7 16:57:32

page_160
< previous page
page_160
next page >
Page 160
where c1 is the number of student clashes, c2 is the number of exams clashing for other reasons, c3 is the number of 
time-window violations, c4 is the number of desk capacity violations, c5 is the number of occurrences of second order 
conflict and c6 is the number of large exams out of place.
After considerable experimentation with this formulation using very slow cooling it became apparent that the only way 
to guarantee that the binding constraints were satisfied was to impose relatively high weights, which effectively meant 
that through most of the search constraint violations were not reintroduced once they had been eliminated. From this 
point of view the search is mimicking a two-phase process, converging with respect to the highly weighted elements 
first and then dealing with the secondary objectives later. However, as the following discussion shows, improved 
performance can be obtained by strictly dividing the search into two separate phases and running each independently.
The initial experiments suggested that the objective of placing large exams early in the exam period was relatively easy 
to satisfy. Therefore this was incorporated into the time-window constraints for the remaining experiments. In the two-
phase approach the cost function for the first phase becomes:
and the objective is to reduce this to zero. This problem proved to be relatively easy to solve and setting all the wi, equal 
to one, and adopting a cooling rate of 0.99 every 1000 iterations produced zero cost solutions quickly and easily. Phase 
2 then commenced with the final solution from phase 1 and worked on a reduced solution space from which all 
solutions with f1 > 0 had been removed. The objective in this phase was to minimise the second order conflict given by 
cost function:
Very slow cooling was necessary in order to get the best results and the final schedule was chosen as result of both 
experimentation and theoretical considerations. In this schedule the temperature is initialised with a value of 20 and is 
reduced according to t → 0.99t every 5000 iterations. In order to compare this method with the single-phase approach, 
the schedule for the latter was adjusted to allow the same total number of iterations as were used in phases 1 and 2. The 
results using three different random number streams are shown in the first two rows of Table 9.1.
Table 9.1 Units of second-order conflict (c5) in final solutions of three independent runs
Run 1
Run 2
Run 3
Mean
Single phase, original neighbourhood:
367
385
365
372.3
Two phase, original neighbourhood:
313
341
336
330.0
Two phase, Kempe chain neighbourhood:
262
265
257
261.3
Note: all other costs were successfully reduced to zero.
 
  
< previous page
page_160
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_160.html2009-7-7 16:57:33

page_161
< previous page
page_161
next page >
Page 161
It is clear that using two separate phases gives better results. The reasons for this become apparent when we consider the 
behaviour of the search in phase 2, and compare it with that at moderate and low temperatures using the single-phase 
method. In the single-phase approach moves which will cause additional clashes will be generated but will almost 
certainly be rejected due to the large increase in cost. In the two-phase approach only those moves which do not cause 
clashes are generated. Further investigation suggested that around 75% of solutions are removed in phase 2. Therefore, 
if the single-phase approach is allowed 5000 iterations at each temperature, only around 1250 of these will be valid 
moves in terms of the phase 2 criteria. Thus the two-phase method is effectively cooling four times more slowly. Due to 
the problem size and complexity slow cooling is important in order to obtain good results, and the two-phase approach 
allows the search to make maximum use of the time available in generating only promising moves.
The above factors are sufficient in themselves to recommend a two-phase approach for this problem. However, dealing 
with different objectives in different phases has the added advantage that the standard algorithm can be modified to take 
advantage of specific features of the sub-problems which may not exist when the problem is considered as a whole. 
Such modifications may be designed to improve the efficiency of the search, or they may improve the quality of the 
final solution. In phase 1 optimal solutions of zero cost are found relatively easily, but there is still potential for 
increased efficiency. As the objective is to reduce the cost to zero it is reasonable to assume that the number of exams 
contributing to the cost function will gradually decrease as the search progresses. Moving an exam which is not making 
any contribution to cost cannot result in an improvement and does little to help the progress of the search. It is therefore 
sensible to redefine the neighbourhoods to include only moves relating to exams which are contributing to some 
element of the cost. This results in a more efficient search. Note that this modification would not be sensible in the 
single-phase approach where large numbers of exams continue to contribute to second order conflict, even close to 
optimal solutions.
Phase 2 lends itself to modifications which improve solution quality. These were motivated by a concern that removing 
so many solutions from the phase 1 solution space might cause it to become too sparsely connected for an effective 
search, and in some circumstances might even disconnect the space entirely. In order to overcome this the 
neighbourhood structure was changed so that each solution had an increased set of neighbours. The choice of 
neighbourhood was again motivated by the graph-colouring model. The solution space in phase 2 consists of feasible 
colourings in 24 colours and any new neighbourhood must maintain this feasibility. This was achieved using a 
neighbourhood based on Kempe chains. These essentially define subsets of vertices in two colours which can be 
swapped without rendering the resulting colouring infeasible. Full details can be found in Thompson and Dowsland 
[TD95]. The results from this neighbourhood are shown in the last row of Table 9.1. The improvement in solution 
quality is clear, and in view of this, the two-phase approach with the above modifications was chosen as the most 
appropriate method to produce the annual examination timetable at Swansea University, where it has been in use since 
1993.
  
< previous page
page_161
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_161.html2009-7-7 16:57:33

page_162
< previous page
page_162
next page >
Page 162
9.4 
Laboratory Scheduling
Our second problem concerns the allocation of students to practical classes. Each year two groups of approximately 150 
students must be allocated to a weekly laboratory session. Each group has its own laboratory which is available for 
around 20 hours each week and has a capacity of 24. Each student has his or her own individual lecture timetable and 
the primary objective is to find a feasible solution which minimises the total number of sessions required. There are also 
a number of secondary objectives, some of which are quantifiable, while others are subjective and can only be satisfied 
by producing a number of alternative solutions. There are inevitably some students who register late for the course or 
who are forced to change their practical time due to unforseen changes in their lecture timetable. Ideally these changes 
should be made without affecting the rest of the schedule. This is most likely to be achieved if the spare capacity is 
evenly spread over all the sessions rather than being concentrated in one or two. This is obviously equivalent to 
distributing the students evenly among those sessions utilised. Not all students are required to attend every week. 
Around 80% of weeks will be attended by everyone, but the remaining 20% are used to support a course taken by a 
subset of around 40% of the students. Ideally this special subgroup should be allocated to a minimum subset of sessions 
and should be distributed evenly among this set.
In practice, if there are ns students the number of sessions required is 
 (where 
 denotes rounding up 
to the nearest integer). Thus the problem can be formulated as that of finding a solution satisfying the following hard 
constraints:
1. only n sessions utilised
2. every student allocated to exactly one session
3. every student allocated at a time when he or she is free to attend
4. no more than 24 students per session
and as many of the following soft constraints as possible:
5. the number of students in each utilised session is uniform
6. a special subgroup of students are allocated uniformly to a minimum subset of practicals.
The set of available sessions will be denoted by Q, the set of students by S and the special subgroup of students involved 
in the constraint (6) by T.
The solution method is again based on simulated annealing, and, as with the examination scheduling problem, the first 
stage in implementing the procedure was to determine which constraints should be imposed on the solution space and 
which should be incorporated into the cost function. It was decided that the solution space should consist of all 
allocations in which each student was placed at a time when he or she was free to attend. The capacity constraints and 
the number of sessions used were to be included in the cost function, together with additional terms for the soft 
constraints. The objective is then to reduce the capacity overflow to zero and to minimise the number of sessions 
required. However this is not a practical definition of cost, as it will give rise to a stepped function with large plateaux 
which will not be able to guide the search towards good solutions. In reformulating these objectives into a cost function 
suitable for local search it proved convenient to include soft constraint
  
< previous page
page_162
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_162.html2009-7-7 16:57:34

page_163
< previous page
page_163
next page >
Page 163
(5). Ideally, if n sessions are to be utilised each will be attended by 
 or 
 students, or by zero students. 
What is required is a cost function which is zero at any of these extremes, and which will guide the search towards 
emptying sparsely populated sessions and filling those which are more densely populated. This will be achieved if all 
moves which move a student into a session with lower occupancy are uphill moves, which implies that the gradient of 
the cost function must be decreasing as the occupancy increases. As the sine function has the required property between 
0 and π was decided to define a penalty for each session, q, as follows. Let U denote 
, L denote 
 and z
(q) denote the number of students allocated to session q. Define penalties:
where
The full cost function was made up of these penalties added to a term for capacity overflow, summed over the set of 
feasible sessions. The problem of satisfying (1) to (5) with student set S and session set Q will be denoted P(S, Q) and 
the cost function defined above by f(S, Q).
It is clear that the problem of satisfying (6) is simply that of satisfying (1) to (5) with the values of n and the parameters 
of f defined with respect to T, i.e. problem P(T, Q) with cost function f(T, Q). In theory this can be achieved by adding 
the terms of f(T, Q) to f(S, Q) in a weighted sum. As it was expected that in practice all the constraints could be satisfied 
simultaneously, finding suitable weights was not anticipated to be too difficult a problem. However, as this 
implementation is not straightforward, in that the cost function does not directly reflect the objectives, it was decided to 
test the formulation on problem P(S, Q) before complicating the search with constraint (6). Results from three different 
historical data sets and some randomly generated variants of these were initially disappointing in that the search 
frequently failed to find zero cost solutions. It converged readily to low-cost local optima where most students were 
distributed evenly among n different sessions, but one or two were allocated elsewhere. Examination of the data 
revealed that these students were not able to attend any of the n sessions to which the majority had been allocated. This 
problem was overcome by adding an intelligent diversification phase to guide the search towards better solutions. Full 
details are not important here, and can be found in Dowsland [Dow93]. However the following points are relevant.
1. The diversification phase is called as soon as it is clear that the search is converging irrevocably to one of these local 
but not global optima.
2. The cost function is adjusted by the addition of further penalty terms relating to the students who cannot attend the 
selected n practicals.
3. The temperature parameter goes through a gradual heating phase which terminates when a more promising local 
optimum is approached or when it reaches a threshold value.
With this modification the search repeatedly converged to optimal zero cost solutions.
  
< previous page
page_163
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_163.html2009-7-7 16:57:35

page_164
< previous page
page_164
next page >
Page 164
An attempt was therefore made to satisfy constraint (6) by incorporating f(T, Q) into the cost function. This proved to be 
unsuccessful, whatever weights were employed. This lack of success was due to three reasons, the first two of which 
were immediately apparent. First, when the weights were such that the search tended to converge with respect to f(S, Q) 
before fully considering f(T, Q) all the utilised sessions were filled to their ideal capacity as f(S, Q) converged to zero. 
Further moves were required in order to optimise with respect to f(T, Q) but any move of a student in T resulted in an 
imbalance of seat utilisation, resulting in rejection due to the increase in the heavily weighted part of the cost function. 
Second, the behaviour of the search with respect to f(T, Q) mirrors that with respect to f(s, Q) and thus requires a 
diversification phase to avoid deep local optima. Both these observations lead to the conclusion that a two-phase 
approach should be utilised. In this way the neighbourhoods in phase 2 can be redefined to be made up of swaps 
between a student in T and one in S T. The need for diversification strengthens the argument for separate phases because 
of the way in which the temperature parameter behaves after diversification. It is unlikely that convergence with respect 
to both parts of the cost function will occur at the same time. Therefore diversification with respect to one will be 
initialised while the other is still converging. In order to maintain this convergence the temperature must be cooling, but 
for the diversification penalties to work effectively it is necessary for the temperature to be increasing. A two-phase 
approach will avoid this conflict.
Such an approach was implemented as follows. The first phase solves P(S, Q) and selects a set of n practicals. This will 
be denoted by R. In the second phase the solution space is reduced to contain only the n sessions in R, and the 
neighbourhood is altered to a swap neighbourhood as described above. Problem P(T, R) is then solved using cost 
function f(T, R). This strategy was not successful, in that the search still frequently failed to reduce the cost to zero in 
the second phase. Further examination of the data revealed the third reason for failure P(T, Q) is far harder to solve than 
P(S, Q) because there are very few subgroups of sessions of the correct size which cover all the students in T. Phase 1 
severely restricts the solution space, as only a subset of the available sessions are considered. In the previous example 
this resulted in a sparsely connected space which was overcome by redefining the neighbourhoods. This approach is not 
suitable here as it possible that all optimal solutions have been removed. We therefore take an alternative approach of 
reorganising the phases. In this final implementation phase 1 solves P(T, Q), completely ignoring the remaining 
students. The allocation of students in T is then fixed. The second phase solves P(S, Q) using f(S, Q), but employing a 
neighbourhood structure which does not allow moves for students in T. This approach was successful in producing 
solutions satisfying all conditions (1) to (6) and is currently used as the basis of an interactive support system which 
schedules the practicals at the start of each academic year.
Unlike the examination scheduling problem, the difficulty with a single-phase approach is not that the important 
constraints cannot be satisfied without high weights. The important factor here is the need for a diversification phase 
which is required in order to optimise both parts of the cost function, and which requires special action with respect to 
the temperature. This example also illustrates that the allocation of an objective/constraint to a particular phase does not 
only depend on its importance, but that, in common with Wright's observations, the difficulty of
  
< previous page
page_164
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_164.html2009-7-7 16:57:35

page_165
< previous page
page_165
next page >
Page 165
satisfying the objective/constraint should also be taken into account.
9.5 
Conclusions
The above examples illustrate how a phased approach can be superior to using a weighted cost function when solving 
those highly constrained or multi-objective problems for which feasible solutions are dificult to find. In the first 
problem the only way of getting a feasible solution was to impose heavy weights on the important terms in the cost 
function. In these circumstances a phased approach allows the available computation time to be used more efficiently, in 
that solutions causing large increases in the cost function, due to increases in the heavily weighted terms, are not 
considered in phase 2. It was also possible to improve both phases using problem specific modifications i.e. a reduced 
neighbourhood in phase 1, and a new neighbourhood structure in phase 2. In the second example the phased approach 
allows the search to take advantage of a non-standard enhancement based on a diversification routine which 
incorporates a 'heating' phase for the temperature function. Here the aim was to completely satisfy all the objectives, and 
this was best achieved by solving the most difficult, rather than the most important, objective in phase 1.
The main argument against such an approach is that the solution space will be more sparsely connected in later phases, 
so that optimal solutions may no longer be easily reachable from the starting solution. This occurred to some extent in 
both our examples. In the exam scheduling problem the use of a Kempe chain neighbourhood was sufficient to 
overcome the problem. In the laboratory scheduling problem the original choice of phases resulted in a phase 2 
optimum which was not only disconnected from the starting solution, but was no longer included in the search space. 
This problem was largely overcome by reversing the phases. This makes it more likely that some optimal solutions will 
be attainable, but it is still possible to generate phase 1 solutions which exclude all zero cost solutions when the 
remaining students are added. The current implementation includes a diversification option which returns to phase 1 to 
produce a new solution should this situation arise.
It should, however, be noted that this 'reachability' problem will also be implicit in a single-phase approach with heavy 
weights, as paths through the solution space which visit heavily weighted solutions are unlikely to be accepted in the 
middle and late phases of the search. A phased approach has the advantage that by simplifying the objectives in each 
phase it makes it easier to identify and implement modifications aimed at overcoming reachability problems.
Problems in which finding a feasible solution is itself a difficult task pose special difficulties for local search methods. 
Although many can be solved well using a series of weights, there are others for which this approach has not been able 
to produce solutions of the required quality. Some of these will belong to the class of problems for which simulated 
annealing is simply not appropriate, but there are others for which a phased simulated annealing algorithm is able to 
provide optimal or near optimal solutions within reasonable computational time. The two examples cited in this paper 
have illustrated two very different reasons for using phases as opposed to weights. Both are currently implemented at 
Swansea and have been used live for a number of years. In both cases the users are able to obtain good quality solutions 
in terms of meeting
  
< previous page
page_165
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_165.html2009-7-7 16:57:36

page_166
< previous page
page_166
next page >
Page 166
constraints and objectives at different levels of importance, while benefiting from the flexibility of simulated annealing 
in dealing with minor modifications and last minute changes.
References
[Abr91] Abramson D. (1991) Constructing school timetables using simulated annealing: sequential and parallel 
algorithms. Management Science 37: 98131.
[AD93] Abramson D. and Dang H. (1993) School timetables: a case study in simulated annealing. In Vidal R. (ed) 
Applied Simulated Annealing, number 396 in Lecture notes in Economics and Mathematical Systems, pages 103124.
Springer-Verlag, Berlin.
[Bal91] Balakrishnan N. (1991) Examination scheduling: A computerised application. OMEGA 19: 3741.
[CHdW87] Chams M., Hertz A., and de Werra D. (1987) Some experiments with simulated annealing for colouring 
graphs. European Journal of Operational Research 32: 260266.
[DLR93] Dige P., Lund C., and Ravn H. (1993) Timetabling by simulated annealing. In Vidal R. (ed) Applied Simulated 
Annealing, number 396 in Lecture notes in Economics and Mathematical Systems, pages 151174. Springer-Verlag, 
Berlin.
[Dow93] Dowsland K. (1993) Using simulated annealing for the efficient allocation of students to practical classes. In 
Vidal R. (ed) Applied Simulated Annealing, number 396 in Lecture notes in Economics and Mathematical Systems, 
pages 125150. Springer-Verlag, Berlin.
[FHKW94] Forbes M., Holt J., Kilby P., and Watts A. (1994) BUDI: A software system for bus dispatching. J. Opt. 
Res. Soc. 45: 497508.
[TD95] Thompson J. and Dowsland K. (1995) Variants of simulated annealing for the examination timetabling 
problem. Annals of O.R 60.
[Wri91] Wright M. (1991) Scheduling English cricket umpires. J. Opl. Res. Soc. 42: 447452.
[Wri92] Wright M. (1992) A fair allocation of county cricket opponents. J. Opl. Res. Soc. 43: 195202.
[Wri94] Wright M. (1994) Timetabling county cricket fixtures using a form of tabu search. J. Opl. Res. Soc. 45: 758770.
  
< previous page
page_166
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_166.html2009-7-7 16:57:36

page_167
< previous page
page_167
next page >
Page 167
10 
A Tabu Search Algorithm for Some Discrete-Continuous Scheduling Problems
Joanna Józefowska, Grzegorz Waligóra and Jan Weglarz *
Abstract
In this paper a tabu search (TS) algorithm for solving some discrete-continuous scheduling problems is proposed. Two 
inter-related subproblems of a discrete-continuous scheduling problem are distinguished: (i) constructing a feasible 
sequence of jobs on machines and (ii) allocating the continuous resource among jobs already sequenced. An application 
of the tabu search algorithm operating on a space of feasible sequences with the schedule length as the objective 
function is presented. Two methods of tabu list management are compared on a basis of a computational experiment : a 
method in which solutions on the tabu list are ordered according to the FIFO rule and the reverse elimination method.
10.1 
Introduction
A problem of scheduling jobs on parallel machines under additional resources is considered. In a classical model 
considered in the literature (see e.g. Blazewicz* et al.[BESW94]), the amount of the additional resource which can be 
allotted to a job at a time takes a value from a given finite set. For this model a number of results are known concerning 
the computational complexity analysis as well as exact and approximation algorithms. However, in many practical 
situations an additional resource can be allotted to a job in an arbitrary amount from a given interval. Such resources are 
called continuously divisible or simply continuous ones. Discrete-continuous scheduling problems occur when jobs 
simultaneously require for their processing both discrete and continuous resources (Józefowska, Weglarz[JW95]).
Let us consider the simplest situation of this type, where there is one discrete resource, which is a set of parallel, 
identical machines, and one continuous, renewable resource. The total amount of the continuous resource available at a 
time is limited.
Modern Heuristic Search Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_167
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_167.html2009-7-7 16:57:37

page_168
< previous page
page_168
next page >
Page 168
The processing rate of a job depends on the amount of a continuous resource allotted to this job at a time. Notice that 
even this simple case already concerns a wide variety of practical situations, for example when jobs are assigned to 
multiple parallel processors driven by a common (electric, hydraulic or pneumatic) power source. Let us consider, for 
instance, m refuelling terminals driven by a pump (a common power source) used to refuel a given fleet of n boats. The 
terminals are the machines and the boats correspond to the jobs. An optimization problem can be formulated as a 
problem of scheduling the fleet of boats on the terminals with the schedule length as the optimization criterion. This 
problem was considered in Dror et al.[DSL87] but in a purely discrete way in which job processing times were 
dependent on the number of jobs in operation. In our model, the delivery of the pump is the continuous resource and the 
refuelling rate of a boat depends on the part of the delivery of the pump allotted to the terminal refuelling this boat at a 
time. Notice that the above practical problem is only an example of a fairly large class of real-life situations which can 
be modelled as discrete-continuous scheduling problems. These are, for example, the problems in which machines 
(grinding, mixing etc.) or electrolytic tanks are driven by a common electric power source or the problems in which 
processors of a multiprocessor computer system use a common primary memory treated as a continuous resource (this is 
well justified in practical cases when the number of memory pages runs into hundreds) (Weglarz *[Weg80]). Using the 
discrete-continuous approach one can systematically study properties of feasible or optimal schedules as well as 
algorithms for finding them.
10.2 
Problem Formulation
In (Józefowska, Weglarz[JW95]) the following model of the discrete-continuous scheduling problem is discussed. Each 
job i, i = 1, 2, . . .,n requires for its processing at time t a machine j, j = 1,2, . . .,m from a set of parallel, identical 
machines and, simultaneously, an amount (unknown in advance) of a continuous resource ui(t). Assume that each 
machine can process at most one job at a time, jobs are nonpre-emptable and independent and all are available at the 
start of the process. Processing rate of job i is described by the equation:
where
xi(t) is the state of job i at time t,
ui(t) is the amount of the resource allotted to job i at time t,
fi is a continuous, nondecreasing function, fi (0) = 0,
Ci is the (unknown in advance) completion time of job i,
 is the final state (or the processing demand) of job i.
Assume that 
 for every t.
The last inequality expresses the fact that the continuous resource is renewable. For every t, the total amount of the 
resource allotted to all jobs processed at time t must
  
< previous page
page_168
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_168.html2009-7-7 16:57:37

page_169
< previous page
page_169
next page >
Page 169
not exceed the total available amount of the resource which is assumed equal to one without loss of generality.
State xi(t) of job i at time t is a measure of work related to the processing of job i up to time t. For example, in the boat 
refuelling problem described in the introduction, xi(t) is the volume of fuel bunkers of boat i refuelled up to time t. In 
the same example 
 is the total volume of fuel bunkers of boat i and ui(t) means the part of the delivery of the 
pump used for refuelling boat i at time t. The problem is to find a sequence of jobs on machines and, simultaneously, a 
continuous resource allocation, which minimize the schedule length M = max{Ci}. Of course, the problem is NP-hard 
because scheduling nonpre-emptable jobs on parallel machines to minimize the schedule length is NP-hard even without 
any additional resource (Lenstra et. al.[JKL77]). Notice that the problem defined can be decomposed into two inter-
related subproblems: (i) to sequence jobs on machines and (ii) to allocate the continuous resource among jobs already 
sequenced.
It can be proved that for functions fi such that fi(ui) ≤ ciui,ci = fi(1), i = 1,2, . . ., n the problem is trivial because the 
schedule length is minimized by scheduling all jobs on one machine, where each job is processed using the total 
available amount of the continuous resource. Let us assume that functions fi,i = 1, 2, . . .,n are concave. Then (see 
(Józefowska, Weglarz *[JW95])) the parallel configuration is optimal and for n ≤ m (i.e., in fact, n = m) in an optimal 
schedule jobs are processed using the following constant resource amounts:
where M* is the (unique) positive root of the equation:
In consequence, for concave functions fi and n > m each feasible schedule can be divided into p ≤ n intervals Mk defined 
by the consecutive completion times of jobs, such that the number of jobs (or rather their parts) performed in Mk, k = 
1,2, . . .,p does not exceed m and the continuous resource allocation among jobs in Mk remains constant. Let Zk denote 
the combination of jobs corresponding to the interval Mk. Thus, with each feasible schedule a feasible sequence S of 
combinations Zk is associated, where the number of elements in each combination does not exceed m, each job appears 
in at least one combination and if a job appears in more than one combination, then these combinations are consecutive 
ones (nonpre-emptability).
Example 1
Consider the following feasible schedule of 5 jobs on 3 machines in which it is assumed for simplicity ui(t) = 1/3 for 
every t (Figure 10.1).
  
< previous page
page_169
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_169.html2009-7-7 16:57:39

page_170
< previous page
page_170
next page >
Page 170
Figure 10.1 
The idea of discrete-continuous scheduling. An example of a 
schedule.
The corresponding feasible sequence of combinations Z1, Z2, Z3 is of the form :
Now, the processing demand 
 of each job can be divided into parts 
 corresponding to particular time 
intervals (combinations), as it is shown in Fig. 10.2.
Figure 10.2 
The idea of discrete-continuous scheduling. A division of 
processing demands of jobs.
For a given feasible sequence S one can find an optimal division of processing demands of jobs among combinations in 
S, i.e. such a division which leads to a schedule of the minimal length from among all feasible schedules generated by S. 
To this end a nonlinear mathematical programming problem can be formulated in which the sum of the minimal-length 
intervals (i.e. parts of a feasible schedule) generated by consecutive combinations in S, as functions of 
, is 
minimized subject to obvious constraints. To define these functions equation (10.3) can be used, written for each Mk, k 
= 1,2, . . .,p. In this way the properties of optimal resource allocations for n = m are fully utilized, simplifying the 
resulting optimization problem. It is
  
< previous page
page_170
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_170.html2009-7-7 16:57:41

page_171
< previous page
page_171
next page >
Page 171
worth mentioning that the optimization problem obtained is always a convex one (see Józefowska and Weglarz *
[JW95]). Of course, knowing a division of 
 one can easily calculate corresponding resource allocations using (10.2) 
and (10.3).
Formalizing the above idea let 
 be the minimal length of an interval Mk, generated by Zk∈ S, as a 
function of 
, and let Ki be the set of all indices of Zk's such that i ∈ Zk. Then a solution of the following 
mathematical programming problem defines an optimal demand division (and in consequence an optimal resource 
allocation) for a given feasible sequence S :
Problem P.
Minimize
subject to
where 
 is the unique positive root of the equation
Thus, the problem is to minimize a convex function subject to linear constraints.
The importance of the mathematical programming problem P follows from the fact that it finds an optimal demand 
division for a given feasible sequence S. Based on this formulation one can also prove some properties of optimal 
resource allocations which simplify the procedure of finding optimal schedules for some classes of fi's (see Józefowska, 
Weglarz[JW95]).
To find an optimal schedule when a feasible sequence is not given one has to answer the question: which feasible 
sequences should be taken into account ? Let POS (a potentially optimal set) be a set of feasible sequences containing at 
least one sequence corresponding to an optimal schedule. It is clear that it is desirable to generate the smallest possible 
POS. However, in general, one has to consider a POS containing all sequences S composed of p = n m + 1 m-element 
combinations (i.e. so-called general POS) [JW95]. If the number of feasible sequences S in a POS is still not very large 
one can solve problem P for each S from this POS and choose one (or all) schedule with the minimal length. However, 
the cardinality of a general POS grows exponentially with the number of jobs. Therefore it is justified to apply local 
search metaheuristics operating on a space of feasible sequences. A local search heuristic generates a set of feasible 
sequences S (a subset of a general POS) for a given problem instance and finds an optimal resource allocation for each 
sequence S in S. Of course, in general, an optimal schedule is found if S=POS. In this paper a heuristic approach based 
on the tabu search (TS) algorithm is presented. Although the idea is a general one, a computational experiment will be 
performed for functions fi, i = 1, 2, . . ., n of the form:
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_171.html（第 1／2 页）2009-7-7 16:57:41

page_171
< previous page
page_171
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_171.html（第 2／2 页）2009-7-7 16:57:41

page_172
< previous page
page_172
next page >
Page 172
This assumption significantly simplifies the solution of the ''continuous part" of the problem. For fixed values of ci and 
α in (10.4) an instance of the problem is defined by the number of jobs n, the number of machines m and a vector of 
processing demands (final states) 
.
10.3 
An Application of the Tabu Search Algorithm
In order to apply a tabu search algorithm [Glo89, Glo90] to a combinatorial optimization problem one has to define the 
following elements:
the representation of a feasible solution,
the way of creating a starting solution,
the form of the objective function and the method of calculating its values,
the mechanism of generating a neighbourhood,
the structure and the length of a tabu list.
In the following sections the definitions of the above elements for the considered discrete-continuous scheduling 
problem will be presented.
10.3.1 
Representation of a Feasible Solution
As it has been mentioned, TS operates on a space of feasible sequences and thus a single feasible solution is simply a 
feasible sequence S of the form already discussed. Because the general POS is considered, a feasible solution consists of 
exactly n m + 1 m-element combinations of jobs. It is easy to observe that in this case every combination Zk, k = 2, . . .,p 
= n m + 1 differs from the previous one by exactly one element. Otherwise repetitions of combinations in a feasible 
sequence would appear. Such feasible sequences with repeated combinations are redundant and should be excluded 
from considerations.
10.3.2 
Creating a Starting Solution
Two methods of creating a starting solution have been applied. In the first method successive jobs are generated 
randomly but a job is accepted only if it does not violate feasibility of a sequence. It can be done in the following way:
the first combination is generated randomly as an m-element combination without repetitions from the n-element set of 
jobs,
every next combination Zk, k = 2, . . ., p = n m + 1 is created from the previous one by generating randomly a position in 
the combination (from 1 to m) and inserting into that position a randomly generated job which has not appeared in the 
sequence so far. A sequence of combinations obtained in the above way is random and fulfils the feasibility conditions.
  
< previous page
page_172
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_172.html2009-7-7 16:57:42

page_173
< previous page
page_173
next page >
Page 173
In the second method a feasible solution generated randomly is transformed according to the vector of processing 
demands in such a way that the job which occurs the greatest number of times is replaced by the job with the greatest 
processing demand and so on. In consequence, a job with a larger processing demand appears in a greater number of 
combinations. It is obvious that for identical fi, i = 1, 2, . . ., n, such a strategy always increases the probability of 
finding an optimal schedule.
10.3.3 
Objective Function
The value of the objective function of a feasible solution is defined as the minimal schedule length for the 
corresponding feasible sequence S. For the class of functions fi considered it is given by the following formula:
where Pi, i = 1, . . ., m denotes the set of jobs assigned to machine i. Formula (10.5) follows from the fact that in this 
case the minimal schedule length depends on how the jobs are assigned to the machines but is independent of the order 
of jobs on a particular machine (Józefowska, Weglarz *[JW95]).
10.3.4 
Mechanism of Generating a Neighbourhood
A neighbour of a feasible solution is obtained from this solution by replacing one job occurring in a position in the 
sequence by another job. The number of positions is, of course, (n m + 1) · m. It is obvious that a job may be replaced 
by another one only if it occurs more than once in the feasible solution and only in the first or last combination in that it 
occurs (non pre-emptability). A neighbourhood consists of feasible solutions obtained by performing all possible 
replacements of that type. Assume that job i occurs in combination Zk and also in either Zk1 or Zk+1. Then job i from 
combination Zk is replaced either by
1. job j from Zk1 if (j is not in Zk) and (i is in Zk1)
or by
2. job l from Zk+1 if (l is not in Zk) and (i is in Zk+1).
Example 2
Let n = 5, m = 3 and S = {1,2,3}{2,3,4}{3,4,5}. The set of the neighbouring solutions is:
{1,4,3} {2,3,4} {3,4,5}
{1,2,4} {2,3,4} {3,4,5}
{1,2,3} {1,3,4} {3,4,5}
{1,2,3} {2,3,5} {3,4,5}
{1,2,3} {2,3,4} {2,4,5}
{1,2,3} {2,3,4} {3,2,5}.
It can be proved that the above neighbourhood generation mechanism enables every single feasible solution to be visited.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_173.html（第 1／2 页）2009-7-7 16:57:42

page_173
  
< previous page
page_173
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_173.html（第 2／2 页）2009-7-7 16:57:42

page_174
< previous page
page_174
next page >
Page 174
10.3.5 
Structure of the Tabu List
Two strategies of the tabu list management have been tested. The first one is a static method and corresponds to the 
simplest version of TS. In this case the tabu list contains feasible solutions and works as a queue. A solution stays tabu 
for a fixed number of iterations. Let us call this version the simple tabu search (STS) algorithm. The second strategy is 
based on a dynamic method called the reverse elimination method (REM)[Vos93]. In this case the tabu list contains 
attributes of moves, where a move is a modification leading from a feasible solution to a neighbouring one.
Tabu List Management for STS This version corresponds to the simplest and most general form of a TS algorithm. 
Every visited solution is appended to the tabu list, causing the removal from the list of the oldest existing one (according 
to the FIFO algorithm). In the worst case checking the status of a solution requires examining the whole list. Because 
the order of jobs in a combination is not important (it is a set, not a sequence), solutions on the tabu list are sorted within 
every combination in order to identify them correctly. For example, combinations {1,2,3} and {3,1,2} represent the 
same part of a schedule.
Example 3
Tabu list management for STS where n = 4, m = 3.
visited solutions
tabu list
1. {1,2,3}{2,3,4}
1. {1,2,3}{2,3,4}
2. {1,4,3}{2,3,4}
1. {1,2,3}{2,3,4}
2. {1,3,4}{2,3,4}
3. {1,4,2}{2,3,4}
1. {1,2,3}{2,3,4}
2. {1,3,4}{2,3,4}
3. {1,2,4}{2,3,4}
e.g. {1,3,2}{2,3,4} tabu.
 
Tabu List Management Based on REM The basic structure for this version is a move. It can be defined as a 3tuple: 
(index of a combination, job replaced, job introduced). It represents a difference between two neighbouring solutions. In 
this case a single element of the tabu list is a so-called residual cancellation sequence (RCS) which is, in fact, a set of 
moves. If the starting solution is denoted by #1 then RCS number j (the j-th element of the tabu list) is the minimal set 
of moves by which the current solution differs from solution #j (i.e. visited in the j-th iteration).
REM uses the fact that any solution can be revisited in the next iteration only if it is a neighbour of the current solution 
(i.e. if the set of moves by which it differs from the current solution consists of one element only). In consequence, 
every move occurring on the tabu list as a one-element RCS is tabu in the next iteration. Thus, in order to define the 
status of a move, it is sufficient to check only those elements of the tabu list which contain exactly one move.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_174.html（第 1／2 页）2009-7-7 16:57:43

page_174
The way of updating the tabu list using REM is the following. At the start of the algorithm all RCSs are empty. If a 
move (k, i, j) is performed, a new one-element RCS containing its reverse (i.e. move (k, j, i)) is created and added to the 
tabu list. Then
  
< previous page
page_174
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_174.html（第 2／2 页）2009-7-7 16:57:43

page_175
< previous page
page_175
next page >
Page 175
all non-empty RCSs are updated in the following way:
if the move (k,i,j) belongs to the RCS considered 
then it is eliminated (i.e. cancelled) from the RCS 
otherwise the move (k,j,i) is added to the RCS.
It is very important that every RCS should be the minimal set of moves by which the current solution differs from an 
already visited one. To maintain this condition one has to reduce a RCS every time it is possible. Let v denote a move 
which is to be added to the tabu list. Then there are two possible kinds of reduction :
1. an added move v and a move already existing in a RCS create a new move and the number of elements of the RCS 
remains the same;
Example: if RCS=(1,4,3) . . . and v = (1,3,2) then RCS=(1,4,2) . . .
or if RCS=(1,4,3) . . . and v = (1,5,4) then RCS=(1,5,3) . . .
2. an added move v and two moves already existing in a RCS create a new move and the number of elements of the 
RCS decreases by 1;
Example: if RCS=(1,5,4) . . . (1,3,2) . . . and v = (1,2,5) then
RCS=(1,3,2) + (1,2,5) + (1,5,4) . . . = (1,3,4) . . .
Of course, the reducing operation is possible only among moves having the same first element.
It is easy to observe that in the minimal set the following condition has to be fulfilled:
for every two moves (k,i,j) and (k,p,q) in a RCS: 
(i ≠ p) and (i ≠ q) and (j ≠ p) and (j ≠ q).
REM guarantees that solutions will never be revisited. It is obvious, however, that the number of RCSs creating the tabu 
list increases with the number of iterations. The criterion of removing RCSs from the list (and replacing them by new 
ones) is based on the fact that the number of elements in the j-th RCS defines the distance between the current solution 
and solution #j. After the number of iterations equal to the assumed length of the tabu list, an RCS corresponding to the 
most distant solution from the current one is removed from the list rather than the oldest RCS. It is based on the 
assumption that going back to the most distant solution should be least likely. Thus, in this case the tabu list does not 
work as a queue (in contrast to STS).
Example 4
Tabu list management based on REM where n=5, m=3.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_175.html（第 1／2 页）2009-7-7 16:57:44

page_175
visited solutions
tabu list
1. {1,2,3}{2,3,4}{3,4,5}
move (2,2,1)
2. {1,2,3}{1,3,4}{3,4,5}
1. (2,1,2)
move (2,4,5)
3. {1,2,3}{1,3,5}{3,4,5}
1. (2,1,2)(2,5,4)
move (2,1,2)
2. (2,5,4)
 
  
< previous page
page_175
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_175.html（第 2／2 页）2009-7-7 16:57:44

page_176
< previous page
page_176
next page >
Page 176
4. {1,2,3}{2,3,5}{3,4,5}
1. (2,5,4)
move (1,3,5)
2. (2,5,4)(2,2,1)
3. (2,2,1)
5. {1,2,5}{2,3,5}{3,4,5}
1. (2,5,4)(1,5,3)
move (1,2,3)
2. (2,5,4)(2,2,1)(1,5,3)
3. (2,2,1)(1,5,3)
4. (1,5,3)
6. {1,3,5}{2,3,5}{3,4,5}
1. (2,5,4)(1,5,3)(1,3,2)
= (2,5,4)(1,5,2)
2. (2,5,4)(2,2,1)(1,5,2)
3. (2,2,1)(1,5,2)
4. (1,5,2)
5. (1,3,2)
 
10.4 
Computational Experiments
A tabu search algorithm has been tested to examine the influence of the type of a starting solution and the strategy of the 
tabu list management on the performance and the convergence of the algorithm. For each combination of (n, m) 
(starting with (10,2), finishing with (100,10)) 1000 instances (vectors of processing demands) have been randomly 
generated and solved by TS using all four combinations of starting solution and tabu list management parameters. The 
stop criterion was a prespecified computational time limit. The algorithm has been implemented in C++ and the 
experiment has been carried out on a Silicon Graphics Power Challenge computer with four RISC TFP 75MHz 
processors in the Poznan * Supercomputing & Networking Center.
All the results have been compared with the lower bound calculated from (10.5) under the assumption that nonpre-
emptability is violated. The minimal schedule length is reached if the particular components in (10.5) are equal to each 
other. The average relative deviation from the lower bound for each version of the algorithm and the computational time 
limit for one instance of the problem are presented in Table 10.1.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_176.html（第 1／2 页）2009-7-7 16:57:44

page_176
The results show that a starting solution dependent on processing demands improves the performance of the algorithm 
in the overwhelming majority of cases (the only exception is the problem of the biggest size (100,10)). Thus, it seems to 
be better for TS to start with a good initial solution. It is worth mentioning that in our experiment a good initial solution 
is a solution with a 'good' value of the objective function (i.e. close to the lower bound). In this sense a good starting 
solution may be a deep local optimum making the search process more difficult.
  
< previous page
page_176
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_176.html（第 2／2 页）2009-7-7 16:57:44

page_177
< previous page
page_177
next page >
Page 177
Table 10.1 The results of the computational experiment: Average relative deviations from lower 
bound in percent. r starting solutions generated randomly, d starting solutions dependent on 
processing demands
(n,m)
t [sec.]
STS/r
STS/d
REM/r
REM/d
(10,2)
0.1
0.010005 0.006290 0.004943 0.003763
(10,3)
0.2
0.194713 0.075277 0.114693 0.051590
(20,2)
0.3
0.001241 0.000355 0.000525 0.000192
(20,3)
0.75
0.111323 0.027547 0.086444 0.026085
(50,2)
2
0.000161 0.000012 0.000124 0.000006
(50,3)
6
0.009094 0.003323 0.007371 0.002898
(100,10)
50
0.074322 0.100000 0.070342 0.097967
 
As far as the tabu list management is concerned, the results obtained for REM appeared to be significantly better than 
for STS in every case, but especially for small problem sizes. It is not very difficult to give some reasons for such a 
situation. Firstly, defining the solution status is much simpler using REM. There is no necessity to examine the whole 
list and the examining process is faster because of much simpler data structures. Secondly, the probability of going back 
to a solution just removed from the tabu list is much less using REM (in STS it may happen immediately afterwards). 
This significantly decreases the possibility of cycling in the algorithm. Finally, as a result of computational 
simplifications, TS using REM is able to perform a greater number of iterations within the same time period. Moreover, 
the reverse elimination method gives important information about the state of the searching process in the sense of the 
distance between already visited solutions.
Figures 10.3, 10.4 and 10.5 present the convergence curves of both versions of the algorithm for n=10, 20, 50, m=3 and 
starting solutions dependent on processing demands. Curves for m=2 as well as for random starting solutions are very 
similar. The figures show the average relative deviation from the lower bound of the best solution found as a function of 
the iteration number. For iteration numbers not greater than 15 STS produces better results than REM. Although the 
differences are very small, STS finds a solution relatively close to the lower bound faster than REM. The advantage of 
REM becomes more significant, however, with the growth of the number of iterations. For both methods there exists an 
iteration number after which no really vital improvement can be observed. This number is smaller for the STS method. 
However, for both versions this iteration number decreases with the increase of the problem size.
It is worth noticing that for a given number of machines the average distance from the lower bound decreases with the 
growth of the problem size (the number of jobs) for both STS and REM.
  
< previous page
page_177
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_177.html2009-7-7 16:57:45

page_178
< previous page
page_178
next page >
Page 178
Figure 10.3 
Convergence curves for n=10 and m=3.
Figure 10.4 
Convergence curves for n=20 and m=3.
  
< previous page
page_178
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_178.html2009-7-7 16:57:45

page_179
< previous page
page_179
next page >
Page 179
Figure 10.5 
Convergence curves for n=50 and m=3.
10.5 
Final Remarks
In this paper an application of a tabu search algorithm to a class of discrete- continuous scheduling problems is 
presented. Particular components of the algorithm are defined, discussed and evaluated in a computational experiment. 
In further research it is planned to compare all the results obtained with optimal solutions and also to perform a 
computational experiment for a wider class of functions fi. Moreover, further experiments with the length of the tabu list 
are needed in order to avoid cycles and lead to good solutions. Also a parallelization of the algorithm is planned to make 
the searching process more efficient.
10.6 
Acknowledgements
This research has been supported by the KBN Grant No 8T11F 01008p02.
References
[BESW94] Blazewicz J., Ecker K. H., Schmidt G., and Weglarz J. (1994) Scheduling in Computer and Manufacturing 
Systems. Springer Verlag, Berlin,
  
< previous page
page_179
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_179.html2009-7-7 16:57:46

page_180
< previous page
page_180
next page >
Page 180
2nd edition.
[DSL87] Dror M., Stern H. J., and Lenstra J. K. (1987) Parallel machine scheduling with processing rates dependent on 
number of jobs in operation. Management Science 33(8): 10011009.
[Glo89] Glover F. (1989) Tabu search. ORSA J. Computing 1: 190206.
[Glo90] Glover F. (1990) Tabu search. ORSA J. Computing 12: 432.
[JKL77] J. K. Lenstra A. H. G. Rinnooy Kan P. B. (1977) Complexity of machine scheduling problems. Ann. Discrete 
Math. 1.
[JW95] Józefowska J. and Weglarz * J. (1995) On a methodology for discrete-continuous scheduling. Technical Report 
RA-95/004, Inst. of Comp. Science, Poznan University of Technology.
[Vos93] Voss S. (1993) Concepts for parallel tabu search. In Symposium on Applied Mathematical Programming and 
Modelling, Volume of Extended Abstracts. Budapest.
[Weg80] Weglarz J. (1980) Multiprocessor scheduling with memory allocation a deterministic approach. IEEE Trans. 
Computers C-29(8): 703710.
  
< previous page
page_180
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_180.html2009-7-7 16:57:46

page_181
< previous page
page_181
next page >
Page 181
11 
The Analysis of Waste Flow Data from Multi-Unit Industrial Complexes Using Genetic Algorithms
Hugh M. Cartwright and Ben Jesson
Abstract
The discharge of liquid waste from industrial chemical sites is heavily regulated. In order to control waste flow within 
sites, and identify the source if excess pollution is released, a thorough interpretation of data delivered by on-site 
chemical monitoring stations is essential. This paper discusses how the genetic algorithm, operating upon three-
dimensional strings, can be used for this task, and can help in the design of waste flow networks.
11.1 
Introduction
The genetic algorithm (GA) is a search and optimisation technique inspired by the manipulations of natural selection. It 
operates within a search space consisting of the set of points which define all possible solutions to a problem. At each of 
these points the value of an 'objective function' can in principle be calculated, which indicates the quality of the solution 
to which the point corresponds. The task of the algorithm is to move through this search space in an intelligent fashion, 
looking for points with optimal or near-optimal values of the objective function. There is a trade-off in such a search 
between the quality of the best solution found and the time required to find it; it is clear that to be of use any searching 
algorithm whatever its nature must deliver high-quality solutions in reasonable time.
The GA is an iterative procedure, which starts from randomly chosen (and, therefore, typically poor) solutions and 
attempts to improve them progressively. This is, of course, far from a unique approach, but in other respects the 
algorithm is fundamentally different from more traditional methods. The most far-reaching
Modern Heuristic Search Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_181
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_181.html2009-7-7 16:57:46

page_182
< previous page
page_182
next page >
Page 182
difference is that the GA manipulates not one solution, but a population of perhaps 50 to 100 solutions. It can therefore 
take advantage of a high degree of parallelism, since all solutions are manipulated using the same set of operators, and 
can, during most stages of the algorithm, be operated upon independently.
As an intelligent algorithm, the GA creates a 'collective memory' of the characteristics of above-average solutions to a 
problem. Large numbers of candidate solutions are inspected as the algorithm proceeds, and since the information 
which the solutions contain may be useful later in the calculation, a portion of this information is passed from one cycle 
to the next. This memory is of particular value when the search space is large and complex, and is a distinctive and 
crucial feature of the algorithm. A further distinction from traditional methods is that the GA is stochastically driven, so 
is less likely to become trapped by local maxima that might pin down more deterministic algorithms. GA's are also 
generally tolerant about the nature of the objective function; this function need not be continuous, nor have a definable 
gradient, and this flexibility expands the range of problems to which the GA can be applied. Indeed, the objective 
function may even generate substantial noise, so that repeated sampling of a point returns different values, without 
seriously disrupting operation of the algorithm.
In a genetic algorithm, the population is composed of a multitude of trial solutions to a problem. Each of these solutions 
which, in GA terminology, are often known as 'strings', consists of a list of items which may be binary digits, integers or 
characters from some other alphabet. A GA string completely describes an individual, and therefore contains all the 
information necessary to specify the trial solution to a problem which the individual represents. The quality of any 
arbitrary solution can be determined using the objective function, which is usually referred to as the 'fitness function'. 
The GA attempts to evolve its population of strings using a range of operators, with the aim of locating the string which 
has the maximum fitness, since this individual is, in principle, the best possible solution. The operation of the algorithm 
is described in detail in standard texts on the subject (Goldberg[Gol89], Davies[Dav91]).
11.2 
Applicability of the Genetic Algorithm
The GA has proved to be of value in a wide range of problems (see, for example, Eogarty, Ireson and Bull[FIB95], 
Reeves[Ree95], Cartwright and Cattell[CC95], and references therein), but is applicable only if the problem satisfies 
certain criteria. Firstly, an appropriate fitness function must exist, so that the quality of every conceivable solution to the 
problem which the algorithm could generate can be assessed. Ideally, the global maximum of the fitness function should 
correspond to the optimum solution, though this is not necessarily a trivial requirement. The fitness function should also 
yield a fine-grained set of fitness values, so that the fitness of partially correct solutions exceeds that of less desirable 
ones (this, of course, implies that some meaning can be attached to the idea of a partial solution). This requirement 
arises because the fitness function provides a sense of direction to the search, so that the algorithm can move in a 
purposeful way towards a good solution. Without such direction, the algorithm would fiddle aimlessly with the strings 
which comprise the population. (One should not, however, try to interpret the idea of direction too
  
< previous page
page_182
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_182.html2009-7-7 16:57:47

page_183
< previous page
page_183
next page >
Page 183
literally; the direction of movement of a genetic algorithm across a search space is rather different from the direction 
that would be chosen by, say, a hill-climbing algorithm, since the algorithms manoeuvre about the search space in quite 
different ways).
A further criterion for efficient searching is that a suitable recipe must be available, according to which solutions to the 
problem are encoded as a string (Goldberg[Gol91], Schaffer and Eshelman[SE93]). This is a key requirement, since an 
unsuitable encoding can destroy much of the power of the algorithm. For example, the operations that 'cut-and-splice' 
parent strings to form child strings must largely retain the better portions of superior strings, so that those portions can 
be combined with others in new ways. If the encoding is such that valuable portions are always, or very frequently, split 
up by the cut-and-splice operation, then the progress of the GA will be severely hampered, and the algorithm is unlikely 
to converge to acceptable solutions.
When these criteria are satisfied, the GA can be a powerful and robust search technique that often out-performs more 
traditional methods such as exhaustive search, or hill-climbing methods. These methods are by no means made 
redundant by algorithms such as the GA, however, and much recent research combines the GA with features of these 
methods to yield powerful hybrid algorithms.
11.3 
The Flow of Industrial Waste within Large Complexes
Large industrial complexes, comprising as many as 2530 individual factories, are not uncommon within the chemical 
industry. Clustering of a number of independent synthetic units on a single site leads to economies of scale through the 
sharing of resources, and minimisation of transportation and storage costs. Among the resources usually shared is the 
system whose role is to dispose of liquid chemical waste (Figure 11.1).
This waste is delivered into and carried away by a pipe network before release into natural waterways. To minimise the 
impact of chemical waste on the environment, legal restrictions exist on the amounts and types of waste that may be 
introduced into natural systems from such complexes. The outputs of large sites are monitored, both by site operators 
and environmental agencies, and substantial financial penalties may be imposed if permitted discharge levels are 
exceeded.
  
< previous page
page_183
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_183.html2009-7-7 16:57:47

page_184
< previous page
page_184
next page >
Page 184
Figure 11.1 
An industrial waste disposal network.
To control effectively discharge from industrial plants, a knowledge of the composition and quantity of waste produced 
by each factory is crucial information. If permitted discharge levels at the site outfall are exceeded, the source of the 
pollution within the complex can be traced, provided that sufficiently detailed information is available about the waste 
output of each factory. This information can be provided by monitoring stations installed at points within the pipe 
network (Figure 11.2), whose role is to measure and record key data on identity and concentrations of the main 
pollutants, and additional information such as rate of waste flow or pH. In principle, such a network of monitoring 
stations yields sufficient information to trace all significant pollution events back to their source. Unfortunately, the cost 
of installing monitoring stations capable of generating a complete and continuous profile of the waste output for every 
factory is high, and it is rarely feasible to install a station to monitor the output of each factory. As a consequence, a 
limited number of monitoring stations are generally installed at what are believed (or guessed) to be key positions in the 
disposal system (Figure 11.3). A restricted set of stations naturally provides much less information than would be 
available if monitoring stations were positioned at the outlet of every factory. When a pollution event occurs, it may be 
difficult, or even impossible to determine which factory was responsible from the reduced information provided by this 
cut-down network. Without this information one cannot take remedial action to prevent recurrence. We can illustrate the 
difficulty by considering a very simple case, with two factories and a single monitoring station (Figure 11.4).
  
< previous page
page_184
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_184.html2009-7-7 16:57:48

page_185
< previous page
page_185
next page >
Page 185
Figure 11.2 
The ideal waste monitoring network.
Figure 11.3 
A realistic waste monitoring network.
  
< previous page
page_185
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_185.html2009-7-7 16:57:48

page_186
< previous page
page_186
next page >
Page 186
Figure 11.4 
A minimal monitoring network.
The monitoring station records concentrations in the effluent of total organic carbon (TOC) and chlorine of, say, 100 
ppm and 75 ppm respectively, but the raw measurements provide, in themselves, no clue to how we should apportion 
chlorine and carbon production between the two factories. Without additional information no algorithm could determine 
what proportions of the elements came from which factory.
However, the outputs of real factories are not infinitely variable; certain parameters are known or can be predicted, such 
as the types of waste produced at particular points in a production cycle, or the relationships between the amounts of 
certain pollutants discharged by the factories. In the example above, factory A might perhaps generate purely inorganic 
waste and therefore produce no organic carbon. By contrast, factory B might produce predominantly chlorocarbon 
waste, containing a carbon to chlorine ratio of 2:1. Given such additional data we could attribute 25 ppm chlorine to 
factory A and 100 ppm carbon and 50 ppm chlorine to factory B. A complete or partial interpretation of the waste 
passing through a particular monitoring station is thus possible, provided that appropriate raw data from the factories are 
available.
Nevertheless, in real complexes the known parameters and the relationships between them are likely to be less clear-cut. 
The ratio between two pollutants discharged by a factory, for example, might not be fixed, but only known to lie within 
certain limits. This is a 'fuzzy' relationship, and presents difficulties to many types of algorithm; it is a complex task to 
work backwards from imprecise monitoring station data to derive the individual outputs of each factory.
A more fundamental difficulty also exists. Quite apart from the variability of the monitoring station data, the search 
space is extremely large; there may be numerous combinations of factory discharge levels which reproduce the 
monitoring station data with reasonable fidelity. For example, if a plant contained only four factories, each producing 
waste characterised by four parameters (typically, the rate of flow, pH and the concentration of two elements) recorded 
over a period of five time steps, a complete solution consists of the specification of 80 parameters (4 factories × 4 
parameters × 5 times steps). If each parameter was specified as a percentage to a precision of one part in a hundred, 
10160 different solutions to the problem would exist. An exhaustive search across such a space would not be productive 
and, unless the search space were particularly well-behaved, hill-climbing algorithms and their relations would also
  
< previous page
page_186
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_186.html2009-7-7 16:57:48

page_187
< previous page
page_187
next page >
Page 187
be inappropriate. The real-world problem is considerably more daunting, involving perhaps twenty synthetic units 
discharging through monitoring stations which record 6 to 10 parameters on a virtually continuous basis. Solutions 
consist of many hundreds or thousands of values. The task of identifying high-quality solutions is demanding, and only 
fairly sophisticated algorithms such as the GA are likely to be successful.
11.4 
The Algorithm
11.4.1 
String Encoding
Each string within the GA population represents a possible solution to the waste-flow analysis problem. The string 
specifies the amount of waste of each type discharged by each factory at each time step for which readings are available 
at the monitoring station. If such strings are to be manipulated successfully by the GA, compatibility between the string 
encoding and the choice of crossover operator is vital.
A key consideration in this respect is the relative size of different values in the strings. The raw data generated by the 
monitoring stations may be in a variety of different units, and therefore vary over different numerical ranges. For 
example, the volume of discharge may be of the order of thousands of cubic metres per hour, whereas pH values lie 
within the range 0 to 14. If all values were used without any form of scaling, the fitness of each string would be 
determined almost entirely by the degree to which large values such as flow rate were fitted, while small values such as 
the pH would be virtually ignored. In this algorithm all the quantities in the string are therefore scaled to the same range, 
0 to 1000. This helps to ensure that all types of quantity are treated with equal weight by the GA, and it allows the string 
to be internally represented as integers without serious loss of precision, which is computationally desirable.
Once scaled, the factory discharges must be collected together to create the string. In most GA applications a linear 
representation of the solution to a problem is used. For this chemical waste problem we might consider forming such a 
string in the manner depicted in Figure 11.5.
  
< previous page
page_187
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_187.html2009-7-7 16:57:49

page_188
< previous page
page_188
next page >
Page 188
Figure 11.5 
A possible linear string encoding.
When such a linear string is processed by the operators within the algorithm, the crossover operator will cut and splice 
the string at randomly chosen positions along the length of the string. It is easy to see that this is potentially a very 
destructive operation. If, for example, an individual arose in a GA run in which all pH data were well fitted, this 
individual would almost certainly be fitter than others in its generation since it would successfully predict the 
monitoring station pH measurements. However, in a linear encoding the pH values for different factories are spread 
throughout the string, so that the crossover operation will generally divide the pH values between different child strings, 
thereby destroying or at best diluting this valuable information. To circumvent this difficulty one might consider a linear 
encoding which placed all the pH values together. However, this would in turn separate other related string elements, 
such as all discharges from a particular factory at a given time. This inevitable disruption of logically connected string 
elements by crossover will severely affect the performance of the GA and prevent it converging on any high-quality 
results.
To counteract this, a representation of the data must be chosen that does not separate related string elements. This 
problem has been discussed in the literature[CH93] and can satisfactorily be resolved by using a multidimensional 
string. In this algorithm, we have used for the first time a three-dimensional string, forming a matrix of data values that, 
after scaling, directly corresponds to an array of factory discharge data (Figure 11.6).
  
< previous page
page_188
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_188.html2009-7-7 16:57:49

page_189
< previous page
page_189
next page >
Page 189
Figure 11.6 
Three-dimensional string encoding.
In this form of string, every item of data in the string is located near to the items to which it most closely relates. 
Therefore crossover does not unavoidably disrupt strings and the GA evolution can now proceed with a good chance of 
success.
11.4.2 
The Fitness Function
Two factors are used to assess the quality of a string. The more important factor relates to the deviation between the 
data recorded at the monitoring station and the equivalent data predicted by the string. A high-quality string codes for a 
solution which faithfully reproduces the monitoring data. Each string is therefore initially decoded to give factory 
discharge data. From these data the corresponding flow of waste within the pipe network is found, and the consequent 
monitoring station readings are calculated. These calculated monitoring station measurements are then compared to the 
actual measurements. The poorer the agreement, the lower the fitness of the individual. In this way the GA is 
encouraged to search for a set of factory discharges that accurately mimic the recorded monitoring station data.
Every string is also assessed in terms of the extent to which it meets known constraints on the discharges from each 
factory. Information such as the maximum possible range over which discharge of a particular pollutant may vary, the 
typical short-term variation of each pollutant and ratios between different quantities may all be built into the algorithm. 
If the output of a factory, as encoded in a string, then deviates from some specified bounds, the fitness of the individual 
is reduced by an amount related to the extent of the deviation. The GA is thus gently guided towards solutions that not 
only reproduce the monitoring station data but that also give realistic factory emissions.
  
< previous page
page_189
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_189.html2009-7-7 16:57:49

page_190
< previous page
page_190
next page >
Page 190
11.4.3 
Crossover Operator
The GA operators used to select parent strings and to establish the next generation (tournament selection with 
replacement and elitist selection respectively) have been chosen for simplicity and speed of execution, and their 
refinement will be the subject of future work on this algorithm. The crossover and mutation operators are less standard 
due the unusual nature of the problem.
The new crossover operator that has been developed is an extension of Harris' UNBLOX operator (Harris[Har91]) 
which was proposed to perform an unbiased two-point crossover on a two-dimensional string. In this method, two 
points are chosen within the string matrix, and define a rectangular section of that matrix. This area is then swapped 
between individuals. In three dimensions the requirement that crossover be unbiased leads to a variety of crossover 
operators (Figures 11.7, 11.8). To ensure that the operator is unbiased with respect to each of the string elements, the 
crossover area is allowed to 'wrap around' the string matrix. All locations within the string are then selected for 
crossover with equal probability, and therefore can be fitted with equal weight by the GA.
Figure 11.7 
A simple three-dimensional crossover operation.
  
< previous page
page_190
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_190.html2009-7-7 16:57:50

page_191
< previous page
page_191
next page >
Page 191
Figure 11.8 
Crossover volumes wrapping around in 0, 
1, 2 or 3 directions.
11.4.4 
Mutation Operator
In traditional binary GA representations, individuals are mutated with a very low probability, commonly 0.01 or lower. 
However, when the items in the string are not binary but chosen from a large range of integers (or from the infinite 
range of real numbers), mutation is considerably more important. For example, suppose each position in a population of 
50 strings contains a random integer in the range 0 to 1000. For any given position, a maximum of 50 of the possible 
1000 integers will occur in the population. Since crossover does not introduce new values into the population but merely 
reshuffles existing ones, without mutation at least 950 integers would be unavailable to each location in the string. 
Mutation ensures that the entire search space is accessible to the GA, but if the mutation rate is low, the time required to 
cover the range of permissible values within a string becomes extremely large. Therefore in this algorithm, the mutation 
probability is high at 0.5 per string. Moreover, it is tested five times for each individual, so that each new child 
individual produced will, on average receive 2.5 mutations.
The form of the mutation used is one in which an existing string element is displaced by a random amount, chosen from 
within a maximum mutation range that varies during the GA run (Bramlette[Bra91]). Initially the mutation can generate 
a new value selected arbitrarily from within the complete range of integers, so that the whole search space is available to 
the GA. The range decreases exponentially to a small value and then remains constant for the remainder of the run. This 
allows the GA to investigate the search space very widely initially, but later gives the GA more local search behaviour, 
so that it refines the solutions it has found. There are obvious
  
< previous page
page_191
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_191.html2009-7-7 16:57:50

page_192
< previous page
page_192
next page >
Page 192
similarities here with simulated annealing.
11.5 
Results
11.5.1 
Input Data Generation
To evaluate the performance of the algorithm, and investigate its behaviour, a simulated chemical plant has been used. 
Each monitoring station can measure any combination of waste flow parameters to any chosen degree of precision. The 
characteristics of each factory are also specified, including any bounds and relationships that are required. The factories 
are then simulated over a period of time and their outputs used to calculate the 'recorded' monitoring station data. 
Finally, these recorded data are overlaid with Gaussian noise. There are several advantages to using simulated data; in 
particular it is possible to determine the performance of the algorithm by direct comparison of the results obtained with 
the original factory discharges used in creating the data set.
11.5.2 
Results for a Four Factory System
The first data set for which results are reported is a modest one, involving four factories and three monitoring stations 
(Figure 11.9), though even for a system of this size 100 variables are required within the GA string.
Figure 11.9 
The initial waste flow network.
A portion of typical factory discharge data produced by the simulation is given in Table 11.1. Flow rates are measured 
in m3 per hour, and concentrations in ppm.
Repeated GA runs were made using the monitoring station data produced by these discharge data, with a population size 
of 50 and a run length of 10 000 generations. The GA converged to solutions of similar fitness within 4 000 to 5 000 
generations. Table 11.2 shows a portion of the best string generated in one run. For ease of comparison
  
< previous page
page_192
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_192.html2009-7-7 16:57:51

page_193
< previous page
page_193
next page >
Page 193
Table 11.1
Time Step
Factory 1
Factory 2
Flow
pH
conc 1
conc 2
conc 3
Flow
pH
conc 1
conc 2
conc 3
1
1396
3.66
207
525
54.7
5572
12.36
677
358
9
2
1840
3.68
18
46
77.1
5589
12.57
665
366
7.7
3
1353
3.55
270
697
54.9
5635
12.17
658
343
9.7
4
1447
3.37
49
111
43.8
5834
11.84
641
325
7.6
5
1319
2.83
48
114
60.2
5604
11.97
618
304
9.1
 
Table 11.2
Time Step
Factory 1
Factory 1
Flow
pH
conc 1
conc 2
conc 3
Flow
pH
conc 1
conc 2
conc 3
1
1380
3.65
207
525
54.7
5600
12.33
673
362
9.1
2
1830
3.68
18
46
77.1
5560
12.59
668
364
7.4
3
1350
3.56
270
697
54.9
5640
12.18
657
347
9.4
4
1450
3.37
49
111
43.8
5830
11.79
642
329
7.3
5
1330
2.83
48
114
60.2
5570
11.97
623
311
9.4
 
with the known 'theoretical' results, the string is plotted in Figure 11.10 as a scatter graph. It is evident from the figure that 
agreement is generally good, but that three values for factory 3 are poorly fitted by the algorithm. A similar plot for the emissions 
of factory 3 alone indicates that the most badly fitted points all correspond to pH measurements. We can understand why 
agreement with the simulation is poorer for factory 3 by considering the geometry of the flow network and the flow rate data for 
the four factories (Table 11.3).
Factory 1 is the sole input to the first monitoring station (Figure 11.9), and, as shown in Figure 11.10, the data from this factory 
are well fitted by the GA. Similarly the outputs of factory 4 are available to the GA as the differences between the data of the 
second and third monitoring stations. The second monitoring station records the combined outputs of the first three factories. Of 
these, factory 2 has by far the greatest output volume, and therefore has the greatest influence on the combined waste, which is 
measured in terms of pH's and concentrations, 'swamping' the much smaller output of factory 3. The GA therefore has 
correspondingly little information about it, and is unable to fit it effectively.
Of all the parameters used to characterise the waste, the pH is the most susceptible to this swamping effect. For example at time 
step 1 in the data set, factory 2 produces
Table 11.3
Time Step
1
2
3
4
5
Factory 1
1396
1840
1353
1447
1319
Factory 2
5572
5589
5635
5834
5604
Factory 3
901
952
927
902
959
Factory 4
7566
7556
7585
7595
7603
 
  
< previous page
page_193
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_193.html2009-7-7 16:57:52

page_194
< previous page
page_194
next page >
Page 194
Figure 11.10 
A scatter graph of calculated versus actual data.
waste at 5572 m3 hour1 at a pH of 12.36, whereas factory 3 produces waste at 901 m3 hour1 at a pH of 4.84. 
Combining these amounts of waste gives a total waste flow of 6473 m3 hour1 at a pH of 12.29 (pH readings are 
calculated on a logarithmic scale, so cannot simply be summed). This figure clearly gives much more information about 
factory 2 than it does about factory 3.
From this example of the application of the algorithm, a general conclusion can be drawn about how it might handle 
other, more complex, problems. The GA unavoidably concentrates on those discharges that have the most significant 
effect on the eventual output of the waste network. It is clear, therefore, that the GA, in general, will be able to 
accurately determine the parameters describing high-output factories, whilst those factories with smaller outputs will be 
relatively poorly fitted. This is an inherent feature of the GA fitness function, which is primarily driven by the 
monitoring station data. Whilst it would be possible to override this tendency through the introduction of different 
weights into the fitness function, it is not necessarily in itself undesirable.
The speed of the algorithm on this model problem is encouraging, in view of the need to scale up the calculation to 
handle real-world complexes. Increasing the number of factories or the number of monitored quantities would of course 
increase the size of the string used and dramatically increase the size and complexity of the search space. However, 
early results on much larger systems have provided good evidence that the algorithm continues to perform well 
(Cartwright, Jesson and Sztandera[CJSon]).
11.6 
Applications of the Algorithm
In its current form the algorithm works on a batch of monitoring station data, but it can be recast into a form suitable for 
continuous analysis. In this form a continuous stream of data from the monitoring network is analysed to reveal the 
outputs of factories at any given time. The algorithm could then provide rapid feedback to the
  
< previous page
page_194
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_194.html2009-7-7 16:57:53

page_195
< previous page
page_195
next page >
Page 195
plant and factory managers. The algorithm also has potential as an analytical tool for the design and maintenance of 
waste disposal networks. The GA can be applied to the analysis of simulated data within waste networks of various 
geometries, in order to assess the optimum points within that network to locate monitoring stations. It could equally 
well provide guidance on the types of pollutant it would be most productive to monitor at those stations. If the GA, 
when applied to the data from a given network, provided solutions with substantial uncertainty in one or more 
parameters, this would indicate that extra monitoring data will be required to pin down the solution more precisely. By 
examining how the consistency of the results depends upon the geometry of the network, high-quality geometries can be 
developed.
11.7 
Acknowledgements
We are pleased to acknowledge the support of Hewlett-Packard for this research.
References
[Bra91] Bramlette M. F. (1991) Initialization, mutation and selection methods in genetic algorithms for function 
optimization. In Proceedings of Fourth International Conference on Genetic Algorithms, page 101. San Mateo.
[CC95] Cartwright H. M. and Cattell J. R. (1995) Studies of continuous-flow chemical synthesis using genetic 
algorithms. In Rayward-Smith V. J. (ed) Applications of Modern Heuristic Methods, pages 129144. Alfred Waller.
[CH93] Cartwright H. M. and Harris S. P. (1993) Analysis of the distribution of airborne pollution using genetic 
algorithms. Atmospheric Environment 27A: 17831791.
[CJSon] Cartwright H. M., Jesson B., and Sztandera L. M. (In preparation) Intelligent algorithmic interpretation of 
waste flow data from multi-unit industrial complexes. Information Sciences.
[Dav91] Davies L. D. (1991) Handbook of Genetic Algorithms. Van Nostrand, New York.
[FIB95] Fogarty T. C., Ireson N. S., and Bull L. (1995) Genetics based machine learning applications in industry and 
commerce. In Rayward-Smith V. J. (ed) Applications of Modern Heuristic Methods, pages 91110. Alfred Waller.
[Go189] Goldberg D. E. (1989) Genetic Algorithms in Search, Optimisation and Machine Learning. Addison Wesley, 
Reading, Mass.
[Go191] Goldberg D. E. (1991) The theory of virtual alphabets. In Schwefel H. P. and Manner R. (eds) Parallel 
Problem Solving from Nature. Springer-Verlag, Berlin.
[Har91] Harris S. P. (1991) Chemical mass balance calculations using genetic algorithms. PhD thesis, Chemistry Part 
II, Oxford University.
[Ree95] Reeves C. R. (1995) Genetic algorithms and combinatorial optimisation. In Rayward-Smith V. J. (ed) 
Applications of Modern Heuristic Methods, pages 111126. Alfred Waller.
[SE93] Schaffer J. D. and Eshelman L. J. (1993) Real-coded genetic algorithms and interval schemata. In Whitley L. D. 
(ed) Foundations of Genetic Algorithms 2. Morgan Kaufmann, San Mateo.
  
< previous page
page_195
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_195.html2009-7-7 16:57:53

page_197
< previous page
page_197
next page >
Page 197
12 
The Evolution of Solid Object Designs Using Genetic Algorithms
P. J. Bentley and J. P. Wakefield
Abstract
This paper describes an attempt to enable computers to generate truly novel conceptual designs of three-dimensional 
solid objects by using genetic algorithms (GAs). These designs are represented using spatial partitions of 'stretched 
cubes' with optional intersecting planes[BW94]. Each individual three-dimensional solid object has its functionality 
specified by an explicit objective function, which is utilised by GAs to evolve candidate designs.
12.1 
Introduction
The genetic algorithm (GA) is a highly flexible and powerful search algorithm that uses principles of evolution 
observed in nature to direct its search[Hol75, Gol89]. GAs can tackle optimisation problems if these problems are 
formulated in terms of search, where the search space is the space containing all possible combinations of parameter 
values, and the optimal solution is the point in that space where the parameters take optimal values. Indeed, GAs are 
commonly used to optimise designs, with some remarkable results[PD94]. However, in this paper we aim to 
demonstrate that GAs are able to do more than just optimise existing designs we propose that they can create entirely 
new designs from scratch.
The area of design creation using genetic algorithms is a relatively unexplored area. Using GAs to create new designs 
has the potentially great benefit of new conceptual designs being automatically created in addition to optimal designs. 
Research has so far been performed in limited ways, such as the use of GAs to create new conceptual designs from high-
level building blocks[PY93] although this often seems to be little more than the optimisation of connections between 
existing designs rather than true design by the GA. Genetic art is becoming more popular, with various voting systems 
now being on-line on the Internet (for example John Mount's 'Interactive Genetic Art' at http://robocop.modmath.cs.cmu.
edu.8001). Other art-evolution systems using
Modern HeuristicSearch Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith© 1996 John Wiley & Sons Ltd.
  
< previous page
page_197
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_197.html2009-7-7 16:57:53

page_198
< previous page
page_198
next page >
Page 198
humans as design evaluators have been created[Daw86, TL92], but as yet very few, if any, systems exist that can evolve 
a design from scratch with no human input during the evolution process. This paper describes an early prototype of an 
evolutionary design system, capable of doing just that.
12.2 
Representation
Evolving designs from scratch rather than optimising existing designs requires a very different approach to the 
representation of designs. When optimising an existing design, only selected parameters need have their values 
optimised (e.g. for a jet-turbine blade, such parameters could be the angle and rotation speed of the blade). To allow a 
GA to create a new design, the GA must be able to modify more than a small selected part of that design it must be able 
to modify every part of the design. This means that a design representation is required, which is suitable for 
manipulation by GAs. Many such representations exist, and some are in use by the evolutionary-art systems: a variant 
of constructive solid geometry (CSG) is used by Todd and Latham[TL92], others use fractal equations (e.g. John Mount 
see the WWW address given above), and Dawkins[Daw86] uses tree-like structures. For a system capable of designing 
a wide variety of different solid object designs, however, a more generic representation is needed.
A variant of spatial-partitioning representation has been developed for this work [BW94]. This representation uses 
spatial partitions of variable sizes, each one being a 'cuboid' with variable width, height, depth and position in space. 
Each primitive shape can also be intersected by a plane of variable orientation, but this refinement to the representation 
will not be considered in this paper. The main benefit of using such a variable-sized primitive shape as a spatial-
partition is that few partitions are required to represent designs. Significantly, the fewer the partitions in a design, the 
fewer the number of parameters that need to be considered by the GA. The main disadvantage, however, is that it is 
possible to represent illegal designs using the representation, i.e. it is possible for two spatial-partitions to overlap each 
other causing redundancy and ambiguity.
Thus some form of correction is needed, to convert overlapping primitives into non-overlapping primitives. Since this 
correction process needs to be executed for every new candidate design in the population, every generation, it must be 
fast.
12.3 
Correcting Illegal Designs
Various alternative methods for correcting illegal designs were examined:
12.3.1 
Method 1
Intersecting primitives are located by detecting whether any corner of one primitive lies within another primitive, Figure 
12.1. If they are found to overlap, the positions of the primitives are examined relative to one another. If the difference 
in position
  
< previous page
page_198
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_198.html2009-7-7 16:57:54

page_199
< previous page
page_199
next page >
Page 199
between the two primitives is greatest in the x direction, the widths of the primitives are reduced, if the difference is 
greatest in the y direction, the heights are reduced, and for a greatest difference in the z direction, the depths are 
reduced, Figure 12.2. Using the positions to determine which sides to move ensures that the primitives are changed by 
the minimum amount possible. When reducing the dimensions, they are reduced until instead of overlapping, the 
appropriate sides are touching. To prevent too much unnecessary squashing, all other sides of the primitives are kept in 
the same place by moving the positions of the two primitives slightly.
Figure 12.1 
Two primitive 
shapes are detected as 
overlapping: a corner of 
one primitive lies within 
the other.
Figure 12.2 
Distance between overlapping primitives is greatest 
along y-axis: heights (and ypositions) are changed.
This method of squashing any overlapping primitives until they touch was found to be very successful. However, a flaw 
in the overlapping detection method became apparent: it is possible for two primitives to overlap without any of the 
corners of one being within the other, Figure 12.3. This meant an alternative detection method was required.
  
< previous page
page_199
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_199.html2009-7-7 16:57:54

page_200
< previous page
page_200
next page >
Page 200
Figure 12.3 
Primitives not 
detected as intersecting: no 
corner of one lies within 
the other.
12.3.2 
Method 2
In this method, primitives begin 'life' as single points (specified by the centre positions of the primitives as described in 
the genotype). They are then grown in size by a small amount for a specific number of iterations. When two primitives 
grow to touch each other, the touching sides stop growing, otherwise the sides continue to grow until they reach the size 
specified by the genotype, Figure 12.4.
Figure 12.4 
Growing the primitives.
This method was found to work most of the time, and indeed mirrors nature attractively in that genotypes are mapped to 
phenotypes by a growth process. Another attraction was the way it was implemented the time complexity was reduced 
from O(np2) for the first method to O(npc), where n is the number of individuals in the population, p is the number of 
primitives in a design, and c is the constant (maximum) number of iterations the primitives were grown for.
  
< previous page
page_200
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_200.html2009-7-7 16:57:55

page_201
< previous page
page_201
next page >
Page 201
Figure 12.5 
Two primitives begin too close together and 
grow to overlap each other.
Unfortunately, the method did not work all of the time: it is unable to detect whether two primitives starting too close to 
one another and growing by too large an amount are touching or not, and so they grow to overlap each other, Figure 
12.5. A more serious problem though, was one of speed despite being a potentially faster method for very high numbers 
of primitives in a design, it was actually a very slow method for usable numbers of primitives. Another method was 
needed.
12.3.3 
Method 3
This method of squashing overlapping primitives is really a combination of parts of the first two methods. Primitives, 
when detected as overlapping, are squashed as described in the first method, Figure 12.2. To detect whether the 
primitives were overlapping, an idea was taken and extended from the second method. This method examines the sides 
of the two primitives and if at least one side of one primitive is between two sides of the other primitive for all three 
axes, the two primitives are overlapping. The time complexity returns to O(np2), but because the detection is nothing 
more than a conditional check, (see Figure 12.6) the process runs very quickly indeed.
Figure 12.6 
If side 1 or 3 lies between a and c, or if 
side a or c lies between 1 and 3, and if side 2 or 4 lies 
between b and d or if side b or d lies between 2 and 4, 
the two primitives are overlapping.
So the third method provides an effective way to correct illegal designs. However, this creates the question: where 
should designs be corrected?
  
< previous page
page_201
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_201.html2009-7-7 16:57:55

page_202
< previous page
page_202
next page >
Page 202
12.3.4 
Where to Correct the Designs
Each candidate design (or phenotype) is defined by the representation, and coded as the genotype which is directly 
manipulated by the GA. The question is then, should incorrect designs be corrected in the genotype, in the phenotype, or 
should they be prevented from existing in the population at all?
To deal with the last, first: if every time the GA creates an illegal design, that design is discarded, the GA will take 
substantially longer to evolve designs, since illegal designs are often evolved. If the illegal designs are allowed to exist 
in a population, but are corrected in the genotype, the population of candidate designs becomes very static, with little 
evolution occurring. This is because a high proportion of all new designs evolved by the GA are illegal designs if these 
are corrected every time, evolution is effectively being undone so the population stagnates.
So the solution must be to correct the phenotypes. This is achieved by allowing an illegal design (a design with spatial 
partitions overlapping) in the genotype, but correcting the design when it is mapped to the phenotype. In this way, the 
genotype no longer directly specifies the phenotype (or design), but instead gives directions that should be carried out if 
permissible. This method corresponds well with nature. For example, suppose that there is a single gene within a 
person's genotype that specifies how tall he should be (in reality it is probably a collection of genes). It might 'say' his 
ideal height should be 6 ft 5 in but in real life he might only be 6 ft. Why? Because whilst growing, he was restricted by 
gravity. If he had grown up in space without the restriction of gravity, he may well have become 6 ft 5 in.
Equally, the designs are restricted by the rules of the representation. Even though in the genotype, a primitive shape 
may be given a large width, when the genotype is mapped to the phenotype, that primitive may be squashed to a smaller 
size and new position by its neighbours. It is apparent that in nature, evolution takes into account such restrictions our 
genes compensate for gravity when specifying our heights, so although the human genes may be asking for a height of 6 
ft. 5 in. they 'know' that, because of gravity, this equates to 6 ft. Equally, as is shown by the experiments below, the 
evolution of the GA takes into account the restriction of the design representation, and compensates.
Figure 12.7 
Operation of the prototype evolutionary design system.
So, the GA evolves new designs by manipulating coded indirect representations in the genotypes, which are mapped to 
the direct representation of the phenotypes,
  
< previous page
page_202
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_202.html2009-7-7 16:57:55

page_203
< previous page
page_203
next page >
Page 203
Figure 12.7. Some form of guidance is obviously necessary to direct the evolution of the GA. This is provided by simple 
evaluation software effectively a software version of the design specification. There follows a series of experiments to 
demonstrate the way this evolutionary design system can evolve new designs from scratch, guided only by such 
evaluation software.
12.4 
The Evolution of a Table
The genetic algorithm used for the following series of experiments remains unchanged throughout, apart from varying 
the population size and the number of generations it runs for. A basic canonical GA was used[Müh92], with primitive 
'roulette wheel' partner selection[Gol89], real coding[Gol92] and no special multi-objective function handling 
capabilities. Designs were all limited to five primitives and the population was initialised with primitives of random size 
and location at the beginning of every test run (i.e. starting from scratch). Evaluation software was utilised to describe 
the functionality of a table by the use of an objective function consisting of a weighted sum of a number of design 
objectives.
Five experiments were performed, with the evaluation software being made more sophisticated for each experiment by 
the addition of an extra design objective.
12.4.1 
Size
Perhaps the most basic requirement for a table is that of appropriate size: a table the size of a house or a matchbox is of 
no great use. The size of the design is specified by minimum and maximum extents for the left, right, back, front, top 
and bottom of the design. The fitness of a candidate design decreases the further it is from the ideal size. Using a 
population of 50 individuals, designs of the required size had evolved after 25 generations, Figure 12.8.
  
< previous page
page_203
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_203.html2009-7-7 16:57:56

page_204
< previous page
page_204
next page >
Page 204
Figure 12.8 
Evaluation of size only.
12.4.2 
Mass
Another basic, but vital requirement is that of mass: a table weighing a ton would not be very practical. An ideal mass is 
defined, the fitness of the design decreasing the more the actual mass differs from the ideal mass. The ideal mass was 
defined as being a very low value. Designs with good fitnesses (the right size and very close to the ideal mass) had 
evolved after 100 generations, with a population of 50, Figure 12.9.
  
< previous page
page_204
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_204.html2009-7-7 16:57:56

page_205
< previous page
page_205
next page >
Page 205
Figure 12.9 
Evaluation of size and low mass.
The easiest way for the GA to create designs of low mass is to reduce the dimensions of the primitive shapes that make 
up the designs. This can produce fragmented designs however, where primitives become disconnected from the main 
part of the design. Since this work concentrates on the evolution of a single object at a time, such fragmented designs 
are heavily penalised. The fitness of fragmented designs only based on the sum of the distance of each primitive from 
the origin (normally the centre of the design). This addition to the evaluation software means that in a population of 
fragmented designs, evolution will first create whole designs by drawing in the primitives towards the origin. In a 
population of whole designs, a fragmented design is very unfit and normally never reproduces.
12.4.3 
Stability
A more complex requirement is stability under gravity the table must stand upright, and not fall over under its own 
weight. The stability of a candidate design is determined by calculating whether the centre of mass of the design falls 
within the outer extents of the primitive(s) touching the ground. (The ground is assumed to be at the lowest part of the 
design.) The more unstable the design is, the worse its fitness becomes. To help evolution, the population size was 
increased to 80 [GDC92]. After 130 generations, some very fit designs had evolved (the right size, stable, and almost 
the right mass), Figure 12.10 and 12.11. Strangely, evolution to good designs seemed remarkably improved with the 
addition of this new constraint.
  
< previous page
page_205
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_205.html2009-7-7 16:57:57

page_206
< previous page
page_206
next page >
Page 206
Figure 12.10 
Evaluation of size, low mass and stability.
Figure 12.11 
Evaluation of size, low mass and stability.
  
< previous page
page_206
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_206.html2009-7-7 16:57:58

page_207
< previous page
page_207
next page >
Page 207
12.4.4 
Flat Surface
Perhaps the most important requirement of a table is its flat, upper surface the table top. A good table should have a flat 
surface at an appropriate height from the ground, and covering an appropriate area. These criteria were evaluated by 
randomly sampling, five times, the top of the design within the required area of the table top, and checking how much 
the height at these points differed from the required height. (Analogous to five objects of negligible mass, e.g. feathers, 
being dropped onto the design within the area for which the flat surface is required. Should a feather be supported at too 
high or too low a height, or not be supported at all, the fitness of the design decreases proportionately.)
Since this evaluation is of a random nature, a design may be given a different fitness value every time it is evaluated 
(unless it fully satisfies the constraint). Not surprisingly, the GA found evolution to good designs a little harder to 
perform, so the population size was increased to 100, and the GA was allowed to run for 300 generations. The resulting 
designs were usually very good, but compromises were being made by the GA. It was not always able to fully satisfy all 
of the constraints since, in order to have a large flat surface, the size of the primitives must be increased, but to have a 
low mass, the size of the primitives must be decreased. Results were often designs with excellent table tops, but too 
massive (Figure 12.12), or designs with unusual multi-level table tops with the right mass (Figure 12.13). All designs 
were still the right size and stable, however.
Figure 12.12 
Evaluation of size, low mass, stability and flat top.
  
< previous page
page_207
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_207.html2009-7-7 16:57:58

page_208
< previous page
page_208
next page >
Page 208
Figure 12.13 
Evaluation of size, low mass, stability and flat top.
12.4.5 
Supportiveness and Stability
One final requirement for a table is that it must be able to support objects on its flat upper surface. Although the tables 
evolved so far stand up on their own, if an object was to be placed on them, most would still topple over. Greater 
stability is therefore required. This is achieved by a simple extension to the 'stability under gravity' constraint, which 
requires the table to be able to support a heavy object placed at the very edges of its extremities without falling over. 
The mass of the object was deliberately set very high (twice the required mass of the table itself).
The GA was allowed to run for 500 generations, and produced some good designs. Again compromises were made the 
GA quickly 'discovered' that an easy way to support a heavy object is to make the table even heavier, so it was 
necessary to increase the weighting of the 'low mass' constraint. Once this was done, the GA produced some remarkably 
fit designs, coming up with two main methods for increased stability: a very low centre of mass (Figure 12.14) or a very 
wide base (Figure 12.15), as well as producing designs the right size, nearly perfect flat surfaces, and almost the right 
mass.
  
< previous page
page_208
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_208.html2009-7-7 16:57:59

page_209
< previous page
page_209
next page >
Page 209
Figure 12.14 
Evaluation of size, low mass, flat top and greater stability.
Figure 12.15 
Evaluation of size, low mass, flat top and greater stability.
  
< previous page
page_209
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_209.html2009-7-7 16:57:59

page_210
< previous page
page_210
next page >
Page 210
12.4.6 
What the Tables are Not
Although it should be apparent that the GA can indeed create novel, usable designs for tables, it is also clear that the 
tables do not resemble the sort of four-legged table most of us are used to. This should not really be surprising, since the 
GA has no world knowledge and so cannot be influenced by existing designs. (Also the small number of primitives 
combined with the initial random seeding of the population is a factor in producing some of the more unusual designs.) 
As the software stands, it is theoretically possible for the GA to evolve a four-legged table, but it is highly unlikely. It 
would require four primitive shapes to be precisely the same length so that they all touch the ground at once. Any 
primitive which was a fraction too long or too short would no longer behave as a leg, thus resulting in a poor design, so 
a mutation or crossover would have to produce all four legs in one unlikely jump.
However, it is not at all difficult to make the GA produce four-legged tables. By enforcing symmetry about the x and z 
axis, more traditional, symmetrical table designs can be evolved. (An alternative approach would be to seed the initial 
population with existing table designs and allow the GA to combine the best features of these, but this would no longer 
be designing from scratch.)
The tables produced are not evaluated for aesthetics, which would require human input, and no stress analysis takes 
place, so deficiencies in these areas are occasionally apparent.
12.5 
Conclusions
The genetic algorithm is capable of more than design optimisation it can evolve entirely new designs. To do this, a 
flexible representation is required, to allow the GA to modify every part of the design. As shown in this paper, the GA 
can automatically take into account the background correction of illegal designs (i.e. those not following the rules of the 
representation). It can cope with partially conflicting constraints (e.g. the low mass against the large table-top and 
greater stability), and randomly varying evaluations (e.g. the random sampling of the table-top evaluation) and still 
produce some excellent results.
In the 'evolution of a table' example, the multiobjective function defining the table was treated as a single objective 
function by weighting and summing the separate functions. Although the GA coped surprisingly well, this is not an 
ideal solution[Go189] and in the future the GA will be modified to evolve Pareto optimal solutions to such multicriteria 
design problems[HN93].
Future work will also be directed towards the creation of a system capable of evolving shapes other than 'cuboid' in 
appearance by allowing the GA to use the refinement to the representation, i.e. primitive shapes will be permitted to 
have portions 'sliced' off them. It is also anticipated that the GA can be made to evolve not only the position and 
dimensions of the primitive shapes, but also the number of primitives making up a design.
  
< previous page
page_210
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_210.html2009-7-7 16:57:59

page_211
< previous page
page_211
next page >
Page 211
References
[BW94] Bentley P. J. and Wakefield J. P. (1994) Generic representation of solids for genetic search. Microcomputers in 
Civil Engineering 11(3): to appear.
[Daw86] Dawkins R. (1986) The Blind Watchmaker. Longman Scientific & Technical Publishers, Harlow.
[GDC92] Goldberg D. E., Deb K., and Clark J. H. (1992) Genetic algorithms, noise, and the sizing of populations. 
Complex Systems 6: 333362.
[Gol89] Goldberg D. E. (1989) Genetic Algorithms in Search, Optimization & Machine Learning. Addison- Wesley, 
Reading, MA.
[Gol92] Goldberg D. E. (1992) Real-coded genetic algorithms, virtual alphabets and blocking. Complex Systems 5: 139-
167.
[HN93] Horn J. and Nafpliotis N. (1993) Multiobjective optimisation using the niched pareto genetic algorithm. 
Technical Report 93005, Illinois Genetic Algorithms Laboratory.
[Hol75] Holland J. H. (1975) Adaptation in Natural and Artificial Systems. University of Michigan Press, Ann Arbor.
[Müh92] Mühlenbein H. (1992) Darwin's continent cycle theory and its simulation by the prisoner's dilemma. In Conf. 
Proc. 1st European Conference on Artificial Life Towards a Practice of Autonomous Devices, Paris, pages 236244.
[PD94] Parmee I. C. and Denham M. J. (1994) The integration of adaptive search techniques with current engineering 
design practice. In Proceedings from Adaptive Computing in Engineering Design and Control -'94, Plymouth, pages 1-
13.
[PY93] Pham D. T. and Yang Y. (1993) A genetic algorithm based preliminary design system. Journal of Automobile 
Engineers 207(D2): 127133.
[TL92] Todd S. and Latham W. (1992) Evolutionary Art and Computers. Academic Press, New York.
  
< previous page
page_211
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_211.html2009-7-7 16:58:00

page_213
< previous page
page_213
next page >
Page 213
13 
The Convoy Movement Problem with Initial Delays
Y. N. Lee, G. P. McKeown and V. J. Rayward-Smith
Abstract
The convoy movement problem (CMP) with initial delays is presented and three methods for its solution are discussed 
and compared. These methods are a pure branch-and-bound (B&B) algorithm, a hybrid genetic algorithm / B&B 
algorithm and a pure genetic algorithm. Each approach is shown to have its advantages and the hybrid method also 
provides an illustration of a genetic algorithm in which the fitness of a solution depends upon the time when it is 
evaluated.
13.1 
Introduction
In [YNLRS94], a combinatorial optimization problem called the convoy movement problem (CMP) is introduced and 
described. A branch-and-bound algorithm for solving a basic version of the CMP, and a description of its 
implementation on a network of transputers, is given in [LMRS94]. In this paper, we describe work carried out to 
incorporate delays into the model and, in particular, the use of genetic algorithms within solution techniques. In this 
section, we give a brief formal description of the CMP with delays and in subsequent sections, three different 
approaches to solving this problem are discussed, two of which use genetic algorithms. Results are presented for 
experiments on test data provided by the DRA (Malvern) for each of these approaches. Finally, we give our conclusions 
and raise a number of issues relating to further work.
In the CMP, we are given a collection of m military units (convoys),
Convoy i is to move from a given source position, si, to a given target position, ti, over a limited route network. The 
objective is to find a set of paths with the minimal completion time for all of the convoys subject to certain constraints.
Different types of convoy may move at different speeds along the same parts of
Modern HeuristicSearch Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith© 1996 John Wiley & Sons Ltd.
  
< previous page
page_213
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_213.html2009-7-7 16:58:00

page_214
< previous page
page_214
next page >
Page 214
the network. Indeed, some types of convoy may not be able to use some parts of the network at all. Furthermore, a 
convoy of a given type may travel at a different speed when moving in one direction along part of the network than it 
does when moving in the opposite direction.
Let K = {0,1,  . . .,p 1} denote the set of convoy types and let V = {0, . . ., n 1} denote the set of intersection points in the 
route network. For each type of convoy, k ∈ K, a matrix [Cuvk] is specified. Each Cuvk value is either a positive integer 
or ∞. If Cuvk is ∞ then no convoy of type k may travel directly from vertex u to vertex v. Otherwise, Cuvk represents the 
number of time units required by a convoy of type k to travel directly from vertex v to vertex v. A convoy travels 
'directly' from vertex u to vertex v if and only if it passes through no other intersection point en-route from u to v. We 
will refer to Cuvk as the 'cost' for a convoy of type k travelling along the edge from u to v.
Paths followed by different convoys do not have to be disjoint but two convoys may not occupy the same part of the 
network at the same time. With each convoy, i, we associate a time window, wi∈ Z+, representing the number of time 
units that the entire convoy takes to pass a point on the slowest part of the network for this type of convoy. wi is so 
defined that if the head of convoy i arrives at a point at time t, then the tail of i arrives at that point at time t + wi,. Thus 
convoy i occupies this point from the start of the tth time unit up to the end of the (t + wi)th time unit.
Associated with each convoy, i, is its earliest ready time, ei. This is the earliest time at which convoy i can start its 
movement. Without loss of generality, we may assume that ei = 0 for at least one convoy. However, convoy i does not 
have to start at time ei but may be delayed by di time units. Convoy i would then start at time ei + di. Although ei is a 
fixed parameter for a particular problem instance, di is a variable whose value is to be determined.
We define a path of length l by π = vo  . . . vl, where vg∈ V, g = 0, . . ., l. The cost of path π for a convoy of type k is 
defined by
and its length is denoted by lπ Let IIuv denote the set of all possible paths, uv1 . . .uh v from u ∈ V to v ∈ V via zero or 
more intermediate vertices vg∈ V, and let πi, denote a path taken by convoy i. Then πi is a feasible path for convoy i 
 and cost (πi,ki) < ∞. If πi = vo  . . . vg  . . . vl then πig denotes vo  . . . vg, i.e. πig denotes the first g legs of 
πi. To refer to individual vertices in a path, we use vig to denote the (g + 1)th vertex in πi thus
The solution space, S, for the CMP is an m-tuple of paths, one for each of the convoys. We define S to include partial 
paths for convoys as well as complete paths. Thus,
A feasible solution to the CMP is then a solution (π0, . . ., πm1) ∈ S such that vi = ti, for each i and for which 
constraints relating to the unique occupancy of parts of the network at any time are satisfied.
  
< previous page
page_214
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_214.html2009-7-7 16:58:01

page_215
< previous page
page_215
next page >
Page 215
The cost of the movement of a convoy is equal to the sum of the path cost, the actual starting time and the time window 
for the convoy. The time window is included here to allow time for the tail of the convoy to arrive at the target position. 
Thus the movement cost of convoy i is defined as
Figure 13.1 
Constraints for the CMP.
The primary objective of the CMP is to find an optimal solution having the least completion time over all feasible 
movements, i.e. to minimize
where the minimum is taken over all feasible (π0, . . ., πm1) ∈ S. To avoid generating paths with unnecessarily high 
costs, we include a secondary objective to minimize the total completion time of all the movements,
Again, we seek to minimize objective function (02) over all feasible movements in S. The CMP constraints are specified 
formally in Figure 13.1.
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_215.html（第 1／2 页）2009-7-7 16:58:02

page_215
< previous page
page_215
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_215.html（第 2／2 页）2009-7-7 16:58:02

page_216
< previous page
page_216
next page >
Page 216
Constraints (C1) ensure that each convoy finishes its movement on or before its specified finishing time. Constraints 
(C2) prevent more than one convoy using the same vertex at the same time. In these constraints, qig denotes the set of 
time units during which convoy i occupies vertex vig on its path πi. If convoy i is of type k, then the time, aig, at which 
the head of convoy i arrives at vig is given by
The time at which the tail of convoy i arrives at vig is
It then follows that
The qualification in (C2) that g and h are not both simultaneously zero means that two convoys may share the same 
starting vertex. Constraints (C3) ensure that no convoy overtakes any other convoy on any edge at any time. We also 
prohibit any convoy from passing any other convoy moving in the opposite direction along the same edge (C4).
Allowing delayed starts means that a convoy can wait at its source location for an arbitrary length of time if this leads to 
a better overall solution. When a convoy, i, waits at its source location, si, other convoys are allowed to use s, until 
convoy i starts to move. For simplicity, we assume that the initial delay for a convoy must be a multiple of a prescribed 
waiting interval, or waiting cost. The given waiting interval may be different for different source vertices and may also 
be different at a given source vertex for different types of convoy. For convenience, we assume that the waiting 
intervals associated with vertices are represented by diagonal entries in the [cuvk] matrices. The starting time of a 
convoy, i, with source vertex, v, is thus 
, where ri≥ 0 is a variable to be determined and represents the 
number of waiting intervals that convoy i waits at its source vertex, v. Other convoys can use vertex v from time 0 to 
time 
.
13.2 
A Branch-and-Bound Approach
We have developed a program implementing a branch-and-bound (B&B) algorithm for solving the CMP with delays. 
This program is based on the program described in [MRSLL94b] and details are given in [MRSLL94a]. The programs 
have been developed within the general B&B framework described in [MRST91] and build on the associated kernel 
software.
The CMP has a number of features in common with multi-commodity problems (see, for example, [KH80, MW84, 
GC94]).
All experiments reported in this report were performed on a DEC Alpha 3000/400 running OSF/1 and using the DEC cc 
compiler with -02 optimization turned on. The timings reported are measured in CPU seconds. We have used four test 
problems for our experiments. Problems 1 and 2 were supplied by the DRA and problem 3 is our own generated 
problem. A summary of the DRA problems is shown in Table 13.1. All of the edges in these examples are bi-directional.
  
< previous page
page_216
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_216.html2009-7-7 16:58:03

page_217
< previous page
page_217
next page >
Page 217
Table 13.1 DRA test problems
No. of 
vertices
No. of 
edges
No. of 
convoys
Problem 1
160
212
17
Problem 2
530
724
25
 
Test problem 2a is a variation of test problem 2 in which the input order of the convoys is different. Test problem 3 has 
been specially generated to test the pathological situation where every vertex in the network is connected to several 
vertices which form a bottleneck. Figure 13.2 shows an example of such a network. Test problem 3 has 15 vertices on 
each side of the bottleneck and a bottleneck comprising just three nodes. The 15 vertices either side of the bottleneck 
form the source and target locations for 15 convoys. These source and target vertices are connected directly to the 
bottleneck vertices and do not have any other connections. All connections are bi-directional and the cost of each edge 
is generated randomly between 1 and 19. Note that the cost of travelling in one direction is not necessarily the same as 
the cost of travelling in the other direction. All 15 convoys are of the same type and have time windows set to 1. This 
means that any convoy will take 2 time units to pass any point on the network.
Figure 13.2 
Test problem 3.
Various parameters of the B&B code can be adjusted easily by altering the input data files. However, for the results we 
report here, we used a depth-first search method; both 01 and 02 were used in the bounding and priority functions, with 
01 having the primary role. We took a time window for each convoy of 10 for test problems 1 and 2 and of 1 for test 
problem 3, our initial bounds for the two objective functions
  
< previous page
page_217
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_217.html2009-7-7 16:58:04

page_218
< previous page
page_218
next page >
Page 218
were (∞, ∞) and the path length limit was 70. The maximum size of the active pool was 3000.
For each problem, a maximum of 4 experiments were conducted. For experiment 1, no delay was allowed. For 
experiments 2, 3 and 4, delayed starts were allowed. The waiting intervals at all the source locations are set to the same 
value for each experiment. For all problems, the waiting intervals used in experiments 2 and 3 are prescribed as 1 and 2, 
respectively. For experiment 4, the value 11 (the average of all time windows + 1) is used for problems 1 and 2 and the 
value 3 is used for problem 3. Note that the waiting intervals at all source locations are set to the same value for 
simplicity only. They can take different values if desired.
Table 13.2 shows the results. The optimal value found in experiment 1 is (1339, 16 064) when delays are disallowed. 
However, when delayed starts are allowed, better solutions are found. The optimal values for experiments 2, 3 and 4 are 
(1339, 15792), (1339, 15 799) and (1339, 15 819), respectively. As we can see from Table 13.2, the price we pay for an 
improved solution is an increased execution time. This is because the size of search space is significantly increased 
when delayed starts are permitted. However, we should also note that in all of the three cases, a solution that is better 
than the optimal solution of experiment 1 is found in a shorter time.
Table 13.2 Results of test problem 1 B&B approach
Experiment
1
2
3
4
Waiting Costs
0
1
2
11
First Solution
O1, O2
1339,16 201
1339,16 201
1339,16 201
1339,16 201
Time (secs)
0.83
0.87
0.85
0.86
Intermediate Solution
O1, O2
1339,16 149
1339,16 060
1339,16 060
1339,16 057
Time (secs)
0.95
2.56
3.13
3.46
Optimal Solution
O1, O2
1339,16 064
1339,15 792
1339,15 799
1339,15 819
Time (secs)
3.77
242.00
135.20
21.29
Termination Time
Time (secs)
5.87
279.09
152.85
24.64
 
If we interpret no delayed starts as having very high waiting costs at all source locations, then in general, the total 
execution time of the B&B increases with decreasing waiting costs while the quality of the optimal solutions improves 
with decreasing waiting costs.
We also performed some experiments using best-first search. For experiment 1, the algorithm terminates after about 5 
seconds, which is faster than the time for the depth-first search. However, for experiments 2 and 3, the active pool 
overflowed. This may be expected because the waiting costs are set to very low values. This means that nodes with 
delayed starts are likely to have very good bounds. With a best-first search, these nodes will be expanded first, which in 
turn generates more nodes with delayed
  
< previous page
page_218
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_218.html2009-7-7 16:58:04

page_219
< previous page
page_219
next page >
Page 219
starts. Thus, the active pool can very quickly become flooded with these nodes.
Table 13.3 shows the results of test problem 2. As problem 2 is much larger than problem 1, with a very large search 
space, we have not managed to solve this problem to optimality. Despite the fact that the program has not terminated, 
we have been able to find some very good solutions within a relatively short time. The best values found for 
experiments 1, 2 and 3 are (2377, 35648), (2377, 35644) and (2377, 35644), respectively. The value for the primary 
objective, 01, in these solutions is optimal whilst the value for the secondary objective, 02, is within 4.7% of the lower 
bound of the optimal value. We force the termination of all three experiments after resolving another 500 to 1000 
clashes without producing a better solution. The percentage deviation shows how close a solution is from a lower bound 
for the optimal value.
Table 13.3 Results of test problem 2 B&B approach
Experiment
1
2
3
Waiting Cost
0
1
2
Lower Bound of Optimal Objectives : 2 377,34 070
First Solution
O1, O2
2377,35 701
2 377,35 701
2 377,35 701
% Deviation
0, 4.78
0, 4.78
0, 4.78
Time (secs)
11.86
11.95
12.05
Intermediate Solution
O1, O2
2377, 35 654
2 377, 35 648
2 377, 35 648
% Deviation
0, 4.65
0, 4.63
0, 4.63
Time (secs)
56.16
417.45.76
82.74
Best Solution
O1, O2
2377,35 648
2377,35 644
2377,35644
% Deviation
0, 4.63
0, 4.62
0, 4.62
Time (secs)
81.68
454.78
170.41
Forced Termination
Time (secs)
139.13
579.43
322.79
 
Table 13.4 shows the results of test problem 2a. For each experiment in this set, we have left the program running for 
5000 seconds. We can see from the table that, in all our cases, a longer time is needed to generate a good solution with 
this ordering than with the ordering of problem 2.
For problems 1 and 2, we obtained the optimal 01 value in the first solution produced. However, for problem 3, in all 
four experiments reported in Table 13.5, the 01 value of the first solution found is not optimal. This is due to the 
bottleneck in the road network as a result of which the critical convoy is not so easily identifiable. Delayed starts 
improve the quality of the solution significantly. Both 01 and 02 values are improved with delayed starts. As expected, 
experiment 2 takes longer to terminate than experiment 3 but generates better values. Experiment 4, surprisingly, did 
not terminate as quickly as we expected. It had not terminated after 1700 seconds, which is longer than the time for 
experiment 2. This may be due to the high waiting cost used (greater than wi + 1), which makes the scheduling around 
the bottleneck even
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_219.html（第 1／2 页）2009-7-7 16:58:05

page_219
< previous page
page_219
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_219.html（第 2／2 页）2009-7-7 16:58:05

page_220
< previous page
page_220
next page >
Page 220
Table 13.4 Results of test problem 2A B&B approach
Experiment
1
2
3
Waiting Cost
0
1
2
Lower Bound of Optimal Objectives : 2 377,34 070
First Solution
O1, O2
2 377,36 508
2 377,36 508
2 377,36 508
% Deviation
0, 7.16
0, 7.16
0, 7.16
Time (secs)
13.68
13.69
14.01
Best Solution
O1, O2
2 377,36 070
2 377,36 070
2 377,36 070
% Deviation
0, 5.87
0, 5.87
0, 5.87
Time (secs)
2 225.33
2 219.09
2 216.93
Forced Termination
Time (secs)
5000
5000
5000
 
harder.
Table 13.5 Results of test problem 3 B&B approach
Experiment
1
2
3
4
Waiting Costs
1
2
3
First Solution
O1, O2
30, 278
30, 278
30, 278
28, 254
Time (secs)
0.54
0.56
0.54
0.51
Optimal Solution
O1, O2
26,238
23, 210
23, 215
24, 220
Time (secs)
2.51
1 551.20
315.20
4.70
Termination Time
Time (secs)
71.02
1 616.57
349.12
 
We can see from our results that:
1. Delayed starts do in general produce better solutions. The trade-off for this is longer execution times.
2. The finer the waiting costs, the better the solutions. The trade-off is again greater computational time.
3. The ordering of the convoys affects the positions of individual solutions in the search tree, which in turn affects the 
total execution time.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_220.html（第 1／2 页）2009-7-7 16:58:06

page_220
One of the problems with our approach is that the B&B search tree is potentially very large and it is not physically 
possible to store the whole tree. Therefore, it is important to use a search strategy which minimizes the proportion of the 
search tree
  
< previous page
page_220
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_220.html（第 2／2 页）2009-7-7 16:58:06

page_221
< previous page
page_221
next page >
Page 221
which must be held in memory. This effectively rules out both breadth-first and best-first search strategies as they 
require far too much memory to be practical.
While the use of a depth-first search can keep a check on the storage requirements, we have very little control over the 
execution time of the algorithm. Indeed, if we start off on the wrong branch, the algorithm may not terminate within a 
reasonable time. This is because the depth-first search chooses a node for expansion based on its position in the tree 
rather than the quality (bound) of the node. Thus, with a large search tree, the execution time is heavily dependent on 
finding a good solution at an early stage in order to allow early pruning of the tree.
The structure of the search tree is determined by the branching strategy used. We have described and discussed various 
branching strategies and their effects on the structure of the search tree in [MRSLL94b]. All the branching strategies we 
have considered may be strongly affected by the ordering in which the convoys are considered, although the extent of 
the impact varies from strategy to strategy. This impact is clearly shown by the difference between the results of 
problem 2 and 2A. In some cases, the difference is much more significant. Unfortunately, we have not currently been 
able to identify characteristics for good and bad orderings. Although we suspect there is no single best ordering for all 
problems, we expect that there may be good orderings for problems with certain characteristics. As the performance 
may vary significantly with different orderings, this may be worthy of further research.
A node in the search tree represents an m-tuple of convoy paths, one for each convoy, from its source location, si, to the 
current location, ui. In our current implementation, we use a fairly simple bound calculation based on the current cost 
(from si to ui) and the minimum cost path from ui to ti.
13.3 
A Hybrid Approach : 
Using GAs for Delays and B&B for Paths
In this section, we present a hybrid approach for solving the CMP with delays. We use a GA to compute the delays and 
a B&B algorithm to compute the paths. We will refer to this as the hybrid system.
As we can see from the results presented above, allowing delays does in general produce better solutions. However, by 
incorporating delays into the B&B implementation, the total search space increases significantly.
For large problems, our current B&B implementation may take a very long time to terminate due to the potentially large 
search space. For example, for test problem 2, we do not know how long our B&B program would take to terminate 
naturally, either with or without delays. We have left a version without delays running for more than a week without it 
terminating. By adding delayed starts, the program can take considerably longer to terminate naturally because of the 
increased search space. Thus, using a GA to compute delays may be more effective than a pure B&B approach when 
dealing with large problems. For smaller problems, pure B&B can find the optimal solution very quickly, and we would 
not expect the hybrid approach to be worthwhile in such cases.
The hybrid system is implemented on a DEC Alpha workstation using the X-GAmeter toolkit which provides an X-
window interface. X-GAmeter has been
  
< previous page
page_221
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_221.html2009-7-7 16:58:06

page_222
< previous page
page_222
next page >
Page 222
developed at UEA and provides an easy to use framework and interface for designing and experimenting with GAs for 
optimization problems. General GA functions are included in the toolkit. Details of X-GAmeter can be found in 
[KRSS93].
For any GA, the most important (problem dependent) issues are the representation of the bit string and the definition of 
the fitness function. We now discuss both of these issues for the GA used in our hybrid system.
For simplicity, let the delay time, di, associated with convoy, i, be of the form ri × wi, where wi is a prescribed waiting 
interval. The bit string is then used to represent the m-tuple (ro, ri,  . . ., rm1). Figure 13.3 shows the structure of such a 
bit string. Each ri, is represented by b bits which means that ri can take a value between 0 and 2b 1, inclusively.
Figure 13.3 
Bit string  hybrid approach.
In our system, we allow the values of b and wi to vary from problem to problem, by including them as part of the 
problem data. This allows us to perform various experiments on these two values. The default values are b = 8 and 
. In order to allow experimentation, we have implemented both the binary and Gray code 
representation of the bit string.
In our hybrid system, each evaluation of the fitness function involves an execution of the B&B algorithm to determine 
the paths for the convoys. The bit string is decoded to give a set of delayed starting times for the convoys, and this set is 
then passed to the B&B algorithm for determining the paths. The values of the objective functions 01 and 02 are output 
from the B&B algorithm to the GA.
X-GAmeter restricts the fitness value to be a single number, double or long, so we must encode the pair of objective 
values output from the B&B algorithm as a single number representing the quality of the solution found. In our 
implementation, the fitness value is represented by a long number in which we use the lower 32 bits to represent the 02 
value, β say, and the rest of the higher order bits to represent the 01 value, α say. The fitness value is then given by
This is thus equivalent to giving the 01 value a weighting of 232.
  
< previous page
page_222
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_222.html2009-7-7 16:58:07

page_223
< previous page
page_223
next page >
Page 223
In the context of a GA, it may be argued that a smaller weighting for α is to be preferred. This is because α is the cost of 
a single convoy (the critical convoy) and so depends on only b bits of the bit string, whilst β, which is the total cost of 
all convoys, depends on all of the bits in the string. A large weighting on α may lead to quick convergence to a local 
optimum. Furthermore, as α is based on the b bits that correspond to the critical convoy, other 'good' bits in the bit 
string may not be preserved. This is because the contributions of these bits to the fitness value is so trivial when 
compared with the bits corresponding to the critical convoy. However, when comparing the quality of two solutions for 
the CMP, only if the O1 values are the same for both solutions do we need to consider the O2 values. Thus, we need a 
representation that can differentiate the quality of individual solutions in the GA. Obviously, we could reduce the value 
of the weighting for problems with smaller O2 values. We could also employ some fitness scaling techniques to re-scale 
the raw fitness value.
Generally, the evaluation of the fitness function of a GA is relatively simple. Typically, we can measure the number of 
evaluations performed per second. However, in our case, each evaluation invokes the B&B algorithm for generating the 
paths, which may take a very long time (possibly days or even weeks) before the algorithm terminates. In the 
implementation of our hybrid system, we impose a time limit, max_exec_time, on the execution of the B&B algorithm. 
If the algorithm has not terminated naturally by the end of max_exec_time seconds, we force its termination. The value 
of max_exec_time should be problem dependent. For smaller and/or easier problems, the algorithm may terminate after 
several seconds while for others, it may take tens or hundreds of seconds before a feasible solution is found. Thus, for 
our system, we input the value of max_exec_time as part of the problem data so that the value can be adjusted easily.
Our hybrid system possesses a special feature which makes it rather different from a typical GA system. In our system, 
the evaluation of two identical bit strings at different times may yield different fitness values. This is because the initial 
bound used by the B&B evaluation function may change as the GA progresses. The use of an initial bound gives a 
lower bound for any solution in the search space. Branches of the search tree which do not contain a solution that has 
better objective values than the initial bound can be pruned immediately. Thus, the use of different initial bounds on the 
same search tree may produce different results.
We use the best O1 and O2 values found as the initial bound for each evaluation. Assuming the B&B algorithm is 
allowed to execute to natural termination, it would terminate with either a better overall solution or no new solution. 
However, with the introduction of a time limit on the execution of the B&B algorithm, a number of outcomes may 
occur.
1. The B&B algorithm terminates with an optimal solution. In this case, we return the values of the optimal solution.
2. The B&B algorithm terminates and no new solution is found. This means that the search space corresponding to the 
current bit string does not contain a better solution. To penalize this, we return the current best O1 and O2 values with 
the O1 value increased by 1. As the O1 value has a weighting of 232, this is equivalent to increasing the fitness value
  
< previous page
page_223
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_223.html2009-7-7 16:58:07

page_224
< previous page
page_224
next page >
Page 224
by 232.
3. The B&B algorithm is forced to terminate because of the time limit but a better solution is found. In this case, the 
values of the best solution found are returned.
4. The B&B algorithm is forced to terminate because of the time limit and no new solution is found. This means that the 
search space may or may not contain a better solution. In this case, we return the current best O1 and O2 values with the 
O2 value increased by 1.
As the GA progresses, the initial bound for use in the B&B algorithm typically gets tighter. For larger problems when 
natural termination is unlikely, it is then harder for the B&B algorithm to find a better solution. We would need to relax 
the time constraint to increase the chance of finding a better solution. This is achieved by an automatic increase in the 
time limit after a fixed number of evaluations. Again, this parameter is very problem dependent. Thus, we allow the user 
to input it as part of the problem data. By default, we increase the execution time by 1 CPU second after every 100 
evaluations.
Due to the length of time required to run each of the experiments, only a very limited number of experiments have been 
performed. Unlike most GA experiments where the same experiment is run many times, and from the results of which 
statistics are then compiled, we have only been able to run each of the test problems 3 to 4 times each.
For all the experiments, we use the GA settings shown in Table 13.6. For a general introduction and survey of GA 
terminology, see [BBM93a, BBM93b, Whi94].
Table 13.6 GA settings
Parameters
Setting
Selection
Roulette
Crossover
One Point (60%)
Merge
New Solution
Mutation
1%
Pool Size
10
Parent Chromosome
2 to 10
Termination
No change for 50 generations
 
Table 13.7 shows the results for test problem 1 using the hybrid approach. For comparison purposes, we also present the 
timings (Table 13.8) at which solutions of a similar quality are found using a pure B&B approach. As the waiting 
intervals for these experiments are set to 1, we use the timings from the second experiments for the pure B&B method, 
in which the same waiting interval setting is used.
As we can seen from the tables, the hybrid approach does not perform very well for problem 1 in comparison with the 
pure B&B approach in terms of both the quality of the solution and computation time.
For test problem 2, the results are given in Table 13.9 and 13.10. In this case, a better solution is found using the hybrid 
approach than was found using pure B&B.
  
< previous page
page_224
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_224.html2009-7-7 16:58:08

page_225
< previous page
page_225
next page >
Page 225
Table 13.7 Results of test problem 1 hybrid approach
Run
Best Solution
Average Solution
1
2
3
O1
1339
1339
1339
1339
1339
O2
15 793
15801
15 820
15 793
15 804.67
Time:
Individual
172.79
428.88
685.68
172.79
429.12
Accumulated
172.79
601.67
1287.35
172.79
Generations:
Solution Found
170
258
62
170
163.33
Terminate
220
308
112
220
213.33
Evaluations:
Solution Found
1019
1498
383
1019
967.33
Terminate
1294
1789
657
1294
1246.66
 
Table 13.8 Extracted results of problem 1 using B&B approach
O1,O2
Time
Intermediate Solution
1339, 15 848
14.77
Intermediate Solution
1339, 15 822
14.91
Optimal Solution
1339, 15 792
242.00
Termination
279.09
 
The best O2 value found using the hybrid approach is 35 631, 13 less than that found by the B&B approach. However, 
the execution time of the hybrid approach is much longer.
As different termination conditions are used for the two approaches, we cannot rule out the possibility that a similar 
quality solution may be found using the B&B approach if the same amount of execution time is allowed.
The particular ordering of the convoys in test problem 2A seems to favour the hybrid approach over the B&B approach. 
On comparing the results for problems 2 and 2A, we can see that better results are obtained for problem 2A than for 
problem 2 using the hybrid approach. When using the B&B approach, the results are better for problem 2 than for 
problem 2A.
The results presented in Tables 13.11 and 13.12 show that for problem 2A better solutions are obtained using the hybrid 
approach than using the B&B approach. The best O2 value obtained by the hybrid approach is 35 554, 516 less than the 
value obtained from the B&B approach. In fact, the results of all the runs are better. However, we should note that the 
execution time taken by the hybrid approach far exceeds the execution time used by the B&B approach.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_225.html（第 1／2 页）2009-7-7 16:58:09

page_225
Table 13.13 shows the results of problem 3. When compared with the results obtained using the pure B&B approach 
(Table 13.14), we can see that although the hybrid approach does not find the optimal solution, in each of the three runs 
very
  
< previous page
page_225
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_225.html（第 2／2 页）2009-7-7 16:58:09

page_226
< previous page
page_226
next page >
Page 226
Table 13.9 Results of test problem 2 hybrid approach
Run
Best Solution
Average Solution
1
2
3
O1
2 377
2 377
2 377
2 377
2 377
O2
35 665
35 631
35 676
35 631
35 657.33
Time:
Individual
4794.56
14 509.81
3840.88
14 509.81
7715.08
Accumulated
4794.56
19 304.37
23 145.25
19 304.37
Generations:
Solution Found
23
99
16
99
46
Terminate
73
149
66
149
Evaluations:
Solution Found
145
592
87
592
274.67
Terminate
446
883
388
883
572.33
 
Table 13.10 Extracted Results of Problem 2 using B&B Approach
Waiting Costs = 1
O1, O2
Time
Intermediate Solution
2377, 35 656
265.38
Best Solution Found
2377, 35 644
454.78
 
good solutions were found in a much shorter time than that needed by the pure B&B approach.
From our experiments, we draw the following conclusions.
1. The hybrid approach is computationally very intensive since each evaluation of the fitness function involves the 
execution of a B&B algorithm.
2. The approach is only worthwhile for larger problems or those involving significant bottlenecks. For smaller and 
simpler problems, the amount of time taken to evaluate the initial pool may exceed the time taken for the pure B&B 
algorithm to solve the problem.
3. The length of time required for each evaluation of the fitness function is nowhere near constant. The B&B algorithm 
may terminate immediately or take the whole max_time_limit seconds. This is due to the use of a progressively 
improving initial bound.
In our current implementation, a very simple fitness function is used and the method for penalising bit strings that do 
not produce better solutions is also rather crude. Research is being undertaken to develop better schemes.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_226.html（第 1／2 页）2009-7-7 16:58:09

page_226
The lower bound on the optimal value is calculated using the minimum cost path, with a zero delay being assumed for 
each convoy. If a solution is found with the same O1 value, then we have found the critical convoy. We also know that 
the delay of
  
< previous page
page_226
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_226.html（第 2／2 页）2009-7-7 16:58:09

page_227
< previous page
page_227
next page >
Page 227
Table 13.11 Results of test problem 2A hybrid approach
Run
Best Solution
Average Solution
1
2
3
4
O1
2377
2377
2377
2377
2377
2377
O2
35 586
35 912
35 77
35 554
35 554
35 707.75
Time (secs):
Individual
16 984.06
16 245.38
3402.31
32 850.21
32 850.21
12 210.58
Accumulated
16 984.06
33 229.44
36 631.75
69 481.96
69 481.96
Generations:
Solution Found
149
69
7
224
224
112.25
Terminate
199
119
57
274
274
162.25
Evaluations:
Solution Found
939
422
42
1396
1396
699.75
Terminate
1218
717
352
1686
1686
993.25
 
Table 13.12 Extracted results of problem 2A using B&B approach
Waiting Costs = 1
O1, O2
Time
Best Solution Found
2377, 36 070
2219.09
 
this critical convoy has to be 0. Thus, we could shorten the bit strings by removing the b bits corresponding to the critical 
convoy just determined. This should make the problem easier to solve. This has yet to be implemented.
13.4 
A Pure GA Approach
In this section, we describe a direct GA approach for solving the CMP with delays. We use a GA for computing the delays 
and for selecting paths.
At the preprocessing stage, a fixed number (specified in the data file) of minimum cost paths is generated for each convoy. 
We have implemented the loopless k-shortest path algorithm presented by Yen [Yen71]. These paths and their corresponding 
costs are stored for later use.
As in the hybrid system, for each convoy, i, we use a fixed number of bits (b, say) to encode ri, where di = ri = wi. In 
addition, for each convoy, i, we encode the number corresponding to the selected path. As in the hybrid approach, the number 
of bits used to represent delays is input from the data file. The number of bits used to represent the path number depends on 
the number of (near) minimum cost paths generated during the preprocessing phase. If 16 minimum cost paths are generated, 
then 4 bits are used to represent the path number for each convoy. Figure 13.4 shows the structure of such
  
< previous page
page_227
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_227.html2009-7-7 16:58:10

page_228
< previous page
page_228
next page >
Page 228
Table 13.13 Results of test problem 3 hybrid approach
Run
Best Solution
Average Solution
1
2
3
O1
24
23
23
23
23.33
O2
217
219
222
219
219.33
Time:
Individual
116.93
155.79
136.13
155.79
136.28
Accumulated
116.93
272.72
408.85
272.72
Generations:
Solution Found
134
36
35
36
68.33
Terminate
184
86
65
86
118.33
Evaluations:
Solution Found
749
211
183
211
1143
Terminate
1000
508
430
508
646
 
Table 13.14 Extracted results of problem 3 using B&B approach
Waiting Costs = 1
O1, O2
Time
First Solution Found
30, 278
0.56
Intermediate Solution Found
23, 222
1331.48
Optimal Solution
23, 210
1551.20
Termination
1616.57
 
a bit string. Both binary and Gray code representations have been implemented.
Figure 13.4 
Structure of Bit String  GA Approach
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_228.html（第 1／2 页）2009-7-7 16:58:11

page_228
The set of convoy paths represented within a bit string corresponds to a potential solution. The evaluation of the fitness 
function involves the validation of this solution against all the constraints specified in our model. We check each 
convoy path against
  
< previous page
page_228
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_228.html（第 2／2 页）2009-7-7 16:58:11

page_229
< previous page
page_229
next page >
Page 229
all other paths to make sure that constraints C1, C2, C3 and C4 are not violated, taking into consideration the delays for 
each convoy. The result of the evaluation is a triple which is composed of the infeasibility value of this solution, the O1 
value and the O2 value. The infeasibility value is equal to the total number of jointly infeasible pairs of paths. A jointly 
infeasible pair of paths is defined to be a pair of paths which, if taken by their corresponding convoys, causes one or 
more of the constraints to be violated.
As the evaluation function returns three values, we need to encode them into a single fitness value. As before, let α 
represent the O1 value and let β represent the O2 value. Let ival denote the infeasibility value. The fitness value is then 
calculated in the following way.
where
and
As the evaluation of the fitness function in this case is fairly simple, a large number of experiments have been 
performed to test out the effect of various settings. We have concentrated mainly on testing problem dependent 
parameters such as the number of bits used to represent delays, the number of minimum cost paths generated for each 
convoy, the value of the waiting interval and the bit string representation. We have also tested two different population 
pool sizes. As the ordering of convoys is not significant in this approach, we have not used test problem 2a. For all of 
the experiments, the GA settings given in Table 13.15 are used. Table 13.16 shows the settings for various test sets.
Table 13.15 GA settings
Parameters
Setting
Selection
Roulette
Crossover
One Point (60%)
Merge
New Solution
Mutation
(1%)
Parent Chromosome
2 to 10
Termination
No change for 500 generations
 
  
< previous page
page_229
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_229.html2009-7-7 16:58:11

page_230
< previous page
page_230
next page >
Page 230
Table 13.16 Settings for various test sets
Test 
Sets
Pool 
Size
Bit String 
Encoding
Number of 
Delay Bits 
Per Convoy
Waiting 
Interval
Convoy 
Time 
Window
1 10
Gray 
Code
8 1 10
2
10
Binary
8
1
10
3
10
Gray Code
8
2
10
4
10
Gray Code
6
11
10
5
10
Gray Code
6
1
10
6
10
Gray Code
6
2
10
7
20
Gray Code
6
1
10
 
We should note that the objective values of the test sets may be different for different waiting costs. Thus, there may be 
three different optimal values for the test sets, one for each choice of waiting cost. We should also note that the optimal 
values obtained using this approach may not be as good as the pure B&B approach and the hybrid approach when the 
same waiting costs are used. This is because loops are allowed in paths generated using the B&B algorithm, while the k-
shortest path algorithm only allows loopless paths. Thus, the paths used by the pure GA approach may not be as good as 
those used by the other two approaches.
For test problem 1, experiments were performed on each of the test sets using different numbers of minimum cost paths. 
The results for 10 runs of each experiment were recorded. Table 13.17 summarizes the best solution values recorded 
and Table 13.18 shows the corresponding timings for that particular run. We have omitted the O1 value as the optimal 
O1 value was obtained for all of the runs.
For this test problem, we observe the following. Comparing the results for test sets 1 and 2, we can see that on the 
whole the better results were obtained using the Gray code representation. The results for test sets 1 and 3 indicate that 
using a waiting interval of 1 produces the best results when more than 2 paths are used. The worst set of results is 
obtained with test set 4 when the waiting interval is set to 11; this is expected and is consistent with the results of the 
pure B&B approach. In general, using 6 bits to represent delays produces better results as the total search space is 
reduced. A large population pool does not seem to make much difference.
For test problem 2, our pure GA method failed to find a feasible solution. For our third test problem, however, the pure 
GA does extremely well, consistently finding quite good solutions in very reasonable computational times, as shown in 
Tables 13.19 and 13.20.
13.5 
Conclusions
We have fully developed and implemented three methods of solution of the convoy movement problem with initial 
delays:
  
< previous page
page_230
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_230.html2009-7-7 16:58:12

page_231
< previous page
page_231
next page >
Page 231
Table 13.17 Best results obtained for test problem 1
Test Sets
Number of Minimum Cost Paths Used
1
2
4
8
16
32
64
1
15 810
15 810
15 810
15 810
15 792
15 792
15 817
2
15 794
15 816
15 822
15 815
15 857
15 815
15 823
3
15 800
15 800
15 828
15 814
15 827
15 828
15 820
4
15 881
15 837
15 837
15 863
15 848
15 870
15 841
5
15 792
15 792
15 792
15 792
15 801
15 810
15 792
6
15 800
15 800
15 820
15 824
15 800
15 820
15 800
7
15 792
15 792
15 810
15 814
15 792
15 810
15 794
 
Table 13.18 Timings for best results obtained for test problem 1
Test Sets
Number of Minimum Cost Paths Used
1
2
4
8
16
32
64
1
209.63
197.55
333.02
186.28
376.66
254.28
398.55
2
130.87
90.22
121.51
197.72
191.65
113.65
226.67
3
177.55
210.61
222.43
288.22
377.12
374.22
273.76
4
109.20
130.57
183.01
213.59
296.24
346.91
593.25
5
111.85
167.57
121.54
143.34
181.70
171.23
305.29
6
108.04
148.94
176.05
247.31
225.79
163.92
218.74
7
166.12
199.87
155.75
354.52
190.77
295.65
271.67
 
MA pure branch-and-bound algorithm
A hybrid genetic algorithm using a branch-and-bound algorithm as the evaluation function
A pure genetic algorithm
We have tested these algorithms on DRA data and on limited test data generated by ourselves. Further testing is 
necessary but, even with the limited experiments we have performed, we have been able to show the strength of each 
approach. The pure branch-and-bound approach produces very satisfactory results especially for test problem 1. On 
occasions, it can prove far too slow and we realise that examples are likely to arise where the use of this method will be 
unrealistic. This will remain the case even if we accept that the pure branch-and-bound does not run to optimality but 
terminates after a fixed time, reporting the best solution that it has found. Heuristics will need to be developed. The 
hybrid GA uses a much simpler branch-and-bound algorithm but this code is called many, many times. From a research 
point of view, the hybrid offers the challenge of a changing environment, whereby the fitness of a solution depends on 
the time when it is evaluated. This is an unusual phenomenon in GA research and has yet to be fully explored. The 
hybrid GA appears to produce satisfactory solutions especially for test problem 2. However, the costs of running even a 
simple
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_231.html（第 1／2 页）2009-7-7 16:58:13

page_231
< previous page
page_231
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_231.html（第 2／2 页）2009-7-7 16:58:13

page_232
< previous page
page_232
next page >
Page 232
Table 13.19 Best results obtained for test problem 3
Test Sets
Number of Minimum Cost Paths Used
1
2
4
8
16
32
64
5
25 231
23 221
23 230
23 232
23 245
23 248
24 226
6
26 236
26 230
24 249
23 200
23 227
25 228
26 257
 
Table 13.20 Timings for Best Results Obtained for Test Problem 3
Test Sets
Number of Minimum Cost Paths Used
1
2
4
8
16
32
64
5
10.76
14.12
18.91
19.67
28.56
37.54
61.23
6
9.38
11.33
15.04
27.49
24.03
38.33
45.64
 
branch-and-bound might mean that it will not provide a viable solution to certain problems.
As far as the pure GA approach is concerned, we have shown that that too has potential, especially for applications 
involving considerable bottlenecks. It produced good quality results for our contrived problem 3, in very reasonable 
computational times.
Incorporating start delays has given us scope for improving the quality of the convoy routes. However, delays are 
currently limited to delays at the start only. Delays at certain intermediate nodes may be militarily feasible and such 
delays could enhance still further the quality of the solutions. Such delays are currently being incorporated into the 
model and into the suite of algorithms.
References
[BBM93a] Beasley D., Bull D. R., and Martin R. R. (1993) An overview of genetic algorithms : Part 1, fundamentals. 
University Computing 15(2): 5869.
[BBM93b] Beasley D., Bull D. R., and Martin R. R. (1993) An overview of genetic algorithms : Part 2, research topics. 
University Computing 15(4): 170181.
[GC94] Gendron B. and Crainic T. G. (1994) Bounding procedures for multicommodity capacitated fixed charge 
network design problems. Technical Report CRT-95-12, Centre de Recherche sur les Transports, University of 
Montreal.
[KH80] Kennington J. L. and Helgason R. V. (1980) Algorithms for Network Programming. John Wiley, New York.
[KRSS93] Kapsalis A., Rayward-Smith V. J., and Smith G. D. (1993) Fast sequential and parallel implementation of 
genetic algorithms using the gameter toolkit. In Proc. Int. Conference on Neural Networks and Genetic Algorithms. 
Innsbruck.
[LMRS94] Lee Y. N., McKeown G. P., and Rayward-Smith V. J. (1994) Solving
  
< previous page
page_232
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_232.html2009-7-7 16:58:13

page_233
< previous page
page_233
next page >
Page 233
the convoy movement problem using branch-and bound on a network of transputers. In A. De Gloria M. R. J. and 
Marini D. (eds) Transputer Applications and Systems '94, pages 786796. IOS Press.
[MRSLL94a] McKeown G. P., Rayward-Smith V. J., Lee Y. N., and Ling P. D. (1994) The convoy movement problem 
with delays. Technical report, University of East Anglia, Norwich.
[MRSLL94b] McKeown G. P., Rayward-Smith V. J., Lee Y. N., and Ling P. D. (1994) Massively parallel integer 
programming. Technical Report CB/RAE/9/4/2162/025, University of East Anglia, Norwich.
[MRST91] McKeown G. P., Rayward-Smith V. J., and Turpin H. J. (1991) Branch-and-bound as a higher order 
function. Annals of Operations Research 33: 379401.
[MW84] Magnanti T. L. and Wong R. T. (1984) Network design and transportation planning: Models and algorithms. 
Transportation Science 18: 155.
[Whi94] Whitley D. (1994) A genetic algorithm tutorial. Journal of Statistics and Computing 4: 6585.
[Yen71] Yen J. Y. (1971) Finding the k shortest loopless paths in a network. Manag. Sci. 17: 712716.
[YNLRS94] Y. N. Lee G. P. M. and Rayward-Smith V. J. (1994) The convoy movement problem. Technical Report 
SYS-C94-01, University of East Anglia, Norwich.
  
< previous page
page_233
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_233.html2009-7-7 16:58:14

page_235
< previous page
page_235
next page >
Page 235
14 
A Comparison of Heuristics for Telecommunications Traffic Routing
Jason W. Mann and George D. Smith
Abstract
In the routing of traffic across telecommunications networks, shortest path routing is often used on a call by call basis. 
Although this is the most efficient solution for each particular call, it can lead to ineffective use of the global network 
and even congestion. The approach adopted here is to search for routing strategies that attempt to spread the expected 
traffic across the network by forcing the utilisation for each link below a specified threshold. We compare the quality of 
solutions obtained and efficiency of two approaches to the search for such strategies, namely genetic algorithms and 
simulated annealing.
14.1 
Introduction
The telecommunications industry is one of the most important industries in the modern world and its continuing growth 
relies, among other things, on the solution of many differing and difficult optimisation problems. In order to cope with 
an ever increasing demand and rapidly changing market conditions, as well as the incorporation of new technologies, 
the industry needs to apply the whole spectrum of optimisation techniques to the solution of decision problems in 
network design, network management, resource allocation, fault management, fraud management and many other 
complex problem areas. These techniques range from problem specific, exact algorithms such as those based on shortest 
path routines for routing, through linear and integer programming to the generic heuristics such as simulated annealing, 
genetic algorithms, tabu search and constraint satisfaction techniques.
In this paper, we describe a particular case study which the authors have carried out as part of a long-term project with 
Nortel Technology, formerly BNR Europe Ltd.
In the following section, we describe the particular flavour of the heuristic techniques
Modern HeuristicSearch Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith© 1996 John Wiley & Sons Ltd.
  
< previous page
page_235
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_235.html2009-7-7 16:58:14

page_236
< previous page
page_236
next page >
Page 236
used in this study, namely genetic algorithms (GAs) and simulated annealing (SA), and review some relevant 
applications of GAs and SA within the telecommunications domain. In Section 14.3, we introduce the particular routing 
problem studied and describe the representation used for both the GA and SA application. It may be argued that in some 
applications, a different representation may be more effective for one paradigm than another, but in this case the 
representation chosen was extremely effective for both GA and SA approaches and is similar to that successfully 
adopted in [SHY93] and [LMRS94] for the representation of routes over a network.
The resulting routing strategies obtained using the particular GA and SA applications are compared, not only with each 
other, but also with the routing strategy based on a shortest path routine to route the traffic on a set of specified 
networks. These results are presented in Section 14.4.
The work was carried out on a UNIX HP 9000 712/80 workstation using the GA toolkit GAmeter and the SA toolkit 
SAmson, both developed at UEA to aid comparative studies using GAs and SA. The fitness function developed for the 
problem can be used in either of the toolkits, provided the same representation is used.
14.2 
Background
Genetic algorithms and simulated annealing algorithms are members of a family of heuristics commonly known as 
modern heuristic techniques, see [Ree93, RS95]. We give a brief introduction to each technique in order to explain the 
terminology used in the description of the parameters.
14.2.1 
The Genetic Algorithm
GA 
    {objective is to minimise value(p) such 
    that p ∈ U } 
    P, Q, R : multi-set of solutions ⊂ U; 
    initialise(P); 
    while not finish(P) do 
        begin 
        Q := select(P); 
        R := create(Q); 
        P := merge(P, Q, R) 
        end 
end GA
 
Figure 14.1 
The genetic algorithm paradigm.
Genetic algorithms are general search paradigms that simulate the mechanisms of natural evolution, see [Gol89]. The 
GA starts with a population of solutions
  
< previous page
page_236
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_236.html2009-7-7 16:58:15

page_237
< previous page
page_237
next page >
Page 237
to the problem in hand and, using selection of the fittest mechanisms and genetic recombination processes, evolves 
populations of solutions which converge to (near) optimal solutions to the problem.
A typical version of the basic algorithm, as used in function and combinatorial optimisation, is shown in Figure 14.1. 
Here, P is the population of chromosomes, each one representing a solution to the problem and each evaluated 
according to some objective function, Q represents the gene pool containing the parents selected from P and R is the 
pool of offspring created from Q using genetic operators.
In applying our GA to the problem described in Section 14.3, we must fix parameters such as population size, crossover 
mechanism, crossover rate, etc. As part of this study, a pre-trial phase is used to choose the select, create and merge 
mechanisms (see Figure 14.1) from a limited choice described below. The remaining parameters are fixed at values that 
are now used by many GA practitioners to be typical values acceptable for a wide range of applications. This includes 
setting the crossover rate to 60% and mutation rate to 0.1%, for example. In addition, we choose to use a small 
population size (20) but with selection and replacement mechanisms designed to help retain diversity. The stopping 
criterion used is to finish if there is no improvement in the best solution after 500 evaluations. This final point is 
discussed in more detail in Section 14.4.
The function select in Figure 14.1 is used to sample from the (initial) population P in order to generate a gene pool, Q. 
The desired quality in any effective select mechanism is that it should simulate selection of the fittest. Thus, good 
solutions should have a better than average chance of being selected while bad solutions should have a worse than 
average chance. The toolkit GAmeter has many choices of select built in, but, for the purposes of this study, we limit the 
choice of select to two commonly used mechanisms:
Roulette Select with a probability based on the relative fitness of each solution.
Tournament Randomly select two solutions (2-party tournament) and accept the better solution into Q. Repeat this the 
required number of times until Q is filled.
Similarly, GAmeter includes many genetic crossover operators that can be called from the create function. For the 
purposes of this study, we limit the crossover operator to one of the following commonly used operators:
1-point crossover see [Gol89],
Uniform crossover see [Sys89].
Finally, the merge process determines the makeup of the next generation. Once again, many different mechanisms have 
been proposed and implemented for this process and the most effective of these have been incorporated into GAmeter. 
We limit the choice of merge here to two such mechanisms:
Best fit With this mechanism, the best solution in R replaces the worst solution in P until either all solutions in P are 
better than the remaining solutions in R or R is empty.
New Solutions Here, only the solutions in R that do not already exist in P and are better than the current members of P 
are accepted
  
< previous page
page_237
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_237.html2009-7-7 16:58:15

page_238
< previous page
page_238
next page >
Page 238
into P, replacing solutions in P that are worse. This has the effect of retaining the diversity of the population throughout 
the search and the added bonus of providing a multitude of effective routing strategies when the search is complete.
Genetic algorithms have been used extensively in the design and management of telecommunications networks. One of 
the first 'real-world' applications of GAs was in the determination of network link sizes [CD87, DC87]. This process is 
the final stage in the design of wide-area packet switching networks that satisfy customer performance criteria while 
minimising network cost. The chromosome used in these studies was a set of n link speeds, where n is the number of 
links in the network. Each link speed was chosen from a set of possible link speeds for the associated link. The 
evaluation ran a simulator routing algorithm to assess the performance of the resulting network against a set of criteria, 
for example, link utilisation should not exceed 90%, or average link utilisation should not exceed 60%, etc. Compared 
with a greedy heuristic, the GA found better, and in fact, near optimal solutions, but used more processing power to do 
so. More recently, Davis has worked on dynamic anticipatory routing [CDQ91] and survivable network design 
[DOCQ93]. Several heuristics, including GAs and GA-hybrids, were used in approaches to the solution of this problem. 
The work by [DOCQ93] compared a GA with a mixed integer programming approach and a greedy algorithm, with the 
GA finding better solutions in a reasonably fast time for large networks.
A new dynamic routing method based on genetic algorithms has been proposed by [SHY93] for the real-time 
management of dynamic traffic changes in broadband networks. The authors applied their model to a simple network 
and periodically searched for a new solution when the traffic changed such that the maximum call loss-rate exceeded a 
pre-set target value. They concluded that a solution can be obtained by a genetic algorithm, even after only a few 
generations, and thus within a very short span of real time. One of the techniques they used to speed up the search was 
to seed the population of a new search with solutions that had been used previously. In other words, past routing 
patterns were injected into the initial population in order to improve convergence. This memory of past solutions may 
be useful in itself as a collection of 'optimal' network configurations for different scenarios. See also [PW91] for an 
application of GAs to route traffic on an ATM network.
There are many other applications of genetic algorithms to network management and network design problems in 
telecommunications and the reader is referred to [Man95a] for a full discussion.
14.2.2 
Simulated Annealing
Simulated annealing, as a mechanism for searching the (feasible) search space of an optimisation problem, was first 
proposed by [KGV83] and, independently, by [Cer85], Their algorithms were based on that of [MRR×53] which 
simulated the controlled cooling of a material by a process now known as annealing. The simulated annealing technique 
is essentially local search in which a move to an inferior solution is allowed with a probability that decreases, as the 
search progresses, according to some, typically Boltzmann-type, distribution.
  
< previous page
page_238
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_238.html2009-7-7 16:58:16

page_239
< previous page
page_239
next page >
Page 239
A basic, sequential version of the SA paradigm for optimisation problems is described in Figure 14.2 and a detailed 
overview of SA can be found in [Dow93].
  SA 
  {objective is to minimise value(p) such that p ∈ U} 
  p := p0 where p0 is some initial solution 
  T := T0 where T0 is some initial temperature 
  while not finish(p, T) do 
      while not reached  equilibrium do 
      begin 
      randomly select p′ ∈ N(p); 
      ∆ := value(p)  value(p′); 
      if ∆ > 0 then p := p′ 
            else 
            begin 
            generate x ∈ uniform[0,1]; 
            if x < exp(∆/T) then p := p′ 
            end {else} 
      end {while}; 
      T := cool(T) 
  end {while} 
  accept p as the solution. 
  end {SA}
 
Figure 14.2 
A basic version of the simulated annealing paradigm.
As with the GA, all parameters in the SA algorithm must be set prior to its use. We fix certain parameters and use a pre-
trial phase to select others from a limited selection. The fixed parameters for the SA approach are the type of 
neighbourhood, in this case fixed as a simple random 'flip' of bits, the initial temperature, fixed at 20, which is around 
the best value obtained for all networks using an initial 'heating' process to determine the best starting temperature, and 
the number of iterations per temperature step, here fixed at 30 for the Geometric Cooling, and at 1 for the Lundy & 
Mees Cooling, see below. As with the GA, the stopping criterion is to finish if there is no improvement after 500 
evaluations.
The parameter settings determined by a pre-trial phase are the cool function and the reached-equilibrium condition, see 
Figure 14.2. The function cool determines the cooling schedule, i.e. the sequence of values of the temperature 
parameter, T, used during a run of the algorithm. Typically, this consists of a sequence of monotonically decreasing 
values of T in order to reduce the probability exp(∆/T) of moving to a worse solution, the rate of decrease determined by 
the cooling rate function. A large number of variations in the choice of the cooling rate function exists, see, for example,
  
< previous page
page_239
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_239.html2009-7-7 16:58:17

page_240
< previous page
page_240
next page >
Page 240
[Dow93]. We choose to select from two mechanisms commonly used:
Geometric Cooling Here the temperature is reduced according to the formula
Here the value of α is chosen close to one, typically in the range 0.85 to 0.95. (The value 0.9 is used in this study.)
Lundy & Mees Cooling This is an example of a study [LM86] using inhomogeneous Markov chains, i.e. the 
temperature is decreased in gradual steps, according to the formula
where β is a constant very close to 0. (The value used in this study is β = 0.01.) Normally, when this cooling rate is 
used, the number of iterations at each temperature step is set to 1.
Moreover, the condition reached-equilibrium determines the number of iterations, i.e. neighbourhood moves, at each 
temperature step, basically the length of the (homogeneous) Markov chain at each temperature step. Once again, there 
are many ways of choosing this condition and we limit the choice available to two effective mechanisms:
Constant number of iterations Here, the number of iterations is simply governed by the number of proposed moves1, i.e. 
the user specifies a value for this, usually large for homogeneous systems and 1 for the Lundy & Mees cooling function, 
and the temperature is reduced after this has been reached and the counter reset.
Variable number of iterations (feedback) Arguably, a better way of ensuring sufficient searching at lower temperatures 
is to only allow a temperature reduction after a prescribed number of accepted moves.
An additional feature which is occasionally used in SA is to periodically increase the temperature, T, to reduce the 
likelihood of becoming trapped in a local minimum. The term tempering is used to describe this process. If we define 
cooling mechanism to mean the general behaviour of the temperature parameter, T, then the choices considered for the 
cooling mechanism are:
Monotonic The temperature is reduced according to the chosen cooling rate and the process stops when some final 
temperature is reached, or some other stopping criterion is achieved.
Tempering The temperature is reduced, according to the chosen cooling rate, until some value, or threshold, is 
achieved; it is then increased significantly and allowed to continue reducing. This occurs periodically. Within SAmson, 
we have adopted the following scheme to simulate the tempering process:
1 A proposed move is the process of generating a neighbour p′ of p, whether this new solution is accepted or 
not. An accepted move is when the proposed solution p′ becomes the current solution p.
  
< previous page
page_240
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_240.html2009-7-7 16:58:18

page_241
< previous page
page_241
next page >
Page 241
1. Select Tempering and enter a threshold value η, say 10%.
2. Start with a temperature T = Ts (= T0, initially).
3. Reduce temperature according to chosen cooling rate until T = η % of Ts.
4. Reset the start temperature: Ts := Ts/2 and go to step 2.
Once again, with two options for each of the mechanisms cooling rate, reached-equilibrium and cooling mechanism, we 
have eight possible combinations of these parameter settings alone. By applying each of these to a pre-trial network, we 
are able to determine an effective combination to be used for the full trial. The details are described in Section 14.4.
Although simulated annealing has been applied to solve problems in the telecommunications domain, there are certainly 
fewer such studies than with GAs, particularly in routing problems. One such study, [PCR95], compares an SA 
approach with a classifier system, hill climbing and random search on a routing and scheduling problem. Within a given 
telecommunications network, requests for services will arrive at indeterminate times. Those requests will need to be 
routed and possibly scheduled within a certain time frame. The various search techniques were applied to the scheduling 
element of this problem, a standard router being used to complete the evaluation of a schedule. The SA technique 
consistently outperformed the other techniques in terms of quality of solution obtained.
[CM94] used SA to determine the most effective upgrade to existing networks, in terms of customer satisfaction and 
cost-effectiveness. Upgrading all switches, for example, would achieve maximum customer satisfaction but at a 
prohibitive cost. The question is, which subset of switches to upgrade to maximise some weighted sum of the two 
objectives. The SA implementation provided very satisfactory results, but no comparison was made with other 
techniques.
14.3 
Problem Specification
A common problem in the field of telecommunications is the routing of traffic through a given network. Automatic 
route finding is a central part of any telecommunications network system. Whatever the underlying network technology, 
there is the need to send a signal from one node to another across a route. Although the use of shortest path routing for 
each single call request ensures the best possible route for that particular request, it can be shown to lead to suboptimal 
routing or even highly congested network routing solutions when one considers the network utilisation as a whole 
[Wan92]. Use of global information, including not only the available link capacities but also the expected traffic profile 
for the period in question, can lead to routing strategies designed to minimise congestion and make better use of 
available network resources.
This is the particular question posed for this study: what routing strategies should be adopted for a particular network to 
ensure that no link is over-utilised and, if possible, that all links are evenly loaded, i.e. a balanced network is achieved, 
for the predicted traffic profile.
The work presented in this study centres around the design of optimal routing
  
< previous page
page_241
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_241.html2009-7-7 16:58:19

page_242
< previous page
page_242
next page >
Page 242
strategies for synchronous digital hierarchy (SDH) networks, but the approach used can be adopted for any 
(telecommunications) network. An SDH network uses three transmission modes: STM-1 (155 Mbits/s), STM-4 (622 
Mbits/s) and STM-16 (2.4 Gbits/s).
We are given a telecommunications network over which we must route multiple traffic requests in such a manner as to 
achieve a feasible routing assignment, i.e. no link is over-capacitated (hard constraint). This is the primary objective. In 
addition, we impose a secondary objective that link utilisations should all be below a specified, fixed target utilisation. 
Finally, the routing assignment attempts to minimise the communications costs, assuming that costs are assigned to link 
usage.
Specifically, we are given a network G = (N,E), where N is the set of n nodes and E is the set of m edges. Associated 
with each edge e ∈ E is a bandwidth, b(e), and a cost, c(e). For the applications we have been considering, the network 
is always bidirectional, there is only a single type of traffic, n is in the range 7 to 50 and m is between 9 and 125.
The bandwidths, {b(e) | e ∈ E}, lie in {63,4 × 63,16 × 63}, which is determined by the number of VC12s (virtual 
containers) which fit onto an STM1, STM4 and STM16, respectively, see [New93]. We thus introduce a bandwidth 
factor which is a number in {1,4,16}. This is multiplied by 63 to give the actual bandwidth.
For each distinct v, w ∈ N, there is an amount of network traffic, t(v, w), which must be routed from v to w in the 
network. This traffic must all be routed on the same path, P(v, w), which has to be determined. The total traffic, f(e), on 
any edge e ∈ E is given by
This must not exceed the bandwidth of that edge, i.e.
This can be achieved by minimising the deviation f(e) b(e), so long as f (e) > b(e), i.e.
A second objective is to find a minimum cost allocation of traffic through the network, satisfying constraint (02). The 
cost of routing all traffic, t(v, w), on the path, P(v, w), between v and w is given by
This is summed over all possible source/destination pairs to yield the second objective
  
< previous page
page_242
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_242.html2009-7-7 16:58:20

page_243
< previous page
page_243
next page >
Page 243
bearing in mind the bidirectional constraint so we are summing only over n(n 1)/2 possible pairs.
The third and final objective specified is to minimise the deviation from a target utilisation, u, for each link on the 
network, i.e.
Thus, so long as f(e) > (u × b(e)/100), for at least one link e ∈ E, there will exist some pressure on the optimisation 
process to find a more balanced solution. For our experiments, we set u = 50.
The full and final objective is therefore to
Here, p, w1 and w2 are weights applied to the various terms in the evaluation function. These can be adjusted as 
necessary to prioritise any of the objectives. Obviously, attaining objective (O2) means that we have automatically 
attained objective (O2). However, the inclusion of both terms in the evaluation function means that we can concentrate 
on feasible solutions while still retaining the flexibility to choose between low cost or more balanced solutions.
Ideally, the values for the weights p, w1 and w2 would be determined for each problem studied in order to achieve the 
desired characteristics in the solution. After a large number of trials on different networks, concentrating on well-
balanced, feasible routing strategies, the preferred values for these parameters, and the values used throughout the 
experiments, are p = 10, w1 = 1 and w2 = 5.
The value of the objective function (O2) can be determined once the routing allocation has been done. This brings us to 
the representation of the routing decisions. The objective of the exercise is to determine the path, P(v, w), for each v, w 
∈ N, to minimise the expression (O2). If there are n nodes in the network, then there are n(n 1)/2 possible pairs of 
connections. Thus the representation is simply a string of n(n 1)/2 paths, one for each source-destination pair.
For the genetic algorithm and the simulated annealing algorithm, therefore, the representation contains n(n 1)/2 'genes' 
or parameters, each representing a path for a unique pair. In order to avoid explicitly representing each path, each gene 
holds a pointer to a look-up table held in memory, one for each connection pair. The question is: which paths should we 
hold in this look-up table? The obvious choice is the kth shortest paths [Yen71], where k has to be decided. By limiting 
the size of k, one can ensure that only reasonable routes are considered for each connection. For our purposes, we found 
that k = 8 was acceptable, thus ensuring that excessively long paths were removed from consideration without limiting 
the search space sufficiently to preclude an effective search. For larger networks, or networks with multiple links
  
< previous page
page_243
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_243.html2009-7-7 16:58:22

page_244
< previous page
page_244
next page >
Page 244
between nodes, this figure may need to be adjusted accordingly. Indeed, there is an argument for having different values 
of k for different node pairs, depending on 'distance' apart. There is a danger that, for small values of k, we may be 
precluding the optimal solution from our search. However, we argue that we are basing the search on the estimated 
traffic profile, and that effective (sub)-optimal solutions found within a reasonable time frame are acceptable. A more 
effective strategy, and one that we are currently pursuing, is to hold the k shortest diverse paths, where diverse can mean 
node or link diverse.
As previously stated, we limit the choice of the path, P(v,w), to be one of the eight shortest paths. In this case, we can 
represent this choice using three bits for each source/destination pair. This can clearly be generalised to obtain a k bit 
representation should we wish to consider the 2k shortest paths. When the problem is initially loaded, a pre-processing 
stage determines, for each node pair, the eight shortest possible paths plus associated costs, and stores them in a look-up 
table. A routing solution is thus represented as a concatenation of n(n 1)/2 three-bit strings. This is then fed into the 
'fitness' function (of the GA or SA application) to determine an overall fitness for that routing strategy, depending on the 
values of the weights p, w1 and w2.
The results reported in Section 14.4 were obtained using the GA toolkit GAmeter and the SA toolkit SAmson. The 
coding of the evaluation function, equation (O2), once tested, can be used in either toolkits, allowing ready comparison 
of the two techniques for a given problem, see [MKS95, Man95b]. Each of these toolkits has a Windows-based 
interface developed on X/Motif on Unix. In addition to the many interactive problem-independent features available, the 
user can develop problem-dependent windows to allow easy input of parameter choices and to produce visual displays 
of the results.
The resulting GA and SA applications for this problem have two problem-dependent windows in common. The first of 
these is a Problem Parameters dialog box, in which the user can set the values of the three parameters, p, w1 and w2, 
appearing in the objective equation (O2), see [Man95a]. The second dialog box which is common to both toolkits is the 
Utilisation Table of the best solution found so far, see Figure 14.3. This displays, for each link in the network,
the nodes adjoining the link,
the bandwidth (or capacity),
the load (i.e. the amount of traffic on the link) and
the utilisation (= load/bandwidth × 100).
Additionally, the GA application, GAmeter, offers the Utilisation Table for any selected member of the current 
population, thus presenting a wider choice of strategies.
14.4 
Results
To compare the performances of GAs and SA applied to this problem, it is necessary to compare related attributes. We 
note here that the SA application can be enhanced by incorporating some 'delta evaluations'. This is due to the fact that 
any new solution is generated from an existing solution through a relatively small change, based on some 
neighbourhood operator. It is possible to derive the 'fitness' of the new solution from
  
< previous page
page_244
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_244.html2009-7-7 16:58:23

page_245
< previous page
page_245
next page >
Page 245
Figure 14.3 
An example Utilisation Table  in this case for 
the best solution in the current population in the GAmeter 
application.
that of the current solution and the cost of the change. This makes the SA approach much more effective in terms of 
processing time. Recent studies [SKRSK95] comparing the number of evaluations per second achieved using both a GA 
and SA approach for the Radio Link Frequency Assignment Problem showed that SA can achieve at least 50 times the 
number of evaluations per second achieved by the corresponding GA. Building 'delta evaluation' into the GA to a 
limited extent reduced this advantage to around 20 times the number.
For this reason, we do not use the CPU time as one of the measures of comparison, although we note in passing that this 
is a major advantage of SA over GAs. Instead we compare the number of function evaluations in both algorithms. Thus 
for the GA, we present the results as Fitness versus Number of evaluations while for the SA, we use Fitness versus 
Number of proposed moves2.
As described previously, we fix many of the parameters used within each algorithm and use a pre-trial study on a 
moderately sized network (12 nodes) to determine the remaining parameter settings.
2 Each proposed move represents an evaluation of a new solution.
  
< previous page
page_245
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_245.html（第 1／2 页）2009-7-7 16:58:23

page_245
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_245.html（第 2／2 页）2009-7-7 16:58:24

page_246
< previous page
page_246
next page >
Page 246
Table 14.1 The parameter options for the GA and SA applications tested 
during the pre-trial phase
GA
Options
select mechanism
Roulette (R)
Tournament (T)
create mechanism
One-point (O)
Uniform (U)
merge mechanism
Best Fit (B)
New Solutions (N)
SA
Options
cooling rate
Geometric (G)
Lundy and Mees (L)
reached-equilibrium
Constant (C)
Feedback (F)
cooling mechanism
Monotonic (M)
Tempering (T)
 
For the GA, we chose the select, create and merge functions from one of two options, see Section 14.2.1 and Table 
14.1. Similarly, for the SA application, we selected the cooling rate, reached-equilibrium condition and cooling 
mechanism from one of two options, see Section 14.2.2 and Table 14.1. Thus there were a total of 8 parameter settings 
for each heuristic. Each experiment on this pre-trial network was repeated 10 times for each setting. This set of 
experiments was designed, not so much to determine 'optimal' settings for these parameters, but to allow a fair 
comparison of the techniques through moderate tuning. Many more combinations would have to be tested, and many 
more runs would be required, to achieve the most effective GA or SA parameters for this problem. Even then, we 
cannot guarantee that this would be the best set for a wide range of problems.
The results of this 'tuning' phase are shown in Table 14.2, where the letters in columns 2, 3 and 4 refer to the entries in 
Table 14.1.
For each heuristic, the choice of parameter setting which gave a minimum value to a weighted sum of the minimum 
value, the average and the standard deviation was chosen as the representative setting. This turned out to be parameter 
set 3 for the GA (Roulette, One-point crossover and New Solutions) and parameter set 3 for the SA (Geometric, 
Constant and Tempering). The case for the final choice of the SA settings is not so clear cut as that for the GA.
  
< previous page
page_246
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_246.html2009-7-7 16:58:24

page_247
< previous page
page_247
next page >
Page 247
Table 14.2 The results of the 'tuning' stage of the heuristics. The letters in columns 2, 3 and 4 
refer to the entries in Table 14.1
GA trial
Select
Create
Merger
Min.
Ave.
SD
1
R
O
B
718.5 734.35
18.0
2
R
U
B
722.0 751.05
29.9
3
R
O
N
714.5 731.85
15.2
4
R
U
N
714.5 733.10
18.3
5
T
O
B
717.5 740.90
30.5
6
T
U
B
719.5 740.50
19.2
7
T
O
N
718.0 730.00
17.6
8
T
U
N
720.5 747.95 40.25
SA trial
Cooling Rate
Temp. Equil.
Cooling Mech.
Min.
Ave.
SD
1
G
C
M
716.5 748.15
29.0
2
G
F
M
721.0 731.95
15.5
3
G
C
T
718.0 737.95
18.2
4
G
F
T
749.0 783.45
25.8
5
L
C
M
717.0 751.25
38.3
6
L
F
M
717.0 743.00
28.5
7
L
C
T
718.5 744.25
38.3
8
L
F
T
717.0 747.05
32.6
 
The question remains: for how long do we run each experiment? There are many options open to us, including running 
for the same number of evaluations or for the same time. The choice we made was to stop the run when there was very 
little likelihood of further improvement. In fact, as noted earlier, each run ceased when no improvement on the best 
solution occurred after 500 evaluations. Charts showing the convergence of the fitness function for both approaches 
showed a striking similarity for any given network, indicating that we were comparing the GA and SA at a similar state 
of convergence, see [Man95a]. We note in passing, however, that the SA achieves this state in an order of magnitude 
fewer evaluations than the GA for all problems studied.
Now that we have set the parameters for the GA and the SA applications, the final step is to apply the resulting 
algorithms to a range of test networks with known traffic profiles. The networks chosen are shown in Figure 14.4.
  
< previous page
page_247
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_247.html2009-7-7 16:58:26

page_248
< previous page
page_248
next page >
Page 248
Figure 14.4 
The problem networks.
The first network is a small network, with 7 nodes and 9 links. However, the size of the network is only half the story, 
the other half being the predicted traffic profile. We assume for this network that the link capacities are uniform 
(STM1s).
Network 2 is a typically meshed network containing 15 nodes and 21 links. Once again, we assume that the link 
capacities are uniform (STM1s). The main characteristic to note is the 'bridge', i.e. the single link to one node. Although 
telecommunications networks are generally designed to avoid this situation, this may be the result of a link failure. 
Apart from this, the network possesses a fair degree of survivability under link failure.
Network 3 is a larger, highly meshed network, typical of telecommunications networks that have had to grow to meet a 
growing demand. Not only does the number of nodes and links increase, but also the quantity of traffic on the network. 
Once again, we assume that the link capacities are uniform (STM1s).
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_248.html（第 1／2 页）2009-7-7 16:58:26

page_248
< previous page
page_248
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_248.html（第 2／2 页）2009-7-7 16:58:26

page_249
< previous page
page_249
next page >
Page 249
Finally, we consider a network consisting of a 'ring of rings'. Ring structures are desirable properties in 
telecommunications networks as they offer a high degree of survivability. If a link fails, the traffic can still be routed 
through the network. Such ring-based structures are very rarely symmetrical, and the network used in this final example 
is skewed accordingly. An additional feature in this final example is the use of STM4s forming the inner ring, shown as 
bolder lines, the remaining links being STM1s. Recall that an STM4 can carry 4 times the traffic of an STM1. Any node 
transmitting to a node in another ring must use the inner ring to do so, thus the requirement for more capacity for the 
inner ring. Note also that, should there only be two levels of rings, with no crossover links, then there are at most only 
eight unique paths between any two nodes, thus simplifying the task of generating the eight shortest paths.
The results of applying the GA and the SA algorithm, with parameter settings prescribed by the pre-trial phase, are 
shown in Table 14.3. Each algorithm was applied 10 times to each network and the minimum, average and standard 
deviation of the number of evaluations (Ev) and the resulting fitness values (Fv) are shown for each.
If we restrict attention initially to the quality of solutions obtained, i.e. the fitness values obtained, there is very little to 
separate the two algorithms. For network 1, SA finds the best solution 7 times out of 10, while the GA manages it only 
4 times. Thus the SA approach has a more consistent performance for this network. However, the GA is marginally 
better for network 2 and slightly better for network 3, while SA performs more consistently for network 4.
The main difference between the two approaches is in the number of evaluations required to attain this level of quality. 
Reference to the Ev columns in Table 14.3 shows us that the GA approach requires anything between 5 and 15 times as 
many evaluations as SA to achieve the same level of quality.
For completeness, we note that, for network 1, a typical, randomly created initial solution has a fitness value in the 
range 800 to 900, and the traffic matrix used in this network, when routed using a shortest path strategy, yields one over-
utilised link and has a fitness of 369.0. For network 2, a typical initial solution has a fitness of approximately 5800 and a 
shortest path routing strategy yields two over-utilised links and a fitness of 3245.0. For network 3, a typical initial 
solution has a fitness around 40 029, while the shortest path routing generates 5 over-utilised links and a fitness of 30 
915. Finally, the shortest path routing strategy for network 4 yields a solution that has 4 overloaded links, with a fitness 
of 3007.5. This last result is depicted in Figure 14.5, where the link utilisation obtained using the shortest path strategy 
is compared with the best found using either the GA or the SA approach (Table 14.3).
  
< previous page
page_249
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_249.html2009-7-7 16:58:27

page_250
< previous page
page_250
next page >
Page 250
Table 14.3 GA/SA results for all networks. The figures in brackets are the number of times the minimum 
was found in 10 runs, if greater than 1
Genetic Algorithm
Simulated Annealing
Ev
Fv
Ev
Fv
Network 1
Min
2 207
225.5 (4)
Min
571
225.5 (7)
Average
6 916
236.9
Average
1475
230.35
St. Dev.
4 502
13.5
St. Dev.
477
7.9
Network 2
Min
23 090
2790 (10)
Min
4913
2790 (4)
Average
25 315
2790
Average
5783
2791.6
St. Dev.
1 706
0
St. Dev.
735
1.7
Network 3
Min
325 649 28 257.5
Min
22 070
28 364.0
Average
360 867 28 323.8
Average
24 788
28 519.6
St. Dev.
36 353
44.2
St. Dev.
1725
89.7
Network 4
Min
16 715
2343.5
Min
3377
2343.5
Average
22 533
2366.4
Average
4860
2365.0
St. Dev.
6104
14.5
St. Dev.
1234
10.6
 
For comparison, we report in Table 14.4 the approximate execution times for the various approaches on each problem 
network. Note that the shortest path execution time is the preprocessing time taken to determine all the 8 shortest paths 
for each problem.
Table 14.4 Approximate execution times for each algorithm
Shortest Path Times (secs)
GA Time (secs)
SA Time (secs)
Network 1
0.02
10
1
Network 2
0.2
200
20
Network 3
0.35
17 000
700
Network 4
17
350
25
 
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_250.html（第 1／2 页）2009-7-7 16:58:28

page_250
The times for the GA and SA applications are the times to finish the run, i.e. no improvement after 500 evaluations. It is 
obvious from Network 3, for instance, that the GA is continuing to improve the solution very gradually even late on in a 
run.
  
< previous page
page_250
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_250.html（第 2／2 页）2009-7-7 16:58:28

page_251
< previous page
page_251
next page >
Page 251
Figure 14.5 
Comparison of the link utilisation of different routing 
strategies for network 4. The top chart shows the link utilisation 
based on the shortest path routing, with 4 violated links. The 
bottom chart shows the best routing solution found by both GAs and 
SA.
14.5 
Current and Future Work
The work presented in this paper is currently being extended to incorporate:
Routing of different traffic types For instance, data traffic may incur different costs and require a different priority 
objective than video or voice.
Minimising delay for delay-sensitive traffic (voice, video) This is an additional objective which is built into equation 
(14.7).
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_251.html（第 1／2 页）2009-7-7 16:58:28

page_251
< previous page
page_251
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_251.html（第 2／2 页）2009-7-7 16:58:28

page_252
< previous page
page_252
next page >
Page 252
Splitting of traffic between multiple routes In many networks, the capability to split traffic across multiple links is an 
important factor with respect to reducing network overload and increasing network survivability. The actual percentage 
split may be user-controlled or built in to the optimisation process.
Multiple destination routing For video-on-demand services, for instance, there is a requirement to route the same traffic 
to multiple destinations. The problem then extends to locating the distribution nodes, i.e. nodes at which one copy of a 
traffic item arrives and from which multiple copies are sent on different links to different destinations. This problem is a 
variant of the Steiner Tree Problem in Graphs (see [KRSS93]).
Miscellaneous constraints These include the deliberate inclusion or exclusion of particular nodes and/or links. For 
example, some (emergency) services may be routed only over the shortest paths.
Provision of alternative routes This feature returns multiple routes for each service request, allowing the network 
operator or real-time router to choose the most appropriate.
Link and node diverse routing To improve survivability, multiple routes can be generated in such a manner that no node 
or link appears in more than one route.
14.6 
Conclusions
A genetic algorithm and a simulated annealing algorithm have been applied to a general routing problem on a small set 
of sample telecommunications networks. The aim of the exercise is to route the traffic across the network in an attempt 
to balance the load. The results obtained have been compared with those using a traditional shortest path strategy. For 
all the sample networks, the solutions obtained using either the GA or the SA were feasible, that is to say that no link 
was over utilised. In many of the solutions, the secondary constraint of pushing the utilisation below 50% was achieved 
on the majority of links. In all cases, the shortest path algorithm failed to find a feasible routing strategy.
Although both GA and SA are stochastic techniques, there is evidence that for this problem, both methods provide 
consistently good results, with relatively small standard deviations in the fitness values found. There was little 
dispersion in the 10 run plots of fitness versus number of evaluations for the GA and similarly for the SA, see [Man95a].
Although the GA and the SA application perform equally well, the GA displays a slight edge for the larger and more 
complex network. However, in terms of the quality of solution obtained generally, there is very little to separate the 
techniques. With more intuitive tuning of either algorithm, it is likely that comparable performance will be attained even 
for larger networks.
  
< previous page
page_252
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_252.html2009-7-7 16:58:29

page_253
< previous page
page_253
next page >
Page 253
The SA application is, however, much more effective in terms of processor time. Typically, the SA converges within 
10% to 25% of the number of function evaluations required by the GA to converge. Added to that the possibility of 
building in 'delta evaluations' to the SA application would argue in favour of this approach.
In conclusion, we can say that using global information about the state of the network can lead to better utilisation of the 
resources, and to a more survivable network as a result. This study has shown that both genetic algorithms and 
simulated annealing can be used to provide routing strategies to meet multi-objective requirements.
14.7 
Acknowledgements
The research is being undertaken within a Teaching Company Scheme programme, grant reference GR/K40086, 
between Nortel Technology (formerly BNR Europe Ltd) and the University of East Anglia.
References
[CD87] Coombs S. and Davis L. (1987) Genetic algorithms and communication link speed design: Constraints and 
operators. In Grefenstette J. (ed) Proceedings of the Second International Conference on Genetic Algorithms and their 
Applications, pages 257260. Lawrence Erlbaum Associates, Hillsdale, NJ.
[CDQ91] Cox L. A., Davis L., and Qiu Y. (1991) Dynamic anticipatory routing in circuit-switched telecommunications 
networks. In Davis L. (ed) Handbook of Genetic Algorithms. Van Nostrand, New York.
[Cer85] Cerny V. (1985) A thermodynamical approach to the travelling salesman problem: an efficient simulation 
algorithm. J. of Optimization Theory and Applic. 45: 4155.
[CM94] Crabtree I. B. and Munaf D. (1994) Planning beneficial and profitable network upgrade paths. BT Technology 
Journal 12(4).
[DC87] Davis L. and Coombs S. (1987) Genetic algorithms and communication link speed design: Theoretical 
considerations. In Grefenstette J. (ed) Proceedings of the Second International Conference on Genetic Algorithms and 
their Applications, pages 252256. Lawrence Erlbaum Associates, Hillsdale, NJ.
[DOCQ93] Davis L., Orvosh D., Cox A., and Qiu Y. (1993) A genetic algorithm for survivable network design. In 
Forrest S. (ed) Proc. Fifth Int. Conf. on Genetic Algorithms, pages 14081415. Morgan Kaufmann, San Mateo, CA.
[Dow93] Dowsland K. (1993) Simulated annealing. In Reeves C. (ed) Modern Heuristic Techniques, chapter 2, pages 
2069. Blackwell Scientific, Oxford.
[Gol89] Goldberg D. (1989) Genetic Algorithms in Search, Optimization, and Machine Learning. Addison-Wesley, 
Reading, Mass.
[KGV83] Kirkpatrick S., Gelatt C. D., and Vecchi M. P. (1983) Optimization by simulated annealing. Science 220
(4598): 671680.
[KRSS93] Kapsalis A., Rayward-Smith V. J., and Smith G. D. (1993) Solving the graphical Steiner tree problem using 
genetic algorithms. Journal of the Operational Research Society 44(4): 397406.
[LM86] Lundy M. and Mees A. (1986) Convergence of an annealing algorithm. Math. Prog. 34: 111124.
[LMRS94] Lee Y. N., McKeown G. P., and Rayward-Smith V. J. (1994) The
convoy movement problem with delays. DRA Project Final Report; completed
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_253.html（第 1／2 页）2009-7-7 16:58:30

page_253
< previous page
page_253
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_253.html（第 2／2 页）2009-7-7 16:58:30

page_254
< previous page
page_254
next page >
Page 254
December 1994.
[Man95a] Mann J. W. (1995) Applications of genetic algorithms in telecommunications. Master's thesis, University of 
East Anglia.
[Man95b] Mann J. W. (1995) X-SAmson v1.0 User Manual. Technical report, University of East Anglia.
[MKS95] Mann J. W., Kapsalis A., and Smith G. D. (1995) The GAmeter toolkit. In Rayward-Smith V. (ed) 
Applications of Modern Heuristic Methods, chapter 12, pages 195209. Alfred Waller.
[MRR+53] Metropolis N., Rosenbluth A. W., Rosenbluth M. N., Teller A. H., and Teller E. (1953) Equation of state 
calculation by fast computing machines. J. of Chem. Phys. 21: 10871091.
[New93] Newall C. (ed) (1993) Synchronous Transmission Systems. Northern Telecom Europe Ltd, London.
[PCR95] Purohit B., Clark T., and Richards T. (1995) Techniques for routing and scheduling services on a 
transmissions network. BT Technology Journal 13(1): 164172.
[PW91] Pan H. and Wang I. Y. (1991) The bandwidth allocation of ATM through genetic algorithms. In Proceedings of 
GLOBECOM'91, pages 4.4.14.4.5. IEEE Press.
[Ree93] Reeves C. (ed) (1993) Modern Heuristic Techniques for Combinatorial Problems. Blackwell Scientific.
[RS95] Rayward-Smith V. (1995) Applications of Modern Heuristic Methods. Alfred Waller.
[SHY93] Shimamoto N., Hiramatsu A., and Yamasaki K. (1993) Dynamic routing control based on a genetic algorithm. 
In Proceedings of IEEE International Conference on Neural Networks, pages 11231128. IEEE Computer Science Press, 
San Francisco, CA.
[SKRSK95] Smith G. D., Kapsalis A., Rayward-Smith V. J., and Kolen A. (1995) Radio link frequency assignment 
problem: Implementation and testing of genetic algorithms. Technical Report 2.1, EUCLID CALMA.
[Sys89] Syswerda G. (1989) Uniform crossover in genetic algorithms. In Schaffer J. (ed) Proceedings of the Third 
International Conference on Genetic Algorithms, pages 29. Morgan Kaufmann, Los Altos, CA.
[Wan92] Wang (1992) dRouting Control in Networks. PhD thesis, Imperial College, London.
[Yen71] Yen J. (1971) Finding the k shortest loopless paths in a network. Management Science 17(11): 712716.
  
< previous page
page_254
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_254.html2009-7-7 16:58:30

page_255
< previous page
page_255
next page >
Page 255
15 
A Brief Comparison of Some Evolutionary Optimization Methods
A.J. Keane
Abstract
The subject of evolutionary computing is a rapidly developing one where many new search methods are being proposed 
all the time. Inevitably, some of these new methods will not still be in current use in a few years as a small subset 
becomes the preferred choice of the optimization community. A key part in this process is the back-to-back testing of 
competing methods on test problems that simulate real problems. This brief paper presents results for such a test, 
focusing on the robustness of four methods when applied to a pair of high dimensional test functions. The work 
presented shows that some modern search methods can be highly sensitive to mistuning of their control parameters and, 
moreover, that different problems may well require radically different settings of such parameters if the searches are to 
be effective.
15.1 
Introduction
This paper briefly reviews the behaviour of four different evolutionary optimization methods when applied to a pair of 
difficult, high dimension test functions. The methods considered are the genetic algorithm (GA) [Gol89], evolutionary 
programming (EP) [Fog93], evolution strategies (ES) [BHS91] and simulated annealing (SA) [KGV83]. The two 
problems considered are the royal road function proposed by Holland, here called 'jhrr' [MFH92] and the fifty 
dimensional 'bump' problem introduced by Keane [Kea95]. These two problems are very hard for most optimizers to 
deal with: 'jhrr' is highly discontinuous and very misleading for most methods. Conversely, 'bump' is quite smooth but 
contains tens of thousands of peaks, all of similar heights. Moreover, its optimal value is defined by the presence of a 
constraint boundary.
The purpose of the work presented is to see how well the different methods cope with these tasks and, more 
importantly, to investigate what parameter settings are required
Modern HeuristicSearch Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith© 1996 John Wiley & Sons Ltd.
  
< previous page
page_255
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_255.html2009-7-7 16:58:31

page_256
< previous page
page_256
next page >
Page 256
for robust performance on these two very different problems. It should be stated at the outset, however, that in terms of 
the sophistication with which each method is implemented, the GA used is the most complex and to some extent should 
therefore be expected to give the best results. Nonetheless, the comparisons are representative of the kind of tests 
typically set for new optimization methods, i.e., comparison with the best available alternatives. Still, the reader should 
be warned that more sophisticated implementations of the other methods could be expected to improve their overall best 
performance. Finally, in this introductory session, it must be remembered that for any given problem it is always 
possible to produce a dedicated optimizer that will work better than any other, more general purpose approach: thus 
comparisons between methods should be treated with care.
15.2 
The Optimization Problems
The optimization problems to be tackled here have already been discussed in the literature and so will be only briefly 
outlined. The 'bump' problem is defined as
for
subject to
starting from
where the xi are the variables (expressed in radians) and n is the number of dimensions. This function gives a highly 
bumpy surface (Figure 15.1 shows the surface for n = 2) where the true global optimum is usually defined by the 
product constraint. The 'jhrr' problem takes a binary string as input and produces a real value which must be maximized. 
There are no constraints to be satisfied. The string is composed of 2k non-overlapping contiguous regions, each of 
length b + g. With Holland's defaults, k = 4, b = 8, g = 7, there are 16 regions of length 15, giving an overall string 
length of 240. Each region is divided into two non-overlapping pieces. The first, of length b, is called the block and the 
second of length g the gap. In the fitness calculation only the bits in the block part of each region are considered. The 
calculation consists of two steps. The part calculation adds to the block's fitness by v for every 1 it contains up to a limit 
of m*. If a block contains more than m* 1's but less than b 1's it receives v for each 1 over the limit. The default settings 
are v = 0.02 and m* = 4, so a block with six 1's is assigned a fitness of (6 4) × 0.02 = 0.04. Lastly, if a block consists
  
< previous page
page_256
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_256.html2009-7-7 16:58:32

page_257
< previous page
page_257
next page >
Page 257
entirely of 1's it receives nothing from the part calculation but it is then considered complete. This then leads to the 
bonus calculation which rewards complete blocks. The first complete block receives an additional u*, default 1.0, and 
each subsequent block u, default 0.3. Next, adjacent pairs of complete blocks are rewarded in the same way and then 
four complete blocks in a row and so on until all 16 blocks are complete. This leads to the maximum objective value 
which is 1.0 + (1.0 + 0.3) + (1.0 + 3 × 0.3) +(1.0 + 7 × 0.3) + (1.0 + 15 × 0.3) = 12.8. The presence of the gap regions, 
which do not affect the calculation, ensures that there are a considerable number of strings that have this maximal value.
Although the royal road function was designed to be easy for a GA to deal with and hard for other methods, it turns out 
that even GA's find it difficult to solve [FM93]. It is therefore useful as an extreme example of a case where the 
relationship between the variables in the function and its value are extremely non-linear, exhibiting many step-like 
changes.
15.3 
The Optimizers
Optimization problems of the sort discussed here are characterized by having many variables, highly non-linear 
relationships between the variables, and an objective function that has many peaks and troughs: in short they are 
difficult to deal with. The search for methods that can cope with such problems has led to the subject of evolutionary 
computation. Techniques in this field are characterized by a stochastic approach to the search for improved solutions, 
guided by some kind of evolutionary control strategy. There are four main methods in use today: (i) genetic algorithms 
(GA) [Gol89], where the methods of Darwinian evolution are applied to the selection of 'fitter' designs; (ii) evolutionary 
programming (EP) [Fog93], which is a more heuristic approach to the problem based on ranked mutations; (iii) 
evolution strategies (ES) [BHS91], where individual designs are mutated using adaptive mutation rates, individually 
tuned to each variable in the problem and (iv) simulated annealing (SA) [KGV83], where the control strategy is based 
on an understanding of the kinetics of solidifying crystals and where changes are sometimes allowed even if they make 
the solution worse. The first three of these methods work on groups of designs called populations while the last deals 
with only one solution at a time.
The versions of the methods used here are fairly standard and as discussed in the references but with the exceptions 
detailed below. All the methods are set up to use either a one-pass external penalty function or an evolving Fiacco-
McCormick internal and external function that is made more severe as the search progresses [Kea94]. These are, of 
course, not used on the 'jhrr' problem, which is unconstrained. The methods are also all set up to work with a binary 
encoding of the real valued design variables using up to 16 bit accuracy (default 12 bit) except the ES which works 
directly on the real valued quantities. This encoding is quite normal with a GA and also allows a simple method for 
effectively randomizing the variables for the SA and EP. Such a scheme helps the methods deal with the 'jhrr' problem, 
as there is then a direct mapping between mutations and function value. The GA used encompasses a number of new 
ideas that have proven well suited to engineering design problems [Kea93] [Kea95]. It uses an elitist survival strategy 
which ensures that the best of
  
< previous page
page_257
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_257.html2009-7-7 16:58:33

page_258
< previous page
page_258
next page >
Page 258
each generation always enters the next generation and has optional niche forming to prevent dominance by a few 
moderately successful designs preventing wide ranging searches. The main parameters used to control the method may 
be summarized as:
1. Ngen the number of generations allowed (default 10);
2. Npop the population size or number of trials used per generation which is therefore inversely related to the number of 
generations given a fixed number of trials in total (default 100);
3. P[best] the proportion of the population that survive to the next generation (default 0.8);
4. P [cross] the proportion of the surviving population that are allowed to breed (default 0.8);
5. P[invert] the proportion of this population that have their genetic material re-ordered (default 0.5);
6. P[mutation] the proportion of the new generation's genetic material that is randomly changed (default 0.005);
7. a proportionality flag which selects whether the new generation is biased in favour of the most successful members of 
the previous generation or alternatively if all P[best] survivors are propagated equally (default TRUE) and
8. the penalty function choice.
The niche forming method used here is based on MacQueen's Adaptive KMEAN algorithm [And75] which has recently 
been applied with some success to multi-peak problems [YG93]. This algorithm subdivides the population into clusters 
that have similar properties. The members of each cluster are then penalized according to how many members the 
cluster has and how far it lies from the cluster centre. It also, optionally, restricts the crossover process that forms the 
heart of the GA, so that large successful clusters mix solely with themselves. This aids convergence of the method, 
since radical new ideas are prevented from contaminating such sub-pools. The version of the algorithm used here is 
controlled by:
1. Dmin the minimum non-dimensional Euclidean distance between cluster centres, with clusters closer than this being 
collapsed (default 0.1);
2. Dmax the maximum non-dimensional Euclidean radius of a cluster, beyond which clusters sub-divide (default 0.2);
3. Nclust the initial number of clusters into which a generation is divided (default 25);
4. Nbreed the minimum number of members in a cluster before exclusive inbreeding within the cluster takes place 
(default 5) and
5. α the penalising index for cluster members, which determines how severely members sharing an over-crowded niche 
will suffer, with small numbers giving less penalty (default 0.2), i.e., the objective functions of members of a cluster of 
m solutions are scaled by mmin(α,1)[1 (E/Dmax)α] + (E/Dmax)α, where E is the Euclidean distance of the member 
from its cluster centre (which is always less than Dmax).
The EP routine is exactly as per the references, except that, as already noted, it works with a binary encoding and uses a 
choice of penalty function. It works by
  
< previous page
page_258
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_258.html2009-7-7 16:58:33

page_259
< previous page
page_259
next page >
Page 259
forming a given number of random guesses and then attempts to improve on them, maintaining a number of best 
guesses as the process continues (the population). In this method evolution is carried out by forming a mutated child 
from each parent in the population where mutation is related to the objective function so that successful ideas are 
mutated less. The objective functions of the children are then formed and a stochastic ranking process used to select the 
next parents from the combined set of parents and children. The best of the solutions is kept unchanged at each pass to 
ensure that only improvements are allowed. The mutation is controlled by a variable which sets the order of the 
mutation process with ranking. In all cases the best parent is not mutated and almost all bits in the worst are changed. 
The stochastic process for deciding which elements survive involves jousting each member against a tournament of 
other members selected at random (including possibly itself) with a score being allocated for the number in the 
tournament worse than the case being examined. Having scored all members of both parent and child generations the 
best half are kept to form the next set of parents. The average number of solutions in the tournament is set by a control 
variable. The ES routine used allows either the (µ = λ) or the (µ, λ) variants but does not use correlated mutations. It 
allows for either discrete or intermediate recombination on both design variables and the mutation control vectors, with 
the mutations being applied to non-dimensionalized versions of the design vectors. The algorithm works by forming a 
given number of random guesses and then attempts to improve on them, maintaining a number of best guesses as the 
process continues (the population). Evolution is carried out by forming a child population from mating pairs of parents, 
followed by mutation. The next generation is then made up from good members of the old and child populations. Two 
methods can be used: either the best from the combined and child populations are kept (µ = λ) or, alternatively, only the 
best from the child population are used (µ, λ), with any shortfall being made up from the best of the parents. The 
mutations are controlled by vectors of standard deviations which also evolve with the various population members. The 
rate at which these S.D.'s change is controlled by a parameter of the method. Additionally, the breeding of new children 
can be discrete and then individual design vector values are taken from either parent randomly, or intermediate when the 
values are the average of those of the parents. A similar interchange takes place between the vectors of S.D.'s of the 
parent designs. The best of the solutions is remembered at each pass to ensure that the final result is the best solution of 
all. The SA used has a logarithmic cooling schedule which is based on the following strategy:
1. choose the total number of trials N;
2. set NT = NP;
3. then for i = 1 to NT let 
 and do N/NT tests at this temperature.
Here there are three parameters: P, W and C : P lies between 0 and 1 with 0 implying a random search at one 
temperature and 1 a single trial at each temperature. W is set to lie in the range 1 to 10 and C 0.1 to 10, thus large values 
of W give a wide range of temperatures, while large values of C bias this range to lower, colder values (W of 1.0 fixes 
all temperatures to be the same while C of 2.0 biases the temperatures equally between hot and cold). Here the default 
values are 0.3333, 5.0 and 2.0 respectively. Finally the 'Boltzmann' test is applied as follows:
  
< previous page
page_259
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_259.html2009-7-7 16:58:34

page_260
< previous page
page_260
next page >
Page 260
1. always keep a better result;
2. keep a worse result if exp((Unew Uold)/(dUfirstTi)) > R where Uold is the previous objective function value, Unew is 
the new (worse) value, R is a random number between 0 and 1 and dUfirst is the magnitude of the difference between 
the objective function of the starting point in the search and the first trial at the highest temperature (and takes the place 
of the Boltzmann constant).
The use of dUfirst effectively non-dimensionalizes the annealing temperature and is vital for the schedule to be general 
purpose. The method used here also allows the user to control how big the changes are between the current point and 
the next point. This is done by using a binary discretization of the design vector and then allowing a given probability of 
a bit flip to get to the next trial (by default a 10% chance of each and every bit being flipped). This method of choosing 
neighbours allows quite radical changes to be proposed at all times and seems to help the search.
15.4 
Initial Results
Application of the various optimizers to the two test problems using 150,000 trials leads to the results of Tables 15.1 
and 15.2, which are illustrated in Figures 15.2 to 15.9. For these runs a population size of 250 was used for the GA, 
while for the EP and ES methods populations of 50 were taken. The (305, λ) version of the ES was used together with a 
child population size of 100, i.e., twice that of the parent population. The large GA population size reflects the results of 
earlier studies using the clustering method which show that it works better with bigger populations. Similar increases 
for the EP and ES methods seem to reduce their efficiency on the functions optimized here. The one pass penalty 
function was adopted for 'bump' in all cases. The figures show the optimization traces for five runs in each case and it is 
seen that all the methods find the two problems very difficult to deal with. It is notable that the searches all tend to get 
stuck while dealing with the 'jhrr' problem. This is no doubt due to the discrete, step-like nature of its objective function. 
The EP method in particular seems to suffer in this respect. Also clearly visible are the 'liquid', 'freezing' and 'frozen' 
stages in the SA runs, indicating that the generic schedules used have spanned the correct temperature ranges. It is also 
clear that the GA seems to work best and the SA worst with the other two methods somewhere in between. It is, of 
course, to be expected that all methods could be tuned to produce improved results (it is known, for example, that a 
hand-coded annealing schedule allows the SA to perform at least as well as the EP and ES methods on the problems 
studied here). A good optimizer should, however, need little attention in this regard if it is to be widely useful. The 
remainder of this paper addresses this aspect of optimization.
  
< previous page
page_260
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_260.html2009-7-7 16:58:34

page_261
< previous page
page_261
next page >
Page 261
Table 15.1 Initial optimization results for 'bump', 150 000 trials
method
1
2
3
4
5
avg.
GA
0.778
0.777
0.785
0.780
0.776
0.779
EP
0.706
0.706
0.634
0.617
0.703
0.673
ES
0.610
0.597
0.595
0.570
0.516
0.578
SA
0.389
0.423
0.357
0.423
0.384
0.395
 
Table 15.2 Initial optimization results for 'jhrr', 150 000 trials.
method
1
2
3
4
5
avg.
GA
7.30
8.02
7.08
10.68
10.66
8.75
EP
3.42
3.86
7.08
5.60
4.08
4.81
ES
7.48
5.60
7.40
5.24
5.74
6.29
SA
4.82
3.84
3.84
3.40
3.62
3.90
 
15.5 
Optimizer Tuning
Having illustrated the difficulties the four methods find with the two test problems, attention is next turned to tuning the 
parameters that control the actions of the optimizers. Selecting these parameters represents an optimization problem in 
its own right and a code has been developed to tackle this essentially recursive problem [Kea95]. It is, as might be 
expected, a difficult and time consuming problem to deal with and is not pursued further here. Nonetheless, the GA 
used does reflect the results of this earlier study on the 'bump' problem, which may go some way to explain its good 
performance. It has not, however, been tuned to 'jhrr' and so its robustness with this very different problem with the 
same parameter settings is encouraging (if the mutation control rate P[mutation] is increased, the GA often achieves the 
true optimum of 12.8 on this problem).
Having noted that a considerable amount of effort has been devoted in the past to tuning the variant of the GA used, 
attention is next focused on the EP and ES methods: is it possible to improve their performance by careful selection of 
their most important control parameters ? The EP method has two such parameters (aside from the choice of penalty 
function, run length, population size and number of bits in the encoding used here). These are the order of the mutation 
process with ranking (Imutnt = 1 for linear, 2 for quadratic, etc.,) and the tournament size used in the selection process 
as a fraction of the population size (ftourn). The ES method has slightly more
  
< previous page
page_261
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_261.html2009-7-7 16:58:35

page_262
< previous page
page_262
next page >
Page 262
controls, these being the size of the child population compared to the main or parent population (fchild), the mutation 
vector evolution control rate (∆σ), the choice of intermediate or discrete crossover for both the design and mutation 
vectors and lastly the distinction between the (µ + λ) and (µ, λ) forms of the method.
Figures 15.10 and 15.11 show contour maps of the variation in the average objective function for 'bump' and 'jhrr', 
respectively. In both cases these are averaged over five runs and are plotted for changes in Imutnt and ftourn using the 
EP method. 20 000 evaluations have been allowed per run and all other parameters kept as per the previous figures. 
Figures 15.12 and 15.13 show similar plots using the ES method for variations in ∆σ and fchild, using discrete variable 
crossover, intermediate mutation vector crossover and the (µ, λ) form of the method, as before and which trials have 
shown best suit both these problems (n.b., as has already been noted, when the child population is smaller than the 
parent population the shortfall is made up from the best of the parents). In all cases the small square marker on the plots 
indicates the default values used in producing Figures 15.2 to 15.9. The contour plots clearly illustrate how strongly the 
control parameters can affect the performance of the two optimizers.
Consider first the EP method and Figures 15.10 and 15.11. These both show that the tournament size does not strongly 
influence the method, with values in the range 0.1 to 0.3 being best for these test problems. The class of mutation has, 
by comparison, a much greater effect. Moreover, linear mutation variations seem to suit the 'jhrr' problem while cubic 
changes are more suitable for 'bump'. Thus no single setting of this parameter can be proposed as providing a general 
purpose value: the quadratic mutation variations used by default being clearly a compromise with variations around this 
value resulting in ±20% changes in performance on the test problems.
Turning next to the ES method and Figures 15.12 and 15.13, it is apparent that both plots show a valley of poor 
performance when the child population is equal in size to the parent population. Moreover, diametrically opposed 
values of both ∆σ and fchild give the best results for the two test problems studied. Worse is the fact that the range of 
performances spanned by the plots vary from 50% to +67% when compared to the default values, indicating that the 
performance of the method is very sensitive to these controls. It does, however, appear that, when correctly set up, the 
ES method can give good performance on the 'jhrr' problem (i.e., with a high value of ∆σ and small child populations).
Finally, consider SA: the method is tuned by modifying the cooling schedule. As has already been set out, the SA 
schedule used here is a general purpose one that can be controlled via three parameters affecting the ratio of trials per 
temperature to number of temperatures, the width of the range of temperatures considered and the bias of this range 
between high and low temperatures. Figures 15.5 and 15.9 demonstrate that the temperatures where most improvements 
take place lie in the middle of those considered and so the parameters P and W were varied with C fixed. This leads to 
the contour maps of Figures 15.14 and 15.15, again for 'bump' and 'jhrr', respectively. Both figures indicate that, using 
the generic cooling schedule adopted here, SA performs significantly worse than any of the three other methods, even 
for quite broad changes in the two key control parameters of the schedule. Nonetheless, the method seems to offer 
relatively consistent performance over a wide range of controls. Such variation as there is does, however, tend in 
different directions for the temperature range parameter W, with 'bump' being best handled with W = 1.2 and
  
< previous page
page_262
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_262.html2009-7-7 16:58:36

page_263
< previous page
page_263
next page >
Page 263
'jhrr' with W = 7. Both methods seem to perform best with low values of P, i.e., few distinct annealing temperatures, 
although variations in this parameter do not give such significant changes in performance. Finally, it should be noted, as 
has been mentioned earlier, that hand-coded schedules allow the SA to perform at least as well as EP and ES on these 
two problems.
15.6 
Conclusion
This brief paper has shown that, although they have some advantages, basic implementations of evolutionary 
programming, evolution strategies and simulated annealing are still all outperformed by an up-to-date GA on the test 
functions considered here. No doubt further refinements of these methods would allow their performance to be 
improved. Such improvements would need, however, to go beyond simple parameter tuning and address new 
mechanisms, such as the niche forming adopted in the GA here. It should also be noted, however, that simple public 
domain GA's [Gre84] are not so robust against such problems [Kea94]. Perhaps less dependent on the sophistication of 
the implementations is the robustness of the various methods. The GA seems to be less sensitive to its control 
parameters than the other methods used here and it is difficult to see how such robustness can be achieved with these 
methods. So, in summary, it may be said that when comparing new optimizers with existing methods described in the 
literature, care should be taken not to test simply against readily available but rather simplistic public domain versions 
of existing methods and then only to compare best results. Nonetheless, it remains true that one of the difficulties of 
working in this field remains that of getting reliable test results for up-to-date methods on functions of interest.
  
< previous page
page_263
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_263.html2009-7-7 16:58:36

page_264
< previous page
page_264
next page >
Page 264
Figure 15.1 
Contour map of the two-dimensional 'bump' problem.
Figure 15.2 
Optimization traces for 'bump' using the GA.
  
< previous page
page_264
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_264.html2009-7-7 16:58:37

page_265
< previous page
page_265
next page >
Page 265
Figure 15.3 
Optimization traces for'bump' using EP.
Figure 15.4 
Optimization traces for'bump' using ES.
  
< previous page
page_265
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_265.html2009-7-7 16:58:37

page_266
< previous page
page_266
next page >
Page 266
Figure 15.5 
Optimization traces for 'bump' using SA.
Figure 15.6 
Optimization traces for 'jhrr' using the GA.
  
< previous page
page_266
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_266.html2009-7-7 16:58:38

page_267
< previous page
page_267
next page >
Page 267
Figure 15.7 
Optimization traces for 'jhrr' using EP.
Figure 15.8 
Optimization traces for 'jhrr' using ES.
  
< previous page
page_267
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_267.html2009-7-7 16:58:38

page_268
< previous page
page_268
next page >
Page 268
Figure 15.9 
Optimization traces for 'jhrr' using SA.
Figure 15.10 
Effect on 'bump' optimizations of variations in EP 
control parameters.
  
< previous page
page_268
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_268.html2009-7-7 16:58:39

page_269
< previous page
page_269
next page >
Page 269
Figure 15.11 
Effect on 'jhrr' optimizations of variations in EP 
control parameters.
Figure 15.12 
Effect on 'bump' optimizations of variations in ES 
control parameters.
  
< previous page
page_269
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_269.html2009-7-7 16:58:39

page_270
< previous page
page_270
next page >
Page 270
Figure 15.13 
Effect on 'jhrr' optimizations of variations in ES 
control parameters.
Figure 15.14 
Effect on 'bump' optimizations of variations in SA 
control parameters.
  
< previous page
page_270
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_270.html2009-7-7 16:58:40

page_271
< previous page
page_271
next page >
Page 271
Figure 15.15 
Effect on 'jhrr' optimizations of variations in SA 
control parameters.
References
[And75] Anderberg M. (1975) Cluster Analysis for Applications. Academic Press, New York.
[BHS91] Back T., Hoffmeister F., and Schwefel H.-P. (1991) A survey of evolution strategies. In Belew R. and Booker 
L. (eds) Proceedings of the 4th International Conference on Genetic Algorithms (ICGA IV), pages 29. Morgan Kaufman 
Publishers, Inc., San Diego.
[Fog93] Fogel D. (1993) Applying evolutionary programming to selected traveling salesman problems. Cybernetics and 
Systems 24(1): 2736.
[Gol89] Goldberg D. (1989) Genetic Algorithms in Search, Optimization and Machine Learning. Addison-Wesley, 
Reading, MA.
[Gre84] Grefenstette J. (1984) A user's guide to GENESIS.
[Kea93] Keane A. (1993) Structural design for enhanced noise performance using genetic algorithm and other 
optimization techniques. In Albrecht R., Reeves C., and Steele N. (eds) Proceedings of the International Conference on 
Artificial Neural Nets and Genetic Algorithms, pages 536543. Springer-Verlag, Innsbruck.
[Kea94] Keane A. (1994) Experiences with optimizers in structural design. In Parmee I. (ed) Proceedings of the 
Conference on Adaptive Computing in Engineering Design and Control 94, pages 1427. P.E.D.C., Plymouth.
[Kea95] Keane A. (1995) Genetic algorithm optimization of multi-peak problems: studies in convergence and 
robustness. Artificial Intelligence in Engineering 9(2): 7583.
[KGV83] Kirkpatrick S., Gelatt C., and Vecchi M. (1983) Optimization by simulated annealing. Science 220(4598): 
671680.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_271.html（第 1／2 页）2009-7-7 16:58:40

page_271
[MPH92] Mitchell M., Forrest S., and Holland J. (1992) The royal road for genetic algorithms: Fitness landscapes and 
ga performance. In Proceedings of the First
  
< previous page
page_271
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_271.html（第 2／2 页）2009-7-7 16:58:40

page_272
< previous page
page_272
next page >
Page 272
European Conference on Artificial Life. MIT Press, Cambridge, MA.
[FM93] Forrest S. and Mitchell M. (1993) Relative building-block fitness and the building-block hypothesis. In Whitley 
L. (ed) Foundations of Genetic Algorithms 2. Morgan Kaufman Publishers, Inc., San Mateo.
[YG93] Yin X. and Germay N. (1993) A fast genetic algorithm with sharing scheme using cluster methods in 
multimodal function optimization. In Albrecht R., Reeves C., and Steele N. (eds) Proceedings of the International 
Conference on Artificial Neural Nets and Genetic Algorithms, pages 450457. Springer-Verlag, Innsbruck.
  
< previous page
page_272
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_272.html2009-7-7 16:58:41

page_273
< previous page
page_273
next page >
Page 273
16 
When 'Herby' met 'ElViS' Experiments with Genetics Based Learning Systems
Paul Devine, Geoff Kendal and Ray Paton
Abstract
A number of mechanisms for learning and adaptation in computational systems are based on biologically motivated 
ideas. Two major approaches are selectionism and connectionism. One technique which variously combines these two is 
the classifier system. The paper discusses the development of two such systems which address practical problems in 
different scientific disciplines: Herby a system for investigating simulations of complex natural world ecologies and 
ElViS a vision system which learns to process the event it perceives.
Some ecological phenomena do not lend themselves to conventional analytical modelling techniques, one area being the 
relationship between the behaviour of the animals within a population and the overall demography of that population. 
Herby allows us to apply evolutionary computational techniques to the investigation of the effects of evolution and 
behavioural adaptation on population dynamics. The focus of our studies with Herby is a computational ecology, that is, 
a software simulation or model (derived from some observed biological system) comprising an environment and 
inhabited by autonomous agents. The design and implementation of such a system based on plant-herbivore interactions 
is described, this utilises connectionist and selectionist techniques on a number of conceptual levels in what is 
essentially a multi-animal approach. A highly object oriented system architecture has been designed to provide 
considerable flexibility in both the nature of the environment and the design of the agent. In addition, it provides a 
simple mechanism which facilitates the possible development and transfer of a suitable level of agent functionality 
while retaining the scope for large populations, and presents some results obtained from this system. Some of the results 
of the simulation experiments are presented and discussed.
ElViS (Ecological Vision System) seeks to apply ecological ideas to the development of computer vision. Biological 
vision invariably exists only as part of a larger system a creature situated within a diverse and dynamic environment. A 
creature has needs
Modern HeuristicSearch Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith© 1996 John Wiley & Sons Ltd.
  
< previous page
page_273
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_273.html2009-7-7 16:58:41

page_274
< previous page
page_274
next page >
Page 274
and desires which must be fulfilled if it is to survive. In this ecological context, vision (and indeed sensory perception in 
general) is just one of the faculties a creature uses to achieve its goals. A theoretical basis for the incremental 
development of adaptable, robust vision systems from an initially simple level is presented. It may be the case that 
general, real world artificial vision will not be possible unless an ecological perspective is taken. This is the thesis 
underlying the development of ElViS. In order to achieve this goal of ecological vision, a learning system is required 
and ElViS uses a classifier system to do this.
Both Herby and ElViS apply ideas from ecology in various ways. The common theme, together with the commonalities 
of the learning systems, are compared and contrasted. This paper concludes with a discussion of some of the general 
principles that have been learned through this work.
16.1 
Introduction
In a previous paper one of the authors discussed the design of adaptable systems through the study and application of 
biological source models [Pat95]. The purpose of this paper is to extend some of the ideas that were developed there and 
also examine a number of issues associated with the design and implementation of complex learning systems which in 
various ways make use of ideas from ecology. Two systems which we are developing will be described Herby and 
ELViS. The former system system seeks to simulate the evolution of behaviour in a computational ecology and the 
latter applies ecological ideas about perception to the design of a computer vision system. Both systems are highly 
adaptive and rely on the capabilities of their agents to learn. We will discuss some of the motivations behind these 
designs. A number of mechanisms for learning and adaptation in computational systems are based on biologically 
motivated ideas. Two major approaches are selectionism and connectionism. One technique which variously combines 
these two is the classifier system [LDJ90].
16.2 
Genetics Based Learning
The mechanism and application of genetic algorithms are well known and in widespread use. However, they do not in 
themselves constitute entities which learn. Holland [Hol92] proposed the learning classifier system (LCS) (Figure 16.2) 
as a vehicle for the application of genetic algorithms to the problem of machine learning [LDJ90]; here we are 
specifically concerned with a subset of classifier systems known as animats [Wil90]. In this context we define these as 
autonomous entities (usually pure software constructs) existing in corresponding environments which present them with 
one or more learning tasks. This approach effectively isolates the investigation of machine learning from the 
engineering and computing problems of locomotion, gathering of sensory input etc.
Briefly, a classifier system consists of a large population of rules (condition-action productions with a numerical 
weighting), these are encoded in the form of binary and trinary strings (Figure 16.2) and compete to be acted upon. 
Rules bid to be placed on the message board when their condition parts match a message already posted;
  
< previous page
page_274
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_274.html2009-7-7 16:58:42

page_275
< previous page
page_275
next page >
Page 275
these messages are environmental input and the action parts of rules that have bid successfully in previous rounds. Bids 
are calculated on the basis of the strength of a rule and its specificity, i.e. the number of non-wildcard characters in the 
condition string. Successful bidders 'pay' their bid to the rules whose postings have triggered them rules which succeed 
in having their action parts acted upon receive any environmental payoff, be that positive, negative or nil. This 'market 
economy' ensures that payoffs are percolated down through the chains of triggering rules and is a form of credit 
assignment known as the bucket brigade algorithm. A genetic algorithm is applied to the rule population and it is the 
operation of this selectionist mechanism in rule discovery that moves the system towards a (local) behavioural optimum 
in the problem space. Classifier systems may possess a more complex structure than the basic one illustrated including 
multiple conditions, tag mediated look-ahead [Hol90] and fuzzy logic.
One of the great attractions of classifier systems over conventional symbolic AI techniques in this area of work is that 
the emphasis is on the development of a learning mechanism, this is environment and problem independent due to the 
nature of the encoding of input, rules and output.
The two systems described in this article utilise variants on the basic LCS outlined above but differ in purpose and 
emphasis. Herby uses a simple LCS as the basis of a large number autonomous agents in a computational ecology, it is 
system in which machine learning techniques are applied to the investigation of ecological phenomena. Conversely, 
Elvis possesses a more complex LCS in a more conventional animat environment, Elvis represents the application of 
ecological concepts to the problem of machine learning, specifically computer vision.
16.3 
Herby
This work is a study of the use of adaptive computational approaches as a mechanism for individual agent based 
modelling of adaptation in ecological systems; it is particularly appropriate to situations which are unsuited to 
conventional analytical techniques (i.e. using continuous mathematics). In this context we define the term computational 
ecology to mean the software simulation or model (derived from some observed biological system) comprising an 
environment and inhabited by autonomous agents. Behavioural ecology is generally concerned with the behaviour of 
animals both individually and in groups. Population biology, on the other hand, concerns itself with with the effects on 
the population as a whole of environmental and biological changes. Traditionally, neither address the demographic 
impact of behavioural changes. Recently there has been a move towards investigating the effects of individual 
behaviour on population dynamics [HM85]; much of this work employs analytical mathematical methods.
The rationale behind Herby is based on the premise that the non-linear behaviour of ecologies cannot be ignored when 
investigating behavioural and demographic phenomena. The importance of individual variation within a population on 
long-term outcomes at both the population and individual levels has already been established by individual based field 
work; the following examples [HDP84] are not strictly behavioural but provide straightforward illustrations of analysis 
at this level of the
  
< previous page
page_275
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_275.html2009-7-7 16:58:42

page_276
< previous page
page_276
next page >
Page 276
Figure 16.1 
A generalised animat.
population. The fish experiment could be interpreted as relying on behavioural effects. The development of size 
distribution in plant populations is very sensitive to the individual organisms involved and slight differences in initial 
conditions. Light is an essential resource for plants and they compete for it, consequently relative differences in plant 
height are critical to the outcome of this competition. If there is a low initial variation in height then the final population 
will be relatively homogeneous in size. Conversely, a high initial variation confers a competitive edge on the taller 
individuals which will capture an increasingly large portion of the available light and as a result grow at a higher rate. 
This positive feedback will accelerate the divergence of sizes and the final population will consist of a few very large 
individuals and a number of much smaller ones. Another instance of the importance of slight differences in initial sizes 
of individuals is a series of experiments involving populations of 250 bass in 50 gallon aquaria. Cannibalism is common 
in bass and it has been observed that a bass is capable of eating another bass up to half of its own size. The corollary is 
that in cases where there is a large initial variation the larger bass tended to eat the smaller bass and therefore grew 
rapidly; a low initial variation led to a larger population of smaller individuals as a consequence of the lack of intra-
species predation. These examples would readily lend themselves to computer simulation. Herby is designed to go 
beyond this and investigate the effects of both variation at the behavioural level and the development of behaviour itself.
The proposed system would out of necessity involve a degree of abstraction from the biological reality but it was 
essential that the essential components and relationships of a real ecology were preserved. A plant-herbivore 
arrangement was chosen and
  
< previous page
page_276
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_276.html2009-7-7 16:58:43

page_277
< previous page
page_277
next page >
Page 277
Figure 16.2 
A learning classifier system.
  
< previous page
page_277
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_277.html2009-7-7 16:58:43

page_278
< previous page
page_278
next page >
Page 278
Figure 16.3 
Herbivore classification.
developed in accordance with the classification scheme shown in Figure 16.3 [CL81] which derived its primary division 
from Monro's dichotomy [Mon67]. In this context 'herbivore' refers not only to cows, sheep etc. but to all creatures 
feeding on vegetable matter and 'grazing' encompasses all eating of plants by animals.
Non-interactive systems are typified by the herbivore's inability to influence the rate at which its resources are renewed. 
Further subdivision produces:
reactive systems where the rate of increase of the herbivores reacts to the rate at which the resources are renewed;
non-reactive systems where the rate of increase of the herbivores is independent of the parameters of resource renewal.
In interactive systems the animals do influence the rate of plant renewal, and this in turn influences the rate of increase 
of the animals. Further subdivision here gives:
laissez-faire systems where the herbivores do not interfere with each other's feeding activities;
interferential systems where the animals do interfere with each other's ability to obtain food.
16.3.1 
System Components
The system has been designed and implemented in accordance with the object oriented paradigm. However, for the 
purposes of this description it is best to focus on it in terms of the environment and the agent. These two components 
constitute the ecology.
  
< previous page
page_278
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_278.html2009-7-7 16:58:44

page_279
< previous page
page_279
next page >
Page 279
16.3.2 
The Environment
The environment comprises a 2D lattice of discrete areas; the boundaries of this lattice are manipulated to produce an 
effectively toroidal world. This format was chosen because the inclusion of boundaries would in itself impose further, 
unnecessary learning tasks on the agents that really lie beyond the scope of this investigation. This toroidal approach has 
been used by other workers [Dew84] and contrasts with simulations where the environment is explicitly bounded. The 
individual areas have two primary attributes, the type of flora growing there (the resource) and its density; inter area 
connectivity exists only along edges and not at vertices. The nature of the environment may be determined in a number 
of ways, in its present state the system allows for four different types of flora (three if the existence of barren areas is 
selected) the distribution of which may be manipulated by the user. The rate of growth and nutritional value of the 
individual species are also user set parameters.
16.3.3 
The Agent
As stated above, the agent employs a classifier system as the mechanism of adaptive change; this operates both in 
respect of 'learning' during the agent's lifetime and long-term evolutionary change over the generations. The agent also 
contains state information:
The accumulated somatic investment (ASI) a measure of the amount of food invested in the agent's own physical state;
residual reproductive value (RRV) a measure of the agent's reproductive success;
age.
A simple form of life history may be imposed on the agent by the use of a predetermined age threshold for maturity at 
which reproduction is possible; this can also be tied into the agent's ASI (see Figure 16.4).
The inputs for the classifier system consist of the following:
type of flora in the current area;
density of flora in the current area;
the presence of other agents in the current area;
the type of flora in adjoining areas;
its own fitness level (determined by somatic investment and reproductive value;
its own state of maturation
and has the following options for action:
eat;
move (with a choice of four adjacent squares this amounts to four options);
reproduce;
do nothing.
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_279.html（第 1／2 页）2009-7-7 16:58:45

page_279
< previous page
page_279
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_279.html（第 2／2 页）2009-7-7 16:58:45

page_280
< previous page
page_280
next page >
Page 280
Figure 16.4 
The agent and its world view
  
< previous page
page_280
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_280.html2009-7-7 16:58:45

page_281
< previous page
page_281
next page >
Page 281
Figure 16.5 
Adaptive Agent Population Over Time
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_281.html（第 1／2 页）2009-7-7 16:58:46

page_281
Figure 16.6 
Random Agent Population Over Time
  
< previous page
page_281
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_281.html（第 2／2 页）2009-7-7 16:58:46

page_282
< previous page
page_282
next page >
Page 282
16.3.4 
The Ecology
Naturally, the combination of the environment and a population of agents constitute the ecology. In a typical run the 
geography of the environment is specified and the attributes of the areas are individually set. The starting population of 
agents is introduced into the environment and the simulation is then left to run for either a predetermined number of 
timesteps or the extinction of the population, whichever arrives first. The setting of various flags and parameters allows 
various herbivore type systems to be simulated in accordance with the previous discussion.
In order to deduce anything about the behaviour of the agents in an adaptive context it was necessary to provide some 
sort of control simulation as a point of reference in addition to comparisons with real biological systems. A considerably 
simplified version of the agent also exists; this has no adaptive abilities whatsoever and its decisions are made on a 
purely pseudo-random basis. Environmental simulations may be performed with this non-adaptive agent but otherwise 
identical parameters; this provides a yardstick for the behaviour of the adaptive agents and illustrates the non-linearity 
introduced to the system by the adaptive agents.
16.3.5 
Interim and Prospectus
The simulations conducted so far are encouraging. One of the main problems with the system is currently being 
addressed in a new version. This relates specifically to the starting conditions of the agents. We feel that our current 
practice of generation a random rule base (this is heavily constrained in size by the necessity of running over one 
thousand animats simultaneously) and letting the agents loose in the environment is unrealistic. We are now moving to a 
system of 'selectively breeding' animats for certain traits prior to simulations.
The two plots (Figures 16.5 and 16.6) illustrate the marked difference in the behaviour observed in the population 
dynamics of the random and adaptive agents. Though the random agents peak at higher population levels in this 
interferential arrangement the striking periodicity that the population exhibits does not truly reflect biological 
observation. The adaptive agents exhibit a more complex dynamic which is reminiscent of real systems.
16.3.6 
Corollary
Herby relies upon the distributed, parallel search heuristic exemplified by genetic algorithms and classifier systems in 
general. In addition to this it embodies a modelling and simulation paradigm based on explicitly representing or 
implementing the individual elements of a system rather than assuming that the aggregate behaviour of the system may 
be described treating collections of discrete entities as discrete entities in themselves. This approach is particularly 
limited where individual variation can be a crucial factor, as is often the case in evolutionary and ecological systems.
  
< previous page
page_282
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_282.html2009-7-7 16:58:46

page_283
< previous page
page_283
next page >
Page 283
16.4 
Exploring Ecological Vision with ElViS
In this section we consider how a complex adaptable system which processes visual information has been developed. 
We show how an appreciation of a number of biologically inspired aspects of perceptual systems can help in the 
specification of a design. This biomimetic approach provides us with insights into how adaptable systems interact with 
their environments and why learning is a key to general purpose computer vision. The learning strategy adopted is based 
on an enhanced classifier system architecture.
We contend that consideration of ecological issues is a significant factor in the design of any autonomous system (see 
also [Hor94]), and it may be the case that general, real world artificial vision will not be possible unless such a 
perspective is taken. This is the hypothesis underlying the development of ElViS (Ecological Vision System).
An assumption underlying much artificial vision research is that the fundamental problem is to discover, from an image 
of a scene, a description of the objects in that scene in terms of their appearance, location, and so on [Ros88]. Within 
this framework, the task of a 'vision system' is essentially to recognize and label visible objects. In contrast, biological 
vision invariably exists only as part of a larger system a creature situated within a diverse and dynamic environment 
[Wil90]. A creature has needs and desires which must be fulfilled if it is to survive. In this ecological context, vision 
(and indeed sensory perception in general) is just one of the faculties an agent uses to achieve its goals: others include 
the ability to move in, adapt to, and manipulate the environment.
An important point becomes apparent if vision is considered from an ecological viewpoint, that being the 
complementary nature of agent and environment. Arbib notes:
'the job of the visual system is to provide the [agent] not with a representation of the world in the abstract, but 
rather with the information it needs to interact with the world' [Arb89]
Rather than following a set sequence of actions, a visual system's activity is dependent on the situation. Examples 
include: searching for a specific object; supplying contextual information based on what has already been discovered; or 
providing more detailed analysis of potentially interesting regions of the environment [Arb89]. The problems of 
recognising objects in images of arbitrary scenes are well documented in the literature [Ros88]. However, within an 
ecological framework object recognition is not the sole task of the visual processor. Ideally vision should provide an 
agent with as much information about its environment as is necessary given its current needs and situation. For instance, 
to move through a crowd one does not require a detailed description of the appearance of each individual encountered; 
however, the latter may become necessary when searching the crowd for a specific person.
Survival in all but the simplest static environments and certainly in the real world requires that an agent has some ability 
to adapt to unexpected situations. Therefore on encountering an unknown object, a visual system should not fail utterly, 
but rather provide as much information as possible, since the survival of the agent may depend on a quick but 
rudimentary assessment of the situation. A system concerned solely with labelling objects would not be able to provide 
this level of functionality. Survival
  
< previous page
page_283
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_283.html2009-7-7 16:58:47

page_284
< previous page
page_284
next page >
Page 284
needs greatly influence behaviour, and as a consequence have a powerful effect on the formation of an agent's percepts 
[Wil90]: this fundamental point can only be acknowledged in computer vision research if an ecological approach is 
taken.
Research along the lines of the recently emerging 'active vision' paradigm (e.g. [Baj88] and [Cli90]) embodies the above 
ideas to varying degrees. A major goal of the ElViS project is to develop a sound theoretical framework incorporating 
all of the above considerations upon which future artificial vision research can build.
Gibson was a strong proponent of the ecological approach to perception [Gib79]. He described an environment as 
consisting of substances, a medium, and the surfaces that separate the two. The medium permits locomotion and the 
sensing of substances at all places. Locomotion and behaviour are continually regulated by the senses. The substances 
of the environment need to be distinguished, and a powerful way of doing so is by seeing their surfaces. Gibson argued 
that the fundamental ways in which surfaces are laid out in the environment have an intrinsic meaning for behaviour, 
unlike the abstract, formal concepts of mathematical space. The theory states that 'affordances' embody this behavioural 
significance these being what the substances in the environment offer or provide the creature, for good or ill [Gib79]. 
Consequently one does not have to classify and label things in order to discover what they afford. Affordances are 
perceived through the detection of variations over time (so called 'ecological events') in the local structure of the 
'ambient optic array' the many-times-reflected light within the environment arriving at a point of observation. The 
implicit notion that vision is continuous is perhaps the most important aspect of Gibson's work with respect to artificial 
vision research: vision is concerned with detecting events in the environment, rather than some arbitrary idea of 
regularity (edges, homogeneous regions, etc..) in an image.
In order to exploit Gibsonian ideas computationally, it is necessary to investigate what constitutes an 'event' in terms of 
changes in an agent's environment which warrant changes in its actions and/or its view of the world. Two recent papers 
([MA93] and [SM94]) maintain that the discovery and analysis of unanticipated changes in the environment is both a 
necessary and fundamental requirement of vision. The ElViS project takes the notion of unexpectedness as the basis for 
ecological events. Given this, the question arises: how can detection of unexpected changes in the optic array be 
physically realised given a sequence of discrete images?
'Stimulus profusion' the sheer volume of sensory input which must be considered is a significant problem in the 
perception of real environments [Wil90]. Attempts to reduce the data to manageable levels generally rely upon finding 
tokens in the visual field (such as high contrast edges or grey-level homogeneous regions) which are then utilised by 
further processes. However, such techniques are invariably 'brittle', being heuristically chosen for their ability in very 
limited domains (e.g. [Oht83]). When one considers that most methods of token extraction are dependent upon image 
characteristics rather than environmental ones, perhaps this is not surprising. Animat theory maintains that to overcome 
the problems of stimulus profusion and brittleness, the generality and simplicity of percepts must be ensured [Wil90]: it 
is the interaction of such percepts which leads to more specific information about the environment.
The ElViS approach looks towards human vision research in an attempt to fulfil these criteria. Surfaces are perceived by 
humans from range images, photographs, line drawings, and so on where the only common theme is the preservation of 
spatial
  
< previous page
page_284
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_284.html2009-7-7 16:58:47

page_285
< previous page
page_285
next page >
Page 285
structure [WT83]. Human vision immediately detects spatial organisation such as symmetry, proximity, and repetition 
phenomena the Gestalt psychologists described (but did little to explain) in detail [Zus70]. However, most computer 
vision approaches either ignore this [Low85] or attempt ad hoc mimicry of such abilities with no theoretical grounding 
[Pop94]. At a computational level it is difficult to define what perceived structure actually is [Lig91].
One attempt at an explanation relies on the concept of 'non-accidentalness' [WT83]: that regular structural relationships 
in images (such as parallelism or collinearity) are unlikely to arise by chance, and therefore they imply possible 
underlying causal relationships in the scene. Non-accidentalness was proposed as a means to aid object recognition, but 
we extend the idea to ecological vision, the theory being that spatiotemporal relationships in the optic array reflect likely 
spatiotemporal relationships in the environment. Since the environment's structure greatly influences an agent's possible 
and required movement and other actions, the ability to perceive it is of great importance. It is hypothesised that 
spatiotemportal relationships such as parallelism, collinearity and constant velocity embody important information 
regarding the structure of the environment, and are hence worthy of attention. It should be noted that as candidate 
percepts, spatiotemporal relationships adhere to the animat recommendations (simplicity and generality), are 
independent of scale, and apply just as well to sequences of images as to the optic array, yet do not rely on arbitrary 
definitions of tokens. Hence they form an ideal basis for developing ecological vision within a camera-based system.
The key to this approach is that structure in the optic array is not likely to arise by chance. It follows that when 
spatiotemporal relationships are discovered in the optic array, to ascertain whether they have arisen by chance (i.e.: 
whether their existence is dependent on the agent's position and velocity) they must be continually monitored for 
changes. A basic requirement of ElViS is the discovery and tracking of spatiotemporal relationships in the structure of 
the optic array. These two activities cannot be considered in isolation, since they are interdependent. For example, if 
when monitoring two discovered instances of collinearity their motion appears to be related, there may be sufficient 
evidence to deduce the existence of a composite relationship embodying the two. In the real world, such an occurrence 
might be due to the detection of two sides of a single object. Variation in the structure of the optic array is dependent 
upon two factors: changes in the point of observation; and changes in the surroundings. The former can be predicted to a 
certain extent, since it is a reasonable ecological premise that an agent is party to information regarding its movement. 
Hence such changes are not wholly unexpected. The latter usually requires an adjustment of the agent's world view, so 
these changes can be classed as unexpected in an immediate sense.
Consideration of spatiotemporal relationships within the ecological framework discussed earlier leads to the following 
definition: Unexpected variations in spatiotemporal relationships discovered in the optic array form the basis of what is 
here termed an 'optic event'. An optic event indicates the need to focus activity on specific aspects of the optic array 
(and hence, likely aspects of the environment) which the agent has been hitherto unaware of. On analysing an optic 
event, the creature's view of the world should be readjusted to accommodate the new information and its actions altered 
as necessary.
  
< previous page
page_285
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_285.html2009-7-7 16:58:48

page_286
< previous page
page_286
next page >
Page 286
Figure 16.7 
Example of an optic event.
One important issue arises when vision is considered in this way. Any event which occurs may significantly affect the 
agent's view of the world, even though the event itself may appear relatively trivial when considered alone. An informal 
example demonstrates this. If the circle in Figure 16.7(a) is moving to the right, suppose in time the arrangement 
resembles 16.7(b), and later 16.7(c). Considered in isolation (ie: locally), the circle in 16.7(c) has become slightly 
deformed, and following 16.7(a) and 16.7(b) this could be considered an unexpected change: an optic event has 
occurred. Looking at the whole picture results in a strong perception that the circle has moved behind the rectangle: this 
is a global consequence of the event. This simple example illustrates how high level changes in the world view can 
emerge from relatively simple variations in the optic array. This effect is compounded in a real world environment, 
where many types of optic event may occur over a range of scales (both spatial and temporal). The emergent nature of 
the event-based approach is clear. There is no single executive control mechanism, but rather a collection of many 
interacting 'optic event detectors' which may all influence the behaviour of the visual system and the resulting view of 
the world.
The ElViS design draws on Arbib's schema theory [Arb89] in an attempt to model such interactive behaviour. A schema 
is an active modular entity embodying both perceptual structures and motor control, which is defined by its interaction 
with a physical environment (via sensors, actuators, and the exchange of information with other schemas). A schema-
based system's representation of its environment is embodied in 'the pattern of relationships between all its partial 
representations' [AC90]p.143, those being its component schemata. During execution, the use, recall and representation 
of the system's knowledge is embodied in a network of schema 'instances'. This network is dynamic: schema 
instantiation may be triggered by requests from other instances or by events in the environment; deinstantiation occurs if 
an instance has fulfilled its purpose or if it becomes obsolete given the current situation.
The notion of an 'abstract conformation' (AC) has been developed as a schema-inspired means of implementing an optic 
event detector as described above. Each type of AC has two separate components. The first the 'discovery' component 
constantly scans the optic array for new instances of a given type of spatiotemporal relationship (such as parallelism). 
Discovery of such a relationship triggers the creation of an
  
< previous page
page_286
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_286.html2009-7-7 16:58:48

page_287
< previous page
page_287
next page >
Page 287
Figure 16.8 
An abstract conformation system.
instance of the AC's second part the 'tracking' component. This latter comprises the following:
1. Properties
Physical attributes specific to the instance (e.g.: distance from agent)
Confidences that the instance is representative of underlying structure in the environment (and therefore that it is of use)
2. Behavioural specification
How the spatiotemporal relationship embodied by the AC is expected to evolve given the possible actions of the agent
What new optic events should be posted and how properties should be affected when expectancies are refuted
What effect optic events posted by other ACs have on the instance's properties
What property values/variations warrant the posting of new optic events
What property values/variations affect the instance's existence (i.e.: its relevance to the current world view)
An abstract conformation based visual system is shown in Figure 16.8. Each AC
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_287.html（第 1／2 页）2009-7-7 16:58:48

page_287
< previous page
page_287
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_287.html（第 2／2 页）2009-7-7 16:58:48

page_288
< previous page
page_288
next page >
Page 288
receives notification of the agent's actions, and may influence future actions depending on its specific role in the system. 
An extension to the system (described elsewhere) incorporates higher level goal-oriented visual activity through the 
application of 'ecological context' (see [Ken94]).
Since a schema's interaction is the crucial aspect of its operation, the actual means of implementation is not generally a 
primary issue [AC90]. However, there are several reasons why the ElViS project employs an adaptive approach. Firstly, 
animat theory maintains that adaptive techniques should be utilised wherever possible to minimise the introduction of 
new mechanisms [Wil90]. Secondly, in real world environments spatiotemporal relationships are unlikely to be precise. 
For example: parallelism undergoes distortion due to perspective effects; a body with constant velocity appears to speed 
up on approaching the agent; and all relationships are subject to sensor noise. Mathematically, it is difficult to specify 
such aberrations, but it may be possible to learn common ones by example. Finally, it may be relatively easy to describe 
roughly what optic events an abstract conformation should respond to, and how its properties should be altered 
accordingly. However, following the previous point, specifying such behaviour precisely is a problem.
Ideally, it would be preferable to describe an AC's behaviour in loose terms, and through adaptation, fine tune these to 
improve performance within the environment. A closer look at an AC's behaviour shows that it examines input 
messages in the context of internal property values, and as a result updates the internal properties and produces output 
messages. A classifier system [BGH90] (described previously in this paper) seems appropriate for this kind of task, 
provided two extensions are made, which follow directly from the above points. The first is the necessity of handling 
internal properties as well as input and output messages. The second and most important point is that it is necessary to 
handle rough or vague rules and properties. Some work along the lines of this second point has already been done, 
through the combination of classifier systems and fuzzy theory [VR91]. Briefly, fuzzy theory provides a way to 
mathematically describe vague rules such as 'if temperature is hot then turn fan up' and use such rules in complex 
inference strategies (see [Kos94] for an in depth study).
The current ElViS prototype is following a research strategy in which ACs are implemented using an enhanced fuzzy 
classifier system (EFCS). The researcher using ElViS defines the fuzzy properties and the input and output messages 
relevant to the AC, and describes the fuzzy rule base in broad terms. An example of such a rule is 'if distance is far-
away and visual-angle is small and movement-type is step-forward then change-in-visual-angle is small'.
A simulated environment has been completed which enables the automatic generation of various forms of training data. 
Selected rules from an AC's rule base are then refined through the application of this training data, and new rules are 
produced through the application of a genetic algorithm.
References
[AC90] Arbib M. and Cobas A. (1990) Schemas for prey-catching in frog and toad. In From Animals to Animats Proc 
1st Int Conf on Simulation of Adaptive
  
< previous page
page_288
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_288.html2009-7-7 16:58:49

page_289
< previous page
page_289
next page >
Page 289
Behaviour.
[Arb89] Arbib M. (1989) The Metaphorical Brain 2: Neural Networks & Beyond. Wiley.
[Baj88] Bajcsy R. (1988) Active perception. Proc IEEE Special Issue on Computer Vision 76(8).
[BGH90] Booker L., Goldberg D., and Holland J. (1990) Classifier systems and genetic algorihms. Artificial 
Intelligence.
[CL81] Coughley G. and Laughton J. (1981) Plant-herbivore systems. In May R. (ed) Theoretical Ecology. Blackwell.
[Cli90] Cliff D. (1990) The computational hoverfly: A study in computational neuroethology. In From Animals to 
Animats Proc 1st Int Conf on Simulation of Adaptive Behaviour.
[Dew84] Dewdney A. (1984) Sharks and fish wage war on the planet wa-tor. Scientific American 251(6): 1422.
[Gib79] Gibson J. (1979) The ecological approach to visual perception. Boston: Houghton-Mifflin.
[HDP84] Huston M., DeAngelis D., and Post W. (1984) New computer models unify ecological theory. Bioscience 38
(10): 682691.
[HM85] Hassell M. and May R. (1985) From individual behaviour to population dynamics. In Silby R. and Smith R. 
(eds) Behavioural Ecology Ecological Consequences of Adaptive Behaviour. Blackwell.
[Hol90] Holland J. (1990) Concerning the emergence of tag-mediated lookahead in classifier systems. In Forrest S. (ed) 
Emergent Computation, volume 42, pages 188201. MA:MIT.
[Hol92] Holland J. H. (1992) Adaptation in Natural and Artificial Systems. MIT Press.
[Hor94] Horswill I. (1994) An ecological analysis of a system for detecting nods of the head. In Paton R. C. (ed) 
Computing with Biological Metaphors. Chapman & Hall.
[Ken94] Kendal G. (December 1994) Towards artificial ecological vision. Technical report, University of Liverpool.
[Kos94] Kosko B. (1994) Fuzzy Thinking : The new science of Fuzzy Logic. Flamingo.
[LDJ90] L.Booker, D.E.Goldberg, and J.H.Holland (1990) Classifier systems and genetic algorithms. Artificial 
Intelligence.
[Lig91] Ligomenides P. A. (1991) Early perception and structural identity: neural implementation. In Proceedings of 
the SPIE Intelligent Robots and Computer Vision X, volume 1608.
[Low85] Lowe D. G. (1985) Perceptual Organisation and Visual Recognition. Kluwer Academic.
[MA93] Marshall J. A. and Alley R. K. (October 1993) A self-organizing neural network that learns to detect and 
represent visual depth from occlusion events. In Bowyer K. W. and Hall L. (eds) Proceedings of the AAAI Fall 
Symposium on Machine Learning and Computer Vision, pages 7074. Research Triangle Park, NC.
[Mon67] Monro J. (1967) The exploitation and conservation of resources by populations of insects. Journal of Animal 
Ecology 36: 53147.
[Oht83] Ohta Y. (1983) Knowledge-based interpretation of outdoor natural colour scenes. Pitman Advanced Publishing 
Program, London.
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_289.html（第 1／2 页）2009-7-7 16:58:49

page_289
[Pat95] Paton R. (1995) Designing adaptable systems through the study of biological sources. In Rayward-Smith V. (ed) 
Applications of Modern Heuristic Methods, pages 3954. Alfred Waller/Unicom.
[Pop94] Pope A. R. (January 1994) Model-based object recognition: A survey of recent research. Technical Report 
TR9404, University of British Columbia, Computer Science Dept.
[Ros88] Rosenfeld A. (1988) Computer vision: Basic principles. Proc 
IEEE 
  
< previous page
page_289
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_289.html（第 2／2 页）2009-7-7 16:58:49

page_290
< previous page
page_290
next page >
Page 290
Special Issue on Computer Vision 76(8).
[SM94] Sommerhoff G. and MacDorman K. (1994) An account of consciousness in physical and functional terms. 
Integrative Physiology and Behavioural Science 29(2): 151181.
[VR91] Valenzuela-Rendón M. (1991) The fuzzy classifier system: motivations and first results. In Schwefel H. and 
Männer R. (eds) Parallel Problem Solving from Nature, pages 330334. Springer (Verlag) Berlin.
[Wil90] Wilson S. W. (1990) The Animat path to AI. In From Animals to Animats Proc 1st Int Conf on Simulation of 
Adaptive Behaviour.
[WT83] Witkin A. P. and Tenenbaum J. M. (1983) On the Role of Structure in Vision. Academic Press (New York).
[Zus70] Zusne L. (1970) Visual Perception of Form. Academic Press (New York).
  
< previous page
page_290
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_290.html2009-7-7 16:58:50

page_291
< previous page
page_291
next page >
Page 291
Index
(MGT+OrOPT)5 heuristic, 132, 146, 147
A
acceptance function, 10
adaptable system, 274, 283
adaptive memory, vi, 41, 55
animat, 274, 275, 282, 284, 285, 287
assignment problem, 2, 4
Bi-Quadratic (Bi-QAP), 76
Quadratic (QAP), 40, 47, 53, 54, 67, 76
Quadratic Semi (QSAP), 53, 54
Radio Link Frequency, 246
ATM network, 239
attraction basin, 60, 69, 71-73
autonomous agents, 273, 275
B
Boltzmann distribution, 239
Branch-and-Bound (B&B), 4, 61, 213, 216, 218-229, 231, 232
best-first search, 218-220
branching strategy, 220, 221
depth-first search, 218-220
search tree, 220, 221, 224
bucket brigade algorithm, 275
bump problem, 256, 261, 264
C
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_291.html（第 1／3 页）2009-7-7 16:58:51

page_291
chaotic attractors, 68, 69
chromosome, 17, 18, 20, 21, 87, 92, 93, 237, 238
classifier system, 241, 273-275, 279, 282, 283, 288
Enhanced Fuzzy Classifier System (EFCS), 288
Learning Classifier System (LCS), 274, 275, 277
combinatorial optimization, 2, 4, 5, 28, 39, 40, 48, 62, 77, 97, 98, 115, 116, 124, 130, 135, 174, 213, 237
computational ecology, 273-275
computer vision, 274, 275, 283-285
constraint satisfaction, v, 25, 26
constraint satisfaction problem (CSOP), 26
crossover, 17-21, 98, 99, 101, 102, 190-193, 211, 237, 249, 258, 262
1-point, 238, 247
complementary, 99, 101, 103
1-Point (1CX), 102, 104-110
1-point (1CX), 106
2-point (2CX), 102-110
crossover operator, vi, 99, 189, 190, 192, 238
partially mapped (PMX), 21, 99, 100
UNBLOX operator, 192
uniform, 20, 22, 238
cutting planes, 40, 61
D
delta evaluations, 245, 246, 253
descent
algorithm, 130
methods, 5, 6
designs, 199, 200, 202-206, 208, 209, 211
candidate, 199, 200, 203, 205, 206
conceptual, 199
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_291.html（第 2／3 页）2009-7-7 16:58:51

page_291
creation, 199
evolutionary design system, 200, 204
fragmented, 206
illegal, 200, 203, 204, 211
objectives, 205
optimisation, 211
representation, 200, 203, 204
solid object, 200
divide-and-conquer, 87, 95
dynamic programming, 4
E
elite solutions, 46, 53, 54
ElViS, 273-275, 283-288
evolution, 64, 71-75, 191, 199, 200, 203, 204, 206, 208, 211, 236, 255, 257, 259, 262, 273, 274, 279, 283
Evolution Strategies (ES), 61, 255, 257-263, 265, 269, 270
evolutionary optimisation, 255
Evolutionary Programming (EP), 255, 257-263, 265, 267-269
F
Fast Component Mounter (FCM), 85, 86, 89, 93, 95
feasible solution, 3, 5, 7, 29, 35, 37, 39, 48, 49, 87, 93, 116, 118, 132, 155, 156, 158,
  
< previous page
page_291
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_291.html（第 3／3 页）2009-7-7 16:58:51

page_292
< previous page
page_292
next page >
Page 292
159, 162, 165, 166, 174-176, 215, 224, 231, 243
feedback-based parameter tuning, 60
fitness function, 184, 185, 191, 196, 222-224, 226, 227, 229, 230, 236, 244, 247
flight scheduling problem, 25, 27, 34, 35, 37
freight train regrouping problem (FTRP), 25, 27, 36, 37
fuzzy theory, 288
G
GAmeter, 222, 223, 236-238, 244, 245
genes, 17, 18, 95, 204, 244
Genetic Algorithms (GAs), v, vi, 1, 2, 17-22, 83, 87, 89, 92, 93, 95, 97-101, 106-111, 183-185, 189-197, 199, 200, 203, 
204, 206, 208, 209, 211, 222, 224, 225, 227, 229-233, 236-241, 244-247, 249-253, 255-258, 260, 261, 263, 264, 266, 
274, 282
alleles, 17, 18
breeding, 17
population, 17-22, 98, 106, 184, 185, 189, 193, 194, 230, 231, 236-238, 245, 258, 275
replacement, 21, 22, 192, 237
schema theorem, 18, 19, 83, 84
selection, 19, 20, 97, 98, 111, 183, 192, 236, 237, 240
roulette wheel partner selection, 20, 204
stochastic universal, 20
tournament, 20, 192
genetic art, 199
genotype, 18, 201-204
global optimum, 25
graph colouring, 159, 162
graph partitioning, 97, 100, 101, 103, 104, 111
greedy algorithm, 238
H
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_292.html（第 1／3 页）2009-7-7 16:58:52

page_292
Hamming distance, 66, 69-75, 104
Herby, 273-276, 278, 282
Heuristic Layout Generator (HLG), 87, 89, 90, 92, 93, 95
hill climbing, 101, 103, 111, 185, 188, 241
1-bit Hamming hill climber (HC), 98, 102-105, 107-110
hybridisation, 97
I
integer programming (IP), 1, 3, 41, 236, 238
interchange process, 137, 138
iterative improvement algorithm, 116-118, 123, 124
K
k-shortest path algorithm, 228, 231
Kempe chains, 161, 162, 165
knapsack problem
0-1 Knapsack Problem, 2, 3, 76
multi-constraint (MCKP), 53
Multi-Knapsack Problem, 76
L
LB5+VRP heuristic, 133, 146, 147
Line Balancing Problem, 84, 85
linear programming (LP), v, 1-4, 61, 236
local optimisation, 97-99, 111
local search, v, 1, 26, 39-41, 49, 55, 59-62, 69, 97, 98, 108, 111, 115, 116, 118, 122-124, 129, 130, 135, 156, 158, 160, 
163, 166, 173, 193, 239
LPK heuristic, 101-111
M
MacQueen's Adaptive KMEAN algorithm, 258
Markov chain, 7, 8, 61, 240
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_292.html（第 2／3 页）2009-7-7 16:58:52

page_292
MBSA heuristic, 133, 146, 147
meshed network, 248
minimum spanning tree, 49, 117
capacitated minimum spanning tree (CMST), 53, 54
modified route perturbation procedure (MRPERT), 133, 134, 143-147, 149-151
Monro's dichotomy, 278
mutation, 17-19, 21, 22, 103, 105, 107-110, 193, 211, 237, 257-259, 261, 262
mutation operator, 192, 193
N
neighbourhood, v, vi, 5, 7, 9-12, 14, 17, 22, 25, 28, 30, 33-35, 42, 49, 51, 55, 59-62, 64, 68, 69, 77, 98, 99, 101, 102, 
104, 106, 108, 111, 115-124, 129, 130, 134, 135, 137, 138, 140, 141, 151, 156, 159-162, 164, 165, 174, 175, 240, 246
restricted, 10
neighbourhood search (NS), v, 5-7, 11, 12, 17, 98-100, 111
neural networks, 76, 77
No-wait Flowshop Problem, 97, 99
O
objective function, vi, 1, 3, 5, 12, 16, 18, 26, 36, 41, 43, 53, 99, 104, 130, 135, 139, 169, 174, 175, 178, 183, 184, 199, 
204, 205, 211, 216, 218, 223, 237, 244, 257, 259, 260, 262
Operational Research (OR), vi, 1, 4, 20, 22
Optic Event, 285-287
Or-OPT exchange procedure, 132
P
packet switching networks, 238
parameter optimization problem, 83, 87, 95
Pareto optimal solutions, 211
phenotype, 18, 202-204
Potentially Optimal Set (POS), 173, 174
  
< previous page
page_292
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_292.html（第 3／3 页）2009-7-7 16:58:52

page_293
< previous page
page_293
next page >
Page 293
R
Radio Link Frequency Assignment Problem, 246
random search, 28, 62, 241, 260
Reactive Search (RS), 46, 59-61, 76, 79
recombination, 17, 20, 22, 97-101, 104, 106, 108-111, 236, 259
representation, 21, 49, 83, 84, 124, 131, 174, 189, 190, 193, 200, 211, 222, 223, 228, 230, 231, 236, 244, 283, 286
reproduction, 18, 97
route perturbation procedure (RPERT), 130, 133, 134, 143-146, 149, 151
routing, 97, 129, 130, 133, 134, 150, 151, 235, 236, 238, 241-244, 249, 251-253
dynamic anticipatory, 238
shortest path, 235, 236, 242, 249, 251
royal road function, 255, 257
S
SAmson, 236, 241, 244
scheduling, 1, 25, 27, 3437, 155, 156, 158, 159, 162, 163, 165, 169171, 220, 241
discrete-continuous, 169, 170, 172, 174, 181
multi-objective, 155
schema, 286, 287
set covering problem, 3, 86
shift process, 137, 138
Simplex algorithm, 4
Simulated Annealing (SA), v, 1, 2, 68, 1013, 17, 22, 2530, 3237, 62, 97, 103, 116, 155, 156, 158160, 163, 166, 194, 
235, 236, 239241, 244247, 249, 250, 252, 253, 255, 257260, 262, 263, 266, 268, 270, 271
annealing schedules, 2530, 32, 33, 35
cooling schedule, 7, 8, 10, 62, 240, 259, 262
Lundy & Mees Cooling, 11, 240
geometric cooling, 240
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_293.html（第 1／3 页）2009-7-7 16:58:53

page_293
Localised Simulated Annealing (LSA), 25, 2737
reannealing, 10
temperature, 711, 2530, 32, 33, 35, 37, 158, 161, 164, 165, 239241, 259, 260, 262, 263
tempering, 241, 246, 247
solid geometry constructive solid geometry (CSG), 200
spatial-partitioning, 199, 200, 204
starting solution, 5, 45, 148, 149, 174, 176, 178, 179
Steiner Tree Problem, vi, 49, 115, 121, 124
directed graphs (SPD), 48, 49, 52, 54
graphs (SP), 4850, 54, 115118, 121, 124, 252
rectilinear, 4850, 52
Steiner tree, 48, 115, 117124
strings, 1721, 61, 62, 6467, 69, 70, 77, 83, 98, 104, 183185, 189193, 244, 257, 275
bit, 70, 83, 104, 224, 227
encoding, 189191
multi-dimensional, 190
T
Tabu Search (TS), v, vi, 1, 2, 11, 12, 14, 16, 17, 22, 3943, 6466, 116, 118, 120, 129, 130, 135137, 140, 141, 143, 144, 
148, 151, 156, 158, 169, 173, 174, 176, 178, 179, 181, 236
aspiration, 12, 15, 17, 43, 44, 53, 135, 142, 143
cancellation sequence, 4246, 5255
residual cancellation sequence (RCS), 4446, 176, 177
candidate list strategies, 17, 42, 135
diversification, 12, 16, 41, 46, 5355, 6062, 6669, 73, 151
hashing, 13, 46, 65, 66, 68
hashing function, 46, 65, 69, 142
influence, 12, 15, 16, 59, 178
intensification, 12, 16, 41, 46, 55, 60, 62, 151
memory, 11, 13, 41, 43, 54, 55, 6568, 135, 140
adaptive, 41, 55
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_293.html（第 2／3 页）2009-7-7 16:58:53

page_293
attribute based, 41
explicit, 41
frequency, 12, 15, 16
frequency based, 41
long term, 15, 41, 42, 55
short term, 15, 41, 42, 55
move, 5, 6, 4148, 55, 6167, 70, 73, 135, 137, 176, 177
1-interchange (1IM), 138142
complementary, 41, 44, 46
multi-attribute, 44, 45
moving gap, 47, 48, 54
prohibition, 12, 43, 59, 61, 63, 64, 66, 67, 7377
quality, 12, 15, 16, 22, 140, 142, 148, 149
reactive (RTS), 46, 54, 59, 64, 66, 68, 73, 7679, 148
recency, 12, 13, 15, 17, 41
Reverse Elimination Method (REM), 4246, 49, 51, 5355, 65, 148, 169, 176, 177, 179
Simple Tabu Search (STS), 65, 176, 177, 179
tabu list, 11, 4147, 65, 135, 141, 142, 147, 148, 151, 169, 174, 176, 177, 179, 181
management, 39, 42, 43, 169, 176179
tabu tenure, 12, 15, 41, 45, 53
telecommunications networks, 48, 235, 238, 241, 242, 248, 249, 252
  
< previous page
page_293
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_293.html（第 3／3 页）2009-7-7 16:58:53

page_294
< previous page
page_294
Page 294
time complexity, 117119, 121123, 202, 203
timetabling, 155, 156, 158
Travelling Salesman Problem (TSP), 4, 5, 9, 10, 20, 21, 3335, 87, 98, 99, 131, 132, 138
Euclidean, 124
V
vehicle fleet mix problem (VFM), 129133, 137, 141, 149
Tabu Search vehicle fleet mix problem (TSVFM), 143147, 149, 150
video-on-demand, 252
virtual containers, 242
W
warehouse location problem (WLP), 48, 5254
  
< previous page
page_294
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_294.html2009-7-7 16:58:53

page_iii
< previous page
page_iii
next page >
Page iii
Modern Heuristic Search Methods
Edited by 
V. J. Rayward-Smith
University of East Anglia, Norwich
I. H. Osman
University of Kent, Canterbury
C. R. Reeves
Coventry University
G. D. Smith
University of East Anglia, Norwich
  
< previous page
page_iii
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_iii.html2009-7-7 16:58:53

page_iv
< previous page
page_iv
next page >
Page iv
Copyright © 1996 by John Wiley & Sons Ltd, 
Baffins Lane, Chichester, 
West Sussex PO19 1UD, England
National 01243 779777 
International (+44) 1243 779777
All rights reserved.
No part of this book may be reproduced by any means, or transmitted, or translated into a machine language without the 
written permission of the publisher.
Other Wiley Editorial Offices
John Wiley & Sons, Inc., 605 Third Avenue, 
New York, NY 10158-0012, USA
Jacaranda Wiley Ltd, 33 Park Road, Milton, 
Queensland 4064, Australia
John Wiley & Sons (Canada) Ltd, 22 Worcester Road, 
Rexdale, Ontario M9W 1L1, Canada
John Wiley & Sons (Asia) Pte Ltd, 2 Clementi Loop #02-01, 
Jin Xing Distripark, Singapore 0512
British Library Cataloguing in Publication Data
A catalogue record for this book is available from the British Library
ISBN  0 471 96280 5
Produced from camera-ready copy supplied by the editors
Printed and bound in Great Britain by Bookcraft (Bath) Ltd
This book is printed on acid-free paper responsibly manufactured from sustainable forestation, for which at least two 
trees are planted for each one used for paper production.
  
< previous page
page_iv
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_iv.html2009-7-7 16:58:54

page_ix
< previous page
page_ix
next page >
Page ix
Preface
The last few decades have been an exciting time for those of us working in optimisation. The theoretical foundations of 
complexity have been laid starting with Cook's theorem in the seventies and followed by a plethora of results on NP-
Completeness and the definition of yet more complexity classes. Added to this, we have had remarkable progress in 
mathematical programming. In Linear Programming, we have had Khachiyan's polynomial time algorithm and a whole 
spectrum of more practical, interior point algorithms. Together with techniques such as branch-and-cut and polyhedral 
combinatorics and the very significant advances in computer technology, these greatly enhance the prospect of finding 
exact solutions to difficult optimisation problems. Nevertheless, the vast majority of large scale, commercial 
optimisation problems pose a real challenge and their exact solution usually remains computationally intractable.
The Artificial Intelligence community has also made considerable advances and many of the methods that they espouse 
are now coming to fruition and can be used for practical applications. For example, expert systems, case-based 
reasoning, constraint satisfaction programming and genetic algorithms have all come from the AI stable and are now 
widely used by industry to solve large real-world problems.
Local search is a more mundane technique, at heart quite naive, but nevertheless, for many of the most difficult real-
world problems, it provides one of the very best ways of obtaining a near optimal solution with reasonable 
computational effort. During the last three decades, local search has also been developed and powerful variants of the 
simple neighbourhood search paradigm have emerged. The problem with neighbourhood search is its propensity to 
prematurely converge to a local optimum. One way of avoiding this is to use variable depth search which allows the 
exploration of a more complex neighbourhood in an attempt to find an improved solution. Other recent methods, 
sometimes known as 'modern heuristic techniques', are also designed to escape from the local optima traps. In general, 
they still do not guarantee an optimal solution nor even to meet given performance bounds. However, as this book 
shows, they often work exceptionally well in practice and have many advantages including versatility, efficiency and 
simplicity.
Three of the main modern heuristic methods are simulated annealing, tabu search and genetic algorithms but, with the 
wider use of hybrid methods, the distinctions will become increasingly blurred. Simulated annealing uses a probability 
function which
Modern Heuristic Search Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_ix
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_ix.html2009-7-7 16:58:54

page_v
< previous page
page_v
next page >
Page v
List of Contributors
E. H. L. Arts 
Department of Mathematics & Computer Science 
Eindhoven University of Technology 
Eindhoven, Netherlands
N. A. S. Azarmi 
IC-Parc 
Imperial College 
London
R. Battiti 
Laboratorio di Informatica e Calculo (LAICA) 
Dipartimento di Matematica 
Universita Segli Studi di Trento 
Trento, Italy
P. J. Bentley 
Dept. of Electrical Engineering 
University of Huddersfield 
Huddersfield
H. M. Cartwright 
Physical Chemistry Laboratory 
Oxford University 
Oxford
P. Devine 
The Liverpool Biocomputation Group 
Department of Computer Science 
University of Liverpool 
Liverpool
K. A. Dowsland 
European Business Management School 
Swansea University 
Swansea
L. J. Eshelman 
Philips Laboratories 
Briarcliff Manor 
New York, USA
C. Höhn 
Institut fur Grundlagen 
Elektrotechnik/Electronik 
Technische Universitat 
Dresden, Germany
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_v.html（第 1／2 页）2009-7-7 16:58:55

page_v
B. Jesson 
Physical Chemistry Laboratory 
Oxford University 
Oxford
Modern HeuristicSearch Methods
Editor V. J. Rayward-Smith, I. H. Osman, C. R. Reeves and G. D. Smith©1996 John Wiley & Sons Ltd.
  
< previous page
page_v
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_v.html（第 2／2 页）2009-7-7 16:58:55

page_vi
< previous page
page_vi
next page >
Page vi
Y. J. Jiang 
IC-Parc 
Imperial College 
London
J. Józefowska 
Institute of Computing Science 
Poznan University of Technology 
Poznan, Poland
A. J. Keane 
Dept. of Engineering Science 
University of Oxford 
Oxford
G. Kendall 
The Liverpool Biocomputation Group 
Department of Computer Science 
University of Liverpool 
Liverpool
Y. N. Lee 
School of Information Systems 
University of East Anglia 
Norwich
Y. H. Li 
IC-Parc 
Imperial College 
London
G. P. McKeown 
School of Information Systems 
University of East Anglia 
Norwich
J. W. Mann 
Nortel Technology 
Harlow
I. H. Osman 
Institute of Mathematics & Statistics 
University of Kent 
Canterbury
R. Paton 
The Liverpool Biocomputation Group 
Department of Computer Science 
University of Liverpool 
Liverpool
V. J. Rayward-Smith 
School of Information Systems 
University of East Anglia 
Norwich
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_vi.html（第 1／2 页）2009-7-7 16:58:55

page_vi
C. R. Reeves 
School of Mathematical 
& Information Sciences 
Coventry University 
Coventry
  
< previous page
page_vi
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_vi.html（第 2／2 页）2009-7-7 16:58:55

page_vii
< previous page
page_vii
next page >
Page vii
E. B. Richards 
IC-Parc 
Imperial College 
London
S. Salhi 
School of Mathematics & Statistics 
University of Birmingham 
Birmingham
J. D. Schaffer 
Philips Laboratories 
Briarcliff Manor 
New York, USA
M. E. M. Severens 
Dept. of Mathematics & Computer Science 
Eindhoven University of Technology 
Eindhoven, Netherlands
G. D. Smith 
School of Information Systems 
University of East Anglia 
Norwich
M. G. A. Verhoeven 
Dept. of Mathematics & Computer Science 
Eindhoven University of Technology 
Eindhoven 
Netherlands
S. Voß 
Institut für Wirtschaftswissenschaften 
Abteilung Allgemeine BWL 
Wirtschaftsinformatik 
und Informationsmanagement 
Technische Universitat Carolo-Wilhelmina 
Braunschweig, Germany
J. P. Wakefield 
Department of Electrical Engineering 
University of Huddersfield 
Huddersfield
G. Waligóra 
Institute of Computing Science 
Poznan University of Technology 
Poznan, Poland
J. Weglarz * 
Institute of Computing Science 
Poznan University of Technology 
Poznan, Poland
  
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_vii.html（第 1／2 页）2009-7-7 16:58:56

page_vii
< previous page
page_vii
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_vii.html（第 2／2 页）2009-7-7 16:58:56

page_x
< previous page
page_x
next page >
Page x
allows a move from a solution to a worse solution with a decreasing probability as the search progresses. Tabu Search 
uses a deterministic rather than stochastic search. Tabu Search moves to the best admissible solution even if this causes 
the objective function to deteriorate but, to avoid cycling, a short term adaptive memory is used. With genetic 
algorithms, a pool of solutions is used and the neighbourhood function is extended to act on pairs of solutions (using the 
so-called crossover operator. The pool is then maintained according to the 'survival of the fittest' principle. These 
methods can be hybridised with each other or with more traditional Operational Research techniques. Moreover, they 
have considerable scope for parallel or distributed implementation.
Anyone working in practical operational research, artificial intelligence or any area of decision support needs to be a 
polymath. Modern heuristic techniques have to be part of his or her weaponry. To meet the need for training in this area, 
UNICOM have run a series of conferences, workshops and seminars on this and related topics. These conferences have 
brought together academic researchers and those working in commerce and industry. The result has been most fruitful. 
In the Spring of 1995, a particularly successful meeting on 'Adaptive Decision Technologies' was held at Brunel 
University. One of the three main streams was on modern heuristic methods. This book grew out of this meeting but it is 
not simply the conference proceedings. It comprises one specially written introductory chapter and a carefully refereed 
selection of extended versions of the best papers presented at that meeting.
The book is divided into three sections. The introductory chapter provides a clear overview of the basic techniques and 
useful pointers to further reading and to current research. The second section concerns some of the most recent and 
exciting developments of the basic techniques. There are suggestions not only for extending and improving the 
techniques but also for hybridising and incorporating automatic adaptation. The third section contains a number of case 
studies, surveys and comparative studies. These span a wide range of application areas ranging from the classic Steiner 
tree problem to more practical problems arising in telecommunications and data analysis.
In the preparation of this text, a large number of people have made significant contributions. Firstly, the staff of 
UNICOM, in particular Julie Valentine, Mark Salter and Professor Gautam Mitra were instrumental in organising the 
Conference and in encouraging us to produce this book. The four of us who edited this book called upon a large number 
of referees who, as usual, we do not name individually but to whom we owe a considerable debt of gratitude. They gave 
invaluable advice and, in many cases, detailed and always constructive comments. The main credit though must go to 
the authors who have researched their chapters and worked hard to produce manuscripts in the limited time available. 
During the final preparation phase, we have had essential administrative and technical help from staff at the University 
of East Anglia, in particular from Grace Granger and from Romek Szczesniak.
V. J. RAYWARD-SMITH, 
CHAIRMAN, ADT95 CONFERENCE.
  
< previous page
page_x
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_x.html2009-7-7 16:58:56

page_xi
< previous page
page_xi
next page >
Page xi
Contents
List of Contributors
v
Preface
ix
Introduction
1 
Modern Heuristic Techniques
1
Abstract
1
1.1 Introduction
1
1.2 Combinatorial Optimization
2
1.3 The Case for Heuristics
3
1.4 Neighbourhood Search
5
1.5 Simulated Annealing
6
1.5.1 A Simple Example
7
1.5.2 Generic Decisions
8
1.5.3 Problem Specific Decisions
10
1.5.4 Other Factors
10
1.5.5 Summary
11
1.6 Tabu Search
11
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xi.html（第 1／2 页）2009-7-7 16:58:57

page_xi
1.6.1 Basic Concepts
11
1.6.2 Recency
12
1.6.3 Frequency
15
1.6.4 Summary
17
1.7 Genetic Algorithms
17
1.7.1 Crossover
18
1.7.2 Mutation
18
1.7.3 Reproduction
18
1.7.4 A Simple Example
19
1.7.5 Modifications and Enhancements
21
1.7.6 Summary
23
1.8 Conclusion
23
References
24
 
  
< previous page
page_xi
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xi.html（第 2／2 页）2009-7-7 16:58:57

page_xii
< previous page
page_xii
next page >
Page xii
Techniques
2 
Localized Simulated Annealing in Constraint Satisfaction and Optimization
27
Abstract
27
2.1 Introduction
27
2.2 Subspaces and Their Evaluation
29
2.3 Localized Simulated Annealing (LSA)
30
2.4 Analysis and Justification
32
2.5 Empirical Results
35
2.5.1 The Flight Scheduling Problem
36
2.5.2 The Freight Train Regrouping Problem
38
2.6 Conclusion
39
References
39
3 
Observing Logical Interdependencies in Tabu Search: Methods and Results
41
Abstract
41
3.1 Introduction
41
3.2 Tabu Search
42
3.2.1 Historical Development
42
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xii.html（第 1／3 页）2009-7-7 16:58:58

page_xii
3.2.2 Guiding Process Versus Application Process
43
3.2.3 Tabu Navigation Method Static Tabu Search
45
3.2.4 Reverse Elimination Method and Cancellation Sequence Method
45
3.2.5 Logical Interdependencies
48
3.2.6 Moving Gap
48
3.3 Applications in Location Planning
50
3.3.1 Network Design and Steiner Trees
50
3.3.2 Warehouse Location Problems
54
3.3.3 Further Location Problems
55
3.4 Conclusions
56
References
57
4 
Reactive Search: Toward Self-tuning Heuristics
61
Abstract
61
4.1 Reactive Search: Feedback Applied to Heuristics
61
4.2 Tabu Search: Beyond Local Search
63
4.3 Some Forms of Tabu Search
64
4.3.1 Dynamical Systems
65
4.3.2 Implementations
66
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xii.html（第 2／3 页）2009-7-7 16:58:58

page_xii
4.4 Reactive Tabu Search (RTS)
68
4.4.1 Self-adjusted Prohibition Period
68
4.4.2 The Escape Mechanism
69
4.4.3 Fast Algorithms for Using the Search History
70
4.5 Different Ways of Escaping from a Local Attractor
71
4.5.1 Strict-TS
72
4.5.2 Fixed-TS
74
 
  
< previous page
page_xii
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xii.html（第 3／3 页）2009-7-7 16:58:58

page_xiii
< previous page
page_xiii
next page >
Page xiii
4.5.3 Reactive-TS
75
4.6 Applications of Reactive Search: A Review
78
4.6.1 Combinatorial Tasks
78
4.6.2 Continuous Optimization
79
4.6.3 Sub-symbolic Machine Learning (Neural Networks)
79
4.6.4 VLSI Systems with Learning Capabilities
79
4.7 Software
80
4.8 Acknowledgements
81
References
81
5 
Combinatorial Optimization by Genetic Algorithms: The Value of the Genotype/Phenotype Distinction
85
Abstract
85
5.1 Background
85
5.2 The Line Balancing Problem
86
5.3 An Inadequate Approach
88
5.4 A GA Approach
89
5.5 How Well Does it Work?
94
5.6 Conclusions
96
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xiii.html（第 1／3 页）2009-7-7 16:58:59

page_xiii
References
96
6 
Integrating Local Search into Genetic Algorithms
99
Abstract
99
6.1 Introduction
99
6.2 GAs and NS
100
6.3 No-wait Flowshop Problem
101
6.4 Graph Partitioning
102
6.4.1 The LPK Heuristic
103
6.4.2 Complementary Crossover
103
6.4.3 1-bit Hamming Hill Climber (HC)
104
6.4.4 Mutation (MUT)
104
6.5 Evaluation of the Operators
105
6.6 Recombination
107
6.7 Conclusion
113
References
113
Case Studies
7 
Local Search for Steiner Trees in Graphs
117
Abstract
117
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xiii.html（第 2／3 页）2009-7-7 16:58:59

page_xiii
7.1 Introduction
117
7.2 Local Search
118
7.2.1 The Steiner Tree Problem in Graphs (SP)
118
7.3 Neighborhoods for the SP
119
7.4 Computational Results
126
References
129
 
  
< previous page
page_xiii
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xiii.html（第 3／3 页）2009-7-7 16:58:59

page_xiv
< previous page
page_xiv
next page >
Page xiv
8 
Local Search Strategies for the Vehicle Fleet Mix Problem
131
Abstract
131
8.1 Introduction
131
8.2 The Vehicle Fleet Mix (VFM) Problem
133
8.2.1 Representation of the VFM
133
8.2.2 Literature Review of the VFM
134
8.3 Modified RPERT Procedure (MRPERT)
135
8.4 Tabu Search
137
8.5 Computational Experience
144
8.6 Conclusion and Future Directions
151
8.7 Acknowledgement
151
References
152
9 
SA Solutions for Timetabling
155
Abstract
155
9.1 Introduction
155
9.2 Local Search, Simulated Annealing and Scheduling Problems
156
9.3 Examination Scheduling
158
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xiv.html（第 1／3 页）2009-7-7 16:59:00

page_xiv
9.4 Laboratory Scheduling
162
9.5 Conclusions
165
References
166
10 
A Tabu Search Algorithm for Some Discrete-Continuous Scheduling Problems
167
Abstract
167
10.1 Introduction
167
10.2 Problem Formulation
168
10.3 An Application of the Tabu Search Algorithm
172
10.3.1 Representation of a Feasible Solution
172
10.3.2 Creating a Starting Solution
172
10.3.3 Objective Function
173
10.3.4 Mechanism of Generating a Neighbourhood
173
10.3.5 Structure of the Tabu List
174
10.4 Computational Experiments
176
10.5 Final Remarks
179
10.6 Acknowledgements
179
References
179
11 
The Analysis of Waste Flow Data from Multi-Unit Industrial Complexes Using Genetic Algorithms
181
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xiv.html（第 2／3 页）2009-7-7 16:59:00

page_xiv
Abstract
181
11.1 Introduction
181
11.2 Applicability of the Genetic Algorithm
182
11.3 The Flow of Industrial Waste within Large Complexes
183
11.4 The Algorithm
187
11.4.1 String Encoding
187
11.4.2 The Fitness Function
189
 
  
< previous page
page_xiv
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xiv.html（第 3／3 页）2009-7-7 16:59:00

page_xv
< previous page
page_xv
next page >
Page xv
11.4.3 Crossover Operator
190
11.4.4 Mutation Operator
191
11.5 Results
192
11.5.1 Input Data Generation
192
11.5.2 Results for a Four Factory System
192
11.6 Applications of the Algorithm
194
11.7 Acknowledgements
195
References
195
12 
The Evolution of Solid Object Designs Using Genetic Algorithms
197
Abstract
197
12.1 Introduction
197
12.2 Representation
198
12.3 Correcting Illegal Designs
198
12.3.1 Method 1
198
12.3.2 Method 2
200
12.3.3 Method 3
201
12.3.4 Where to Correct the Designs
202
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xv.html（第 1／3 页）2009-7-7 16:59:02

page_xv
12.4 The Evolution of a Table
203
12.4.1 Size
203
12.4.2 Mass
204
12.4.3 Stability
205
12.4.4 Flat Surface
207
12.4.5 Supportiveness and Stability
208
12.4.6 What the Tables are Not
210
12.5 Conclusions
210
References
211
13 
The Convoy Movement Problem with Initial Delays
213
Abstract
213
13.1 Introduction
213
13.2 A Branch-and-Bound Approach
216
13.3 A Hybrid Approach : Using GAs for Delays and B&B for Paths
221
13.4 A Pure GA Approach
227
13.5 Conclusions
230
References
232
14 
A Comparison of Heuristics for Telecommunications Traffic Routing
235
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xv.html（第 2／3 页）2009-7-7 16:59:02

page_xv
Abstract
235
14.1 Introduction
235
14.2 Background
236
14.2.1 The Genetic Algorithm
236
14.2.2 Simulated Annealing
238
14.3 Problem Specification
241
14.4 Results
244
14.5 Current and Future Work
251
14.6 Conclusions
252
 
  
< previous page
page_xv
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xv.html（第 3／3 页）2009-7-7 16:59:02

page_xvi
< previous page
page_xvi
next page >
Page xvi
14.7 Acknowledgements
253
References
253
15 
A Brief Comparison of Some Evolutionary Optimization Methods
255
Abstract
255
15.1 Introduction
255
15.2 The Optimization Problems
256
15.3 The Optimizers
257
15.4 Initial Results
260
15.5 Optimizer Tuning
261
15.6 Conclusion
263
References
271
16 
When 'Herby' met 'ElViS' Experiments with Genetics Based Learning Systems
273
Abstract
273
16.1 Introduction
274
16.2 Genetics Based Learning
274
16.3 Herby
275
16.3.1 System Components
278
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xvi.html（第 1／2 页）2009-7-7 16:59:03

page_xvi
16.3.2 The Environment
279
16.3.3 The Agent
279
16.3.4 The Ecology
282
16.3.5 Interim and Prospectus
282
16.3.6 Corollary
282
16.4 Exploring Ecological Vision with ElViS
283
References
288
Index
291
 
  
< previous page
page_xvi
next page >
file:///C|/Documents and Settings/Red Apple/桌面/0471962805/files/page_xvi.html（第 2／2 页）2009-7-7 16:59:03

